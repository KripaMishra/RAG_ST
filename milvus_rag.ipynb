{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus in ./.venv/lib/python3.12/site-packages (2.4.4)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.35.12)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.66.4)\n",
      "Requirement already satisfied: setuptools>=67 in ./.venv/lib/python3.12/site-packages (from pymilvus) (70.2.0)\n",
      "Requirement already satisfied: grpcio<=1.63.0,>=1.49.1 in ./.venv/lib/python3.12/site-packages (from pymilvus) (1.63.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in ./.venv/lib/python3.12/site-packages (from pymilvus) (4.25.3)\n",
      "Requirement already satisfied: environs<=9.5.0 in ./.venv/lib/python3.12/site-packages (from pymilvus) (9.5.0)\n",
      "Requirement already satisfied: ujson>=2.0.0 in ./.venv/lib/python3.12/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in ./.venv/lib/python3.12/site-packages (from pymilvus) (2.2.2)\n",
      "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in ./.venv/lib/python3.12/site-packages (from pymilvus) (2.4.8)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: marshmallow>=3.0.0 in ./.venv/lib/python3.12/site-packages (from environs<=9.5.0->pymilvus) (3.21.3)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from environs<=9.5.0->pymilvus) (1.0.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.12/site-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pymilvus openai requests tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Steps/formatted_data.txt', \"r\") as file:\n",
    "    file_text = file.read()\n",
    "\n",
    "text_lines = file_text.split(\"# \")\n",
    "text_lines=text_lines[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "gpt4all_embd = GPT4AllEmbeddings(gpt4all_kwargs = {'allow_download': 'True'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04277612641453743,\n",
       " 0.10129088908433914,\n",
       " -0.006817298475652933,\n",
       " 0.057935841381549835,\n",
       " -0.08241669088602066,\n",
       " 0.000717853254172951,\n",
       " 0.1161101758480072,\n",
       " 0.04431316256523132,\n",
       " 0.0029810876585543156,\n",
       " -0.04819929972290993,\n",
       " 0.02081991545855999,\n",
       " -0.0255855992436409,\n",
       " 0.037783246487379074,\n",
       " 0.011655323207378387,\n",
       " 0.036262061446905136,\n",
       " 0.021460847929120064,\n",
       " 0.03147372975945473,\n",
       " 0.09942182898521423,\n",
       " -0.024596963077783585,\n",
       " -0.051403190940618515,\n",
       " -0.08457593619823456,\n",
       " -0.03413455933332443,\n",
       " -0.007385370787233114,\n",
       " -0.027680249884724617,\n",
       " 0.020576393231749535,\n",
       " 0.06600981950759888,\n",
       " 0.05382384732365608,\n",
       " 0.045808691531419754,\n",
       " -0.058252330869436264,\n",
       " -0.038526393473148346,\n",
       " -0.044863034039735794,\n",
       " -0.008439263328909874,\n",
       " 0.07171168923377991,\n",
       " -0.03242506831884384,\n",
       " -0.019726792350411415,\n",
       " -0.033407460898160934,\n",
       " -0.06818772852420807,\n",
       " 0.020988978445529938,\n",
       " 0.04738631471991539,\n",
       " 0.04877737909555435,\n",
       " -0.09575902670621872,\n",
       " -0.024076281115412712,\n",
       " -0.0031958085019141436,\n",
       " -0.01993895322084427,\n",
       " 0.07366184890270233,\n",
       " 0.00029477017233148217,\n",
       " -0.022871142253279686,\n",
       " 0.0035536703653633595,\n",
       " 0.09805549681186676,\n",
       " 0.014069848693907261,\n",
       " -0.12933111190795898,\n",
       " -0.05282123014330864,\n",
       " -0.023296410217881203,\n",
       " 0.029762502759695053,\n",
       " 0.01902463287115097,\n",
       " -0.058185677975416183,\n",
       " -0.0121430903673172,\n",
       " 0.057991500943899155,\n",
       " -0.007621044293045998,\n",
       " 0.027204452082514763,\n",
       " 0.03766965493559837,\n",
       " 0.0037694121710956097,\n",
       " -0.06131165847182274,\n",
       " -0.018667465075850487,\n",
       " 0.0624670647084713,\n",
       " -0.10097433626651764,\n",
       " 0.03191188722848892,\n",
       " 0.043262138962745667,\n",
       " -0.035637132823467255,\n",
       " 0.01870306394994259,\n",
       " 0.032853879034519196,\n",
       " 0.0441824346780777,\n",
       " 0.005057203583419323,\n",
       " -0.002854956081137061,\n",
       " -0.09481343626976013,\n",
       " -0.038086898624897,\n",
       " -0.006265816744416952,\n",
       " -0.00372689007781446,\n",
       " 0.03296860307455063,\n",
       " 0.03513038158416748,\n",
       " -0.01482244674116373,\n",
       " -0.061960410326719284,\n",
       " -0.03636256605386734,\n",
       " -0.06367615610361099,\n",
       " -0.053075388073921204,\n",
       " 0.04544311761856079,\n",
       " 0.005199937615543604,\n",
       " 0.03732270747423172,\n",
       " -0.028714710846543312,\n",
       " 0.10642586648464203,\n",
       " -0.015957428142428398,\n",
       " 0.010859799571335316,\n",
       " -0.011549624614417553,\n",
       " -0.04242714121937752,\n",
       " -0.07065771520137787,\n",
       " -0.05176636949181557,\n",
       " 0.038602832704782486,\n",
       " 0.053729098290205,\n",
       " -0.02773112989962101,\n",
       " 0.1640320122241974,\n",
       " 0.09115983545780182,\n",
       " 0.009605173021554947,\n",
       " 0.0938781276345253,\n",
       " -0.03006012737751007,\n",
       " 0.0486452616751194,\n",
       " 0.016238944604992867,\n",
       " 0.005113449413329363,\n",
       " -0.03060091845691204,\n",
       " 0.08328007906675339,\n",
       " 0.02771257795393467,\n",
       " -0.03734223172068596,\n",
       " -0.04298041760921478,\n",
       " -0.08672496676445007,\n",
       " -0.007480314467102289,\n",
       " -0.049927908927202225,\n",
       " 0.030707305297255516,\n",
       " 0.07945740222930908,\n",
       " 0.011828772723674774,\n",
       " -0.02103147841989994,\n",
       " 0.007667629513889551,\n",
       " 0.06458882987499237,\n",
       " 0.0288500115275383,\n",
       " 0.014198479242622852,\n",
       " -0.017651164904236794,\n",
       " -0.013929995708167553,\n",
       " 0.0509074330329895,\n",
       " -0.07024984061717987,\n",
       " 7.077085320467179e-34,\n",
       " -0.028465166687965393,\n",
       " -0.12732627987861633,\n",
       " 0.0499105304479599,\n",
       " 0.09733646363019943,\n",
       " -0.06299702823162079,\n",
       " -0.026724202558398247,\n",
       " -0.030288981273770332,\n",
       " 0.010149678215384483,\n",
       " -0.11243917793035507,\n",
       " -0.0712156593799591,\n",
       " -0.03424717113375664,\n",
       " 0.042354974895715714,\n",
       " -0.004546170588582754,\n",
       " -0.050451356917619705,\n",
       " 0.04902935028076172,\n",
       " 0.04288377985358238,\n",
       " -0.008352586068212986,\n",
       " -0.07541812211275101,\n",
       " -0.023757146671414375,\n",
       " 0.07779037952423096,\n",
       " -0.004950086586177349,\n",
       " 0.003183473367244005,\n",
       " 0.01912536844611168,\n",
       " 0.09053229540586472,\n",
       " -0.04255908355116844,\n",
       " -0.07069474458694458,\n",
       " 0.06920064985752106,\n",
       " -0.0035603917203843594,\n",
       " -0.04839242249727249,\n",
       " 0.05359644815325737,\n",
       " -0.011213853023946285,\n",
       " -0.022015569731593132,\n",
       " -0.019949598237872124,\n",
       " 0.01268594991415739,\n",
       " 0.02143673226237297,\n",
       " -0.05624103173613548,\n",
       " 0.05606759339570999,\n",
       " -0.052155304700136185,\n",
       " -0.03720484673976898,\n",
       " -0.12554523348808289,\n",
       " -0.023025721311569214,\n",
       " -0.014918891713023186,\n",
       " -0.037332162261009216,\n",
       " 0.019691532477736473,\n",
       " -0.04278472810983658,\n",
       " 0.04401347413659096,\n",
       " 0.05283689498901367,\n",
       " -0.0345333069562912,\n",
       " 0.016381025314331055,\n",
       " 0.0099142761901021,\n",
       " -0.0762079656124115,\n",
       " -0.02078237570822239,\n",
       " -0.018066031858325005,\n",
       " -0.0323905311524868,\n",
       " -0.001912271953187883,\n",
       " 0.005665680859237909,\n",
       " 0.04364008456468582,\n",
       " 0.009041892364621162,\n",
       " 0.005575364921241999,\n",
       " 0.03326677903532982,\n",
       " -0.027423463761806488,\n",
       " -0.003461626823991537,\n",
       " -0.07412216067314148,\n",
       " 0.0012832912616431713,\n",
       " -0.04009405896067619,\n",
       " -0.06066092848777771,\n",
       " 0.014842349104583263,\n",
       " -0.01487714797258377,\n",
       " 0.06695740669965744,\n",
       " 0.014902316965162754,\n",
       " 0.03515219688415527,\n",
       " -0.013554628938436508,\n",
       " 0.07535599172115326,\n",
       " 0.028379110619425774,\n",
       " -0.0636606439948082,\n",
       " 0.03379865363240242,\n",
       " -0.05089002847671509,\n",
       " 0.01964571513235569,\n",
       " -0.024957740679383278,\n",
       " -0.021080324426293373,\n",
       " -0.025597231462597847,\n",
       " -0.007526733912527561,\n",
       " 0.053599536418914795,\n",
       " -0.00905454158782959,\n",
       " 0.05018944665789604,\n",
       " 0.11371365189552307,\n",
       " 0.011551902629435062,\n",
       " -0.026725729927420616,\n",
       " -0.01067406963557005,\n",
       " 0.10371391475200653,\n",
       " -0.045385632663965225,\n",
       " 0.08489247411489487,\n",
       " 0.08401017636060715,\n",
       " -0.06794971227645874,\n",
       " -0.0043634227477014065,\n",
       " -2.2114599099248696e-33,\n",
       " -0.0033859789837151766,\n",
       " -0.010250397026538849,\n",
       " -0.051992230117321014,\n",
       " 0.036521587520837784,\n",
       " 0.023030897602438927,\n",
       " 0.018148133531212807,\n",
       " 0.020568206906318665,\n",
       " 0.03896027058362961,\n",
       " -0.05299153923988342,\n",
       " 0.016705073416233063,\n",
       " -0.02033732645213604,\n",
       " -0.1185540109872818,\n",
       " 0.08857133984565735,\n",
       " 0.07446790486574173,\n",
       " 0.0018551073735579848,\n",
       " 0.0037027772050350904,\n",
       " 0.058799922466278076,\n",
       " -0.034291401505470276,\n",
       " -0.09375840425491333,\n",
       " -0.00952152069658041,\n",
       " -0.030522484332323074,\n",
       " 0.03817564621567726,\n",
       " 0.028307702392339706,\n",
       " 0.028624463826417923,\n",
       " -0.0705191045999527,\n",
       " -0.045581381767988205,\n",
       " 0.03097776509821415,\n",
       " 0.00020120531553402543,\n",
       " -0.18353135883808136,\n",
       " 0.06563328206539154,\n",
       " -0.04645363986492157,\n",
       " -0.06526657193899155,\n",
       " -0.015091964043676853,\n",
       " 0.008097954094409943,\n",
       " 0.007462373469024897,\n",
       " 0.02785084769129753,\n",
       " 0.05072192847728729,\n",
       " 0.011309022083878517,\n",
       " 0.004025622736662626,\n",
       " 0.052011583000421524,\n",
       " 0.0146428057923913,\n",
       " 0.032365333288908005,\n",
       " -0.04975094646215439,\n",
       " 0.10137056559324265,\n",
       " -0.04198186844587326,\n",
       " -0.04654723033308983,\n",
       " -0.07812275737524033,\n",
       " 0.06283418089151382,\n",
       " 0.054703328758478165,\n",
       " -0.06726143509149551,\n",
       " -0.02197057008743286,\n",
       " 0.0012696362100541592,\n",
       " -0.03956541791558266,\n",
       " -0.0414552316069603,\n",
       " 0.0019320260034874082,\n",
       " -0.01558582205325365,\n",
       " -0.053294118493795395,\n",
       " -0.027992818504571915,\n",
       " 0.01109349261969328,\n",
       " 0.08210854977369308,\n",
       " -0.020047539845108986,\n",
       " -0.022441323846578598,\n",
       " 0.010150963440537453,\n",
       " -0.01676160655915737,\n",
       " 0.07427572458982468,\n",
       " 0.05760955065488815,\n",
       " -0.005018450319766998,\n",
       " 0.07068724930286407,\n",
       " 0.01723119057714939,\n",
       " -0.0037021676544100046,\n",
       " 0.08525118231773376,\n",
       " -0.012019726447761059,\n",
       " -0.019085729494690895,\n",
       " 0.04907912015914917,\n",
       " -0.014810356311500072,\n",
       " -0.07906260341405869,\n",
       " -0.09420084208250046,\n",
       " 0.08342834562063217,\n",
       " -0.03704242408275604,\n",
       " -0.007130368147045374,\n",
       " 0.06288130581378937,\n",
       " -0.04418405890464783,\n",
       " -0.08025369048118591,\n",
       " 0.02072712779045105,\n",
       " 0.006426321342587471,\n",
       " -0.02967206947505474,\n",
       " 0.01905231922864914,\n",
       " 0.00218578171916306,\n",
       " -0.043288227170705795,\n",
       " -0.03499336913228035,\n",
       " 0.06488646566867828,\n",
       " 0.07342064380645752,\n",
       " -0.05949019268155098,\n",
       " -0.014575272798538208,\n",
       " -0.05223521962761879,\n",
       " -1.7085641346170632e-08,\n",
       " -0.056838952004909515,\n",
       " -0.03275628387928009,\n",
       " -0.032350461930036545,\n",
       " -0.010732723399996758,\n",
       " 0.08565448224544525,\n",
       " 0.016581512987613678,\n",
       " -0.008787048980593681,\n",
       " -0.0010269595077261329,\n",
       " 0.05051892623305321,\n",
       " 0.053463391959667206,\n",
       " 0.08247552812099457,\n",
       " 0.06312523782253265,\n",
       " -0.004532268736511469,\n",
       " 0.11822961270809174,\n",
       " 0.0031118346378207207,\n",
       " -0.03550570830702782,\n",
       " -0.007206404116004705,\n",
       " 0.1068253368139267,\n",
       " 0.008788722567260265,\n",
       " 0.029456820338964462,\n",
       " 0.04044097661972046,\n",
       " -0.017744436860084534,\n",
       " -0.05994826927781105,\n",
       " 0.010387193411588669,\n",
       " 0.010493290610611439,\n",
       " 0.0118027338758111,\n",
       " -0.020758135244250298,\n",
       " 0.05732450634241104,\n",
       " 0.11578460782766342,\n",
       " -0.016871223226189613,\n",
       " -0.07340902090072632,\n",
       " 0.09808021783828735,\n",
       " -0.0596063956618309,\n",
       " -0.08538591116666794,\n",
       " -0.13958609104156494,\n",
       " -0.05691966414451599,\n",
       " -0.006801377050578594,\n",
       " 0.014844778925180435,\n",
       " -0.019580597057938576,\n",
       " 0.026565207168459892,\n",
       " -0.010721970349550247,\n",
       " 0.0024585346691310406,\n",
       " 0.0001852424320532009,\n",
       " 0.0105126416310668,\n",
       " -0.051605723798274994,\n",
       " -0.00021051619842182845,\n",
       " 0.048028141260147095,\n",
       " 0.02932359091937542,\n",
       " -0.04967796057462692,\n",
       " -0.11924071609973907,\n",
       " -0.0463096983730793,\n",
       " 0.007081341464072466,\n",
       " 0.09044080227613449,\n",
       " 0.060250964015722275,\n",
       " 0.04324029013514519,\n",
       " 0.007502180058509111,\n",
       " 0.0647231787443161,\n",
       " 0.012481173500418663,\n",
       " -0.0450659915804863,\n",
       " 0.018630066886544228,\n",
       " 0.10382872074842453,\n",
       " 0.018891187384724617,\n",
       " -0.023241084069013596,\n",
       " -0.011421342380344868]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"jajajaj\"\n",
    "gpt4all_embd.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GPT4AllEmbeddings\n__root__\n  gpt4all.gpt4all.Embed4All() argument after ** must be a mapping, not NoneType (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT4AllEmbeddings\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2.gguf2.f16.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4AllEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GPT4AllEmbeddings\n__root__\n  gpt4all.gpt4all.Embed4All() argument after ** must be a mapping, not NoneType (type=type_error)"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    text=text\n",
    "    model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "    gpt4all_kwargs = {'allow_download': 'True'}\n",
    "    embeddings = GPT4AllEmbeddings(\n",
    "        model_name=model_name,\n",
    "        gpt4all_kwargs=gpt4all_kwargs\n",
    "    )\n",
    "    return embeddings.embed_query(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Steps/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=\"AIzaSyBkusJgxx02K6ecX5FAkwTIoqhtoJ7xf_8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**How to Brush Your Teeth**\\n\\n1. **Gather your supplies.** You will need a toothbrush, toothpaste, and a glass of water.\\n2. **Wet your toothbrush.** Run the toothbrush under the tap until it is damp.\\n3. **Apply toothpaste to the toothbrush.** Squeeze a small amount of toothpaste onto the toothbrush.\\n4. **Start brushing.** Brush your teeth in small circles, starting at the top and working your way down. Be sure to brush all surfaces of your teeth, including the front, back, and sides.\\n5. **Floss your teeth.** After brushing, floss your teeth to remove any food particles that may be stuck between them.\\n6. **Rinse your mouth.** Rinse your mouth with water to remove any residual toothpaste and food particles.\\n7. **Dry your toothbrush.** After brushing, dry your toothbrush off and store it in a clean, dry place.\\n\\n**Tips for brushing your teeth:**\\n\\n* Brush your teeth twice a day, for at least two minutes each time.\\n* Use a soft-bristled toothbrush.\\n* Brush in a gentle, circular motion.\\n* Don't brush too hard, as this can damage your teeth and gums.\\n* Floss your teeth once a day, or more if you have braces or other dental appliances.\\n* See your dentist regularly for checkups and cleanings.\\n\\n**By following these tips, you can help keep your teeth and gums healthy and strong.**\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('how to brush my teeth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='heheheheheheeh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=len(embed_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(uri=\"./milvus_demo2.db\")\n",
    "\n",
    "collection_name = \"my_rag_collection\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# milvus_client.create_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     dimension=embedding_dim,\n",
    "#     metric_type=\"IP\",  # Inner product distance\n",
    "#     consistency_level=\"Strong\",  # Strong consistency level\n",
    "# )\n",
    "\n",
    "## note that in above line, we haven't declared the schema field, this leads to auto schema creation, and auto indexing allocation \n",
    "\n",
    "# from pymilvus import MilvusClient, DataType\n",
    "\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=5)\n",
    "\n",
    "\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=\"my_rag_collection\", \n",
    "    schema=schema, \n",
    ")\n",
    "index_params = MilvusClient.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\",\n",
    "    metric_type=\"COSINE\",\n",
    "    index_type=\"FLAT\",\n",
    "    index_name=\"vector_index\",\n",
    "    params={ \"nlist\": 128 }\n",
    ")\n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=\"my_rag_collection\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the indexing details;\n",
    "\n",
    "res = client.list_indexes(\n",
    "    collection_name=\"customized_setup\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "\n",
    "\n",
    "# res = client.describe_index(\n",
    "#     collection_name=\"customized_setup\",\n",
    "#     index_name=\"vector_index\"\n",
    "# )\n",
    "\n",
    "# print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines=text_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NvidiaSearchInput.mount({\"apiUrl\": \"https://api-prod.nvidia.com/search/graphql\", \"destination\": \"search.html\", \"path\": \"/cuda/\", \"site\": \"https://docs.nvidia.com\"}); Release Notes CUDA Features Archive EULA Installation Guides Quick Start Guide Installation Guide Windows Installation Guide Linux Programming Guides Programming Guide Best Practices Guide Maxwell Compatibility Guide Pascal Compatibility Guide Volta Compatibility Guide Turing Compatibility Guide NVIDIA Ampere GPU Architecture Compatibility Guide Hopper Compatibility Guide Ada Compatibility Guide Maxwell Tuning Guide Pascal Tuning Guide Volta Tuning Guide Turing Tuning Guide NVIDIA Ampere GPU Architecture Tuning Guide Hopper Tuning Guide Ada Tuning Guide PTX ISA Video Decoder PTX Interoperability Inline PTX Assembly CUDA API References CUDA Runtime API CUDA Driver API CUDA Math API cuBLAS cuDLA API NVBLAS nvJPEG cuFFT CUB CUDA C++ Standard Library cuFile API Reference Guide cuRAND cuSPARSE NPP nvJitLink nvFatbin NVRTC (Runtime Compilation) Thrust cuSOLVER PTX Compiler API References PTX Compiler APIs Miscellaneous CUDA Demo Suite CUDA on WSL CUDA on EFLOW Multi-Instance GPU (MIG) CUDA Compatibility CUPTI Debugger API GPUDirect RDMA GPUDirect Storage vGPU Tools NVCC CUDA-GDB Compute Sanitizer Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Systems Nsight Compute Nsight Visual Studio Edition Profiler CUDA Binary Utilities White Papers Floating Point and IEEE 754 Incomplete-LU and Cholesky Preconditioned Iterative Methods Application Notes CUDA for Tegra Compiler SDK libNVVM API libdevice User’s Guide NVVM IR landing » CUDA Toolkit Documentation 12.5 Update 1 CUDA Toolkit Archive - Send Feedback CUDA Toolkit Documentation 12.5 Update 1 \\uf0c1 Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications.\\nWith the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\\nThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application.\\nUsing built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs.\\nRelease Notes The Release Notes for the CUDA Toolkit.\\nCUDA Features Archive The list of CUDA features by release.\\nEULA The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools.\\nIf you do not agree with the terms and conditions of the license agreement, then do not download or use the software.\\nInstallation Guides \\uf0c1 Quick Start Guide This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system.\\nInstallation Guide Windows This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems.\\nInstallation Guide Linux This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems.\\nProgramming Guides \\uf0c1 Programming Guide This guide provides a detailed discussion of the CUDA programming model and programming interface.\\nIt then describes the hardware implementation, and provides guidance on how to achieve maximum performance.\\nThe appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API.\\nBest Practices Guide This guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures.\\nThe intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit.\\nMaxwell Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Maxwell.\\nPascal Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Pascal.\\nVolta Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Volta.\\nTuring Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Turing.\\nNVIDIA Ampere GPU Architecture Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.\\nHopper Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs.\\nThis document provides guidance to ensure that your software applications are compatible with Hopper architecture.\\nAda Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs.\\nThis document provides guidance to ensure that your software applications are compatible with Ada architecture.\\nMaxwell Tuning Guide Maxwell is NVIDIA’s 4th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features.\\nPascal Tuning Guide Pascal is NVIDIA’s 5th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features.\\nVolta Tuning Guide Volta is NVIDIA’s 6th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features.\\nTuring Tuning Guide Turing is NVIDIA’s 7th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features.\\nNVIDIA Ampere GPU Architecture Tuning Guide NVIDIA Ampere GPU Architecture is NVIDIA’s 8th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture’s features.\\nHopper Tuning Guide Hopper GPU Architecture is NVIDIA’s 9th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture’s features.\\nAda Tuning Guide The NVIDIA® Ada GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications.\\nThe NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes.\\nThis guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features.\\nPTX ISA This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).\\nPTX exposes the GPU as a data-parallel computing device.\\nVideo Decoder NVIDIA Video Decoder (NVCUVID) is deprecated.\\nInstead, use the NVIDIA Video Codec SDK ( https://developer.nvidia.com/nvidia-video-codec-sdk ).\\nPTX Interoperability This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code.\\nInline PTX Assembly This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code.\\nIt describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter.\\nCUDA API References \\uf0c1 CUDA Runtime API Fields in structures might appear in order that is different from the order of declaration.\\nCUDA Driver API Fields in structures might appear in order that is different from the order of declaration.\\nCUDA Math API The CUDA math API.\\ncuBLAS The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime.\\nIt allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs.\\ncuDLA API The cuDLA API.\\nNVBLAS The NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library.\\nnvJPEG The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications.\\ncuFFT The cuFFT library user guide.\\nCUB The user guide for CUB.\\nCUDA C++ Standard Library The API reference for libcu++, the CUDA C++ standard library.\\ncuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology.\\ncuRAND The cuRAND library user guide.\\ncuSPARSE The cuSPARSE library user guide.\\nNPP NVIDIA NPP is a library of functions for performing CUDA accelerated processing.\\nThe initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas.\\nNPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains.\\nThe NPP library is written to maximize flexibility, while maintaining high performance.\\nnvJitLink The user guide for the nvJitLink library.\\nnvFatbin The user guide for the nvFatbin library.\\nNVRTC (Runtime Compilation) NVRTC is a runtime compilation library for CUDA C++.\\nIt accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX.\\nThe PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API.\\nThis facility can often provide optimizations and performance not possible in a purely offline static compilation.\\nThrust The C++ parallel algorithms library.\\ncuSOLVER The cuSOLVER library user guide.\\nPTX Compiler API References \\uf0c1 PTX Compiler APIs This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library.\\nMiscellaneous \\uf0c1 CUDA Demo Suite This document describes the demo applications shipped with the CUDA Demo Suite.\\nCUDA on WSL This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2).\\nThe guide covers installation and running CUDA applications and containers in this environment.\\nMulti-Instance GPU (MIG) This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA® A100 GPU.\\nCUDA Compatibility This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade.\\nCUPTI The CUPTI-API.\\nThe CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications.\\nDebugger API The CUDA debugger API.\\nGPUDirect RDMA A technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express.\\nThis document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model.\\nGPUDirect Storage The documentation for GPUDirect Storage.\\nvGPU vGPUs that support CUDA.\\nTools \\uf0c1 NVCC This is a reference document for nvcc, the CUDA compiler driver.\\nnvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process.\\nCUDA-GDB The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware.\\nCUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger.\\nCompute Sanitizer The user guide for Compute Sanitizer.\\nNsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Eclipse Plugins Edition getting started guide Nsight Systems The documentation for Nsight Systems.\\nNsight Compute The NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications.\\nIt provides detailed performance metrics and API debugging via a user interface and command line tool.\\nNsight Visual Studio Edition The documentation for Nsight Visual Studio Edition.\\nProfiler This is the guide to the Profiler.\\nCUDA Binary Utilities The application notes for cuobjdump, nvdisasm, and nvprune.\\nWhite Papers \\uf0c1 Floating Point and IEEE 754 A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs.\\nThe purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide.\\nIncomplete-LU and Cholesky Preconditioned Iterative Methods In this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods.\\nWe focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively.\\nAlso, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms.\\nApplication Notes \\uf0c1 CUDA for Tegra This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU).\\nIt also discusses EGL interoperability.\\nCompiler SDK \\uf0c1 libNVVM API The libNVVM API.\\nlibdevice User’s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels.\\nNVVM IR NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR.\\nThe NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels).\\nHigh-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2007-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(true); });1.\\nCUDA 12.5 Update 1 Release Notes 1.1.\\nCUDA Toolkit Major Component Versions 1.2.\\nNew Features 1.2.1.\\nGeneral CUDA 1.2.2.\\nCUDA Compiler 1.2.3.\\nCUDA Developer Tools 1.3.\\nResolved Issues 1.3.1.\\nCUDA Compiler 1.4.\\nKnown Issues and Limitations 1.5.\\nDeprecated or Dropped Features 1.5.1.\\nDeprecated or Dropped Architectures 1.5.2.\\nDeprecated Operating Systems 1.5.3.\\nDeprecated Toolchains 1.5.4.\\nCUDA Tools 2.\\nCUDA Libraries 2.1. cuBLAS Library 2.1.1. cuBLAS: Release 12.5 Update 1 2.1.2. cuBLAS: Release 12.5 2.1.3. cuBLAS: Release 12.4 Update 1 2.1.4. cuBLAS: Release 12.4 2.1.5. cuBLAS: Release 12.3 Update 1 2.1.6. cuBLAS: Release 12.3 2.1.7. cuBLAS: Release 12.2 Update 2 2.1.8. cuBLAS: Release 12.2 2.1.9. cuBLAS: Release 12.1 Update 1 2.1.10. cuBLAS: Release 12.0 Update 1 2.1.11. cuBLAS: Release 12.0 2.2. cuFFT Library 2.2.1. cuFFT: Release 12.5 2.2.2. cuFFT: Release 12.4 Update 1 2.2.3. cuFFT: Release 12.4 2.2.4. cuFFT: Release 12.3 Update 1 2.2.5. cuFFT: Release 12.3 2.2.6. cuFFT: Release 12.2 2.2.7. cuFFT: Release 12.1 Update 1 2.2.8. cuFFT: Release 12.1 2.2.9. cuFFT: Release 12.0 Update 1 2.2.10. cuFFT: Release 12.0 2.3. cuSOLVER Library 2.3.1. cuSOLVER: Release 12.5 Update 1 2.3.2. cuSOLVER: Release 12.5 2.3.3. cuSOLVER: Release 12.4 Update 1 2.3.4. cuSOLVER: Release 12.4 2.3.5. cuSOLVER: Release 12.2 Update 2 2.3.6. cuSOLVER: Release 12.2 2.4. cuSPARSE Library 2.4.1. cuSPARSE: Release 12.5 Update 1 2.4.2. cuSPARSE: Release 12.5 2.4.3. cuSPARSE: Release 12.4 2.4.4. cuSPARSE: Release 12.3 Update 1 2.4.5. cuSPARSE: Release 12.3 2.4.6. cuSPARSE: Release 12.2 Update 1 2.4.7. cuSPARSE: Release 12.1 Update 1 2.4.8. cuSPARSE: Release 12.0 Update 1 2.4.9. cuSPARSE: Release 12.0 2.5.\\nMath Library 2.5.1.\\nCUDA Math: Release 12.5 2.5.2.\\nCUDA Math: Release 12.4 2.5.3.\\nCUDA Math: Release 12.3 2.5.4.\\nCUDA Math: Release 12.2 2.5.5.\\nCUDA Math: Release 12.1 2.5.6.\\nCUDA Math: Release 12.0 2.6.\\nNVIDIA Performance Primitives (NPP) 2.6.1.\\nNPP: Release 12.4 2.6.2.\\nNPP: Release 12.0 2.7. nvJPEG Library 2.7.1. nvJPEG: Release 12.4 2.7.2. nvJPEG: Release 12.3 Update 1 2.7.3. nvJPEG: Release 12.2 2.7.4. nvJPEG: Release 12.0 3.\\nNotices 3.1.\\nNotice 3.2.\\nOpenCL 3.3.\\nTrademarks Release Notes » 1.\\nCUDA 12.5 Update 1 Release Notes v12.5 | PDF | Archive NVIDIA CUDA Toolkit Release Notes The Release Notes for the CUDA Toolkit.\\nCUDA 12.5 Update 1 Release Notes \\uf0c1 The release notes for the NVIDIA® CUDA® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html .\\nNote The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.\\n1.1.\\nCUDA Toolkit Major Component Versions \\uf0c1 CUDA Components Starting with CUDA 11, the various components in the toolkit are versioned independently.\\nFor CUDA 12.5 Update 1, the table below indicates the versions: Table 1 CUDA 12.5 Update 1 Component Versions \\uf0c1 Component Name Version Information Supported Architectures Supported Platforms CUDA C++ Core Compute Libraries Thrust 2.4.0 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUB 2.4.0 libcu++ 2.4.0 Cooperative Groups 12.5.82 CUDA Compatibility 12.5.36505571 aarch64-jetson Linux CUDA Runtime (cudart) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuobjdump 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUPTI 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuxxfilt (demangler) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA Demo Suite 12.5.82 x86_64 Linux, Windows CUDA GDB 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, WSL CUDA Nsight Eclipse Plugin 12.5.82 x86_64 Linux CUDA NVCC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvdisasm 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA NVML Headers 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvprof 12.5.82 x86_64 Linux, Windows CUDA nvprune 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVRTC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL NVTX 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVVP 12.5.82 x86_64, Linux, Windows CUDA OpenCL 12.5.39 x86_64 Linux, Windows CUDA Profiler API 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA Compute Sanitizer API 12.5.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuBLAS 12.5.3.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuDLA 12.5.82 aarch64-jetson Linux CUDA cuFFT 11.2.3.61 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuFile 1.10.1.7 x86_64, arm64-sbsa, aarch64-jetson Linux CUDA cuRAND 10.3.6.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSOLVER 11.6.3.83 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSPARSE 12.5.1.3 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NPP 12.3.0.159 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvFatbin 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJitLink 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJPEG 12.3.2.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL Nsight Compute 2024.2.1.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL (Windows 11) Nsight Systems 2024.2.3.38 x86_64, arm64-sbsa, Linux, Windows, WSL Nsight Visual Studio Edition (VSE) 2024.2.1.24155 x86_64 (Windows) Windows nvidia_fs 1 2.20.6 x86_64, arm64-sbsa, aarch64-jetson Linux Visual Studio Integration 12.5.82 x86_64 (Windows) Windows NVIDIA Linux Driver 555.42.06 x86_64, arm64-sbsa Linux NVIDIA Windows Driver 555.85 x86_64 (Windows) Windows, WSL CUDA Driver Running a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit.\\nSee Table 3 .\\nFor more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus .\\nEach release of the CUDA Toolkit requires a minimum version of the CUDA driver.\\nThe CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.\\nMore information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades .\\nNote : Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.\\nThe minimum required driver version for CUDA minor version compatibility is shown below.\\nCUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility \\uf0c1 CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.x >=525.60.13 >=528.33 CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x >=450.80.02 >=452.39 CUDA 11.0 (11.0.3) >=450.36.06** >=451.22** * Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode – please read the CUDA Compatibility Guide for details.\\n** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.\\nThe version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.\\nTable 3 CUDA Toolkit and Corresponding Driver Versions \\uf0c1 CUDA Toolkit Toolkit Driver Version Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.5 Update 1 >=555.42.06 >=555.85 CUDA 12.5 GA >=555.42.02 >=555.85 CUDA 12.4 Update 1 >=550.54.15 >=551.78 CUDA 12.4 GA >=550.54.14 >=551.61 CUDA 12.3 Update 1 >=545.23.08 >=546.12 CUDA 12.3 GA >=545.23.06 >=545.84 CUDA 12.2 Update 2 >=535.104.05 >=537.13 CUDA 12.2 Update 1 >=535.86.09 >=536.67 CUDA 12.2 GA >=535.54.03 >=536.25 CUDA 12.1 Update 1 >=530.30.02 >=531.14 CUDA 12.1 GA >=530.30.02 >=531.14 CUDA 12.0 Update 1 >=525.85.12 >=528.33 CUDA 12.0 GA >=525.60.13 >=527.41 CUDA 11.8 GA >=520.61.05 >=520.06 CUDA 11.7 Update 1 >=515.48.07 >=516.31 CUDA 11.7 GA >=515.43.04 >=516.01 CUDA 11.6 Update 2 >=510.47.03 >=511.65 CUDA 11.6 Update 1 >=510.47.03 >=511.65 CUDA 11.6 GA >=510.39.01 >=511.23 CUDA 11.5 Update 2 >=495.29.05 >=496.13 CUDA 11.5 Update 1 >=495.29.05 >=496.13 CUDA 11.5 GA >=495.29.05 >=496.04 CUDA 11.4 Update 4 >=470.82.01 >=472.50 CUDA 11.4 Update 3 >=470.82.01 >=472.50 CUDA 11.4 Update 2 >=470.57.02 >=471.41 CUDA 11.4 Update 1 >=470.57.02 >=471.41 CUDA 11.4.0 GA >=470.42.01 >=471.11 CUDA 11.3.1 Update 1 >=465.19.01 >=465.89 CUDA 11.3.0 GA >=465.19.01 >=465.89 CUDA 11.2.2 Update 2 >=460.32.03 >=461.33 CUDA 11.2.1 Update 1 >=460.32.03 >=461.09 CUDA 11.2.0 GA >=460.27.03 >=460.82 CUDA 11.1.1 Update 1 >=455.32 >=456.81 CUDA 11.1 GA >=455.23 >=456.38 CUDA 11.0.3 Update 1 >= 450.51.06 >= 451.82 CUDA 11.0.2 GA >= 450.51.05 >= 451.48 CUDA 11.0.1 RC >= 450.36.06 >= 451.22 CUDA 10.2.89 >= 440.33 >= 441.22 CUDA 10.1 (10.1.105 general release, and updates) >= 418.39 >= 418.96 CUDA 10.0.130 >= 410.48 >= 411.31 CUDA 9.2 (9.2.148 Update 1) >= 396.37 >= 398.26 CUDA 9.2 (9.2.88) >= 396.26 >= 397.44 CUDA 9.1 (9.1.85) >= 390.46 >= 391.29 CUDA 9.0 (9.0.76) >= 384.81 >= 385.54 CUDA 8.0 (8.0.61 GA2) >= 375.26 >= 376.51 CUDA 8.0 (8.0.44) >= 367.48 >= 369.30 CUDA 7.5 (7.5.16) >= 352.31 >= 353.66 CUDA 7.0 (7.0.28) >= 346.46 >= 347.62 For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation.\\nNote that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.\\nFor running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers .\\nDuring the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).\\nFor more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software .\\nFor meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas .\\n1.2.\\nNew Features \\uf0c1 This section lists new general CUDA and CUDA compilers features.\\n1.2.1.\\nGeneral CUDA \\uf0c1 In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.\\nEnd-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.\\nMPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms.\\nMore details can be found here .\\n1.2.2.\\nCUDA Compiler \\uf0c1 For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5 .\\n1.2.3.\\nCUDA Developer Tools \\uf0c1 For changes to nvprof and Visual Profiler, see the changelog .\\nFor new features, improvements, and bug fixes in Nsight Systems, see the changelog .\\nFor new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog .\\nFor new features, improvements, and bug fixes in CUPTI, see the changelog .\\nFor new features, improvements, and bug fixes in Nsight Compute, see the changelog .\\nFor new features, improvements, and bug fixes in Compute Sanitizer, see the changelog .\\nFor new features, improvements, and bug fixes in CUDA-GDB, see the changelog .\\n1.3.\\nResolved Issues \\uf0c1 1.3.1.\\nCUDA Compiler \\uf0c1 Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.\\nResolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0.\\nAlso added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.\\nResolved a compiler issue that caused different results when compiling with the -G flag than without the flag.\\nFixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.\\nResolved issues seen when compiling cuBLASDx device functions, in some conditions leading to “Misaligned shared or local address”.\\nFix to correct the calculation of write-after-read hazard latency.\\n1.4.\\nKnown Issues and Limitations \\uf0c1 Runfile will not be supported for Amazon Linux 2023.\\nConfidential Computing is not supported on CUDA 12.5.\\nPlease continue to use CUDA 12.4 and drivers r550.xx to use these features.\\nLaunching Cooperative Group kernels with MPS is not supported on Tegra platforms.\\n1.5.\\nDeprecated or Dropped Features \\uf0c1 Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release.\\nWe recommend that developers employ alternative solutions to these features in their software.\\n1.5.1.\\nDeprecated or Dropped Architectures \\uf0c1 NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.\\n1.5.2.\\nDeprecated Operating Systems \\uf0c1 NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.\\nCUDA 12.5 is the last release to support Debian 10.\\nSupport for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.\\n1.5.3.\\nDeprecated Toolchains \\uf0c1 CUDA Toolkit 12.4 deprecated support for the following host compilers: Microsoft Visual C/C++ (MSVC) 2017 All GCC versions prior to GCC 7.3 1.5.4.\\nCUDA Tools \\uf0c1 Support for the macOS host client of CUDA-GDB is deprecated.\\nIt will be dropped in an upcoming release.\\n2.\\nCUDA Libraries \\uf0c1 This section covers CUDA Libraries release notes for 12.x releases.\\nCUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.\\n2.1. cuBLAS Library \\uf0c1 2.1.1. cuBLAS: Release 12.5 Update 1 \\uf0c1 New Features Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.\\nKnown Issues The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases.\\nA workaround is to implement batching manually.\\nThis will be fixed in a future release.\\ncublasGemmGroupedBatchedEx and cublasgemmGroupedBatched have large CPU overheads.\\nThis will be addressed in an upcoming release.\\nResolved Issues Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.\\ncublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).\\n2.1.2. cuBLAS: Release 12.5 \\uf0c1 New Features cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.\\nThis enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type.\\nRefer to cublasGemmGroupedBatchedEx for more details.\\nKnown Issues cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.\\nResolved Issues cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter.\\nFor instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.\\ncuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.\\n2.1.3. cuBLAS: Release 12.4 Update 1 \\uf0c1 Known Issues Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.\\nThis will be fixed in an upcoming release.\\ncublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter.\\nFor example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results.\\nResolved Issues cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error.\\nIn particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 ( CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 ).\\nReduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul() , cublasLtMatmulAlgoCheck() , and cublasLtMatmulAlgoGetHeuristic() .\\nThe issue was introduced in CUDA Toolkit 12.4. cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG).\\nThe issue was introduced in cuBLAS 11.8.\\n2.1.4. cuBLAS: Release 12.4 \\uf0c1 New Features cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.\\nSingle precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH .\\nGrouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).\\nPlease see gemmGroupedBatched for more details.\\nKnown Issues When the current context has been created using cuGreenCtxCreate() , cuBLAS does not properly detect the number of SMs available.\\nThe user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget() .\\nBLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE .\\nThis is the same known issue documented in cuBLAS 12.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace.\\nThe issue exists since cuBLAS 11.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided.\\nThe issue exists since cuBLAS 11.6.\\nWhen captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync .\\nHowever, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail.\\nTo avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.\\n2.1.5. cuBLAS: Release 12.3 Update 1 \\uf0c1 New Features Improved performance of heuristics cache for workloads that have a high eviction rate.\\nKnown Issues BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE .\\nThe expected behavior is that the corresponding computations would be skipped.\\nYou may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped.\\nIf strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST .\\nResolved Issues cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.\\nWhen an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute() .\\nFixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).\\ncublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.\\n2.1.6. cuBLAS: Release 12.3 \\uf0c1 New Features Improved performance on NVIDIA L40S Ada GPUs.\\nKnown Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.\\nWhen an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute() .\\nTo workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit() .\\n2.1.7. cuBLAS: Release 12.2 Update 2 \\uf0c1 New Features cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.\\nIt does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.\\nThis improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.\\n2.1.8. cuBLAS: Release 12.2 \\uf0c1 Known Issues cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%.\\nThere is currently no workaround for this issue.\\nSome Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE .\\nThe kernels apply the first batch’s bias vector to all batches.\\n2.1.9. cuBLAS: Release 12.1 Update 1 \\uf0c1 New Features Support for FP8 on NVIDIA Ada GPUs.\\nImproved performance on NVIDIA L4 Ada GPUs.\\nIntroduced an API that instructs the cuBLASLt library to not use some CPU instructions.\\nThis is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance.\\nRefer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions .\\nKnown Issues When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure).\\nAs a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses.\\nIf one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function.\\nThe same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t .\\nThe issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.\\n2.1.10. cuBLAS: Release 12.0 Update 1 \\uf0c1 New Features Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.\\nKnown Issues For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB).\\nIn the current and previous releases, cuBLAS allocates 256 MiB.\\nThis will be addressed in a future release.\\nA possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.\\nResolved Issues Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.\\nThis began in the CUDA Toolkit 12.0 release.\\nAdded forward compatible single precision complex GEMM that does not require workspace.\\n2.1.11. cuBLAS: Release 12.0 \\uf0c1 New Features cublasLtMatmul now supports FP8 with a non-zero beta.\\nAdded int64 APIs to enable larger problem sizes; refer to 64-bit integer interface .\\nAdded more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.\\nKnown Issues There are no forward compatible kernels for single precision complex gemms that do not require workspace.\\nSupport will be added in a later release.\\nResolved Issues Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE ) could return incorrect results for the bias gradient.\\ncublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.\\nDeprecations Disallow including cublas.h and cublas_v2.h in the same translation unit.\\nRemoved: CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t .\\nNo kernels utilize these stages anymore.\\ncublasLt3mMode_t , CUBLASLT_MATMUL_PREF_MATH_MODE_MASK , and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t .\\nInstead, use the corresponding flags from cublasLtNumericalImplFlags_t .\\nCUBLASLT_MATMUL_PREF_POINTER_MODE_MASK , CUBLASLT_MATMUL_PREF_EPILOGUE_MASK , and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t .\\nThe corresponding parameters are taken directly from cublasLtMatmulDesc_t .\\nCUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t .\\nThis mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.\\n2.2. cuFFT Library \\uf0c1 2.2.1. cuFFT: Release 12.5 \\uf0c1 New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes .\\nWe recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance.\\nYou can enable JIT LTO kernels using the per-plan properties cuFFT API.\\n2.2.2. cuFFT: Release 12.4 Update 1 \\uf0c1 Resolved Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ) in CUDA 12.4.\\nThis routine has now been removed from the header.\\n2.2.3. cuFFT: Release 12.4 \\uf0c1 New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.\\nAdded per-plan properties to the cuFFT API.\\nThese new routines can be leveraged to give users more control over the behavior of cuFFT.\\nCurrently they can be used to enable JIT LTO kernels for 64-bit FFTs.\\nImproved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.\\nKnown Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ).\\nThis routine is not supported by cuFFT, and will be removed from the header in a future release.\\nResolved Issues Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e.\\nusing the ostride component of the Advanced Data Layout API ).\\nFixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL .\\nFrom now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.\\n2.2.4. cuFFT: Release 12.3 Update 1 \\uf0c1 Known Issues Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior.\\nThis issue will be fixed in an upcoming release of cuFFT.\\nResolved Issues Complex-to-complex (C2C) execution functions ( cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.\\n2.2.5. cuFFT: Release 12.3 \\uf0c1 New Features Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.\\nImproved accuracy for double precision prime and composite FFT sizes with factors larger than 127.\\nSlightly improved planning times for some FFT sizes.\\n2.2.6. cuFFT: Release 12.2 \\uf0c1 New Features cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs .\\nImproved performance of 1000+ of FFTs of sizes ranging from 62 to 16380.\\nThe improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.\\nReduced the size of the static libraries when compared to cuFFT in the 12.1 release.\\nResolved Issues cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.\\ncuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.\\n2.2.7. cuFFT: Release 12.1 Update 1 \\uf0c1 Known Issues cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy ) and another thread calls any API (except cufftCreate or cufftDestroy ), and when the total number of plans alive exceeds 1023. cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.\\n2.2.8. cuFFT: Release 12.1 \\uf0c1 New Features Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800.\\nThe improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.\\nKnown Issues Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms.\\nAn upcoming release will update the cuFFT callback implementation, removing this limitation.\\ncuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.\\nResolved Issues cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.\\n2.2.9. cuFFT: Release 12.0 Update 1 \\uf0c1 Resolved Issues Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.\\n2.2.10. cuFFT: Release 12.0 \\uf0c1 New Features PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.\\nKnown Issues cuFFT plan generation time increases due to PTX JIT compiling.\\nRefer to Plan Initialization TIme .\\nResolved Issues cuFFT plans had an unintentional small memory overhead (of a few kB) per plan.\\nThis is resolved.\\n2.3. cuSOLVER Library \\uf0c1 2.3.1. cuSOLVER: Release 12.5 Update 1 \\uf0c1 Resolved Issues The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.\\n2.3.2. cuSOLVER: Release 12.5 \\uf0c1 New Features Performance improvements of cusolverDnXgesvd and cusolverDngesvd if jobu != \\'N\\' or jobvt != \\'N\\' .\\nPerformance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR .\\nLower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.\\nKnown Issues With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice .\\nAs a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)) , with auto ALIGN_32 = []( int64_t val ) { return (( val + 31 ) / 32 ) * 32 ; }; and auto sizeofCudaDataType = []( cudaDataType dt ) { if ( dt == CUDA_R_32F ) return sizeof ( float ); if ( dt == CUDA_R_64F ) return sizeof ( double ); if ( dt == CUDA_C_32F ) return sizeof ( cuComplex ); if ( dt == CUDA_C_64F ) return sizeof ( cuDoubleComplex ); }; 2.3.3. cuSOLVER: Release 12.4 Update 1 \\uf0c1 New Features The performance of cusolverDnXlarft has been improved.\\nFor large matrices, the speedup might exceed 100x.\\nThe performance on H100 is now consistently better than on A100.\\nThe change in cusolverDnXlarft also results in a modest speedup in cusolverDnormqr , cusolverDnormtr , and cusolverDnXsyevd .\\nThe performance of cusolverDnXgesvd when singular vectors are sought has been improved.\\nThe job configuration that computes both left and right singular vectors is up to 1.5x faster.\\nResolved Issues cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.\\nDeprecations Using long-deprecated cusolverDnPotrf , cusolverDnPotrs , cusolverDnGeqrf , cusolverDnGetrf , cusolverDnGetrs , cusolverDnSyevd , cusolverDnSyevdx , cusolverDnGesvd , and their accompanying bufferSize functions will result in a deprecation warning.\\nThe warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf , cusolverDnXpotrs , cusolverDnXgeqrf , cusolverDnXgetrf , cusolverDnXgetrs , cusolverDnXsyevd , cusolverDnXsyevdx , cusolverDnXgesvd , and the corresponding bufferSize functions instead.\\n2.3.4. cuSOLVER: Release 12.4 \\uf0c1 New Features cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced.\\ncusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.\\nKnown Issues cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size.\\nAs a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.\\n2.3.5. cuSOLVER: Release 12.2 Update 2 \\uf0c1 Resolved Issues Fixed an issue with cusolverDngesvd() , cusolverDnGesvd() , and cusolverDnXgesvd() , which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘ N ’.\\n2.3.6. cuSOLVER: Release 12.2 \\uf0c1 New Features A new API to ensure deterministic results or allow non-deterministic results for improved performance.\\nSee cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode() .\\nAffected functions are: cusolverDngeqrf() , cusolverDnsyevd() , cusolverDnsyevdx() , cusolverDngesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvdr() , and cusolverDnXgesvdp() .\\nKnown Issues Concurrent executions of cusolverDngetrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.\\n2.4. cuSPARSE Library \\uf0c1 2.4.1. cuSPARSE: Release 12.5 Update 1 \\uf0c1 New Features Added support for BSR format in cusparseSpMM .\\nResolved Issues cusparseSpMM() would sometimes get incorrect results when alpha=0 , num_batches>1 , batch_stride indicates that there is padding between batches.\\ncusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).\\ncusparseSpMM returned the wrong result when k=0 (for example when A has zero columns).\\nThe correct behavior is doing C \\\\*= beta .\\nThe bug behavior was not modifying C at all.\\ncusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.\\nSliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.\\nSliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.\\n2.4.2. cuSPARSE: Release 12.5 \\uf0c1 New Features Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.\\nResolved Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.\\n2.4.3. cuSPARSE: Release 12.4 \\uf0c1 New Features Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess() .\\nAdded support for mixed real and complex types for cusparseSpMM() .\\nAdded a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM() .\\nKnown Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.\\nResolved Issues cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.\\n2.4.4. cuSPARSE: Release 12.3 Update 1 \\uf0c1 New Features Added support for block sizes of 64 and 128 in cusparseSDDMM() .\\nAdded a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.\\n2.4.5. cuSPARSE: Release 12.3 \\uf0c1 New Features The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.\\nThe cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.\\nKnown Issues The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.\\nWrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.\\nResolved Issues cusparseSpSV() provided indeterministic results in some cases.\\nFixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.\\nFixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.\\n2.4.6. cuSPARSE: Release 12.2 Update 1 \\uf0c1 New Features The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes.\\nSee logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api .\\nResolved Issues Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.\\nClarified the supported operations for cusparseSDDMM() .\\ncusparseCreateConstSlicedEll() now uses const pointers.\\nFixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.\\ncusparseSpSM_bufferSize() could ask slightly less memory than needed.\\ncusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.\\nDeprecations Several legacy APIs have been officially deprecated.\\nA compile-time warning has been added to all of them.\\n2.4.7. cuSPARSE: Release 12.1 Update 1 \\uf0c1 New Features Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine ( cusparseSDDMM ).\\nIntroduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication ( cusparseSpMV ) and triangular solver with a single right-hand side ( cusparseSpSV ).\\nAdded a new API call ( cusparseSpSV_updateMatrix ) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.\\n2.4.8. cuSPARSE: Release 12.0 Update 1 \\uf0c1 New Features cusparseSDDMM() now supports mixed precision computation.\\nImproved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.\\nImproved cusparseSpMV() performance with a new load balancing algorithm.\\ncusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.\\nResolved Issues cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.\\n2.4.9. cuSPARSE: Release 12.0 \\uf0c1 New Features JIT LTO functionalities ( cusparseSpMMOp() ) switched from driver to nvJitLto library.\\nStarting from CUDA 12.0 the user needs to link to libnvJitLto.so , see cuSPARSE documentation .\\nJIT LTO performance has also been improved for cusparseSpMMOpPlan() .\\nIntroduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet() .\\nNow the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.\\nAdded two new algorithms to cusparseSpGEMM() with lower memory utilization.\\nThe first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.\\nAdded int8_t support to cusparseGather() , cusparseScatter() , and cusparseCsr2cscEx2() .\\nImproved cusparseSpSV() performance for both the analysis and the solving phases.\\nImproved cusparseSpSM() performance for both the analysis and the solving phases.\\nImproved cusparseSDDMM() performance and added support for batch computation.\\nImproved cusparseCsr2cscEx2() performance.\\nResolved Issues cusparseSpSV() and cusparseSpSM() could produce wrong results.\\ncusparseDnMatGetStridedBatch() did not accept batchStride == 0 .\\nDeprecations Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.\\n2.5.\\nMath Library \\uf0c1 2.5.1.\\nCUDA Math: Release 12.5 \\uf0c1 Known Issues As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss.\\nNew interval shall read (-23.0001; -2.2637).\\nThis finding is applicable to CUDA 12.5 and all previous versions.\\n2.5.2.\\nCUDA Math: Release 12.4 \\uf0c1 Resolved Issues Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.\\n2.5.3.\\nCUDA Math: Release 12.3 \\uf0c1 New Features Performance of SIMD Integer CUDA Math APIs was improved.\\nResolved Issues The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.\\nKnown Issues Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g.\\npass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half , __half2 , __nv_bfloat16 , __nv_bfloat162 types implementations and expose the user program to undefined behavior.\\nNote, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing .\\nThis behavior may improve in future versions of the headers.\\n2.5.4.\\nCUDA Math: Release 12.2 \\uf0c1 New Features CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side support for many of the arithmetic operations and conversions.\\n__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default.\\nThese may cause build issues due to ambiguous overloads resolution.\\nUsers are advised to update their code to select proper overloads.\\nTo opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ Resolved Issues During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity.\\nNVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer.\\nThe affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation.\\nAs JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT.\\nThis is a software algorithm fix and is not tied to specific hardware.\\nUpdated the observed worst case error bounds for single precision intrinsic functions __expf() , __exp10f() and double precision functions asinh() , acosh() .\\n2.5.5.\\nCUDA Math: Release 12.1 \\uf0c1 New Features Performance and accuracy improvements in atanf , acosf , asinf , sinpif , cospif , powf , erff , and tgammaf .\\n2.5.6.\\nCUDA Math: Release 12.0 \\uf0c1 New Features Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions.\\nRefer to https://docs.nvidia.com/cuda/cuda-math-api/index.html .\\nKnown Issues Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected.\\nAffected CUDA Math APIs: __ddiv_rn() .\\nAffected CUDA language operation: double precision / operation in the device code.\\nDeprecations All previously deprecated undocumented APIs are removed from CUDA 12.0.\\n2.6.\\nNVIDIA Performance Primitives (NPP) \\uf0c1 2.6.1.\\nNPP: Release 12.4 \\uf0c1 New Features Enhanced large file support with size_t .\\n2.6.2.\\nNPP: Release 12.0 \\uf0c1 Deprecations Deprecating non-CTX API support from next release.\\nResolved Issues A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.\\n2.7. nvJPEG Library \\uf0c1 2.7.1. nvJPEG: Release 12.4 \\uf0c1 New Features IDCT performance optimizations for single image CUDA decode.\\nZero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE .\\n2.7.2. nvJPEG: Release 12.3 Update 1 \\uf0c1 New Features New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.\\n2.7.3. nvJPEG: Release 12.2 \\uf0c1 New Features Added support for JPEG Lossless decode (process 14, FO prediction).\\nnvJPEG is now supported on L4T.\\n2.7.4. nvJPEG: Release 12.0 \\uf0c1 New Features Immproved the GPU Memory optimisation for the nvJPEG codec.\\nResolved Issues An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.\\nAn issue with CMYK four component color conversion is now resolved.\\nKnown Issues Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.\\nDeprecations The reuse of Huffman table in Encoder ( nvjpegEncoderParamsCopyHuffmanTables ).\\n1 Only available on select Linux distros 3.\\nNotices \\uf0c1 3.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n3.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1.\\nCUDA 11.6 Features 1.1.\\nCompiler 1.1.1.\\nVS2022 Support 1.1.2.\\nNew instructions in public PTX 1.1.3.\\nUnused Kernel Optimization 1.1.4.\\nNew -arch=native option 1.1.5.\\nGenerate PTX from nvlink: 1.1.6.\\nBullseye support 1.1.7.\\nINT128 developer tool support 2.\\nNotices 2.1.\\nNotice 2.2.\\nOpenCL 2.3.\\nTrademarks CUDA Features Archive » 1.\\nCUDA 11.6 Features v12.5 | PDF | Archive NVIDIA CUDA Features Archive The list of CUDA features by release.\\nCUDA 11.6 Features \\uf0c1 1.1.\\nCompiler \\uf0c1 1.1.1.\\nVS2022 Support \\uf0c1 CUDA 11.6 officially supports the latest VS2022 as host compiler.\\nA separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here .\\nA future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it.\\n1.1.2.\\nNew instructions in public PTX \\uf0c1 New instructions for bit mask creation—BMSK, and sign extension—SZEXT, are added to the public PTX ISA.\\nYou can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT .\\n1.1.3.\\nUnused Kernel Optimization \\uf0c1 In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations.\\nThis was an opt-in feature but in 11.6, this feature is enabled by default.\\nAs mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations.\\n$ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info 1.1.4.\\nNew -arch=native option \\uf0c1 In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1.\\nThis -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system.\\nThis can be particularly helpful for testing when applications are run on the same system they are compiled in.\\n1.1.5.\\nGenerate PTX from nvlink: \\uf0c1 Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN: nvcc -dlto -dlink -ptx Device linking by nvlink is the final stage in the CUDA compilation process.\\nApplications that have multiple source translation units have to be compiled in separate compilation mode.\\nLTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit.\\nHowever, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file.\\nWith the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization.\\n1.1.6.\\nBullseye support \\uf0c1 NVCC compiled source code now works with the code coverage tool Bullseye.\\nThe code coverage is only for the CPU or the host functions.\\nCode coverage for device function is not supported through bullseye.\\n1.1.7.\\nINT128 developer tool support \\uf0c1 In 11.5, CUDA C++ support for 128 bit was added.\\nIn 11.6, developer tools support the datatype as well.\\nWith the latest version of libcu++, int 128 data datype is supported by math functions.\\n2.\\nNotices \\uf0c1 2.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n2.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 2.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1.\\nLicense Agreement for NVIDIA Software Development Kits 1.1.\\nLicense 1.1.1.\\nLicense Grant 1.1.2.\\nDistribution Requirements 1.1.3.\\nAuthorized Users 1.1.4.\\nPre-Release SDK 1.1.5.\\nUpdates 1.1.6.\\nComponents Under Other Licenses 1.1.7.\\nReservation of Rights 1.2.\\nLimitations 1.3.\\nOwnership 1.4.\\nNo Warranties 1.5.\\nLimitation of Liability 1.6.\\nTermination 1.7.\\nGeneral 2.\\nCUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits 2.1.\\nLicense Scope 2.2.\\nDistribution 2.3.\\nOperating Systems 2.4.\\nAudio and Video Encoders and Decoders 2.5.\\nLicensing 2.6.\\nAttachment A 2.7.\\nAttachment B EULA » 1.\\nLicense Agreement for NVIDIA Software Development Kits v12.5 | PDF | Archive End User License Agreement NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement.\\nLast updated: October 8, 2021 The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools.\\nIf you do not agree with the terms and conditions of the license agreement, then do not download or use the software.\\nLast updated: October 8, 2021.\\nPreface The Software License Agreement in Chapter 1 and the Supplement in Chapter 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit.\\nBy accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.\\nNVIDIA Driver Description This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.\\nNVIDIA CUDA Toolkit Description The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.\\nDefault Install Location of CUDA Toolkit Windows platform: %ProgramFiles%\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v#.',\n",
       " 'Linux platform: /usr/local/cuda-#.',\n",
       " 'Mac platform: /Developer/NVIDIA/CUDA-#.',\n",
       " 'NVIDIA CUDA Samples Description CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples.\\nThey are no longer included in the CUDA toolkit.\\nNVIDIA Nsight Visual Studio Edition (Windows only) Description NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.\\nDefault Install Location of Nsight Visual Studio Edition Windows platform: %ProgramFiles(x86)%\\\\NVIDIA Corporation\\\\Nsight Visual Studio Edition #.',\n",
       " '1.\\nLicense Agreement for NVIDIA Software Development Kits \\uf0c1 Important Notice—Read before downloading, installing, copying or using the licensed software: This license agreement, including exhibits attached (“Agreement”) is a legal agreement between you and NVIDIA Corporation (“NVIDIA”) and governs your use of a NVIDIA software development kit (“SDK”).\\nEach SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.\\nThis Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.\\nIf you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case “you” will mean the entity you represent.\\nIf you don’t have the required age or authority to accept this Agreement, or if you don’t accept all the terms and conditions of this Agreement, do not download, install or use the SDK.\\nYou agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.\\n1.1.\\nLicense \\uf0c1 1.1.1.\\nLicense Grant \\uf0c1 Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to: Install and use the SDK, Modify and create derivative works of sample source code delivered in the SDK, and Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement.\\n1.1.2.\\nDistribution Requirements \\uf0c1 These are the distribution requirements for you to exercise the distribution grant: Your application must have material additional functionality, beyond the included portions of the SDK.\\nThe distributable portions of the SDK shall only be accessed by your application.\\nThe following notice shall be included in modifications and derivative works of sample source code distributed: “This software contains source code provided by NVIDIA Corporation.” Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.\\nThe terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA’s intellectual property rights.\\nAdditionally, you agree that you will protect the privacy, security and legal rights of your application users.\\nYou agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK.\\n1.1.3.\\nAuthorized Users \\uf0c1 You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.\\nIf you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.\\nYou are responsible for the compliance with the terms of this Agreement by your authorized users.\\nIf you become aware that your authorized users didn’t follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.\\n1.1.4.\\nPre-Release SDK \\uf0c1 The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials.\\nUse of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.\\nYou may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.\\nNVIDIA may choose not to make available a commercial version of any pre-release SDK.\\nNVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability.\\n1.1.5.\\nUpdates \\uf0c1 NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK.\\nUnless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement.\\nYou agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you.\\nWhile NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK.\\n1.1.6.\\nComponents Under Other Licenses \\uf0c1 The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK.\\nIf and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.\\nSubject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses.\\n1.1.7.\\nReservation of Rights \\uf0c1 NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.\\n1.2.\\nLimitations \\uf0c1 The following license limitations apply to your use of the SDK: You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.\\nExcept as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK.\\nFor clarity, you may not distribute or sublicense the SDK as a stand-alone product.\\nUnless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.\\nYou may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.\\nYou may not use the SDK in any manner that would cause it to become subject to an open source software license.\\nAs examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be: Disclosed or distributed in source code form; Licensed for the purpose of making derivative works; or Redistributable at no charge.\\nYou acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a “Critical Application”).\\nExamples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications.\\nNVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses.\\nYou are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.\\nYou agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney’s fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.\\nYou may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.\\n1.3.\\nOwnership \\uf0c1 NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2 .\\nThis SDK may include software and materials from NVIDIA’s licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.\\nYou hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIA’s rights under Section 1.3.1 .\\nYou may, but don’t have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK.\\nFor any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you.\\nNVIDIA will use feedback at its choice.\\nNVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com .\\n1.4.\\nNo Warranties \\uf0c1 THE SDK IS PROVIDED BY NVIDIA “AS IS” AND “WITH ALL FAULTS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT.\\nNO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE.\\n1.5.\\nLimitation of Liability \\uf0c1 TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY.\\nIN NO EVENT WILL NVIDIA’S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00.\\nTHE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.\\nThese exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose.\\nThese exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different.\\n1.6.\\nTermination \\uf0c1 This Agreement will continue to apply until terminated by either you or NVIDIA as described below.\\nIf you want to terminate this Agreement, you may do so by stopping to use the SDK.\\nNVIDIA may, at any time, terminate this Agreement if: (i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIA’s intellectual property rights); (ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or (iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIA’s sole discretion, the continued use of it is no longer commercially viable.\\nUpon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control.\\nYour prior distributions in accordance with this Agreement are not affected by the termination of this Agreement.\\nUpon written request, you will certify in writing that you have complied with your commitments under this section.\\nUpon any termination of this Agreement all provisions survive except for the license grant provisions.\\n1.7.\\nGeneral \\uf0c1 If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission.\\nAny attempted assignment not approved by NVIDIA in writing shall be void and of no effect.\\nNVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.\\nYou agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.\\nThis Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles.\\nThe United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed.\\nYou agree to all terms of this Agreement in the English language.\\nThe state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement.\\nNotwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.\\nIf any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect.\\nUnless otherwise specified, remedies are cumulative.\\nEach party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.\\nThe SDK has been developed entirely at private expense and is “commercial items” consisting of “commercial computer software” and “commercial computer software documentation” provided with RESTRICTED RIGHTS.\\nUse, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable.\\nContractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.\\nThe SDK is subject to United States export laws and regulations.\\nYou agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasury’s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations.\\nThese laws include restrictions on destinations, end users and end use.\\nBy accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law.\\nAny notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax.\\nYou agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements.\\nPlease direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.\\nThis Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license.\\nAny additional and/or conflicting terms on documents issued by you are null, void, and invalid.\\nAny amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.\\n2.\\nCUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits \\uf0c1 The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (“Agreement”) as modified by this supplement.\\nCapitalized terms used but not defined below have the meaning assigned to them in the Agreement.\\nThis supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement.\\nIn the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.\\n2.1.\\nLicense Scope \\uf0c1 The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.\\n2.2.\\nDistribution \\uf0c1 The portions of the SDK that are distributable under the Agreement are listed in Attachment A.\\n2.3.\\nOperating Systems \\uf0c1 Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).\\n2.4.\\nAudio and Video Encoders and Decoders \\uf0c1 You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies.\\nNVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.\\n2.5.\\nLicensing \\uf0c1 If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions @ nvidia .\\ncom .\\n2.6.\\nAttachment A \\uf0c1 The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.\\nComponent CUDA Runtime Windows cudart.dll, cudart_static.lib, cudadevrt.lib Mac OSX libcudart.dylib, libcudart_static.a, libcudadevrt.a Linux libcudart.so, libcudart_static.a, libcudadevrt.a Android libcudart.so, libcudart_static.a, libcudadevrt.a Component CUDA FFT Library Windows cufft.dll, cufftw.dll, cufft.lib, cufftw.lib Mac OSX libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a Linux libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Android libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Component CUDA BLAS Library Windows cublas.dll, cublasLt.dll Mac OSX libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a Linux libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Android libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Component NVIDIA “Drop-in” BLAS Library Windows nvblas.dll Mac OSX libnvblas.dylib Linux libnvblas.so Component CUDA Sparse Matrix Library Windows cusparse.dll, cusparse.lib Mac OSX libcusparse.dylib, libcusparse_static.a Linux libcusparse.so, libcusparse_static.a Android libcusparse.so, libcusparse_static.a Component CUDA Linear Solver Library Windows cusolver.dll, cusolver.lib Mac OSX libcusolver.dylib, libcusolver_static.a Linux libcusolver.so, libcusolver_static.a Android libcusolver.so, libcusolver_static.a Component CUDA Random Number Generation Library Windows curand.dll, curand.lib Mac OSX libcurand.dylib, libcurand_static.a Linux libcurand.so, libcurand_static.a Android libcurand.so, libcurand_static.a Component NVIDIA Performance Primitives Library Windows nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib Mac OSX libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a Linux libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Android libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Component NVIDIA JPEG Library Windows nvjpeg.lib, nvjpeg.dll Linux libnvjpeg.so, libnvjpeg_static.a Component Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP Mac OSX libculibos.a Linux libculibos.a Component NVIDIA Runtime Compilation Library and Header All nvrtc.h Windows nvrtc.dll, nvrtc-builtins.dll Mac OSX libnvrtc.dylib, libnvrtc-builtins.dylib Linux libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a Component NVIDIA Optimizing Compiler Library Windows nvvm.dll Mac OSX libnvvm.dylib Linux libnvvm.so Component NVIDIA JIT Linking Library Windows libnvJitLink.dll, libnvJitLink.lib Linux libnvJitLink.so, libnvJitLink_static.a Component NVIDIA Common Device Math Functions Library Windows libdevice.10.bc Mac OSX libdevice.10.bc Linux libdevice.10.bc Component CUDA Occupancy Calculation Header Library All cuda_occupancy.h Component CUDA Half Precision Headers All cuda_fp16.h, cuda_fp16.hpp Component CUDA Profiling Tools Interface (CUPTI) Library Windows cupti.dll Mac OSX libcupti.dylib Linux libcupti.so Component NVIDIA Tools Extension Library Windows nvToolsExt.dll, nvToolsExt.lib Mac OSX libnvToolsExt.dylib Linux libnvToolsExt.so Component NVIDIA CUDA Driver Libraries Linux libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a Component NVIDIA CUDA File IO Libraries and Header All cufile.h Linux libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply: The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software.\\n2.7.\\nAttachment B \\uf0c1 Additional Licensing Obligations The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions: Licensee’s use of the GDB third party component is subject to the terms and conditions of GNU GPL v3: This product includes copyrighted third-party software licensed under the terms of the GNU General Public License v3 (\"GPL v3\").\\nAll third-party software packages are copyright by their respective authors.\\nGPL v3 terms and conditions are hereby incorporated into the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses.\\nTo obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests @ nvidia .\\nThis offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.\\nComponent License CUDA-GDB GPL v3 Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licensee’s use of the H.264 video codecs are solely the responsibility of Licensee.\\nLicensee’s use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0.\\nApache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference.\\nhttp://www.apache.org/licenses/LICENSE-2.0.html In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.\\nBoost Software License - Version 1.0 - August 17th, 2003 .\\n.\\nPermission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT.\\nIN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nLicensee’s use of the LLVM third party component is subject to the following terms and conditions: ====================================================== LLVM Release License ====================================================== University of Illinois/NCSA Open Source License Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.\\nAll rights reserved.\\nDeveloped by: LLVM Team University of Illinois at Urbana-Champaign http://llvm.org Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\\n* Neither the names of the LLVM Team, University of Illinois at Urbana- Champaign, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\\nLicensee’s use of the PCRE third party component is subject to the following terms and conditions: ------------ PCRE LICENCE ------------ PCRE is a library of functions to support regular expressions whose syntax and semantics are as close as possible to those of the Perl 5 language.\\nRelease 8 of PCRE is distributed under the terms of the \"BSD\" licence, as specified below.\\nThe documentation for PCRE, supplied in the \"doc\" directory, is distributed under the same terms as the software itself.\\nThe basic library functions are written in C and are freestanding.\\nAlso included in the distribution is a set of C++ wrapper functions, and a just- in-time compiler that can be used to optimize pattern matching.\\nThese are both optional features that can be omitted when the library is built.\\nTHE BASIC LIBRARY FUNCTIONS --------------------------- Written by: Philip Hazel Email local part: ph10 Email domain: cam.ac.uk University of Cambridge Computing Service, Cambridge, England.\\nCopyright (c) 1997-2012 University of Cambridge All rights reserved.\\nPCRE JUST-IN-TIME COMPILATION SUPPORT ------------------------------------- Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2010-2012 Zoltan Herczeg All rights reserved.\\nSTACK-LESS JUST-IN-TIME COMPILER -------------------------------- Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2009-2012 Zoltan Herczeg All rights reserved.\\nTHE C++ WRAPPER FUNCTIONS ------------------------- Contributed by: Google Inc.\\nCopyright (c) 2007-2012, Google Inc. All rights reserved.\\nTHE \"BSD\" LICENCE ----------------- Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of the University of Cambridge nor the name of Google Inc. nor the names of their contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\nIN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nSome of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2007-2009, Regents of the University of California All rights reserved.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Neither the name of the University of California, Berkeley nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\nIN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nSome of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.\\n* The name of the author may not be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2010 The University of Tennessee.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer listed in this license in the documentation and/or other materials provided with the distribution.\\n* Neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2012, The Science and Technology Facilities Council (STFC).\\n* Neither the name of the STFC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nIN NO EVENT SHALL THE STFC BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nSome of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows: -- (C) Copyright 2013 King Abdullah University of Science and Technology Authors: Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa) David Keyes (david.keyes@kaust.edu.sa) Hatem Ltaief (hatem.ltaief@kaust.edu.sa) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Neither the name of the King Abdullah University of Science and Technology nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS\\'\\' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\nIN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows: Copyright (c) 2012, University of Illinois.\\nDeveloped by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Neither the names of IMPACT Group, University of Illinois, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\\nSome of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license: Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima University.\\nCopyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima University and University of Tokyo.\\n* Neither the name of the Hiroshima University nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license: Copyright 2010-2011, D. E. Shaw Research.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of D. E. Shaw Research nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license: Copyright (c) 2015-2017, Norbert Juffa All rights reserved.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1.\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\nIN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nLicensee’s use of the lz4 third party component is subject to the following terms and conditions: Copyright (C) 2011-2013, Yann Collet.\\nBSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\nThe NPP library uses code from the Boost Math Toolkit, and is subject to the following license: Boost Software License - Version 1.0 - August 17th, 2003 .\\nPortions of the Nsight Eclipse Edition is subject to the following license: The Eclipse Foundation makes available all content in this plug-in (\"Content\").\\nUnless otherwise indicated below, the Content is provided to you under the terms and conditions of the Eclipse Public License Version 1.0 (\"EPL\").\\nA copy of the EPL is available at http:// www.eclipse.org/legal/epl-v10.html.\\nFor purposes of the EPL, \"Program\" will mean the Content.\\nIf you did not receive this Content directly from the Eclipse Foundation, the Content is being redistributed by another party (\"Redistributor\") and different terms and conditions may apply to your use of any object code in the Content.\\nCheck the Redistributor\\'s license that was provided with the Content.\\nIf no such license exists, contact the Redistributor.\\nUnless otherwise indicated below, the terms and conditions of the EPL still apply to any source code in the Content and such source code may be obtained at http://www.eclipse.org.\\nSome of the cuBLAS library routines uses code from OpenAI, which is subject to the following license: License URL https://github.com/openai/openai-gemm/blob/master/LICENSE License Text The MIT License Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nLicensee’s use of the Visual Studio Setup Configuration Samples is subject to the following license: The MIT License (MIT) Copyright (C) Microsoft Corporation.\\nLicensee’s use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.\\nThe DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license .\\nComponents of the driver and compiler used for binary management, including nvFatBin, nvcc, and cuobjdump, use the Zstandard library which is subject to the following license: BSD License For Zstandard software Copyright (c) Meta Platforms, Inc. and affiliates.\\n* Neither the name Facebook, nor Meta, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); });1.\\nIntroduction 2.\\nWindows 2.1.\\nNetwork Installer 2.2.\\nLocal Installer 2.3.\\nPip Wheels - Windows 2.4.\\nConda 3.\\nLinux 3.1.\\nLinux x86_64 3.1.1.\\nRedhat / CentOS 3.1.1.1.\\nRPM Installer 3.1.1.2.\\nRunfile Installer 3.1.2.\\nFedora 3.1.2.1.\\nRPM Installer 3.1.2.2.\\nRunfile Installer 3.1.3.\\nSUSE Linux Enterprise Server 3.1.3.1.\\nRPM Installer 3.1.3.2.\\nRunfile Installer 3.1.4.\\nOpenSUSE 3.1.4.1.\\nRPM Installer 3.1.4.2.\\nRunfile Installer 3.1.5.\\nAmazon Linux 2023 3.1.5.1.\\nPrepare Amazon Linux 2023 3.1.5.2.\\nLocal Repo Installation for Amazon Linux 3.1.5.3.\\nNetwork Repo Installation for Amazon Linux 3.1.5.4.\\nCommon Installation Instructions for Amazon Linux 3.1.6.\\nPip Wheels - Linux 3.1.7.\\nConda 3.1.8.\\nWSL 3.1.9.\\nUbuntu 3.1.9.1.\\nDebian Installer 3.1.9.2.\\nRunfile Installer 3.1.10.\\nDebian 3.1.10.1.\\nDebian Installer 3.1.10.2.\\nRunfile Installer 4.\\nNotices 4.1.\\nNotice 4.2.\\nOpenCL 4.3.\\nTrademarks Quick Start Guide » 1.\\nIntroduction v12.5 | PDF | Archive CUDA Quick Start Guide Minimal first-steps instructions to get CUDA running on a standard system.\\nIntroduction \\uf0c1 This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.\\nThese instructions are intended to be used on a clean installation of a supported platform.\\nFor questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide .\\nThe CUDA installation packages can be found on the CUDA Downloads Page .\\n2.\\nWindows \\uf0c1 When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer.\\nThe Network Installer allows you to download only the files you need.\\nThe Local Installer is a stand-alone installer with a large initial download.\\nFor more details, refer to the Windows Installation Guide .\\n2.1.\\nNetwork Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nLaunch the downloaded installer package.\\nRead and accept the EULA.\\nSelect next to download and install all components.\\nOnce the download completes, the installation will begin automatically.\\nOnce the installation completes, click “next” to acknowledge the Nsight Visual Studio Edition installation summary.\\nClick close to close the installer.\\nNavigate to the Samples’ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody .\\nOpen the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln .\\nOpen the Build menu within Visual Studio and click Build Solution .\\nNavigate to the CUDA Samples build directory and run the nbody sample.\\nNote Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources.\\n2.2.\\nLocal Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nSelect next to install all components.\\nOnce the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.\\nOpen the nbody Visual Studio solution file for the version of Visual Studio you have installed.\\n2.3.\\nPip Wheels - Windows \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.\\nThese packages are intended for runtime use and do not currently include developer tools (these can be installed separately).\\nPlease note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.\\nPrerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo.\\nIf your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules.\\nIf these Python modules are out-of-date then the commands which follow later in this section may fail.\\npy -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.\\npy -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version.\\n“cu12” should be read as “cuda12”.\\nnvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 2.4.\\nConda \\uf0c1 The Conda packages are available at https://anaconda.org/nvidia .\\nInstallation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.\\nLinux \\uf0c1 CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on.\\n3.1.\\nLinux x86_64 \\uf0c1 For development on the x86_64 architecture.\\nIn some cases, x86_64 systems may act as host platforms targeting other architectures.\\nSee the Linux Installation Guide for more details.\\n3.1.1.\\nRedhat / CentOS \\uf0c1 When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer.\\nThe Runfile Installer is only available as a Local Installer.\\nThe RPM Installer is available as both a Local Installer and a Network Installer.\\nIn the case of the RPM installers, the instructions for the Local and Network variants are the same.\\nFor more details, refer to the Linux Installation Guide .\\n3.1.1.1.\\nRPM Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nInstall EPEL to satisfy the DKMS dependency by following the instructions at EPEL’s website .\\nEnable optional repos : On RHEL 8 Linux only, execute the following steps to enable optional repositories.\\nOn x86_64 workstation: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Install the repository meta-data, clean the yum cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo yum clean expire-cache sudo yum install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.1.2.\\nRunfile Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nDisable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.\\nRun the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.2.\\nFedora \\uf0c1 When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer.\\n3.1.2.1.\\nInstall the RPMFusion free repository to satisfy the Akmods dependency: su -c \\'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm\\' Install the repository meta-data, clean the dnf cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo dnf clean expire-cache sudo dnf install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.2.2.\\nDisable the Nouveau drivers: Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the below command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.\\nRun the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface.\\nSet up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.3.\\nSUSE Linux Enterprise Server \\uf0c1 When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer.\\n3.1.3.1.\\nInstall the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo SUSEConnect --product PackageHub/15/x86_64 sudo zypper refresh sudo rpm --erase gpg-pubkey-7fa2af80* sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd .\\n3.1.3.2.\\nReboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.\\nRun the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd .\\n3.1.4.\\nOpenSUSE \\uf0c1 When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer.\\n3.1.4.1.\\nInstall the repository meta-data, refresh the Zypper cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo zypper refresh sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.4.2.\\nDisable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.\\n3.1.5.\\nAmazon Linux 2023 \\uf0c1 3.1.5.1.\\nPrepare Amazon Linux 2023 \\uf0c1 Perform the pre-installation actions.\\nThe kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo .\\n3.1.5.2.\\nLocal Repo Installation for Amazon Linux \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*.x86_64.rpm 3.1.5.3.\\nNetwork Repo Installation for Amazon Linux \\uf0c1 Enable the network repository and clean the DN cache: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo sudo dnf clean expire-cache 3.1.5.4.\\nCommon Installation Instructions for Amazon Linux \\uf0c1 These instructions apply to both local and network installation for Amazon Linux.\\nInstall CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory.\\nFor pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.\\nReboot the system: sudo reboot Perform the post-installation actions.\\n3.1.6.\\nPip Wheels - Linux \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.\\npython3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.\\npython3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version.\\nnvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 nvidia-opencl-cu12 nvidia-nvjitlink-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 nvidia-opencl-cu125 nvidia-nvjitlink-cu125 3.1.7.\\nInstallation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.1.8.\\nWSL \\uf0c1 These instructions must be used if you are installing in a WSL environment.\\nDo not use the Ubuntu instructions in this case.\\nInstall repository meta-data sudo dpkg -i cuda-repo-__.deb Update the CUDA public GPG key sudo apt-key del 7fa2af80 When installing using the local repo: sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/ When installing using the network repo: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb Pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-.pin sudo mv cuda-.pin /etc/apt/preferences.d/cuda-repository-pin-600 Update the Apt repository cache and install CUDA sudo apt-get update sudo apt-get install cuda 3.1.9.\\nUbuntu \\uf0c1 When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer.\\nThe Debian Installer is available as both a Local Installer and a Network Installer.\\nIn the case of the Debian installers, the instructions for the Local and Network variants are the same.\\n3.1.9.1.\\nDebian Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nInstall the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA: sudo dpkg --install cuda-repo--..deb sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb sudo add-apt-repository contrib sudo apt-get update sudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.9.2.\\nDisable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters.\\n3.1.10.\\nDebian \\uf0c1 When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer.\\n3.1.10.1.\\nInstall the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA: sudo dpkg -i cuda-repo-__.deb sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb sudo add-apt-repository contrib sudo apt-get update sudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.10.2.\\n4.\\nNotices \\uf0c1 4.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n4.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 4.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2015-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jun 25, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1.\\nIntroduction 1.1.\\nSystem Requirements 1.2.\\nAbout This Document 2.\\nInstalling CUDA Development Tools 2.1.\\nVerify You Have a CUDA-Capable GPU 2.2.\\nDownload the NVIDIA CUDA Toolkit 2.3.\\nInstall the CUDA Software 2.3.1.\\nUninstalling the CUDA Software 2.4.\\nUsing Conda to Install the CUDA Software 2.4.1.\\nConda Overview 2.4.2.\\nInstallation 2.4.3.\\nUninstallation 2.4.4.\\nInstalling Previous CUDA Releases 2.5.\\nUse a Suitable Driver Model 2.6.\\nVerify the Installation 2.6.1.\\nRunning the Compiled Examples 3.\\nPip Wheels 4.\\nCompiling CUDA Programs 4.1.\\nCompiling Sample Projects 4.2.\\nSample Projects 4.3.\\nBuild Customizations for New Projects 4.4.\\nBuild Customizations for Existing Projects 5.\\nAdditional Considerations 6.\\nNotices 6.1.\\nNotice 6.2.\\nOpenCL 6.3.\\nTrademarks Installation Guide Windows » 1.\\nIntroduction v12.5 | PDF | Archive CUDA Installation Guide for Microsoft Windows The installation instructions for the CUDA Toolkit on Microsoft Windows systems.\\nIntroduction \\uf0c1 CUDA ® is a parallel computing platform and programming model invented by NVIDIA.\\nIt enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).\\nCUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms.\\nWith CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.\\nSupport heterogeneous computation where applications use both the CPU and GPU.\\nSerial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU.\\nAs such, CUDA can be incrementally applied to existing applications.\\nThe CPU and GPU are treated as separate devices that have their own memory spaces.\\nThis configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.\\nCUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads.\\nThese cores have shared resources including a register file and a shared memory.\\nThe on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.\\nThis guide will show you how to install and check the correct operation of the CUDA development tools.\\n1.1.\\nSystem Requirements \\uf0c1 To use CUDA on your system, you will need the following installed: A CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) Supported Microsoft Windows ® operating systems: Microsoft Windows 11 21H2 Microsoft Windows 11 22H2-SV2 Microsoft Windows 11 23H2 Microsoft Windows 10 21H2 Microsoft Windows 10 22H2 Microsoft Windows Server 2022 Table 1 Windows Compiler Support in CUDA 12.5 \\uf0c1 Compiler* IDE Native x86_64 Cross-compilation (32-bit on 64-bit) C++ Dialect MSVC Version 193x Visual Studio 2022 17.x YES Not supported C++14 (default), C++17, C++20 MSVC Version 192x Visual Studio 2019 16.x YES C++14 (default), C++17 MSVC Version 191x Visual Studio 2017 15.x (RTW and all updates) YES C++14 (default), C++17 * Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5.\\n32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit.\\nUse the CUDA Toolkit from earlier releases for 32-bit compilation.\\nCUDA Driver will continue to support running 32-bit application binaries on GeForce GPUs until Ada.\\nAda will be the last architecture with driver support for 32-bit applications.\\nHopper does not support 32-bit applications.\\nSupport for running x86 32-bit applications on x86_64 Windows is limited to use with: CUDA Driver CUDA Runtime (cudart) CUDA Math Library (math.h) 1.2.\\nAbout This Document \\uf0c1 This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment.\\nYou do not need previous experience with CUDA or experience with parallel computation.\\n2.\\nInstalling CUDA Development Tools \\uf0c1 Basic instructions can be found in the Quick Start Guide .\\nRead on for more detailed instructions.\\nThe setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps: Verify the system has a CUDA-capable GPU.\\nDownload the NVIDIA CUDA Toolkit.\\nInstall the NVIDIA CUDA Toolkit.\\nTest that the installed software runs correctly and communicates with the hardware.\\n2.1.\\nVerify You Have a CUDA-Capable GPU \\uf0c1 You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager .\\nHere you will find the vendor name and model of your graphics card(s).\\nIf you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus , that GPU is CUDA-capable.\\nThe Release Notes for the CUDA Toolkit also contain a list of supported products.\\nThe Windows Device Manager can be opened via the following steps: Open a run window from the Start Menu Run: control /name Microsoft.DeviceManager 2.2.\\nDownload the NVIDIA CUDA Toolkit \\uf0c1 The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads .\\nChoose the platform you are using and one of the following installer formats: Network Installer: A minimal installer which later downloads packages required for installation.\\nOnly the packages selected during the selection phase of the installer are downloaded.\\nThis installer is useful for users who want to minimize download time.\\nFull Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download.\\nThis installer is useful for systems which lack network access and for enterprise deployment.\\nThe CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.\\nDownload Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file.\\nIf either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.\\n2.3.\\nInstall the CUDA Software \\uf0c1 Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.\\nNote The driver and toolkit must be installed for CUDA to function.\\nIf you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.\\nNote The installation may fail if Windows Update starts after the installation has begun.\\nWait until Windows Update is complete and then try the installation again.\\nGraphical Installation Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.\\nSilent Installation The installer can be executed in silent mode by executing the package with the -s flag.\\nAdditional parameters can be passed which will install specific subpackages instead of all packages.\\nSee the table below for a list of all the subpackage names.\\nTable 2 Possible Subpackage Names \\uf0c1 Subpackage Name Subpackage Description Toolkit Subpackages (defaults to C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5) cuda_profiler_api_12.5 CUDA Profiler API.\\ncudart_12.5 CUDA Runtime libraries.\\ncuobjdump_12.5 Extracts information from cubin files.\\ncupti_12.5 The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.\\ncuxxfilt_12.5 The CUDA cu++ filt demangler tool.\\ndemo_suite_12.5 Prebuilt demo applications using CUDA.\\ndocumentation_12.5 CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.\\nnvcc_12.5 CUDA compiler.\\nnvdisasm_12.5 Extracts information from standalone cubin files.\\nnvfatbin_12.5 Library for creating fatbinaries at runtime.\\nnvjitlink_12.5 nvJitLink library.\\nnvml_dev_12.5 NVML development libraries and headers.\\nnvprof_12.5 Tool for collecting and viewing CUDA application profiling data from the command-line.\\nnvprune_12.5 Prunes host object files and libraries to only contain device code for the specified targets.\\nnvrtc_12.5 nvrtc_dev_12.5 NVRTC runtime libraries.\\nnvtx_12.5 NVTX on Windows.\\nopencl_12.5 OpenCL library.\\nvisual_profiler_12.5 Visual Profiler.\\nsanitizer_12.5 Compute Sanitizer API.\\nthrust_12.5 CUDA Thrust.\\ncublas_12.5 cublas_dev_12.5 cuBLAS runtime libraries.\\ncufft_12.5 cufft_dev_12.5 cuFFT runtime libraries.\\ncurand_12.5 curand_dev_12.5 cuRAND runtime libraries.\\ncusolver_12.5 cusolver_dev_12.5 cuSOLVER runtime libraries.\\ncusparse_12.5 cusparse_dev_12.5 cuSPARSE runtime libraries.\\nnpp_12.5 npp_dev_12.5 NPP runtime libraries.\\nnvjpeg_12.5 nvjpeg_dev_12.5 nvJPEG libraries.\\nnsight_compute_12.5 Nsight Compute.\\nnsight_systems_12.5 Nsight Systems.\\nnsight_vse_12.5 Installs the Nsight Visual Studio Edition plugin in all VS. occupancy_calculator_12.5 Installs the CUDA_Occupancy_Calculator.xls tool.\\nvisual_studio_integration_12.5 Installs CUDA project wizard and builds customization files in VS. Driver Subpackages Display.Driver The NVIDIA Display Driver.\\nRequired to run CUDA applications.\\nFor example, to install only the compiler and driver components: .exe -s nvcc_12.1 Display.Driver Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.\\nExtracting and Inspecting the Files Manually Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation.\\nThe full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip .\\nOnce extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration.\\nWithin each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.\\nNote Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration.\\nThis is intended for enterprise-level deployment.\\n2.3.1.\\nUninstalling the CUDA Software \\uf0c1 All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.\\n2.4.\\nUsing Conda to Install the CUDA Software \\uf0c1 This section describes the installation and configuration of CUDA when using the Conda installer.\\nThe Conda packages are available at https://anaconda.org/nvidia .\\n2.4.1.\\nConda Overview \\uf0c1 The Conda installation installs the CUDA Toolkit.\\nThe installation steps are listed below.\\n2.4.2.\\nInstallation \\uf0c1 To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda - c nvidia 2.4.3.\\nUninstallation \\uf0c1 To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 2.4.4.\\nInstalling Previous CUDA Releases \\uf0c1 All Conda packages released under a specific CUDA version are labeled with that release version.\\nTo install a previous version, include that label in the install command such as: conda install cuda - c nvidia / label / cuda -11.3.0 Note Some CUDA releases do not move to new versions of all installable components.\\nWhen this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as: conda install cuda - c nvidia / label / cuda -11.3.0 - c nvidia / label / cuda -11.3.1 This example will install all packages released as part of CUDA 11.3.1.\\n2.5.\\nUse a Suitable Driver Model \\uf0c1 On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate: The WDDM driver model is used for display devices.\\nThe Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.\\nTCC is enabled by default on most recent NVIDIA Tesla GPUs.\\nTo check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).\\nNote Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.\\nNote NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode.\\n2.6.\\nVerify the Installation \\uf0c1 Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware.\\nTo do this, you need to compile and run some of the included sample programs.\\n2.6.1.\\nRunning the Compiled Examples \\uf0c1 The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window.\\nYou can display a Command Prompt window by going to: Start > All Programs > Accessories > Command Prompt CUDA Samples are located in https://github.com/nvidia/cuda-samples .\\nTo use the samples, clone the project, build the samples, and run them using the instructions on the Github page.\\nTo verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program.\\nThe sample can be built using the provided VS solution files in the deviceQuery folder.\\nThis assumes that you used the default installation directory structure.\\nIf CUDA is installed and configured correctly, the output should look similar to Figure 1 .\\nFigure 1 Valid Results from deviceQuery CUDA Sample \\uf0c1 The exact appearance and the output lines might be different on your system.\\nThe important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.\\nIf a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.\\nRunning the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly.\\nThe output should resemble Figure 2 .\\nFigure 2 Valid Results from bandwidthTest CUDA Sample \\uf0c1 The device name (second line) and the bandwidth numbers vary from system to system.\\nThe important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.\\nIf the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.\\nTo see a graphical representation of what CUDA can do, run the particles sample at https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles 3.\\nPip Wheels \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.\\nThese packages are intended for runtime use and do not currently include developer tools (these can be installed separately).\\nPlease note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.\\nPrerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo.\\nIf your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules.\\nIf these Python modules are out-of-date then the commands which follow later in this section may fail.\\npy -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.\\npy -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version.\\n“cu12” should be read as “cuda12”.\\nnvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 nvidia-opencl-cu12 These metapackages install the following packages: nvidia-cublas-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 nvidia-opencl-cu125 4.\\nCompiling CUDA Programs \\uf0c1 The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code.\\nTo build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022.\\nYou can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples 4.1.\\nCompiling Sample Projects \\uf0c1 The bandwidthTest project is a good sample project to build and run.\\nIt is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest .\\nIf you elected to use the default installation location, the output is placed in CUDA Samples\\\\v12.5\\\\bin\\\\win64\\\\Release .\\nBuild the program using the appropriate solution file and run the executable.\\nIf all works correctly, the output should be similar to Figure 2 .\\n4.2.\\nSample Projects \\uf0c1 The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.\\nA few of the example projects require some additional setup.\\nThese sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.\\nThe environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.\\nTable 3 CUDA Visual Studio .props locations \\uf0c1 Visual Studio CUDA 12.5 .props file Install Directory Visual Studio 2015 (deprecated) C:Program Files (x86)\\\\MSBuild\\\\Microsoft.Cpp\\\\v4.0\\\\V140\\\\BuildCustomizations Visual Studio 2017 \\\\Common7\\\\IDE\\\\VC\\\\VCTargets\\\\BuildCustomizations Visual Studio 2019 C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Professional\\\\MSBuild\\\\Microsoft\\\\VC\\\\v160\\\\BuildCustomizations Visual Studio 2022 C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Professional\\\\MSBuild\\\\Microsoft\\\\VC\\\\v170\\\\BuildCustomizations You can reference this CUDA 12.5.props file when building your own CUDA applications.\\n4.3.\\nBuild Customizations for New Projects \\uf0c1 When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations.\\nTo accomplish this, click File-> New | Project… NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version.\\nFor example, selecting the “CUDA 12.5 Runtime” template will configure your project for use with the CUDA 12.5 Toolkit.\\nThe new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIA’s Build Customizations.\\nAll standard capabilities of Visual Studio C++ projects will be available.\\nTo specify a custom CUDA Toolkit location, under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field as desired.\\nNote that the selected toolkit must match the version of the Build Customizations.\\nNote A supported version of MSVC must be installed to use this feature.\\n4.4.\\nBuild Customizations for Existing Projects \\uf0c1 When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations.\\nThis can be done using one of the following two methods: Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizations… , then select the CUDA Toolkit version you would like to target.\\nAlternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit.\\nFirst add a CUDA build customization to your project as above.\\nThen, right click on the project name and select Properties .\\nUnder CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) .\\nNote that the $(CUDA_PATH) environment variable is set by the installer.\\nWhile Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.\\nIf you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes.\\nYou can access the value of the $(CUDA_PATH) environment variable via the following steps: Open a run window from the Start Menu.\\nRun: control sysdm.cpl Select the Advanced tab at the top of the window.\\nClick Environment Variables at the bottom of the window.\\nFiles which contain CUDA code must be marked as a CUDA C/C++ file.\\nThis can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item , selecting NVIDIA CUDA 12.5\\\\CodeCUDA C/C++ File , and then selecting the file you wish to add.\\nFor advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following: msbuild /t:Rebuild /p:CudaToolkitDir=\"drive:/path/to/new/toolkit/\" 5.\\nAdditional Considerations \\uf0c1 Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs.\\nTo begin using CUDA to accelerate the performance of your own applications, consult the CUDA C Programming Guide, located in the CUDA Toolkit documentation directory.\\nA number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Visual Studio Edition, and NVIDIA Visual Profiler.\\nFor technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/ .\\n6.\\nNotices \\uf0c1 6.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n6.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 6.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1.\\nIntroduction 1.1.\\nSystem Requirements 1.2.\\nOS Support Policy 1.3.\\nHost Compiler Support Policy 1.3.1.\\nSupported C++ Dialects 1.4.\\nAbout This Document 2.\\nPre-installation Actions 2.1.\\nVerify You Have a CUDA-Capable GPU 2.2.\\nVerify You Have a Supported Version of Linux 2.3.\\nVerify the System Has gcc Installed 2.4.\\nVerify the System has the Correct Kernel Headers and Development Packages Installed 2.5.\\nInstall GPUDirect Storage 2.6.\\nChoose an Installation Method 2.7.\\nDownload the NVIDIA CUDA Toolkit 2.8.\\nAddress Custom xorg.conf, If Applicable 2.9.\\nHandle Conflicting Installation Methods 3.\\nPackage Manager Installation 3.1.\\nOverview 3.2.\\nRHEL 8 / Rocky 8 3.2.1.\\nPrepare RHEL 8 / Rocky 8 3.2.2.\\nLocal Repo Installation for RHEL 8 / Rocky 8 3.2.3.\\nNetwork Repo Installation for RHEL 8 / Rocky 8 3.2.4.\\nCommon Instructions for RHEL 8 / Rocky 8 3.3.\\nRHEL 9 / Rocky 9 3.3.1.\\nPrepare RHEL 9 / Rocky 9 3.3.2.\\nLocal Repo Installation for RHEL 9 / Rocky 9 3.3.3.\\nNetwork Repo Installation for RHEL 9 / Rocky 9 3.3.4.\\nCommon Instructions for RHEL 9 / Rocky 9 3.4.\\nKylinOS 10 3.4.1.\\nPrepare KylinOS 10 3.4.2.\\nLocal Repo Installation for KylinOS 3.4.3.\\nNetwork Repo Installation for KylinOS 3.4.4.\\nCommon Instructions for KylinOS 10 3.5.\\nFedora 3.5.1.\\nPrepare Fedora 3.5.2.\\nLocal Repo Installation for Fedora 3.5.3.\\nNetwork Repo Installation for Fedora 3.5.4.\\nCommon Installation Instructions for Fedora 3.6.\\nSLES 3.6.1.\\nPrepare SLES 3.6.2.\\nLocal Repo Installation for SLES 3.6.3.\\nNetwork Repo Installation for SLES 3.6.4.\\nCommon Installation Instructions for SLES 3.7.\\nOpenSUSE 3.7.1.\\nPrepare OpenSUSE 3.7.2.\\nLocal Repo Installation for OpenSUSE 3.7.3.\\nNetwork Repo Installation for OpenSUSE 3.7.4.\\nCommon Installation Instructions for OpenSUSE 3.8.\\nWSL 3.8.1.\\nPrepare WSL 3.8.2.\\nLocal Repo Installation for WSL 3.8.3.\\nNetwork Repo Installation for WSL 3.8.4.\\nCommon Installation Instructions for WSL 3.9.\\nUbuntu 3.9.1.\\nPrepare Ubuntu 3.9.2.\\nLocal Repo Installation for Ubuntu 3.9.3.\\nNetwork Repo Installation for Ubuntu 3.9.4.\\nCommon Installation Instructions for Ubuntu 3.10.\\nDebian 3.10.1.\\nPrepare Debian 3.10.2.\\nLocal Repo Installation for Debian 3.10.3.\\nNetwork Repo Installation for Debian 3.10.4.\\nCommon Installation Instructions for Debian 3.11.\\nAmazon Linux 2023 3.11.1.\\nPrepare Amazon Linux 2023 3.11.2.\\nLocal Repo Installation for Amazon Linux 3.11.3.\\nNetwork Repo Installation for Amazon Linux 3.11.4.\\nCommon Installation Instructions for Amazon Linux 3.12.\\nAdditional Package Manager Capabilities 3.12.1.\\nAvailable Packages 3.12.2.\\nMeta Packages 3.12.3.\\nOptional 32-bit Packages for Linux x86_64 .deb/.rpm 3.12.4.\\nPackage Upgrades 4.\\nDriver Installation 5.\\nNVIDIA Open GPU Kernel Modules 5.1.\\nCUDA Runfile 5.2.\\nDebian 5.3.\\nFedora 5.4.\\nKylinOS 10 5.5.\\nRHEL 9 and Rocky 9 5.6.\\nRHEL 8 and Rocky 8 5.7.\\nOpenSUSE and SLES 5.8.\\nUbuntu 6.\\nPrecompiled Streams 6.1.\\nPrecompiled Streams Support Matrix 6.2.\\nModularity Profiles 7.\\nKickstart Installation 7.1.\\nRHEL 8 / Rocky Linux 8 7.2.\\nRHEL 9 / Rocky Linux 9 8.\\nRunfile Installation 8.1.\\nRunfile Overview 8.2.\\nInstallation 8.3.\\nDisabling Nouveau 8.3.1.\\nFedora 8.3.2.\\nRHEL / Rocky and KylinOS 8.3.3.\\nOpenSUSE 8.3.4.\\nSLES 8.3.5.\\nWSL 8.3.6.\\nUbuntu 8.3.7.\\nDebian 8.4.\\nDevice Node Verification 8.5.\\nAdvanced Options 8.6.\\nUninstallation 9.\\nConda Installation 9.1.\\nConda Overview 9.2.\\nInstalling CUDA Using Conda 9.3.\\nUninstalling CUDA Using Conda 9.4.\\nInstalling Previous CUDA Releases 9.5.\\nUpgrading from cudatoolkit Package 10.\\nPip Wheels 11.\\nTarball and Zip Archive Deliverables 11.1.\\nParsing Redistrib JSON 11.2.\\nImporting Tarballs into CMake 11.3.\\nImporting Tarballs into Bazel 12.\\nCUDA Cross-Platform Environment 12.1.\\nCUDA Cross-Platform Installation 12.2.\\nCUDA Cross-Platform Samples 13.\\nPost-installation Actions 13.1.\\nMandatory Actions 13.1.1.\\nEnvironment Setup 13.2.\\nRecommended Actions 13.2.1.\\nInstall Persistence Daemon 13.2.2.\\nInstall Writable Samples 13.2.3.\\nVerify the Installation 13.2.3.1.\\nVerify the Driver Version 13.2.3.2.\\nRunning the Binaries 13.2.4.\\nInstall Nsight Eclipse Plugins 13.2.5.\\nLocal Repo Removal 13.3.\\nOptional Actions 13.3.1.\\nInstall Third-party Libraries 13.3.2.\\nInstall the Source Code for cuda-gdb 13.3.3.\\nSelect the Active Version of CUDA 14.\\nAdvanced Setup 15.\\nFrequently Asked Questions 15.1.\\nHow do I install the Toolkit in a different location?\\n15.2.\\nWhy do I see “nvcc: No such file or directory” when I try to build a CUDA application?\\n15.3.\\nWhy do I see “error while loading shared libraries: : cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library?\\n15.4.\\nWhy do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu?\\n15.5.\\nHow can I tell X to ignore a GPU for compute-only use?\\n15.6.\\nWhy doesn’t the cuda-repo package install the CUDA Toolkit and Drivers?\\n15.7.\\nHow do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04?\\n15.8.\\nWhat do I do if the display does not load, or CUDA does not work, after performing a system update?\\n15.9.\\nHow do I install a CUDA driver with a version less than 367 using a network repo?\\n15.10.\\nHow do I install an older CUDA version using a network repo?\\n15.11.\\nWhy does the installation on SUSE install the Mesa-dri-nouveau dependency?\\n15.12.\\nHow do I handle “Errors were encountered while processing: glx-diversions”?\\n16.\\nAdditional Considerations 17.\\nSwitching between Driver Module Flavors 18.\\nRemoving CUDA Toolkit and Driver 19.\\nNotices 19.1.\\nNotice 19.2.\\nOpenCL 19.3.\\nTrademarks 20.\\nCopyright Installation Guide for Linux » 1.\\nIntroduction v12.5 | PDF | Archive NVIDIA CUDA Installation Guide for Linux The installation instructions for the CUDA Toolkit on Linux.\\nIntroduction \\uf0c1 CUDA ® is a parallel computing platform and programming model invented by NVIDIA ® .\\nIt enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).\\nCUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms.\\nWith CUDA C/C\\ufeff+\\ufeff+, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.\\nSupport heterogeneous computation where applications use both the CPU and GPU.\\nSerial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU.\\nAs such, CUDA can be incrementally applied to existing applications.\\nThe CPU and GPU are treated as separate devices that have their own memory spaces.\\nThis configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.\\nCUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads.\\nThese cores have shared resources including a register file and a shared memory.\\nThe on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.\\nThis guide will show you how to install and check the correct operation of the CUDA development tools.\\n1.1.\\nSystem Requirements \\uf0c1 To use NVIDIA CUDA on your system, you will need the following installed: CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release.\\nThe following table lists the supported Linux distributions.\\nPlease review the footnotes associated with the table.\\nTable 1 Native Linux Distribution Support in CUDA 12.5 Update 1 \\uf0c1 Distribution Kernel 1 Default GCC GLIBC x86_64 RHEL 9.y (y =10.x >=11.x >=22.x 22.x 1.4.\\nAbout This Document \\uf0c1 This document is intended for readers familiar with the Linux environment and the compilation of C programs from the command line.\\nYou do not need previous experience with CUDA or experience with parallel computation.\\nNote: This guide covers installation only on systems with X Windows installed.\\nNote Many commands in this document might require superuser privileges.\\nOn most distributions of Linux, this will require you to log in as root.\\nFor systems that have enabled the sudo package, use the sudo prefix for all necessary commands.\\n2.\\nPre-installation Actions \\uf0c1 Some actions must be taken before the CUDA Toolkit and Driver can be installed on Linux: Verify the system has a CUDA-capable GPU.\\nVerify the system is running a supported version of Linux.\\nVerify the system has gcc installed.\\nVerify the system has the correct kernel headers and development packages installed.\\nDownload the NVIDIA CUDA Toolkit.\\nHandle conflicting installation methods.\\nNote You can override the install-time prerequisite checks by running the installer with the -override flag.\\nRemember that the prerequisites will still be required to use the NVIDIA CUDA Toolkit.\\n2.1.\\nVerify You Have a CUDA-Capable GPU \\uf0c1 To verify that your GPU is CUDA-capable, go to your distribution’s equivalent of System Properties, or, from the command line, enter: lspci | grep -i nvidia If you do not see any settings, update the PCI hardware database that Linux maintains by entering update-pciids (generally found in /sbin ) at the command line and rerun the previous lspci command.\\nIf your graphics card is from NVIDIA and it is listed in https://developer.nvidia.com/cuda-gpus , your GPU is CUDA-capable.\\nThe Release Notes for the CUDA Toolkit also contain a list of supported products.\\n2.2.\\nVerify You Have a Supported Version of Linux \\uf0c1 The CUDA Development Tools are only supported on some specific distributions of Linux.\\nThese are listed in the CUDA Toolkit release notes.\\nTo determine which distribution and release number you’re running, type the following at the command line: uname -m && cat /etc/*release You should see output similar to the following, modified for your particular system: x86_64 Red Hat Enterprise Linux Workstation release 6.0 (Santiago) The x86_64 line indicates you are running on a 64-bit system.\\nThe remainder gives information about your distribution.\\n2.3.\\nVerify the System Has gcc Installed \\uf0c1 The gcc compiler is required for development using the CUDA Toolkit.\\nIt is not required for running CUDA applications.\\nIt is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly.\\nTo verify the version of gcc installed on your system, type the following on the command line: gcc --version If an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web.\\n2.4.\\nVerify the System has the Correct Kernel Headers and Development Packages Installed \\uf0c1 The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt.\\nFor example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed.\\nWhile the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed.\\nHowever, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using.\\nTherefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version.\\nThe version of the kernel your system is running can be found by running the following command: uname -r This is the version of the kernel headers and development packages that must be installed prior to installing the CUDA Drivers.\\nThis command will be used multiple times below to specify the version of the packages to install.\\nNote that below are the common-case scenarios for kernel usage.\\nMore advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running.\\nNote If you perform a system update which changes the version of the Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed.\\nOtherwise, the CUDA Driver will fail to work with the new kernel.\\n2.5.\\nInstall GPUDirect Storage \\uf0c1 If you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package.\\nGDS packages can be installed using the CUDA packaging guide.\\nFollow the instructions in MLNX_OFED Requirements and Installation .\\nGDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode.\\nInstallation instructions for them differ slightly.\\nCompatibility mode is the only mode that is supported on certain distributions due to software dependency limitations.\\nFull GDS support is restricted to the following Linux distros: Ubuntu 20.04, Ubuntu 22.04 RHEL 8.3, RHEL 8.4, RHEL 9.0 Starting with CUDA toolkit 12.2.2, GDS kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the NVIDIA open kernel driver.\\nFollow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages.\\n2.6.\\nChoose an Installation Method \\uf0c1 The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages).\\nThe distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution’s native package management system.\\nThe distribution-specific packages interface with the distribution’s native package management system.\\nIt is recommended to use the distribution-specific packages, where possible.\\nNote For both native as well as cross development, the toolkit must be installed using the distribution-specific installer.\\nSee the CUDA Cross-Platform Installation section for more details.\\n2.7.\\nDownload the NVIDIA CUDA Toolkit \\uf0c1 The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads .\\nChoose the platform you are using and download the NVIDIA CUDA Toolkit.\\nThe CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.\\nDownload Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file.\\nIf either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.\\nTo calculate the MD5 checksum of the downloaded file, run the following: md5sum 2.8.\\nAddress Custom xorg.conf, If Applicable \\uf0c1 The driver relies on an automatically generated xorg.conf file at /etc/X11/xorg.conf .\\nIf a custom-built xorg.conf file is present, this functionality will be disabled and the driver may not work.\\nYou can try removing the existing xorg.conf file, or adding the contents of /etc/X11/xorg.conf.d/00-nvidia.conf to the xorg.conf file.\\nThe xorg.conf file will most likely need manual tweaking for systems with a non-trivial GPU configuration.\\n2.9.\\nHandle Conflicting Installation Methods \\uf0c1 Before installing CUDA, any previous installations that could conflict should be uninstalled.\\nThis will not affect systems which have not had CUDA installed previously, or systems where the installation method has been preserved (RPM/Deb vs. Runfile).\\nSee the following charts for specifics.\\nTable 3 CUDA Toolkit Installation Compatibility Matrix \\uf0c1 Installed Toolkit Version == X.Y Installed Toolkit Version != X.Y RPM/Deb run RPM/Deb run Installing Toolkit Version X.Y RPM/Deb No Action Uninstall Run No Action No Action run Uninstall RPM/Deb Uninstall Run No Action No Action Table 4 NVIDIA Driver Installation Compatibility Matrix \\uf0c1 Installed Driver Version == X.Y Installed Driver Version != X.Y RPM/Deb run RPM/Deb run Installing Driver Version X.Y RPM/Deb No Action Uninstall Run No Action Uninstall Run run Uninstall RPM/Deb No Action Uninstall RPM/Deb No Action Use the following command to uninstall a Toolkit runfile installation: sudo /usr/local/cuda-X.Y/bin/cuda-uninstaller Use the following command to uninstall a Driver runfile installation: sudo /usr/bin/nvidia-uninstall Use the following commands to uninstall an RPM/Deb installation: sudo dnf remove ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 5/5 [01:08<00:00, 13.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 5, 'ids': [0, 1, 2, 3, 4], 'cost': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(text_lines, desc=\"Creating embeddings\")):\n",
    "    data.append({\"id\": i, \"vector\": embed_text(line), \"text\": line})\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the visual studio edition?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_res = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[\n",
    "        embed_text(question)\n",
    "    ],  # Use the `emb_text` function to convert the question to an embedding vector\n",
    "    limit=3,  # Return top 3 results\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "    output_fields=[\"text\"],  # Return the text field\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"NVIDIA CUDA Samples Description CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples.\\nThey are no longer included in the CUDA toolkit.\\nNVIDIA Nsight Visual Studio Edition (Windows only) Description NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications.\\nDefault Install Location of Nsight Visual Studio Edition Windows platform: %ProgramFiles(x86)%\\\\NVIDIA Corporation\\\\Nsight Visual Studio Edition #.\",\n",
      "        0.42950037121772766\n",
      "    ],\n",
      "    [\n",
      "        \"1.\\nLicense Agreement for NVIDIA Software Development Kits \\uf0c1 Important Notice\\u2014Read before downloading, installing, copying or using the licensed software: This license agreement, including exhibits attached (\\u201cAgreement\\u201d) is a legal agreement between you and NVIDIA Corporation (\\u201cNVIDIA\\u201d) and governs your use of a NVIDIA software development kit (\\u201cSDK\\u201d).\\nEach SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation.\\nThis Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used.\\nIf you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case \\u201cyou\\u201d will mean the entity you represent.\\nIf you don\\u2019t have the required age or authority to accept this Agreement, or if you don\\u2019t accept all the terms and conditions of this Agreement, do not download, install or use the SDK.\\nYou agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.\\n1.1.\\nLicense \\uf0c1 1.1.1.\\nLicense Grant \\uf0c1 Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to: Install and use the SDK, Modify and create derivative works of sample source code delivered in the SDK, and Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement.\\n1.1.2.\\nDistribution Requirements \\uf0c1 These are the distribution requirements for you to exercise the distribution grant: Your application must have material additional functionality, beyond the included portions of the SDK.\\nThe distributable portions of the SDK shall only be accessed by your application.\\nThe following notice shall be included in modifications and derivative works of sample source code distributed: \\u201cThis software contains source code provided by NVIDIA Corporation.\\u201d Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only.\\nThe terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA\\u2019s intellectual property rights.\\nAdditionally, you agree that you will protect the privacy, security and legal rights of your application users.\\nYou agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK.\\n1.1.3.\\nAuthorized Users \\uf0c1 You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf.\\nIf you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network.\\nYou are responsible for the compliance with the terms of this Agreement by your authorized users.\\nIf you become aware that your authorized users didn\\u2019t follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences.\\n1.1.4.\\nPre-Release SDK \\uf0c1 The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials.\\nUse of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss.\\nYou may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems.\\nNVIDIA may choose not to make available a commercial version of any pre-release SDK.\\nNVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability.\\n1.1.5.\\nUpdates \\uf0c1 NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK.\\nUnless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement.\\nYou agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you.\\nWhile NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK.\\n1.1.6.\\nComponents Under Other Licenses \\uf0c1 The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK.\\nIf and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict.\\nSubject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses.\\n1.1.7.\\nReservation of Rights \\uf0c1 NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement.\\n1.2.\\nLimitations \\uf0c1 The following license limitations apply to your use of the SDK: You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK.\\nExcept as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK.\\nFor clarity, you may not distribute or sublicense the SDK as a stand-alone product.\\nUnless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA.\\nYou may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK.\\nYou may not use the SDK in any manner that would cause it to become subject to an open source software license.\\nAs examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be: Disclosed or distributed in source code form; Licensed for the purpose of making derivative works; or Redistributable at no charge.\\nYou acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a \\u201cCritical Application\\u201d).\\nExamples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications.\\nNVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses.\\nYou are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements.\\nYou agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney\\u2019s fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms.\\nYou may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform.\\n1.3.\\nOwnership \\uf0c1 NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2 .\\nThis SDK may include software and materials from NVIDIA\\u2019s licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights.\\nYou hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIA\\u2019s rights under Section 1.3.1 .\\nYou may, but don\\u2019t have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK.\\nFor any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you.\\nNVIDIA will use feedback at its choice.\\nNVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com .\\n1.4.\\nNo Warranties \\uf0c1 THE SDK IS PROVIDED BY NVIDIA \\u201cAS IS\\u201d AND \\u201cWITH ALL FAULTS.\\u201d TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT.\\nNO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE.\\n1.5.\\nLimitation of Liability \\uf0c1 TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY.\\nIN NO EVENT WILL NVIDIA\\u2019S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00.\\nTHE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.\\nThese exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose.\\nThese exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different.\\n1.6.\\nTermination \\uf0c1 This Agreement will continue to apply until terminated by either you or NVIDIA as described below.\\nIf you want to terminate this Agreement, you may do so by stopping to use the SDK.\\nNVIDIA may, at any time, terminate this Agreement if: (i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIA\\u2019s intellectual property rights); (ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or (iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIA\\u2019s sole discretion, the continued use of it is no longer commercially viable.\\nUpon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control.\\nYour prior distributions in accordance with this Agreement are not affected by the termination of this Agreement.\\nUpon written request, you will certify in writing that you have complied with your commitments under this section.\\nUpon any termination of this Agreement all provisions survive except for the license grant provisions.\\n1.7.\\nGeneral \\uf0c1 If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission.\\nAny attempted assignment not approved by NVIDIA in writing shall be void and of no effect.\\nNVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified.\\nYou agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement.\\nThis Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles.\\nThe United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed.\\nYou agree to all terms of this Agreement in the English language.\\nThe state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement.\\nNotwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.\\nIf any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect.\\nUnless otherwise specified, remedies are cumulative.\\nEach party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement.\\nThe SDK has been developed entirely at private expense and is \\u201ccommercial items\\u201d consisting of \\u201ccommercial computer software\\u201d and \\u201ccommercial computer software documentation\\u201d provided with RESTRICTED RIGHTS.\\nUse, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable.\\nContractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.\\nThe SDK is subject to United States export laws and regulations.\\nYou agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasury\\u2019s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations.\\nThese laws include restrictions on destinations, end users and end use.\\nBy accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law.\\nAny notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax.\\nYou agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements.\\nPlease direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department.\\nThis Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license.\\nAny additional and/or conflicting terms on documents issued by you are null, void, and invalid.\\nAny amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.\\n2.\\nCUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits \\uf0c1 The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (\\u201cAgreement\\u201d) as modified by this supplement.\\nCapitalized terms used but not defined below have the meaning assigned to them in the Agreement.\\nThis supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement.\\nIn the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.\\n2.1.\\nLicense Scope \\uf0c1 The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs.\\n2.2.\\nDistribution \\uf0c1 The portions of the SDK that are distributable under the Agreement are listed in Attachment A.\\n2.3.\\nOperating Systems \\uf0c1 Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files).\\n2.4.\\nAudio and Video Encoders and Decoders \\uf0c1 You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies.\\nNVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders.\\n2.5.\\nLicensing \\uf0c1 If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions @ nvidia .\\ncom .\\n2.6.\\nAttachment A \\uf0c1 The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable.\\nComponent CUDA Runtime Windows cudart.dll, cudart_static.lib, cudadevrt.lib Mac OSX libcudart.dylib, libcudart_static.a, libcudadevrt.a Linux libcudart.so, libcudart_static.a, libcudadevrt.a Android libcudart.so, libcudart_static.a, libcudadevrt.a Component CUDA FFT Library Windows cufft.dll, cufftw.dll, cufft.lib, cufftw.lib Mac OSX libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a Linux libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Android libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Component CUDA BLAS Library Windows cublas.dll, cublasLt.dll Mac OSX libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a Linux libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Android libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Component NVIDIA \\u201cDrop-in\\u201d BLAS Library Windows nvblas.dll Mac OSX libnvblas.dylib Linux libnvblas.so Component CUDA Sparse Matrix Library Windows cusparse.dll, cusparse.lib Mac OSX libcusparse.dylib, libcusparse_static.a Linux libcusparse.so, libcusparse_static.a Android libcusparse.so, libcusparse_static.a Component CUDA Linear Solver Library Windows cusolver.dll, cusolver.lib Mac OSX libcusolver.dylib, libcusolver_static.a Linux libcusolver.so, libcusolver_static.a Android libcusolver.so, libcusolver_static.a Component CUDA Random Number Generation Library Windows curand.dll, curand.lib Mac OSX libcurand.dylib, libcurand_static.a Linux libcurand.so, libcurand_static.a Android libcurand.so, libcurand_static.a Component NVIDIA Performance Primitives Library Windows nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib Mac OSX libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a Linux libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Android libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Component NVIDIA JPEG Library Windows nvjpeg.lib, nvjpeg.dll Linux libnvjpeg.so, libnvjpeg_static.a Component Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP Mac OSX libculibos.a Linux libculibos.a Component NVIDIA Runtime Compilation Library and Header All nvrtc.h Windows nvrtc.dll, nvrtc-builtins.dll Mac OSX libnvrtc.dylib, libnvrtc-builtins.dylib Linux libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a Component NVIDIA Optimizing Compiler Library Windows nvvm.dll Mac OSX libnvvm.dylib Linux libnvvm.so Component NVIDIA JIT Linking Library Windows libnvJitLink.dll, libnvJitLink.lib Linux libnvJitLink.so, libnvJitLink_static.a Component NVIDIA Common Device Math Functions Library Windows libdevice.10.bc Mac OSX libdevice.10.bc Linux libdevice.10.bc Component CUDA Occupancy Calculation Header Library All cuda_occupancy.h Component CUDA Half Precision Headers All cuda_fp16.h, cuda_fp16.hpp Component CUDA Profiling Tools Interface (CUPTI) Library Windows cupti.dll Mac OSX libcupti.dylib Linux libcupti.so Component NVIDIA Tools Extension Library Windows nvToolsExt.dll, nvToolsExt.lib Mac OSX libnvToolsExt.dylib Linux libnvToolsExt.so Component NVIDIA CUDA Driver Libraries Linux libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a Component NVIDIA CUDA File IO Libraries and Header All cufile.h Linux libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply: The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software.\\n2.7.\\nAttachment B \\uf0c1 Additional Licensing Obligations The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions: Licensee\\u2019s use of the GDB third party component is subject to the terms and conditions of GNU GPL v3: This product includes copyrighted third-party software licensed under the terms of the GNU General Public License v3 (\\\"GPL v3\\\").\\nAll third-party software packages are copyright by their respective authors.\\nGPL v3 terms and conditions are hereby incorporated into the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses.\\nTo obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests @ nvidia .\\nThis offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION.\\nComponent License CUDA-GDB GPL v3 Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licensee\\u2019s use of the H.264 video codecs are solely the responsibility of Licensee.\\nLicensee\\u2019s use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0.\\nApache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference.\\nhttp://www.apache.org/licenses/LICENSE-2.0.html In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries.\\nBoost Software License - Version 1.0 - August 17th, 2003 .\\n.\\nPermission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \\\"Software\\\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor.\\nTHE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT.\\nIN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nLicensee\\u2019s use of the LLVM third party component is subject to the following terms and conditions: ====================================================== LLVM Release License ====================================================== University of Illinois/NCSA Open Source License Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign.\\nAll rights reserved.\\nDeveloped by: LLVM Team University of Illinois at Urbana-Champaign http://llvm.org Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\\"Software\\\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\\n* Neither the names of the LLVM Team, University of Illinois at Urbana- Champaign, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\\nTHE SOFTWARE IS PROVIDED \\\"AS IS\\\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\\nIN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\\nLicensee\\u2019s use of the PCRE third party component is subject to the following terms and conditions: ------------ PCRE LICENCE ------------ PCRE is a library of functions to support regular expressions whose syntax and semantics are as close as possible to those of the Perl 5 language.\\nRelease 8 of PCRE is distributed under the terms of the \\\"BSD\\\" licence, as specified below.\\nThe documentation for PCRE, supplied in the \\\"doc\\\" directory, is distributed under the same terms as the software itself.\\nThe basic library functions are written in C and are freestanding.\\nAlso included in the distribution is a set of C++ wrapper functions, and a just- in-time compiler that can be used to optimize pattern matching.\\nThese are both optional features that can be omitted when the library is built.\\nTHE BASIC LIBRARY FUNCTIONS --------------------------- Written by: Philip Hazel Email local part: ph10 Email domain: cam.ac.uk University of Cambridge Computing Service, Cambridge, England.\\nCopyright (c) 1997-2012 University of Cambridge All rights reserved.\\nPCRE JUST-IN-TIME COMPILATION SUPPORT ------------------------------------- Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2010-2012 Zoltan Herczeg All rights reserved.\\nSTACK-LESS JUST-IN-TIME COMPILER -------------------------------- Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2009-2012 Zoltan Herczeg All rights reserved.\\nTHE C++ WRAPPER FUNCTIONS ------------------------- Contributed by: Google Inc.\\nCopyright (c) 2007-2012, Google Inc. All rights reserved.\\nTHE \\\"BSD\\\" LICENCE ----------------- Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of the University of Cambridge nor the name of Google Inc. nor the names of their contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \\\"AS IS\\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\nIN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nSome of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2007-2009, Regents of the University of California All rights reserved.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Neither the name of the University of California, Berkeley nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR \\\"AS IS\\\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\nIN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nSome of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata.\\n* The name of the author may not be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2010 The University of Tennessee.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer listed in this license in the documentation and/or other materials provided with the distribution.\\n* Neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2012, The Science and Technology Facilities Council (STFC).\\n* Neither the name of the STFC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nIN NO EVENT SHALL THE STFC BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nSome of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows: -- (C) Copyright 2013 King Abdullah University of Science and Technology Authors: Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa) David Keyes (david.keyes@kaust.edu.sa) Hatem Ltaief (hatem.ltaief@kaust.edu.sa) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Neither the name of the King Abdullah University of Science and Technology nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\\nIN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows: Copyright (c) 2012, University of Illinois.\\nDeveloped by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\\"Software\\\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Neither the names of IMPACT Group, University of Illinois, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\\nSome of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license: Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima University.\\nCopyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima University and University of Tokyo.\\n* Neither the name of the Hiroshima University nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license: Copyright 2010-2011, D. E. Shaw Research.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of D. E. Shaw Research nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nSome of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license: Copyright (c) 2015-2017, Norbert Juffa All rights reserved.\\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1.\\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\nIN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\nLicensee\\u2019s use of the lz4 third party component is subject to the following terms and conditions: Copyright (C) 2011-2013, Yann Collet.\\nBSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\nThe NPP library uses code from the Boost Math Toolkit, and is subject to the following license: Boost Software License - Version 1.0 - August 17th, 2003 .\\nPortions of the Nsight Eclipse Edition is subject to the following license: The Eclipse Foundation makes available all content in this plug-in (\\\"Content\\\").\\nUnless otherwise indicated below, the Content is provided to you under the terms and conditions of the Eclipse Public License Version 1.0 (\\\"EPL\\\").\\nA copy of the EPL is available at http:// www.eclipse.org/legal/epl-v10.html.\\nFor purposes of the EPL, \\\"Program\\\" will mean the Content.\\nIf you did not receive this Content directly from the Eclipse Foundation, the Content is being redistributed by another party (\\\"Redistributor\\\") and different terms and conditions may apply to your use of any object code in the Content.\\nCheck the Redistributor's license that was provided with the Content.\\nIf no such license exists, contact the Redistributor.\\nUnless otherwise indicated below, the terms and conditions of the EPL still apply to any source code in the Content and such source code may be obtained at http://www.eclipse.org.\\nSome of the cuBLAS library routines uses code from OpenAI, which is subject to the following license: License URL https://github.com/openai/openai-gemm/blob/master/LICENSE License Text The MIT License Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc.\\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\\"Software\\\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\\nLicensee\\u2019s use of the Visual Studio Setup Configuration Samples is subject to the following license: The MIT License (MIT) Copyright (C) Microsoft Corporation.\\nLicensee\\u2019s use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0.\\nThe DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license .\\nComponents of the driver and compiler used for binary management, including nvFatBin, nvcc, and cuobjdump, use the Zstandard library which is subject to the following license: BSD License For Zstandard software Copyright (c) Meta Platforms, Inc. and affiliates.\\n* Neither the name Facebook, nor Meta, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright \\u00a9 2024, NVIDIA Corporation.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); });1.\\nIntroduction 2.\\nWindows 2.1.\\nNetwork Installer 2.2.\\nLocal Installer 2.3.\\nPip Wheels - Windows 2.4.\\nConda 3.\\nLinux 3.1.\\nLinux x86_64 3.1.1.\\nRedhat / CentOS 3.1.1.1.\\nRPM Installer 3.1.1.2.\\nRunfile Installer 3.1.2.\\nFedora 3.1.2.1.\\nRPM Installer 3.1.2.2.\\nRunfile Installer 3.1.3.\\nSUSE Linux Enterprise Server 3.1.3.1.\\nRPM Installer 3.1.3.2.\\nRunfile Installer 3.1.4.\\nOpenSUSE 3.1.4.1.\\nRPM Installer 3.1.4.2.\\nRunfile Installer 3.1.5.\\nAmazon Linux 2023 3.1.5.1.\\nPrepare Amazon Linux 2023 3.1.5.2.\\nLocal Repo Installation for Amazon Linux 3.1.5.3.\\nNetwork Repo Installation for Amazon Linux 3.1.5.4.\\nCommon Installation Instructions for Amazon Linux 3.1.6.\\nPip Wheels - Linux 3.1.7.\\nConda 3.1.8.\\nWSL 3.1.9.\\nUbuntu 3.1.9.1.\\nDebian Installer 3.1.9.2.\\nRunfile Installer 3.1.10.\\nDebian 3.1.10.1.\\nDebian Installer 3.1.10.2.\\nRunfile Installer 4.\\nNotices 4.1.\\nNotice 4.2.\\nOpenCL 4.3.\\nTrademarks Quick Start Guide \\u00bb 1.\\nIntroduction v12.5 | PDF | Archive CUDA Quick Start Guide Minimal first-steps instructions to get CUDA running on a standard system.\\nIntroduction \\uf0c1 This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform.\\nThese instructions are intended to be used on a clean installation of a supported platform.\\nFor questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide .\\nThe CUDA installation packages can be found on the CUDA Downloads Page .\\n2.\\nWindows \\uf0c1 When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer.\\nThe Network Installer allows you to download only the files you need.\\nThe Local Installer is a stand-alone installer with a large initial download.\\nFor more details, refer to the Windows Installation Guide .\\n2.1.\\nNetwork Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nLaunch the downloaded installer package.\\nRead and accept the EULA.\\nSelect next to download and install all components.\\nOnce the download completes, the installation will begin automatically.\\nOnce the installation completes, click \\u201cnext\\u201d to acknowledge the Nsight Visual Studio Edition installation summary.\\nClick close to close the installer.\\nNavigate to the Samples\\u2019 nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody .\\nOpen the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln .\\nOpen the Build menu within Visual Studio and click Build Solution .\\nNavigate to the CUDA Samples build directory and run the nbody sample.\\nNote Run samples by navigating to the executable\\u2019s location, otherwise it will fail to locate dependent resources.\\n2.2.\\nLocal Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nSelect next to install all components.\\nOnce the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary.\\nOpen the nbody Visual Studio solution file for the version of Visual Studio you have installed.\\n2.3.\\nPip Wheels - Windows \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.\\nThese packages are intended for runtime use and do not currently include developer tools (these can be installed separately).\\nPlease note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.\\nPrerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo.\\nIf your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules.\\nIf these Python modules are out-of-date then the commands which follow later in this section may fail.\\npy -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.\\npy -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version.\\n\\u201ccu12\\u201d should be read as \\u201ccuda12\\u201d.\\nnvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 2.4.\\nConda \\uf0c1 The Conda packages are available at https://anaconda.org/nvidia .\\nInstallation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.\\nLinux \\uf0c1 CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on.\\n3.1.\\nLinux x86_64 \\uf0c1 For development on the x86_64 architecture.\\nIn some cases, x86_64 systems may act as host platforms targeting other architectures.\\nSee the Linux Installation Guide for more details.\\n3.1.1.\\nRedhat / CentOS \\uf0c1 When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer.\\nThe Runfile Installer is only available as a Local Installer.\\nThe RPM Installer is available as both a Local Installer and a Network Installer.\\nIn the case of the RPM installers, the instructions for the Local and Network variants are the same.\\nFor more details, refer to the Linux Installation Guide .\\n3.1.1.1.\\nRPM Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nInstall EPEL to satisfy the DKMS dependency by following the instructions at EPEL\\u2019s website .\\nEnable optional repos : On RHEL 8 Linux only, execute the following steps to enable optional repositories.\\nOn x86_64 workstation: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Install the repository meta-data, clean the yum cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo yum clean expire-cache sudo yum install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.1.2.\\nRunfile Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nDisable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Reboot into runlevel 3 by temporarily adding the number \\u201c3\\u201d and the word \\u201cnomodeset\\u201d to the end of the system\\u2019s kernel boot parameters.\\nRun the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.2.\\nFedora \\uf0c1 When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer.\\n3.1.2.1.\\nInstall the RPMFusion free repository to satisfy the Akmods dependency: su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm' Install the repository meta-data, clean the dnf cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo dnf clean expire-cache sudo dnf install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.2.2.\\nDisable the Nouveau drivers: Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the below command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Reboot into runlevel 3 by temporarily adding the number \\u201c3\\u201d and the word \\u201cnomodeset\\u201d to the end of the system\\u2019s kernel boot parameters.\\nRun the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface.\\nSet up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.3.\\nSUSE Linux Enterprise Server \\uf0c1 When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer.\\n3.1.3.1.\\nInstall the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo SUSEConnect --product PackageHub/15/x86_64 sudo zypper refresh sudo rpm --erase gpg-pubkey-7fa2af80* sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd .\\n3.1.3.2.\\nReboot into runlevel 3 by temporarily adding the number \\u201c3\\u201d and the word \\u201cnomodeset\\u201d to the end of the system\\u2019s kernel boot parameters.\\nRun the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd .\\n3.1.4.\\nOpenSUSE \\uf0c1 When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer.\\n3.1.4.1.\\nInstall the repository meta-data, refresh the Zypper cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo zypper refresh sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.4.2.\\nDisable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd Reboot into runlevel 3 by temporarily adding the number \\u201c3\\u201d and the word \\u201cnomodeset\\u201d to the end of the system\\u2019s kernel boot parameters.\\n3.1.5.\\nAmazon Linux 2023 \\uf0c1 3.1.5.1.\\nPrepare Amazon Linux 2023 \\uf0c1 Perform the pre-installation actions.\\nThe kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo .\\n3.1.5.2.\\nLocal Repo Installation for Amazon Linux \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*.x86_64.rpm 3.1.5.3.\\nNetwork Repo Installation for Amazon Linux \\uf0c1 Enable the network repository and clean the DN cache: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo sudo dnf clean expire-cache 3.1.5.4.\\nCommon Installation Instructions for Amazon Linux \\uf0c1 These instructions apply to both local and network installation for Amazon Linux.\\nInstall CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory.\\nFor pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory.\\nReboot the system: sudo reboot Perform the post-installation actions.\\n3.1.6.\\nPip Wheels - Linux \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.\\npython3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.\\npython3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version.\\nnvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 nvidia-opencl-cu12 nvidia-nvjitlink-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 nvidia-opencl-cu125 nvidia-nvjitlink-cu125 3.1.7.\\nInstallation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.1.8.\\nWSL \\uf0c1 These instructions must be used if you are installing in a WSL environment.\\nDo not use the Ubuntu instructions in this case.\\nInstall repository meta-data sudo dpkg -i cuda-repo-__.deb Update the CUDA public GPG key sudo apt-key del 7fa2af80 When installing using the local repo: sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/ When installing using the network repo: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb Pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-.pin sudo mv cuda-.pin /etc/apt/preferences.d/cuda-repository-pin-600 Update the Apt repository cache and install CUDA sudo apt-get update sudo apt-get install cuda 3.1.9.\\nUbuntu \\uf0c1 When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer.\\nThe Debian Installer is available as both a Local Installer and a Network Installer.\\nIn the case of the Debian installers, the instructions for the Local and Network variants are the same.\\n3.1.9.1.\\nDebian Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation.\\nInstall the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA: sudo dpkg --install cuda-repo--..deb sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb sudo add-apt-repository contrib sudo apt-get update sudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.9.2.\\nDisable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u Reboot into runlevel 3 by temporarily adding the number \\u201c3\\u201d and the word \\u201cnomodeset\\u201d to the end of the system\\u2019s kernel boot parameters.\\n3.1.10.\\nDebian \\uf0c1 When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer.\\n3.1.10.1.\\nInstall the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA: sudo dpkg -i cuda-repo-__.deb sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb sudo add-apt-repository contrib sudo apt-get update sudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody .\\n3.1.10.2.\\n4.\\nNotices \\uf0c1 4.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (\\u201cNVIDIA\\u201d) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (\\u201cTerms of Sale\\u201d).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer\\u2019s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer\\u2019s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer\\u2019s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, \\u201cMATERIALS\\u201d) ARE BEING PROVIDED \\u201cAS IS.\\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA\\u2019s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n4.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 4.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright \\u00a9 2015-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jun 25, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \\\"undefined\\\"){_satellite.pageBottom();}1.\\nIntroduction 1.1.\\nSystem Requirements 1.2.\\nAbout This Document 2.\\nInstalling CUDA Development Tools 2.1.\\nVerify You Have a CUDA-Capable GPU 2.2.\\nDownload the NVIDIA CUDA Toolkit 2.3.\\nInstall the CUDA Software 2.3.1.\\nUninstalling the CUDA Software 2.4.\\nUsing Conda to Install the CUDA Software 2.4.1.\\nConda Overview 2.4.2.\\nInstallation 2.4.3.\\nUninstallation 2.4.4.\\nInstalling Previous CUDA Releases 2.5.\\nUse a Suitable Driver Model 2.6.\\nVerify the Installation 2.6.1.\\nRunning the Compiled Examples 3.\\nPip Wheels 4.\\nCompiling CUDA Programs 4.1.\\nCompiling Sample Projects 4.2.\\nSample Projects 4.3.\\nBuild Customizations for New Projects 4.4.\\nBuild Customizations for Existing Projects 5.\\nAdditional Considerations 6.\\nNotices 6.1.\\nNotice 6.2.\\nOpenCL 6.3.\\nTrademarks Installation Guide Windows \\u00bb 1.\\nIntroduction v12.5 | PDF | Archive CUDA Installation Guide for Microsoft Windows The installation instructions for the CUDA Toolkit on Microsoft Windows systems.\\nIntroduction \\uf0c1 CUDA \\u00ae is a parallel computing platform and programming model invented by NVIDIA.\\nIt enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).\\nCUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms.\\nWith CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.\\nSupport heterogeneous computation where applications use both the CPU and GPU.\\nSerial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU.\\nAs such, CUDA can be incrementally applied to existing applications.\\nThe CPU and GPU are treated as separate devices that have their own memory spaces.\\nThis configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.\\nCUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads.\\nThese cores have shared resources including a register file and a shared memory.\\nThe on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.\\nThis guide will show you how to install and check the correct operation of the CUDA development tools.\\n1.1.\\nSystem Requirements \\uf0c1 To use CUDA on your system, you will need the following installed: A CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) Supported Microsoft Windows \\u00ae operating systems: Microsoft Windows 11 21H2 Microsoft Windows 11 22H2-SV2 Microsoft Windows 11 23H2 Microsoft Windows 10 21H2 Microsoft Windows 10 22H2 Microsoft Windows Server 2022 Table 1 Windows Compiler Support in CUDA 12.5 \\uf0c1 Compiler* IDE Native x86_64 Cross-compilation (32-bit on 64-bit) C++ Dialect MSVC Version 193x Visual Studio 2022 17.x YES Not supported C++14 (default), C++17, C++20 MSVC Version 192x Visual Studio 2019 16.x YES C++14 (default), C++17 MSVC Version 191x Visual Studio 2017 15.x (RTW and all updates) YES C++14 (default), C++17 * Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5.\\n32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit.\\nUse the CUDA Toolkit from earlier releases for 32-bit compilation.\\nCUDA Driver will continue to support running 32-bit application binaries on GeForce GPUs until Ada.\\nAda will be the last architecture with driver support for 32-bit applications.\\nHopper does not support 32-bit applications.\\nSupport for running x86 32-bit applications on x86_64 Windows is limited to use with: CUDA Driver CUDA Runtime (cudart) CUDA Math Library (math.h) 1.2.\\nAbout This Document \\uf0c1 This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment.\\nYou do not need previous experience with CUDA or experience with parallel computation.\\n2.\\nInstalling CUDA Development Tools \\uf0c1 Basic instructions can be found in the Quick Start Guide .\\nRead on for more detailed instructions.\\nThe setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps: Verify the system has a CUDA-capable GPU.\\nDownload the NVIDIA CUDA Toolkit.\\nInstall the NVIDIA CUDA Toolkit.\\nTest that the installed software runs correctly and communicates with the hardware.\\n2.1.\\nVerify You Have a CUDA-Capable GPU \\uf0c1 You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager .\\nHere you will find the vendor name and model of your graphics card(s).\\nIf you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus , that GPU is CUDA-capable.\\nThe Release Notes for the CUDA Toolkit also contain a list of supported products.\\nThe Windows Device Manager can be opened via the following steps: Open a run window from the Start Menu Run: control /name Microsoft.DeviceManager 2.2.\\nDownload the NVIDIA CUDA Toolkit \\uf0c1 The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads .\\nChoose the platform you are using and one of the following installer formats: Network Installer: A minimal installer which later downloads packages required for installation.\\nOnly the packages selected during the selection phase of the installer are downloaded.\\nThis installer is useful for users who want to minimize download time.\\nFull Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download.\\nThis installer is useful for systems which lack network access and for enterprise deployment.\\nThe CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.\\nDownload Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file.\\nIf either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.\\n2.3.\\nInstall the CUDA Software \\uf0c1 Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality.\\nNote The driver and toolkit must be installed for CUDA to function.\\nIf you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit.\\nNote The installation may fail if Windows Update starts after the installation has begun.\\nWait until Windows Update is complete and then try the installation again.\\nGraphical Installation Install the CUDA Software by executing the CUDA installer and following the on-screen prompts.\\nSilent Installation The installer can be executed in silent mode by executing the package with the -s flag.\\nAdditional parameters can be passed which will install specific subpackages instead of all packages.\\nSee the table below for a list of all the subpackage names.\\nTable 2 Possible Subpackage Names \\uf0c1 Subpackage Name Subpackage Description Toolkit Subpackages (defaults to C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5) cuda_profiler_api_12.5 CUDA Profiler API.\\ncudart_12.5 CUDA Runtime libraries.\\ncuobjdump_12.5 Extracts information from cubin files.\\ncupti_12.5 The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications.\\ncuxxfilt_12.5 The CUDA cu++ filt demangler tool.\\ndemo_suite_12.5 Prebuilt demo applications using CUDA.\\ndocumentation_12.5 CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc.\\nnvcc_12.5 CUDA compiler.\\nnvdisasm_12.5 Extracts information from standalone cubin files.\\nnvfatbin_12.5 Library for creating fatbinaries at runtime.\\nnvjitlink_12.5 nvJitLink library.\\nnvml_dev_12.5 NVML development libraries and headers.\\nnvprof_12.5 Tool for collecting and viewing CUDA application profiling data from the command-line.\\nnvprune_12.5 Prunes host object files and libraries to only contain device code for the specified targets.\\nnvrtc_12.5 nvrtc_dev_12.5 NVRTC runtime libraries.\\nnvtx_12.5 NVTX on Windows.\\nopencl_12.5 OpenCL library.\\nvisual_profiler_12.5 Visual Profiler.\\nsanitizer_12.5 Compute Sanitizer API.\\nthrust_12.5 CUDA Thrust.\\ncublas_12.5 cublas_dev_12.5 cuBLAS runtime libraries.\\ncufft_12.5 cufft_dev_12.5 cuFFT runtime libraries.\\ncurand_12.5 curand_dev_12.5 cuRAND runtime libraries.\\ncusolver_12.5 cusolver_dev_12.5 cuSOLVER runtime libraries.\\ncusparse_12.5 cusparse_dev_12.5 cuSPARSE runtime libraries.\\nnpp_12.5 npp_dev_12.5 NPP runtime libraries.\\nnvjpeg_12.5 nvjpeg_dev_12.5 nvJPEG libraries.\\nnsight_compute_12.5 Nsight Compute.\\nnsight_systems_12.5 Nsight Systems.\\nnsight_vse_12.5 Installs the Nsight Visual Studio Edition plugin in all VS. occupancy_calculator_12.5 Installs the CUDA_Occupancy_Calculator.xls tool.\\nvisual_studio_integration_12.5 Installs CUDA project wizard and builds customization files in VS. Driver Subpackages Display.Driver The NVIDIA Display Driver.\\nRequired to run CUDA applications.\\nFor example, to install only the compiler and driver components: .exe -s nvcc_12.1 Display.Driver Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required.\\nExtracting and Inspecting the Files Manually Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation.\\nThe full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip .\\nOnce extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration.\\nWithin each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files.\\nNote Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration.\\nThis is intended for enterprise-level deployment.\\n2.3.1.\\nUninstalling the CUDA Software \\uf0c1 All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget.\\n2.4.\\nUsing Conda to Install the CUDA Software \\uf0c1 This section describes the installation and configuration of CUDA when using the Conda installer.\\nThe Conda packages are available at https://anaconda.org/nvidia .\\n2.4.1.\\nConda Overview \\uf0c1 The Conda installation installs the CUDA Toolkit.\\nThe installation steps are listed below.\\n2.4.2.\\nInstallation \\uf0c1 To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda - c nvidia 2.4.3.\\nUninstallation \\uf0c1 To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 2.4.4.\\nInstalling Previous CUDA Releases \\uf0c1 All Conda packages released under a specific CUDA version are labeled with that release version.\\nTo install a previous version, include that label in the install command such as: conda install cuda - c nvidia / label / cuda -11.3.0 Note Some CUDA releases do not move to new versions of all installable components.\\nWhen this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as: conda install cuda - c nvidia / label / cuda -11.3.0 - c nvidia / label / cuda -11.3.1 This example will install all packages released as part of CUDA 11.3.1.\\n2.5.\\nUse a Suitable Driver Model \\uf0c1 On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate: The WDDM driver model is used for display devices.\\nThe Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model.\\nTCC is enabled by default on most recent NVIDIA Tesla GPUs.\\nTo check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details).\\nNote Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device.\\nNote NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode.\\n2.6.\\nVerify the Installation \\uf0c1 Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware.\\nTo do this, you need to compile and run some of the included sample programs.\\n2.6.1.\\nRunning the Compiled Examples \\uf0c1 The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window.\\nYou can display a Command Prompt window by going to: Start > All Programs > Accessories > Command Prompt CUDA Samples are located in https://github.com/nvidia/cuda-samples .\\nTo use the samples, clone the project, build the samples, and run them using the instructions on the Github page.\\nTo verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program.\\nThe sample can be built using the provided VS solution files in the deviceQuery folder.\\nThis assumes that you used the default installation directory structure.\\nIf CUDA is installed and configured correctly, the output should look similar to Figure 1 .\\nFigure 1 Valid Results from deviceQuery CUDA Sample \\uf0c1 The exact appearance and the output lines might be different on your system.\\nThe important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed.\\nIf a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed.\\nRunning the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly.\\nThe output should resemble Figure 2 .\\nFigure 2 Valid Results from bandwidthTest CUDA Sample \\uf0c1 The device name (second line) and the bandwidth numbers vary from system to system.\\nThe important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed.\\nIf the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed.\\nTo see a graphical representation of what CUDA can do, run the particles sample at https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles 3.\\nPip Wheels \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python.\\nThese packages are intended for runtime use and do not currently include developer tools (these can be installed separately).\\nPlease note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment.\\nPrerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo.\\nIf your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules.\\nIf these Python modules are out-of-date then the commands which follow later in this section may fail.\\npy -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module.\\npy -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version.\\n\\u201ccu12\\u201d should be read as \\u201ccuda12\\u201d.\\nnvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 nvidia-opencl-cu12 These metapackages install the following packages: nvidia-cublas-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 nvidia-opencl-cu125 4.\\nCompiling CUDA Programs \\uf0c1 The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code.\\nTo build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022.\\nYou can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples 4.1.\\nCompiling Sample Projects \\uf0c1 The bandwidthTest project is a good sample project to build and run.\\nIt is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest .\\nIf you elected to use the default installation location, the output is placed in CUDA Samples\\\\v12.5\\\\bin\\\\win64\\\\Release .\\nBuild the program using the appropriate solution file and run the executable.\\nIf all works correctly, the output should be similar to Figure 2 .\\n4.2.\\nSample Projects \\uf0c1 The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects.\\nA few of the example projects require some additional setup.\\nThese sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are.\\nThe environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process.\\nTable 3 CUDA Visual Studio .props locations \\uf0c1 Visual Studio CUDA 12.5 .props file Install Directory Visual Studio 2015 (deprecated) C:Program Files (x86)\\\\MSBuild\\\\Microsoft.Cpp\\\\v4.0\\\\V140\\\\BuildCustomizations Visual Studio 2017 \\\\Common7\\\\IDE\\\\VC\\\\VCTargets\\\\BuildCustomizations Visual Studio 2019 C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Professional\\\\MSBuild\\\\Microsoft\\\\VC\\\\v160\\\\BuildCustomizations Visual Studio 2022 C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Professional\\\\MSBuild\\\\Microsoft\\\\VC\\\\v170\\\\BuildCustomizations You can reference this CUDA 12.5.props file when building your own CUDA applications.\\n4.3.\\nBuild Customizations for New Projects \\uf0c1 When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations.\\nTo accomplish this, click File-> New | Project\\u2026 NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version.\\nFor example, selecting the \\u201cCUDA 12.5 Runtime\\u201d template will configure your project for use with the CUDA 12.5 Toolkit.\\nThe new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIA\\u2019s Build Customizations.\\nAll standard capabilities of Visual Studio C++ projects will be available.\\nTo specify a custom CUDA Toolkit location, under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field as desired.\\nNote that the selected toolkit must match the version of the Build Customizations.\\nNote A supported version of MSVC must be installed to use this feature.\\n4.4.\\nBuild Customizations for Existing Projects \\uf0c1 When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations.\\nThis can be done using one of the following two methods: Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizations\\u2026 , then select the CUDA Toolkit version you would like to target.\\nAlternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit.\\nFirst add a CUDA build customization to your project as above.\\nThen, right click on the project name and select Properties .\\nUnder CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) .\\nNote that the $(CUDA_PATH) environment variable is set by the installer.\\nWhile Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2.\\nIf you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes.\\nYou can access the value of the $(CUDA_PATH) environment variable via the following steps: Open a run window from the Start Menu.\\nRun: control sysdm.cpl Select the Advanced tab at the top of the window.\\nClick Environment Variables at the bottom of the window.\\nFiles which contain CUDA code must be marked as a CUDA C/C++ file.\\nThis can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item , selecting NVIDIA CUDA 12.5\\\\CodeCUDA C/C++ File , and then selecting the file you wish to add.\\nFor advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following: msbuild /t:Rebuild /p:CudaToolkitDir=\\\"drive:/path/to/new/toolkit/\\\" 5.\\nAdditional Considerations \\uf0c1 Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs.\\nTo begin using CUDA to accelerate the performance of your own applications, consult the CUDA C Programming Guide, located in the CUDA Toolkit documentation directory.\\nA number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIA \\u00ae Nsight\\u2122 Visual Studio Edition, and NVIDIA Visual Profiler.\\nFor technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/ .\\n6.\\nNotices \\uf0c1 6.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (\\u201cNVIDIA\\u201d) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (\\u201cTerms of Sale\\u201d).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer\\u2019s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer\\u2019s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer\\u2019s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, \\u201cMATERIALS\\u201d) ARE BEING PROVIDED \\u201cAS IS.\\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA\\u2019s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n6.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 6.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright \\u00a9 2009-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \\\"undefined\\\"){_satellite.pageBottom();}1.\\nIntroduction 1.1.\\nSystem Requirements 1.2.\\nOS Support Policy 1.3.\\nHost Compiler Support Policy 1.3.1.\\nSupported C++ Dialects 1.4.\\nAbout This Document 2.\\nPre-installation Actions 2.1.\\nVerify You Have a CUDA-Capable GPU 2.2.\\nVerify You Have a Supported Version of Linux 2.3.\\nVerify the System Has gcc Installed 2.4.\\nVerify the System has the Correct Kernel Headers and Development Packages Installed 2.5.\\nInstall GPUDirect Storage 2.6.\\nChoose an Installation Method 2.7.\\nDownload the NVIDIA CUDA Toolkit 2.8.\\nAddress Custom xorg.conf, If Applicable 2.9.\\nHandle Conflicting Installation Methods 3.\\nPackage Manager Installation 3.1.\\nOverview 3.2.\\nRHEL 8 / Rocky 8 3.2.1.\\nPrepare RHEL 8 / Rocky 8 3.2.2.\\nLocal Repo Installation for RHEL 8 / Rocky 8 3.2.3.\\nNetwork Repo Installation for RHEL 8 / Rocky 8 3.2.4.\\nCommon Instructions for RHEL 8 / Rocky 8 3.3.\\nRHEL 9 / Rocky 9 3.3.1.\\nPrepare RHEL 9 / Rocky 9 3.3.2.\\nLocal Repo Installation for RHEL 9 / Rocky 9 3.3.3.\\nNetwork Repo Installation for RHEL 9 / Rocky 9 3.3.4.\\nCommon Instructions for RHEL 9 / Rocky 9 3.4.\\nKylinOS 10 3.4.1.\\nPrepare KylinOS 10 3.4.2.\\nLocal Repo Installation for KylinOS 3.4.3.\\nNetwork Repo Installation for KylinOS 3.4.4.\\nCommon Instructions for KylinOS 10 3.5.\\nFedora 3.5.1.\\nPrepare Fedora 3.5.2.\\nLocal Repo Installation for Fedora 3.5.3.\\nNetwork Repo Installation for Fedora 3.5.4.\\nCommon Installation Instructions for Fedora 3.6.\\nSLES 3.6.1.\\nPrepare SLES 3.6.2.\\nLocal Repo Installation for SLES 3.6.3.\\nNetwork Repo Installation for SLES 3.6.4.\\nCommon Installation Instructions for SLES 3.7.\\nOpenSUSE 3.7.1.\\nPrepare OpenSUSE 3.7.2.\\nLocal Repo Installation for OpenSUSE 3.7.3.\\nNetwork Repo Installation for OpenSUSE 3.7.4.\\nCommon Installation Instructions for OpenSUSE 3.8.\\nWSL 3.8.1.\\nPrepare WSL 3.8.2.\\nLocal Repo Installation for WSL 3.8.3.\\nNetwork Repo Installation for WSL 3.8.4.\\nCommon Installation Instructions for WSL 3.9.\\nUbuntu 3.9.1.\\nPrepare Ubuntu 3.9.2.\\nLocal Repo Installation for Ubuntu 3.9.3.\\nNetwork Repo Installation for Ubuntu 3.9.4.\\nCommon Installation Instructions for Ubuntu 3.10.\\nDebian 3.10.1.\\nPrepare Debian 3.10.2.\\nLocal Repo Installation for Debian 3.10.3.\\nNetwork Repo Installation for Debian 3.10.4.\\nCommon Installation Instructions for Debian 3.11.\\nAmazon Linux 2023 3.11.1.\\nPrepare Amazon Linux 2023 3.11.2.\\nLocal Repo Installation for Amazon Linux 3.11.3.\\nNetwork Repo Installation for Amazon Linux 3.11.4.\\nCommon Installation Instructions for Amazon Linux 3.12.\\nAdditional Package Manager Capabilities 3.12.1.\\nAvailable Packages 3.12.2.\\nMeta Packages 3.12.3.\\nOptional 32-bit Packages for Linux x86_64 .deb/.rpm 3.12.4.\\nPackage Upgrades 4.\\nDriver Installation 5.\\nNVIDIA Open GPU Kernel Modules 5.1.\\nCUDA Runfile 5.2.\\nDebian 5.3.\\nFedora 5.4.\\nKylinOS 10 5.5.\\nRHEL 9 and Rocky 9 5.6.\\nRHEL 8 and Rocky 8 5.7.\\nOpenSUSE and SLES 5.8.\\nUbuntu 6.\\nPrecompiled Streams 6.1.\\nPrecompiled Streams Support Matrix 6.2.\\nModularity Profiles 7.\\nKickstart Installation 7.1.\\nRHEL 8 / Rocky Linux 8 7.2.\\nRHEL 9 / Rocky Linux 9 8.\\nRunfile Installation 8.1.\\nRunfile Overview 8.2.\\nInstallation 8.3.\\nDisabling Nouveau 8.3.1.\\nFedora 8.3.2.\\nRHEL / Rocky and KylinOS 8.3.3.\\nOpenSUSE 8.3.4.\\nSLES 8.3.5.\\nWSL 8.3.6.\\nUbuntu 8.3.7.\\nDebian 8.4.\\nDevice Node Verification 8.5.\\nAdvanced Options 8.6.\\nUninstallation 9.\\nConda Installation 9.1.\\nConda Overview 9.2.\\nInstalling CUDA Using Conda 9.3.\\nUninstalling CUDA Using Conda 9.4.\\nInstalling Previous CUDA Releases 9.5.\\nUpgrading from cudatoolkit Package 10.\\nPip Wheels 11.\\nTarball and Zip Archive Deliverables 11.1.\\nParsing Redistrib JSON 11.2.\\nImporting Tarballs into CMake 11.3.\\nImporting Tarballs into Bazel 12.\\nCUDA Cross-Platform Environment 12.1.\\nCUDA Cross-Platform Installation 12.2.\\nCUDA Cross-Platform Samples 13.\\nPost-installation Actions 13.1.\\nMandatory Actions 13.1.1.\\nEnvironment Setup 13.2.\\nRecommended Actions 13.2.1.\\nInstall Persistence Daemon 13.2.2.\\nInstall Writable Samples 13.2.3.\\nVerify the Installation 13.2.3.1.\\nVerify the Driver Version 13.2.3.2.\\nRunning the Binaries 13.2.4.\\nInstall Nsight Eclipse Plugins 13.2.5.\\nLocal Repo Removal 13.3.\\nOptional Actions 13.3.1.\\nInstall Third-party Libraries 13.3.2.\\nInstall the Source Code for cuda-gdb 13.3.3.\\nSelect the Active Version of CUDA 14.\\nAdvanced Setup 15.\\nFrequently Asked Questions 15.1.\\nHow do I install the Toolkit in a different location?\\n15.2.\\nWhy do I see \\u201cnvcc: No such file or directory\\u201d when I try to build a CUDA application?\\n15.3.\\nWhy do I see \\u201cerror while loading shared libraries: : cannot open shared object file: No such file or directory\\u201d when I try to run a CUDA application that uses a CUDA library?\\n15.4.\\nWhy do I see multiple \\u201c404 Not Found\\u201d errors when updating my repository meta-data on Ubuntu?\\n15.5.\\nHow can I tell X to ignore a GPU for compute-only use?\\n15.6.\\nWhy doesn\\u2019t the cuda-repo package install the CUDA Toolkit and Drivers?\\n15.7.\\nHow do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04?\\n15.8.\\nWhat do I do if the display does not load, or CUDA does not work, after performing a system update?\\n15.9.\\nHow do I install a CUDA driver with a version less than 367 using a network repo?\\n15.10.\\nHow do I install an older CUDA version using a network repo?\\n15.11.\\nWhy does the installation on SUSE install the Mesa-dri-nouveau dependency?\\n15.12.\\nHow do I handle \\u201cErrors were encountered while processing: glx-diversions\\u201d?\\n16.\\nAdditional Considerations 17.\\nSwitching between Driver Module Flavors 18.\\nRemoving CUDA Toolkit and Driver 19.\\nNotices 19.1.\\nNotice 19.2.\\nOpenCL 19.3.\\nTrademarks 20.\\nCopyright Installation Guide for Linux \\u00bb 1.\\nIntroduction v12.5 | PDF | Archive NVIDIA CUDA Installation Guide for Linux The installation instructions for the CUDA Toolkit on Linux.\\nIntroduction \\uf0c1 CUDA \\u00ae is a parallel computing platform and programming model invented by NVIDIA \\u00ae .\\nIt enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).\\nCUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms.\\nWith CUDA C/C\\ufeff+\\ufeff+, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation.\\nSupport heterogeneous computation where applications use both the CPU and GPU.\\nSerial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU.\\nAs such, CUDA can be incrementally applied to existing applications.\\nThe CPU and GPU are treated as separate devices that have their own memory spaces.\\nThis configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources.\\nCUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads.\\nThese cores have shared resources including a register file and a shared memory.\\nThe on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus.\\nThis guide will show you how to install and check the correct operation of the CUDA development tools.\\n1.1.\\nSystem Requirements \\uf0c1 To use NVIDIA CUDA on your system, you will need the following installed: CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release.\\nThe following table lists the supported Linux distributions.\\nPlease review the footnotes associated with the table.\\nTable 1 Native Linux Distribution Support in CUDA 12.5 Update 1 \\uf0c1 Distribution Kernel 1 Default GCC GLIBC x86_64 RHEL 9.y (y =10.x >=11.x >=22.x 22.x 1.4.\\nAbout This Document \\uf0c1 This document is intended for readers familiar with the Linux environment and the compilation of C programs from the command line.\\nYou do not need previous experience with CUDA or experience with parallel computation.\\nNote: This guide covers installation only on systems with X Windows installed.\\nNote Many commands in this document might require superuser privileges.\\nOn most distributions of Linux, this will require you to log in as root.\\nFor systems that have enabled the sudo package, use the sudo prefix for all necessary commands.\\n2.\\nPre-installation Actions \\uf0c1 Some actions must be taken before the CUDA Toolkit and Driver can be installed on Linux: Verify the system has a CUDA-capable GPU.\\nVerify the system is running a supported version of Linux.\\nVerify the system has gcc installed.\\nVerify the system has the correct kernel headers and development packages installed.\\nDownload the NVIDIA CUDA Toolkit.\\nHandle conflicting installation methods.\\nNote You can override the install-time prerequisite checks by running the installer with the -override flag.\\nRemember that the prerequisites will still be required to use the NVIDIA CUDA Toolkit.\\n2.1.\\nVerify You Have a CUDA-Capable GPU \\uf0c1 To verify that your GPU is CUDA-capable, go to your distribution\\u2019s equivalent of System Properties, or, from the command line, enter: lspci | grep -i nvidia If you do not see any settings, update the PCI hardware database that Linux maintains by entering update-pciids (generally found in /sbin ) at the command line and rerun the previous lspci command.\\nIf your graphics card is from NVIDIA and it is listed in https://developer.nvidia.com/cuda-gpus , your GPU is CUDA-capable.\\nThe Release Notes for the CUDA Toolkit also contain a list of supported products.\\n2.2.\\nVerify You Have a Supported Version of Linux \\uf0c1 The CUDA Development Tools are only supported on some specific distributions of Linux.\\nThese are listed in the CUDA Toolkit release notes.\\nTo determine which distribution and release number you\\u2019re running, type the following at the command line: uname -m && cat /etc/*release You should see output similar to the following, modified for your particular system: x86_64 Red Hat Enterprise Linux Workstation release 6.0 (Santiago) The x86_64 line indicates you are running on a 64-bit system.\\nThe remainder gives information about your distribution.\\n2.3.\\nVerify the System Has gcc Installed \\uf0c1 The gcc compiler is required for development using the CUDA Toolkit.\\nIt is not required for running CUDA applications.\\nIt is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly.\\nTo verify the version of gcc installed on your system, type the following on the command line: gcc --version If an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web.\\n2.4.\\nVerify the System has the Correct Kernel Headers and Development Packages Installed \\uf0c1 The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt.\\nFor example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed.\\nWhile the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed.\\nHowever, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using.\\nTherefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version.\\nThe version of the kernel your system is running can be found by running the following command: uname -r This is the version of the kernel headers and development packages that must be installed prior to installing the CUDA Drivers.\\nThis command will be used multiple times below to specify the version of the packages to install.\\nNote that below are the common-case scenarios for kernel usage.\\nMore advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running.\\nNote If you perform a system update which changes the version of the Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed.\\nOtherwise, the CUDA Driver will fail to work with the new kernel.\\n2.5.\\nInstall GPUDirect Storage \\uf0c1 If you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package.\\nGDS packages can be installed using the CUDA packaging guide.\\nFollow the instructions in MLNX_OFED Requirements and Installation .\\nGDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode.\\nInstallation instructions for them differ slightly.\\nCompatibility mode is the only mode that is supported on certain distributions due to software dependency limitations.\\nFull GDS support is restricted to the following Linux distros: Ubuntu 20.04, Ubuntu 22.04 RHEL 8.3, RHEL 8.4, RHEL 9.0 Starting with CUDA toolkit 12.2.2, GDS kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the NVIDIA open kernel driver.\\nFollow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages.\\n2.6.\\nChoose an Installation Method \\uf0c1 The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages).\\nThe distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution\\u2019s native package management system.\\nThe distribution-specific packages interface with the distribution\\u2019s native package management system.\\nIt is recommended to use the distribution-specific packages, where possible.\\nNote For both native as well as cross development, the toolkit must be installed using the distribution-specific installer.\\nSee the CUDA Cross-Platform Installation section for more details.\\n2.7.\\nDownload the NVIDIA CUDA Toolkit \\uf0c1 The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads .\\nChoose the platform you are using and download the NVIDIA CUDA Toolkit.\\nThe CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.\\nDownload Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file.\\nIf either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.\\nTo calculate the MD5 checksum of the downloaded file, run the following: md5sum 2.8.\\nAddress Custom xorg.conf, If Applicable \\uf0c1 The driver relies on an automatically generated xorg.conf file at /etc/X11/xorg.conf .\\nIf a custom-built xorg.conf file is present, this functionality will be disabled and the driver may not work.\\nYou can try removing the existing xorg.conf file, or adding the contents of /etc/X11/xorg.conf.d/00-nvidia.conf to the xorg.conf file.\\nThe xorg.conf file will most likely need manual tweaking for systems with a non-trivial GPU configuration.\\n2.9.\\nHandle Conflicting Installation Methods \\uf0c1 Before installing CUDA, any previous installations that could conflict should be uninstalled.\\nThis will not affect systems which have not had CUDA installed previously, or systems where the installation method has been preserved (RPM/Deb vs. Runfile).\\nSee the following charts for specifics.\\nTable 3 CUDA Toolkit Installation Compatibility Matrix \\uf0c1 Installed Toolkit Version == X.Y Installed Toolkit Version != X.Y RPM/Deb run RPM/Deb run Installing Toolkit Version X.Y RPM/Deb No Action Uninstall Run No Action No Action run Uninstall RPM/Deb Uninstall Run No Action No Action Table 4 NVIDIA Driver Installation Compatibility Matrix \\uf0c1 Installed Driver Version == X.Y Installed Driver Version != X.Y RPM/Deb run RPM/Deb run Installing Driver Version X.Y RPM/Deb No Action Uninstall Run No Action Uninstall Run run Uninstall RPM/Deb No Action Uninstall RPM/Deb No Action Use the following command to uninstall a Toolkit runfile installation: sudo /usr/local/cuda-X.Y/bin/cuda-uninstaller Use the following command to uninstall a Driver runfile installation: sudo /usr/bin/nvidia-uninstall Use the following commands to uninstall an RPM/Deb installation: sudo dnf remove \",\n",
      "        0.19190137088298798\n",
      "    ],\n",
      "    [\n",
      "        \"NvidiaSearchInput.mount({\\\"apiUrl\\\": \\\"https://api-prod.nvidia.com/search/graphql\\\", \\\"destination\\\": \\\"search.html\\\", \\\"path\\\": \\\"/cuda/\\\", \\\"site\\\": \\\"https://docs.nvidia.com\\\"}); Release Notes CUDA Features Archive EULA Installation Guides Quick Start Guide Installation Guide Windows Installation Guide Linux Programming Guides Programming Guide Best Practices Guide Maxwell Compatibility Guide Pascal Compatibility Guide Volta Compatibility Guide Turing Compatibility Guide NVIDIA Ampere GPU Architecture Compatibility Guide Hopper Compatibility Guide Ada Compatibility Guide Maxwell Tuning Guide Pascal Tuning Guide Volta Tuning Guide Turing Tuning Guide NVIDIA Ampere GPU Architecture Tuning Guide Hopper Tuning Guide Ada Tuning Guide PTX ISA Video Decoder PTX Interoperability Inline PTX Assembly CUDA API References CUDA Runtime API CUDA Driver API CUDA Math API cuBLAS cuDLA API NVBLAS nvJPEG cuFFT CUB CUDA C++ Standard Library cuFile API Reference Guide cuRAND cuSPARSE NPP nvJitLink nvFatbin NVRTC (Runtime Compilation) Thrust cuSOLVER PTX Compiler API References PTX Compiler APIs Miscellaneous CUDA Demo Suite CUDA on WSL CUDA on EFLOW Multi-Instance GPU (MIG) CUDA Compatibility CUPTI Debugger API GPUDirect RDMA GPUDirect Storage vGPU Tools NVCC CUDA-GDB Compute Sanitizer Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Systems Nsight Compute Nsight Visual Studio Edition Profiler CUDA Binary Utilities White Papers Floating Point and IEEE 754 Incomplete-LU and Cholesky Preconditioned Iterative Methods Application Notes CUDA for Tegra Compiler SDK libNVVM API libdevice User\\u2019s Guide NVVM IR landing \\u00bb CUDA Toolkit Documentation 12.5 Update 1 CUDA Toolkit Archive - Send Feedback CUDA Toolkit Documentation 12.5 Update 1 \\uf0c1 Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA\\u00ae CUDA\\u00ae Toolkit provides a development environment for creating high performance GPU-accelerated applications.\\nWith the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\\nThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application.\\nUsing built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs.\\nRelease Notes The Release Notes for the CUDA Toolkit.\\nCUDA Features Archive The list of CUDA features by release.\\nEULA The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools.\\nIf you do not agree with the terms and conditions of the license agreement, then do not download or use the software.\\nInstallation Guides \\uf0c1 Quick Start Guide This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system.\\nInstallation Guide Windows This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems.\\nInstallation Guide Linux This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems.\\nProgramming Guides \\uf0c1 Programming Guide This guide provides a detailed discussion of the CUDA programming model and programming interface.\\nIt then describes the hardware implementation, and provides guidance on how to achieve maximum performance.\\nThe appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API.\\nBest Practices Guide This guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures.\\nThe intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit.\\nMaxwell Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Maxwell.\\nPascal Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Pascal.\\nVolta Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Volta.\\nTuring Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with Turing.\\nNVIDIA Ampere GPU Architecture Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture.\\nThis document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.\\nHopper Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs.\\nThis document provides guidance to ensure that your software applications are compatible with Hopper architecture.\\nAda Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs.\\nThis document provides guidance to ensure that your software applications are compatible with Ada architecture.\\nMaxwell Tuning Guide Maxwell is NVIDIA\\u2019s 4th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features.\\nPascal Tuning Guide Pascal is NVIDIA\\u2019s 5th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features.\\nVolta Tuning Guide Volta is NVIDIA\\u2019s 6th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features.\\nTuring Tuning Guide Turing is NVIDIA\\u2019s 7th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features.\\nNVIDIA Ampere GPU Architecture Tuning Guide NVIDIA Ampere GPU Architecture is NVIDIA\\u2019s 8th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture\\u2019s features.\\nHopper Tuning Guide Hopper GPU Architecture is NVIDIA\\u2019s 9th-generation architecture for CUDA compute applications.\\nApplications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes.\\nThis guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture\\u2019s features.\\nAda Tuning Guide The NVIDIA\\u00ae Ada GPU architecture is NVIDIA\\u2019s latest architecture for CUDA\\u00ae compute applications.\\nThe NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes.\\nThis guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture\\u2019s features.\\nPTX ISA This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).\\nPTX exposes the GPU as a data-parallel computing device.\\nVideo Decoder NVIDIA Video Decoder (NVCUVID) is deprecated.\\nInstead, use the NVIDIA Video Codec SDK ( https://developer.nvidia.com/nvidia-video-codec-sdk ).\\nPTX Interoperability This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code.\\nInline PTX Assembly This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code.\\nIt describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter.\\nCUDA API References \\uf0c1 CUDA Runtime API Fields in structures might appear in order that is different from the order of declaration.\\nCUDA Driver API Fields in structures might appear in order that is different from the order of declaration.\\nCUDA Math API The CUDA math API.\\ncuBLAS The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime.\\nIt allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs.\\ncuDLA API The cuDLA API.\\nNVBLAS The NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library.\\nnvJPEG The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications.\\ncuFFT The cuFFT library user guide.\\nCUB The user guide for CUB.\\nCUDA C++ Standard Library The API reference for libcu++, the CUDA C++ standard library.\\ncuFile API Reference Guide The NVIDIA\\u00ae GPUDirect\\u00ae Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology.\\ncuRAND The cuRAND library user guide.\\ncuSPARSE The cuSPARSE library user guide.\\nNPP NVIDIA NPP is a library of functions for performing CUDA accelerated processing.\\nThe initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas.\\nNPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains.\\nThe NPP library is written to maximize flexibility, while maintaining high performance.\\nnvJitLink The user guide for the nvJitLink library.\\nnvFatbin The user guide for the nvFatbin library.\\nNVRTC (Runtime Compilation) NVRTC is a runtime compilation library for CUDA C++.\\nIt accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX.\\nThe PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API.\\nThis facility can often provide optimizations and performance not possible in a purely offline static compilation.\\nThrust The C++ parallel algorithms library.\\ncuSOLVER The cuSOLVER library user guide.\\nPTX Compiler API References \\uf0c1 PTX Compiler APIs This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library.\\nMiscellaneous \\uf0c1 CUDA Demo Suite This document describes the demo applications shipped with the CUDA Demo Suite.\\nCUDA on WSL This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2).\\nThe guide covers installation and running CUDA applications and containers in this environment.\\nMulti-Instance GPU (MIG) This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA\\u00ae A100 GPU.\\nCUDA Compatibility This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade.\\nCUPTI The CUPTI-API.\\nThe CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications.\\nDebugger API The CUDA debugger API.\\nGPUDirect RDMA A technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express.\\nThis document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model.\\nGPUDirect Storage The documentation for GPUDirect Storage.\\nvGPU vGPUs that support CUDA.\\nTools \\uf0c1 NVCC This is a reference document for nvcc, the CUDA compiler driver.\\nnvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process.\\nCUDA-GDB The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware.\\nCUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger.\\nCompute Sanitizer The user guide for Compute Sanitizer.\\nNsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Eclipse Plugins Edition getting started guide Nsight Systems The documentation for Nsight Systems.\\nNsight Compute The NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications.\\nIt provides detailed performance metrics and API debugging via a user interface and command line tool.\\nNsight Visual Studio Edition The documentation for Nsight Visual Studio Edition.\\nProfiler This is the guide to the Profiler.\\nCUDA Binary Utilities The application notes for cuobjdump, nvdisasm, and nvprune.\\nWhite Papers \\uf0c1 Floating Point and IEEE 754 A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs.\\nThe purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide.\\nIncomplete-LU and Cholesky Preconditioned Iterative Methods In this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods.\\nWe focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively.\\nAlso, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms.\\nApplication Notes \\uf0c1 CUDA for Tegra This application note provides an overview of NVIDIA\\u00ae Tegra\\u00ae memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra\\u00ae integrated GPU (iGPU).\\nIt also discusses EGL interoperability.\\nCompiler SDK \\uf0c1 libNVVM API The libNVVM API.\\nlibdevice User\\u2019s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels.\\nNVVM IR NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR.\\nThe NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels).\\nHigh-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact \\u00a9 Copyright 2007-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(true); });1.\\nCUDA 12.5 Update 1 Release Notes 1.1.\\nCUDA Toolkit Major Component Versions 1.2.\\nNew Features 1.2.1.\\nGeneral CUDA 1.2.2.\\nCUDA Compiler 1.2.3.\\nCUDA Developer Tools 1.3.\\nResolved Issues 1.3.1.\\nCUDA Compiler 1.4.\\nKnown Issues and Limitations 1.5.\\nDeprecated or Dropped Features 1.5.1.\\nDeprecated or Dropped Architectures 1.5.2.\\nDeprecated Operating Systems 1.5.3.\\nDeprecated Toolchains 1.5.4.\\nCUDA Tools 2.\\nCUDA Libraries 2.1. cuBLAS Library 2.1.1. cuBLAS: Release 12.5 Update 1 2.1.2. cuBLAS: Release 12.5 2.1.3. cuBLAS: Release 12.4 Update 1 2.1.4. cuBLAS: Release 12.4 2.1.5. cuBLAS: Release 12.3 Update 1 2.1.6. cuBLAS: Release 12.3 2.1.7. cuBLAS: Release 12.2 Update 2 2.1.8. cuBLAS: Release 12.2 2.1.9. cuBLAS: Release 12.1 Update 1 2.1.10. cuBLAS: Release 12.0 Update 1 2.1.11. cuBLAS: Release 12.0 2.2. cuFFT Library 2.2.1. cuFFT: Release 12.5 2.2.2. cuFFT: Release 12.4 Update 1 2.2.3. cuFFT: Release 12.4 2.2.4. cuFFT: Release 12.3 Update 1 2.2.5. cuFFT: Release 12.3 2.2.6. cuFFT: Release 12.2 2.2.7. cuFFT: Release 12.1 Update 1 2.2.8. cuFFT: Release 12.1 2.2.9. cuFFT: Release 12.0 Update 1 2.2.10. cuFFT: Release 12.0 2.3. cuSOLVER Library 2.3.1. cuSOLVER: Release 12.5 Update 1 2.3.2. cuSOLVER: Release 12.5 2.3.3. cuSOLVER: Release 12.4 Update 1 2.3.4. cuSOLVER: Release 12.4 2.3.5. cuSOLVER: Release 12.2 Update 2 2.3.6. cuSOLVER: Release 12.2 2.4. cuSPARSE Library 2.4.1. cuSPARSE: Release 12.5 Update 1 2.4.2. cuSPARSE: Release 12.5 2.4.3. cuSPARSE: Release 12.4 2.4.4. cuSPARSE: Release 12.3 Update 1 2.4.5. cuSPARSE: Release 12.3 2.4.6. cuSPARSE: Release 12.2 Update 1 2.4.7. cuSPARSE: Release 12.1 Update 1 2.4.8. cuSPARSE: Release 12.0 Update 1 2.4.9. cuSPARSE: Release 12.0 2.5.\\nMath Library 2.5.1.\\nCUDA Math: Release 12.5 2.5.2.\\nCUDA Math: Release 12.4 2.5.3.\\nCUDA Math: Release 12.3 2.5.4.\\nCUDA Math: Release 12.2 2.5.5.\\nCUDA Math: Release 12.1 2.5.6.\\nCUDA Math: Release 12.0 2.6.\\nNVIDIA Performance Primitives (NPP) 2.6.1.\\nNPP: Release 12.4 2.6.2.\\nNPP: Release 12.0 2.7. nvJPEG Library 2.7.1. nvJPEG: Release 12.4 2.7.2. nvJPEG: Release 12.3 Update 1 2.7.3. nvJPEG: Release 12.2 2.7.4. nvJPEG: Release 12.0 3.\\nNotices 3.1.\\nNotice 3.2.\\nOpenCL 3.3.\\nTrademarks Release Notes \\u00bb 1.\\nCUDA 12.5 Update 1 Release Notes v12.5 | PDF | Archive NVIDIA CUDA Toolkit Release Notes The Release Notes for the CUDA Toolkit.\\nCUDA 12.5 Update 1 Release Notes \\uf0c1 The release notes for the NVIDIA\\u00ae CUDA\\u00ae Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html .\\nNote The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases.\\n1.1.\\nCUDA Toolkit Major Component Versions \\uf0c1 CUDA Components Starting with CUDA 11, the various components in the toolkit are versioned independently.\\nFor CUDA 12.5 Update 1, the table below indicates the versions: Table 1 CUDA 12.5 Update 1 Component Versions \\uf0c1 Component Name Version Information Supported Architectures Supported Platforms CUDA C++ Core Compute Libraries Thrust 2.4.0 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUB 2.4.0 libcu++ 2.4.0 Cooperative Groups 12.5.82 CUDA Compatibility 12.5.36505571 aarch64-jetson Linux CUDA Runtime (cudart) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuobjdump 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUPTI 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuxxfilt (demangler) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA Demo Suite 12.5.82 x86_64 Linux, Windows CUDA GDB 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, WSL CUDA Nsight Eclipse Plugin 12.5.82 x86_64 Linux CUDA NVCC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvdisasm 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA NVML Headers 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvprof 12.5.82 x86_64 Linux, Windows CUDA nvprune 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVRTC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL NVTX 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVVP 12.5.82 x86_64, Linux, Windows CUDA OpenCL 12.5.39 x86_64 Linux, Windows CUDA Profiler API 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA Compute Sanitizer API 12.5.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuBLAS 12.5.3.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuDLA 12.5.82 aarch64-jetson Linux CUDA cuFFT 11.2.3.61 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuFile 1.10.1.7 x86_64, arm64-sbsa, aarch64-jetson Linux CUDA cuRAND 10.3.6.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSOLVER 11.6.3.83 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSPARSE 12.5.1.3 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NPP 12.3.0.159 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvFatbin 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJitLink 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJPEG 12.3.2.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL Nsight Compute 2024.2.1.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL (Windows 11) Nsight Systems 2024.2.3.38 x86_64, arm64-sbsa, Linux, Windows, WSL Nsight Visual Studio Edition (VSE) 2024.2.1.24155 x86_64 (Windows) Windows nvidia_fs 1 2.20.6 x86_64, arm64-sbsa, aarch64-jetson Linux Visual Studio Integration 12.5.82 x86_64 (Windows) Windows NVIDIA Linux Driver 555.42.06 x86_64, arm64-sbsa Linux NVIDIA Windows Driver 555.85 x86_64 (Windows) Windows, WSL CUDA Driver Running a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit.\\nSee Table 3 .\\nFor more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus .\\nEach release of the CUDA Toolkit requires a minimum version of the CUDA driver.\\nThe CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases.\\nMore information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades .\\nNote : Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below.\\nThe minimum required driver version for CUDA minor version compatibility is shown below.\\nCUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility \\uf0c1 CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.x >=525.60.13 >=528.33 CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x >=450.80.02 >=452.39 CUDA 11.0 (11.0.3) >=450.36.06** >=451.22** * Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode \\u2013 please read the CUDA Compatibility Guide for details.\\n** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits.\\nThe version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below.\\nTable 3 CUDA Toolkit and Corresponding Driver Versions \\uf0c1 CUDA Toolkit Toolkit Driver Version Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.5 Update 1 >=555.42.06 >=555.85 CUDA 12.5 GA >=555.42.02 >=555.85 CUDA 12.4 Update 1 >=550.54.15 >=551.78 CUDA 12.4 GA >=550.54.14 >=551.61 CUDA 12.3 Update 1 >=545.23.08 >=546.12 CUDA 12.3 GA >=545.23.06 >=545.84 CUDA 12.2 Update 2 >=535.104.05 >=537.13 CUDA 12.2 Update 1 >=535.86.09 >=536.67 CUDA 12.2 GA >=535.54.03 >=536.25 CUDA 12.1 Update 1 >=530.30.02 >=531.14 CUDA 12.1 GA >=530.30.02 >=531.14 CUDA 12.0 Update 1 >=525.85.12 >=528.33 CUDA 12.0 GA >=525.60.13 >=527.41 CUDA 11.8 GA >=520.61.05 >=520.06 CUDA 11.7 Update 1 >=515.48.07 >=516.31 CUDA 11.7 GA >=515.43.04 >=516.01 CUDA 11.6 Update 2 >=510.47.03 >=511.65 CUDA 11.6 Update 1 >=510.47.03 >=511.65 CUDA 11.6 GA >=510.39.01 >=511.23 CUDA 11.5 Update 2 >=495.29.05 >=496.13 CUDA 11.5 Update 1 >=495.29.05 >=496.13 CUDA 11.5 GA >=495.29.05 >=496.04 CUDA 11.4 Update 4 >=470.82.01 >=472.50 CUDA 11.4 Update 3 >=470.82.01 >=472.50 CUDA 11.4 Update 2 >=470.57.02 >=471.41 CUDA 11.4 Update 1 >=470.57.02 >=471.41 CUDA 11.4.0 GA >=470.42.01 >=471.11 CUDA 11.3.1 Update 1 >=465.19.01 >=465.89 CUDA 11.3.0 GA >=465.19.01 >=465.89 CUDA 11.2.2 Update 2 >=460.32.03 >=461.33 CUDA 11.2.1 Update 1 >=460.32.03 >=461.09 CUDA 11.2.0 GA >=460.27.03 >=460.82 CUDA 11.1.1 Update 1 >=455.32 >=456.81 CUDA 11.1 GA >=455.23 >=456.38 CUDA 11.0.3 Update 1 >= 450.51.06 >= 451.82 CUDA 11.0.2 GA >= 450.51.05 >= 451.48 CUDA 11.0.1 RC >= 450.36.06 >= 451.22 CUDA 10.2.89 >= 440.33 >= 441.22 CUDA 10.1 (10.1.105 general release, and updates) >= 418.39 >= 418.96 CUDA 10.0.130 >= 410.48 >= 411.31 CUDA 9.2 (9.2.148 Update 1) >= 396.37 >= 398.26 CUDA 9.2 (9.2.88) >= 396.26 >= 397.44 CUDA 9.1 (9.1.85) >= 390.46 >= 391.29 CUDA 9.0 (9.0.76) >= 384.81 >= 385.54 CUDA 8.0 (8.0.61 GA2) >= 375.26 >= 376.51 CUDA 8.0 (8.0.44) >= 367.48 >= 369.30 CUDA 7.5 (7.5.16) >= 352.31 >= 353.66 CUDA 7.0 (7.0.28) >= 346.46 >= 347.62 For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation.\\nNote that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.\\nFor running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers .\\nDuring the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages).\\nFor more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software .\\nFor meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas .\\n1.2.\\nNew Features \\uf0c1 This section lists new general CUDA and CUDA compilers features.\\n1.2.1.\\nGeneral CUDA \\uf0c1 In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option.\\nEnd-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules.\\nMPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms.\\nMore details can be found here .\\n1.2.2.\\nCUDA Compiler \\uf0c1 For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5 .\\n1.2.3.\\nCUDA Developer Tools \\uf0c1 For changes to nvprof and Visual Profiler, see the changelog .\\nFor new features, improvements, and bug fixes in Nsight Systems, see the changelog .\\nFor new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog .\\nFor new features, improvements, and bug fixes in CUPTI, see the changelog .\\nFor new features, improvements, and bug fixes in Nsight Compute, see the changelog .\\nFor new features, improvements, and bug fixes in Compute Sanitizer, see the changelog .\\nFor new features, improvements, and bug fixes in CUDA-GDB, see the changelog .\\n1.3.\\nResolved Issues \\uf0c1 1.3.1.\\nCUDA Compiler \\uf0c1 Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device.\\nResolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0.\\nAlso added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler.\\nResolved a compiler issue that caused different results when compiling with the -G flag than without the flag.\\nFixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops.\\nResolved issues seen when compiling cuBLASDx device functions, in some conditions leading to \\u201cMisaligned shared or local address\\u201d.\\nFix to correct the calculation of write-after-read hazard latency.\\n1.4.\\nKnown Issues and Limitations \\uf0c1 Runfile will not be supported for Amazon Linux 2023.\\nConfidential Computing is not supported on CUDA 12.5.\\nPlease continue to use CUDA 12.4 and drivers r550.xx to use these features.\\nLaunching Cooperative Group kernels with MPS is not supported on Tegra platforms.\\n1.5.\\nDeprecated or Dropped Features \\uf0c1 Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release.\\nWe recommend that developers employ alternative solutions to these features in their software.\\n1.5.1.\\nDeprecated or Dropped Architectures \\uf0c1 NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5.\\n1.5.2.\\nDeprecated Operating Systems \\uf0c1 NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5.\\nCUDA 12.5 is the last release to support Debian 10.\\nSupport for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated.\\n1.5.3.\\nDeprecated Toolchains \\uf0c1 CUDA Toolkit 12.4 deprecated support for the following host compilers: Microsoft Visual C/C++ (MSVC) 2017 All GCC versions prior to GCC 7.3 1.5.4.\\nCUDA Tools \\uf0c1 Support for the macOS host client of CUDA-GDB is deprecated.\\nIt will be dropped in an upcoming release.\\n2.\\nCUDA Libraries \\uf0c1 This section covers CUDA Libraries release notes for 12.x releases.\\nCUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host.\\n2.1. cuBLAS Library \\uf0c1 2.1.1. cuBLAS: Release 12.5 Update 1 \\uf0c1 New Features Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs.\\nKnown Issues The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases.\\nA workaround is to implement batching manually.\\nThis will be fixed in a future release.\\ncublasGemmGroupedBatchedEx and cublasgemmGroupedBatched have large CPU overheads.\\nThis will be addressed in an upcoming release.\\nResolved Issues Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error.\\ncublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D).\\n2.1.2. cuBLAS: Release 12.5 \\uf0c1 New Features cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs.\\nThis enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type.\\nRefer to cublasGemmGroupedBatchedEx for more details.\\nKnown Issues cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types.\\nResolved Issues cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter.\\nFor instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results.\\ncuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv.\\n2.1.3. cuBLAS: Release 12.4 Update 1 \\uf0c1 Known Issues Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail.\\nThis will be fixed in an upcoming release.\\ncublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter.\\nFor example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results.\\nResolved Issues cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error.\\nIn particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 ( CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 ).\\nReduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul() , cublasLtMatmulAlgoCheck() , and cublasLtMatmulAlgoGetHeuristic() .\\nThe issue was introduced in CUDA Toolkit 12.4. cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG).\\nThe issue was introduced in cuBLAS 11.8.\\n2.1.4. cuBLAS: Release 12.4 \\uf0c1 New Features cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision.\\nSingle precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH .\\nGrouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta).\\nPlease see gemmGroupedBatched for more details.\\nKnown Issues When the current context has been created using cuGreenCtxCreate() , cuBLAS does not properly detect the number of SMs available.\\nThe user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget() .\\nBLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE .\\nThis is the same known issue documented in cuBLAS 12.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace.\\nThe issue exists since cuBLAS 11.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided.\\nThe issue exists since cuBLAS 11.6.\\nWhen captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync .\\nHowever, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail.\\nTo avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory.\\n2.1.5. cuBLAS: Release 12.3 Update 1 \\uf0c1 New Features Improved performance of heuristics cache for workloads that have a high eviction rate.\\nKnown Issues BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE .\\nThe expected behavior is that the corresponding computations would be skipped.\\nYou may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped.\\nIf strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST .\\nResolved Issues cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.\\nWhen an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute() .\\nFixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS).\\ncublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient.\\n2.1.6. cuBLAS: Release 12.3 \\uf0c1 New Features Improved performance on NVIDIA L40S Ada GPUs.\\nKnown Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function.\\nWhen an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute() .\\nTo workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit() .\\n2.1.7. cuBLAS: Release 12.2 Update 2 \\uf0c1 New Features cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel.\\nIt does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times.\\nThis improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable.\\n2.1.8. cuBLAS: Release 12.2 \\uf0c1 Known Issues cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%.\\nThere is currently no workaround for this issue.\\nSome Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE .\\nThe kernels apply the first batch\\u2019s bias vector to all batches.\\n2.1.9. cuBLAS: Release 12.1 Update 1 \\uf0c1 New Features Support for FP8 on NVIDIA Ada GPUs.\\nImproved performance on NVIDIA L4 Ada GPUs.\\nIntroduced an API that instructs the cuBLASLt library to not use some CPU instructions.\\nThis is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance.\\nRefer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions .\\nKnown Issues When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure).\\nAs a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses.\\nIf one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function.\\nThe same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t .\\nThe issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes.\\n2.1.10. cuBLAS: Release 12.0 Update 1 \\uf0c1 New Features Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs.\\nKnown Issues For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB).\\nIn the current and previous releases, cuBLAS allocates 256 MiB.\\nThis will be addressed in a future release.\\nA possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture.\\nResolved Issues Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache.\\nThis began in the CUDA Toolkit 12.0 release.\\nAdded forward compatible single precision complex GEMM that does not require workspace.\\n2.1.11. cuBLAS: Release 12.0 \\uf0c1 New Features cublasLtMatmul now supports FP8 with a non-zero beta.\\nAdded int64 APIs to enable larger problem sizes; refer to 64-bit integer interface .\\nAdded more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux.\\nKnown Issues There are no forward compatible kernels for single precision complex gemms that do not require workspace.\\nSupport will be added in a later release.\\nResolved Issues Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE ) could return incorrect results for the bias gradient.\\ncublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues.\\nDeprecations Disallow including cublas.h and cublas_v2.h in the same translation unit.\\nRemoved: CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t .\\nNo kernels utilize these stages anymore.\\ncublasLt3mMode_t , CUBLASLT_MATMUL_PREF_MATH_MODE_MASK , and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t .\\nInstead, use the corresponding flags from cublasLtNumericalImplFlags_t .\\nCUBLASLT_MATMUL_PREF_POINTER_MODE_MASK , CUBLASLT_MATMUL_PREF_EPILOGUE_MASK , and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t .\\nThe corresponding parameters are taken directly from cublasLtMatmulDesc_t .\\nCUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t .\\nThis mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed.\\n2.2. cuFFT Library \\uf0c1 2.2.1. cuFFT: Release 12.5 \\uf0c1 New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes .\\nWe recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance.\\nYou can enable JIT LTO kernels using the per-plan properties cuFFT API.\\n2.2.2. cuFFT: Release 12.4 Update 1 \\uf0c1 Resolved Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ) in CUDA 12.4.\\nThis routine has now been removed from the header.\\n2.2.3. cuFFT: Release 12.4 \\uf0c1 New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing.\\nAdded per-plan properties to the cuFFT API.\\nThese new routines can be leveraged to give users more control over the behavior of cuFFT.\\nCurrently they can be used to enable JIT LTO kernels for 64-bit FFTs.\\nImproved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes.\\nKnown Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ).\\nThis routine is not supported by cuFFT, and will be removed from the header in a future release.\\nResolved Issues Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e.\\nusing the ostride component of the Advanced Data Layout API ).\\nFixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL .\\nFrom now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension.\\n2.2.4. cuFFT: Release 12.3 Update 1 \\uf0c1 Known Issues Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior.\\nThis issue will be fixed in an upcoming release of cuFFT.\\nResolved Issues Complex-to-complex (C2C) execution functions ( cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context.\\n2.2.5. cuFFT: Release 12.3 \\uf0c1 New Features Callback kernels are more relaxed in terms of resource usage, and will use fewer registers.\\nImproved accuracy for double precision prime and composite FFT sizes with factors larger than 127.\\nSlightly improved planning times for some FFT sizes.\\n2.2.6. cuFFT: Release 12.2 \\uf0c1 New Features cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs .\\nImproved performance of 1000+ of FFTs of sizes ranging from 62 to 16380.\\nThe improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT.\\nReduced the size of the static libraries when compared to cuFFT in the 12.1 release.\\nResolved Issues cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive.\\ncuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently.\\n2.2.7. cuFFT: Release 12.1 Update 1 \\uf0c1 Known Issues cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy ) and another thread calls any API (except cufftCreate or cufftDestroy ), and when the total number of plans alive exceeds 1023. cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans.\\n2.2.8. cuFFT: Release 12.1 \\uf0c1 New Features Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800.\\nThe improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout.\\nKnown Issues Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms.\\nAn upcoming release will update the cuFFT callback implementation, removing this limitation.\\ncuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4.\\nResolved Issues cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit.\\n2.2.9. cuFFT: Release 12.0 Update 1 \\uf0c1 Resolved Issues Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced.\\n2.2.10. cuFFT: Release 12.0 \\uf0c1 New Features PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures.\\nKnown Issues cuFFT plan generation time increases due to PTX JIT compiling.\\nRefer to Plan Initialization TIme .\\nResolved Issues cuFFT plans had an unintentional small memory overhead (of a few kB) per plan.\\nThis is resolved.\\n2.3. cuSOLVER Library \\uf0c1 2.3.1. cuSOLVER: Release 12.5 Update 1 \\uf0c1 Resolved Issues The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved.\\n2.3.2. cuSOLVER: Release 12.5 \\uf0c1 New Features Performance improvements of cusolverDnXgesvd and cusolverDngesvd if jobu != 'N' or jobvt != 'N' .\\nPerformance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR .\\nLower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices.\\nKnown Issues With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice .\\nAs a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)) , with auto ALIGN_32 = []( int64_t val ) { return (( val + 31 ) / 32 ) * 32 ; }; and auto sizeofCudaDataType = []( cudaDataType dt ) { if ( dt == CUDA_R_32F ) return sizeof ( float ); if ( dt == CUDA_R_64F ) return sizeof ( double ); if ( dt == CUDA_C_32F ) return sizeof ( cuComplex ); if ( dt == CUDA_C_64F ) return sizeof ( cuDoubleComplex ); }; 2.3.3. cuSOLVER: Release 12.4 Update 1 \\uf0c1 New Features The performance of cusolverDnXlarft has been improved.\\nFor large matrices, the speedup might exceed 100x.\\nThe performance on H100 is now consistently better than on A100.\\nThe change in cusolverDnXlarft also results in a modest speedup in cusolverDnormqr , cusolverDnormtr , and cusolverDnXsyevd .\\nThe performance of cusolverDnXgesvd when singular vectors are sought has been improved.\\nThe job configuration that computes both left and right singular vectors is up to 1.5x faster.\\nResolved Issues cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes.\\nDeprecations Using long-deprecated cusolverDnPotrf , cusolverDnPotrs , cusolverDnGeqrf , cusolverDnGetrf , cusolverDnGetrs , cusolverDnSyevd , cusolverDnSyevdx , cusolverDnGesvd , and their accompanying bufferSize functions will result in a deprecation warning.\\nThe warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf , cusolverDnXpotrs , cusolverDnXgeqrf , cusolverDnXgetrf , cusolverDnXgetrs , cusolverDnXsyevd , cusolverDnXsyevdx , cusolverDnXgesvd , and the corresponding bufferSize functions instead.\\n2.3.4. cuSOLVER: Release 12.4 \\uf0c1 New Features cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced.\\ncusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes.\\nKnown Issues cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size.\\nAs a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size.\\n2.3.5. cuSOLVER: Release 12.2 Update 2 \\uf0c1 Resolved Issues Fixed an issue with cusolverDngesvd() , cusolverDnGesvd() , and cusolverDnXgesvd() , which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to \\u2018 N \\u2019.\\n2.3.6. cuSOLVER: Release 12.2 \\uf0c1 New Features A new API to ensure deterministic results or allow non-deterministic results for improved performance.\\nSee cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode() .\\nAffected functions are: cusolverDngeqrf() , cusolverDnsyevd() , cusolverDnsyevdx() , cusolverDngesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvdr() , and cusolverDnXgesvdp() .\\nKnown Issues Concurrent executions of cusolverDngetrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock.\\n2.4. cuSPARSE Library \\uf0c1 2.4.1. cuSPARSE: Release 12.5 Update 1 \\uf0c1 New Features Added support for BSR format in cusparseSpMM .\\nResolved Issues cusparseSpMM() would sometimes get incorrect results when alpha=0 , num_batches>1 , batch_stride indicates that there is padding between batches.\\ncusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1).\\ncusparseSpMM returned the wrong result when k=0 (for example when A has zero columns).\\nThe correct behavior is doing C \\\\*= beta .\\nThe bug behavior was not modifying C at all.\\ncusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows.\\nSliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices.\\nSliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes.\\n2.4.2. cuSPARSE: Release 12.5 \\uf0c1 New Features Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector.\\nResolved Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.\\n2.4.3. cuSPARSE: Release 12.4 \\uf0c1 New Features Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess() .\\nAdded support for mixed real and complex types for cusparseSpMM() .\\nAdded a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM() .\\nKnown Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes.\\nResolved Issues cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros.\\n2.4.4. cuSPARSE: Release 12.3 Update 1 \\uf0c1 New Features Added support for block sizes of 64 and 128 in cusparseSDDMM() .\\nAdded a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage.\\n2.4.5. cuSPARSE: Release 12.3 \\uf0c1 New Features The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector.\\nThe cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values.\\nKnown Issues The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous.\\nWrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A.\\nResolved Issues cusparseSpSV() provided indeterministic results in some cases.\\nFixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment.\\nFixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN.\\n2.4.6. cuSPARSE: Release 12.2 Update 1 \\uf0c1 New Features The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes.\\nSee logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api .\\nResolved Issues Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process.\\nClarified the supported operations for cusparseSDDMM() .\\ncusparseCreateConstSlicedEll() now uses const pointers.\\nFixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing.\\ncusparseSpSM_bufferSize() could ask slightly less memory than needed.\\ncusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed.\\nDeprecations Several legacy APIs have been officially deprecated.\\nA compile-time warning has been added to all of them.\\n2.4.7. cuSPARSE: Release 12.1 Update 1 \\uf0c1 New Features Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine ( cusparseSDDMM ).\\nIntroduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication ( cusparseSpMV ) and triangular solver with a single right-hand side ( cusparseSpSV ).\\nAdded a new API call ( cusparseSpSV_updateMatrix ) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step.\\n2.4.8. cuSPARSE: Release 12.0 Update 1 \\uf0c1 New Features cusparseSDDMM() now supports mixed precision computation.\\nImproved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs.\\nImproved cusparseSpMV() performance with a new load balancing algorithm.\\ncusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address.\\nResolved Issues cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows.\\n2.4.9. cuSPARSE: Release 12.0 \\uf0c1 New Features JIT LTO functionalities ( cusparseSpMMOp() ) switched from driver to nvJitLto library.\\nStarting from CUDA 12.0 the user needs to link to libnvJitLto.so , see cuSPARSE documentation .\\nJIT LTO performance has also been improved for cusparseSpMMOpPlan() .\\nIntroduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet() .\\nNow the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions.\\nAdded two new algorithms to cusparseSpGEMM() with lower memory utilization.\\nThe first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks.\\nAdded int8_t support to cusparseGather() , cusparseScatter() , and cusparseCsr2cscEx2() .\\nImproved cusparseSpSV() performance for both the analysis and the solving phases.\\nImproved cusparseSpSM() performance for both the analysis and the solving phases.\\nImproved cusparseSDDMM() performance and added support for batch computation.\\nImproved cusparseCsr2cscEx2() performance.\\nResolved Issues cusparseSpSV() and cusparseSpSM() could produce wrong results.\\ncusparseDnMatGetStridedBatch() did not accept batchStride == 0 .\\nDeprecations Removed deprecated CUDA 11.x APIs, enumerators, and descriptors.\\n2.5.\\nMath Library \\uf0c1 2.5.1.\\nCUDA Math: Release 12.5 \\uf0c1 Known Issues As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss.\\nNew interval shall read (-23.0001; -2.2637).\\nThis finding is applicable to CUDA 12.5 and all previous versions.\\n2.5.2.\\nCUDA Math: Release 12.4 \\uf0c1 Resolved Issues Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules.\\n2.5.3.\\nCUDA Math: Release 12.3 \\uf0c1 New Features Performance of SIMD Integer CUDA Math APIs was improved.\\nResolved Issues The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3.\\nKnown Issues Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g.\\npass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half , __half2 , __nv_bfloat16 , __nv_bfloat162 types implementations and expose the user program to undefined behavior.\\nNote, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing .\\nThis behavior may improve in future versions of the headers.\\n2.5.4.\\nCUDA Math: Release 12.2 \\uf0c1 New Features CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side support for many of the arithmetic operations and conversions.\\n__half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default.\\nThese may cause build issues due to ambiguous overloads resolution.\\nUsers are advised to update their code to select proper overloads.\\nTo opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ Resolved Issues During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity.\\nNVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer.\\nThe affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation.\\nAs JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT.\\nThis is a software algorithm fix and is not tied to specific hardware.\\nUpdated the observed worst case error bounds for single precision intrinsic functions __expf() , __exp10f() and double precision functions asinh() , acosh() .\\n2.5.5.\\nCUDA Math: Release 12.1 \\uf0c1 New Features Performance and accuracy improvements in atanf , acosf , asinf , sinpif , cospif , powf , erff , and tgammaf .\\n2.5.6.\\nCUDA Math: Release 12.0 \\uf0c1 New Features Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions.\\nRefer to https://docs.nvidia.com/cuda/cuda-math-api/index.html .\\nKnown Issues Double precision inputs that cause the double precision division algorithm in the default \\u2018round to nearest even mode\\u2019 produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected.\\nAffected CUDA Math APIs: __ddiv_rn() .\\nAffected CUDA language operation: double precision / operation in the device code.\\nDeprecations All previously deprecated undocumented APIs are removed from CUDA 12.0.\\n2.6.\\nNVIDIA Performance Primitives (NPP) \\uf0c1 2.6.1.\\nNPP: Release 12.4 \\uf0c1 New Features Enhanced large file support with size_t .\\n2.6.2.\\nNPP: Release 12.0 \\uf0c1 Deprecations Deprecating non-CTX API support from next release.\\nResolved Issues A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance.\\n2.7. nvJPEG Library \\uf0c1 2.7.1. nvJPEG: Release 12.4 \\uf0c1 New Features IDCT performance optimizations for single image CUDA decode.\\nZero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE .\\n2.7.2. nvJPEG: Release 12.3 Update 1 \\uf0c1 New Features New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them.\\n2.7.3. nvJPEG: Release 12.2 \\uf0c1 New Features Added support for JPEG Lossless decode (process 14, FO prediction).\\nnvJPEG is now supported on L4T.\\n2.7.4. nvJPEG: Release 12.0 \\uf0c1 New Features Immproved the GPU Memory optimisation for the nvJPEG codec.\\nResolved Issues An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved.\\nAn issue with CMYK four component color conversion is now resolved.\\nKnown Issues Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths.\\nDeprecations The reuse of Huffman table in Encoder ( nvjpegEncoderParamsCopyHuffmanTables ).\\n1 Only available on select Linux distros 3.\\nNotices \\uf0c1 3.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (\\u201cNVIDIA\\u201d) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (\\u201cTerms of Sale\\u201d).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer\\u2019s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer\\u2019s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer\\u2019s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, \\u201cMATERIALS\\u201d) ARE BEING PROVIDED \\u201cAS IS.\\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA\\u2019s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n3.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 3.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright \\u00a9 2007-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \\\"undefined\\\"){_satellite.pageBottom();}1.\\nCUDA 11.6 Features 1.1.\\nCompiler 1.1.1.\\nVS2022 Support 1.1.2.\\nNew instructions in public PTX 1.1.3.\\nUnused Kernel Optimization 1.1.4.\\nNew -arch=native option 1.1.5.\\nGenerate PTX from nvlink: 1.1.6.\\nBullseye support 1.1.7.\\nINT128 developer tool support 2.\\nNotices 2.1.\\nNotice 2.2.\\nOpenCL 2.3.\\nTrademarks CUDA Features Archive \\u00bb 1.\\nCUDA 11.6 Features v12.5 | PDF | Archive NVIDIA CUDA Features Archive The list of CUDA features by release.\\nCUDA 11.6 Features \\uf0c1 1.1.\\nCompiler \\uf0c1 1.1.1.\\nVS2022 Support \\uf0c1 CUDA 11.6 officially supports the latest VS2022 as host compiler.\\nA separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here .\\nA future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it.\\n1.1.2.\\nNew instructions in public PTX \\uf0c1 New instructions for bit mask creation\\u2014BMSK, and sign extension\\u2014SZEXT, are added to the public PTX ISA.\\nYou can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT .\\n1.1.3.\\nUnused Kernel Optimization \\uf0c1 In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations.\\nThis was an opt-in feature but in 11.6, this feature is enabled by default.\\nAs mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations.\\n$ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info 1.1.4.\\nNew -arch=native option \\uf0c1 In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1.\\nThis -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system.\\nThis can be particularly helpful for testing when applications are run on the same system they are compiled in.\\n1.1.5.\\nGenerate PTX from nvlink: \\uf0c1 Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN: nvcc -dlto -dlink -ptx Device linking by nvlink is the final stage in the CUDA compilation process.\\nApplications that have multiple source translation units have to be compiled in separate compilation mode.\\nLTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit.\\nHowever, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file.\\nWith the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization.\\n1.1.6.\\nBullseye support \\uf0c1 NVCC compiled source code now works with the code coverage tool Bullseye.\\nThe code coverage is only for the CPU or the host functions.\\nCode coverage for device function is not supported through bullseye.\\n1.1.7.\\nINT128 developer tool support \\uf0c1 In 11.5, CUDA C++ support for 128 bit was added.\\nIn 11.6, developer tools support the datatype as well.\\nWith the latest version of libcu++, int 128 data datype is supported by math functions.\\n2.\\nNotices \\uf0c1 2.1.\\nNotice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product.\\nNVIDIA Corporation (\\u201cNVIDIA\\u201d) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.\\nNVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use.\\nThis document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.\\nNVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.\\nCustomer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.\\nNVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (\\u201cTerms of Sale\\u201d).\\nNVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document.\\nNo contractual obligations are formed either directly or indirectly by this document.\\nNVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage.\\nNVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer\\u2019s own risk.\\nNVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use.\\nTesting of all parameters of each product is not necessarily performed by NVIDIA.\\nIt is customer\\u2019s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product.\\nWeaknesses in customer\\u2019s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document.\\nNVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.\\nNo license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document.\\nInformation published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof.\\nUse of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.\\nReproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.\\nTHIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, \\u201cMATERIALS\\u201d) ARE BEING PROVIDED \\u201cAS IS.\\u201d NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.\\nTO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\nNotwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA\\u2019s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.\\n2.2.\\nOpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. 2.3.\\nTrademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\nOther company and product names may be trademarks of the respective companies with which they are associated.\\nPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright \\u00a9 2007-2024, NVIDIA Corporation & affiliates.\\nAll rights reserved.\\nLast updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \\\"undefined\\\"){_satellite.pageBottom();}1.\\nLicense Agreement for NVIDIA Software Development Kits 1.1.\\nLicense 1.1.1.\\nLicense Grant 1.1.2.\\nDistribution Requirements 1.1.3.\\nAuthorized Users 1.1.4.\\nPre-Release SDK 1.1.5.\\nUpdates 1.1.6.\\nComponents Under Other Licenses 1.1.7.\\nReservation of Rights 1.2.\\nLimitations 1.3.\\nOwnership 1.4.\\nNo Warranties 1.5.\\nLimitation of Liability 1.6.\\nTermination 1.7.\\nGeneral 2.\\nCUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits 2.1.\\nLicense Scope 2.2.\\nDistribution 2.3.\\nOperating Systems 2.4.\\nAudio and Video Encoders and Decoders 2.5.\\nLicensing 2.6.\\nAttachment A 2.7.\\nAttachment B EULA \\u00bb 1.\\nLicense Agreement for NVIDIA Software Development Kits v12.5 | PDF | Archive End User License Agreement NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement.\\nLast updated: October 8, 2021 The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools.\\nIf you do not agree with the terms and conditions of the license agreement, then do not download or use the software.\\nLast updated: October 8, 2021.\\nPreface The Software License Agreement in Chapter 1 and the Supplement in Chapter 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit.\\nBy accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein.\\nNVIDIA Driver Description This package contains the operating system driver and fundamental system software components for NVIDIA GPUs.\\nNVIDIA CUDA Toolkit Description The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references.\\nDefault Install Location of CUDA Toolkit Windows platform: %ProgramFiles%\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v#.\",\n",
      "        0.1598527431488037\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "\"\"\"\n",
    "USER_PROMPT = f\"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_question= f\"You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided. Use the following pieces of information: {context} /n and answer the following question {question} \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised InvalidArgument: 400 Request payload size exceeds the limit: 50000 bytes..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised InvalidArgument: 400 Request payload size exceeds the limit: 50000 bytes..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised InvalidArgument: 400 Request payload size exceeds the limit: 50000 bytes..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised InvalidArgument: 400 Request payload size exceeds the limit: 50000 bytes..\n",
      "Retrying langchain_google_genai.llms._completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised InvalidArgument: 400 Request payload size exceeds the limit: 50000 bytes..\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 Request payload size exceeds the limit: 50000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_question\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:346\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    343\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    344\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 346\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    358\u001b[0m     )\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:703\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    697\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    701\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    702\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:882\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    868\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    869\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    870\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    881\u001b[0m     ]\n\u001b[0;32m--> 882\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:740\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    739\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    741\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:727\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    719\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 727\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    736\u001b[0m         )\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_google_genai/llms.py:303\u001b[0m, in \u001b[0;36mGoogleGenerativeAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mc) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates])\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     prompt_generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcandidates:\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_google_genai/llms.py:96\u001b[0m, in \u001b[0;36m_completion_with_retry\u001b[0;34m(llm, prompt, is_gemini, stream, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mmessage:\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_gemini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/tenacity/__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    416\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/tenacity/__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/langchain_google_genai/llms.py:91\u001b[0m, in \u001b[0;36m_completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(prompt, is_gemini, stream, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_gemini:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m     85\u001b[0m             contents\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     86\u001b[0m             stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m             request_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm\u001b[38;5;241m.\u001b[39mtimeout} \u001b[38;5;28;01mif\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m         )\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mFailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mmessage:\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/generativeai/text.py:205\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, prompt, temperature, candidate_count, max_output_tokens, top_p, top_k, safety_settings, stop_sequences, client, request_options)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the API to generate text based on the provided prompt.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    A `types.Completion` containing the model's text completion response.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m request \u001b[38;5;241m=\u001b[39m _make_generate_text_request(\n\u001b[1;32m    194\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    195\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     stop_sequences\u001b[38;5;241m=\u001b[39mstop_sequences,\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_generate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/generativeai/text.py:243\u001b[0m, in \u001b[0;36m_generate_response\u001b[0;34m(request, client, request_options)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_default_text_client()\n\u001b[0;32m--> 243\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;241m.\u001b[39mto_dict(response)\n\u001b[1;32m    246\u001b[0m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m palm_safety_types\u001b[38;5;241m.\u001b[39mconvert_filters_to_enums(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py:880\u001b[0m, in \u001b[0;36mTextServiceClient.generate_text\u001b[0;34m(self, request, model, prompt, temperature, candidate_count, max_output_tokens, top_p, top_k, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Steps/.venv/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 Request payload size exceeds the limit: 50000 bytes."
     ]
    }
   ],
   "source": [
    "llm.invoke(final_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nomic\n",
      "  Downloading nomic-3.0.38.tar.gz (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in ./.venv/lib/python3.12/site-packages (from nomic) (8.1.7)\n",
      "Collecting jsonlines (from nomic)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting loguru (from nomic)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from nomic) (13.7.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from nomic) (2.32.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from nomic) (1.26.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from nomic) (2.2.2)\n",
      "Requirement already satisfied: pydantic in ./.venv/lib/python3.12/site-packages (from nomic) (2.8.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from nomic) (4.66.4)\n",
      "Collecting pyarrow (from nomic)\n",
      "  Downloading pyarrow-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pillow (from nomic)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pyjwt in ./.venv/lib/python3.12/site-packages (from nomic) (2.8.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./.venv/lib/python3.12/site-packages (from jsonlines->nomic) (23.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->nomic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->nomic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->nomic) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic->nomic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.12/site-packages (from pydantic->nomic) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.12/site-packages (from pydantic->nomic) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->nomic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->nomic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->nomic) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->nomic) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->nomic) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->nomic) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->nomic) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->nomic) (1.16.0)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: nomic\n",
      "  Building wheel for nomic (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nomic: filename=nomic-3.0.38-py3-none-any.whl size=45883 sha256=dcf0fe38dd6775a8dda9ced7b1c7d8dc5ce5f0e87ee2446e7c14e0c2629206af\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/e4/1e/af/a2b039833ae477c077c999867e217837444c9bc7565c2c6412\n",
      "Successfully built nomic\n",
      "Installing collected packages: pyarrow, pillow, loguru, jsonlines, nomic\n",
      "Successfully installed jsonlines-4.0.0 loguru-0.7.2 nomic-3.0.38 pillow-10.4.0 pyarrow-16.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embed4All' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEmbed4All\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnomic-embed-text-v1.f16.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Embed4All' is not defined"
     ]
    }
   ],
   "source": [
    "Embed4All(\"nomic-embed-text-v1.f16.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "text() missing 1 required positional argument: 'texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnomic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embed\n\u001b[0;32m----> 2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of embeddings created:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(embeddings))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of dimensions per embedding:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: text() missing 1 required positional argument: 'texts'"
     ]
    }
   ],
   "source": [
    "from nomic import embed\n",
    "embeddings = embed.text([\"String 1\", \"String 2\"], inference_mode=\"local\")['embeddings']\n",
    "print(\"Number of embeddings created:\", len(embeddings))\n",
    "print(\"Number of dimensions per embedding:\", len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt4all_embd \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4AllEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnomic-embed-text-v1.f16.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "gpt4all_embd = GPT4AllEmbeddings(\"nomic-embed-text-v1.f16.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_ai21\n",
      "  Downloading langchain_ai21-0.1.7-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting ai21<3.0.0,>=2.7.0 (from langchain_ai21)\n",
      "  Downloading ai21-2.9.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.4 in ./.venv/lib/python3.12/site-packages (from langchain_ai21) (0.2.12)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.12/site-packages (from langchain_ai21) (0.2.2)\n",
      "Collecting ai21-tokenizer<1.0.0,>=0.11.0 (from ai21<3.0.0,>=2.7.0->langchain_ai21)\n",
      "  Downloading ai21_tokenizer-0.11.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.3 in ./.venv/lib/python3.12/site-packages (from ai21<3.0.0,>=2.7.0->langchain_ai21) (0.6.7)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./.venv/lib/python3.12/site-packages (from ai21<3.0.0,>=2.7.0->langchain_ai21) (0.27.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.3.0 in ./.venv/lib/python3.12/site-packages (from ai21<3.0.0,>=2.7.0->langchain_ai21) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.9.0 in ./.venv/lib/python3.12/site-packages (from ai21<3.0.0,>=2.7.0->langchain_ai21) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.4->langchain_ai21) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.4->langchain_ai21) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.4->langchain_ai21) (0.1.84)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.4->langchain_ai21) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.4->langchain_ai21) (2.8.2)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in ./.venv/lib/python3.12/site-packages (from ai21-tokenizer<1.0.0,>=0.11.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (4.4.0)\n",
      "Collecting sentencepiece<1.0.0,>=0.2.0 (from ai21-tokenizer<1.0.0,>=0.11.0->ai21<3.0.0,>=2.7.0->langchain_ai21)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.15.0 in ./.venv/lib/python3.12/site-packages (from ai21-tokenizer<1.0.0,>=0.11.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (0.19.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.3->ai21<3.0.0,>=2.7.0->langchain_ai21) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.3->ai21<3.0.0,>=2.7.0->langchain_ai21) (0.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (1.0.5)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (3.7)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from httpx<0.28.0,>=0.27.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.4->langchain_ai21) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.4->langchain_ai21) (3.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.4->langchain_ai21) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.4->langchain_ai21) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.4->langchain_ai21) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.4->langchain_ai21) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.4->langchain_ai21) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (0.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.3->ai21<3.0.0,>=2.7.0->langchain_ai21) (1.0.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (2024.6.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1.0.0,>=0.15.0->ai21-tokenizer<1.0.0,>=0.11.0->ai21<3.0.0,>=2.7.0->langchain_ai21) (4.66.4)\n",
      "Downloading langchain_ai21-0.1.7-py3-none-any.whl (15 kB)\n",
      "Downloading ai21-2.9.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ai21_tokenizer-0.11.2-py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, ai21-tokenizer, ai21, langchain_ai21\n",
      "Successfully installed ai21-2.9.1 ai21-tokenizer-0.11.2 langchain_ai21-0.1.7 sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ai21 import AI21Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key=os.getenv('AI21_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uTEjP5rAMBfnspLNRmSxQ54ztsZbL5af'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AI21Embeddings()\n",
    "query_result = embeddings.embed_query(\"Hello embeddings world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
