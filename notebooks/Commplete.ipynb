{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using \"DPR encoder for context\" for generating embeddings of passage.\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "import torch\n",
    "\n",
    "# Connect to Milvus and Elasticsearch\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "\n",
    "# Load DPR context encoder\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# Define Milvus collection schema\n",
    "dim = 768  # DPR embedding dimension\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "]\n",
    "schema = CollectionSchema(fields, \"DPR passages collection\")\n",
    "collection = Collection(\"passages\", schema)\n",
    "\n",
    "# Create index in Milvus\n",
    "index_params = {\n",
    "    \"metric_type\": \"IP\",\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"params\": {\"nlist\": 1024}\n",
    "}\n",
    "collection.create_index(\"embedding\", index_params)\n",
    "\n",
    "def encode_passage(passage):\n",
    "    inputs = context_tokenizer(passage, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = context_encoder(**inputs).pooler_output\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "def index_document(doc_id, content):\n",
    "    # Index in Elasticsearch for BM25\n",
    "    es.index(index=\"documents\", id=doc_id, body={\"content\": content})\n",
    "\n",
    "    # Index in Milvus for DPR\n",
    "    embedding = encode_passage(content)\n",
    "    collection.insert([[doc_id], [embedding.tolist()]])\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    {\"id\": 1, \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"id\": 2, \"content\": \"The Eiffel Tower is located in Paris.\"},\n",
    "    # ... more documents ...\n",
    "]\n",
    "\n",
    "for doc in documents:\n",
    "    index_document(doc[\"id\"], doc[\"content\"])\n",
    "\n",
    "# Remember to flush after inserting a batch\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query expansion \n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "model_name = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def expand_query_with_keywords(query, num_expansions=3, num_keywords=5):\n",
    "    # Generate expanded queries\n",
    "    input_text = f\"expand query: {query}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=num_expansions,\n",
    "        num_beams=num_expansions,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    expanded_queries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # Generate keywords\n",
    "    keyword_input = f\"generate keywords for: {query}\"\n",
    "    keyword_ids = tokenizer(keyword_input, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    keyword_outputs = model.generate(\n",
    "        keyword_ids,\n",
    "        max_length=30,\n",
    "        num_return_sequences=1,\n",
    "        num_beams=num_keywords,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    keywords = tokenizer.decode(keyword_outputs[0], skip_special_tokens=True).split()\n",
    "\n",
    "    # Combine original query, expanded queries, and keywords\n",
    "    final_queries = [query] + expanded_queries\n",
    "    final_queries = [f\"{q} {' '.join(keywords)}\" for q in final_queries]\n",
    "\n",
    "    return final_queries\n",
    "\n",
    "# Example usage\n",
    "original_query = \"What is the capital of France?\"\n",
    "expanded_queries = expand_query_with_keywords(original_query)\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"Expanded queries with keywords:\")\n",
    "for i, q in enumerate(expanded_queries):\n",
    "    print(f\"{i+1}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "from pymilvus import connections, Collection\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Connect to Milvus and Elasticsearch\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "\n",
    "# Load models\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "cross_encoder = SentenceTransformer('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Assume we have a Milvus collection named 'documents' and an Elasticsearch index 'documents'\n",
    "milvus_collection = Collection(\"documents\")\n",
    "\n",
    "def encode_query(query):\n",
    "    input_ids = question_tokenizer(query, return_tensors='pt')['input_ids']\n",
    "    with torch.no_grad():\n",
    "        embeddings = question_encoder(input_ids).pooler_output\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "def encode_passage(passage):\n",
    "    input_ids = context_tokenizer(passage, return_tensors='pt', max_length=512, truncation=True)['input_ids']\n",
    "    with torch.no_grad():\n",
    "        embeddings = context_encoder(input_ids).pooler_output\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "def dpr_search(query, top_k=100):\n",
    "    query_vector = encode_query(query)\n",
    "    search_params = {\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}}\n",
    "    results = milvus_collection.search(\n",
    "        data=[query_vector.tolist()],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=top_k,\n",
    "        output_fields=[\"id\"]\n",
    "    )\n",
    "    return [(hit.entity.get('id'), hit.score) for hit in results[0]]\n",
    "\n",
    "def bm25_search(query, top_k=100):\n",
    "    response = es.search(index=\"documents\", body={\n",
    "        \"query\": {\"match\": {\"content\": query}},\n",
    "        \"size\": top_k\n",
    "    })\n",
    "    return [(hit['_id'], hit['_score']) for hit in response['hits']['hits']]\n",
    "\n",
    "def hybrid_search(query, top_k=100, alpha=0.5):\n",
    "    bm25_results = bm25_search(query, top_k)\n",
    "    dpr_results = dpr_search(query, top_k)\n",
    "    \n",
    "    # Combine and normalize scores\n",
    "    all_ids = set([id for id, _ in bm25_results + dpr_results])\n",
    "    combined_scores = {}\n",
    "    for id in all_ids:\n",
    "        bm25_score = next((score for doc_id, score in bm25_results if doc_id == id), 0)\n",
    "        dpr_score = next((score for doc_id, score in dpr_results if doc_id == id), 0)\n",
    "        combined_scores[id] = alpha * bm25_score + (1 - alpha) * dpr_score\n",
    "\n",
    "    results=sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return  results\n",
    "\n",
    "def rerank(query, documents, top_k=10):\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    scored_docs = list(zip(documents, scores))\n",
    "    return sorted(scored_docs, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "def retrieve_and_rerank(query, top_k=10):\n",
    "    # Hybrid search\n",
    "    search_results = hybrid_search(query, top_k=top_k*2)\n",
    "    \n",
    "    # Fetch full documents (assuming we have a function to do this)\n",
    "    top_doc_texts = fetch_documents([doc_id for doc_id, _ in search_results])\n",
    "    \n",
    "    # Rerank\n",
    "    reranked_docs = rerank(query, top_doc_texts, top_k)\n",
    "    \n",
    "    return reranked_docs\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the capital of France?\"\n",
    "results = retrieve_and_rerank(query)\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.4f}, Document: {doc[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
