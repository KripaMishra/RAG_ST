{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/home/ubuntu/Steps/nvidia_docs/nvidia_docs/spiders/output.json'\n",
    "\n",
    "# Open and read the entire JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now `data` is a list of dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvJPEG A GPU accelerated JPEG codec library. The nvJPEG library provides high-performance, GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. The library offers single and batched JPEG decoding capabilities which efficiently utilize the available GPU resources for optimum performance; and the flexibility for users to manage the memory allocation needed for decoding. The nvJPEG library enables the following functions: use the JPEG image data stream as input; retrieve the width and height of the image from the data stream, and use this retrieved information to manage the GPU memory allocation and the decoding. A dedicated API is provided for retrieving the image information from the raw JPEG image data stream. Note Throughout this document, the terms “CPU” and “Host” are used synonymously. Similarly, the terms “GPU” and “Device” are synonymous. The nvJPEG library supports the following: JPEG options: Baseline and Progressive JPEG decoding/encoding 8 bits per pixel Huffman bitstream decoding Upto 4 channel JPEG bitstreams 8- and 16-bit quantization tables The following chroma subsampling for the 3 color channels Y, Cb, Cr (Y, U, V): 4:4:4 4:2:2 4:2:0 4:4:0 4:1:1 4:1:0 Features: Hybrid decoding using both the CPU (i.e., host) and the GPU (i.e., device). Hardware acceleration for baseline JPEG decode on  supported platforms . Input to the library is in the host memory, and the output is in the GPU memory. Single image and batched image decoding. Single phase and multiple phases decoding. Color space conversion. User-provided memory manager for the device and pinned host memory allocations. The encoding functions of the nvJPEG library perform GPU-accelerated compression of user’s image data to the JPEG bitstream. User can provide input data in a number of formats and colorspaces, and control the encoding process with parameters. Encoding functionality will allocate temporary buffers using user-provided memory allocator. Before calling the encoding functions the user should perform a few prerequisite steps using the helper functions described in  nvJPEG Encoder Helper API Reference . Not all nvJPEG types are thread safe. When using decoder APIs across multiple threads, the following decoder types should be instantiated separately for each thread:  nvjpegJpegStream_t ,  nvjpegJpegState_t ,  nvjpegBufferDevice_t ,  nvjpegBufferPinned_t When using encoder APIs across multiple threads,  nvjpegEncoderState_t  should be instantiated separately for each thread. For user-provided allocators (inputs to  nvJPEGCreateEx() ), the user needs to ensure thread safety. The nvJPEG states and handles are bound to the device that was set as current during their creation. Using these states and handles with another device set as current is undefined. The user is responsible of keeping track of the current device. Hardware accelerated JPEG decode is available on the following GPUs - A100, A30, H100. Platforms which support hardware accelerated JPEG decode: Windows Linux (x86_64, PowerPC, ARM64) \\u200bThe nvJPEG library provides functions for both the decoding of a single image, and batched decoding of multiple images. For single-image decoding you provide the data size and a pointer to the file data, and the decoded image is placed in the output buffer. To use the nvJPEG library, start by calling the helper functions for initialization. Create nvJPEG library handle with one of the helper functions  nvjpegCreateSimple()   or   nvjpegCreateEx() . Create JPEG state with the helper function  nvjpegJpegStateCreate() . See  nvJPEG Type Declarations  and  nvjpegJpegStateCreate() . The following helper functions are available in the nvJPEG library: nvjpegStatus_t   nvjpegGetProperty(libraryPropertyType   type,   int   *value); [DEPRECATED]   nvjpegStatus_t   nvjpegCreate(nvjpegBackend_t   backend,   nvjpegHandle_t   *handle   ,   nvjpeg_dev_allocator   allocator); nvjpegStatus_t   nvjpegCreateSimple(nvjpegHandle_t   *handle); nvjpegStatus_t   nvjpegCreateEx(nvjpegBackend_t   backend,   nvjpegDevAllocator_t   *dev_allocator,   nvjpegPinnedAllocator_t   *pinned_allocator,   unsigned   int   flags,   nvjpegHandle_t   *handle); nvjpegStatus_t   nvjpegDestroy(nvjpegHandle_t   handle); nvjpegStatus_t   nvjpegJpegStateCreate(nvjpegHandle_t   handle,   nvjpegJpegState_t   *jpeg_handle); nvjpegStatus_t   nvjpegJpegStateDestroy(nvjpegJpegState   handle); Other helper functions such as  nvjpegSet*()  and  nvjpegGet*()  can be used to configure the library functionality on per-handle basis. Refer to the  helper API reference  for more details. Retrieve the width and height information from the JPEG-encoded image by using the  nvjpegGetImageInfo()  function. Below is the signature of  nvjpegGetImageInfo() function: For each image to be decoded, pass the JPEG data pointer and data length to the above function. The  nvjpegGetImageInfo()  function is thread safe. One of the outputs of the above  nvjpegGetImageInfo()  function is  nvjpegChromaSubsampling_t . This parameter is an enum type, and its enumerator list is composed of the chroma subsampling property retrieved from the JPEG image. See  nvJPEG Chroma Subsampling . Use the  nvjpegDecode()  function in the nvJPEG library to decode this single JPEG image. See the signature of this function below: In the above  nvjpegDecode()  function, the parameters  nvjpegOutputFormat_t ,  nvjpegImage_t , and  cudaStream_t  can be used to set the output behavior of the  nvjpegDecode() function. You provide the  cudaStream_t  parameter to indicate the stream to which your asynchronous tasks are submitted. The ``nvjpegOutputFormat_t`` parameter: The  nvjpegOutputFormat_t  parameter can be set to one of the  output_format  settings below: output_format Meaning NVJPEG_OUTPUT_UNCHANGED Return the decoded image planar format. NVJPEG_OUTPUT_RGB Convert to planar RGB. NVJPEG_OUTPUT_BGR Convert to planar BGR. NVJPEG_OUTPUT_RGBI Convert to interleaved RGB. NVJPEG_OUTPUT_BGRI Convert to interleaved BGR. NVJPEG_OUTPUT_Y Return the Y component only. NVJPEG_OUTPUT_YUV Return in the YUV planar format. NVJPEG_OUTPUT_UNCHANGEDI_U16 Return the decoded image interleaved format. For example, if  output_format  is set to  NVJPEG_OUTPUT_Y  or  NVJPEG_OUTPUT_RGBI , or  NVJPEG_OUTPUT_BGRI  then the output is written only to channel[0] of  nvjpegImage_t , and the other channels are not touched. Alternately, in the case of planar output, the data is written to the corresponding channels of the  nvjpegImage_t  destination structure. Finally, in the case of grayscale JPEG and RGB output, the luminance is used to create the grayscale RGB. The below table explains the combinations of the output formats and the number of channels supported by the library. No of Channels in bitstream 1 2 3 4 Output Format NVJPEG_OUTPUT_UNCHANGED Yes Yes Yes Yes NVJPEG_OUTPUT_YUV Only the first channel of the output is populated No Yes No NVJPEG_OUTPUT_Y Yes No Yes Yes (a) NVJPEG_OUTPUT_RGB Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGR Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_RGBI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGRI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_UNCHANGEDI_U16 Yes(c) Yes No No NOTES: Must be enabled using  nvjpegDecodeParamsSetAllowCMYK() . Luminance is used to create the grayscale RGB. Supported only by  NVJPEG_BACKEND_LOSSLESS_JPEG  backend. As mentioned above, an important benefit of the  nvjpegGetImageInfo() function is the ability to utilize the image information retrieved from the the input JPEG image to allocate proper GPU memory for your decoding operation. The  nvjpegGetImageInfo()  function returns the  widths ,  heights  and  nComponents  parameters. You can use the retrieved parameters,  widths ,  heights  and  nComponents , to calculate the required size for the output buffers, either for a single decoded JPEG, or for every decoded JPEG in a batch. To optimally set the  destination  parameter for the  nvjpegDecode()  function, use the following guidelines: For the output_format: NVJPEG_OUTPUT_Y destination.pitch[0] should be at least:  width[0] destination.channel[0] should be at least of size:  destination.pitch[0]*height[0] For the output_format destination.pitch[c] should be at least: destination.channel[c] should be at least of size: NVJPEG_OUTPUT_YUV width[c] for c = 0, 1, 2 destination.pitch[c]*height[c] for c = 0, 1, 2 NVJPEG_OUTPUT_RGB and NVJPEG_OUTPUT_BGR width[0] for c = 0, 1, 2 destination.pitch[0]*height[0] for c = 0, 1, 2 NVJPEG_OUTPUT_RGBI and NVJPEG_OUTPUT_BGRI width[0]*3 destination.pitch[0]*height[0] NVJPEG_OUTPUT_UNCHANGED width[c] for c = [ 0, nComponents - 1 ] destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] NVJPEG_OUTPUT_UNCHANGEDI_U16 width[c]* nComponents* sizeof(unsigned short) destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] Ensure that the  nvjpegImage_t  structure (or structures, in the case of batched decode) is filled with the pointers and pitches of allocated buffers. The  nvjpegImage_t  structure that holds the output pointers is defined as follows: NVJPEG_MAX_COMPONENT is the maximum number of color components the nvJPEG library supports in the current release. For generic images, this is the maximum number of encoded channels that the library is able to decompress. Finally, when you call the  nvjpegDecode()  function with the parameters as described above, the  nvjpegDecode()  function fills the output buffers with the decoded data. The nvJPEG library allows further separation of the host and device phases of the decode process. The host phase of the decoding will not need to access to device resources. A few examples of decoupled APIs can be found under  Decode API - Decoupled Decoding. Below is the sequence of API calls to decode a single image Initialize all the items that are used in the decoding process: Create the library handle using one of the library handle initialization routines. Choose decoder implementation  nvjpegBackend_t , and create decoder using  nvjpegDecoderCreate() . Create JPEG decoder state using  nvjpegDecoderStateCreate() . Create JPEG stream using  nvjpegJpegStreamCreate() . Create the pinned and device buffers used by the decoder using the below APIs respectively. These buffers are used to store intermediate decoding results. nvjpegBufferPinnedCreate() nvjpegBufferDeviceCreate() Link the buffers to the JPEG state using the following APIs respectively: nvjpegStateAttachPinnedBuffer() nvjpegStateAttachDeviceBuffer() Create decode parameters using the below API. This is used to set the output format, and enable ROI decode: nvjpegDecodeParamsCreate() Perform decoding: Parse the jpeg bit-stream using  nvjpegJpegStreamParse() Encoded bitstream information, like channel dimensions, can be retrieved using the below API. This information is used to allocate the output pointers in  nvjpegImage_t . nvjpegJpegStreamGetComponentsNum() nvjpegJpegStreamGetComponentDimensions() Call the decode API in the below sequence to decode the image: nvjpegDecodeJpegHost() nvjpegDecodeJpegTransferToDevice() nvjpegDecodeJpegDevice() For the batched image decoding you provide pointers to multiple file data in the memory, and also provide the buffer sizes for each file data. The nvJPEG library will decode these multiple images, and will place the decoded data in the output buffers that you specified in the parameters. For batched image decoding in single phase, follow these steps: Call  nvjpegDecodeBatchedInitialize()  function to initialize the batched decoder. Specify the batch size in the  batch_size  parameter. See  nvjpegDecodeBatchedInitialize() . Next, call  nvjpegDecodeBatched()  for each new batch. Make sure to pass the parameters that are correct to the specific batch of images. If the size of the batch changes, or if the batch decoding fails, then call the  nvjpegDecodeBatchedInitialize()  function again. The  nvjpegBackend_t  enum is used to select either default back-end by default, or use GPU decoding for baseline JPEG images, or use CPU for Huffman decoding. Member Description NVJPEG_BACKEND_DEFAULT Back-end is selected internally. NVJPEG_BACKEND_HYBRID Uses CPU for Huffman decoding. NVJPEG_BACKEND_GPU_HYBRID Uses GPU for Huffman decoding.  nvjpegDecodeBatched  will use GPU decoding for baseline JPEG images with interleaved scan when batch size is greater than 50. The  decoupled APIs  will use GPU assisted Huffman decoding. NVJPEG_BACKEND_HARDWARE Uses  Hardware Acceleration  for decode. Supports baseline JPEG images with single scan with 1 or 3 channels. 410 and 411 chroma subsamplings are not supported. NVJPEG_BACKEND_GPU_HYBRID_DEVICE Supports input bitstream on device memory. Can be used only with batched decode APIs for baseline JPEG images without restart intervals. NVJPEG_BACKEND_HARDWARE_DEVICE Supports input bitstream on device memory. Can be used only with batched decode APIs. Uses  Hardware Acceleration  for decode. Supports baseline JPEG images with single scan with 1 or 3 channels. 410 and 411 chroma subsamplings are not supported. NVJPEG_BACKEND_LOSSLESS_JPEG Supports lossless jpeg bitstreams as defined in the jpeg 92 standard. Bitstreams with up to 2 channels and prediction mode 1 are supported. This handle stores the bit-stream parameters on the host. This helps retrieve bitstream meta-data using APIs defined in  nvJPEG Stream API . This  nvjpegBufferDevice_t  is used by decoder states to store the intermediate information in device memory. This decoder parameter handle stores the parameters like output format, and the ROI decode parameters that are set using APIs defined in  nvJPEG Chroma Subsampling . This  nvjpegBufferPinned_t  handle is used by decoder states to store the intermediate information on pinned memory. This decoder handle stores the intermediate decoder data, which is shared across the decoding stages. This decoder handle is initialized for a given  nvjpegBackend_t . It is used as input to the  Decode API—Decoupled Decoding . When the  nvjpegPinnedAllocator_t   *allocator  parameter in the  nvjpegCreateEx()  function is set as a pointer to the above  nvjpegPinnedAllocator_t  structure, then this structure will be used for allocating and releasing host pinned memory for copying data to/from device. The function prototypes for the memory allocation and memory freeing functions are similar to the  cudaHostAlloc()  and  cudaFreeHost()  functions. They will return 0 in case of success, and non-zero otherwise. However, if the  nvjpegPinnedAllocator_t   *allocator  parameter in the  nvjpegCreateEx()  function is set to NULL, then the default memory allocation functions  cudaHostAlloc()  and  cudaFreeHost()  will be used. When using  nvjpegCreate()  or  nvjpegCreateSimple()  function to create library handle, the default host pinned memory allocator will be used. Extended pinned allocators support stream ordered allocations along with user defined context information  pinned_ctx . When invoking the allocators, nvJPEG will pass  pinned_ctx  as input to the extended pinned allocators. The  nvjpegImage_t  structure (or structures, in the case of batched decode) is used to fill with the pointers and pitches of allocated buffers. The  nvjpegImage_t  structure that holds the output pointers. Member Description NVJPEG_MAX_COMPONENT Maximum number of color components the nvJPEG library supports. For generic images, this is the maximum number of encoded channels that the library is able to decompress. Users can tell the library to use their own device memory allocator. The function prototypes for the memory allocation and memory freeing functions are similar to the  cudaMalloc() and  cudaFree()  functions. They should return 0 in case of success, and non-zero otherwise. A pointer to the  nvjpegDevAllocator_t  structure, with properly filled fields, should be provided to the  nvjpegCreate()  function. NULL is accepted, in which case the default memory allocation functions  cudaMalloc()  and  cudaFree()  is used. When the  nvjpegDevAllocator_t   *allocator  parameter in the  nvjpegCreate()  or  nvjpegCreateEx()  function is set as a pointer to the above  nvjpegDevAllocator_t  structure, then this structure is used for allocating and releasing the device memory. The function prototypes for the memory allocation and memory freeing functions are similar to the  cudaMalloc()  and  cudaFree()  functions. They should return 0 in case of success, and non-zero otherwise. However, if the  nvjpegDevAllocator_t   *allocator  parameter in the  nvjpegCreate()  or  nvjpegCreateEx()  function is set to NULL, then the default memory allocation functions  cudaMalloc()  and  cudaFree()  will be used. When using  nvjpegCreateSimple()  function to create library handle the default device memory allocator will be used. Extended device allocators support stream ordered allocations along with user defined context information  dev_ctx . When invoking the allocators, nvJPEG will pass  dev_ctx  as input to the extended device allocators. The  nvjpegJpegState  structure stores the temporary JPEG information. It should be initialized before any usage. This JPEG state handle can be reused after being used in another decoding. The same JPEG handle should be used across the decoding phases for the same image or batch. Multiple threads are allowed to share the JPEG state handle only when processing same batch during first phase ( nvjpegDecodePhaseOne ) . The library handle is used in any consecutive nvJPEG library calls, and should be initialized first. The library handle is thread safe, and can be used by multiple threads simultaneously. The  nvjpegImage_t struct holds the pointers to the output buffers, and holds the corresponding strides of those buffers for the image decoding. See  Single Image Decoding  on how to set up the  nvjpegImage_t  struct. The  nvjpegJpegEncoding_t  enum lists the JPEG encoding types that are supported by the nvJPEG library The enum values are based on the markers defined in the JPEG specification Member Description NVJPEG_ENCODING_UNKNOWN This value is returned for all the JPEG markers not supported by the nvJPEG library. NVJPEG_ENCODING_BASELINE_DCT Corresponds to the JPEG marker 0xc0, refer to the JPEG spec for more details. NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN Corresponds to the JPEG marker 0xc1, refer to the JPEG spec for more details. NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN Corresponds to the JPEG marker 0xc2, refer to the JPEG spec for more details. NVJPEG_ENCODING_LOSSLESS_HUFFMAN Corresponds to the JPEG marker 0xc3, refer to the JPEG spec for more details. The  nvjpegScaleFactor_t  enum lists all the scale factors supported by the library. This feature is supported when nvjpeg handles are intstaniated using NVJPEG_BACKEND_HARDWARE Member Description NVJPEG_SCALE_NONE Decoded output is not scaled NVJPEG_SCALE_1_BY_2 Decoded output width and height are scaled by a factor of 1/2 NVJPEG_SCALE_1_BY_4 Decoded output width and height are scaled by a factor of 1/4 NVJPEG_SCALE_1_BY_8 Decoded output width and height are scaled by a factor of 1/8 nvJPEG flags provide additional controls when initializing the library using  nvJPEGCreateEx()  or  nvJPEGCreateExV2()  . It is possible to combine the flags as they are bit fields. Member Description NVJPEG_FLAGS_DEFAULT Corresponds to default library behavior. NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE To be used when the library is initialized with NVJPEG_BACKEND_HARDWARE. It will be ignored for other back-ends. nvjpeg in batched decode mode buffers additional images to achieve optimal performance. Use this flag to disable buffering of additional images. NVJPEG_FLAGS_ENABLE_MEMORY_POOLS [Deprecated] Starting with CUDA 11.1 this flag will be ignored. NVJPEG_FLAGS_BITSTREAM_STRICT nvJPEG library will try to decode a bitstream even if it doesn’t strictly follow the JPEG specification. Using this flag will return an error in such cases. NVJPEG_FLAGS_REDUCED_MEMORY_DECODE When using  NVJPEG_BACKEND_HYBRID  or  NVJPEG_BACKEND_GPU_HYBRID  backends, enabling this flag will reduce the memory usage of the decoding whenever possible. NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY Using this flag enables zero-copy memory when feasible on supported platforms. NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION Using this flag enables the decoder to use interpolation when performing chroma upsampling during the YCbCr to RGB conversion stage. The  nvjpegExifOrientation_t  enum represents the exif orientation in a jfif(jpeg) file. Exif orientation information is typically used to denote the digital camera sensor orientation at the time of image capture. Member Description NVJPEG_ORIENTATION_UNKNOWN Exif orientation information is not available in the bitstream. NVJPEG_ORIENTATION_NORMAL Decode output remains unchanged. NVJPEG_ORIENTATION_FLIP_HORIZONTAL Decoded output should be mirrored/flipped horizontally. NVJPEG_ORIENTATION_ROTATE_180 Decoded output should be rotated 180 degrees. NVJPEG_ORIENTATION_FLIP_VERTICAL Decoded output should be mirrored/flipped vertically. NVJPEG_ORIENTATION_TRANSPOSE Decoded output should be flipped/mirrored horizontally followed by a 90 degrees counter-clockwise rotation. NVJPEG_ORIENTATION_ROTATE_90 Decoded output should be rotated 90 degrees counter-clockwise. NVJPEG_ORIENTATION_TRANSVERSE Decoded output should be flipped/mirrored horizontally followed by a 270 degrees counter-clockwise rotation. NVJPEG_ORIENTATION_ROTATE_270 Decoded output should be rotated 270 degrees counter-clockwise. This section describes the nvJPEG decoder API. Gets the numeric value for the major or minor version, or the patch level, of the nvJPEG library. Signature: Parameters: Parameter Input / Output Memory Description libraryPropertyType   type Input Host One of the supported  libraryPropertyType  values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL. int   *value Output Host The numeric value corresponding to the specific  libraryPropertyType  requested. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Gets the numeric value for the major version, minor version, or the patch level of the CUDA toolkit that was used to build nvJPEG library. For the same information on the nvJPEG library itself, see  nvjpegGetProperty() . Signature: Parameters: Parameter Input / Output Memory Description libraryPropertyType   type Input Host One of the supported  libraryPropertyType  values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL. int   *value Output Host The numeric value corresponding to the specific  libraryPropertyType  requested. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Allocates and initializes the library handle. Note This function is deprecated. Use either  nvjpegCreateSimple()  or  nvjpegCreateEx()  functions to create the library handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBackend_t   backend Input Host Backend parameter for  nvjpegDecodeBatched()  API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocator_t   *allocator Input Host Device memory allocator. See  nvjpegDevAllocator_t structure description. If NULL is provided, then the default CUDA runtime  cudaMalloc() and  cudaFree()  functions will be used. nvjpegHandle_t   *handle Input/Output Host The library handle. The  nvjpegBackend_t  parameter is an  enum  type, with the below enumerated list values: Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Allocates and initializes the library handle, with default codec implementations selected by library and default memory allocators. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   *handle Input/Output Host The library handle. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Allocates and initializes the library handle using the provided arguments. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBackend_t   backend Input Host Backend parameter for  nvjpegDecodeBatched()  API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocator_t   *dev_allocator Input Host Device memory allocator. See  nvjpegDevAllocator_t  structure description. If NULL is provided, then the default CUDA runtime functions  cudaMalloc()  and  cudaFree()  will be used. nvjpegPinnedAllocator_t   *pinned_allocator Input Host Pinned host memory allocator. See  nvjpegPinnedAllocator_t structure description. If NULL is provided, then the default CUDA runtime functions  cudaHostAlloc()  and  cudaFreeHost()  will be used. unsigned   int   flags Input Host Refer to  nvJPEG Flags  for details. nvjpegHandle_t   *handle Input/Output Host The library handle. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Allocates and initializes the library handle using the provided arguments. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBackend_t   backend Input Host Backend parameter for  nvjpegDecodeBatched()  API. If this is set to DEFAULT then it automatically chooses one of the underlying algorithms. nvjpegDevAllocatorV2_t   *dev_allocator Input Host Extended device memory allocator. See  nvjpegDevAllocatorV2_t_t  structure description. Cannot be NULL. nvjpegPinnedAllocatorV2_t   *pinned_allocator Input Host Extended pinned memory allocator. See  nvjpegPinnedAllocatorV2_t structure description. Cannot be NULL. unsigned   int   flags Input Host Refer to  nvJPEG Flags  for details. nvjpegHandle_t   *handle Input/Output Host The library handle. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Releases the library handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input/Output Host The library handle to release. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Use the provided padding for all device memory allocations with specified library handle. A large number will help to amortize the need for device memory reallocations when needed. Signature: Parameters: Parameter Input / Output Memory Description size_t   padding Input Host Device memory padding to use for all further device memory allocations. nvjpegHandle_t   handle Input/Output Host The library handle. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Retrieve the device memory padding that is currently used for the specified library handle. Signature: Parameters: Parameter Input / Output Memory Description size_t   *padding Output Host Device memory padding that is currently used for device memory allocations. nvjpegHandle_t   handle Input/Output Host The library handle. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Use the provided padding for all pinned host memory allocations with specified library handle. A large number will help to amortize the need for pinned host memory reallocations when needed. Signature: Parameters: Parameter Input / Output Memory Description size_t   padding Input Host Pinned host memory padding to use for all further pinned host memory allocations. nvjpegHandle_t   handle Input/Output Host The library handle. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Retrieve the pinned host memory padding that is currently used for specified library handle. Signature: Parameters: Parameter Input / Output Memory Description size_t   *padding Output Host Pinned host memory padding that is currently used for pinned host memory allocations. nvjpegHandle_t   handle Input/Output Host The library handle. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Retrieve hardware decoder details such as number of engines and number of cores available in each engine. Signature: Parameters: nvjpegHandle_t   handle Input Host The library handle. unsigned   int*   num_engines Input/Output Host Retrieves number of engines available for decode. Return value of 0 indicates that hardware decoder is not available. unsigned   int*   num_cores_per_engine Input/Output Host Retrieves number of cores per engine. Return value of 0 indicates that hardware decoder is not available. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Allocates and initializes the internal structure required for the JPEG processing. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegState_t   *jpeg_handle Input/Output Host The image state handle. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Releases the image internal structure. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegState   handle Input/Output Host The image state handle. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Creates a decoder handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   nvjpeg_handle Input Host Library handle. nvjpegBackend_t   backend Input Host Backend parameter for the decoder_handle.The back end applies to all the functions under the  decoupled API , when called with this handle. nvjpegJpegDecoder_t   decoder_handle Input/Output Host Decoder state handle. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Destroys the decoder handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t   decoder_handle Input/Output Host Decoder handle. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Determines whether the  decoder_handle  is able to handle the bit-stream stored in  jpeg_stream . Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t   decoder_handle Input Host Decoder state handle nvjpegJpegStream_t   jpeg_stream Input Host Bit stream meta-data nvjpegDecodeParams_t   decode_params Input Host Decoder output configuration int*   is_supported Output Host Return value of 0 indicates bitstream can be decoded by the  decoder_handle , non zero value indicates that the bitstream is not supported Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Creates the  decoder_state  internal structure. The  decoder_state  is associated with the  nvjpegBackend_t  implementation that was used to create the  decoder_handle . Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   nvjpeg_handle Input Host Library handle. nvjpegJpegDecoder_t   decoder_handle Input Host Decoder handle. nvjpegJpegState_t*   decoder_state Input/Output Host nvJPEG Image State Handle. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Creates  jpeg_stream  that is used to parse the JPEG bitstream and store bitstream parameters. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host Library handle nvjpegJpegStream_t   *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Destroys the  jpeg_stream  structure. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t   *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Creates a pinned buffer handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host Library handle. nvjpegPinnedAllocator_t*   pinned_allocator Input Host Pinned host memory allocator. See  nvjpegPinnedAllocator_t  structure description. nvjpegBufferPinned_t*   buffer Input/Output Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Creates a pinned buffer handle using extended allocators. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegPinnedAllocatorV2_t* pinned_allocator Input Host Extended pinned host memory allocator. See  nvjpegPinnedAllocatorV2_t  structure description. nvjpegBufferPinned_t* buffer Input/Output Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Destroys a pinned buffer handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t   buffer Input Host nvJPEG pinned buffer object. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Link the nvJPEG pinned buffer handle to  decoder_state . The  pinned_buffer  is used by the decoder to store the intermediate information that is used across the decoding stages. Pinned buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t   decoder_state Input Host nvJPEG decoder state. nvjpegBufferPinned_t   pinned_buffer Input Host nvJPEG pinned buffer container. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Retrieves the pinned memory pointer and size from the nvJPEG pinned buffer handle. Allows the application to re-use the memory once the decode is complete. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t   buffer Input Host nvJPEG pinned buffer container. size_t*   size Input/Output Host Size in bytes of the pinned buffer. void**   ptr Input/Output Host Pointer to the pinned buffer. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Resize the pinned buffer to the specified size in bytes. This API can be used to pre-allocate the pinned buffer\\nto a large value and avoid allocator calls during decode. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t   buffer Input Host nvJPEG pinned buffer container. size_t*   size Input Host Size in bytes of the pinned buffer. cudaStream_t   stream Input Host CUDA stream to use when  nvjpegBufferPinned_t   buffer  is initialized using stream ordered allocators. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Creates the device buffer handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host Library handle. nvjpegDevAllocator_t*   device_allocator Input Host Device memory allocator. See the  `nvjpegDevAllocator_t  <index.html#nvjpeg-memory-allocator-interface>`__ structure description. nvjpegBufferDevice_t*   buffer Input/Output Host nvJPEG device buffer container. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Creates the device buffer handle using extended allocators. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle. nvjpegDevAllocatorV2_t* device_allocator Input Host Extended device memory allocator. See  nvjpegDevAllocatorV2_t_t  structure description. nvjpegBufferDevice_t* buffer Input/Output Host nvJPEG device buffer container. Returns: nvjpegStatus_t  - An error code as specified in  nvJPEG API Return Codes . Destroys the device buffer handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t   buffer Input Host/Device nvJPEG device buffer container. Device pointers are stored within the host structures. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Link the nvJPEG device buffer handle to the  decoder_state . The  device_buffer  is used by the decoder to store the intermediate information that is used across the decoding stages. Device buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t   decoder_state Input Host nvJPEG decoder state. nvjpegBufferDevice_t   device   buffer Input Host/Device nvJPEG device buffer container. Device pointers are stored within the host structures. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Retrieve the device memory pointer and size from the nvJPEG device buffer handle. Allows the application to re-use the memory after the decode is complete. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t   buffer Input Host nvJPEG device buffer container. size_t*   size Input/Output Host Device buffer size in bytes. void**   ptr Input/Output Host Pointer to the device buffer. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Resize the device buffer to the specified size in bytes. This API can be used to pre-allocate the device buffer\\nto a large value and avoid allocator calls during decode. Signature: Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t   buffer Input Host nvJPEG device buffer container. size_t*   size Input Host Size in bytes of the device buffer. cudaStream_t   stream Input Host CUDA stream to use when  nvjpegBufferDevice_t   buffer  is initialized using stream ordered allocators. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Creates a handle for the parameters. The parameters that can be programmed include: output format, ROI decode, CMYK to RGB conversion. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host Library handle. nvjpegDecodeParams_t   *decode_params Input/Output Host Decode output parameters. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Destroys the  decode_params  handle. Signature: Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t   *decode_params Input/Output Host Decode output parameters. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . The helper functions for retrieving the encoded image information. Decodes the JPEG header and retrieves the basic information about the image. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. const   unsigned   char   *data Input Host Pointer to the encoded data. size_t   length Input Host Size of the encoded data in bytes. int   *nComponents Output Host Chroma subsampling for the 1- or 3- channel encoding. int   *widths Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the width of each channel (up to NVJPEG_MAX_COMPONENT) will be saved. If the channel is not encoded, then the corresponding value would be zero. int   *heights Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the height of each channel (up to NVJPEG_MAX_COMPONENT) will be saved. If the channel is not encoded, then the corresponding value would be zero. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . These functions store the parsed bit-stream data on the host. Parses the bitstream and stores the metadata in the jpeg_stream  struct. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. const   unsigned   char   *data Input Host Pointer to the bit-stream. size_t   length Input Host Bit-stream size. int   save_metadata Input Host (Not enabled. Marked for future use). If not 0, then the JPEG stream metadata (headers, app markers, etc.) will be saved in the internal  JpegStream  structure for future usage.\\nIf 0, then the meta data (headers, app markerms etc.) will be discarded. int   save_stream Input Host If not 0, then the whole jpeg stream will be copied to the internal JpegStream structure, and the pointer to the JPEG file data will not be needed after this call.\\nIf 0, then  JpegStream  will just save the pointers (to JPEG file data), and these pointers will be used later during the image decoding. nvjpegJpegStream_t   jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Parses only the header of the bit-stream and stores the header information in the jpeg_stream  struct. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. const   unsigned   char   *data Input Host Pointer to the bit-stream. size_t   length Input Host Bit-stream size. nvjpegJpegStream_t   jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . To be used when decoding TIFF files with JPEG compression. Parses the JPEG tables bitstream and stores the jpeg tables in  jpeg_stream Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. const   unsigned   char   *data Input Host Pointer to the JPEG tables bitstream. Can be set to NULL to reset the JPEG tables. size_t   length Input Host JPEG tables bitstream size. nvjpegJpegStream_t   jpeg_stream Input/Output Host The nvJPEG bitstream handle that stores the parsed bitstream information. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Extracts the JPEG frame dimensions from the bitstream. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t   jpeg_stream Input Host Bitstream handle. unsigned   int*   width Output Host Frame height. unsigned   int*   height Output Host Frame width. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Extracts the JPEG frame dimensions from the bitstream. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t   jpeg_stream Input Host Bitstream handle. unsigned   int*   components_num Output Host Number of encoded channels in the input. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Extracts the component dimensions from the bitstream. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t   jpeg_stream Input Host Bitstream handle. unsigned   int   component Input Host Component index. unsigned   int*   width Output Host Component height. unsigned   int*   height Output Host Component width. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Gets the chroma subsampling from the  jpeg_stream . For grayscale (single channel) images it returns NVJPEG_CSS_GRAY. For 3-channel images it tries to assign one of the known chroma sub-sampling values based on the sampling information present in the bitstream, else it returns NVJPEG_CSS_UNKNOWN. If the number of channels is 2 or 4, then it returns NVJPEG_CSS_UNKNOWN. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t   jpeg_stream Input Host Bitstream handle. nvjpegChromaSubsampling_t*   chroma_subsampling Output Host Chroma subsampling for the 1- or 3- channel encoding. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This function obtains the JPEG encoding type from the  jpeg_stream . For baseline images it returns NVJPEG_ENCODING_BASELINE_DCT. For progressive images it returns NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN. Signature: Parameters: Parameter Input / Output Memory Description jpeg_stream In Host Input bitstream handle. jpeg_encoding Out Host Encoding type obtained—baseline or progressive. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Extracts the exif orientation from the bitstream. Returns  NVJPEG_ORIENTATION_UNKNOWN  if the exif marker/orientation information is not present. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t   jpeg_stream Input Host Bitstream handle. nvjpegExifOrientation_t   *orientation_flag Output Host Exif orientation in JPEG stream. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Extracts the sample precision(bit depth) from the bitstream. Signature: Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t   jpeg_stream Input Host Bitstream handle. unsigned   int   *precision Output Host Sample precision value. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Functions for decoding single image or batched images in a single phase. Decodes a single image, and writes the decoded image in the desired format to the output buffers. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. From CUDA 11 onwards,  nvjpegDecode()  picks the best available back-end for a given image, user no longer has control on this. If there is a need to select the back-end, then consider using  nvjpegDecodeJpeg . This is a new API added in CUDA 11 which allows user to control the back-end. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegState_t   jpeg_handle Input Host The image state handle. const   unsigned   char   *data Input Host Pointer to the encoded data. size_t   length Input Host Size of the encoded data in bytes. nvjpegOutputFormat_t   output_format Input Host Format in which the decoded output will be saved. nvjpegImage_t   *destination Input/Output Host/Device Pointer to the structure that describes the output destination. This structure should be on the host (CPU), but the pointers in this structure should be pointing to the device (i.e., GPU) memory. See  nvjpegImage_t. cudaStream_t   stream Input Host The CUDA stream where all of the GPU work will be submitted. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This function initializes the batched decoder state. The initialization parameters include the batch size, the maximum number of CPU threads, and the specific output format in which the decoded image will be saved. This function should be called once, prior to decoding the batches of images. Any currently running batched decoding should be finished before calling this function. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegState_t   jpeg_handle Input Host The image state handle. int   batch_size Input Host Batch size. int   max_cpu_threads Input Host This parameter is no longer used by the library. nvjpegOutputFormat_t   output_format Input Host Format in which the decoded output will be saved. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Decodes the batch of images, and writes them to the buffers described in the  destination  parameter in a format provided to  nvjpegDecodeBatchedInitialize()  function. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegState_t   jpeg_handle Input Host The image state handle. const   unsigned   char   *const   *data Input Host Pointer to the first element of array of the input data. The size of the array is assumed to be batch_size provided to  nvjpegDecodeBatchedInitialize()  batch initialization function. const   size_t   *lengths Input Host Pointer to the first element of array of input sizes. Size of array is assumed to be batch_size provided to  nvjpegDecodeBatchedInitialize() , the batch initialization function. nvjpegImage_t   *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors. The size of array is assumed to be batch_size provided to  nvjpegDecodeBatchedInitialize(),  the batch initialization function. See also  nvjpegImage_t . cudaStream_t   stream Input Host The CUDA stream where all the GPU work will be submitted. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This API helps to Decodes the batch of images with ROI, and writes them to the buffers described in the  destination  parameter in a format provided to  nvjpegDecodeBatchedInitialize()  function. This function is asynchronous with respect to the host. All GPU tasks for this function will be submitted to the provided stream. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host nvjpeg library handle. nvjpegJpegState_t   jpeg_handle Input Host The image state handle. const   unsigned   char   *const   *data Input Host Pointer to the first element of array of the input data. The size of the array is assumed to be  batch_size  provided to  nvjpegDecodeBatchedInitialize()  batch initialization function. const   size_t   *lengths Input Host Pointer to the first element of array of input sizes. nvjpegImage_t   *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors. The size of array is assumed to be  batch_size  provided to  nvjpegDecodeBatchedInitialize() , the batch initialization function. See also  nvjpegImage_t . nvjpegDecodeParams_t   *decode_params Input Host Setting ROI Decode parameters cudaStream_t   stream Input Host The CUDA stream where all the GPU work will be submitted. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This API helps determine whether an image can be decoded by  nvjpegDecodeBatched . User can parse the bitstream header using  nvjpegJpegStreamParseHeader  and then call this API to determine whether the image can be decoded. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host nvjpeg library handle. nvjpegJpegStream_t   jpeg_stream Input Host Bit stream meta-data. int*   is_supported Output Host Return value of 0 indicates bitstream can be decoded by the  decoder_handle , non zero value indicates that the bitstream is not supported. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This API helps determine whether an image can be decoded by  nvjpegDecodeBatchedEx . User can parse the bitstream header using  nvjpegJpegStreamParseHeader  and set the ROI in the decode params then call this API to determine whether the image can be decoded. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host nvjpeg library handle. nvjpegJpegStream_t   jpeg_stream Input Host Bit stream meta-data. nvjpegDecodeParams_t   decode_params Input Host Setting ROI Decode parameters. int*   is_supported Output Host Return value of 0 indicates bitstream can be decoded by the  decoder_handle , a non zero value indicates that the bitstream is not supported. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This is an experimental API that can be used with  nvjpegDecodeBatched . When decoding images with varying sizes and chroma subsampling, performance is limited by the repeated cuda calls made by the library to free/allocate device memory. This API attempts to avoid this problem by allocating device memory prior to the actual decoding. Users have the option to call this API with values that are unlikely to be exceeded when  nvjpegDecodeBatched  is called. Note Note:\\nThis functionality is available only when the  nvjpegHandle_t is instantiated using NVJPEG_BACKEND_HARDWARE. It is currently a No Op for other backends. This API only provides a hint for initial allocation. If the image dimensions at the time of decode exceed what was provided, then the library will resize the device buffers. If the images being decoded have different chroma subsamplings, then the  chroma_subsampling  field should be set to NVJPEG_CSS_444 to ensure that the device memory can be reused. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegState_t   jpeg_handle Input Host The image state handle. int   batch_size Input Host Batch size. int   width Input Host Maximum width of image that will be decoded. int   height Input Host Maximum height of image that will be decoded. nvjpegChromaSubsampling_t   chroma_subsampling Input Host Chroma-subsampling of the images. nvjpegOutputFormat_t   output_format Input Host Format in which the decoded output will be saved. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . To be used along with batched decode APIs when decoding JPEG bitstreams from a TIFF file. This function parses the JPEG tables bitstream to extract the JPEG tables. The external Huffman and quantization tables will be applied to all the JPEG bitstreams in the batch. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegState_t   jpeg_handle Input/Output Host/Device The image state handle. const   unsigned   char   *data Input Host Pointer to the JPEG tables bitstream. Can be set to NULL to reset the jpeg tables. size_t   length Input Host JPEG tables bitstream size. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This set of decoding API works with the bitstream handles, decode parameter handles, pinned and device buffers handles as input, thus decoupling JPEG bitstream parse, buffer management and setting up decoder parameters from the decode process itself. Currently only multiphase decoding is available. Multiphase decoupled single image decoding consists of three phases: Host Mixed Device Each of the above decodings is carried on according to its individual semantics. Phases on different images can be carried out with different decoding state handles simultaneously, while sharing of some helper objects is possible. See the details of semantics in the individual phases descriptions. Below are a couple of examples of using decoupled API. The following snippet explains how to use the API to prefetch the host stage of the processing: first do all of the host work on the host, and then submit the rest of decoding work to the device. The following snippet explains how pinned and device buffers can be shared across two instances of  nvJPEG Decoder Handle . This is the first stage of the decoupled decoding process. It is done entirely on the host, hence it is synchronous with respect to the host. If a pinned buffer is attached to the decoder state, then the pinned buffer object will be used to allocate the pinned memory required for the host decoding phase. There wouldn’t be allocation if the pinned buffer object already handles the required amount of pinned memory. If a pinned buffer object is not attached, then the state will use heap host memory to allocate the memory required for the host processing. In this phase, device is not participating. Hence the device selection, device initialization, and device memory initialization can be done later in the decoding process. This function works on a parsed stream. The parsed stream handle that is available after calling the  nvjpegJpegStreamParse()  function should be provided to this function. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegDecoder_t   decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t   decoder_state Input Host The nvJPEG decoder state handle. nvjpegDecodeParams_t   decode_params Input Host Handle to decode the output properties. nvjpegJpegStream_t   jpeg_stream Input Host Handle to the parsed bitstream data. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This phase contains both host and device operations. Hence it is a mix of synchronous and asynchronous operations with respect to the host. All the device operations will be submitted to the provided stream. This phase should be called only after the host phase with the same decoder handle, decoder state handle and parsed jpeg stream handle. Device should be initialized and device buffer should be attached to  decoder_state  handle using  nvjpegStateAttachDeviceBuffer()  prior to calling this API. This device buffer object will be resized to the required amount of memory if needed. For the host memory buffer, this phase will use whatever was used in the host phase: either the attached pinned buffer or the state’s host memory buffer. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegDecoder_t   decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t   decoder_state Input Host The nvJPEG decoder state handle. nvjpegJpegStream_t   jpeg_stream Input Host Handle to the parsed bitstream data. cudaStream_t   stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This phase consists of decode operations that take place mainly on the device (no significant host side computation is done). Hence this phase is asynchronous with respect to the host. This phase should be called after  nvjpegDecodeJpegTransferToDevice()  for a given  decoder_state  handle and decoder handle. In this function call, the host memory buffers are not used, so if the pinned buffer was attached to the state, then it can be reused somewhere else. Note that at this point the Jpeg stream handle is not needed anymore, since parts that are needed for device decoding will be copied to the device memory in the previous phase. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegDecoder_t   decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t   decoder_state Input Host The nvJPEG decoder state handle. nvjpegImage_t   *destination Input/Output Host/Device Pointer to a structure that describes the output destination. This structure should be on host, but the pointers in this structure should be pointing to the device memory. See  nvJPEG Image  for details. cudaStream_t   stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This is a single phase API with the flexibility to select nvJPEG back-end when creating an  nvjpegJpegDecoder_t  object. The user has the option to call this API instead of making three separate calls to  nvjpegDecodeJpegHost() ,  nvjpegDecodeJpegTransferToDevice() , and  nvjpegDecodeJpegDevice() . It is required to atttach the device buffer to the decoder state before calling this API. The pinned buffer is optional. If the pinned buffer is not attached, then heap memory will be used for host processing. This function works on a parsed stream. The parsed stream handle that is available after calling the  nvjpegJpegStreamParse()  function should be provided to this function. Signature: Parameters: Parameter Input / Output Memory Description nvjpegHandle_t   handle Input Host The library handle. nvjpegJpegDecoder_t   decoder Input Host The nvJPEG decoder handle. nvjpegJpegState_t   decoder_state Input Host The nvJPEG decoder state handle. nvjpegJpegStream_t   jpeg_stream Input Host Handle to the parsed bitstream data. nvjpegImage_t   *destination Input/Output Host/Device Pointer to a structure that describes the output destination. This structure should be on the host, but the pointers in this structure should be pointing to the device memory. See  nvJPEG Image  for details. nvjpegDecodeParams_t   decode_params Input Host The handle which stores the decode output properties. cudaStream_t   stream Input Host The CUDA stream to which all the GPU tasks will be submitted. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This category of APIs is used to set the decoding parameters. These APIs should be used with the decode APIs defined in  Decode API—Decoupled Decoding . This function is used to set the decode output format. See  nvjpegOutputFormat_t  described in step 6 of  Single Image Decoding . The output parameter of  nvjpegOutputFormat_t  defaults to NVJPEG_OUTPUT_UNCHANGED if not set using this API. Signature: Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t   decode_params Input Host Decode output parameter handle. nvjpegOutputFormat_t   output_format Input Host See step 6 of  Single Image Decoding . Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This function enables the region of interest-only (ROI-only) decode. To disable the ROI-only, i.e., to decode the whole image, set: offset_x  = 0, offset_y  = 0, roi_width  = -1, and roi_height  = -1. Note ROI decode is disabled by default. It is not supported when the nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE. The ROI window cannot go out of image bounds. That is: offset_x  cannot be lower than zero, or offset_x   +   roi_width  cannot be larger than the JPEG image width. If the output format is NVJPEG_OUTPUT_YUV or NVJPEG_OUTPUT_UNCHANGED, then the  offset_x and  offset_y  values have to be multiples of the maximum subsampling factor, as defined in the JPEG standard. Signature: Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t   decode_params Input Host The decode output parameter handle. int   offset_x Input Host Image offset along the horizontal direction relative to the top left corner. int   offset_y Input Host Image offset along the vertical direction relative to the top left corner. int   roi_width Input Host Image width relative to  offset_x . int   roi_height Input Host Image height relative to  offset_y . Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . If enabled, the nvJPEG library assumes that the JPEG with 4 encoded color components is in CMYK colorspace, and enables the conversion to RGB/YUV colorspace. The CMYK-to-RGB conversion is disabled by default. The conversion is based on the subtractive scheme—this behavior matches OpenCV’s handling of 4-component JPEGs. Signature: Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t   decode_params Input Host Decode output parameter handle. int   allow_cmyk Input Host Enable CMYK to RGB conversion. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . Allows the user to scale decode output. Note This feature is currently supported only when nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE. Signature: Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t   decode_params Input Host Decode output parameter handle. nvjpegScaleFactor_t   scale_factor Input Host Set the scaling factor for the decode output. The scale factor is set to NVJPEG_SCALE_NONE by default. The supported values are listed  here . When setting a scale factor value, the recommended allocation of the destination parameters is as follows: Use  nvjpegGetImageInfo() , or  nvjpegJpegStreamGetFrameDimensions()  to extract the dimensions of each channel. Let height[NVJPEG_MAX_COMPONENT] and width[NVJPEG_MAX_COMPONENT] be 2 arrays which store the height and width. The index to these arrays correspond to the channel id. For a channel c, the scaled dimensions are calculated as follows: scaled_height[c] = (height[c] + rounding_factor - 1)/rounding_factor scaled_width[c] = (width[c] + rounding_factor - 1)/rounding_factor when scale_factor = NVJPEG_SCALE_NONE, rounding_factor = 1 when scale_factor = NVJPEG_SCALE_1_BY_2, rounding_factor = 2 when scale_factor = NVJPEG_SCALE_1_BY_4, rounding_factor = 4 when scale_factor = NVJPEG_SCALE_1_BY_8, rounding_factor = 8 For the output_format: NVJPEG_OUTPUT_Y destination.pitch[0] should be at least:  width[0] destination.channel[0] should be at least of size:  destination.pitch[0]*height[0] For the output_format destination.pitch[c] should be at least: destination.channel[c] should be at least of size: NVJPEG_OUTPUT_YUV width[c] for c = 0, 1, 2 destination.pitch[c]*height[c] for c = 0, 1, 2 NVJPEG_OUTPUT_RGB and NVJPEG_OUTPUT_BGR width[0] for c = 0, 1, 2 destination.pitch[0]*height[0] for c = 0, 1, 2 NVJPEG_OUTPUT_RGBI and NVJPEG_OUTPUT_BGRI width[0]*3 destination.pitch[0]*height[0] NVJPEG_OUTPUT_UNCHANGED width[c] for c = [ 0, nComponents - 1 ] destination.pitch[c]*height[c] for c = [ 0, nComponents - 1] Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . This function is used to generate the decoded output based on the exif orientation parameter. When ExifOrientation is enabled, the output buffers should be allocated based on the rotated dimensions. If the orientation is set as  NVJPEG_ORIENTATION_UNKNOWN , the library will default to  NVJPEG_ORIENTATION_HORIZONTAL . ROI Decode and EXIF rotation Exif rotation and ROI Decode can be enabled together. The ROI coordinates should be in the rotated space. Signature: Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t   decode_params Input Host Decode output parameter handle. nvjpegExifOrientation_t   orientation Input Host Set the exif orientation for the decode output. Returns: nvjpegStatus_t  — An error code as specified in  nvJPEG API Return Codes . The nvJPEG API adheres to the following return codes and their indicators: Description of the returned error codes: Returned Error (Returned Code) Description NVJPEG_STATUS_SUCCESS   (0) The API call has finished successfully. Note that many of the calls are asynchronous and some of the errors may be seen only after synchronization. NVJPEG_STATUS_NOT_INITIALIZED   (1) The library handle was not initialized. A call to  nvjpegCreate()  is required to initialize the handle. NVJPEG_STATUS_INVALID_PARAMETER   (2) Wrong parameter was passed. For example, a null pointer as input data, or an image index not in the allowed range. NVJPEG_STATUS_BAD_JPEG   (3) Cannot parse the JPEG stream. Check that the encoded JPEG stream and its size parameters are correct. NVJPEG_STATUS_JPEG_NOT_SUPPORTED   (4) Attempting to decode a JPEG stream that is not supported by the nvJPEG library. NVJPEG_STATUS_ALLOCATOR_FAILURE   (5) The user-provided allocator functions, for either memory allocation or for releasing the memory, returned a non-zero code. NVJPEG_STATUS_EXECUTION_FAILED   (6) Error during the execution of the device tasks. NVJPEG_STATUS_ARCH_MISMATCH   (7) The device capabilities are not enough for the set of input parameters provided (input parameters such as backend, encoded stream parameters, output format). NVJPEG_STATUS_INTERNAL_ERROR   (8) Error during the execution of the device tasks. NVJPEG_STATUS_IMPLEMENTATION_NOT_SUPPORTED   (9) Not supported. NVJPEG_STATUS_INCOMPLETE_BITSTREAM   (10) Bitstream input data incomplete One of the outputs of the  nvjpegGetImageInfo()  API is  nvjpegChromaSubsampling_t . This parameter is an  enum  type, and its enumerator list comprises of the chroma subsampling property retrieved from the encoded JPEG image. The  nvjpegGetImageInfo()  function currently supports the following chroma subsampling types: Refer to the JPEG standard:  https://jpeg.org/jpeg/ nvJPEG Decode sample can be found here:  https://github.com/NVIDIA/CUDALibrarySamples/tree/master/nvJPEG/nvJPEG-Decoder This section describes the encoding functions of the nvJPEG Library. The user should perform the below prerequisite steps before calling the nvJPEG encoding functions. See also  nvJPEG Encoder Helper API Reference . The user should create an encoding parameters structure with  nvjpegEncoderParamsCreate()  function. The function will be initialized with default parameters. User can use an appropriate  nvjpegEncoderParamsSet*()  function to set a specific parameter. The quality parameter can be set, using the  nvjpegEncoderParamsSetQuality()  function, to an integer value between 1 and 100, and this quality parameter will be used as a base for generating the JPEG quantization tables. Note Occasionally, when encoding high entropy input data, such as random images, the encoding can fail if the quality parameter is set too high. This is due to the fact that the compressed bitstream would be larger than the input image. We recommend restarting the encoding with slightly lower quality factor or using a real-world images if possible. The parameters structure should be passed to compression functions. Note The encoding parameters structure can be reused to compress multiple images simultaneously, but no changes to the parameters should be made during the ongoing encoding, or the encoding result will be undefined. The user should create the encoding state structure using  nvjpegEncoderStateCreate()  function. This function will hold intermediate buffers for the encoding process. This state should be passed to the compression functions. Note The encoding state structure can be reused to encode a series of images, but no encoding should be performed on multiple images with the same encoding state at the same time—otherwise the result of the encodings will be undefined. The nvJPEG library provides a few interfaces for compressing the image in different formats and colorspaces. See below. Input for this function is an image in YUV colorspace. See  nvjpegEncodeYUV() . The  source  argument should be filled with the corresponding YUV planar data. The  chroma_subsampling  argument should have the chroma subsampling of the input data. If the chroma subsampling in the encoding parameters is the same as input chroma subsampling, then the user’s input data will be directly used in the JPEG compression. Otherwise chroma will be resampled to match the chroma subsampling of the encoding parameters. Input data should be provided with respect to the subsampling factors. That is, the chrominance image planes should have sizes aligned to the corresponding subsamplings. For example: Image dimensions: 123x321 Input chroma subsampling: NVJPEG_CSS_410 Chroma subsampling factor for this chroma subsampling: 4x2 Given the above, the encoder library expects the user to provide: Y plane with size: 123 x 321 Cb and Cr plane with size: 31 x 161 See  nvjpegEncodeImage() . Input for this function, i.e., how data should be provided in the  source  argument, is determined by the  input_format  argument. For the interleaved formats (ending with  I ) only the first channel is used. For the non-interleaved formats, all the channels in the input format are used. For example, if the user has interleaved the RGB image of size  W   x   H , stored continuously, and the pointer to it is  pImage , then  source  should be: source.channel[0]   =   pImage source.pitch[0]   =   W*3 When the same image is stored in planar format, with image planes pointers stored continuously in the array  pImage[3] , then  source  should be: source.channel[0]   =   pImage[0] source.channel[1]   =   pImage[1] source.channel[2]   =   pImage[2] The  pitch  values for each channel in the  source  parameter should be set accordingly to the data layout. The nvJPEG library will perform the color transformation to the YCbCr, and will compress the result. Often it is not feasible to accurately predict the final compressed data size of the final JPEG stream for any input data and parameters. The nvJPEG library, while encoding, will calculate the size of the final stream, allocate temporary buffer in the encoder state and save the compressed data in the encoding state’s buffer. In order to get final compressed JPEG stream, the user should provide the memory buffer large enough to store this compressed data. There are two options for how to do this: Use the upper bound on compressed JPEG stream size for the given parameters and image dimensions: Use the  nvjpegEncodeRetrieveBitstream()  function to retrieve the maximum possible JPEG stream size at any given time. Allocate the memory buffer at any given time. Encode the image using one of the encoding functions. Retrieve the compressed JPEG stream from the encoder state after successful encoding, using the  nvjpegEncodeRetrieveBitstream()  and the allocated buffer. Wait for the encoding to complete, and retrieve the exact size of required buffer, as below: Encode the image using one of the encoding functions. Use the  nvjpegEncodeRetrieveBitstream()  function to retrieve the size in bytes of the compressed JPEG stream. Allocate the memory buffer of at least this size. Use the  nvjpegEncodeRetrieveBitstream()  function to populate your buffer with the compressed JPEG stream. Note As the same encoding image state can be reused to compress a series of images, the  nvjpegEncodeRetrieveBitstream()  function will return the result for the last compressed image. See below the example code, and the block diagram shown in  Figure 1 , for encoding with nvJPEG Encoder. JPEG Encoding Using nvJPEG Encoder \\uf0c1 This section describes the nvJPEG Encoder Type Declarations. The  nvjpegInputFormat_t  enum is used to select the color model and pixel format of the input image. It is used for conversion to YCbCr during encoding. Member Description NVJPEG_INPUT_RGB Input image is in RGB color model. Pixel format is RGB. NVJPEG_INPUT_BGR Input image is in RGB color model. Pixel format is BGR. NVJPEG_INPUT_RGBI Input image is in RGB color model. Pixel format is interleaved RGB. NVJPEG_INPUT_BGRI Input image is in RGB color model. Pixel format is interleaved BGR. The  nvjpegEncoderState_t  structure stores intermediate buffers and variables used for compression. The  nvjpegEncoderParams_t  structure stores JPEG encode parameters. The nvJPEG Encoder helper functions are used for initializing. Creates encoder state that stores intermediate buffers used in compression. Signature: Parameters: Parameter Input / Output Memory Description handle Input Host Library handle encoder_state Output Host Pointer to the encoder state structure, where the new state will be placed. stream Inputt Host CUDA stream where all the required device operations will be placed. Destroys the encoder state. Signature: Parameters: Parameter Input / Output Memory Description encoder_state Input/Output Host Encoder state structure that will be released. Creates the structure that holds the compression parameters. Signature: Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_params Output Host Pointer to the location where the new parameters structure will be placed. stream Inputt Host CUDA stream where all the required device operations will be placed. Destroys the encoder parameters structure. Signature: Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder params structure that will be released. Sets the parameter quality in the encoder parameters structure. Signature: Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. etype Input Host Encoding type selection (Baseline/Progressive). Default is Baseline. stream Input Host CUDA stream where all the required device operations will be placed. Sets the parameter quality in the encoder parameters structure. Signature: Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameterss structure handle. quality Input Host Integer value of quality between 1 and 100, where 100 is the highest quality. Default value is 70. stream Input Host CUDA stream where all the required device operations will be placed. Sets whether or not to use optimized Huffman. Using optimized Huffman produces smaller JPEG bitstream sizes with the same quality, but with slower performance. Signature: Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. optimized Input Host If this value is 0 then non-optimized Huffman will be used. Otherwise optimized version will be used. Default value is 0. stream Input Host CUDA stream where all the required device operations will be placed. Sets which chroma subsampling will be used for JPEG compression. Signature: Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle. chroma_subsampling Input Host Chroma subsampling that will be used for JPEG compression. If the input is in YUV color model and  chroma_subsampling  is different from the subsampling factors of source image, then the NVJPEG library will convert subsampling to the value of  chroma_subsampling . Default value is 4:4:4. stream Input Host CUDA stream where all the required device operations will be placed. This section describes the nvJPEG Encoder API. Returns the maximum possible buffer size that is needed to store the compressed JPEG stream, for the given input parameters. Signature: Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_params Input/Output Host Encoder parameters structure handle. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. Compresses the image in YUV colorspace to JPEG stream using the provided parameters, and stores it in the state structure. Signature: Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream. encoder_params Input Host Encoder parameters structure handle. source Input Host Pointer to the  nvjpeg  structure that holds the device pointers to the  Y,   U(Cb)   and   V(Cr)  image planes and the respective strides. chroma_subsampling Input Host Chroma subsampling of the input data. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. Compresses the image in the provided format to the JPEG stream using the provided parameters, and stores it in the state structure. Signature: Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream. encoder_params Input Host Encoder parameters structure handle. source Input Host Pointer to the  nvjpeg  structure that holds the device pointers to the  Y,   U(Cb)   and   V(Cr)  image planes and the respective strides. input_format Input Host Value of  nvjpegInputFormat_t  type that describes the input data. image_width Input Host Input image width. image_height Input Host Input image height. stream Input Host CUDA stream where all the required device operations will be placed. Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. If  data  parameter is NULL then the encoder will return compressed stream size in the  length  parameter. If  data  is not NULL then the provided  length  parameter should contain the  data  buffer size. If the provided  length  is less than compressed stream size, then an error will be returned. Otherwise the compressed stream will be stored in the  data  buffer and the actual compressed buffer size will be stored in the  length  parameter. Signature: Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host The  encoder_state  that was previously used in one of the encoder functions. data Input/Output Host Pointer to the buffer in the host memory where the compressed stream will be stored. Can be NULL (see description). length Input/Output Host Pointer to the input buffer size. On return the NVJPEG library will store the actual compressed stream size in this parameter. stream Input Host CUDA stream where all the required device operations will be placed. Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions. data  parameter should be on device memory If  data  parameter is NULL then the encoder will return compressed stream size in the  length  parameter. If  data  is not NULL then the provided  length  parameter should contain the  data  buffer size. If the provided  length  is less than compressed stream size, then an error will be returned. Otherwise the compressed stream will be stored in the  data  buffer and the actual compressed buffer size will be stored in the  length  parameter. Signature: Parameters: Parameter Input / Output Memory Description handle Input Host Library handle. encoder_state Input/Output Host The  encoder_state  that was previously used in one of the encoder functions. data Input/Output Device Pointer to the buffer in the device memory where the compressed stream will be stored. Can be NULL (see description). length Input/Output Host Pointer to the input buffer size. On return the NVJPEG library will store the actual compressed stream size in this parameter. stream Input Host CUDA stream where all the required device operations will be placed. This section describes the transcoding functions of the nvJPEG Library. This section describes the nvJPEG Transcoder helper API. Copies the metadata (JFIF, APP, EXT, and COM markers) from the parsed stream. Signature: Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression. encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. Copies the quantization tables from the parsed stream. Signature: Parameters: Parameter Input / Output Memory Description encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. nvjpegEncoderParamsCopyHuffmanTables() is now deprecated. Due to precision differences in the JPEG encode/decode process, the input huffman tables may no longer be valid for the image being encoded and may result in corrupt bitstream. Signature: Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression. encode_params Out Host Encoder parameters that will be used for compression. jpeg_stream In Host Input parsed stream. stream In Host CUDA stream where all the required device operations will be placed. See below the example code. The following APIs are dropped starting CUDA 11.0 Decoupled APIs, when initialized with  NVJPEG_BACKEND_GPU_HYBRID , may not be able to correctly decode jpeg bitstreams which have out of bound run length codes. This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc. NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. \\n Privacy Policy \\n|\\n Manage My Privacy \\n|\\n Do Not Sell or Share My Data \\n|\\n Terms of Service \\n|\\n Accessibility \\n|\\n Corporate Policies \\n|\\n Product Security \\n|\\n Contact \\n \\n  Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved.\\n \\n       Last updated on Jul 1, 2024.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents written to /home/ubuntu/Steps/content.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File paths\n",
    "input_file_path = '/home/ubuntu/Steps/nvidia_docs/nvidia_docs/spiders/output.json'\n",
    "content_file_path = '/home/ubuntu/Steps/content.txt'\n",
    "waster_file_path = '/home/ubuntu/Steps/waste.txt'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Open the output files in write mode\n",
    "with open(content_file_path, 'w') as content_file:\n",
    "    # Iterate over each item in the JSON array\n",
    "    for item in data:\n",
    "        content = item['content']\n",
    "        content_file.write(content)\n",
    "\n",
    "print(f\"Contents written to {content_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Ensure you have the required nltk data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to split text into sentences considering numerical data\n",
    "def split_sentences(text):\n",
    "    # Use nltk's sent_tokenize to split sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Join sentences that were split incorrectly\n",
    "    formatted_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if re.match(r'^[0-9]+(\\.[0-9]+)+$', sentence):  # Check if the sentence is a number\n",
    "            if formatted_sentences:\n",
    "                formatted_sentences[-1] += ' ' + sentence  # Append to the previous sentence\n",
    "            else:\n",
    "                formatted_sentences.append(sentence)\n",
    "        else:\n",
    "            formatted_sentences.append(sentence)\n",
    "    \n",
    "    return formatted_sentences\n",
    "\n",
    "input_file = '/home/ubuntu/Steps/text1.txt'\n",
    "output_file = '/home/ubuntu/Steps/format.txt'\n",
    "\n",
    "# Read the entire content from the input file\n",
    "with open(input_file, 'r') as f:\n",
    "    content = f.read()\n",
    "print(content[:2])\n",
    "# Split the content into sentences\n",
    "sentences = split_sentences(content)\n",
    "\n",
    "# Write each sentence to the output file, ensuring each ends with a newline\n",
    "with open(output_file, 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence.strip() + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(content[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents written to /home/ubuntu/Steps/temp.txt\n",
      "The content file saved at /home/ubuntu/Steps/text1.txt\n"
     ]
    }
   ],
   "source": [
    "from data_prep import DataPrep\n",
    "\n",
    "\n",
    "input_path = '/home/ubuntu/Steps/nvidia_docs/nvidia_docs/spiders/output.json'\n",
    "temp_path = '/home/ubuntu/Steps/temp.txt'\n",
    "output_path = '/home/ubuntu/Steps/text1.txt'\n",
    "\n",
    "data_prep = DataPrep(input_path, temp_path, output_path)\n",
    "data_prep.get_data()\n",
    "data_prep.split_sentences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path=\"/home/ubuntu/project/Steps/documents_export.json\"\n",
    "with open(document_path,'r')as file:\n",
    "    data=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'content': 'NvidiaSearchInput.mount({\"apiUrl\": \"https://api-prod.nvidia.com/search/graphql\", \"destination\": \"search.html\", \"path\": \"/cuda/\", \"site\": \"https://docs.nvidia.com\"}); Release Notes CUDA Features Archive EULA Installation Guides Quick Start Guide Installation Guide Windows Installation Guide Linux Programming Guides Programming Guide Best Practices Guide Maxwell Compatibility Guide Pascal Compatibility Guide Volta Compatibility Guide Turing Compatibility Guide NVIDIA Ampere GPU Architecture Compatibility Guide Hopper Compatibility Guide Ada Compatibility Guide Maxwell Tuning Guide Pascal Tuning Guide Volta Tuning Guide Turing Tuning Guide NVIDIA Ampere GPU Architecture Tuning Guide Hopper Tuning Guide Ada Tuning Guide PTX ISA Video Decoder PTX Interoperability Inline PTX Assembly CUDA API References CUDA Runtime API CUDA Driver API CUDA Math API cuBLAS cuDLA API NVBLAS nvJPEG cuFFT CUB CUDA C++ Standard Library cuFile API Reference Guide cuRAND cuSPARSE NPP nvJitLink nvFatbin NVRTC (Runtime Compilation) Thrust cuSOLVER PTX Compiler API References PTX Compiler APIs Miscellaneous CUDA Demo Suite CUDA on WSL CUDA on EFLOW Multi-Instance GPU (MIG) CUDA Compatibility CUPTI Debugger API GPUDirect RDMA GPUDirect Storage vGPU Tools NVCC CUDA-GDB Compute Sanitizer Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Systems Nsight Compute Nsight Visual Studio Edition Profiler CUDA Binary Utilities White Papers Floating Point and IEEE 754 Incomplete-LU and Cholesky Preconditioned Iterative Methods Application Notes CUDA for Tegra Compiler SDK libNVVM API libdevice User’s Guide NVVM IR landing » CUDA Toolkit Documentation 12.5 Update 1 CUDA Toolkit Archive - Send Feedback CUDA Toolkit Documentation 12.5 Update 1 \\uf0c1 Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.'},\n",
       " {'id': 2,\n",
       "  'content': 'The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application. Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs. Release Notes The Release Notes for the CUDA Toolkit. CUDA Features Archive The list of CUDA features by release. EULA The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. Installation Guides \\uf0c1 Quick Start Guide This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system. Installation Guide Windows This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems. Installation Guide Linux This guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems. Programming Guides \\uf0c1 Programming Guide This guide provides a detailed discussion of the CUDA programming model and programming interface. It then describes the hardware implementation, and provides guidance on how to achieve maximum performance. The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API. Best Practices Guide This guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures. The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit. Maxwell Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture. This document provides guidance to ensure that your software applications are compatible with Maxwell. Pascal Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture. This document provides guidance to ensure that your software applications are compatible with Pascal. Volta Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture. This document provides guidance to ensure that your software applications are compatible with Volta. Turing Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture. This document provides guidance to ensure that your software applications are compatible with Turing. NVIDIA Ampere GPU Architecture Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture. This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture. Hopper Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs. This document provides guidance to ensure that your software applications are compatible with Hopper architecture. Ada Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs. This document provides guidance to ensure that your software applications are compatible with Ada architecture. Maxwell Tuning Guide Maxwell is NVIDIA’s 4th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features. Pascal Tuning Guide Pascal is NVIDIA’s 5th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features. Volta Tuning Guide Volta is NVIDIA’s 6th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features. Turing Tuning Guide Turing is NVIDIA’s 7th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features. NVIDIA Ampere GPU Architecture Tuning Guide NVIDIA Ampere GPU Architecture is NVIDIA’s 8th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture’s features. Hopper Tuning Guide Hopper GPU Architecture is NVIDIA’s 9th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture’s features. Ada Tuning Guide The NVIDIA® Ada GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features. PTX ISA This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device. Video Decoder NVIDIA Video Decoder (NVCUVID) is deprecated. Instead, use the NVIDIA Video Codec SDK ( https://developer.nvidia.com/nvidia-video-codec-sdk ). PTX Interoperability This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code. Inline PTX Assembly This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code. It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter. CUDA API References \\uf0c1 CUDA Runtime API Fields in structures might appear in order that is different from the order of declaration. CUDA Driver API Fields in structures might appear in order that is different from the order of declaration. CUDA Math API The CUDA math API. cuBLAS The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime. It allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs. cuDLA API The cuDLA API. NVBLAS The NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library. nvJPEG The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications. cuFFT The cuFFT library user guide.'},\n",
       " {'id': 3,\n",
       "  'content': 'CUB The user guide for CUB. CUDA C++ Standard Library The API reference for libcu++, the CUDA C++ standard library. cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology. cuRAND The cuRAND library user guide. cuSPARSE The cuSPARSE library user guide. NPP NVIDIA NPP is a library of functions for performing CUDA accelerated processing. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains. The NPP library is written to maximize flexibility, while maintaining high performance. nvJitLink The user guide for the nvJitLink library. nvFatbin The user guide for the nvFatbin library. NVRTC (Runtime Compilation) NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API. This facility can often provide optimizations and performance not possible in a purely offline static compilation. Thrust The C++ parallel algorithms library. cuSOLVER The cuSOLVER library user guide. PTX Compiler API References \\uf0c1 PTX Compiler APIs This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library. Miscellaneous \\uf0c1 CUDA Demo Suite This document describes the demo applications shipped with the CUDA Demo Suite. CUDA on WSL This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2). The guide covers installation and running CUDA applications and containers in this environment. Multi-Instance GPU (MIG) This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA® A100 GPU. CUDA Compatibility This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade. CUPTI The CUPTI-API. The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications. Debugger API The CUDA debugger API. GPUDirect RDMA A technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express. This document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model. GPUDirect Storage The documentation for GPUDirect Storage. vGPU vGPUs that support CUDA. Tools \\uf0c1 NVCC This is a reference document for nvcc, the CUDA compiler driver. nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process. CUDA-GDB The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware. CUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger. Compute Sanitizer The user guide for Compute Sanitizer. Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Eclipse Plugins Edition getting started guide Nsight Systems The documentation for Nsight Systems. Nsight Compute The NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. Nsight Visual Studio Edition The documentation for Nsight Visual Studio Edition. Profiler This is the guide to the Profiler. CUDA Binary Utilities The application notes for cuobjdump, nvdisasm, and nvprune. White Papers \\uf0c1 Floating Point and IEEE 754 A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs. The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide. Incomplete-LU and Cholesky Preconditioned Iterative Methods In this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively. Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms. Application Notes \\uf0c1 CUDA for Tegra This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). It also discusses EGL interoperability. Compiler SDK \\uf0c1 libNVVM API The libNVVM API. libdevice User’s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels. NVVM IR NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2007-2024, NVIDIA Corporation & affiliates.'},\n",
       " {'id': 4, 'content': 'All rights reserved.'},\n",
       " {'id': 5,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(true); });1. CUDA 12.5 Update 1 Release Notes 1.1.'},\n",
       " {'id': 6,\n",
       "  'content': 'CUDA Toolkit Major Component Versions 1.2. New Features 1.2.1. General CUDA 1.2.2. CUDA Compiler 1.2.3. CUDA Developer Tools 1.3. Resolved Issues 1.3.1. CUDA Compiler 1.4. Known Issues and Limitations 1.5.'},\n",
       " {'id': 7,\n",
       "  'content': 'Deprecated or Dropped Features 1.5.1. Deprecated or Dropped Architectures 1.5.2. Deprecated Operating Systems 1.5.3. Deprecated Toolchains 1.5.4. CUDA Tools 2. CUDA Libraries 2.1. cuBLAS Library 2.1.1. cuBLAS: Release 12.5 Update 1 2.1.2. cuBLAS: Release 12.5 2.1.3. cuBLAS: Release 12.4 Update 1 2.1.4. cuBLAS: Release 12.4 2.1.5. cuBLAS: Release 12.3 Update 1 2.1.6. cuBLAS: Release 12.3 2.1.7. cuBLAS: Release 12.2 Update 2 2.1.8. cuBLAS: Release 12.2 2.1.9. cuBLAS: Release 12.1 Update 1 2.1.10. cuBLAS: Release 12.0 Update 1 2.1.11. cuBLAS: Release 12.0 2.2. cuFFT Library 2.2.1. cuFFT: Release 12.5 2.2.2. cuFFT: Release 12.4 Update 1 2.2.3. cuFFT: Release 12.4 2.2.4. cuFFT: Release 12.3 Update 1 2.2.5. cuFFT: Release 12.3 2.2.6. cuFFT: Release 12.2 2.2.7. cuFFT: Release 12.1 Update 1 2.2.8. cuFFT: Release 12.1 2.2.9. cuFFT: Release 12.0 Update 1 2.2.10. cuFFT: Release 12.0 2.3. cuSOLVER Library 2.3.1. cuSOLVER: Release 12.5 Update 1 2.3.2. cuSOLVER: Release 12.5 2.3.3. cuSOLVER: Release 12.4 Update 1 2.3.4. cuSOLVER: Release 12.4 2.3.5. cuSOLVER: Release 12.2 Update 2 2.3.6. cuSOLVER: Release 12.2 2.4. cuSPARSE Library 2.4.1. cuSPARSE: Release 12.5 Update 1 2.4.2. cuSPARSE: Release 12.5 2.4.3. cuSPARSE: Release 12.4 2.4.4. cuSPARSE: Release 12.3 Update 1 2.4.5. cuSPARSE: Release 12.3 2.4.6. cuSPARSE: Release 12.2 Update 1 2.4.7. cuSPARSE: Release 12.1 Update 1 2.4.8. cuSPARSE: Release 12.0 Update 1 2.4.9. cuSPARSE: Release 12.0 2.5. Math Library 2.5.1. CUDA Math: Release 12.5 2.5.2. CUDA Math: Release 12.4 2.5.3. CUDA Math: Release 12.3 2.5.4. CUDA Math: Release 12.2 2.5.5. CUDA Math: Release 12.1 2.5.6. CUDA Math: Release 12.0 2.6. NVIDIA Performance Primitives (NPP) 2.6.1. NPP: Release 12.4 2.6.2.'},\n",
       " {'id': 8,\n",
       "  'content': 'NPP: Release 12.0 2.7. nvJPEG Library 2.7.1. nvJPEG: Release 12.4 2.7.2. nvJPEG: Release 12.3 Update 1 2.7.3. nvJPEG: Release 12.2 2.7.4. nvJPEG: Release 12.0 3. Notices 3.1.'},\n",
       " {'id': 9,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Release Notes » 1. CUDA 12.5 Update 1 Release Notes v12.5 | PDF | Archive NVIDIA CUDA Toolkit Release Notes The Release Notes for the CUDA Toolkit. CUDA 12.5 Update 1 Release Notes \\uf0c1 The release notes for the NVIDIA® CUDA® Toolkit can be found online at https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html . Note The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12.x releases. 1.1. CUDA Toolkit Major Component Versions \\uf0c1 CUDA Components Starting with CUDA 11, the various components in the toolkit are versioned independently. For CUDA 12.5 Update 1, the table below indicates the versions: Table 1 CUDA 12.5 Update 1 Component Versions \\uf0c1 Component Name Version Information Supported Architectures Supported Platforms CUDA C++ Core Compute Libraries Thrust 2.4.0 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUB 2.4.0 libcu++ 2.4.0 Cooperative Groups 12.5.82 CUDA Compatibility 12.5.36505571 aarch64-jetson Linux CUDA Runtime (cudart) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuobjdump 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUPTI 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuxxfilt (demangler) 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA Demo Suite 12.5.82 x86_64 Linux, Windows CUDA GDB 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, WSL CUDA Nsight Eclipse Plugin 12.5.82 x86_64 Linux CUDA NVCC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvdisasm 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows CUDA NVML Headers 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvprof 12.5.82 x86_64 Linux, Windows CUDA nvprune 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVRTC 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL NVTX 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NVVP 12.5.82 x86_64, Linux, Windows CUDA OpenCL 12.5.39 x86_64 Linux, Windows CUDA Profiler API 12.5.39 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA Compute Sanitizer API 12.5.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuBLAS 12.5.3.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL cuDLA 12.5.82 aarch64-jetson Linux CUDA cuFFT 11.2.3.61 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuFile 1.10.1.7 x86_64, arm64-sbsa, aarch64-jetson Linux CUDA cuRAND 10.3.6.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSOLVER 11.6.3.83 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA cuSPARSE 12.5.1.3 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA NPP 12.3.0.159 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvFatbin 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJitLink 12.5.82 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL CUDA nvJPEG 12.3.2.81 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL Nsight Compute 2024.2.1.2 x86_64, arm64-sbsa, aarch64-jetson Linux, Windows, WSL (Windows 11) Nsight Systems 2024.2.3.38 x86_64, arm64-sbsa, Linux, Windows, WSL Nsight Visual Studio Edition (VSE) 2024.2.1.24155 x86_64 (Windows) Windows nvidia_fs 1 2.20.6 x86_64, arm64-sbsa, aarch64-jetson Linux Visual Studio Integration 12.5.82 x86_64 (Windows) Windows NVIDIA Linux Driver 555.42.06 x86_64, arm64-sbsa Linux NVIDIA Windows Driver 555.85 x86_64 (Windows) Windows, WSL CUDA Driver Running a CUDA application requires the system with at least one CUDA capable GPU and a driver that is compatible with the CUDA Toolkit. See Table 3 . For more information various GPU products that are CUDA capable, visit https://developer.nvidia.com/cuda-gpus . Each release of the CUDA Toolkit requires a minimum version of the CUDA driver. The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases. More information on compatibility can be found at https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades . Note : Starting with CUDA 11.0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below. The minimum required driver version for CUDA minor version compatibility is shown below. CUDA minor version compatibility is described in detail in https://docs.nvidia.com/deploy/cuda-compatibility/index.html Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility \\uf0c1 CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.x >=525.60.13 >=528.33 CUDA 11.8.x CUDA 11.7.x CUDA 11.6.x CUDA 11.5.x CUDA 11.4.x CUDA 11.3.x CUDA 11.2.x CUDA 11.1.x >=450.80.02 >=452.39 CUDA 11.0 (11.0.3) >=450.36.06** >=451.22** * Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode – please read the CUDA Compatibility Guide for details. ** CUDA 11.0 was released with an earlier driver version, but by upgrading to Tesla Recommended Drivers 450.80.02 (Linux) / 452.39 (Windows), minor version compatibility is possible across the CUDA 11.x family of toolkits. The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below. Table 3 CUDA Toolkit and Corresponding Driver Versions \\uf0c1 CUDA Toolkit Toolkit Driver Version Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12.5 Update 1 >=555.42.06 >=555.85 CUDA 12.5 GA >=555.42.02 >=555.85 CUDA 12.4 Update 1 >=550.54.15 >=551.78 CUDA 12.4 GA >=550.54.14 >=551.61 CUDA 12.3 Update 1 >=545.23.08 >=546.12 CUDA 12.3 GA >=545.23.06 >=545.84 CUDA 12.2 Update 2 >=535.104.05 >=537.13 CUDA 12.2 Update 1 >=535.86.09 >=536.67 CUDA 12.2 GA >=535.54.03 >=536.25 CUDA 12.1 Update 1 >=530.30.02 >=531.14 CUDA 12.1 GA >=530.30.02 >=531.14 CUDA 12.0 Update 1 >=525.85.12 >=528.33 CUDA 12.0 GA >=525.60.13 >=527.41 CUDA 11.8 GA >=520.61.05 >=520.06 CUDA 11.7 Update 1 >=515.48.07 >=516.31 CUDA 11.7 GA >=515.43.04 >=516.01 CUDA 11.6 Update 2 >=510.47.03 >=511.65 CUDA 11.6 Update 1 >=510.47.03 >=511.65 CUDA 11.6 GA >=510.39.01 >=511.23 CUDA 11.5 Update 2 >=495.29.05 >=496.13 CUDA 11.5 Update 1 >=495.29.05 >=496.13 CUDA 11.5 GA >=495.29.05 >=496.04 CUDA 11.4 Update 4 >=470.82.01 >=472.50 CUDA 11.4 Update 3 >=470.82.01 >=472.50 CUDA 11.4 Update 2 >=470.57.02 >=471.41 CUDA 11.4 Update 1 >=470.57.02 >=471.41 CUDA 11.4.0 GA >=470.42.01 >=471.11 CUDA 11.3.1 Update 1 >=465.19.01 >=465.89 CUDA 11.3.0 GA >=465.19.01 >=465.89 CUDA 11.2.2 Update 2 >=460.32.03 >=461.33 CUDA 11.2.1 Update 1 >=460.32.03 >=461.09 CUDA 11.2.0 GA >=460.27.03 >=460.82 CUDA 11.1.1 Update 1 >=455.32 >=456.81 CUDA 11.1 GA >=455.23 >=456.38 CUDA 11.0.3 Update 1 >= 450.51.06 >= 451.82 CUDA 11.0.2 GA >= 450.51.05 >= 451.48 CUDA 11.0.1 RC >= 450.36.06 >= 451.22 CUDA 10.2.89 >= 440.33 >= 441.22 CUDA 10.1 (10.1.105 general release, and updates) >= 418.39 >= 418.96 CUDA 10.0.130 >= 410.48 >= 411.31 CUDA 9.2 (9.2.148 Update 1) >= 396.37 >= 398.26 CUDA 9.2 (9.2.88) >= 396.26 >= 397.44 CUDA 9.1 (9.1.85) >= 390.46 >= 391.29 CUDA 9.0 (9.0.76) >= 384.81 >= 385.54 CUDA 8.0 (8.0.61 GA2) >= 375.26 >= 376.51 CUDA 8.0 (8.0.44) >= 367.48 >= 369.30 CUDA 7.5 (7.5.16) >= 352.31 >= 353.66 CUDA 7.0 (7.0.28) >= 346.46 >= 347.62 For convenience, the NVIDIA driver is installed as part of the CUDA Toolkit installation. Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs.'},\n",
       " {'id': 10,\n",
       "  'content': 'For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https://www.nvidia.com/drivers . During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages). For more information on customizing the install process on Windows, see https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html#install-cuda-software . For meta packages on Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-metas . 1.2. New Features \\uf0c1 This section lists new general CUDA and CUDA compilers features. 1.2.1. General CUDA \\uf0c1 In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option. End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules. MPS (Multi-process service) is now supported on L4T and embedded-Linux Tegra platforms. More details can be found here .'},\n",
       " {'id': 11,\n",
       "  'content': '1.2.2. CUDA Compiler \\uf0c1 For changes to PTX, refer to https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-isa-version-8-5 . 1.2.3. CUDA Developer Tools \\uf0c1 For changes to nvprof and Visual Profiler, see the changelog . For new features, improvements, and bug fixes in Nsight Systems, see the changelog . For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog . For new features, improvements, and bug fixes in CUPTI, see the changelog . For new features, improvements, and bug fixes in Nsight Compute, see the changelog . For new features, improvements, and bug fixes in Compute Sanitizer, see the changelog . For new features, improvements, and bug fixes in CUDA-GDB, see the changelog . 1.3. Resolved Issues \\uf0c1 1.3.1. CUDA Compiler \\uf0c1 Resolved an issue found when trying sm89 ptx of FP8 gemm kernel compiled by 12.4 when run on an sm90 device. Resolved an issue in which nvcc failed to compile any CUDA code when specifying C++20 with CUDA 12.5 and Visual Studio 2022 17.10.0. Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler. Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag. Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops. Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to “Misaligned shared or local address”. Fix to correct the calculation of write-after-read hazard latency.'},\n",
       " {'id': 12,\n",
       "  'content': '1.4. Known Issues and Limitations \\uf0c1 Runfile will not be supported for Amazon Linux 2023. Confidential Computing is not supported on CUDA 12.5. Please continue to use CUDA 12.4 and drivers r550.xx to use these features. Launching Cooperative Group kernels with MPS is not supported on Tegra platforms. 1.5. Deprecated or Dropped Features \\uf0c1 Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release. We recommend that developers employ alternative solutions to these features in their software. 1.5.1. Deprecated or Dropped Architectures \\uf0c1 NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12.5. 1.5.2. Deprecated Operating Systems \\uf0c1 NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12.5. CUDA 12.5 is the last release to support Debian 10. Support for Microsoft Windows 10 21H2 and Microsoft Windows 10 21H2 (SV1) is deprecated. 1.5.3. Deprecated Toolchains \\uf0c1 CUDA Toolkit 12.4 deprecated support for the following host compilers: Microsoft Visual C/C++ (MSVC) 2017 All GCC versions prior to GCC 7.3 1.5.4. CUDA Tools \\uf0c1 Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release. 2. CUDA Libraries \\uf0c1 This section covers CUDA Libraries release notes for 12.x releases. CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host. 2.1.'},\n",
       " {'id': 13,\n",
       "  'content': 'cuBLAS Library \\uf0c1 2.1.1. cuBLAS: Release 12.5 Update 1 \\uf0c1 New Features Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs. Known Issues The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases. A workaround is to implement batching manually.'},\n",
       " {'id': 14,\n",
       "  'content': 'This will be fixed in a future release. cublasGemmGroupedBatchedEx and cublasgemmGroupedBatched have large CPU overheads. This will be addressed in an upcoming release. Resolved Issues Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error. cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are out-of-place (C != D). 2.1.2. cuBLAS: Release 12.5 \\uf0c1 New Features cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs. This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type. Refer to cublasGemmGroupedBatchedEx for more details. Known Issues cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types. Resolved Issues cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter. For instance, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results. cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv. 2.1.3. cuBLAS: Release 12.4 Update 1 \\uf0c1 Known Issues Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail. This will be fixed in an upcoming release. cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter. For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results. Resolved Issues cublasLtMatmul ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error. In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 ( CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 ). Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul() , cublasLtMatmulAlgoCheck() , and cublasLtMatmulAlgoGetHeuristic() . The issue was introduced in CUDA Toolkit 12.4. cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG). The issue was introduced in cuBLAS 11.8. 2.1.4. cuBLAS: Release 12.4 \\uf0c1 New Features cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision. Single precision also supports the math mode, CUBLAS_TF32_TENSOR_OP_MATH . Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta). Please see gemmGroupedBatched for more details. Known Issues When the current context has been created using cuGreenCtxCreate() , cuBLAS does not properly detect the number of SMs available. The user may provide the corrected SM count to cuBLAS using an API such as cublasSetSmCountTarget() . BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE . This is the same known issue documented in cuBLAS 12.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace. The issue exists since cuBLAS 11.3 Update 1. cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided. The issue exists since cuBLAS 11.6. When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync . However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.5. cuBLAS: Release 12.3 Update 1 \\uf0c1 New Features Improved performance of heuristics cache for workloads that have a high eviction rate. Known Issues BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE . The expected behavior is that the corresponding computations would be skipped. You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped. If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST . Resolved Issues cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or CUDA Toolkit 12.3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute() . Fixed creation of cuBLAS or cuBLASLt handles on Hopper GPUs under the Multi-Process Service (MPS). cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient. 2.1.6. cuBLAS: Release 12.3 \\uf0c1 New Features Improved performance on NVIDIA L40S Ada GPUs. Known Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1.0, the C and D matrices are the same, the epilogue contains GELU activation function. When an application compiled with cuBLASLt from CUDA Toolkit 12.2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12.2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute() . To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit() . 2.1.7. cuBLAS: Release 12.2 Update 2 \\uf0c1 New Features cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel. It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times. This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable. 2.1.8. cuBLAS: Release 12.2 \\uf0c1 Known Issues cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%. There is currently no workaround for this issue. Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE . The kernels apply the first batch’s bias vector to all batches. 2.1.9. cuBLAS: Release 12.1 Update 1 \\uf0c1 New Features Support for FP8 on NVIDIA Ada GPUs. Improved performance on NVIDIA L4 Ada GPUs. Introduced an API that instructs the cuBLASLt library to not use some CPU instructions. This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance. Refer to https://docs.nvidia.com/cuda/cublas/index.html#disabling-cpu-instructions . Known Issues When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure). As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses. If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function. The same applies to cublasLtMatmulDesc_t and cublasLtMatrixTransformDesc_t . The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes. 2.1.10. cuBLAS: Release 12.0 Update 1 \\uf0c1 New Features Improved performance on NVIDIA H100 SXM and NVIDIA H100 PCIe GPUs. Known Issues For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB). In the current and previous releases, cuBLAS allocates 256 MiB. This will be addressed in a future release. A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture. Resolved Issues Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache. This began in the CUDA Toolkit 12.0 release. Added forward compatible single precision complex GEMM that does not require workspace. 2.1.11. cuBLAS: Release 12.0 \\uf0c1 New Features cublasLtMatmul now supports FP8 with a non-zero beta. Added int64 APIs to enable larger problem sizes; refer to 64-bit integer interface . Added more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux. Known Issues There are no forward compatible kernels for single precision complex gemms that do not require workspace. Support will be added in a later release. Resolved Issues Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE ) could return incorrect results for the bias gradient. cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and CUBLASLT_EPILOGUE_BIAS epilogues. Deprecations Disallow including cublas.h and cublas_v2.h in the same translation unit. Removed: CUBLAS_MATMUL_STAGES_16x80 and CUBLAS_MATMUL_STAGES_64x80 from cublasLtMatmulStages_t . No kernels utilize these stages anymore. cublasLt3mMode_t , CUBLASLT_MATMUL_PREF_MATH_MODE_MASK , and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t . Instead, use the corresponding flags from cublasLtNumericalImplFlags_t . CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK , CUBLASLT_MATMUL_PREF_EPILOGUE_MASK , and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t . The corresponding parameters are taken directly from cublasLtMatmulDesc_t . CUBLASLT_POINTER_MODE_MASK_NO_FILTERING from cublasLtPointerModeMask_t . This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed. 2.2.'},\n",
       " {'id': 15,\n",
       "  'content': 'cuFFT Library \\uf0c1 2.2.1. cuFFT: Release 12.5 \\uf0c1 New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes . We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance. You can enable JIT LTO kernels using the per-plan properties cuFFT API. 2.2.2. cuFFT: Release 12.4 Update 1 \\uf0c1 Resolved Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ) in CUDA 12.4. This routine has now been removed from the header.'},\n",
       " {'id': 16,\n",
       "  'content': '2.2.3. cuFFT: Release 12.4 \\uf0c1 New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing. Added per-plan properties to the cuFFT API. These new routines can be leveraged to give users more control over the behavior of cuFFT. Currently they can be used to enable JIT LTO kernels for 64-bit FFTs. Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes. Known Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt.h ). This routine is not supported by cuFFT, and will be removed from the header in a future release. Resolved Issues Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i.e.'},\n",
       " {'id': 17,\n",
       "  'content': 'using the ostride component of the Advanced Data Layout API ). Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL . From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension. 2.2.4. cuFFT: Release 12.3 Update 1 \\uf0c1 Known Issues Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior. This issue will be fixed in an upcoming release of cuFFT. Resolved Issues Complex-to-complex (C2C) execution functions ( cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context. 2.2.5. cuFFT: Release 12.3 \\uf0c1 New Features Callback kernels are more relaxed in terms of resource usage, and will use fewer registers. Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127.'},\n",
       " {'id': 18, 'content': 'Slightly improved planning times for some FFT sizes.'},\n",
       " {'id': 19,\n",
       "  'content': '2.2.6. cuFFT: Release 12.2 \\uf0c1 New Features cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs . Improved performance of 1000+ of FFTs of sizes ranging from 62 to 16380. The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT. Reduced the size of the static libraries when compared to cuFFT in the 12.1 release. Resolved Issues cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive. cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently. 2.2.7. cuFFT: Release 12.1 Update 1 \\uf0c1 Known Issues cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy ) and another thread calls any API (except cufftCreate or cufftDestroy ), and when the total number of plans alive exceeds 1023. cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans. 2.2.8. cuFFT: Release 12.1 \\uf0c1 New Features Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800. The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout. Known Issues Starting from CUDA 11.8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms. An upcoming release will update the cuFFT callback implementation, removing this limitation. cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11.4. Resolved Issues cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit. 2.2.9. cuFFT: Release 12.0 Update 1 \\uf0c1 Resolved Issues Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced. 2.2.10. cuFFT: Release 12.0 \\uf0c1 New Features PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures. Known Issues cuFFT plan generation time increases due to PTX JIT compiling. Refer to Plan Initialization TIme . Resolved Issues cuFFT plans had an unintentional small memory overhead (of a few kB) per plan. This is resolved.'},\n",
       " {'id': 20,\n",
       "  'content': \"2.3. cuSOLVER Library \\uf0c1 2.3.1. cuSOLVER: Release 12.5 Update 1 \\uf0c1 Resolved Issues The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved. 2.3.2. cuSOLVER: Release 12.5 \\uf0c1 New Features Performance improvements of cusolverDnXgesvd and cusolverDngesvd if jobu != 'N' or jobvt != 'N' . Performance improvements of cusolverDnXgesvdp if jobz = CUSOLVER_EIG_MODE_NOVECTOR . Lower workspace requirement of cusolverDnXgesvdp for tall-and-skinny-matrices. Known Issues With CUDA Toolkit 12.4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice . As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)) , with auto ALIGN_32 = []( int64_t val ) { return (( val + 31 ) / 32 ) * 32 ; }; and auto sizeofCudaDataType = []( cudaDataType dt ) { if ( dt == CUDA_R_32F ) return sizeof ( float ); if ( dt == CUDA_R_64F ) return sizeof ( double ); if ( dt == CUDA_C_32F ) return sizeof ( cuComplex ); if ( dt == CUDA_C_64F ) return sizeof ( cuDoubleComplex ); }; 2.3.3. cuSOLVER: Release 12.4 Update 1 \\uf0c1 New Features The performance of cusolverDnXlarft has been improved. For large matrices, the speedup might exceed 100x. The performance on H100 is now consistently better than on A100. The change in cusolverDnXlarft also results in a modest speedup in cusolverDnormqr , cusolverDnormtr , and cusolverDnXsyevd . The performance of cusolverDnXgesvd when singular vectors are sought has been improved. The job configuration that computes both left and right singular vectors is up to 1.5x faster. Resolved Issues cusolverDnXtrtri_bufferSize now returns the correct workspace size in bytes. Deprecations Using long-deprecated cusolverDnPotrf , cusolverDnPotrs , cusolverDnGeqrf , cusolverDnGetrf , cusolverDnGetrs , cusolverDnSyevd , cusolverDnSyevdx , cusolverDnGesvd , and their accompanying bufferSize functions will result in a deprecation warning. The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf , cusolverDnXpotrs , cusolverDnXgeqrf , cusolverDnXgetrf , cusolverDnXgetrs , cusolverDnXsyevd , cusolverDnXsyevdx , cusolverDnXgesvd , and the corresponding bufferSize functions instead. 2.3.4. cuSOLVER: Release 12.4 \\uf0c1 New Features cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced. cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes. Known Issues cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size. As a workaround the returned size can be multiplied by the size of the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size. 2.3.5. cuSOLVER: Release 12.2 Update 2 \\uf0c1 Resolved Issues Fixed an issue with cusolverDngesvd() , cusolverDnGesvd() , and cusolverDnXgesvd() , which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘ N ’. 2.3.6.\"},\n",
       " {'id': 21,\n",
       "  'content': 'cuSOLVER: Release 12.2 \\uf0c1 New Features A new API to ensure deterministic results or allow non-deterministic results for improved performance. See cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode() . Affected functions are: cusolverDngeqrf() , cusolverDnsyevd() , cusolverDnsyevdx() , cusolverDngesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvdr() , and cusolverDnXgesvdp() . Known Issues Concurrent executions of cusolverDngetrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock. 2.4.'},\n",
       " {'id': 22,\n",
       "  'content': 'cuSPARSE Library \\uf0c1 2.4.1. cuSPARSE: Release 12.5 Update 1 \\uf0c1 New Features Added support for BSR format in cusparseSpMM . Resolved Issues cusparseSpMM() would sometimes get incorrect results when alpha=0 , num_batches>1 , batch_stride indicates that there is padding between batches. cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1). cusparseSpMM returned the wrong result when k=0 (for example when A has zero columns). The correct behavior is doing C \\\\*= beta . The bug behavior was not modifying C at all. cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows. Sliced-ELLPACK cusparseSpSV produced wrong results for diagonal matrices. Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes. 2.4.2. cuSPARSE: Release 12.5 \\uf0c1 New Features Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector. Resolved Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. 2.4.3. cuSPARSE: Release 12.4 \\uf0c1 New Features Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess() . Added support for mixed real and complex types for cusparseSpMM() . Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM() . Known Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes. Resolved Issues cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros. 2.4.4. cuSPARSE: Release 12.3 Update 1 \\uf0c1 New Features Added support for block sizes of 64 and 128 in cusparseSDDMM() . Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage. 2.4.5. cuSPARSE: Release 12.3 \\uf0c1 New Features The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector. The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values. Known Issues The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous. Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A. Resolved Issues cusparseSpSV() provided indeterministic results in some cases. Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment. Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN. 2.4.6. cuSPARSE: Release 12.2 Update 1 \\uf0c1 New Features The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes. See logging API https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-logging-api . Resolved Issues Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process. Clarified the supported operations for cusparseSDDMM() . cusparseCreateConstSlicedEll() now uses const pointers. Fixed wrong results in rare edge cases of cusparseCsr2CscEx2() with base 1 indexing. cusparseSpSM_bufferSize() could ask slightly less memory than needed. cusparseSpMV() now checks the validity of the buffer pointer only when it is strictly needed. Deprecations Several legacy APIs have been officially deprecated.'},\n",
       " {'id': 23,\n",
       "  'content': 'A compile-time warning has been added to all of them.'},\n",
       " {'id': 24,\n",
       "  'content': '2.4.7. cuSPARSE: Release 12.1 Update 1 \\uf0c1 New Features Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine ( cusparseSDDMM ). Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication ( cusparseSpMV ) and triangular solver with a single right-hand side ( cusparseSpSV ). Added a new API call ( cusparseSpSV_updateMatrix ) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step. 2.4.8. cuSPARSE: Release 12.0 Update 1 \\uf0c1 New Features cusparseSDDMM() now supports mixed precision computation. Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs. Improved cusparseSpMV() performance with a new load balancing algorithm. cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address. Resolved Issues cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows. 2.4.9. cuSPARSE: Release 12.0 \\uf0c1 New Features JIT LTO functionalities ( cusparseSpMMOp() ) switched from driver to nvJitLto library. Starting from CUDA 12.0 the user needs to link to libnvJitLto.so , see cuSPARSE documentation . JIT LTO performance has also been improved for cusparseSpMMOpPlan() . Introduced const descriptors for the Generic APIs, for example, cusparseConstSpVecGet() . Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions. Added two new algorithms to cusparseSpGEMM() with lower memory utilization. The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks. Added int8_t support to cusparseGather() , cusparseScatter() , and cusparseCsr2cscEx2() . Improved cusparseSpSV() performance for both the analysis and the solving phases. Improved cusparseSpSM() performance for both the analysis and the solving phases. Improved cusparseSDDMM() performance and added support for batch computation. Improved cusparseCsr2cscEx2() performance. Resolved Issues cusparseSpSV() and cusparseSpSM() could produce wrong results. cusparseDnMatGetStridedBatch() did not accept batchStride == 0 . Deprecations Removed deprecated CUDA 11.x APIs, enumerators, and descriptors. 2.5.'},\n",
       " {'id': 25,\n",
       "  'content': 'Math Library \\uf0c1 2.5.1. CUDA Math: Release 12.5 \\uf0c1 Known Issues As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss. New interval shall read (-23.0001; -2.2637). This finding is applicable to CUDA 12.5 and all previous versions.'},\n",
       " {'id': 26,\n",
       "  'content': '2.5.2. CUDA Math: Release 12.4 \\uf0c1 Resolved Issues Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules. 2.5.3. CUDA Math: Release 12.3 \\uf0c1 New Features Performance of SIMD Integer CUDA Math APIs was improved. Resolved Issues The __hisinf() Math APIs from cuda_fp16.h and cuda_bf16.h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12.3. Known Issues Users of cuda_fp16.h and cuda_bf16.h headers are advised to disable host compilers strict aliasing rules based optimizations (e.g. pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half , __half2 , __nv_bfloat16 , __nv_bfloat162 types implementations and expose the user program to undefined behavior. Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing . This behavior may improve in future versions of the headers.'},\n",
       " {'id': 27,\n",
       "  'content': '2.5.4. CUDA Math: Release 12.2 \\uf0c1 New Features CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side support for many of the arithmetic operations and conversions. __half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default. These may cause build issues due to ambiguous overloads resolution.'},\n",
       " {'id': 28,\n",
       "  'content': 'Users are advised to update their code to select proper overloads. To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ Resolved Issues During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default round-to-nearest-even mode could produce spurious overflow to infinity. NVIDIA recommends that all developers requiring strict IEEE754 compliance update to CUDA Toolkit 12.2 or newer. The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation. As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT. This is a software algorithm fix and is not tied to specific hardware. Updated the observed worst case error bounds for single precision intrinsic functions __expf() , __exp10f() and double precision functions asinh() , acosh() . 2.5.5. CUDA Math: Release 12.1 \\uf0c1 New Features Performance and accuracy improvements in atanf , acosf , asinf , sinpif , cospif , powf , erff , and tgammaf . 2.5.6. CUDA Math: Release 12.0 \\uf0c1 New Features Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions. Refer to https://docs.nvidia.com/cuda/cuda-math-api/index.html . Known Issues Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected. Affected CUDA Math APIs: __ddiv_rn() . Affected CUDA language operation: double precision / operation in the device code. Deprecations All previously deprecated undocumented APIs are removed from CUDA 12.0. 2.6. NVIDIA Performance Primitives (NPP) \\uf0c1 2.6.1. NPP: Release 12.4 \\uf0c1 New Features Enhanced large file support with size_t . 2.6.2. NPP: Release 12.0 \\uf0c1 Deprecations Deprecating non-CTX API support from next release. Resolved Issues A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance. 2.7.'},\n",
       " {'id': 29,\n",
       "  'content': 'nvJPEG Library \\uf0c1 2.7.1. nvJPEG: Release 12.4 \\uf0c1 New Features IDCT performance optimizations for single image CUDA decode. Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE . 2.7.2. nvJPEG: Release 12.3 Update 1 \\uf0c1 New Features New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them. 2.7.3. nvJPEG: Release 12.2 \\uf0c1 New Features Added support for JPEG Lossless decode (process 14, FO prediction). nvJPEG is now supported on L4T. 2.7.4. nvJPEG: Release 12.0 \\uf0c1 New Features Immproved the GPU Memory optimisation for the nvJPEG codec. Resolved Issues An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved. An issue with CMYK four component color conversion is now resolved. Known Issues Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths. Deprecations The reuse of Huffman table in Encoder ( nvjpegEncoderParamsCopyHuffmanTables ). 1 Only available on select Linux distros 3.'},\n",
       " {'id': 30, 'content': 'Notices \\uf0c1 3.1.'},\n",
       " {'id': 31,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 32,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 33,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. CUDA 11.6 Features 1.1.'},\n",
       " {'id': 34,\n",
       "  'content': 'Compiler 1.1.1. VS2022 Support 1.1.2. New instructions in public PTX 1.1.3. Unused Kernel Optimization 1.1.4. New -arch=native option 1.1.5. Generate PTX from nvlink: 1.1.6. Bullseye support 1.1.7. INT128 developer tool support 2. Notices 2.1.'},\n",
       " {'id': 35,\n",
       "  'content': 'Notice 2.2. OpenCL 2.3. Trademarks CUDA Features Archive » 1. CUDA 11.6 Features v12.5 | PDF | Archive NVIDIA CUDA Features Archive The list of CUDA features by release. CUDA 11.6 Features \\uf0c1 1.1. Compiler \\uf0c1 1.1.1. VS2022 Support \\uf0c1 CUDA 11.6 officially supports the latest VS2022 as host compiler. A separate Nsight Visual Studio installer 2022.1.1 must be downloaded from here . A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it. 1.1.2. New instructions in public PTX \\uf0c1 New instructions for bit mask creation—BMSK, and sign extension—SZEXT, are added to the public PTX ISA. You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT . 1.1.3. Unused Kernel Optimization \\uf0c1 In CUDA 11.5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations. This was an opt-in feature but in 11.6, this feature is enabled by default.'},\n",
       " {'id': 36,\n",
       "  'content': 'As mentioned in the 11.5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations. $ nvcc -rdc=true user.cu testlib.a -o user -Xnvlink -ignore-host-info 1.1.4. New -arch=native option \\uf0c1 In addition to the -arch=all and -arch=all-major options added in CUDA 11.5, NVCC introduced -arch= native in CUDA 11.5 update 1. This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system. This can be particularly helpful for testing when applications are run on the same system they are compiled in. 1.1.5. Generate PTX from nvlink: \\uf0c1 Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN: nvcc -dlto -dlink -ptx Device linking by nvlink is the final stage in the CUDA compilation process. Applications that have multiple source translation units have to be compiled in separate compilation mode. LTO (introduced in CUDA 11.4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit. However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link Time Optimization or had to constrain the device code to a single source file. With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization. 1.1.6. Bullseye support \\uf0c1 NVCC compiled source code now works with the code coverage tool Bullseye. The code coverage is only for the CPU or the host functions. Code coverage for device function is not supported through bullseye. 1.1.7. INT128 developer tool support \\uf0c1 In 11.5, CUDA C++ support for 128 bit was added. In 11.6, developer tools support the datatype as well. With the latest version of libcu++, int 128 data datype is supported by math functions. 2.'},\n",
       " {'id': 37,\n",
       "  'content': 'Notices \\uf0c1 2.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 38,\n",
       "  'content': '2.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 39,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. License Agreement for NVIDIA Software Development Kits 1.1.'},\n",
       " {'id': 40, 'content': 'License 1.1.1.'},\n",
       " {'id': 41,\n",
       "  'content': 'License Grant 1.1.2. Distribution Requirements 1.1.3. Authorized Users 1.1.4. Pre-Release SDK 1.1.5. Updates 1.1.6. Components Under Other Licenses 1.1.7. Reservation of Rights 1.2. Limitations 1.3.'},\n",
       " {'id': 42,\n",
       "  'content': 'Ownership 1.4. No Warranties 1.5. Limitation of Liability 1.6. Termination 1.7.'},\n",
       " {'id': 43,\n",
       "  'content': 'General 2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits 2.1. License Scope 2.2.'},\n",
       " {'id': 44, 'content': 'Distribution 2.3.'},\n",
       " {'id': 45,\n",
       "  'content': 'Operating Systems 2.4. Audio and Video Encoders and Decoders 2.5. Licensing 2.6. Attachment A 2.7. Attachment B EULA » 1. License Agreement for NVIDIA Software Development Kits v12.5 | PDF | Archive End User License Agreement NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement. Last updated: October 8, 2021 The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software. Last updated: October 8, 2021. Preface The Software License Agreement in Chapter 1 and the Supplement in Chapter 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the product(s) included herein. NVIDIA Driver Description This package contains the operating system driver and fundamental system software components for NVIDIA GPUs. NVIDIA CUDA Toolkit Description The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references. Default Install Location of CUDA Toolkit Windows platform: %ProgramFiles%\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v#.# Linux platform: /usr/local/cuda-#.# Mac platform: /Developer/NVIDIA/CUDA-#.# NVIDIA CUDA Samples Description CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples. They are no longer included in the CUDA toolkit. NVIDIA Nsight Visual Studio Edition (Windows only) Description NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and graphics applications. Default Install Location of Nsight Visual Studio Edition Windows platform: %ProgramFiles(x86)%\\\\NVIDIA Corporation\\\\Nsight Visual Studio Edition #.# 1. License Agreement for NVIDIA Software Development Kits \\uf0c1 Important Notice—Read before downloading, installing, copying or using the licensed software: This license agreement, including exhibits attached (“Agreement”) is a legal agreement between you and NVIDIA Corporation (“NVIDIA”) and governs your use of a NVIDIA software development kit (“SDK”). Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation. This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used. If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case “you” will mean the entity you represent. If you don’t have the required age or authority to accept this Agreement, or if you don’t accept all the terms and conditions of this Agreement, do not download, install or use the SDK. You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions. 1.1.'},\n",
       " {'id': 46,\n",
       "  'content': 'License \\uf0c1 1.1.1. License Grant \\uf0c1 Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to: Install and use the SDK, Modify and create derivative works of sample source code delivered in the SDK, and Distribute those portions of the SDK that are identified in this Agreement as distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement. 1.1.2.'},\n",
       " {'id': 47,\n",
       "  'content': 'Distribution Requirements \\uf0c1 These are the distribution requirements for you to exercise the distribution grant: Your application must have material additional functionality, beyond the included portions of the SDK. The distributable portions of the SDK shall only be accessed by your application. The following notice shall be included in modifications and derivative works of sample source code distributed: “This software contains source code provided by NVIDIA Corporation.” Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only. The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA’s intellectual property rights. Additionally, you agree that you will protect the privacy, security and legal rights of your application users. You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK. 1.1.3. Authorized Users \\uf0c1 You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf. If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network. You are responsible for the compliance with the terms of this Agreement by your authorized users. If you become aware that your authorized users didn’t follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences. 1.1.4. Pre-Release SDK \\uf0c1 The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. Use of a pre-release SDK may result in unexpected results, loss of data, project delays or other unpredictable damage or loss. You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems. NVIDIA may choose not to make available a commercial version of any pre-release SDK. NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability. 1.1.5. Updates \\uf0c1 NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK. Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement. You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you. While NVIDIA generally maintains compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK. 1.1.6. Components Under Other Licenses \\uf0c1 The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK. If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms associated with the components control only to the extent necessary to resolve the conflict. Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses. 1.1.7. Reservation of Rights \\uf0c1 NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement. 1.2. Limitations \\uf0c1 The following license limitations apply to your use of the SDK: You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK. Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of the SDK. For clarity, you may not distribute or sublicense the SDK as a stand-alone product. Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA. You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK. You may not use the SDK in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be: Disclosed or distributed in source code form; Licensed for the purpose of making derivative works; or Redistributable at no charge. You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a “Critical Application”). Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical applications. NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements. You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney’s fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms. You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform. 1.3. Ownership \\uf0c1 NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1.3.2 . This SDK may include software and materials from NVIDIA’s licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights. You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual property rights, subject to NVIDIA’s rights under Section 1.3.1 . You may, but don’t have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK. For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you. NVIDIA will use feedback at its choice. NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https://developer.nvidia.com . 1.4. No Warranties \\uf0c1 THE SDK IS PROVIDED BY NVIDIA “AS IS” AND “WITH ALL FAULTS.” TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. NO WARRANTY IS MADE ON THE BASIS OF TRADE USAGE, COURSE OF DEALING OR COURSE OF TRADE. 1.5. Limitation of Liability \\uf0c1 TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY. IN NO EVENT WILL NVIDIA’S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT. These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose. These exclusions and limitations of liability form an essential basis of the bargain between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different. 1.6. Termination \\uf0c1 This Agreement will continue to apply until terminated by either you or NVIDIA as described below. If you want to terminate this Agreement, you may do so by stopping to use the SDK. NVIDIA may, at any time, terminate this Agreement if: (i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIA’s intellectual property rights); (ii) you commence or participate in any legal proceeding against NVIDIA with respect to the SDK; or (iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIA’s sole discretion, the continued use of it is no longer commercially viable. Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control. Your prior distributions in accordance with this Agreement are not affected by the termination of this Agreement. Upon written request, you will certify in writing that you have complied with your commitments under this section. Upon any termination of this Agreement all provisions survive except for the license grant provisions. 1.7. General \\uf0c1 If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect. NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified. You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement. This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language. The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction. If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect.'},\n",
       " {'id': 48, 'content': 'Unless otherwise specified, remedies are cumulative.'},\n",
       " {'id': 49,\n",
       "  'content': 'Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement. The SDK has been developed entirely at private expense and is “commercial items” consisting of “commercial computer software” and “commercial computer software documentation” provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S.'},\n",
       " {'id': 50,\n",
       "  'content': 'Government or a U.S. Government subcontractor is subject to the restrictions in this Agreement pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051. The SDK is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasury’s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U.S. or otherwise prohibited from receiving the SDK under U.S. law. Any notice delivered by NVIDIA to you under this Agreement will be delivered via mail, email or fax. You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements. Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department. This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license. Any additional and/or conflicting terms on documents issued by you are null, void, and invalid. Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties.'},\n",
       " {'id': 51,\n",
       "  'content': '2. CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits \\uf0c1 The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (“Agreement”) as modified by this supplement. Capitalized terms used but not defined below have the meaning assigned to them in the Agreement.'},\n",
       " {'id': 52,\n",
       "  'content': 'This supplement is an exhibit to the Agreement and is incorporated as an integral part of the Agreement. In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern.'},\n",
       " {'id': 53,\n",
       "  'content': '2.1. License Scope \\uf0c1 The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs. 2.2.'},\n",
       " {'id': 54,\n",
       "  'content': 'Distribution \\uf0c1 The portions of the SDK that are distributable under the Agreement are listed in Attachment A. 2.3. Operating Systems \\uf0c1 Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files). 2.4. Audio and Video Encoders and Decoders \\uf0c1 You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not limited to, Microsoft, Thomson, Fraunhofer IIS, Sisvel S.p.A., MPEG-LA, and Coding Technologies. NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders. 2.5. Licensing \\uf0c1 If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions @ nvidia . com .'},\n",
       " {'id': 55,\n",
       "  'content': '2.6. Attachment A \\uf0c1 The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9.0 of the 64-bit Windows software, the file cudart64_90.dll is redistributable. Component CUDA Runtime Windows cudart.dll, cudart_static.lib, cudadevrt.lib Mac OSX libcudart.dylib, libcudart_static.a, libcudadevrt.a Linux libcudart.so, libcudart_static.a, libcudadevrt.a Android libcudart.so, libcudart_static.a, libcudadevrt.a Component CUDA FFT Library Windows cufft.dll, cufftw.dll, cufft.lib, cufftw.lib Mac OSX libcufft.dylib, libcufft_static.a, libcufftw.dylib, libcufftw_static.a Linux libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Android libcufft.so, libcufft_static.a, libcufftw.so, libcufftw_static.a Component CUDA BLAS Library Windows cublas.dll, cublasLt.dll Mac OSX libcublas.dylib, libcublasLt.dylib, libcublas_static.a, libcublasLt_static.a Linux libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Android libcublas.so, libcublasLt.so, libcublas_static.a, libcublasLt_static.a Component NVIDIA “Drop-in” BLAS Library Windows nvblas.dll Mac OSX libnvblas.dylib Linux libnvblas.so Component CUDA Sparse Matrix Library Windows cusparse.dll, cusparse.lib Mac OSX libcusparse.dylib, libcusparse_static.a Linux libcusparse.so, libcusparse_static.a Android libcusparse.so, libcusparse_static.a Component CUDA Linear Solver Library Windows cusolver.dll, cusolver.lib Mac OSX libcusolver.dylib, libcusolver_static.a Linux libcusolver.so, libcusolver_static.a Android libcusolver.so, libcusolver_static.a Component CUDA Random Number Generation Library Windows curand.dll, curand.lib Mac OSX libcurand.dylib, libcurand_static.a Linux libcurand.so, libcurand_static.a Android libcurand.so, libcurand_static.a Component NVIDIA Performance Primitives Library Windows nppc.dll, nppc.lib, nppial.dll, nppial.lib, nppicc.dll, nppicc.lib, nppicom.dll, nppicom.lib, nppidei.dll, nppidei.lib, nppif.dll, nppif.lib, nppig.dll, nppig.lib, nppim.dll, nppim.lib, nppist.dll, nppist.lib, nppisu.dll, nppisu.lib, nppitc.dll, nppitc.lib, npps.dll, npps.lib Mac OSX libnppc.dylib, libnppc_static.a, libnppial.dylib, libnppial_static.a, libnppicc.dylib, libnppicc_static.a, libnppicom.dylib, libnppicom_static.a, libnppidei.dylib, libnppidei_static.a, libnppif.dylib, libnppif_static.a, libnppig.dylib, libnppig_static.a, libnppim.dylib, libnppisu_static.a, libnppitc.dylib, libnppitc_static.a, libnpps.dylib, libnpps_static.a Linux libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Android libnppc.so, libnppc_static.a, libnppial.so, libnppial_static.a, libnppicc.so, libnppicc_static.a, libnppicom.so, libnppicom_static.a, libnppidei.so, libnppidei_static.a, libnppif.so, libnppif_static.a libnppig.so, libnppig_static.a, libnppim.so, libnppim_static.a, libnppist.so, libnppist_static.a, libnppisu.so, libnppisu_static.a, libnppitc.so libnppitc_static.a, libnpps.so, libnpps_static.a Component NVIDIA JPEG Library Windows nvjpeg.lib, nvjpeg.dll Linux libnvjpeg.so, libnvjpeg_static.a Component Internal common library required for statically linking to cuBLAS, cuSPARSE, cuFFT, cuRAND, nvJPEG and NPP Mac OSX libculibos.a Linux libculibos.a Component NVIDIA Runtime Compilation Library and Header All nvrtc.h Windows nvrtc.dll, nvrtc-builtins.dll Mac OSX libnvrtc.dylib, libnvrtc-builtins.dylib Linux libnvrtc.so, libnvrtc-builtins.so, libnvrtc_static.a, libnvrtx-builtins_static.a Component NVIDIA Optimizing Compiler Library Windows nvvm.dll Mac OSX libnvvm.dylib Linux libnvvm.so Component NVIDIA JIT Linking Library Windows libnvJitLink.dll, libnvJitLink.lib Linux libnvJitLink.so, libnvJitLink_static.a Component NVIDIA Common Device Math Functions Library Windows libdevice.10.bc Mac OSX libdevice.10.bc Linux libdevice.10.bc Component CUDA Occupancy Calculation Header Library All cuda_occupancy.h Component CUDA Half Precision Headers All cuda_fp16.h, cuda_fp16.hpp Component CUDA Profiling Tools Interface (CUPTI) Library Windows cupti.dll Mac OSX libcupti.dylib Linux libcupti.so Component NVIDIA Tools Extension Library Windows nvToolsExt.dll, nvToolsExt.lib Mac OSX libnvToolsExt.dylib Linux libnvToolsExt.so Component NVIDIA CUDA Driver Libraries Linux libcuda.so, libnvidia-ptxjitcompiler.so, libnvptxcompiler_static.a Component NVIDIA CUDA File IO Libraries and Header All cufile.h Linux libcufile.so, libcufile_rdma.so, libcufile_static.a, libcufile_rdma_static.a In addition to the rights above, for parties that are developing software intended solely for use on Jetson development kits or Jetson modules, and running Linux for Tegra software, the following shall apply: The SDK may be distributed in its entirety, as provided by NVIDIA, and without separation of its components, for you and/or your licensees to create software development kits for use only on the Jetson platform and running Linux for Tegra software. 2.7.'},\n",
       " {'id': 56,\n",
       "  'content': 'Attachment B \\uf0c1 Additional Licensing Obligations The following third party components included in the SOFTWARE are licensed to Licensee pursuant to the following terms and conditions: Licensee’s use of the GDB third party component is subject to the terms and conditions of GNU GPL v3: This product includes copyrighted third-party software licensed under the terms of the GNU General Public License v3 (\"GPL v3\"). All third-party software packages are copyright by their respective authors. GPL v3 terms and conditions are hereby incorporated into the Agreement by this reference: http://www.gnu.org/licenses/gpl.txt Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses. To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests @ nvidia . This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION. Component License CUDA-GDB GPL v3 Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licensee’s use of the H.264 video codecs are solely the responsibility of Licensee. Licensee’s use of the Thrust library is subject to the terms and conditions of the Apache License Version 2.0. Apache License Version 2.0 terms and conditions are hereby incorporated into the Agreement by this reference. http://www.apache.org/licenses/LICENSE-2.0.html In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries. Boost Software License - Version 1.0 - August 17th, 2003 . . Permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Licensee’s use of the LLVM third party component is subject to the following terms and conditions: ====================================================== LLVM Release License ====================================================== University of Illinois/NCSA Open Source License Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign. All rights reserved. Developed by: LLVM Team University of Illinois at Urbana-Champaign http://llvm.org Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution. * Neither the names of the LLVM Team, University of Illinois at Urbana- Champaign, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.'},\n",
       " {'id': 57,\n",
       "  'content': 'IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE. Licensee’s use of the PCRE third party component is subject to the following terms and conditions: ------------ PCRE LICENCE ------------ PCRE is a library of functions to support regular expressions whose syntax and semantics are as close as possible to those of the Perl 5 language. Release 8 of PCRE is distributed under the terms of the \"BSD\" licence, as specified below. The documentation for PCRE, supplied in the \"doc\" directory, is distributed under the same terms as the software itself. The basic library functions are written in C and are freestanding. Also included in the distribution is a set of C++ wrapper functions, and a just- in-time compiler that can be used to optimize pattern matching. These are both optional features that can be omitted when the library is built. THE BASIC LIBRARY FUNCTIONS --------------------------- Written by: Philip Hazel Email local part: ph10 Email domain: cam.ac.uk University of Cambridge Computing Service, Cambridge, England.'},\n",
       " {'id': 58,\n",
       "  'content': 'Copyright (c) 1997-2012 University of Cambridge All rights reserved. PCRE JUST-IN-TIME COMPILATION SUPPORT ------------------------------------- Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2010-2012 Zoltan Herczeg All rights reserved. STACK-LESS JUST-IN-TIME COMPILER -------------------------------- Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail.hu Copyright(c) 2009-2012 Zoltan Herczeg All rights reserved. THE C++ WRAPPER FUNCTIONS ------------------------- Contributed by: Google Inc. Copyright (c) 2007-2012, Google Inc.'},\n",
       " {'id': 59,\n",
       "  'content': 'All rights reserved. THE \"BSD\" LICENCE ----------------- Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the University of Cambridge nor the name of Google Inc. nor the names of their contributors may be used to endorse or promote products derived from this software without specific prior written permission.'},\n",
       " {'id': 60,\n",
       "  'content': 'THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2007-2009, Regents of the University of California All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Neither the name of the University of California, Berkeley nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE AUTHOR \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata. * The name of the author may not be used to endorse or promote products derived from this software without specific prior written permission. Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2010 The University of Tennessee. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer listed in this license in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2012, The Science and Technology Facilities Council (STFC). * Neither the name of the STFC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. IN NO EVENT SHALL THE STFC BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Some of the cuBLAS library routines were written by or derived from code written by Ahmad M. Abdelfattah, David Keyes, and Hatem Ltaief, and are subject to the Apache License, Version 2.0, as follows: -- (C) Copyright 2013 King Abdullah University of Science and Technology Authors: Ahmad Abdelfattah (ahmad.ahmad@kaust.edu.sa) David Keyes (david.keyes@kaust.edu.sa) Hatem Ltaief (hatem.ltaief@kaust.edu.sa) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Neither the name of the King Abdullah University of Science and Technology nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS\\'\\' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE Some of the cuSPARSE library routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows: Copyright (c) 2012, University of Illinois. Developed by: IMPACT Group, University of Illinois, http://impact.crhc.illinois.edu Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Neither the names of IMPACT Group, University of Illinois, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission. Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license: Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima University. Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima University and University of Tokyo. * Neither the name of the Hiroshima University nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. Some of the cuRAND library routines were derived from code developed by D. E. Shaw Research and are subject to the following license: Copyright 2010-2011, D. E. Shaw Research. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of D.'},\n",
       " {'id': 61,\n",
       "  'content': 'E. Shaw Research nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license: Copyright (c) 2015-2017, Norbert Juffa All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Licensee’s use of the lz4 third party component is subject to the following terms and conditions: Copyright (C) 2011-2013, Yann Collet. BSD 2-Clause License (http://www.opensource.org/licenses/bsd-license.php) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. The NPP library uses code from the Boost Math Toolkit, and is subject to the following license: Boost Software License - Version 1.0 - August 17th, 2003 . Portions of the Nsight Eclipse Edition is subject to the following license: The Eclipse Foundation makes available all content in this plug-in (\"Content\"). Unless otherwise indicated below, the Content is provided to you under the terms and conditions of the Eclipse Public License Version 1.0 (\"EPL\"). A copy of the EPL is available at http:// www.eclipse.org/legal/epl-v10.html. For purposes of the EPL, \"Program\" will mean the Content. If you did not receive this Content directly from the Eclipse Foundation, the Content is being redistributed by another party (\"Redistributor\") and different terms and conditions may apply to your use of any object code in the Content. Check the Redistributor\\'s license that was provided with the Content. If no such license exists, contact the Redistributor. Unless otherwise indicated below, the terms and conditions of the EPL still apply to any source code in the Content and such source code may be obtained at http://www.eclipse.org. Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license: License URL https://github.com/openai/openai-gemm/blob/master/LICENSE License Text The MIT License Copyright (c) 2016 OpenAI (http://openai.com), 2016 Google Inc. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Licensee’s use of the Visual Studio Setup Configuration Samples is subject to the following license: The MIT License (MIT) Copyright (C) Microsoft Corporation. Licensee’s use of linmath.h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2.0. The DX12-CUDA sample uses the d3dx12.h header, which is subject to the MIT license . Components of the driver and compiler used for binary management, including nvFatBin, nvcc, and cuobjdump, use the Zstandard library which is subject to the following license: BSD License For Zstandard software Copyright (c) Meta Platforms, Inc. and affiliates.'},\n",
       " {'id': 62,\n",
       "  'content': '* Neither the name Facebook, nor Meta, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation.'},\n",
       " {'id': 63,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); });1. Introduction 2.'},\n",
       " {'id': 64,\n",
       "  'content': 'Windows 2.1. Network Installer 2.2. Local Installer 2.3. Pip Wheels - Windows 2.4. Conda 3. Linux 3.1.'},\n",
       " {'id': 65,\n",
       "  'content': 'Linux x86_64 3.1.1. Redhat / CentOS 3.1.1.1. RPM Installer 3.1.1.2. Runfile Installer 3.1.2. Fedora 3.1.2.1. RPM Installer 3.1.2.2. Runfile Installer 3.1.3. SUSE Linux Enterprise Server 3.1.3.1. RPM Installer 3.1.3.2. Runfile Installer 3.1.4. OpenSUSE 3.1.4.1. RPM Installer 3.1.4.2. Runfile Installer 3.1.5. Amazon Linux 2023 3.1.5.1. Prepare Amazon Linux 2023 3.1.5.2. Local Repo Installation for Amazon Linux 3.1.5.3. Network Repo Installation for Amazon Linux 3.1.5.4. Common Installation Instructions for Amazon Linux 3.1.6. Pip Wheels - Linux 3.1.7. Conda 3.1.8. WSL 3.1.9. Ubuntu 3.1.9.1. Debian Installer 3.1.9.2. Runfile Installer 3.1.10. Debian 3.1.10.1. Debian Installer 3.1.10.2. Runfile Installer 4. Notices 4.1.'},\n",
       " {'id': 66,\n",
       "  'content': 'Notice 4.2. OpenCL 4.3. Trademarks Quick Start Guide » 1. Introduction v12.5 | PDF | Archive CUDA Quick Start Guide Minimal first-steps instructions to get CUDA running on a standard system. Introduction \\uf0c1 This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform. These instructions are intended to be used on a clean installation of a supported platform. For questions which are not answered in this document, please refer to the Windows Installation Guide and Linux Installation Guide . The CUDA installation packages can be found on the CUDA Downloads Page . 2. Windows \\uf0c1 When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer. The Network Installer allows you to download only the files you need. The Local Installer is a stand-alone installer with a large initial download. For more details, refer to the Windows Installation Guide .'},\n",
       " {'id': 67,\n",
       "  'content': '2.1. Network Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation. Launch the downloaded installer package. Read and accept the EULA. Select next to download and install all components.'},\n",
       " {'id': 68,\n",
       "  'content': 'Once the download completes, the installation will begin automatically. Once the installation completes, click “next” to acknowledge the Nsight Visual Studio Edition installation summary.'},\n",
       " {'id': 69,\n",
       "  'content': 'Click close to close the installer. Navigate to the Samples’ nbody directory in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody . Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019.sln . Open the Build menu within Visual Studio and click Build Solution . Navigate to the CUDA Samples build directory and run the nbody sample. Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources. 2.2. Local Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation. Select next to install all components. Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary. Open the nbody Visual Studio solution file for the version of Visual Studio you have installed.'},\n",
       " {'id': 70,\n",
       "  'content': '2.3. Pip Wheels - Windows \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. py -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. “cu12” should be read as “cuda12”. nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 2.4. Conda \\uf0c1 The Conda packages are available at https://anaconda.org/nvidia . Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3. Linux \\uf0c1 CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on. 3.1.'},\n",
       " {'id': 71,\n",
       "  'content': 'Linux x86_64 \\uf0c1 For development on the x86_64 architecture. In some cases, x86_64 systems may act as host platforms targeting other architectures. See the Linux Installation Guide for more details.'},\n",
       " {'id': 72,\n",
       "  'content': '3.1.1. Redhat / CentOS \\uf0c1 When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer. The Runfile Installer is only available as a Local Installer. The RPM Installer is available as both a Local Installer and a Network Installer. In the case of the RPM installers, the instructions for the Local and Network variants are the same. For more details, refer to the Linux Installation Guide .'},\n",
       " {'id': 73,\n",
       "  'content': '3.1.1.1. RPM Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation. Install EPEL to satisfy the DKMS dependency by following the instructions at EPEL’s website . Enable optional repos : On RHEL 8 Linux only, execute the following steps to enable optional repositories. On x86_64 workstation: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Install the repository meta-data, clean the yum cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo yum clean expire-cache sudo yum install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.1.2.'},\n",
       " {'id': 74,\n",
       "  'content': \"Runfile Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation. Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.2. Fedora \\uf0c1 When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer. 3.1.2.1. Install the RPMFusion free repository to satisfy the Akmods dependency: su -c 'dnf install --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm' Install the repository meta-data, clean the dnf cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo dnf clean expire-cache sudo dnf install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.2.2.\"},\n",
       " {'id': 75,\n",
       "  'content': 'Disable the Nouveau drivers: Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the below command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface. Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.3. SUSE Linux Enterprise Server \\uf0c1 When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer. 3.1.3.1. Install the repository meta-data, refresh the Zypper cache, update the GPG key, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo SUSEConnect --product PackageHub/15/x86_64 sudo zypper refresh sudo rpm --erase gpg-pubkey-7fa2af80* sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd . 3.1.3.2. Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux.run --silent Create an xorg.conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the vectorAdd sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAdd . 3.1.4.'},\n",
       " {'id': 76,\n",
       "  'content': 'OpenSUSE \\uf0c1 When installing CUDA on OpenSUSE, you can choose between the Runfile Installer and the RPM Installer. 3.1.4.1. Install the repository meta-data, refresh the Zypper cache, and install CUDA: sudo rpm --install cuda-repo--..rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo zypper refresh sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.4.2.'},\n",
       " {'id': 77,\n",
       "  'content': 'Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. 3.1.5.'},\n",
       " {'id': 78,\n",
       "  'content': 'Amazon Linux 2023 \\uf0c1 3.1.5.1. Prepare Amazon Linux 2023 \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo . 3.1.5.2. Local Repo Installation for Amazon Linux \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*.x86_64.rpm 3.1.5.3. Network Repo Installation for Amazon Linux \\uf0c1 Enable the network repository and clean the DN cache: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo sudo dnf clean expire-cache 3.1.5.4. Common Installation Instructions for Amazon Linux \\uf0c1 These instructions apply to both local and network installation for Amazon Linux. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Reboot the system: sudo reboot Perform the post-installation actions.'},\n",
       " {'id': 79,\n",
       "  'content': '3.1.6. Pip Wheels - Linux \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. python3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 nvidia-opencl-cu12 nvidia-nvjitlink-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 nvidia-opencl-cu125 nvidia-nvjitlink-cu125 3.1.7. Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3.1.8. WSL \\uf0c1 These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case. Install repository meta-data sudo dpkg -i cuda-repo-__.deb Update the CUDA public GPG key sudo apt-key del 7fa2af80 When installing using the local repo: sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring.gpg /usr/share/keyrings/ When installing using the network repo: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb Pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-.pin sudo mv cuda-.pin /etc/apt/preferences.d/cuda-repository-pin-600 Update the Apt repository cache and install CUDA sudo apt-get update sudo apt-get install cuda 3.1.9. Ubuntu \\uf0c1 When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer.'},\n",
       " {'id': 80,\n",
       "  'content': 'The Debian Installer is available as both a Local Installer and a Network Installer. In the case of the Debian installers, the instructions for the Local and Network variants are the same. 3.1.9.1. Debian Installer \\uf0c1 Perform the following steps to install CUDA and verify the installation. Install the repository meta-data, update the GPG key, update the apt-get cache, and install CUDA: sudo dpkg --install cuda-repo--..deb sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb sudo add-apt-repository contrib sudo apt-get update sudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.9.2.'},\n",
       " {'id': 81,\n",
       "  'content': 'Disable the Nouveau drivers: Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters. 3.1.10. Debian \\uf0c1 When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer. 3.1.10.1. Install the repository meta-data, remove old GPG key, install GPG key, update the apt-get cache, and install CUDA: sudo dpkg -i cuda-repo-__.deb sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian10/x86_64/7fa2af80.pub sudo apt-key del 7fa2af80 wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb sudo add-apt-repository contrib sudo apt-get update sudo apt-get -y install cuda Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12.5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.5/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https://github.com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/nbody . 3.1.10.2.'},\n",
       " {'id': 82,\n",
       "  'content': '4. Notices \\uf0c1 4.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 4.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 83,\n",
       "  'content': '4.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2015-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 84,\n",
       "  'content': 'Last updated on Jun 25, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Introduction 1.1.'},\n",
       " {'id': 85, 'content': 'System Requirements 1.2.'},\n",
       " {'id': 86,\n",
       "  'content': 'About This Document 2. Installing CUDA Development Tools 2.1. Verify You Have a CUDA-Capable GPU 2.2. Download the NVIDIA CUDA Toolkit 2.3. Install the CUDA Software 2.3.1. Uninstalling the CUDA Software 2.4. Using Conda to Install the CUDA Software 2.4.1. Conda Overview 2.4.2. Installation 2.4.3. Uninstallation 2.4.4. Installing Previous CUDA Releases 2.5. Use a Suitable Driver Model 2.6. Verify the Installation 2.6.1.'},\n",
       " {'id': 87,\n",
       "  'content': 'Running the Compiled Examples 3. Pip Wheels 4. Compiling CUDA Programs 4.1. Compiling Sample Projects 4.2. Sample Projects 4.3. Build Customizations for New Projects 4.4. Build Customizations for Existing Projects 5. Additional Considerations 6. Notices 6.1. Notice 6.2. OpenCL 6.3. Trademarks Installation Guide Windows » 1. Introduction v12.5 | PDF | Archive CUDA Installation Guide for Microsoft Windows The installation instructions for the CUDA Toolkit on Microsoft Windows systems. Introduction \\uf0c1 CUDA ® is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU). CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources. CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. This guide will show you how to install and check the correct operation of the CUDA development tools. 1.1. System Requirements \\uf0c1 To use CUDA on your system, you will need the following installed: A CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain NVIDIA CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) Supported Microsoft Windows ® operating systems: Microsoft Windows 11 21H2 Microsoft Windows 11 22H2-SV2 Microsoft Windows 11 23H2 Microsoft Windows 10 21H2 Microsoft Windows 10 22H2 Microsoft Windows Server 2022 Table 1 Windows Compiler Support in CUDA 12.5 \\uf0c1 Compiler* IDE Native x86_64 Cross-compilation (32-bit on 64-bit) C++ Dialect MSVC Version 193x Visual Studio 2022 17.x YES Not supported C++14 (default), C++17, C++20 MSVC Version 192x Visual Studio 2019 16.x YES C++14 (default), C++17 MSVC Version 191x Visual Studio 2017 15.x (RTW and all updates) YES C++14 (default), C++17 * Support for Visual Studio 2015 is deprecated in release 11.1; support for Visual Studio 2017 is deprecated in release 12.5. 32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running 32-bit application binaries on GeForce GPUs until Ada. Ada will be the last architecture with driver support for 32-bit applications. Hopper does not support 32-bit applications. Support for running x86 32-bit applications on x86_64 Windows is limited to use with: CUDA Driver CUDA Runtime (cudart) CUDA Math Library (math.h) 1.2. About This Document \\uf0c1 This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment. You do not need previous experience with CUDA or experience with parallel computation. 2. Installing CUDA Development Tools \\uf0c1 Basic instructions can be found in the Quick Start Guide . Read on for more detailed instructions. The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps: Verify the system has a CUDA-capable GPU. Download the NVIDIA CUDA Toolkit. Install the NVIDIA CUDA Toolkit. Test that the installed software runs correctly and communicates with the hardware. 2.1. Verify You Have a CUDA-Capable GPU \\uf0c1 You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager . Here you will find the vendor name and model of your graphics card(s). If you have an NVIDIA card that is listed in https://developer.nvidia.com/cuda-gpus , that GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products. The Windows Device Manager can be opened via the following steps: Open a run window from the Start Menu Run: control /name Microsoft.DeviceManager 2.2. Download the NVIDIA CUDA Toolkit \\uf0c1 The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads . Choose the platform you are using and one of the following installer formats: Network Installer: A minimal installer which later downloads packages required for installation.'},\n",
       " {'id': 88,\n",
       "  'content': 'Only the packages selected during the selection phase of the installer are downloaded. This installer is useful for users who want to minimize download time. Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download. This installer is useful for systems which lack network access and for enterprise deployment. The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources. Download Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. 2.3. Install the CUDA Software \\uf0c1 Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality. Note The driver and toolkit must be installed for CUDA to function. If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit. Note The installation may fail if Windows Update starts after the installation has begun. Wait until Windows Update is complete and then try the installation again. Graphical Installation Install the CUDA Software by executing the CUDA installer and following the on-screen prompts. Silent Installation The installer can be executed in silent mode by executing the package with the -s flag. Additional parameters can be passed which will install specific subpackages instead of all packages.'},\n",
       " {'id': 89,\n",
       "  'content': 'See the table below for a list of all the subpackage names. Table 2 Possible Subpackage Names \\uf0c1 Subpackage Name Subpackage Description Toolkit Subpackages (defaults to C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.5) cuda_profiler_api_12.5 CUDA Profiler API. cudart_12.5 CUDA Runtime libraries. cuobjdump_12.5 Extracts information from cubin files. cupti_12.5 The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications. cuxxfilt_12.5 The CUDA cu++ filt demangler tool. demo_suite_12.5 Prebuilt demo applications using CUDA. documentation_12.5 CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc. nvcc_12.5 CUDA compiler. nvdisasm_12.5 Extracts information from standalone cubin files. nvfatbin_12.5 Library for creating fatbinaries at runtime. nvjitlink_12.5 nvJitLink library. nvml_dev_12.5 NVML development libraries and headers. nvprof_12.5 Tool for collecting and viewing CUDA application profiling data from the command-line. nvprune_12.5 Prunes host object files and libraries to only contain device code for the specified targets. nvrtc_12.5 nvrtc_dev_12.5 NVRTC runtime libraries. nvtx_12.5 NVTX on Windows. opencl_12.5 OpenCL library. visual_profiler_12.5 Visual Profiler. sanitizer_12.5 Compute Sanitizer API. thrust_12.5 CUDA Thrust. cublas_12.5 cublas_dev_12.5 cuBLAS runtime libraries. cufft_12.5 cufft_dev_12.5 cuFFT runtime libraries. curand_12.5 curand_dev_12.5 cuRAND runtime libraries. cusolver_12.5 cusolver_dev_12.5 cuSOLVER runtime libraries. cusparse_12.5 cusparse_dev_12.5 cuSPARSE runtime libraries. npp_12.5 npp_dev_12.5 NPP runtime libraries. nvjpeg_12.5 nvjpeg_dev_12.5 nvJPEG libraries. nsight_compute_12.5 Nsight Compute. nsight_systems_12.5 Nsight Systems. nsight_vse_12.5 Installs the Nsight Visual Studio Edition plugin in all VS. occupancy_calculator_12.5 Installs the CUDA_Occupancy_Calculator.xls tool. visual_studio_integration_12.5 Installs CUDA project wizard and builds customization files in VS. Driver Subpackages Display.Driver The NVIDIA Display Driver. Required to run CUDA applications. For example, to install only the compiler and driver components: .exe -s nvcc_12.1 Display.Driver Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required. Extracting and Inspecting the Files Manually Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation. The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip . Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration. Within each directory is a .dll and .nvi file that can be ignored as they are not part of the installable files. Note Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration. This is intended for enterprise-level deployment.'},\n",
       " {'id': 90,\n",
       "  'content': '2.3.1. Uninstalling the CUDA Software \\uf0c1 All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget. 2.4. Using Conda to Install the CUDA Software \\uf0c1 This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia . 2.4.1. Conda Overview \\uf0c1 The Conda installation installs the CUDA Toolkit. The installation steps are listed below. 2.4.2. Installation \\uf0c1 To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda - c nvidia 2.4.3. Uninstallation \\uf0c1 To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 2.4.4. Installing Previous CUDA Releases \\uf0c1 All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as: conda install cuda - c nvidia / label / cuda -11.3.0 Note Some CUDA releases do not move to new versions of all installable components. When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as: conda install cuda - c nvidia / label / cuda -11.3.0 - c nvidia / label / cuda -11.3.1 This example will install all packages released as part of CUDA 11.3.1. 2.5.'},\n",
       " {'id': 91,\n",
       "  'content': 'Use a Suitable Driver Model \\uf0c1 On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate: The WDDM driver model is used for display devices. The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model. TCC is enabled by default on most recent NVIDIA Tesla GPUs. To check which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details). Note Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device. Note NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode. 2.6. Verify the Installation \\uf0c1 Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the included sample programs. 2.6.1. Running the Compiled Examples \\uf0c1 The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window. You can display a Command Prompt window by going to: Start > All Programs > Accessories > Command Prompt CUDA Samples are located in https://github.com/nvidia/cuda-samples . To use the samples, clone the project, build the samples, and run them using the instructions on the Github page.'},\n",
       " {'id': 92,\n",
       "  'content': 'To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program. The sample can be built using the provided VS solution files in the deviceQuery folder. This assumes that you used the default installation directory structure. If CUDA is installed and configured correctly, the output should look similar to Figure 1 . Figure 1 Valid Results from deviceQuery CUDA Sample \\uf0c1 The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed. If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed. Running the bandwidthTest program, located in the same directory as deviceQuery above, ensures that the system and the CUDA-capable device are able to communicate correctly. The output should resemble Figure 2 . Figure 2 Valid Results from bandwidthTest CUDA Sample \\uf0c1 The device name (second line) and the bandwidth numbers vary from system to system. The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed. If the tests do not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed. To see a graphical representation of what CUDA can do, run the particles sample at https://github.com/NVIDIA/cuda-samples/tree/master/Samples/particles 3. Pip Wheels \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. py -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.ngc.nvidia.com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version. “cu12” should be read as “cuda12”. nvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 nvidia-opencl-cu12 These metapackages install the following packages: nvidia-cublas-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 nvidia-opencl-cu125 4. Compiling CUDA Programs \\uf0c1 The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code.'},\n",
       " {'id': 93,\n",
       "  'content': 'To build the Windows projects (for release or debug mode), use the provided *.sln solution files for Microsoft Visual Studio 2015 (deprecated in CUDA 11.1), 2017, 2019, or 2022. You can use either the solution files located in each of the examples directories in https://github.com/nvidia/cuda-samples 4.1. Compiling Sample Projects \\uf0c1 The bandwidthTest project is a good sample project to build and run. It is located in https://github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest . If you elected to use the default installation location, the output is placed in CUDA Samples\\\\v12.5\\\\bin\\\\win64\\\\Release . Build the program using the appropriate solution file and run the executable.'},\n",
       " {'id': 94,\n",
       "  'content': 'If all works correctly, the output should be similar to Figure 2 . 4.2. Sample Projects \\uf0c1 The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects. A few of the example projects require some additional setup. These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated .props files are. The environment variable is set automatically using the Build Customization CUDA 12.5.props file, and is installed automatically as part of the CUDA Toolkit installation process. Table 3 CUDA Visual Studio .props locations \\uf0c1 Visual Studio CUDA 12.5 .props file Install Directory Visual Studio 2015 (deprecated) C:Program Files (x86)\\\\MSBuild\\\\Microsoft.Cpp\\\\v4.0\\\\V140\\\\BuildCustomizations Visual Studio 2017 \\\\Common7\\\\IDE\\\\VC\\\\VCTargets\\\\BuildCustomizations Visual Studio 2019 C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Professional\\\\MSBuild\\\\Microsoft\\\\VC\\\\v160\\\\BuildCustomizations Visual Studio 2022 C:\\\\Program Files\\\\Microsoft Visual Studio\\\\2022\\\\Professional\\\\MSBuild\\\\Microsoft\\\\VC\\\\v170\\\\BuildCustomizations You can reference this CUDA 12.5.props file when building your own CUDA applications. 4.3. Build Customizations for New Projects \\uf0c1 When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations. To accomplish this, click File-> New | Project… NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version. For example, selecting the “CUDA 12.5 Runtime” template will configure your project for use with the CUDA 12.5 Toolkit. The new project is technically a C++ project (.vcxproj) that is preconfigured to use NVIDIA’s Build Customizations. All standard capabilities of Visual Studio C++ projects will be available. To specify a custom CUDA Toolkit location, under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field as desired. Note that the selected toolkit must match the version of the Build Customizations. Note A supported version of MSVC must be installed to use this feature. 4.4. Build Customizations for Existing Projects \\uf0c1 When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations. This can be done using one of the following two methods: Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizations… , then select the CUDA Toolkit version you would like to target. Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit. First add a CUDA build customization to your project as above. Then, right click on the project name and select Properties . Under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) . Note that the $(CUDA_PATH) environment variable is set by the installer. While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2. If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes. You can access the value of the $(CUDA_PATH) environment variable via the following steps: Open a run window from the Start Menu. Run: control sysdm.cpl Select the Advanced tab at the top of the window. Click Environment Variables at the bottom of the window. Files which contain CUDA code must be marked as a CUDA C/C++ file. This can done when adding the file by right clicking the project you wish to add the file to, selecting Add New Item , selecting NVIDIA CUDA 12.5\\\\CodeCUDA C/C++ File , and then selecting the file you wish to add. For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the following: msbuild /t:Rebuild /p:CudaToolkitDir=\"drive:/path/to/new/toolkit/\" 5. Additional Considerations \\uf0c1 Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C Programming Guide, located in the CUDA Toolkit documentation directory. A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Visual Studio Edition, and NVIDIA Visual Profiler. For technical support on programming questions, consult and participate in the developer forums at https://developer.nvidia.com/cuda/ . 6.'},\n",
       " {'id': 95,\n",
       "  'content': 'Notices \\uf0c1 6.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 6.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 96,\n",
       "  'content': '6.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 97,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Introduction 1.1.'},\n",
       " {'id': 98,\n",
       "  'content': 'System Requirements 1.2. OS Support Policy 1.3. Host Compiler Support Policy 1.3.1. Supported C++ Dialects 1.4. About This Document 2.'},\n",
       " {'id': 99,\n",
       "  'content': 'Pre-installation Actions 2.1. Verify You Have a CUDA-Capable GPU 2.2. Verify You Have a Supported Version of Linux 2.3. Verify the System Has gcc Installed 2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed 2.5. Install GPUDirect Storage 2.6. Choose an Installation Method 2.7. Download the NVIDIA CUDA Toolkit 2.8. Address Custom xorg.conf, If Applicable 2.9. Handle Conflicting Installation Methods 3. Package Manager Installation 3.1. Overview 3.2. RHEL 8 / Rocky 8 3.2.1. Prepare RHEL 8 / Rocky 8 3.2.2. Local Repo Installation for RHEL 8 / Rocky 8 3.2.3. Network Repo Installation for RHEL 8 / Rocky 8 3.2.4. Common Instructions for RHEL 8 / Rocky 8 3.3. RHEL 9 / Rocky 9 3.3.1. Prepare RHEL 9 / Rocky 9 3.3.2. Local Repo Installation for RHEL 9 / Rocky 9 3.3.3. Network Repo Installation for RHEL 9 / Rocky 9 3.3.4. Common Instructions for RHEL 9 / Rocky 9 3.4. KylinOS 10 3.4.1. Prepare KylinOS 10 3.4.2. Local Repo Installation for KylinOS 3.4.3. Network Repo Installation for KylinOS 3.4.4. Common Instructions for KylinOS 10 3.5. Fedora 3.5.1. Prepare Fedora 3.5.2. Local Repo Installation for Fedora 3.5.3. Network Repo Installation for Fedora 3.5.4. Common Installation Instructions for Fedora 3.6. SLES 3.6.1. Prepare SLES 3.6.2. Local Repo Installation for SLES 3.6.3. Network Repo Installation for SLES 3.6.4. Common Installation Instructions for SLES 3.7. OpenSUSE 3.7.1. Prepare OpenSUSE 3.7.2. Local Repo Installation for OpenSUSE 3.7.3. Network Repo Installation for OpenSUSE 3.7.4. Common Installation Instructions for OpenSUSE 3.8. WSL 3.8.1. Prepare WSL 3.8.2. Local Repo Installation for WSL 3.8.3. Network Repo Installation for WSL 3.8.4. Common Installation Instructions for WSL 3.9. Ubuntu 3.9.1. Prepare Ubuntu 3.9.2. Local Repo Installation for Ubuntu 3.9.3. Network Repo Installation for Ubuntu 3.9.4. Common Installation Instructions for Ubuntu 3.10. Debian 3.10.1. Prepare Debian 3.10.2. Local Repo Installation for Debian 3.10.3. Network Repo Installation for Debian 3.10.4. Common Installation Instructions for Debian 3.11. Amazon Linux 2023 3.11.1. Prepare Amazon Linux 2023 3.11.2. Local Repo Installation for Amazon Linux 3.11.3. Network Repo Installation for Amazon Linux 3.11.4. Common Installation Instructions for Amazon Linux 3.12. Additional Package Manager Capabilities 3.12.1. Available Packages 3.12.2. Meta Packages 3.12.3. Optional 32-bit Packages for Linux x86_64 .deb/.rpm 3.12.4. Package Upgrades 4.'},\n",
       " {'id': 100,\n",
       "  'content': 'Driver Installation 5. NVIDIA Open GPU Kernel Modules 5.1. CUDA Runfile 5.2. Debian 5.3. Fedora 5.4. KylinOS 10 5.5. RHEL 9 and Rocky 9 5.6. RHEL 8 and Rocky 8 5.7. OpenSUSE and SLES 5.8.'},\n",
       " {'id': 101,\n",
       "  'content': 'Ubuntu 6. Precompiled Streams 6.1. Precompiled Streams Support Matrix 6.2. Modularity Profiles 7. Kickstart Installation 7.1. RHEL 8 / Rocky Linux 8 7.2. RHEL 9 / Rocky Linux 9 8. Runfile Installation 8.1. Runfile Overview 8.2. Installation 8.3. Disabling Nouveau 8.3.1. Fedora 8.3.2. RHEL / Rocky and KylinOS 8.3.3. OpenSUSE 8.3.4.'},\n",
       " {'id': 102, 'content': 'SLES 8.3.5. WSL 8.3.6. Ubuntu 8.3.7.'},\n",
       " {'id': 103,\n",
       "  'content': 'Debian 8.4. Device Node Verification 8.5. Advanced Options 8.6.'},\n",
       " {'id': 104,\n",
       "  'content': 'Uninstallation 9. Conda Installation 9.1. Conda Overview 9.2. Installing CUDA Using Conda 9.3. Uninstalling CUDA Using Conda 9.4. Installing Previous CUDA Releases 9.5. Upgrading from cudatoolkit Package 10. Pip Wheels 11. Tarball and Zip Archive Deliverables 11.1. Parsing Redistrib JSON 11.2. Importing Tarballs into CMake 11.3. Importing Tarballs into Bazel 12. CUDA Cross-Platform Environment 12.1. CUDA Cross-Platform Installation 12.2. CUDA Cross-Platform Samples 13. Post-installation Actions 13.1. Mandatory Actions 13.1.1. Environment Setup 13.2. Recommended Actions 13.2.1. Install Persistence Daemon 13.2.2. Install Writable Samples 13.2.3. Verify the Installation 13.2.3.1. Verify the Driver Version 13.2.3.2. Running the Binaries 13.2.4. Install Nsight Eclipse Plugins 13.2.5. Local Repo Removal 13.3. Optional Actions 13.3.1.'},\n",
       " {'id': 105,\n",
       "  'content': 'Install Third-party Libraries 13.3.2. Install the Source Code for cuda-gdb 13.3.3. Select the Active Version of CUDA 14. Advanced Setup 15.'},\n",
       " {'id': 106,\n",
       "  'content': 'Frequently Asked Questions 15.1. How do I install the Toolkit in a different location? 15.2. Why do I see “nvcc: No such file or directory” when I try to build a CUDA application? 15.3. Why do I see “error while loading shared libraries: : cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library? 15.4. Why do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu? 15.5. How can I tell X to ignore a GPU for compute-only use? 15.6. Why doesn’t the cuda-repo package install the CUDA Toolkit and Drivers? 15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04? 15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update? 15.9. How do I install a CUDA driver with a version less than 367 using a network repo? 15.10. How do I install an older CUDA version using a network repo? 15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency? 15.12. How do I handle “Errors were encountered while processing: glx-diversions”? 16.'},\n",
       " {'id': 107,\n",
       "  'content': 'Additional Considerations 17. Switching between Driver Module Flavors 18. Removing CUDA Toolkit and Driver 19. Notices 19.1. Notice 19.2. OpenCL 19.3. Trademarks 20.'},\n",
       " {'id': 108,\n",
       "  'content': 'Copyright Installation Guide for Linux » 1. Introduction v12.5 | PDF | Archive NVIDIA CUDA Installation Guide for Linux The installation instructions for the CUDA Toolkit on Linux. Introduction \\uf0c1 CUDA ® is a parallel computing platform and programming model invented by NVIDIA ® . It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU). CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms. With CUDA C/C\\ufeff+\\ufeff+, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation. Support heterogeneous computation where applications use both the CPU and GPU. Serial portions of applications are run on the CPU, and parallel portions are offloaded to the GPU. As such, CUDA can be incrementally applied to existing applications. The CPU and GPU are treated as separate devices that have their own memory spaces. This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources. CUDA-capable GPUs have hundreds of cores that can collectively run thousands of computing threads. These cores have shared resources including a register file and a shared memory. The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus. This guide will show you how to install and check the correct operation of the CUDA development tools. 1.1. System Requirements \\uf0c1 To use NVIDIA CUDA on your system, you will need the following installed: CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain CUDA Toolkit (available at https://developer.nvidia.com/cuda-downloads ) The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release. The following table lists the supported Linux distributions. Please review the footnotes associated with the table. Table 1 Native Linux Distribution Support in CUDA 12.5 Update 1 \\uf0c1 Distribution Kernel 1 Default GCC GLIBC x86_64 RHEL 9.y (y =10.x >=11.x >=22.x 22.x 1.4. About This Document \\uf0c1 This document is intended for readers familiar with the Linux environment and the compilation of C programs from the command line. You do not need previous experience with CUDA or experience with parallel computation. Note: This guide covers installation only on systems with X Windows installed.'},\n",
       " {'id': 109,\n",
       "  'content': 'Note Many commands in this document might require superuser privileges. On most distributions of Linux, this will require you to log in as root. For systems that have enabled the sudo package, use the sudo prefix for all necessary commands. 2. Pre-installation Actions \\uf0c1 Some actions must be taken before the CUDA Toolkit and Driver can be installed on Linux: Verify the system has a CUDA-capable GPU. Verify the system is running a supported version of Linux.'},\n",
       " {'id': 110,\n",
       "  'content': 'Verify the system has gcc installed. Verify the system has the correct kernel headers and development packages installed. Download the NVIDIA CUDA Toolkit. Handle conflicting installation methods. Note You can override the install-time prerequisite checks by running the installer with the -override flag. Remember that the prerequisites will still be required to use the NVIDIA CUDA Toolkit. 2.1. Verify You Have a CUDA-Capable GPU \\uf0c1 To verify that your GPU is CUDA-capable, go to your distribution’s equivalent of System Properties, or, from the command line, enter: lspci | grep -i nvidia If you do not see any settings, update the PCI hardware database that Linux maintains by entering update-pciids (generally found in /sbin ) at the command line and rerun the previous lspci command. If your graphics card is from NVIDIA and it is listed in https://developer.nvidia.com/cuda-gpus , your GPU is CUDA-capable. The Release Notes for the CUDA Toolkit also contain a list of supported products. 2.2. Verify You Have a Supported Version of Linux \\uf0c1 The CUDA Development Tools are only supported on some specific distributions of Linux. These are listed in the CUDA Toolkit release notes. To determine which distribution and release number you’re running, type the following at the command line: uname -m && cat /etc/*release You should see output similar to the following, modified for your particular system: x86_64 Red Hat Enterprise Linux Workstation release 6.0 (Santiago) The x86_64 line indicates you are running on a 64-bit system. The remainder gives information about your distribution.'},\n",
       " {'id': 111,\n",
       "  'content': '2.3. Verify the System Has gcc Installed \\uf0c1 The gcc compiler is required for development using the CUDA Toolkit. It is not required for running CUDA applications. It is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly. To verify the version of gcc installed on your system, type the following on the command line: gcc --version If an error message displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web. 2.4. Verify the System has the Correct Kernel Headers and Development Packages Installed \\uf0c1 The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt. For example, if your system is running kernel version 3.17.4-301, the 3.17.4-301 kernel headers and development packages must also be installed. While the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed. However, it will install the latest version of these packages, which may or may not match the version of the kernel your system is using. Therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version. The version of the kernel your system is running can be found by running the following command: uname -r This is the version of the kernel headers and development packages that must be installed prior to installing the CUDA Drivers. This command will be used multiple times below to specify the version of the packages to install. Note that below are the common-case scenarios for kernel usage. More advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running. Note If you perform a system update which changes the version of the Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed. Otherwise, the CUDA Driver will fail to work with the new kernel. 2.5. Install GPUDirect Storage \\uf0c1 If you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package. GDS packages can be installed using the CUDA packaging guide. Follow the instructions in MLNX_OFED Requirements and Installation . GDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode. Installation instructions for them differ slightly. Compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations. Full GDS support is restricted to the following Linux distros: Ubuntu 20.04, Ubuntu 22.04 RHEL 8.3, RHEL 8.4, RHEL 9.0 Starting with CUDA toolkit 12.2.2, GDS kernel driver package nvidia-gds version 12.2.2-1 (provided by nvidia-fs-dkms 2.17.5-1) and above is only supported with the NVIDIA open kernel driver. Follow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages. 2.6. Choose an Installation Method \\uf0c1 The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages). The distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution’s native package management system.'},\n",
       " {'id': 112,\n",
       "  'content': 'The distribution-specific packages interface with the distribution’s native package management system. It is recommended to use the distribution-specific packages, where possible. Note For both native as well as cross development, the toolkit must be installed using the distribution-specific installer. See the CUDA Cross-Platform Installation section for more details. 2.7. Download the NVIDIA CUDA Toolkit \\uf0c1 The NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads . Choose the platform you are using and download the NVIDIA CUDA Toolkit. The CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources. Download Verification The download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.5.1/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again. To calculate the MD5 checksum of the downloaded file, run the following: md5sum 2.8. Address Custom xorg.conf, If Applicable \\uf0c1 The driver relies on an automatically generated xorg.conf file at /etc/X11/xorg.conf .'},\n",
       " {'id': 113,\n",
       "  'content': 'If a custom-built xorg.conf file is present, this functionality will be disabled and the driver may not work. You can try removing the existing xorg.conf file, or adding the contents of /etc/X11/xorg.conf.d/00-nvidia.conf to the xorg.conf file. The xorg.conf file will most likely need manual tweaking for systems with a non-trivial GPU configuration. 2.9. Handle Conflicting Installation Methods \\uf0c1 Before installing CUDA, any previous installations that could conflict should be uninstalled. This will not affect systems which have not had CUDA installed previously, or systems where the installation method has been preserved (RPM/Deb vs. Runfile). See the following charts for specifics. Table 3 CUDA Toolkit Installation Compatibility Matrix \\uf0c1 Installed Toolkit Version == X.Y Installed Toolkit Version != X.Y RPM/Deb run RPM/Deb run Installing Toolkit Version X.Y RPM/Deb No Action Uninstall Run No Action No Action run Uninstall RPM/Deb Uninstall Run No Action No Action Table 4 NVIDIA Driver Installation Compatibility Matrix \\uf0c1 Installed Driver Version == X.Y Installed Driver Version != X.Y RPM/Deb run RPM/Deb run Installing Driver Version X.Y RPM/Deb No Action Uninstall Run No Action Uninstall Run run Uninstall RPM/Deb No Action Uninstall RPM/Deb No Action Use the following command to uninstall a Toolkit runfile installation: sudo /usr/local/cuda-X.Y/bin/cuda-uninstaller Use the following command to uninstall a Driver runfile installation: sudo /usr/bin/nvidia-uninstall Use the following commands to uninstall an RPM/Deb installation: sudo dnf remove # RHEL 8 / Rocky Linux 8 sudo dnf remove # Fedora sudo zypper remove # OpenSUSE / SLES sudo apt-get --purge remove # Ubuntu 3. Package Manager Installation \\uf0c1 Basic instructions can be found in the Quick Start Guide .'},\n",
       " {'id': 114,\n",
       "  'content': 'Read on for more detailed instructions. 3.1. Overview \\uf0c1 Installation using RPM or Debian packages interfaces with your system’s package management system. When using RPM or Debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in /var/. Such a package only informs the package manager where to find the actual installation packages, but will not install them. If the online network repository is enabled, RPM or Debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper. Distribution-specific instructions detail how to install CUDA: RHEL 8 / Rocky Linux 8 RHEL 9 / Rocky Linux 9 KylinOS 10 Fedora SLES OpenSUSE WSL Ubuntu Debian Amazon Linux 2023 Finally, some helpful package manager capabilities are detailed. These instructions are for native development only.'},\n",
       " {'id': 115,\n",
       "  'content': 'For cross-platform development, see the CUDA Cross-Platform Environment section. Note Optional components such as nvidia-fs , libnvidia_nscq , and fabricmanager are not installed by default and will have to be installed separately as needed. 3.2. RHEL 8 / Rocky 8 \\uf0c1 3.2.1. Prepare RHEL 8 / Rocky 8 \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) If matching kernel-headers and kernel-devel packages are not available for the currently running kernel version, you may need to use the previously shipped version of these packages.'},\n",
       " {'id': 116,\n",
       "  'content': 'See https://bugzilla.redhat.com/show_bug.cgi?id=1986132 for more information. Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau . Those packages are only available on third-party repositories, such as EPEL . Any such third-party repositories must be added to the package manager repository database before installing the NVIDIA driver RPM packages, or missing dependencies will prevent the installation from proceeding. To enable EPEL: sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Enable optional repos: On RHEL 8 Linux only, execute the following steps to enable optional repositories. On x86_64 systems: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.2.2. Local Repo Installation for RHEL 8 / Rocky 8 \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-*..rpm 3.2.3. Network Repo Installation for RHEL 8 / Rocky 8 \\uf0c1 Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: rhel8/cross-linux-sbsa rhel8/sbsa rhel8/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time. Indicate you accept the change when prompted. For upgrades, you must also also fetch an updated .repo entry: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Clean Yum repository cache: sudo dnf clean expire-cache 3.2.4. Common Instructions for RHEL 8 / Rocky 8 \\uf0c1 These instructions apply to both local and network installation. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. For pre-existing projects which use libcuda.so , it may be useful to add a symbolic link from libcuda.so in the /usr/lib{,64} directory. Reboot the system: sudo reboot Perform the post-installation actions.'},\n",
       " {'id': 117,\n",
       "  'content': '3.3. RHEL 9 / Rocky 9 \\uf0c1 3.3.1. Prepare RHEL 9 / Rocky 9 \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau . To enable EPEL: sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm Enable optional repos: On RHEL 9 Linux only, execute the following steps to enable optional repositories. On x86_64 systems: subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-9-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.3.2. Local Repo Installation for RHEL 9 / Rocky 9 \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-*..rpm 3.3.3. Network Repo Installation for RHEL 9 / Rocky 9 \\uf0c1 Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: rhel9/cross-linux-sbsa rhel9/sbsa rhel9/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . For upgrades, you must also also fetch an updated .repo entry: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Clean Yum repository cache: sudo dnf clean expire-cache 3.3.4. Common Instructions for RHEL 9 / Rocky 9 \\uf0c1 These instructions apply to both local and network installation.'},\n",
       " {'id': 118,\n",
       "  'content': '3.4. KylinOS 10 \\uf0c1 3.4.1. Prepare KylinOS 10 \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Choose an installation method: local repo or network repo . 3.4.2. Local Repo Installation for KylinOS \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo-kylin10-X-Y-local-*..rpm 3.4.3. Network Repo Installation for KylinOS \\uf0c1 Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/kylin10/x86_64/cuda-$distro.repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . Clean Yum repository cache: sudo dnf clean expire-cache 3.4.4. Common Instructions for KylinOS 10 \\uf0c1 These instructions apply to both local and network installation. 3.5.'},\n",
       " {'id': 119,\n",
       "  'content': 'Fedora \\uf0c1 3.5.1. Prepare Fedora \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.5.2. Local Repo Installation for Fedora \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-*.x86_64.rpm where distro is fedora37 or fedora39 , for example. 3.5.3. Network Repo Installation for Fedora \\uf0c1 Enable the network repo: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo where $distro should be replaced by one of the following: fedora37 fedora39 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of Fedora, the dnf package manager will prompt the user to accept new keys when installing packages the first time. For upgrades, you must also fetch an updated .repo entry: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/x86_64/cuda-$distro.repo Clean DNF repository cache: sudo dnf clean expire-cache 3.5.4. Common Installation Instructions for Fedora \\uf0c1 These instructions apply to both local and network installation for Fedora. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Note The CUDA driver installation may fail if the RPMFusion non-free repository is enabled. In this case, CUDA installations should temporarily disable the RPMFusion non-free repository. sudo dnf --disablerepo=\"rpmfusion-nonfree*\" install cuda It may be necessary to rebuild the grub configuration files, particularly if you use a non-default partition scheme. If so, then run this below command, and reboot the system: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system: sudo reboot Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. Perform the post-installation actions.'},\n",
       " {'id': 120,\n",
       "  'content': \"3.6. SLES \\uf0c1 3.6.1. Prepare SLES \\uf0c1 Perform the pre-installation actions. The kernel development packages for the currently running kernel can be installed with: sudo zypper install -y kernel--devel= To run the above command, you will need the variant and version of the currently running kernel. Use the output of the uname command to determine the currently running kernel’s variant and version: $ uname -r 3.16.6-2-default In the above example, the variant is default and version is 3.16.6-2 . The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\\\\-default//') The kernel headers and development packages for the currently running kernel can be installed with: sudo zypper install -y kernel--devel= On SLES12 SP4, install the Mesa-libgl-devel Linux packages before proceeding. See Mesa-libGL-devel. Add the user to the video group: sudo usermod -a -G video Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.6.2. Local Repo Installation for SLES \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo-sles15-X-Y-local-*.x86_64.rpm 3.6.3. Network Repo Installation for SLES \\uf0c1 Enable the network repo: sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo where $distro/$arch should be replaced by one of the following: sles15/cross-linux-sbsa sles15/sbsa sles15/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On a fresh installation of SLES, the zypper package manager will prompt the user to accept new keys when installing packages the first time. For upgrades, you must also also fetch an updated .repo entry: sudo zypper removerepo cuda-$distro-$arch sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repo Refresh Zypper repository cache: sudo SUSEConnect --product PackageHub/15/ sudo zypper refresh 3.6.4. Common Installation Instructions for SLES \\uf0c1 These instructions apply to both local and network installation for SLES. Install CUDA SDK: sudo zypper install cuda-toolkit Install CUDA Samples GL dependencies: Refer to CUDA Cross-Platform Samples . 3.7.\"},\n",
       " {'id': 121,\n",
       "  'content': 'OpenSUSE \\uf0c1 3.7.1. Prepare OpenSUSE \\uf0c1 Perform the pre-installation actions. The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed \\'s/\\\\-default//\\') Add the user to the video group: sudo usermod -a -G video Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo . 3.7.2. Local Repo Installation for OpenSUSE \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo-opensuse15-.x86_64.rpm 3.7.3. Network Repo Installation for OpenSUSE \\uf0c1 Enable the network repo: sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685 . On fresh installation of openSUSE, the zypper package manager will prompt the user to accept new keys when installing packages the first time. For upgrades, you must also also fetch an updated .repo entry: sudo zypper removerepo cuda-opensuse15-x86_64 sudo zypper addrepo https://developer.download.nvidia.com/compute/cuda/repos/opensuse15/x86_64/cuda-opensuse15.repo Refresh Zypper repository cache: sudo zypper refresh 3.7.4. Common Installation Instructions for OpenSUSE \\uf0c1 These instructions apply to both local and network installation for OpenSUSE. Install CUDA SDK: sudo zypper install cuda-toolkit Reboot the system: sudo reboot Perform the post-installation actions. 3.8. WSL \\uf0c1 These instructions must be used if you are installing in a WSL environment. Do not use the Ubuntu instructions in this case; it is important to not install the cuda-drivers packages within the WSL environment. 3.8.1. Prepare WSL \\uf0c1 Perform the pre-installation actions. Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.8.2. Local Repo Installation for WSL \\uf0c1 Install local repositiry on file system: sudo dpkg -i cuda-repo-wsl-ubuntu-X-Y-local_*_x86_64.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo-wsl-ubuntu-X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ 3.8.3. Network Repo Installation for WSL \\uf0c1 The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc . This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended. Install the newcuda-keyring package: wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-archive-keyring.gpg sudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg Enable the network repository: echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/ /\" | sudo tee /etc/apt/sources.list.d/cuda-wsl-ubuntu-x86_64.list Add pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600 3.8.4. Common Installation Instructions for WSL \\uf0c1 These instructions apply to both local and network installation for WSL. Update the Apt repository cache: sudo apt-get update Install CUDA SDK: sudo apt-get install cuda-toolkit Perform the post-installation actions. 3.9.'},\n",
       " {'id': 122,\n",
       "  'content': 'Ubuntu \\uf0c1 3.9.1. Prepare Ubuntu \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo .'},\n",
       " {'id': 123,\n",
       "  'content': '3.9.2. Local Repo Installation for Ubuntu \\uf0c1 Install local repository on file system: sudo dpkg -i cuda-repo-__.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo--X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ Add pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos//x86_64/cuda-.pin sudo mv cuda-.pin /etc/apt/preferences.d/cuda-repository-pin-600 3.9.3. Network Repo Installation for Ubuntu \\uf0c1 The new GPG public key for the CUDA repository is 3bf863cc . Install the new cuda-keyring package: wget https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-keyring_1.1-1_all.deb sudo dpkg -i cuda-keyring_1.1-1_all.deb where $distro/$arch should be replaced by one of the following: ubuntu1604/x86_64 ubuntu1804/cross-linux-sbsa ubuntu1804/sbsa ubuntu1804/x86_64 ubuntu2004/cross-linux-aarch64 ubuntu2004/arm64 ubuntu2004/cross-linux-sbsa ubuntu2004/sbsa ubuntu2004/x86_64 ubuntu2204/sbsa ubuntu2204/x86_64 Note arm64-Jetson repos: native: $distro/arm64 cross: $distro/cross-linux-aarch64 sudo dpkg -i cuda-keyring_1.1-1_all.deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-archive-keyring.gpg sudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg Enable the network repository: echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/// /\" | sudo tee /etc/apt/sources.list.d/cuda--.list Add pin file to prioritize CUDA repository: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-.pin sudo mv cuda-.pin /etc/apt/preferences.d/cuda-repository-pin-600 3.9.4. Common Installation Instructions for Ubuntu \\uf0c1 These instructions apply to both local and network installation for Ubuntu.'},\n",
       " {'id': 124,\n",
       "  'content': 'Update the Apt repository cache: sudo apt-get update Install CUDA SDK: Note These two commands must be executed separately. sudo apt-get install cuda-toolkit To include all GDS packages: sudo apt-get install nvidia-gds Reboot the system sudo reboot Perform the Post-installation Actions 3.10. Debian \\uf0c1 3.10.1.'},\n",
       " {'id': 125,\n",
       "  'content': 'Prepare Debian \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Enable the contrib repository: sudo add-apt-repository contrib Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo . 3.10.2. Local Repo Installation for Debian \\uf0c1 Install local repository on file system: sudo dpkg -i cuda-repo--X-Y-local_*_x86_64.deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo--X-Y-local/cuda-*-keyring.gpg /usr/share/keyrings/ 3.10.3. Network Repo Installation for Debian \\uf0c1 The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc . Install the new cuda-keyring package: wget https://developer.download.nvidia.com/compute/cuda/repos///cuda-keyring_1.1-1_all.deb where $distro/$arch should be replaced by one of the following: debian10/x86_64 debian11/x86_64 sudo dpkg -i cuda-keyring_1.1-1_all.deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https://developer.download.nvidia.com/compute/cuda/repos//x86_64/cuda-archive-keyring.gpg sudo mv cuda-archive-keyring.gpg /usr/share/keyrings/cuda-archive-keyring.gpg Enable the network repository: echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos//x86_64/ /\" | sudo tee /etc/apt/sources.list.d/cuda--x86_64.list 3.10.4. Common Installation Instructions for Debian \\uf0c1 These instructions apply to both local and network installation for Debian.'},\n",
       " {'id': 126,\n",
       "  'content': 'Update the Apt repository cache: sudo apt-get update Note If you are using Debian 10, you may instead need to run: sudo apt-get --allow-releaseinfo-change update Install CUDA SDK: sudo apt-get -y install cuda Reboot the system: sudo reboot Perform the post-installation actions. 3.11.'},\n",
       " {'id': 127,\n",
       "  'content': 'Amazon Linux 2023 \\uf0c1 3.11.1. Prepare Amazon Linux 2023 \\uf0c1 Perform the pre-installation actions. The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo . 3.11.2. Local Repo Installation for Amazon Linux \\uf0c1 Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*.x86_64.rpm 3.11.3. Network Repo Installation for Amazon Linux \\uf0c1 Enable the network repository: sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo Clean DNF repository cache: sudo dnf clean expire-cache 3.11.4. Common Installation Instructions for Amazon Linux \\uf0c1 These instructions apply to both local and network installation for Amazon Linux. Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda.so symbolic link, if necessary: The libcuda.so library is installed in the /usr/lib{,64}/nvidia directory. 3.12.'},\n",
       " {'id': 128,\n",
       "  'content': 'Additional Package Manager Capabilities \\uf0c1 Below are some additional capabilities of the package manager that users can take advantage of. 3.12.1. Available Packages \\uf0c1 The recommended installation package is the cuda package. This package will install the full set of other CUDA packages required for native development and should cover most scenarios. The cuda package installs all the available packages for native developments. That includes the compiler, the debugger, the profiler, the math libraries, and so on. For x86_64 platforms, this also includes Nsight Eclipse Edition and the visual profilers. It also includes the NVIDIA driver package. On supported platforms, the cuda-cross-aarch64 and cuda-cross-sbsa packages install all the packages required for cross-platform development to arm64-Jetson and arm64-Server, respectively. The libraries and header files of the target architecture’s display driver package are also installed to enable the cross compilation of driver applications. The cuda-cross- packages do not install the native display driver. Note 32-bit compilation native and cross-compilation is removed from CUDA 12.0 and later Toolkit. Use the CUDA Toolkit from earlier releases for 32-bit compilation. CUDA Driver will continue to support running existing 32-bit applications on existing GPUs except Hopper. Hopper does not support 32-bit applications. Ada will be the last architecture with driver support for 32-bit applications.'},\n",
       " {'id': 129,\n",
       "  'content': 'The packages installed by the packages above can also be installed individually by specifying their names explicitly. The list of available packages be can obtained with: yum --disablerepo=\"*\" --enablerepo=\"cuda*\" list available # RedHat dnf --disablerepo=\"*\" --enablerepo=\"cuda*\" list available # Fedora zypper packages -r cuda # OpenSUSE & SLES cat /var/lib/apt/lists/*cuda*Packages | grep \"Package:\" # Ubuntu 3.12.2. Meta Packages \\uf0c1 Meta packages are RPM/Deb/Conda packages which contain no (or few) files but have multiple dependencies. They are used to install many CUDA packages when you may not know the details of the packages you want. The following table lists the meta packages. Table 5 Meta Packages Available for CUDA 12.4 \\uf0c1 Meta Package Purpose cuda Installs all CUDA Toolkit and Driver packages. Handles upgrading to the next version of the cuda package when it’s released. cuda-12-5 Installs all CUDA Toolkit and Driver packages. Remains at version 12.5 until an additional version of CUDA is installed. cuda-toolkit-12-5 Installs all CUDA Toolkit packages required to develop CUDA applications. Does not include the driver. cuda-toolkit-15 Installs all CUDA Toolkit packages required to develop applications. Will not upgrade beyond the 12.x series toolkits. cuda-toolkit Installs all CUDA Toolkit packages required to develop applications. Handles upgrading to the next 12.x version of CUDA when it’s released. cuda-tools-12-5 Installs all CUDA command line and visual tools. cuda-runtime-12-5 Installs all CUDA Toolkit packages required to run CUDA applications, as well as the Driver packages. cuda-compiler-12-5 Installs all CUDA compiler packages. cuda-libraries-12-5 Installs all runtime CUDA Library packages. cuda-libraries-dev-12-5 Installs all development CUDA Library packages. cuda-drivers Installs all NVIDIA Driver packages with proprietary kernel modules. Handles upgrading to the next version of the Driver packages when they’re released. cuda-drivers-555 Installs all NVIDIA Driver packages with proprietary kernel modules. Will not upgrade beyond the 555 branch drivers. 3.12.3. Optional 32-bit Packages for Linux x86_64 .deb/.rpm \\uf0c1 These packages provide 32-bit driver libraries needed for things such as Steam (popular game app store/launcher), older video games, and some compute applications. For Debian 10 and Debian 11: sudo dpkg --add-architecture i386 sudo apt-get update sudo apt-get install libcuda1-i386 nvidia-driver-libs-i386 For Debian 12: sudo dpkg --add-architecture i386 sudo apt-get update apt install nvidia-driver-libs:i386 For Ubuntu: sudo dpkg --add-architecture i386 sudo apt-get update sudo apt-get install libnvidia-compute-:i386 libnvidia-decode-:i386 \\\\ libnvidia-encode-:i386 libnvidia-extra-:i386 libnvidia-fbc1-:i386 \\\\ libnvidia-gl-:i386 Where is the driver version, for example 495. For Fedora and RHEL8+: sudo dnf install nvidia-driver-cuda-libs.i686 nvidia-driver-devel.i686 \\\\ nvidia-driver-libs.i686 nvidia-driver-NvFBCOpenGL.i686 nvidia-driver-NVML.i686 Note There is no modularity profile support. For openSUSE/SLES: sudo zypper install nvidia-compute-G06-32bit nvidia-gl-G06-32bit nvidia-video-G06-32bit 3.12.4. Package Upgrades \\uf0c1 The cuda package points to the latest stable release of the CUDA Toolkit. When a new version is available, use the following commands to upgrade the toolkit and driver: sudo dnf install cuda-toolkit # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-toolkit # OpenSUSE and SLES sudo apt-get install cuda-toolkit # Ubuntu and Debian The cuda-cross- packages can also be upgraded in the same manner. The cuda-drivers package points to the latest driver release available in the CUDA repository. When a new version is available, use the following commands to upgrade the driver: sudo dnf module install nvidia-driver:latest-dkms # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-drivers nvidia-gfxG04-kmp-default # OpenSUSE and SLES sudo apt-get install cuda-drivers # Ubuntu and Debian Some desktop environments, such as GNOME or KDE, will display a notification alert when new packages are available. To avoid any automatic upgrade, and lock down the toolkit installation to the X.Y release, install the cuda-X-Y or cuda-cross--X-Y package. Side-by-side installations are supported. For instance, to install both the X.Y CUDA Toolkit and the X.Y+1 CUDA Toolkit, install the cuda-X.Y and cuda-X.Y+1 packages. 4. Driver Installation \\uf0c1 This section is for users who want to install a specific driver version. For Debian and Ubuntu: sudo apt-get install cuda-drivers- For example: sudo apt-get install cuda-drivers-535 For OpenSUSE and SLES: sudo zypper -v install cuda-drivers- For example: sudo zypper -v install cuda-drivers-550 This allows you to get the highest version in the specified branch. For Fedora and RHEL8+: sudo dnf module install nvidia-driver:/ where profile by default is “ default ” and does not need to be specified. Example dkms streams: 450-dkms or latest-dkms Example precompiled streams: 450 or latest Note Precompiled streams are only supported on RHEL8 x86_64 and RHEL9 x86_64. To uninstall or change streams on Fedora and RHEL8: sudo dnf module remove --all nvidia-driver sudo dnf module reset nvidia-driver 5. NVIDIA Open GPU Kernel Modules \\uf0c1 The NVIDIA Linux GPU Driver contains several kernel modules: nvidia.ko nvidia-modeset.ko nvidia-uvm.ko nvidia-drm.ko nvidia-peermem.ko Starting in the 515 driver release series, two “flavors” of these kernel modules are provided: Proprietary - this is the flavor that NVIDIA has historically shipped. Open-source - published kernel modules that are dual licensed MIT/GPLv2. These are new starting in release 515. With every driver release, the source code to the open kernel modules will be published on https://github.com/NVIDIA/open-gpu-kernel-modules and a tarball will be provided on https://download.nvidia.com/XFree86/ . Verify that your NVIDIA GPU is at least Turing or newer generation. lspci | grep VGA Experimental support for GeForce and Quadro SKUs can be enabled with: echo \"options nvidia NVreg_OpenRmEnableUnsupportedGpus=1\" | sudo tee /etc/modprobe.d/nvidia-gsp.conf To install NVIDIA Open GPU Kernel Modules, follow the instructions below. 5.1. CUDA Runfile \\uf0c1 Pass the CLI argument to the CUDA runfile to opt in to NVIDIA Open GPU Kernel Modules: sh cuda___linux.run -m=kernel-open 5.2. Debian \\uf0c1 Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-kernel-open-dkms Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-kernel-open-dkms=-1 Install the rest of the NVIDIA driver packages: sudo apt-get install -v cuda-drivers- For example: sudo apt-get install -v nvidia-kernel-open-dkms=550.90.07-1 sudo apt-get install -v cuda-drivers-550 5.3. Fedora \\uf0c1 Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.4. KylinOS 10 \\uf0c1 Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.5. RHEL 9 and Rocky 9 \\uf0c1 Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.6. RHEL 8 and Rocky 8 \\uf0c1 Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5.7. OpenSUSE and SLES \\uf0c1 Install the NVIDIA Open GPU Kernel Modules package: sudo zypper install nvidia-open-driver-G06-kmp-default Install the rest of the NVIDIA driver packages: sudo zypper install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp- | sed \\'s| ||g\\' | awk -F \\'|\\' \\'// {print $2\"=\"$4}\\') Install the rest of the NVIDIA driver packages: sudo zypper -v install cuda-drivers- For example: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp-default | sed \\'s| ||g\\' | awk -F \\'|\\' \\'/550/ {print $2\"=\"$4}\\') sudo zypper -v install cuda-drivers-550 5.8. Ubuntu \\uf0c1 Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-driver--open Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers- Note End-users on Ubuntu should upgrade their NVIDIA Open GPU kernel modules using the following: sudo apt-get install --verbose-versions nvidia-kernel-source-550-open cuda-drivers-550 OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-driver--open Install the rest of the NVIDIA driver packages: sudo apt-get install -v cuda-drivers- For example: sudo apt-get install -v nvidia-driver-550-open sudo apt-get install -v cuda-drivers-550 6. Precompiled Streams \\uf0c1 Precompiled streams offer an optional method of streamlining the installation process.'},\n",
       " {'id': 130,\n",
       "  'content': 'The advantages of precompiled streams: Precompiled: faster boot up after driver and/or kernel updates Pre-tested: kernel and driver combination has been validated Removes gcc dependency: no compiler installation required Removes dkms dependency: enabling EPEL repository not required Removes kernel-devel and kernel-headers dependencies: no black screen if matching packages are missing When using precompiled drivers, a plugin for the dnf package manager is enabled that cleans up stale .ko files. To prevent system breakages, the NVIDIA dnf plugin also prevents upgrading to a kernel for which no precompiled driver yet exists. This can delay the application of security fixes but ensures that a tested kernel and driver combination is always used. A warning is displayed by dnf during that upgrade situation: NOTE: Skipping kernel installation since no NVIDIA driver kernel module package kmod-nvidia-${driver}-${kernel} ... could be found Packaging templates and instructions are provided on GitHub to allow you to maintain your own precompiled kernel module packages for custom kernels and derivative Linux distros: NVIDIA/yum-packaging-precompiled-kmod To use the new driver packages on RHEL 8 or RHEL 9: First, ensure that the Red Hat repositories are enabled: RHEL 8: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms or RHEL 9: subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms Choose one of the four options below depending on the desired driver: latest always updates to the highest versioned driver (precompiled): sudo dnf module install nvidia-driver:latest locks the driver updates to the specified driver branch (precompiled): sudo dnf module install nvidia-driver: Note Replace with the appropriate driver branch streams, for example 520, 515, 470, or 450. latest-dkms always updates to the highest versioned driver (non-precompiled): sudo dnf module install nvidia-driver:latest-dkms Note This is the default stream.'},\n",
       " {'id': 131,\n",
       "  'content': '-dkms locks the driver updates to the specified driver branch (non-precompiled): sudo dnf module install nvidia-driver:-dkms Note Valid streams include 520-dkms , 515-dkms , 470-dkms , and 450-dkms . 6.1. Precompiled Streams Support Matrix \\uf0c1 This table shows the supported precompiled and legacy DKMS streams for each driver. NVIDIA Driver Precompiled Stream Legacy DKMS Stream Open DKMS Stream Highest version latest latest-dkms open-dkms Locked at 520.x 520 520-dkms 520-open Locked at 515.x 515 515-dkms 515-open Prior to switching between module streams, first reset: sudo dnf module reset nvidia-driver Note This is also required for upgrading between branch locked streams. Or alternatively: sudo dnf module switch-to nvidia-driver: 6.2. Modularity Profiles \\uf0c1 Modularity profiles work with any supported modularity stream and allow for additional use cases. These modularity profiles are available on RHEL8+ and Fedora. Table 6 Table 5. List of nvidia-driver Module Profiles \\uf0c1 Stream Profile Use Case Default /default Installs all the driver packages in a stream. Kickstart /ks Performs unattended Linux OS installation using a config file. NVSwitch Fabric /fm Installs all the driver packages plus components required for bootstrapping an NVSwitch system (including the Fabric Manager and NSCQ telemetry). Source /src Source headers for compilation (precompiled streams only). For example: sudo dnf module nvidia-driver:/default sudo dnf module nvidia-driver:/ks sudo dnf module nvidia-driver:/fm sudo dnf module nvidia-driver:/src You can install multiple modularity profiles using BASH curly brace expansion, for example: sudo dnf module install nvidia-driver:latest/{default,src} See https://developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams in the Developer Blog and https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/precompiled/ for more information. 7. Kickstart Installation \\uf0c1 7.1. RHEL 8 / Rocky Linux 8 \\uf0c1 Enable the EPEL repository: repo --name=epel --baseurl=http://download.fedoraproject.org/pub/epel/8/Everything/x86_64/ Enable the CUDA repository: repo --name=cuda-rhel8 --baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/ In the packages section of the ks.cfg file, make sure you are using the /ks profile and :latest-dkms stream: @nvidia-driver:latest-dkms/ks Perform the post-installation actions. 7.2. RHEL 9 / Rocky Linux 9 \\uf0c1 Enable the EPEL repository: repo --name=epel --baseurl=http://download.fedoraproject.org/pub/epel/9/Everything/x86_64/ Enable the CUDA repository: repo --name=cuda-rhel9 --baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/ In the packages section of the ks.cfg file, make sure you are using the /ks profile and :latest-dkms stream: @nvidia-driver:latest-dkms/ks Perform the post-installation actions. 8.'},\n",
       " {'id': 132,\n",
       "  'content': 'Runfile Installation \\uf0c1 Basic instructions can be found in the Quick Start Guide . This section describes the installation and configuration of CUDA when using the standalone installer. The standalone installer is a “.run” file and is completely self-contained. 8.1. Runfile Overview \\uf0c1 The Runfile installation installs the NVIDIA Driver and CUDA Toolkit via an interactive ncurses-based interface. The installation steps are listed below.'},\n",
       " {'id': 133,\n",
       "  'content': 'Distribution-specific instructions on disabling the Nouveau drivers as well as steps for verifying device node creation are also provided. Finally, advanced options for the installer and uninstallation steps are detailed below. The Runfile installation does not include support for cross-platform development. 8.2.'},\n",
       " {'id': 134,\n",
       "  'content': 'Installation \\uf0c1 Perform the pre-installation actions . Disable the Nouveau drivers . Reboot into text mode (runlevel 3). This can usually be accomplished by adding the number “3” to the end of the system’s kernel boot parameters. Since the NVIDIA drivers are not yet installed, the text terminals may not display correctly. Temporarily adding “nomodeset” to the system’s kernel boot parameters may fix this issue. Consult your system’s bootloader documentation for information on how to make the above boot parameter changes. The reboot is required to completely unload the Nouveau drivers and prevent the graphical interface from loading. The CUDA driver cannot be installed while the Nouveau drivers are loaded or while the graphical interface is active. Verify that the Nouveau drivers are not loaded. If the Nouveau drivers are still loaded, consult your distribution’s documentation to see if further steps are needed to disable Nouveau. Run the installer and follow the on-screen prompts: sudo sh cuda__linux.run The installer will prompt for the following: EULA Acceptance CUDA Driver installation CUDA Toolkit installation, location, and /usr/local/cuda symbolic link The default installation location for the toolkit is /usr/local/cuda-12.4 : The /usr/local/cuda symbolic link points to the location where the CUDA Toolkit was installed. This link allows projects to use the latest CUDA Toolkit without any configuration file update. The installer must be executed with sufficient privileges to perform some actions. When the current privileges are insufficient to perform an action, the installer will ask for the user’s password to attempt to install with root privileges. Actions that cause the installer to attempt to install with root privileges are: installing the CUDA Driver installing the CUDA Toolkit to a location the user does not have permission to write to creating the /usr/local/cuda symbolic link Running the installer with sudo , as shown above, will give permission to install to directories that require root permissions. Directories and files created while running the installer with sudo will have root ownership. If installing the driver, the installer will also ask if the openGL libraries should be installed. If the GPU used for display is not an NVIDIA GPU, the NVIDIA openGL libraries should not be installed. Otherwise, the openGL libraries used by the graphics driver of the non-NVIDIA GPU will be overwritten and the GUI will not work. If performing a silent installation, the --no-opengl-libs option should be used to prevent the openGL libraries from being installed. See the Advanced Options section for more details.'},\n",
       " {'id': 135,\n",
       "  'content': 'If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf , may need to be modified. In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system. For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file. Consult the xorg.conf documentation for more information. Note Installing Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries. Reboot the system to reload the graphical interface: sudo reboot Verify the device nodes are created properly. Perform the post-installation actions . 8.3. Disabling Nouveau \\uf0c1 To install the Display Driver, the Nouveau drivers must first be disabled. Each distribution of Linux has a different method for disabling Nouveau. The Nouveau drivers are loaded if the following command prints anything: lsmod | grep nouveau 8.3.1. Fedora \\uf0c1 Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the following command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system. 8.3.2. RHEL / Rocky and KylinOS \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force 8.3.3. OpenSUSE \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd 8.3.4. SLES \\uf0c1 No actions to disable Nouveau are required as Nouveau is not installed on SLES. 8.3.5. WSL \\uf0c1 No actions to disable Nouveau are required as Nouveau is not installed on WSL. 8.3.6. Ubuntu \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.3.7. Debian \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.4. Device Node Verification \\uf0c1 Check that the device files /dev/nvidia* exist and have the correct (0666) file permissions. These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver. Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuid nvidia-modprobe tool that is bundled with the NVIDIA Driver. However, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below: #!/bin/bash /sbin/modprobe nvidia if [ \"$?\"\\n-eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\"\\n-eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk \\'{print $1}\\'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi 8.5. Advanced Options \\uf0c1 Action Options Used Explanation Silent Installation --silent Required for any silent installation.'},\n",
       " {'id': 136,\n",
       "  'content': 'Performs an installation with no further user-input and minimal command-line output based on the options provided below. Silent installations are useful for scripting the installation of CUDA. Using this option implies acceptance of the EULA. The following flags can be used to customize the actions taken during installation. At least one of --driver , --uninstall , and --toolkit must be passed if running with non-root permissions. --driver Install the CUDA Driver. --toolkit Install the CUDA Toolkit. --toolkitpath= Install the CUDA Toolkit to the directory. If not provided, the default path of /usr/local/cuda-12.4 is used. --defaultroot= Install libraries to the directory. If the is not provided, then the default path of your distribution is used. This only applies to the libraries installed outside of the CUDA Toolkit path. Extraction --extract= Extracts to the the following: the driver runfile, the raw files of the toolkit to . This is especially useful when one wants to install the driver using one or more of the command-line options provided by the driver installer which are not exposed in this installer. Overriding Installation Checks --override Ignores compiler, third-party library, and toolkit detection checks which would prevent the CUDA Toolkit from installing. No OpenGL Libraries --no-opengl-libs Prevents the driver installation from installing NVIDIA’s GL libraries. Useful for systems where the display is driven by a non-NVIDIA GPU. In such systems, NVIDIA’s GL libraries could prevent X from loading properly. No man pages --no-man-page Do not install the man pages under /usr/share/man . Overriding Kernel Source --kernel-source-path= Tells the driver installation to use as the kernel source directory when building the NVIDIA kernel module. Required for systems where the kernel source is installed to a non-standard location. Running nvidia-xconfig --run-nvidia-xconfig Tells the driver installation to run nvidia-xconfig to update the system X configuration file so that the NVIDIA X driver is used. The pre-existing X configuration file will be backed up. No nvidia-drm kernel module --no-drm Do not install the nvidia-drm kernel module. This option should only be used to work around failures to build or install the nvidia-drm kernel module on systems that do not need the provided features. Custom Temporary Directory Selection --tmpdir= Performs any temporary actions within instead of /tmp . Useful in cases where /tmp cannot be used (doesn’t exist, is full, is mounted with ‘noexec’, etc.). Show Installer Options --help Prints the list of command-line options to stdout.'},\n",
       " {'id': 137,\n",
       "  'content': '8.6. Uninstallation \\uf0c1 To uninstall the CUDA Toolkit, run the uninstallation script provided in the bin directory of the toolkit. By default, it is located in /usr/local/cuda-12.4/bin : sudo /usr/local/cuda-12.4/bin/cuda-uninstaller To uninstall the NVIDIA Driver, run nvidia-uninstall : sudo /usr/bin/nvidia-uninstall To enable the Nouveau drivers, remove the blacklist file created in the Disabling Nouveau section, and regenerate the kernel initramfs/initrd again as described in that section. 9. Conda Installation \\uf0c1 This section describes the installation and configuration of CUDA when using the Conda installer. The Conda packages are available at https://anaconda.org/nvidia . 9.1. Conda Overview \\uf0c1 The Conda installation installs the CUDA Toolkit. 9.2. Installing CUDA Using Conda \\uf0c1 To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia 9.3. Uninstalling CUDA Using Conda \\uf0c1 To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 9.4. Installing Previous CUDA Releases \\uf0c1 All Conda packages released under a specific CUDA version are labeled with that release version. To install a previous version, include that label in the install command such as: conda install cuda -c nvidia/label/cuda-11.3.0 9.5. Upgrading from cudatoolkit Package \\uf0c1 If you had previously installed CUDA using the cudatoolkit package and want to maintain a similar install footprint, you can limit your installation to the following packages: cuda-libraries-dev cuda-nvcc cuda-nvtx cuda-cupti Note Some extra files, such as headers, will be included in this installation which were not included in the cudatoolkit package. If you need to reduce your installation further, replace cuda-libraries-dev with the specific libraries you need. 10. Pip Wheels \\uf0c1 NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python. These packages are intended for runtime use and do not currently include developer tools (these can be installed separately). Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment. Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA NGC PyPI repo. If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules. If these Python modules are out-of-date then the commands which follow later in this section may fail. python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module. python3 -m pip install nvidia-pyindex If your project is using a requirements.txt file, then you can add the following line to your requirements.txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https://pypi.org/simple Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version. “cu12” should be read as “cuda12”. nvidia-cuda-runtime-cu12 nvidia-cuda-cccl-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-opencl-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cublas-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 These metapackages install the following packages: nvidia-cuda-runtime-cu125 nvidia-cuda-cccl-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-opencl-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 11. Tarball and Zip Archive Deliverables \\uf0c1 In an effort to meet the needs of a growing customer base requiring alternative installer packaging formats, as well as a means of input into community CI/CD systems, tarball and zip archives are available for each component.'},\n",
       " {'id': 138,\n",
       "  'content': 'These tarball and zip archives, known as binary archives, are provided at https://developer.download.nvidia.com/compute/cuda/redist/ . These component .tar.xz and .zip binary archives do not replace existing packages such as .deb, .rpm, runfile, conda, etc. and are not meant for general consumption, as they are not installers.'},\n",
       " {'id': 139,\n",
       "  'content': 'However this standardized approach will replace existing .txz archives. For each release, a JSON manifest is provided such as redistrib_11.4.2.json , which corresponds to the CUDA 11.4.2 release label (CUDA 11.4 update 2) which includes the release date, the name of each component, license name, relative URL for each platform and checksums. Package maintainers are advised to check the provided LICENSE for each component prior to redistribution. Instructions for developers using CMake and Bazel build systems are provided in the next sections. 11.1.'},\n",
       " {'id': 140,\n",
       "  'content': 'Parsing Redistrib JSON \\uf0c1 The following example of a JSON manifest contains keys for each component: name, license, version, and a platform array which includes relative_path, sha256, md5, and size (bytes) for each archive. { \"release_date\": \"2021-09-07\", \"cuda_cudart\": { \"name\": \"CUDA Runtime (cudart)\", \"license\": \"CUDA Toolkit\", \"version\": \"11.4.108\", \"linux-x86_64\": { \"relative_path\": \"cuda_cudart/linux-x86_64/cuda_cudart-linux-x86_64-11.4.108-archive.tar.xz\", \"sha256\": \"d08a1b731e5175aa3ae06a6d1c6b3059dd9ea13836d947018ea5e3ec2ca3d62b\", \"md5\": \"da198656b27a3559004c3b7f20e5d074\", \"size\": \"828300\" }, \"linux-ppc64le\": { \"relative_path\": \"cuda_cudart/linux-ppc64le/cuda_cudart-linux-ppc64le-11.4.108-archive.tar.xz\", \"sha256\": \"831dffe062ae3ebda3d3c4010d0ee4e40a01fd5e6358098a87bb318ea7c79e0c\", \"md5\": \"ca73328e3f8e2bb5b1f2184c98c3a510\", \"size\": \"776840\" }, \"linux-sbsa\": { \"relative_path\": \"cuda_cudart/linux-sbsa/cuda_cudart-linux-sbsa-11.4.108-archive.tar.xz\", \"sha256\": \"2ab9599bbaebdcf59add73d1f1a352ae619f8cb5ccec254093c98efd4c14553c\", \"md5\": \"aeb5c19661f06b6398741015ba368102\", \"size\": \"782372\" }, \"windows-x86_64\": { \"relative_path\": \"cuda_cudart/windows-x86_64/cuda_cudart-windows-x86_64-11.4.108-archive.zip\", \"sha256\": \"b59756c27658d1ea87a17c06d064d1336576431cd64da5d1790d909e455d06d3\", \"md5\": \"7f6837a46b78198402429a3760ab28fc\", \"size\": \"2897751\" } } } A JSON schema is provided at https://developer.download.nvidia.com/compute/redist/redistrib-v2.schema.json . A sample script that parses these JSON manifests is available on GitHub : Downloads each archive Validates SHA256 checksums Extracts archives Flattens into a collapsed directory structure Table 7 Available Tarball and Zip Archives \\uf0c1 Product Example CUDA Toolkit ./parse_redist.py --product cuda --label 12.3.2 cuBLASMp ./parse_redist.py --product cublasmp --label 0.1.0 cuDNN ./parse_redist.py --product cudnn --label 8.9.6.50 cuDSS ./parse_redist.py --product cudss --label 0.1.0 cuQuantum ./parse_redist.py --product cuquantum --label 23.10.0 cuSPARSELt ./parse_redist.py --product cusparselt --label 0.5.2 cuTENSOR ./parse_redist.py --product cutensor --label 1.7.0 NVIDIA driver ./parse_redist.py --product nvidia-driver --label 535.129.03 nvJPEG2000 ./parse_redist.py --product nvjpeg2000 --label 0.7.5 NVPL ./parse_redist.py --product nvpl --label 23.11 nvTIFF ./parse_redist.py --product nvtiff --label 0.3.0 11.2. Importing Tarballs into CMake \\uf0c1 The recommended module for importing these tarballs into the CMake build system is via FindCUDAToolkit (3.17 and newer).'},\n",
       " {'id': 141,\n",
       "  'content': 'Note The FindCUDA module is deprecated. The path to the extraction location can be specified with the CUDAToolkit_ROOT environmental variable. For example CMakeLists.txt and commands, see cmake/1_FindCUDAToolkit/ . For older versions of CMake, the ExternalProject_Add module is an alternative method. For example CMakeLists.txt file and commands, see cmake/2_ExternalProject/ .'},\n",
       " {'id': 142,\n",
       "  'content': '11.3. Importing Tarballs into Bazel \\uf0c1 The recommended method of importing these tarballs into the Bazel build system is using http_archive and pkg_tar . For an example, see bazel/1_pkg_tar/ .'},\n",
       " {'id': 143,\n",
       "  'content': '12. CUDA Cross-Platform Environment \\uf0c1 Cross development for arm64-sbsa is supported on Ubuntu 20.04, Ubuntu 22.04, RHEL 8, RHEL 9, and SLES 15. Cross development for arm64-Jetson is only supported on Ubuntu 20.04 We recommend selecting a host development environment that matches the supported cross-target environment. This selection helps prevent possible host/target incompatibilities, such as GCC or GLIBC version mismatches.'},\n",
       " {'id': 144,\n",
       "  'content': '12.1. CUDA Cross-Platform Installation \\uf0c1 Some of the following steps may have already been performed as part of the native Ubuntu installation . Such steps can safely be skipped. These steps should be performed on the x86_64 host system, rather than the target system. To install the native CUDA Toolkit on the target system, refer to the native Ubuntu installation section. Perform the pre-installation actions. Install repository meta-data package with: sudo dpkg -i cuda-repo-cross-_all.deb where indicates the operating system, architecture, and/or the version of the package. Update the Apt repository cache: sudo apt-get update Install the appropriate cross-platform CUDA Toolkit: For aarch64: sudo apt-get install cuda-cross-aarch64 For QNX: sudo apt-get install cuda-cross-qnx Perform the post-installation actions. 12.2. CUDA Cross-Platform Samples \\uf0c1 CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples. 13.'},\n",
       " {'id': 145,\n",
       "  'content': 'Post-installation Actions \\uf0c1 The post-installation actions must be manually performed. These actions are split into mandatory, recommended, and optional sections. 13.1. Mandatory Actions \\uf0c1 Some actions must be taken after the installation before the CUDA Toolkit and Driver can be used. 13.1.1. Environment Setup \\uf0c1 The PATH variable needs to include export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} . Nsight Compute has moved to /opt/nvidia/nsight-compute/ only in rpm/deb installation method. When using .run installer it is still located under /usr/local/cuda-12.4/ . To add this path to the PATH variable: export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} In addition, when using the runfile installation method, the LD_LIBRARY_PATH variable needs to contain /usr/local/cuda-12.4/lib64 on a 64-bit system, or /usr/local/cuda-12.4/lib on a 32-bit system To change the environment variables for 64-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} To change the environment variables for 32-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Note that the above paths change when using a custom install path with the runfile installation method. 13.2.'},\n",
       " {'id': 146,\n",
       "  'content': 'Recommended Actions \\uf0c1 Other actions are recommended to verify the integrity of the installation.'},\n",
       " {'id': 147,\n",
       "  'content': '13.2.1. Install Persistence Daemon \\uf0c1 NVIDIA is providing a user-space daemon on Linux to support persistence of driver state across CUDA job runs. The daemon approach provides a more elegant and robust solution to this problem than persistence mode. For more details on the NVIDIA Persistence Daemon, see the documentation here . The NVIDIA Persistence Daemon can be started as the root user by running: /usr/bin/nvidia-persistenced --verbose This command should be run on boot. Consult your Linux distribution’s init documentation for details on how to automate this. 13.2.2. Install Writable Samples \\uf0c1 CUDA Samples are now located in https://github.com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples. 13.2.3. Verify the Installation \\uf0c1 Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware. To do this, you need to compile and run some of the sample programs, located in https://github.com/nvidia/cuda-samples . Note Ensure the PATH and, if using the runfile installation method, LD_LIBRARY_PATH variables are set correctly .'},\n",
       " {'id': 148,\n",
       "  'content': '13.2.3.1. Verify the Driver Version \\uf0c1 If you installed the driver, verify that the correct version of it is loaded.'},\n",
       " {'id': 149,\n",
       "  'content': 'If you did not install the driver, or are using an operating system where the driver is not loaded via a kernel module, such as L4T, skip this step. When the driver is loaded, the driver version can be found by executing the command cat /proc/driver/nvidia/version Note that this command will not work on an iGPU/dGPU system. 13.2.3.2. Running the Binaries \\uf0c1 After compilation, find and run deviceQuery from https://github.com/nvidia/cuda-samples . If the CUDA software is installed and configured correctly, the output for deviceQuery should look similar to that shown in Figure 1 . Figure 1 Figure 1. Valid Results from deviceQuery CUDA Sample \\uf0c1 The exact appearance and the output lines might be different on your system. The important outcomes are that a device was found (the first highlighted line), that the device matches the one on your system (the second highlighted line), and that the test passed (the final highlighted line). If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, this likely means that the /dev/nvidia* files are missing or have the wrong permissions. On systems where SELinux is enabled, you might need to temporarily disable this security feature to run deviceQuery . To do this, type: setenforce 0 from the command line as the superuser. Running the bandwidthTest program ensures that the system and the CUDA-capable device are able to communicate correctly. Its output is shown in Figure 2 . Figure 2 Figure 2. Valid Results from bandwidthTest CUDA Sample \\uf0c1 Note that the measurements for your CUDA-capable device description will vary from system to system. The important point is that you obtain measurements, and that the second-to-last line (in Figure 2 ) confirms that all necessary tests passed. Should the tests not pass, make sure you have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed. If you run into difficulties with the link step (such as libraries not being found), consult the Linux Release Notes found in https://github.com/nvidia/cuda-samples . 13.2.4. Install Nsight Eclipse Plugins \\uf0c1 To install Nsight Eclipse plugins, an installation script is provided: /usr/local/cuda-12.4/bin/nsight_ee_plugins_manage.sh install Refer to Nsight Eclipse Plugins Installation Guide for more details. 13.2.5.'},\n",
       " {'id': 150,\n",
       "  'content': 'Local Repo Removal \\uf0c1 Removal of the local repo installer is recommended after installation of CUDA SDK . Ubuntu and Debian sudo apt-get remove --purge \"cuda-repo--X-Y-local*\" Fedora sudo dnf remove \"cuda-repo--X-Y-local*\" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove \"cuda-repo--X-Y-local*\" openSUSE 15 and SLES 15 sudo zypper remove \"cuda-repo--X-Y-local*\" Removal of the local repo installer is recommended after installation of NVIDA driver . Ubuntu and Debian sudo apt-get remove --purge \"nvidia-driver-local-repo-*\" Fedora sudo dnf remove \"nvidia-driver-local-repo-*\" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove \"nvidia-driver-local-repo-*\" openSUSE 15 and SLES 15 sudo zypper remove \"nvidia-driver-local-repo-*\" 13.3. Optional Actions \\uf0c1 Other options are not necessary to use the CUDA Toolkit, but are available to provide additional features. 13.3.1. Install Third-party Libraries \\uf0c1 Some CUDA samples use third-party libraries which may not be installed by default on your system. These samples attempt to detect any required libraries when building. If a library is not detected, it waives itself and warns you which library is missing. To build and run these samples, you must install the missing libraries.'},\n",
       " {'id': 151,\n",
       "  'content': 'In cases where these dependencies are not installed, follow the instructions below. RHEL 8 / Rocky Linux 8 sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\\ make mesa-libGLU-devel freeimage-devel libglfw3-devel RHEL 9 / Rocky Linux 9 sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\\ make mesa-libGLU-devel freeimage-devel libglfw3-devel KylinOS 10 sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\\ make mesa-libGLU-devel freeimage-devel libglfw3-devel Fedora sudo dnf install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\\ make mesa-libGLU-devel freeimage-devel libglfw3-devel SLES sudo zypper install libglut3 libX11-devel libXi6 libXmu6 libGLU1 make OpenSUSE sudo zypper install freeglut-devel libX11-devel libXi-devel libXmu-devel \\\\ make Mesa-libGL-devel freeimage-devel Ubuntu sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \\\\ libxmu-dev libxi-dev libglu1-mesa-dev libfreeimage-dev libglfw3-dev Debian sudo apt-get install g++ freeglut3-dev build-essential libx11-dev \\\\ libxmu-dev libxi-dev libglu1-mesa-dev libfreeimage-dev libglfw3-dev 13.3.2. Install the Source Code for cuda-gdb \\uf0c1 The cuda-gdb source must be explicitly selected for installation with the runfile installation method.'},\n",
       " {'id': 152,\n",
       "  'content': 'During the installation, in the component selection page, expand the component “CUDA Tools 12.4” and select cuda-gdb-src for installation. It is unchecked by default. To obtain a copy of the source code for cuda-gdb using the RPM and Debian installation methods, the cuda-gdb-src package must be installed. The source code is installed as a tarball in the /usr/local/cuda-12.4/extras directory. 13.3.3. Select the Active Version of CUDA \\uf0c1 For applications that rely on the symlinks /usr/local/cuda and /usr/local/cuda-MAJOR , you may wish to change to a different installed version of CUDA using the provided alternatives. To show the active version of CUDA and all available versions: update-alternatives --display cuda To show the active minor version of a given major CUDA release: update-alternatives --display cuda-12 To update the active version of CUDA: sudo update-alternatives --config cuda 14. Advanced Setup \\uf0c1 Below is information on some advanced setup scenarios which are not covered in the basic instructions above. Table 8 Advanced Setup Scenarios when Installing CUDA \\uf0c1 Scenario Instructions Install CUDA using the Package Manager installation method without installing the NVIDIA GL libraries. Fedora Install CUDA using the following command: sudo dnf install cuda-toolkit-12-4 \\\\ nvidia-driver-cuda akmod-nvidia Follow the instructions here to ensure that Nouveau is disabled. If performing an upgrade over a previous installation, the NVIDIA kernel module may need to be rebuilt by following the instructions here . OpenSUSE/SLES On some system configurations the NVIDIA GL libraries may need to be locked before installation using: sudo zypper addlock nvidia-glG04 Install CUDA using the following command: sudo zypper install --no-recommends cuda-toolkit-12-4 \\\\ nvidia-computeG04 \\\\ nvidia-gfxG04-kmp-default Follow the instructions here to ensure that Nouveau is disabled. Ubuntu This functionality isn’t supported on Ubuntu. Instead, the driver packages integrate with the Bumblebee framework to provide a solution for users who wish to control what applications the NVIDIA drivers are used for. See Ubuntu’s Bumblebee wiki for more information. Upgrade from a RPM/Deb driver installation which includes the diagnostic driver packages to a driver installation which does not include the diagnostic driver packages. RHEL/CentOS Remove diagnostic packages using the following command: sudo yum remove cuda-drivers-diagnostic \\\\ xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal. Fedora Remove diagnostic packages using the following command: sudo dnf remove cuda-drivers-diagnostic \\\\ xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal. OpenSUSE/SLES Remove diagnostic packages using the following command: sudo zypper remove cuda-drivers-diagnostic \\\\ nvidia-diagnosticG04 Follow the instructions here to continue installation as normal. Ubuntu Remove diagnostic packages using the following command: sudo apt-get purge cuda-drivers-diagnostic \\\\ nvidia-384-diagnostic Follow the instructions here to continue installation as normal. Use a specific GPU for rendering the display.'},\n",
       " {'id': 153,\n",
       "  'content': 'Add or replace a Device entry in your xorg.conf file, located at /etc/X11/xorg.conf . The Device entry should resemble the following: Section \"Device\" Identifier \"Device0\" Driver \"driver_name\" VendorName \"vendor_name\" BusID \"bus_id\" EndSection The details will you will need to add differ on a case-by-case basis. For example, if you have two NVIDIA GPUs and you want the first GPU to be used for display, you would replace “ driver_name ” with “ nvidia ”, “ vendor_name ” with “ NVIDIA Corporation ” and “ bus_id ” with the Bus ID of the GPU. The Bus ID will resemble “PCI:00:02.0” and can be found by running lspci . Install CUDA to a specific directory using the Package Manager installation method. RPM The RPM packages don’t support custom install locations through the package managers (Yum and Zypper), but it is possible to install the RPM packages to a custom location using rpm’s --relocate parameter: sudo rpm --install --relocate /usr/local/cuda-12.4=/new/toolkit package.rpm You will need to install the packages in the correct dependency order; this task is normally taken care of by the package managers. For example, if package “foo” has a dependency on package “bar”, you should install package “bar” first, and package “foo” second. You can check the dependencies of a RPM package as follows: rpm -qRp package.rpm Note that the driver packages cannot be relocated. Deb The Deb packages do not support custom install locations. It is however possible to extract the contents of the Deb packages and move the files to the desired install location. See the next scenario for more details one xtracting Deb packages. Extract the contents of the installers. Runfile The Runfile can be extracted into the standalone Toolkit and Driver Runfiles by using the --extract parameter. The Toolkit standalone Runfiles can be further extracted by running: ./runfile.run --tar mxvf The Driver Runfile can be extracted by running: ./runfile.run -x RPM The RPM packages can be extracted by running: rpm2cpio package.rpm | cpio -idmv Deb The Deb packages can be extracted by running: dpkg-deb -x package.deb output_dir Modify Ubuntu’s apt package manager to query specific architectures for specific repositories. This is useful when a foreign architecture has been added, causing “404 Not Found” errors to appear when the repository meta-data is updated.'},\n",
       " {'id': 154,\n",
       "  'content': 'Each repository you wish to restrict to specific architectures must have its sources.list entry modified. This is done by modifying the /etc/apt/sources.list file and any files containing repositories you wish to restrict under the /etc/apt/sources.list.d/ directory. Normally, it is sufficient to modify only the entries in /etc/apt/sources.list An architecture-restricted repository entry looks like: deb [arch=,] For example, if you wanted to restrict a repository to only the amd64 and i386 architectures, it would look like: deb [arch=amd64,i386] It is not necessary to restrict the deb-src repositories, as these repositories don’t provide architecture-specific packages. For more details, see the sources.list manpage.'},\n",
       " {'id': 155,\n",
       "  'content': 'The nvidia.ko kernel module fails to load, saying some symbols are unknown. For example: nvidia: Unknown symbol drm_open (err 0) Check to see if there are any optionally installable modules that might provide these symbols which are not currently installed. For the example of the drm_open symbol, check to see if there are any packages which provide drm_open and are not already installed. For instance, on Ubuntu 14.04, the linux-image-extra package provides the DRM kernel module (which provides drm_open ). This package is optional even though the kernel headers reflect the availability of DRM regardless of whether this package is installed or not. The runfile installer fails to extract due to limited space in the TMP directory. This can occur on systems with limited storage in the TMP directory (usually /tmp ), or on systems which use a tmpfs in memory to handle temporary storage. In this case, the --tmpdir command-line option should be used to instruct the runfile to use a directory with sufficient space to extract into. More information on this option can be found here . Re-enable Wayland after installing the RPM driver on Fedora. Wayland is disabled during installation of the Fedora driver RPM due to compatability issues. To re-enable Wayland, comment out this line in /etc/gdm/custom.conf : WaylandEnable=false In case of the error: E: Failed to fetch file:/var/cuda-repo File not found Debian and Ubuntu This can occur when installing CUDA after uninstalling a different version. Use the following command before installation: sudo rm -v /var/lib/apt/lists/*cuda* /var/lib/apt/lists/*nvidia* Verbose installation on Debian and Ubuntu Use the --verbose-versions flag, for example: sudo apt-get install --verbose-versions cuda 15. Frequently Asked Questions \\uf0c1 15.1.'},\n",
       " {'id': 156,\n",
       "  'content': '\\uf0c1 The Runfile installation asks where you wish to install the Toolkit during an interactive install. If installing using a non-interactive install, you can use the --toolkitpath parameter to change the install location: ./runfile.run --silent \\\\ --toolkit --toolkitpath=/my/new/toolkit The RPM and Deb packages cannot be installed to a custom install location directly using the package managers. See the “Install CUDA to a specific directory using the Package Manager installation method” scenario in the Advanced Setup section for more information. \\uf0c1 Your PATH environment variable is not set up correctly. Ensure that your PATH includes the bin directory where you installed the Toolkit, usually /usr/local/cuda-12.4/bin . export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} 15.3. \\uf0c1 Your LD_LIBRARY_PATH environment variable is not set up correctly. Ensure that your LD_LIBRARY_PATH includes the lib and/or lib64 directory where you installed the Toolkit, usually /usr/local/cuda-12.4/lib{,64} : export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 15.4. \\uf0c1 These errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the system’s sources.list file. Repositories that do not host packages for the newly added architecture will present this error. While noisy, the error itself does no harm. Please see the Advanced Setup section for details on how to modify your sources.list file to prevent these errors. \\uf0c1 To make sure X doesn’t use a certain GPU for display, you need to specify which other GPU to use for display. For more information, please refer to the “Use a specific GPU for rendering the display” scenario in the Advanced Setup section. \\uf0c1 When using RPM or Deb, the downloaded package is a repository package.'},\n",
       " {'id': 157,\n",
       "  'content': 'See the Package Manager Installation section for more details. \\uf0c1 After installing CUDA, set the driver value for the intel device in /etc/X11/xorg.conf to ‘ modesetting ’ as shown below: Section \"Device\" Identifier \"intel\" Driver \"modesetting\" ... EndSection To prevent Ubuntu from reverting the change in xorg.conf, edit /etc/default/grub to add “ nogpumanager ” to GRUB_CMDLINE_LINUX_DEFAULT. Run the following command to update grub before rebooting: sudo update-grub 15.8. \\uf0c1 System updates may include an updated Linux kernel.'},\n",
       " {'id': 158,\n",
       "  'content': 'In many cases, a new Linux kernel will be installed without properly updating the required Linux kernel headers and development packages. To ensure the CUDA driver continues to work when performing a system update, rerun the commands in the Kernel Headers and Development Packages section. Additionally, on Fedora, the Akmods framework will sometimes fail to correctly rebuild the NVIDIA kernel module packages when a new Linux kernel is installed. When this happens, it is usually sufficient to invoke Akmods manually and regenerate the module mapping files by running the following commands in a virtual console, and then rebooting: sudo akmods --force sudo depmod You can reach a virtual console by hitting ctrl+alt+f2 at the same time. \\uf0c1 To install a CUDA driver at a version earlier than 367 using a network repo, the required packages will need to be explicitly installed at the desired version. For example, to install 352.99, instead of installing the cuda-drivers metapackage at version 352.99, you will need to install all required packages of cuda-drivers at version 352.99. \\uf0c1 Depending on your system configuration, you may not be able to install old versions of CUDA using the cuda metapackage. In order to install a specific version of CUDA, you may need to specify all of the packages that would normally be installed by the cuda metapackage at the version you want to install. If you are using yum to install certain packages at an older version, the dependencies may not resolve as expected. In this case you may need to pass “ --setopt=obsoletes=0 ” to yum to allow an install of packages which are obsoleted at a later version than you are trying to install. \\uf0c1 This dependency comes from the SUSE repositories and shouldn’t affect the use of the NVIDIA driver or the CUDA Toolkit. To disable this dependency, you can lock that package with the following command: sudo zypper al Mesa-dri-nouveau 15.12. \\uf0c1 This sometimes occurs when trying to uninstall CUDA after a clean .deb installation. Run the following commands: sudo apt-get install glx-diversions --reinstall sudo apt-get remove nvidia-alternative Then re-run the commands from Removing CUDA Toolkit and Driver . Additional Considerations \\uf0c1 Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C++ Programming Guide, located in /usr/local/cuda-12.4/doc . A number of helpful development tools are included in the CUDA Toolkit to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Eclipse Edition, NVIDIA Visual Profiler, CUDA-GDB, and CUDA-MEMCHECK. For technical support on programming questions, consult and participate in the developer forums at https://forums.developer.nvidia.com/c/accelerated-computing/cuda/206 . 17. Switching between Driver Module Flavors \\uf0c1 Use the following steps to switch between the NVIDIA driver legacy and open module flavors on your system. Note If switching to open module, experimental support for GeForce and Quadro SKUs can be enabled with: echo \"options nvidia NVreg_OpenRmEnableUnsupportedGpus=1\" | sudo tee /etc/modprobe.d/nvidia-gsp.conf Note Replace XXX with the NVIDIA driver branch number such as 550. Fedora, RHEL 9 / Rocky Linux 9, RHEL 8 / Rocky Linux 8 To switch between legacy and open: uninstall, then reinstall. Kylin OS To switch between legacy and open: uninstall, then reinstall. Ubuntu To switch from legacy to open: sudo apt-get --purge remove nvidia-kernel-source-XXX sudo apt-get install --verbose-versions nvidia-kernel-open-XXX sudo apt-get install --verbose-versions cuda-drivers-XXX To switch from open to legacy: sudo apt-get remove --purge nvidia-kernel-open-XXX sudo apt-get install --verbose-versions cuda-drivers-XXX Debian To switch from legacy to open: sudo apt-get --purge remove nvidia-kernel-dkms sudo apt-get install --verbose-versions nvidia-kernel-open-dkms sudo apt-get install --verbose-versions cuda-drivers-XXX To switch from open to legacy: sudo apt-get remove --purge nvidia-kernel-open-dkms sudo apt-get install --verbose-versions cuda-drivers-XXX OpenSUSE To switch from legacy to open: sudo zypper remove nvidia-driver-G06-kmp-default sudo zypper install --details nvidia-open-driver-G06-kmp-default sudo zypper install --details cuda-drivers-XXX To switch from open to legacy: sudo zypper remove nvidia-open-driver-G06-kmp-default sudo zypper install --details cuda-drivers-XXX SLES To switch from legacy to open: sudo zypper remove nvidia-driver-G06-kmp-default nvidia-driver-G06-kmp-azure sudo zypper install --details nvidia-open-driver-G06-kmp-default nvidia-open-driver-G06-kmp-azure sudo zypper install --details cuda-drivers-XXX To switch from open to legacy: sudo zypper remove nvidia-open-driver-G06-kmp-default nvidia-driver-G06-open-kmp-azure sudo zypper install --details cuda-drivers-XXX Note The Azure package is only available for SLES (x86_64). 18.'},\n",
       " {'id': 159,\n",
       "  'content': 'Removing CUDA Toolkit and Driver \\uf0c1 Follow the below steps to properly uninstall the CUDA Toolkit and NVIDIA Drivers from your system. These steps will ensure that the uninstallation will be clean. KylinOS 10 To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\\ \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver RHEL 9 / Rocky Linux 9 To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\\ \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver RHEL 8 / Rocky Linux 8 To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\\ \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver Fedora To remove CUDA Toolkit: sudo dnf remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\\ \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo dnf module remove --all nvidia-driver To reset the module stream: sudo dnf module reset nvidia-driver To remove 3rd party NVIDIA Drivers: sudo dnf remove \"*nvidia*\" OpenSUSE / SLES To remove CUDA Toolkit: sudo zypper remove \"cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\\ \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo zypper remove \"*nvidia*\" Ubuntu and Debian To remove CUDA Toolkit: sudo apt-get --purge remove \"*cuda*\" \"*cublas*\" \"*cufft*\" \"*cufile*\" \"*curand*\" \\\\ \"*cusolver*\" \"*cusparse*\" \"*gds-tools*\" \"*npp*\" \"*nvjpeg*\" \"nsight*\" \"*nvvm*\" To remove NVIDIA Drivers: sudo apt-get --purge remove \"*nvidia*\" \"libxnvctrl*\" To clean up the uninstall: sudo apt-get autoremove 19. Notices \\uf0c1 19.1.'},\n",
       " {'id': 160,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 19.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 161,\n",
       "  'content': '19.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 162,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 20. Copyright \\uf0c1 © 2009-2024 NVIDIA Corporation & affiliates. All rights reserved. This product includes software developed by the Syncro Soft SRL ( http://www.sync.ro/ ). Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates.'},\n",
       " {'id': 163,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); });1. Introduction 1.1.'},\n",
       " {'id': 164,\n",
       "  'content': 'The Benefits of Using GPUs 1.2. CUDA®: A General-Purpose Parallel Computing Platform and Programming Model 1.3. A Scalable Programming Model 1.4. Document Structure 2. Programming Model 2.1. Kernels 2.2. Thread Hierarchy 2.2.1. Thread Block Clusters 2.3. Memory Hierarchy 2.4. Heterogeneous Programming 2.5. Asynchronous SIMT Programming Model 2.5.1. Asynchronous Operations 2.6. Compute Capability 3. Programming Interface 3.1. Compilation with NVCC 3.1.1. Compilation Workflow 3.1.1.1. Offline Compilation 3.1.1.2. Just-in-Time Compilation 3.1.2. Binary Compatibility 3.1.3. PTX Compatibility 3.1.4. Application Compatibility 3.1.5. C++ Compatibility 3.1.6. 64-Bit Compatibility 3.2. CUDA Runtime 3.2.1. Initialization 3.2.2. Device Memory 3.2.3. Device Memory L2 Access Management 3.2.3.1. L2 cache Set-Aside for Persisting Accesses 3.2.3.2. L2 Policy for Persisting Accesses 3.2.3.3. L2 Access Properties 3.2.3.4. L2 Persistence Example 3.2.3.5. Reset L2 Access to Normal 3.2.3.6. Manage Utilization of L2 set-aside cache 3.2.3.7. Query L2 cache Properties 3.2.3.8. Control L2 Cache Set-Aside Size for Persisting Memory Access 3.2.4. Shared Memory 3.2.5. Distributed Shared Memory 3.2.6. Page-Locked Host Memory 3.2.6.1. Portable Memory 3.2.6.2. Write-Combining Memory 3.2.6.3. Mapped Memory 3.2.7. Memory Synchronization Domains 3.2.7.1. Memory Fence Interference 3.2.7.2. Isolating Traffic with Domains 3.2.7.3. Using Domains in CUDA 3.2.8. Asynchronous Concurrent Execution 3.2.8.1. Concurrent Execution between Host and Device 3.2.8.2. Concurrent Kernel Execution 3.2.8.3. Overlap of Data Transfer and Kernel Execution 3.2.8.4. Concurrent Data Transfers 3.2.8.5. Streams 3.2.8.5.1. Creation and Destruction of Streams 3.2.8.5.2. Default Stream 3.2.8.5.3. Explicit Synchronization 3.2.8.5.4. Implicit Synchronization 3.2.8.5.5. Overlapping Behavior 3.2.8.5.6. Host Functions (Callbacks) 3.2.8.5.7. Stream Priorities 3.2.8.6. Programmatic Dependent Launch and Synchronization 3.2.8.6.1. Background 3.2.8.6.2.'},\n",
       " {'id': 165,\n",
       "  'content': 'API Description 3.2.8.6.3. Use in CUDA Graphs 3.2.8.7. CUDA Graphs 3.2.8.7.1. Graph Structure 3.2.8.7.1.1. Node Types 3.2.8.7.1.2. Edge Data 3.2.8.7.2. Creating a Graph Using Graph APIs 3.2.8.7.3. Creating a Graph Using Stream Capture 3.2.8.7.3.1. Cross-stream Dependencies and Events 3.2.8.7.3.2. Prohibited and Unhandled Operations 3.2.8.7.3.3.'},\n",
       " {'id': 166,\n",
       "  'content': 'Invalidation 3.2.8.7.4. CUDA User Objects 3.2.8.7.5. Updating Instantiated Graphs 3.2.8.7.5.1. Graph Update Limitations 3.2.8.7.5.2. Whole Graph Update 3.2.8.7.5.3. Individual node update 3.2.8.7.5.4. Individual node enable 3.2.8.7.6. Using Graph APIs 3.2.8.7.7. Device Graph Launch 3.2.8.7.7.1. Device Graph Creation 3.2.8.7.7.1.1. Device Graph Requirements 3.2.8.7.7.1.2. Device Graph Upload 3.2.8.7.7.1.3. Device Graph Update 3.2.8.7.7.2. Device Launch 3.2.8.7.7.2.1. Device Launch Modes 3.2.8.7.7.2.1.1. Fire and Forget Launch 3.2.8.7.7.2.1.2. Graph Execution Environments 3.2.8.7.7.2.1.3. Tail Launch 3.2.8.7.7.2.1.3.1. Tail Self-launch 3.2.8.7.7.2.1.4. Sibling Launch 3.2.8.7.8. Conditional Graph Nodes 3.2.8.7.8.1. Conditional Handles 3.2.8.7.8.2. Condtional Node Body Graph Requirements 3.2.8.7.8.3. Conditional IF Nodes 3.2.8.7.8.4. Conditional WHILE Nodes 3.2.8.8. Events 3.2.8.8.1. Creation and Destruction of Events 3.2.8.8.2. Elapsed Time 3.2.8.9. Synchronous Calls 3.2.9. Multi-Device System 3.2.9.1. Device Enumeration 3.2.9.2. Device Selection 3.2.9.3. Stream and Event Behavior 3.2.9.4. Peer-to-Peer Memory Access 3.2.9.4.1. IOMMU on Linux 3.2.9.5. Peer-to-Peer Memory Copy 3.2.10. Unified Virtual Address Space 3.2.11. Interprocess Communication 3.2.12. Error Checking 3.2.13. Call Stack 3.2.14. Texture and Surface Memory 3.2.14.1. Texture Memory 3.2.14.1.1. Texture Object API 3.2.14.1.2. 16-Bit Floating-Point Textures 3.2.14.1.3. Layered Textures 3.2.14.1.4. Cubemap Textures 3.2.14.1.5. Cubemap Layered Textures 3.2.14.1.6. Texture Gather 3.2.14.2. Surface Memory 3.2.14.2.1. Surface Object API 3.2.14.2.2. Cubemap Surfaces 3.2.14.2.3. Cubemap Layered Surfaces 3.2.14.3. CUDA Arrays 3.2.14.4. Read/Write Coherency 3.2.15. Graphics Interoperability 3.2.15.1. OpenGL Interoperability 3.2.15.2. Direct3D Interoperability 3.2.15.2.1. Direct3D 9 Version 3.2.15.2.2. Direct3D 10 Version 3.2.15.2.3. Direct3D 11 Version 3.2.15.3. SLI Interoperability 3.2.16. External Resource Interoperability 3.2.16.1. Vulkan Interoperability 3.2.16.1.1. Matching device UUIDs 3.2.16.1.2. Importing Memory Objects 3.2.16.1.3. Mapping Buffers onto Imported Memory Objects 3.2.16.1.4. Mapping Mipmapped Arrays onto Imported Memory Objects 3.2.16.1.5. Importing Synchronization Objects 3.2.16.1.6. Signaling/Waiting on Imported Synchronization Objects 3.2.16.2. OpenGL Interoperability 3.2.16.3. Direct3D 12 Interoperability 3.2.16.3.1. Matching Device LUIDs 3.2.16.3.2. Importing Memory Objects 3.2.16.3.3. Mapping Buffers onto Imported Memory Objects 3.2.16.3.4. Mapping Mipmapped Arrays onto Imported Memory Objects 3.2.16.3.5. Importing Synchronization Objects 3.2.16.3.6. Signaling/Waiting on Imported Synchronization Objects 3.2.16.4. Direct3D 11 Interoperability 3.2.16.4.1. Matching Device LUIDs 3.2.16.4.2. Importing Memory Objects 3.2.16.4.3. Mapping Buffers onto Imported Memory Objects 3.2.16.4.4. Mapping Mipmapped Arrays onto Imported Memory Objects 3.2.16.4.5. Importing Synchronization Objects 3.2.16.4.6. Signaling/Waiting on Imported Synchronization Objects 3.2.16.5. NVIDIA Software Communication Interface Interoperability (NVSCI) 3.2.16.5.1. Importing Memory Objects 3.2.16.5.2. Mapping Buffers onto Imported Memory Objects 3.2.16.5.3. Mapping Mipmapped Arrays onto Imported Memory Objects 3.2.16.5.4. Importing Synchronization Objects 3.2.16.5.5. Signaling/Waiting on Imported Synchronization Objects 3.3. Versioning and Compatibility 3.4. Compute Modes 3.5. Mode Switches 3.6. Tesla Compute Cluster Mode for Windows 4. Hardware Implementation 4.1. SIMT Architecture 4.2. Hardware Multithreading 5. Performance Guidelines 5.1. Overall Performance Optimization Strategies 5.2. Maximize Utilization 5.2.1. Application Level 5.2.2. Device Level 5.2.3. Multiprocessor Level 5.2.3.1. Occupancy Calculator 5.3. Maximize Memory Throughput 5.3.1. Data Transfer between Host and Device 5.3.2. Device Memory Accesses 5.4. Maximize Instruction Throughput 5.4.1. Arithmetic Instructions 5.4.2. Control Flow Instructions 5.4.3. Synchronization Instruction 5.5. Minimize Memory Thrashing 6. CUDA-Enabled GPUs 7. C++ Language Extensions 7.1. Function Execution Space Specifiers 7.1.1. __global__ 7.1.2.'},\n",
       " {'id': 167,\n",
       "  'content': '__device__ 7.1.3. __host__ 7.1.4. Undefined behavior 7.1.5. __noinline__ and __forceinline__ 7.1.6. __inline_hint__ 7.2. Variable Memory Space Specifiers 7.2.1. __device__ 7.2.2. __constant__ 7.2.3. __shared__ 7.2.4. __grid_constant__ 7.2.5. __managed__ 7.2.6. __restrict__ 7.3. Built-in Vector Types 7.3.1. char, short, int, long, longlong, float, double 7.3.2. dim3 7.4.'},\n",
       " {'id': 168,\n",
       "  'content': 'Built-in Variables 7.4.1. gridDim 7.4.2. blockIdx 7.4.3. blockDim 7.4.4. threadIdx 7.4.5. warpSize 7.5. Memory Fence Functions 7.6. Synchronization Functions 7.7. Mathematical Functions 7.8. Texture Functions 7.8.1. Texture Object API 7.8.1.1. tex1Dfetch() 7.8.1.2. tex1D() 7.8.1.3. tex1DLod() 7.8.1.4. tex1DGrad() 7.8.1.5. tex2D() 7.8.1.6. tex2D() for sparse CUDA arrays 7.8.1.7. tex2Dgather() 7.8.1.8. tex2Dgather() for sparse CUDA arrays 7.8.1.9. tex2DGrad() 7.8.1.10. tex2DGrad() for sparse CUDA arrays 7.8.1.11. tex2DLod() 7.8.1.12. tex2DLod() for sparse CUDA arrays 7.8.1.13. tex3D() 7.8.1.14. tex3D() for sparse CUDA arrays 7.8.1.15. tex3DLod() 7.8.1.16. tex3DLod() for sparse CUDA arrays 7.8.1.17. tex3DGrad() 7.8.1.18. tex3DGrad() for sparse CUDA arrays 7.8.1.19. tex1DLayered() 7.8.1.20.'},\n",
       " {'id': 169,\n",
       "  'content': 'tex1DLayeredLod() 7.8.1.21. tex1DLayeredGrad() 7.8.1.22. tex2DLayered() 7.8.1.23. tex2DLayered() for sparse CUDA arrays 7.8.1.24. tex2DLayeredLod() 7.8.1.25. tex2DLayeredLod() for sparse CUDA arrays 7.8.1.26. tex2DLayeredGrad() 7.8.1.27. tex2DLayeredGrad() for sparse CUDA arrays 7.8.1.28. texCubemap() 7.8.1.29.'},\n",
       " {'id': 170,\n",
       "  'content': 'texCubemapGrad() 7.8.1.30. texCubemapLod() 7.8.1.31. texCubemapLayered() 7.8.1.32. texCubemapLayeredGrad() 7.8.1.33. texCubemapLayeredLod() 7.9. Surface Functions 7.9.1. Surface Object API 7.9.1.1. surf1Dread() 7.9.1.2. surf1Dwrite 7.9.1.3. surf2Dread() 7.9.1.4. surf2Dwrite() 7.9.1.5. surf3Dread() 7.9.1.6. surf3Dwrite() 7.9.1.7. surf1DLayeredread() 7.9.1.8. surf1DLayeredwrite() 7.9.1.9. surf2DLayeredread() 7.9.1.10. surf2DLayeredwrite() 7.9.1.11. surfCubemapread() 7.9.1.12. surfCubemapwrite() 7.9.1.13. surfCubemapLayeredread() 7.9.1.14. surfCubemapLayeredwrite() 7.10. Read-Only Data Cache Load Function 7.11. Load Functions Using Cache Hints 7.12. Store Functions Using Cache Hints 7.13. Time Function 7.14. Atomic Functions 7.14.1. Arithmetic Functions 7.14.1.1. atomicAdd() 7.14.1.2. atomicSub() 7.14.1.3. atomicExch() 7.14.1.4. atomicMin() 7.14.1.5. atomicMax() 7.14.1.6. atomicInc() 7.14.1.7. atomicDec() 7.14.1.8. atomicCAS() 7.14.2. Bitwise Functions 7.14.2.1. atomicAnd() 7.14.2.2. atomicOr() 7.14.2.3. atomicXor() 7.15. Address Space Predicate Functions 7.15.1. __isGlobal() 7.15.2. __isShared() 7.15.3. __isConstant() 7.15.4. __isGridConstant() 7.15.5. __isLocal() 7.16. Address Space Conversion Functions 7.16.1. __cvta_generic_to_global() 7.16.2. __cvta_generic_to_shared() 7.16.3. __cvta_generic_to_constant() 7.16.4. __cvta_generic_to_local() 7.16.5. __cvta_global_to_generic() 7.16.6. __cvta_shared_to_generic() 7.16.7. __cvta_constant_to_generic() 7.16.8. __cvta_local_to_generic() 7.17. Alloca Function 7.17.1. Synopsis 7.17.2.'},\n",
       " {'id': 171, 'content': 'Description 7.17.3.'},\n",
       " {'id': 172,\n",
       "  'content': 'Example 7.18. Compiler Optimization Hint Functions 7.18.1. __builtin_assume_aligned() 7.18.2. __builtin_assume() 7.18.3. __assume() 7.18.4. __builtin_expect() 7.18.5. __builtin_unreachable() 7.18.6.'},\n",
       " {'id': 173,\n",
       "  'content': 'Restrictions 7.19. Warp Vote Functions 7.20. Warp Match Functions 7.20.1. Synopsis 7.20.2. Description 7.21. Warp Reduce Functions 7.21.1. Synopsis 7.21.2. Description 7.22. Warp Shuffle Functions 7.22.1. Synopsis 7.22.2.'},\n",
       " {'id': 174, 'content': 'Description 7.22.3.'},\n",
       " {'id': 175,\n",
       "  'content': 'Examples 7.22.3.1. Broadcast of a single value across a warp 7.22.3.2. Inclusive plus-scan across sub-partitions of 8 threads 7.22.3.3. Reduction across a warp 7.23. Nanosleep Function 7.23.1. Synopsis 7.23.2.'},\n",
       " {'id': 176, 'content': 'Description 7.23.3.'},\n",
       " {'id': 177,\n",
       "  'content': 'Example 7.24. Warp Matrix Functions 7.24.1. Description 7.24.2.'},\n",
       " {'id': 178,\n",
       "  'content': 'Alternate Floating Point 7.24.3. Double Precision 7.24.4. Sub-byte Operations 7.24.5. Restrictions 7.24.6. Element Types and Matrix Sizes 7.24.7. Example 7.25.'},\n",
       " {'id': 179, 'content': 'DPX 7.25.1.'},\n",
       " {'id': 180,\n",
       "  'content': 'Examples 7.26. Asynchronous Barrier 7.26.1. Simple Synchronization Pattern 7.26.2. Temporal Splitting and Five Stages of Synchronization 7.26.3. Bootstrap Initialization, Expected Arrival Count, and Participation 7.26.4. A Barrier’s Phase: Arrival, Countdown, Completion, and Reset 7.26.5. Spatial Partitioning (also known as Warp Specialization) 7.26.6. Early Exit (Dropping out of Participation) 7.26.7.'},\n",
       " {'id': 181,\n",
       "  'content': 'Completion Function 7.26.8. Memory Barrier Primitives Interface 7.26.8.1. Data Types 7.26.8.2. Memory Barrier Primitives API 7.27. Asynchronous Data Copies 7.27.1. memcpy_async API 7.27.2. Copy and Compute Pattern - Staging Data Through Shared Memory 7.27.3. Without memcpy_async 7.27.4. With memcpy_async 7.27.5. Asynchronous Data Copies using cuda::barrier 7.27.6. Performance Guidance for memcpy_async 7.27.6.1. Alignment 7.27.6.2.'},\n",
       " {'id': 182,\n",
       "  'content': 'Trivially copyable 7.27.6.3. Warp Entanglement - Commit 7.27.6.4. Warp Entanglement - Wait 7.27.6.5. Warp Entanglement - Arrive-On 7.27.6.6. Keep Commit and Arrive-On Operations Converged 7.28. Asynchronous Data Copies using cuda::pipeline 7.28.1. Single-Stage Asynchronous Data Copies using cuda::pipeline 7.28.2. Multi-Stage Asynchronous Data Copies using cuda::pipeline 7.28.3. Pipeline Interface 7.28.4. Pipeline Primitives Interface 7.28.4.1. memcpy_async Primitive 7.28.4.2. Commit Primitive 7.28.4.3. Wait Primitive 7.28.4.4. Arrive On Barrier Primitive 7.29. Asynchronous Data Copies using Tensor Memory Access (TMA) 7.29.1. Using TMA to transfer one-dimensional arrays 7.29.2. Using TMA to transfer multi-dimensional arrays 7.29.2.1. Multi-dimensional TMA PTX wrappers 7.30. Profiler Counter Function 7.31. Assertion 7.32. Trap function 7.33. Breakpoint Function 7.34. Formatted Output 7.34.1. Format Specifiers 7.34.2. Limitations 7.34.3. Associated Host-Side API 7.34.4. Examples 7.35. Dynamic Global Memory Allocation and Operations 7.35.1. Heap Memory Allocation 7.35.2. Interoperability with Host Memory API 7.35.3. Examples 7.35.3.1. Per Thread Allocation 7.35.3.2. Per Thread Block Allocation 7.35.3.3. Allocation Persisting Between Kernel Launches 7.36. Execution Configuration 7.37. Launch Bounds 7.38. Maximum Number of Registers per Thread 7.39. #pragma unroll 7.40. SIMD Video Instructions 7.41. Diagnostic Pragmas 8. Cooperative Groups 8.1. Introduction 8.2. What’s New in Cooperative Groups 8.2.1. CUDA 12.2 8.2.2. CUDA 12.1 8.2.3. CUDA 12.0 8.3. Programming Model Concept 8.3.1. Composition Example 8.4. Group Types 8.4.1. Implicit Groups 8.4.1.1. Thread Block Group 8.4.1.2. Cluster Group 8.4.1.3. Grid Group 8.4.1.4. Multi Grid Group 8.4.2. Explicit Groups 8.4.2.1. Thread Block Tile 8.4.2.1.1. Warp-Synchronous Code Pattern 8.4.2.1.2. Single thread group 8.4.2.2. Coalesced Groups 8.4.2.2.1. Discovery Pattern 8.5. Group Partitioning 8.5.1. tiled_partition 8.5.2. labeled_partition 8.5.3. binary_partition 8.6. Group Collectives 8.6.1. Synchronization 8.6.1.1. barrier_arrive and barrier_wait 8.6.1.2. sync 8.6.2. Data Transfer 8.6.2.1. memcpy_async 8.6.2.2. wait and wait_prior 8.6.3. Data Manipulation 8.6.3.1. reduce 8.6.3.2. Reduce Operators 8.6.3.3. inclusive_scan and exclusive_scan 8.6.4. Execution control 8.6.4.1. invoke_one and invoke_one_broadcast 8.7. Grid Synchronization 8.8. Multi-Device Synchronization 9. CUDA Dynamic Parallelism 9.1. Introduction 9.1.1.'},\n",
       " {'id': 183, 'content': 'Overview 9.1.2.'},\n",
       " {'id': 184,\n",
       "  'content': 'Glossary 9.2. Execution Environment and Memory Model 9.2.1. Execution Environment 9.2.1.1. Parent and Child Grids 9.2.1.2. Scope of CUDA Primitives 9.2.1.3. Synchronization 9.2.1.4. Streams and Events 9.2.1.5. Ordering and Concurrency 9.2.1.6. Device Management 9.2.2. Memory Model 9.2.2.1. Coherence and Consistency 9.2.2.1.1. Global Memory 9.2.2.1.2. Zero Copy Memory 9.2.2.1.3. Constant Memory 9.2.2.1.4. Shared and Local Memory 9.2.2.1.5. Local Memory 9.2.2.1.6. Texture Memory 9.3. Programming Interface 9.3.1. CUDA C++ Reference 9.3.1.1. Device-Side Kernel Launch 9.3.1.1.1. Launches are Asynchronous 9.3.1.1.2. Launch Environment Configuration 9.3.1.2. Streams 9.3.1.2.1. The Implicit (NULL) Stream 9.3.1.2.2. The Fire-and-Forget Stream 9.3.1.2.3. The Tail Launch Stream 9.3.1.3. Events 9.3.1.4. Synchronization 9.3.1.5. Device Management 9.3.1.6. Memory Declarations 9.3.1.6.1. Device and Constant Memory 9.3.1.6.2. Textures and Surfaces 9.3.1.6.3. Shared Memory Variable Declarations 9.3.1.6.4. Symbol Addresses 9.3.1.7. API Errors and Launch Failures 9.3.1.7.1. Launch Setup APIs 9.3.1.8. API Reference 9.3.2. Device-side Launch from PTX 9.3.2.1. Kernel Launch APIs 9.3.2.1.1. cudaLaunchDevice 9.3.2.1.2. cudaGetParameterBuffer 9.3.2.2. Parameter Buffer Layout 9.3.3. Toolkit Support for Dynamic Parallelism 9.3.3.1. Including Device Runtime API in CUDA Code 9.3.3.2. Compiling and Linking 9.4.'},\n",
       " {'id': 185,\n",
       "  'content': 'Programming Guidelines 9.4.1. Basics 9.4.2. Performance 9.4.2.1. Dynamic-parallelism-enabled Kernel Overhead 9.4.3. Implementation Restrictions and Limitations 9.4.3.1. Runtime 9.4.3.1.1. Memory Footprint 9.4.3.1.2. Pending Kernel Launches 9.4.3.1.3. Configuration Options 9.4.3.1.4. Memory Allocation and Lifetime 9.4.3.1.5. SM Id and Warp Id 9.4.3.1.6. ECC Errors 9.5. CDP2 vs CDP1 9.5.1. Differences Between CDP1 and CDP2 9.5.2. Compatibility and Interoperability 9.6. Legacy CUDA Dynamic Parallelism (CDP1) 9.6.1. Execution Environment and Memory Model (CDP1) 9.6.1.1. Execution Environment (CDP1) 9.6.1.1.1. Parent and Child Grids (CDP1) 9.6.1.1.2. Scope of CUDA Primitives (CDP1) 9.6.1.1.3. Synchronization (CDP1) 9.6.1.1.4. Streams and Events (CDP1) 9.6.1.1.5. Ordering and Concurrency (CDP1) 9.6.1.1.6. Device Management (CDP1) 9.6.1.2. Memory Model (CDP1) 9.6.1.2.1. Coherence and Consistency (CDP1) 9.6.1.2.1.1. Global Memory (CDP1) 9.6.1.2.1.2. Zero Copy Memory (CDP1) 9.6.1.2.1.3. Constant Memory (CDP1) 9.6.1.2.1.4. Shared and Local Memory (CDP1) 9.6.1.2.1.5. Local Memory (CDP1) 9.6.1.2.1.6. Texture Memory (CDP1) 9.6.2. Programming Interface (CDP1) 9.6.2.1. CUDA C++ Reference (CDP1) 9.6.2.1.1. Device-Side Kernel Launch (CDP1) 9.6.2.1.1.1. Launches are Asynchronous (CDP1) 9.6.2.1.1.2. Launch Environment Configuration (CDP1) 9.6.2.1.2. Streams (CDP1) 9.6.2.1.2.1. The Implicit (NULL) Stream (CDP1) 9.6.2.1.3. Events (CDP1) 9.6.2.1.4. Synchronization (CDP1) 9.6.2.1.4.1. Block Wide Synchronization (CDP1) 9.6.2.1.5. Device Management (CDP1) 9.6.2.1.6. Memory Declarations (CDP1) 9.6.2.1.6.1. Device and Constant Memory (CDP1) 9.6.2.1.6.2. Textures and Surfaces (CDP1) 9.6.2.1.6.3. Shared Memory Variable Declarations (CDP1) 9.6.2.1.6.4. Symbol Addresses (CDP1) 9.6.2.1.7. API Errors and Launch Failures (CDP1) 9.6.2.1.7.1. Launch Setup APIs (CDP1) 9.6.2.1.8. API Reference (CDP1) 9.6.2.2. Device-side Launch from PTX (CDP1) 9.6.2.2.1. Kernel Launch APIs (CDP1) 9.6.2.2.1.1. cudaLaunchDevice (CDP1) 9.6.2.2.1.2. cudaGetParameterBuffer (CDP1) 9.6.2.2.2. Parameter Buffer Layout (CDP1) 9.6.2.3. Toolkit Support for Dynamic Parallelism (CDP1) 9.6.2.3.1. Including Device Runtime API in CUDA Code (CDP1) 9.6.2.3.2. Compiling and Linking (CDP1) 9.6.3. Programming Guidelines (CDP1) 9.6.3.1. Basics (CDP1) 9.6.3.2. Performance (CDP1) 9.6.3.2.1. Synchronization (CDP1) 9.6.3.2.2. Dynamic-parallelism-enabled Kernel Overhead (CDP1) 9.6.3.3. Implementation Restrictions and Limitations (CDP1) 9.6.3.3.1. Runtime (CDP1) 9.6.3.3.1.1. Memory Footprint (CDP1) 9.6.3.3.1.2. Nesting and Synchronization Depth (CDP1) 9.6.3.3.1.3. Pending Kernel Launches (CDP1) 9.6.3.3.1.4. Configuration Options (CDP1) 9.6.3.3.1.5. Memory Allocation and Lifetime (CDP1) 9.6.3.3.1.6. SM Id and Warp Id (CDP1) 9.6.3.3.1.7. ECC Errors (CDP1) 10. Virtual Memory Management 10.1. Introduction 10.2. Query for Support 10.3. Allocating Physical Memory 10.3.1. Shareable Memory Allocations 10.3.2. Memory Type 10.3.2.1. Compressible Memory 10.4. Reserving a Virtual Address Range 10.5. Virtual Aliasing Support 10.6. Mapping Memory 10.7. Controlling Access Rights 11. Stream Ordered Memory Allocator 11.1. Introduction 11.2.'},\n",
       " {'id': 186,\n",
       "  'content': 'Query for Support 11.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync) 11.4. Memory Pools and the cudaMemPool_t 11.5. Default/Implicit Pools 11.6. Explicit Pools 11.7. Physical Page Caching Behavior 11.8. Resource Usage Statistics 11.9. Memory Reuse Policies 11.9.1. cudaMemPoolReuseFollowEventDependencies 11.9.2. cudaMemPoolReuseAllowOpportunistic 11.9.3. cudaMemPoolReuseAllowInternalDependencies 11.9.4. Disabling Reuse Policies 11.10. Device Accessibility for Multi-GPU Support 11.11. IPC Memory Pools 11.11.1. Creating and Sharing IPC Memory Pools 11.11.2. Set Access in the Importing Process 11.11.3. Creating and Sharing Allocations from an Exported Pool 11.11.4. IPC Export Pool Limitations 11.11.5. IPC Import Pool Limitations 11.12. Synchronization API Actions 11.13.'},\n",
       " {'id': 187,\n",
       "  'content': 'Addendums 11.13.1. cudaMemcpyAsync Current Context/Device Sensitivity 11.13.2. cuPointerGetAttribute Query 11.13.3. cuGraphAddMemsetNode 11.13.4. Pointer Attributes 12. Graph Memory Nodes 12.1. Introduction 12.2.'},\n",
       " {'id': 188,\n",
       "  'content': 'Support and Compatibility 12.3. API Fundamentals 12.3.1. Graph Node APIs 12.3.2. Stream Capture 12.3.3. Accessing and Freeing Graph Memory Outside of the Allocating Graph 12.3.4. cudaGraphInstantiateFlagAutoFreeOnLaunch 12.4. Optimized Memory Reuse 12.4.1. Address Reuse within a Graph 12.4.2. Physical Memory Management and Sharing 12.5. Performance Considerations 12.5.1. First Launch / cudaGraphUpload 12.6. Physical Memory Footprint 12.7. Peer Access 12.7.1. Peer Access with Graph Node APIs 12.7.2. Peer Access with Stream Capture 13. Mathematical Functions 13.1.'},\n",
       " {'id': 189,\n",
       "  'content': 'Standard Functions 13.2. Intrinsic Functions 14. C++ Language Support 14.1. C++11 Language Features 14.2. C++14 Language Features 14.3. C++17 Language Features 14.4. C++20 Language Features 14.5. Restrictions 14.5.1. Host Compiler Extensions 14.5.2. Preprocessor Symbols 14.5.2.1. __CUDA_ARCH__ 14.5.3. Qualifiers 14.5.3.1. Device Memory Space Specifiers 14.5.3.2. __managed__ Memory Space Specifier 14.5.3.3. Volatile Qualifier 14.5.4. Pointers 14.5.5. Operators 14.5.5.1. Assignment Operator 14.5.5.2. Address Operator 14.5.6. Run Time Type Information (RTTI) 14.5.7. Exception Handling 14.5.8.'},\n",
       " {'id': 190,\n",
       "  'content': 'Standard Library 14.5.9. Namespace Reservations 14.5.10. Functions 14.5.10.1. External Linkage 14.5.10.2. Implicitly-declared and explicitly-defaulted functions 14.5.10.3. Function Parameters 14.5.10.3.1. __global__ Function Argument Processing 14.5.10.3.2. Toolkit and Driver Compatibility 14.5.10.3.3. Link Compatibility across Toolkit Revisions 14.5.10.4. Static Variables within Function 14.5.10.5. Function Pointers 14.5.10.6. Function Recursion 14.5.10.7. Friend Functions 14.5.10.8. Operator Function 14.5.10.9. Allocation and Deallocation Functions 14.5.11. Classes 14.5.11.1. Data Members 14.5.11.2. Function Members 14.5.11.3. Virtual Functions 14.5.11.4. Virtual Base Classes 14.5.11.5. Anonymous Unions 14.5.11.6. Windows-Specific 14.5.12.'},\n",
       " {'id': 191,\n",
       "  'content': 'Templates 14.5.13. Trigraphs and Digraphs 14.5.14. Const-qualified variables 14.5.15. Long Double 14.5.16. Deprecation Annotation 14.5.17. Noreturn Annotation 14.5.18. [[likely]] / [[unlikely]] Standard Attributes 14.5.19. const and pure GNU Attributes 14.5.20. __nv_pure__ Attribute 14.5.21. Intel Host Compiler Specific 14.5.22. C++11 Features 14.5.22.1. Lambda Expressions 14.5.22.2. std::initializer_list 14.5.22.3. Rvalue references 14.5.22.4. Constexpr functions and function templates 14.5.22.5. Constexpr variables 14.5.22.6. Inline namespaces 14.5.22.6.1. Inline unnamed namespaces 14.5.22.7. thread_local 14.5.22.8. __global__ functions and function templates 14.5.22.9. __managed__ and __shared__ variables 14.5.22.10. Defaulted functions 14.5.23. C++14 Features 14.5.23.1. Functions with deduced return type 14.5.23.2. Variable templates 14.5.24. C++17 Features 14.5.24.1. Inline Variable 14.5.24.2. Structured Binding 14.5.25. C++20 Features 14.5.25.1. Module support 14.5.25.2. Coroutine support 14.5.25.3. Three-way comparison operator 14.5.25.4. Consteval functions 14.6. Polymorphic Function Wrappers 14.7. Extended Lambdas 14.7.1. Extended Lambda Type Traits 14.7.2. Extended Lambda Restrictions 14.7.3. Notes on __host__ __device__ lambdas 14.7.4. *this Capture By Value 14.7.5. Additional Notes 14.8.'},\n",
       " {'id': 192,\n",
       "  'content': 'Code Samples 14.8.1. Data Aggregation Class 14.8.2. Derived Class 14.8.3. Class Template 14.8.4. Function Template 14.8.5. Functor Class 15. Texture Fetching 15.1. Nearest-Point Sampling 15.2. Linear Filtering 15.3. Table Lookup 16. Compute Capabilities 16.1. Feature Availability 16.2. Features and Technical Specifications 16.3. Floating-Point Standard 16.4. Compute Capability 5.x 16.4.1. Architecture 16.4.2. Global Memory 16.4.3. Shared Memory 16.5. Compute Capability 6.x 16.5.1. Architecture 16.5.2. Global Memory 16.5.3. Shared Memory 16.6. Compute Capability 7.x 16.6.1. Architecture 16.6.2. Independent Thread Scheduling 16.6.3. Global Memory 16.6.4. Shared Memory 16.7. Compute Capability 8.x 16.7.1. Architecture 16.7.2. Global Memory 16.7.3. Shared Memory 16.8. Compute Capability 9.0 16.8.1. Architecture 16.8.2. Global Memory 16.8.3. Shared Memory 16.8.4. Features Accelerating Specialized Computations 17. Driver API 17.1. Context 17.2. Module 17.3. Kernel Execution 17.4. Interoperability between Runtime and Driver APIs 17.5. Driver Entry Point Access 17.5.1. Introduction 17.5.2. Driver Function Typedefs 17.5.3. Driver Function Retrieval 17.5.3.1. Using the driver API 17.5.3.2. Using the runtime API 17.5.3.3. Retrieve per-thread default stream versions 17.5.3.4. Access new CUDA features 17.5.4. Potential Implications with cuGetProcAddress 17.5.4.1. Implications with cuGetProcAddress vs Implicit Linking 17.5.4.2. Compile Time vs Runtime Version Usage in cuGetProcAddress 17.5.4.3. API Version Bumps with Explicit Version Checks 17.5.4.4. Issues with Runtime API Usage 17.5.4.5. Issues with Runtime API and Dynamic Versioning 17.5.4.6. Issues with Runtime API allowing CUDA Version 17.5.4.7. Implications to API/ABI 17.5.5. Determining cuGetProcAddress Failure Reasons 18. CUDA Environment Variables 19. Unified Memory Programming 19.1. Unified Memory Introduction 19.1.1. System Requirements for Unified Memory 19.1.2. Programming Model 19.1.2.1. Allocation APIs for System-Allocated Memory 19.1.2.2. Allocation API for CUDA Managed Memory: cudaMallocManaged() 19.1.2.3. Global-Scope Managed Variables Using __managed__ 19.1.2.4. Difference between Unified Memory and Mapped Memory 19.1.2.5. Pointer Attributes 19.1.2.6. Runtime detection of Unified Memory Support Level 19.1.2.7. GPU Memory Oversubscription 19.1.2.8. Performance Hints 19.1.2.8.1. Data Prefetching 19.1.2.8.2. Data Usage Hints 19.1.2.8.3. Querying Data Usage Attributes on Managed Memory 19.2. Unified memory on devices with full CUDA Unified Memory support 19.2.1. System-Allocated Memory: in-depth examples 19.2.1.1. File-backed Unified Memory 19.2.1.2. Inter-Process Communication (IPC) with Unified Memory 19.2.2. Performance Tuning 19.2.2.1. Memory Paging and Page Sizes 19.2.2.1.1. Choosing the right page size 19.2.2.1.2. CPU and GPU page tables: hardware coherency vs. software coherency 19.2.2.2. Direct Unified Memory Access from host 19.2.2.3. Host Native Atomics 19.3. Unified memory on devices without full CUDA Unified Memory support 19.3.1. Unified memory on devices with only CUDA Managed Memory support 19.3.2. Unified memory on Windows or devices with compute capability 5.x 19.3.2.1. Data Migration and Coherency 19.3.2.2. GPU Memory Oversubscription 19.3.2.3. Multi-GPU 19.3.2.4. Coherency and Concurrency 19.3.2.4.1. GPU Exclusive Access To Managed Memory 19.3.2.4.2. Explicit Synchronization and Logical GPU Activity 19.3.2.4.3. Managing Data Visibility and Concurrent CPU + GPU Access with Streams 19.3.2.4.4. Stream Association Examples 19.3.2.4.5. Stream Attach With Multithreaded Host Programs 19.3.2.4.6. Advanced Topic: Modular Programs and Data Access Constraints 19.3.2.4.7. Memcpy()/Memset() Behavior With Stream-associated Unified Memory 20. Lazy Loading 20.1. What is Lazy Loading? 20.2. Lazy Loading version support 20.2.1. Driver 20.2.2. Toolkit 20.2.3. Compiler 20.3. Triggering loading of kernels in lazy mode 20.3.1. CUDA Driver API 20.3.2. CUDA Runtime API 20.4. Querying whether Lazy Loading is turned on 20.5. Possible issues when adopting lazy loading 20.5.1. Concurrent execution 20.5.2. Allocators 20.5.3. Autotuning 21. Extended GPU Memory 21.1. Preliminaries 21.1.1.'},\n",
       " {'id': 193,\n",
       "  'content': 'EGM Platforms: System topology 21.1.2. Socket Identifiers: What are they? How to access them?'},\n",
       " {'id': 194,\n",
       "  'content': '21.1.3. Allocators and EGM support 21.1.4. Memory management extensions to current APIs 21.2. Using the EGM Interface 21.2.1. Single-Node, Single-GPU 21.2.2. Single-Node, Multi-GPU 21.2.2.1. Using VMM APIs 21.2.2.2. Using CUDA Memory Pool 21.2.3. Multi-Node, Single-GPU 22. Notices 22.1.'},\n",
       " {'id': 195,\n",
       "  'content': 'Notice 22.2. OpenCL 22.3. Trademarks CUDA C++ Programming Guide » 1. Introduction v12.5 | PDF | Archive CUDA C++ Programming Guide The programming guide to the CUDA model and interface. Changes from Version 12.4 Added section Asynchronous Data Copies using Tensor Memory Access (TMA) . Added Unified Memory Programming guide supporting Grace Hopper with Address Translation Service (ATS) and Heterogeneous Memory Management (HMM ) on x86. Introduction \\uf0c1 1.1. The Benefits of Using GPUs \\uf0c1 The Graphics Processing Unit (GPU) 1 provides much higher instruction throughput and memory bandwidth than the CPU within a similar price and power envelope. Many applications leverage these higher capabilities to run faster on the GPU than on the CPU (see GPU Applications ). Other computing devices, like FPGAs, are also very energy efficient, but offer much less programming flexibility than GPUs. This difference in capabilities between the GPU and the CPU exists because they are designed with different goals in mind. While the CPU is designed to excel at executing a sequence of operations, called a thread , as fast as possible and can execute a few tens of these threads in parallel, the GPU is designed to excel at executing thousands of them in parallel (amortizing the slower single-thread performance to achieve greater throughput). The GPU is specialized for highly parallel computations and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control. The schematic Figure 1 shows an example distribution of chip resources for a CPU versus a GPU. Figure 1 The GPU Devotes More Transistors to Data Processing \\uf0c1 Devoting more transistors to data processing, for example, floating-point computations, is beneficial for highly parallel computations; the GPU can hide memory access latencies with computation, instead of relying on large data caches and complex flow control to avoid long memory access latencies, both of which are expensive in terms of transistors. In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance. Applications with a high degree of parallelism can exploit this massively parallel nature of the GPU to achieve higher performance than on the CPU. 1.2. CUDA®: A General-Purpose Parallel Computing Platform and Programming Model \\uf0c1 In November 2006, NVIDIA ® introduced CUDA ® , a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. CUDA comes with a software environment that allows developers to use C++ as a high-level programming language. As illustrated by Figure 2 , other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC. Figure 2 GPU Computing Applications. CUDA is designed to support various languages and application programming interfaces. \\uf0c1 1.3. A Scalable Programming Model \\uf0c1 The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems. The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores. The CUDA parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C. At its core are three key abstractions — a hierarchy of thread groups, shared memories, and barrier synchronization — that are simply exposed to the programmer as a minimal set of language extensions. These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block. This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3 , and only the runtime system needs to know the physical multiprocessor count. This scalable programming model allows the GPU architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions: from the high-performance enthusiast GeForce GPUs and professional Quadro and Tesla computing products to a variety of inexpensive, mainstream GeForce GPUs (see CUDA-Enabled GPUs for a list of all CUDA-enabled GPUs). Figure 3 Automatic Scalability \\uf0c1 Note A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors. 1.4.'},\n",
       " {'id': 196,\n",
       "  'content': 'Document Structure \\uf0c1 This document is organized into the following sections: Introduction is a general introduction to CUDA. Programming Model outlines the CUDA programming model. Programming Interface describes the programming interface. Hardware Implementation describes the hardware implementation. Performance Guidelines gives some guidance on how to achieve maximum performance. CUDA-Enabled GPUs lists all CUDA-enabled devices. C++ Language Extensions is a detailed description of all extensions to the C++ language. Cooperative Groups describes synchronization primitives for various groups of CUDA threads. CUDA Dynamic Parallelism describes how to launch and synchronize one kernel from another. Virtual Memory Management describes how to manage the unified virtual address space. Stream Ordered Memory Allocator describes how applications can order memory allocation and deallocation. Graph Memory Nodes describes how graphs can create and own memory allocations. Mathematical Functions lists the mathematical functions supported in CUDA. C++ Language Support lists the C++ features supported in device code. Texture Fetching gives more details on texture fetching. Compute Capabilities gives the technical specifications of various devices, as well as more architectural details. Driver API introduces the low-level driver API. CUDA Environment Variables lists all the CUDA environment variables. Unified Memory Programming introduces the Unified Memory programming model. 1 The graphics qualifier comes from the fact that when the GPU was originally created, two decades ago, it was designed as a specialized processor to accelerate graphics rendering. Driven by the insatiable market demand for real-time, high-definition, 3D graphics, it has evolved into a general processor used for many more workloads than just graphics rendering.'},\n",
       " {'id': 197,\n",
       "  'content': '2. Programming Model \\uf0c1 This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++. An extensive description of CUDA C++ is given in Programming Interface . Full code for the vector addition example used in this chapter and the next can be found in the vectorAdd CUDA sample . 2.1. Kernels \\uf0c1 CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels , that, when called, are executed N times in parallel by N different CUDA threads , as opposed to only once like regular C++ functions. A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new >> execution configuration syntax (see C++ Language Extensions ). Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables. As an illustration, the following sample code, using the built-in variable threadIdx , adds two vectors A and B of size N and stores the result into vector C : // Kernel definition __global__ void VecAdd ( float * A , float * B , float * C ) { int i = threadIdx . x ; C [ i ] = A [ i ] + B [ i ]; } int main () { ... // Kernel invocation with N threads VecAdd >> ( A , B , C ); ... } Here, each of the N threads that execute VecAdd() performs one pair-wise addition. 2.2. Thread Hierarchy \\uf0c1 For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index , forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block . This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume. The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy) , the thread ID of a thread of index (x, y) is (x + y Dx) ; for a three-dimensional block of size (Dx, Dy, Dz) , the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy) . As an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C : // Kernel definition __global__ void MatAdd ( float A [ N ][ N ], float B [ N ][ N ], float C [ N ][ N ]) { int i = threadIdx . x ; int j = threadIdx . y ; C [ i ][ j ] = A [ i ][ j ] + B [ i ][ j ]; } int main () { ... // Kernel invocation with one block of N * N * 1 threads int numBlocks = 1 ; dim3 threadsPerBlock ( N , N ); MatAdd >> ( A , B , C ); ... } There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads. However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks. Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks as illustrated by Figure 4 . The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system. Figure 4 Grid of Thread Blocks \\uf0c1 The number of threads per block and the number of blocks per grid specified in the >> syntax can be of type int or dim3 . Two-dimensional blocks or grids can be specified as in the example above. Each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in blockIdx variable. The dimension of the thread block is accessible within the kernel through the built-in blockDim variable. Extending the previous MatAdd() example to handle multiple blocks, the code becomes as follows. // Kernel definition __global__ void MatAdd ( float A [ N ][ N ], float B [ N ][ N ], float C [ N ][ N ]) { int i = blockIdx . x * blockDim . x + threadIdx . x ; int j = blockIdx . y * blockDim . y + threadIdx . y ; if ( i >> ( A , B , C ); ... } A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. The grid is created with enough blocks to have one thread per matrix element as before. For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case. Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated by Figure 3 , enabling programmers to write code that scales with the number of cores. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed. Shared Memory gives an example of using shared memory. In addition to __syncthreads() , the Cooperative Groups API provides a rich set of thread-synchronization primitives. For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads() is expected to be lightweight. 2.2.1. Thread Block Clusters \\uf0c1 With the introduction of NVIDIA Compute Capability 9.0 , the CUDA programming model introduces an optional level of hierarchy called Thread Block Clusters that are made up of thread blocks. Similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a GPU Processing Cluster (GPC) in the GPU. Similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or three-dimension as illustrated by Figure 5 . The number of thread blocks in a cluster can be user-defined, and a maximum of 8 thread blocks in a cluster is supported as a portable cluster size in CUDA. Note that on GPU hardware or MIG configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly. Identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is architecture-specific and can be queried using the cudaOccupancyMaxPotentialClusterSize API. Figure 5 Grid of Thread Block Clusters \\uf0c1 Note In a kernel launched using cluster support, the gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes. The rank of a block in a cluster can be found using the Cluster Group API. A thread block cluster can be enabled in a kernel either using a compiler time kernel attribute using __cluster_dims__(X,Y,Z) or using the CUDA kernel launch API cudaLaunchKernelEx . The example below shows how to launch a cluster using compiler time kernel attribute. The cluster size using kernel attribute is fixed at compile time and then the kernel can be launched using the classical >> . If a kernel uses compile-time cluster size, the cluster size cannot be modified when launching the kernel. // Kernel definition // Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension __global__ void __cluster_dims__ ( 2 , 1 , 1 ) cluster_kernel ( float * input , float * output ) { } int main () { float * input , * output ; // Kernel invocation with compile time cluster size dim3 threadsPerBlock ( 16 , 16 ); dim3 numBlocks ( N / threadsPerBlock . x , N / threadsPerBlock . y ); // The grid dimension is not affected by cluster launch, and is still enumerated // using number of blocks. // The grid dimension must be a multiple of cluster size. cluster_kernel >> ( input , output ); } A thread block cluster size can also be set at runtime and the kernel can be launched using the CUDA kernel launch API cudaLaunchKernelEx . The code example below shows how to launch a cluster kernel using the extensible API. // Kernel definition // No compile time attribute attached to the kernel __global__ void cluster_kernel ( float * input , float * output ) { } int main () { float * input , * output ; dim3 threadsPerBlock ( 16 , 16 ); dim3 numBlocks ( N / threadsPerBlock . y ); // Kernel invocation with runtime cluster size { cudaLaunchConfig_t config = { 0 }; // The grid dimension is not affected by cluster launch, and is still enumerated // using number of blocks. // The grid dimension should be a multiple of cluster size. config .'},\n",
       " {'id': 198,\n",
       "  'content': 'gridDim = numBlocks ; config . blockDim = threadsPerBlock ; cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ]. id = cudaLaunchAttributeClusterDimension ; attribute [ 0 ]. val . clusterDim . x = 2 ; // Cluster size in X-dimension attribute [ 0 ]. y = 1 ; attribute [ 0 ].'},\n",
       " {'id': 199, 'content': 'z = 1 ; config .'},\n",
       " {'id': 200,\n",
       "  'content': 'attrs = attribute ; config . numAttrs = 1 ; cudaLaunchKernelEx ( & config , cluster_kernel , input , output ); } } In GPUs with compute capability 9.0, all the thread blocks in the cluster are guaranteed to be co-scheduled on a single GPU Processing Cluster (GPC) and allow thread blocks in the cluster to perform hardware-supported synchronization using the Cluster Group API cluster.sync() . Cluster group also provides member functions to query cluster group size in terms of number of threads or number of blocks using num_threads() and num_blocks() API respectively. The rank of a thread or block in the cluster group can be queried using dim_threads() and dim_blocks() API respectively. Thread blocks that belong to a cluster have access to the Distributed Shared Memory. Thread blocks in a cluster have the ability to read, write, and perform atomics to any address in the distributed shared memory. Distributed Shared Memory gives an example of performing histograms in distributed shared memory. 2.3. Memory Hierarchy \\uf0c1 CUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 6 . Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. Thread blocks in a thread block cluster can perform read, write, and atomics operations on each other’s shared memory. All threads have access to the same global memory. There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses ). Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory ). The global, constant, and texture memory spaces are persistent across kernel launches by the same application. Figure 6 Memory Hierarchy \\uf0c1 2.4. Heterogeneous Programming \\uf0c1 As illustrated by Figure 7 , the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program. This is the case, for example, when the kernels execute on a GPU and the rest of the C++ program executes on a CPU. The CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM, referred to as host memory and device memory , respectively. Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in Programming Interface ). This includes device memory allocation and deallocation as well as data transfer between host and device memory. Unified Memory provides managed memory to bridge the host and device memory spaces. Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device. See Unified Memory Programming for an introduction to Unified Memory. Figure 7 Heterogeneous Programming \\uf0c1 Note Serial code executes on the host while parallel code executes on the device. 2.5. Asynchronous SIMT Programming Model \\uf0c1 In the CUDA programming model a thread is the lowest level of abstraction for doing a computation or a memory operation. Starting with devices based on the NVIDIA Ampere GPU architecture, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model. The asynchronous programming model defines the behavior of asynchronous operations with respect to CUDA threads. The asynchronous programming model defines the behavior of Asynchronous Barrier for synchronization between CUDA threads. The model also explains and defines how cuda::memcpy_async can be used to move data asynchronously from global memory while computing in the GPU. 2.5.1. Asynchronous Operations \\uf0c1 An asynchronous operation is defined as an operation that is initiated by a CUDA thread and is executed asynchronously as-if by another thread. In a well formed program one or more CUDA threads synchronize with the asynchronous operation. The CUDA thread that initiated the asynchronous operation is not required to be among the synchronizing threads. Such an asynchronous thread (an as-if thread) is always associated with the CUDA thread that initiated the asynchronous operation. An asynchronous operation uses a synchronization object to synchronize the completion of the operation. Such a synchronization object can be explicitly managed by a user (e.g., cuda::memcpy_async ) or implicitly managed within a library (e.g., cooperative_groups::memcpy_async ). A synchronization object could be a cuda::barrier or a cuda::pipeline . These objects are explained in detail in Asynchronous Barrier and Asynchronous Data Copies using cuda::pipeline . These synchronization objects can be used at different thread scopes. A scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation. The following table defines the thread scopes available in CUDA C++ and the threads that can be synchronized with each. Thread Scope Description cuda::thread_scope::thread_scope_thread Only the CUDA thread which initiated asynchronous operations synchronizes. cuda::thread_scope::thread_scope_block All or any CUDA threads within the same thread block as the initiating thread synchronizes. cuda::thread_scope::thread_scope_device All or any CUDA threads in the same GPU device as the initiating thread synchronizes. cuda::thread_scope::thread_scope_system All or any CUDA or CPU threads in the same system as the initiating thread synchronizes. These thread scopes are implemented as extensions to standard C++ in the CUDA Standard C++ library. 2.6. Compute Capability \\uf0c1 The compute capability of a device is represented by a version number, also sometimes called its “SM version”. This version number identifies the features supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU. The compute capability comprises a major revision number X and a minor revision number Y and is denoted by X.Y . Devices with the same major revision number are of the same core architecture. The major revision number is 9 for devices based on the NVIDIA Hopper GPU architecture, 8 for devices based on the NVIDIA Ampere GPU architecture, 7 for devices based on the Volta architecture, 6 for devices based on the Pascal architecture, 5 for devices based on the Maxwell architecture, and 3 for devices based on the Kepler architecture. The minor revision number corresponds to an incremental improvement to the core architecture, possibly including new features. Turing is the architecture for devices of compute capability 7.5, and is an incremental update based on the Volta architecture. CUDA-Enabled GPUs lists of all CUDA-enabled devices along with their compute capability. Compute Capabilities gives the technical specifications of each compute capability. Note The compute capability version of a particular GPU should not be confused with the CUDA version (for example, CUDA 7.5, CUDA 8, CUDA 9), which is the version of the CUDA software platform . The CUDA platform is used by application developers to create applications that run on many generations of GPU architectures, including future GPU architectures yet to be invented. While new versions of the CUDA platform often add native support for a new GPU architecture by supporting the compute capability version of that architecture, new versions of the CUDA platform typically also include software features that are independent of hardware generation. The Tesla and Fermi architectures are no longer supported starting with CUDA 7.0 and CUDA 9.0, respectively. 3. Programming Interface \\uf0c1 CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device. It consists of a minimal set of extensions to the C++ language and a runtime library. The core language extensions have been introduced in Programming Model . They allow programmers to define a kernel as a C++ function and use some new syntax to specify the grid and block dimension each time the function is called. A complete description of all extensions can be found in C++ Language Extensions . Any source file that contains some of these extensions must be compiled with nvcc as outlined in Compilation with NVCC . The runtime is introduced in CUDA Runtime . It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc. A complete description of the runtime can be found in the CUDA reference manual. The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application. The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device. Most applications do not use the driver API as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code. As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed. The driver API is introduced in Driver API and fully described in the reference manual.'},\n",
       " {'id': 201,\n",
       "  'content': '3.1. Compilation with NVCC \\uf0c1 Kernels can be written using the CUDA instruction set architecture, called PTX , which is described in the PTX reference manual. It is however usually more effective to use a high-level programming language such as C++. In both cases, kernels must be compiled into binary code by nvcc to execute on the device. nvcc is a compiler driver that simplifies the process of compiling C++ or PTX code: It provides simple and familiar command line options and executes them by invoking the collection of tools that implement the different compilation stages. This section gives an overview of nvcc workflow and command options. A complete description can be found in the nvcc user manual. 3.1.1. Compilation Workflow \\uf0c1 3.1.1.1. Offline Compilation \\uf0c1 Source files compiled with nvcc can include a mix of host code (i.e., code that executes on the host) and device code (i.e., code that executes on the device). nvcc ’s basic workflow consists in separating device code from host code and then: compiling the device code into an assembly form ( PTX code) and/or binary form ( cubin object), and modifying the host code by replacing the >> syntax introduced in Kernels (and described in more details in Execution Configuration ) by the necessary CUDA runtime function calls to load and launch each compiled kernel from the PTX code and/or cubin object. The modified host code is output either as C++ code that is left to be compiled using another tool or as object code directly by letting nvcc invoke the host compiler during the last compilation stage. Applications can then: Either link to the compiled host code (this is the most common case), Or ignore the modified host code (if any) and use the CUDA driver API (see Driver API ) to load and execute the PTX code or cubin object. 3.1.1.2. Just-in-Time Compilation \\uf0c1 Any PTX code loaded by an application at runtime is compiled further to binary code by the device driver. This is called just-in-time compilation . Just-in-time compilation increases application load time, but allows the application to benefit from any new compiler improvements coming with each new device driver. It is also the only way for applications to run on devices that did not exist at the time the application was compiled, as detailed in Application Compatibility . When the device driver just-in-time compiles some PTX code for some application, it automatically caches a copy of the generated binary code in order to avoid repeating the compilation in subsequent invocations of the application. The cache - referred to as compute cache - is automatically invalidated when the device driver is upgraded, so that applications can benefit from the improvements in the new just-in-time compiler built into the device driver. Environment variables are available to control just-in-time compilation as described in CUDA Environment Variables As an alternative to using nvcc to compile CUDA C++ device code, NVRTC can be used to compile CUDA C++ device code to PTX at runtime. NVRTC is a runtime compilation library for CUDA C++; more information can be found in the NVRTC User guide. 3.1.2.'},\n",
       " {'id': 202,\n",
       "  'content': 'Binary Compatibility \\uf0c1 Binary code is architecture-specific. A cubin object is generated using the compiler option -code that specifies the targeted architecture: For example, compiling with -code=sm_80 produces binary code for devices of compute capability 8.0. Binary compatibility is guaranteed from one minor revision to the next one, but not from one minor revision to the previous one or across major revisions. In other words, a cubin object generated for compute capability X.y will only execute on devices of compute capability X.z where z≥y . Note Binary compatibility is supported only for the desktop. It is not supported for Tegra. Also, the binary compatibility between desktop and Tegra is not supported. 3.1.3. PTX Compatibility \\uf0c1 Some PTX instructions are only supported on devices of higher compute capabilities. For example, Warp Shuffle Functions are only supported on devices of compute capability 5.0 and above. The -arch compiler option specifies the compute capability that is assumed when compiling C++ to PTX code. So, code that contains warp shuffle, for example, must be compiled with -arch=compute_50 (or higher). PTX code produced for some specific compute capability can always be compiled to binary code of greater or equal compute capability. Note that a binary compiled from an earlier PTX version may not make use of some hardware features. For example, a binary targeting devices of compute capability 7.0 (Volta) compiled from PTX generated for compute capability 6.0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal. As a result, the final binary may perform worse than would be possible if the binary were generated using the latest version of PTX. PTX code compiled to target architecture conditional features only run on the exact same physical architecture and nowhere else. Arch conditional PTX code is not forward and backward compatible. Example code compiled with sm_90a or compute_90a only runs on devices with compute capability 9.0 and is not backward or forward compatible. 3.1.4. Application Compatibility \\uf0c1 To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability as described in Binary Compatibility and PTX Compatibility . In particular, to be able to execute code on future architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled for these devices (see Just-in-Time Compilation ). Which PTX and binary code gets embedded in a CUDA C++ application is controlled by the -arch and -code compiler options or the -gencode compiler option as detailed in the nvcc user manual. For example, nvcc x.cu -gencode arch=compute_50,code=sm_50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=\\\\\"compute_70,sm_70\\\\\" embeds binary code compatible with compute capability 5.0 and 6.0 (first and second -gencode options) and PTX and binary code compatible with compute capability 7.0 (third -gencode option). Host code is generated to automatically select at runtime the most appropriate code to load and execute, which, in the above example, will be: 5.0 binary code for devices with compute capability 5.0 and 5.2, 6.0 binary code for devices with compute capability 6.0 and 6.1, 7.0 binary code for devices with compute capability 7.0 and 7.5, PTX code which is compiled to binary code at runtime for devices with compute capability 8.0 and 8.6. x.cu can have an optimized code path that uses warp reduction operations, for example, which are only supported in devices of compute capability 8.0 and higher. The __CUDA_ARCH__ macro can be used to differentiate various code paths based on compute capability. It is only defined for device code. When compiling with -arch=compute_80 for example, __CUDA_ARCH__ is equal to 800 . If x.cu is compiled for architecture conditional features example with sm_90a or compute_90a , the code can only run on devices with compute capability 9.0. Applications using the driver API must compile code to separate files and explicitly load and execute the most appropriate file at runtime. The Volta architecture introduces Independent Thread Scheduling which changes the way threads are scheduled on the GPU. For code relying on specific behavior of SIMT scheduling in previous architectures, Independent Thread Scheduling may alter the set of participating threads, leading to incorrect results. To aid migration while implementing the corrective actions detailed in Independent Thread Scheduling , Volta developers can opt-in to Pascal’s thread scheduling with the compiler option combination -arch=compute_60 -code=sm_70 . The nvcc user manual lists various shorthands for the -arch , -code , and -gencode compiler options. For example, -arch=sm_70 is a shorthand for -arch=compute_70 -code=compute_70,sm_70 (which is the same as -gencode arch=compute_70,code=\\\\\"compute_70,sm_70\\\\\" ). 3.1.5. C++ Compatibility \\uf0c1 The front end of the compiler processes CUDA source files according to C++ syntax rules. Full C++ is supported for the host code. However, only a subset of C++ is fully supported for the device code as described in C++ Language Support . 3.1.6. 64-Bit Compatibility \\uf0c1 The 64-bit version of nvcc compiles device code in 64-bit mode (i.e., pointers are 64-bit). Device code compiled in 64-bit mode is only supported with host code compiled in 64-bit mode. 3.2. CUDA Runtime \\uf0c1 The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart.lib or libcudart.a , or dynamically via cudart.dll or libcudart.so . Applications that require cudart.dll and/or cudart.so for dynamic linking typically include them as part of the application installation package. It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime. All its entry points are prefixed with cuda . As mentioned in Heterogeneous Programming , the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory. Device Memory gives an overview of the runtime functions used to manage device memory. Shared Memory illustrates the use of shared memory, introduced in Thread Hierarchy , to maximize performance. Page-Locked Host Memory introduces page-locked host memory that is required to overlap kernel execution with data transfers between host and device memory. Asynchronous Concurrent Execution describes the concepts and API used to enable asynchronous concurrent execution at various levels in the system. Multi-Device System shows how the programming model extends to a system with multiple devices attached to the same host. Error Checking describes how to properly check the errors generated by the runtime. Call Stack mentions the runtime functions used to manage the CUDA C++ call stack. Texture and Surface Memory presents the texture and surface memory spaces that provide another way to access device memory; they also expose a subset of the GPU texturing hardware. Graphics Interoperability introduces the various functions the runtime provides to interoperate with the two main graphics APIs, OpenGL and Direct3D. 3.2.1. Initialization \\uf0c1 As of CUDA 12.0, the cudaInitDevice() and cudaSetDevice() calls initialize the runtime and the primary context associated with the specified device. Absent these calls, the runtime will implicitly use device 0 and self-initialize as needed to process other runtime API requests. One needs to keep this in mind when timing runtime function calls and when interpreting the error code from the first call into the runtime. Before 12.0, cudaSetDevice() would not initialize the runtime and applications would often use the no-op runtime call cudaFree(0) to isolate the runtime initialization from other api activity (both for the sake of timing and error handling). The runtime creates a CUDA context for each device in the system (see Context for more details on CUDA contexts). This context is the primary context for this device and is initialized at the first runtime function which requires an active context on this device. It is shared among all the host threads of the application. As part of this context creation, the device code is just-in-time compiled if necessary (see Just-in-Time Compilation ) and loaded into device memory. This all happens transparently. If needed, for example, for driver API interoperability, the primary context of a device can be accessed from the driver API as described in Interoperability between Runtime and Driver APIs . When a host thread calls cudaDeviceReset() , this destroys the primary context of the device the host thread currently operates on (i.e., the current device as defined in Device Selection ). The next runtime function call made by any host thread that has this device as current will create a new primary context for this device. Note The CUDA interfaces use global state that is initialized during host program initiation and destroyed during host program termination. The CUDA runtime and driver cannot detect if this state is invalid, so using any of these interfaces (implicitly or explicitly) during program initiation or termination after main) will result in undefined behavior. As of CUDA 12.0, cudaSetDevice() will now explicitly initialize the runtime after changing the current device for the host thread. Previous versions of CUDA delayed runtime initialization on the new device until the first runtime call was made after cudaSetDevice() . This change means that it is now very important to check the return value of cudaSetDevice() for initialization errors. The runtime functions from the error handling and version management sections of the reference manual do not initialize the runtime. 3.2.2. Device Memory \\uf0c1 As mentioned in Heterogeneous Programming , the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory. Kernels operate out of device memory, so the runtime provides functions to allocate, deallocate, and copy device memory, as well as transfer data between host memory and device memory. Device memory can be allocated either as linear memory or as CUDA arrays . CUDA arrays are opaque memory layouts optimized for texture fetching. They are described in Texture and Surface Memory . Linear memory is allocated in a single unified address space, which means that separately allocated entities can reference one another via pointers, for example, in a binary tree or linked list. The size of the address space depends on the host system (CPU) and the compute capability of the used GPU: Table 1 Linear Memory Address Space \\uf0c1 x86_64 (AMD64) POWER (ppc64le) ARM64 up to compute capability 5.3 (Maxwell) 40bit 40bit 40bit compute capability 6.0 (Pascal) or newer up to 47bit up to 49bit up to 48bit Note On devices of compute capability 5.3 (Maxwell) and earlier, the CUDA driver creates an uncommitted 40bit virtual address reservation to ensure that memory allocations (pointers) fall into the supported range. This reservation appears as reserved virtual memory, but does not occupy any physical memory until the program actually allocates memory. Linear memory is typically allocated using cudaMalloc() and freed using cudaFree() and data transfer between host memory and device memory are typically done using cudaMemcpy() . In the vector addition code sample of Kernels , the vectors need to be copied from host memory to device memory: // Device code __global__ void VecAdd ( float * A , float * B , float * C , int N ) { int i = blockDim . x * blockIdx . x ; if ( i >> ( d_A , d_B , d_C , N ); // Copy result from device memory to host memory // h_C contains the result in host memory cudaMemcpy ( h_C , d_C , size , cudaMemcpyDeviceToHost ); // Free device memory cudaFree ( d_A ); cudaFree ( d_B ); cudaFree ( d_C ); // Free host memory ... } Linear memory can also be allocated through cudaMallocPitch() and cudaMalloc3D() . These functions are recommended for allocations of 2D or 3D arrays as it makes sure that the allocation is appropriately padded to meet the alignment requirements described in Device Memory Accesses , therefore ensuring best performance when accessing the row addresses or performing copies between 2D arrays and other regions of device memory (using the cudaMemcpy2D() and cudaMemcpy3D() functions). The returned pitch (or stride) must be used to access array elements. The following code sample allocates a width x height 2D array of floating-point values and shows how to loop over the array elements in device code: // Host code int width = 64 , height = 64 ; float * devPtr ; size_t pitch ; cudaMallocPitch ( & devPtr , & pitch , width * sizeof ( float ), height ); MyKernel >> ( devPtr , pitch , width , height ); // Device code __global__ void MyKernel ( float * devPtr , size_t pitch , int width , int height ) { for ( int r = 0 ; r >> ( devPitchedPtr , width , height , depth ); // Device code __global__ void MyKernel ( cudaPitchedPtr devPitchedPtr , int width , int height , int depth ) { char * devPtr = devPitchedPtr . ptr ; size_t pitch = devPitchedPtr . pitch ; size_t slicePitch = pitch * height ; for ( int z = 0 ; z ( ptr ); // Global Memory data pointer stream_attribute . accessPolicyWindow . num_bytes = num_bytes ; // Number of bytes for persistence access. // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize) stream_attribute . hitRatio = 0.6 ; // Hint for cache hit ratio stream_attribute . hitProp = cudaAccessPropertyPersisting ; // Type of access property on cache hit stream_attribute . missProp = cudaAccessPropertyStreaming ; // Type of access property on cache miss. //Set the attributes to a CUDA stream of type cudaStream_t cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); When a kernel subsequently executes in CUDA stream , memory accesses within the global memory extent [ptr..ptr+num_bytes) are more likely to persist in the L2 cache than accesses to other global memory locations. L2 persistence can also be set for a CUDA Graph Kernel Node as shown in the example below: CUDA GraphKernelNode Example cudaKernelNodeAttrValue node_attribute ; // Kernel level attributes data structure node_attribute . base_ptr = reinterpret_cast ( ptr ); // Global Memory data pointer node_attribute . // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize) node_attribute . hitRatio = 0.6 ; // Hint for cache hit ratio node_attribute . hitProp = cudaAccessPropertyPersisting ; // Type of access property on cache hit node_attribute . //Set the attributes to a CUDA Graph Kernel node of type cudaGraphNode_t cudaGraphKernelNodeSetAttribute ( node , cudaKernelNodeAttributeAccessPolicyWindow , & node_attribute ); The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property. In both of the examples above, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property. Which specific memory accesses are classified as persisting (the hitProp ) is random with a probability of approximately hitRatio ; the probability distribution depends upon the hardware architecture and the memory extent. For example, if the L2 set-aside cache size is 16KB and the num_bytes in the accessPolicyWindow is 32KB: With a hitRatio of 0.5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area. With a hitRatio of 1.0, the hardware will attempt to cache the whole 32KB window in the set-aside L2 cache area. Since the set-aside area is smaller than the window, cache lines will be evicted to keep the most recently used 16KB of the 32KB data in the set-aside portion of the L2 cache. The hitRatio can therefore be used to avoid thrashing of cache lines and overall reduce the amount of data moved into and out of the L2 cache. A hitRatio value below 1.0 can be used to manually control the amount of data different accessPolicyWindow s from concurrent CUDA streams can cache in L2. For example, let the L2 set-aside cache size be 16KB; two concurrent kernels in two different CUDA streams, each with a 16KB accessPolicyWindow , and both with hitRatio value 1.0, might evict each others’ cache lines when competing for the shared L2 resource. However, if both accessPolicyWindows have a hitRatio value of 0.5, they will be less likely to evict their own or each others’ persisting cache lines. 3.2.3.3. L2 Access Properties \\uf0c1 Three types of access properties are defined for different global memory data accesses: cudaAccessPropertyStreaming : Memory accesses that occur with the streaming property are less likely to persist in the L2 cache because these accesses are preferentially evicted. cudaAccessPropertyPersisting : Memory accesses that occur with the persisting property are more likely to persist in the L2 cache because these accesses are preferentially retained in the set-aside portion of L2 cache. cudaAccessPropertyNormal : This access property forcibly resets previously applied persisting access property to a normal status. Memory accesses with the persisting property from previous CUDA kernels may be retained in L2 cache long after their intended use. This persistence-after-use reduces the amount of L2 cache available to subsequent kernels that do not use the persisting property. Resetting an access property window with the cudaAccessPropertyNormal property removes the persisting (preferential retention) status of the prior access, as if the prior access had been without an access property. 3.2.3.4. L2 Persistence Example \\uf0c1 The following example shows how to set-aside L2 cache for persistent accesses, use the set-aside L2 cache in CUDA kernels via CUDA Stream and then reset the L2 cache. cudaStream_t stream ; cudaStreamCreate ( & stream ); // Create CUDA stream cudaDeviceProp prop ; // CUDA device properties variable cudaGetDeviceProperties ( & prop , device_id ); // Query GPU properties size_t size = min ( int ( prop . l2CacheSize * 0.75 ) , prop . persistingL2CacheMaxSize ); cudaDeviceSetLimit ( cudaLimitPersistingL2CacheSize , size ); // set-aside 3/4 of L2 cache for persisting accesses or the max allowed size_t window_size = min ( prop . accessPolicyMaxWindowSize , num_bytes ); // Select minimum of user defined num_bytes and max window size. cudaStreamAttrValue stream_attribute ; // Stream level attributes data structure stream_attribute . base_ptr = reinterpret_cast ( data1 ); // Global Memory data pointer stream_attribute . num_bytes = window_size ; // Number of bytes for persistence access stream_attribute . hitProp = cudaAccessPropertyPersisting ; // Persistence Property stream_attribute . missProp = cudaAccessPropertyStreaming ; // Type of access property on cache miss cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); // Set the attributes to a CUDA Stream for ( int i = 0 ; i >> ( data1 ); // This data1 is used by a kernel multiple times } // [data1 + num_bytes) benefits from L2 persistence cuda_kernelB >> ( data1 ); // A different kernel in the same stream can also benefit // from the persistence of data1 stream_attribute . num_bytes = 0 ; // Setting the window size to 0 disable it cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); // Overwrite the access policy attribute to a CUDA Stream cudaCtxResetPersistingL2Cache (); // Remove any persistent lines in L2 cuda_kernelC >> ( data2 ); // data2 can now benefit from full L2 in normal mode 3.2.3.5. Reset L2 Access to Normal \\uf0c1 A persisting L2 cache line from a previous CUDA kernel may persist in L2 long after it has been used. Hence, a reset to normal for L2 cache is important for streaming or normal memory accesses to utilize the L2 cache with normal priority. There are three ways a persisting access can be reset to normal status. Reset a previous persisting memory region with the access property, cudaAccessPropertyNormal . Reset all persisting L2 cache lines to normal by calling cudaCtxResetPersistingL2Cache() . Eventually untouched lines are automatically reset to normal.'},\n",
       " {'id': 203,\n",
       "  'content': 'Reliance on automatic reset is strongly discouraged because of the undetermined length of time required for automatic reset to occur.'},\n",
       " {'id': 204,\n",
       "  'content': '3.2.3.6. Manage Utilization of L2 set-aside cache \\uf0c1 Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams. However, the L2 set-aside cache portion is shared among all these concurrent CUDA kernels. As a result, the net utilization of this set-aside cache portion is the sum of all the concurrent kernels’ individual use. The benefits of designating memory accesses as persisting diminish as the volume of persisting accesses exceeds the set-aside L2 cache capacity. To manage utilization of the set-aside L2 cache portion, an application must consider the following: Size of L2 set-aside cache. CUDA kernels that may concurrently execute. The access policy window for all the CUDA kernels that may concurrently execute. When and how L2 reset is required to allow normal or streaming accesses to utilize the previously set-aside L2 cache with equal priority. 3.2.3.7. Query L2 cache Properties \\uf0c1 Properties related to L2 cache are a part of cudaDeviceProp struct and can be queried using CUDA runtime API cudaGetDeviceProperties CUDA Device Properties include: l2CacheSize : The amount of available L2 cache on the GPU. persistingL2CacheMaxSize : The maximum amount of L2 cache that can be set-aside for persisting memory accesses. accessPolicyMaxWindowSize : The maximum size of the access policy window. 3.2.3.8. Control L2 Cache Set-Aside Size for Persisting Memory Access \\uf0c1 The L2 set-aside cache size for persisting memory accesses is queried using CUDA runtime API cudaDeviceGetLimit and set using CUDA runtime API cudaDeviceSetLimit as a cudaLimit . The maximum value for setting this limit is cudaDeviceProp::persistingL2CacheMaxSize . enum cudaLimit { /* other fields not shown */ cudaLimitPersistingL2CacheSize }; 3.2.4. Shared Memory \\uf0c1 As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier. Shared memory is expected to be much faster than global memory as mentioned in Thread Hierarchy and detailed in Shared Memory . It can be used as scratchpad memory (or software managed cache) to minimize global memory accesses from a CUDA block as illustrated by the following matrix multiplication example. The following code sample is a straightforward implementation of matrix multiplication that does not take advantage of shared memory. Each thread reads one row of A and one column of B and computes the corresponding element of C as illustrated in Figure 8 . A is therefore read B.width times from global memory and B is read A.height times. // Matrices are stored in row-major order: // M(row, col) = *(M.elements + row * M.width + col) typedef struct { int width ; int height ; float * elements ; } Matrix ; // Thread block size #define BLOCK_SIZE 16 // Forward declaration of the matrix multiplication kernel __global__ void MatMulKernel ( const Matrix , const Matrix , Matrix ); // Matrix multiplication - Host code // Matrix dimensions are assumed to be multiples of BLOCK_SIZE void MatMul ( const Matrix A , const Matrix B , Matrix C ) { // Load A and B to device memory Matrix d_A ; d_A . width = A .'},\n",
       " {'id': 205,\n",
       "  'content': 'width ; d_A . height = A . height ; size_t size = A . width * A . height * sizeof ( float ); cudaMalloc ( & d_A . elements , size ); cudaMemcpy ( d_A . elements , A . elements , size , cudaMemcpyHostToDevice ); Matrix d_B ; d_B . width = B . width ; d_B . height = B . height ; size = B . width * B . height * sizeof ( float ); cudaMalloc ( & d_B . elements , size ); cudaMemcpy ( d_B . elements , B . elements , size , cudaMemcpyHostToDevice ); // Allocate C in device memory Matrix d_C ; d_C . width = C .'},\n",
       " {'id': 206,\n",
       "  'content': 'width ; d_C . height = C . height ; size = C . width * C . height * sizeof ( float ); cudaMalloc ( & d_C . elements , size ); // Invoke kernel dim3 dimBlock ( BLOCK_SIZE , BLOCK_SIZE ); dim3 dimGrid ( B . width / dimBlock . x , A .'},\n",
       " {'id': 207,\n",
       "  'content': 'height / dimBlock . y ); MatMulKernel >> ( d_A , d_B , d_C ); // Read C from device memory cudaMemcpy ( C . elements , d_C . elements , size , cudaMemcpyDeviceToHost ); // Free device memory cudaFree ( d_A . elements ); cudaFree ( d_B . elements ); cudaFree ( d_C . elements ); } // Matrix multiplication kernel called by MatMul() __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { // Each thread computes one element of C // by accumulating results into Cvalue float Cvalue = 0 ; int row = blockIdx . y ; int col = blockIdx . x ; for ( int e = 0 ; e >> ( d_A , d_B , d_C ); // Read C from device memory cudaMemcpy ( C . elements ); } // Matrix multiplication kernel called by MatMul() __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); // Each thread computes one element of Csub // by accumulating results into Cvalue float Cvalue = 0 ; // Thread row and column within Csub int row = threadIdx . y ; int col = threadIdx . x ; // Loop over all the sub-matrices of A and B that are // required to compute Csub // Multiply each pair of sub-matrices together // and accumulate the results for ( int m = 0 ; m // Distributed Shared memory histogram kernel __global__ void clusterHist_kernel ( int * bins , const int nbins , const int bins_per_block , const int * __restrict__ input , size_t array_size ) { extern __shared__ int smem []; namespace cg = cooperative_groups ; int tid = cg :: this_grid (). thread_rank (); // Cluster initialization, size and calculating local bin offsets. cg :: cluster_group cluster = cg :: this_cluster (); unsigned int clusterBlockRank = cluster . block_rank (); int cluster_size = cluster . dim_blocks (). x ; for ( int i = threadIdx . x ; i = nbins ) binid = nbins - 1 ; //Find destination block rank and offset for computing //distributed shared memory histogram int dst_block_rank = ( int )( binid / bins_per_block ); int dst_offset = binid % bins_per_block ; //Pointer to target block shared memory int * dst_smem = cluster . map_shared_rank ( smem , dst_block_rank ); //Perform atomic update of the histogram bin atomicAdd ( dst_smem + dst_offset , 1 ); } // cluster synchronization is required to ensure all distributed shared // memory operations are completed and no thread block exits while // other thread blocks are still accessing distributed shared memory cluster . sync (); // Perform global memory histogram, using the local distributed memory histogram int * lbins = bins + cluster . block_rank () * bins_per_block ; for ( int i = threadIdx . x ; i a ( 0 ); __managed__ cuda :: atomic b ( 0 ); Thread 1 (SM) x = 1 ; a = 1 ; Thread 2 (SM) while ( a != 1 ) ; assert ( x == 1 ); b = 1 ; Thread 3 (CPU) while ( b != 1 ) ; assert ( x == 1 ); Consider the example above. The CUDA memory consistency model guarantees that the asserted condition will be true, so the write to x from thread 1 must be visible to thread 3, before the write to b from thread 2. The memory ordering provided by the release and acquire of a is only sufficient to make x visible to thread 2, not thread 3, as it is a device-scope operation. The system-scope ordering provided by release and acquire of b , therefore, needs to ensure not only writes issued from thread 2 itself are visible to thread 3, but also writes from other threads that are visible to thread 2. This is known as cumulativity. As the GPU cannot know at the time of execution which writes have been guaranteed at the source level to be visible and which are visible only by chance timing, it must cast a conservatively wide net for in-flight memory operations. This sometimes leads to interference: because the GPU is waiting on memory operations it is not required to at the source level, the fence/flush may take longer than necessary. Note that fences may occur explicitly as intrinsics or atomics in code, like in the example, or implicitly to implement synchronizes-with relationships at task boundaries. A common example is when a kernel is performing computation in local GPU memory, and a parallel kernel (e.g. from NCCL) is performing communications with a peer. Upon completion, the local kernel will implicitly flush its writes to satisfy any synchronizes-with relationships to downstream work. This may unnecessarily wait, fully or partially, on slower nvlink or PCIe writes from the communication kernel. 3.2.7.2. Isolating Traffic with Domains \\uf0c1 Beginning with Hopper architecture GPUs and CUDA 12.0, the memory synchronization domains feature provides a way to alleviate such interference. In exchange for explicit assistance from code, the GPU can reduce the net cast by a fence operation. Each kernel launch is given a domain ID. Writes and fences are tagged with the ID, and a fence will only order writes matching the fence’s domain. In the concurrent compute vs communication example, the communication kernels can be placed in a different domain. When using domains, code must abide by the rule that ordering or synchronization between distinct domains on the same GPU requires system-scope fencing . Within a domain, device-scope fencing remains sufficient. This is necessary for cumulativity as one kernel’s writes will not be encompassed by a fence issued from a kernel in another domain. In essence, cumulativity is satisfied by ensuring that cross-domain traffic is flushed to the system scope ahead of time. Note that this modifies the definition of thread_scope_device . However, because kernels will default to domain 0 as described below, backward compatibility is maintained.'},\n",
       " {'id': 208,\n",
       "  'content': '3.2.7.3. Using Domains in CUDA \\uf0c1 Domains are accessible via the new launch attributes cudaLaunchAttributeMemSyncDomain and cudaLaunchAttributeMemSyncDomainMap . The former selects between logical domains cudaLaunchMemSyncDomainDefault and cudaLaunchMemSyncDomainRemote , and the latter provides a mapping from logical to physical domains. The remote domain is intended for kernels performing remote memory access in order to isolate their memory traffic from local kernels. Note, however, the selection of a particular domain does not affect what memory access a kernel may legally perform. The domain count can be queried via device attribute cudaDevAttrMemSyncDomainCount . Hopper has 4 domains. To facilitate portable code, domains functionality can be used on all devices and CUDA will report a count of 1 prior to Hopper. Having logical domains eases application composition. An individual kernel launch at a low level in the stack, such as from NCCL, can select a semantic logical domain without concern for the surrounding application architecture. Higher levels can steer logical domains using the mapping. The default value for the logical domain if it is not set is the default domain, and the default mapping is to map the default domain to 0 and the remote domain to 1 (on GPUs with more than 1 domain). Specific libraries may tag launches with the remote domain in CUDA 12.0 and later; for example, NCCL 2.16 will do so. Together, this provides a beneficial use pattern for common applications out of the box, with no code changes needed in other components, frameworks, or at application level.'},\n",
       " {'id': 209,\n",
       "  'content': 'An alternative use pattern, for example in an application using nvshmem or with no clear separation of kernel types, could be to partition parallel streams. Stream A may map both logical domains to physical domain 0, stream B to 1, and so on. // Example of launching a kernel with the remote logical domain cudaLaunchAttribute domainAttr ; domainAttr . id = cudaLaunchAttrMemSyncDomain ; domainAttr . val = cudaLaunchMemSyncDomainRemote ; cudaLaunchConfig_t config ; // Fill out other config fields config . attrs = & domainAttr ; config . numAttrs = 1 ; cudaLaunchKernelEx ( & config , myKernel , kernelArg1 , kernelArg2 ...); // Example of setting a mapping for a stream // (This mapping is the default for streams starting on Hopper if not // explicitly set, and provided for illustration) cudaLaunchAttributeValue mapAttr ; mapAttr . memSyncDomainMap . default_ = 0 ; mapAttr . remote = 1 ; cudaStreamSetAttribute ( stream , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); // Example of mapping different streams to different physical domains, ignoring // logical domain settings cudaLaunchAttributeValue mapAttr ; mapAttr . remote = 0 ; cudaStreamSetAttribute ( streamA , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); mapAttr . default_ = 1 ; mapAttr . remote = 1 ; cudaStreamSetAttribute ( streamB , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); As with other launch attributes, these are exposed uniformly on CUDA streams, individual launches using cudaLaunchKernelEx , and kernel nodes in CUDA graphs. A typical use would set the mapping at stream level and the logical domain at launch level (or bracketing a section of stream use) as described above. Both attributes are copied to graph nodes during stream capture. Graphs take both attributes from the node itself, essentially an indirect way of specifying a physical domain. Domain-related attributes set on the stream a graph is launched into are not used in execution of the graph. 3.2.8. Asynchronous Concurrent Execution \\uf0c1 CUDA exposes the following operations as independent tasks that can operate concurrently with one another: Computation on the host; Computation on the device; Memory transfers from the host to the device; Memory transfers from the device to the host; Memory transfers within the memory of a given device; Memory transfers among devices. The level of concurrency achieved between these operations will depend on the feature set and compute capability of the device as described below. 3.2.8.1. Concurrent Execution between Host and Device \\uf0c1 Concurrent host execution is facilitated through asynchronous library functions that return control to the host thread before the device completes the requested task. Using asynchronous calls, many device operations can be queued up together to be executed by the CUDA driver when appropriate device resources are available. This relieves the host thread of much of the responsibility to manage the device, leaving it free for other tasks. The following device operations are asynchronous with respect to the host: Kernel launches; Memory copies within a single device’s memory; Memory copies from host to device of a memory block of 64 KB or less; Memory copies performed by functions that are suffixed with Async ; Memory set function calls. Programmers can globally disable asynchronicity of kernel launches for all CUDA applications running on a system by setting the CUDA_LAUNCH_BLOCKING environment variable to 1. This feature is provided for debugging purposes only and should not be used as a way to make production software run reliably. Kernel launches are synchronous if hardware counters are collected via a profiler (Nsight, Visual Profiler) unless concurrent kernel profiling is enabled. Async memory copies might also be synchronous if they involve host memory that is not page-locked. 3.2.8.2. Concurrent Kernel Execution \\uf0c1 Some devices of compute capability 2.x and higher can execute multiple kernels concurrently. Applications may query this capability by checking the concurrentKernels device property (see Device Enumeration ), which is equal to 1 for devices that support it. The maximum number of kernel launches that a device can execute concurrently depends on its compute capability and is listed in Table 21 . A kernel from one CUDA context cannot execute concurrently with a kernel from another CUDA context. The GPU may time slice to provide forward progress to each context. If a user wants to run kernels from multiple process simultaneously on the SM, one must enable MPS. Kernels that use many textures or a large amount of local memory are less likely to execute concurrently with other kernels. 3.2.8.3. Overlap of Data Transfer and Kernel Execution \\uf0c1 Some devices can perform an asynchronous memory copy to or from the GPU concurrently with kernel execution. Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration ), which is greater than zero for devices that support it. If host memory is involved in the copy, it must be page-locked. It is also possible to perform an intra-device copy simultaneously with kernel execution (on devices that support the concurrentKernels device property) and/or with copies to or from the device (for devices that support the asyncEngineCount property). Intra-device copies are initiated using the standard memory copy functions with destination and source addresses residing on the same device. 3.2.8.4. Concurrent Data Transfers \\uf0c1 Some devices of compute capability 2.x and higher can overlap copies to and from the device. Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration ), which is equal to 2 for devices that support it. In order to be overlapped, any host memory involved in the transfers must be page-locked. 3.2.8.5. Streams \\uf0c1 Applications manage the concurrent operations described above through streams . A stream is a sequence of commands (possibly issued by different host threads) that execute in order. Different streams, on the other hand, may execute their commands out of order with respect to one another or concurrently; this behavior is not guaranteed and should therefore not be relied upon for correctness (for example, inter-kernel communication is undefined). The commands issued on a stream may execute when all the dependencies of the command are met. The dependencies could be previously launched commands on same stream or dependencies from other streams. The successful completion of synchronize call guarantees that all the commands launched are completed. 3.2.8.5.1. Creation and Destruction of Streams \\uf0c1 A stream is defined by creating a stream object and specifying it as the stream parameter to a sequence of kernel launches and host device memory copies. The following code sample creates two streams and allocates an array hostPtr of float in page-locked memory. cudaStream_t stream [ 2 ]; for ( int i = 0 ; i >> ( outputDevPtr + i * size , inputDevPtr + i * size , size ); cudaMemcpyAsync ( hostPtr + i * size , outputDevPtr + i * size , size , cudaMemcpyDeviceToHost , stream [ i ]); } Each stream copies its portion of input array hostPtr to array inputDevPtr in device memory, processes inputDevPtr on the device by calling MyKernel() , and copies the result outputDevPtr back to the same portion of hostPtr . Overlapping Behavior describes how the streams overlap in this example depending on the capability of the device. Note that hostPtr must point to page-locked host memory for any overlap to occur. Streams are released by calling cudaStreamDestroy() . for ( int i = 0 ; i device memory copies that do not specify any stream parameter, or equivalently that set the stream parameter to zero, are issued to the default stream. They are therefore executed in order. For code that is compiled using the --default-stream per-thread compilation flag (or that defines the CUDA_API_PER_THREAD_DEFAULT_STREAM macro before including CUDA headers ( cuda.h and cuda_runtime.h )), the default stream is a regular stream and each host thread has its own default stream. Note #define CUDA_API_PER_THREAD_DEFAULT_STREAM 1 cannot be used to enable this behavior when the code is compiled by nvcc as nvcc implicitly includes cuda_runtime.h at the top of the translation unit. In this case the --default-stream per-thread compilation flag needs to be used or the CUDA_API_PER_THREAD_DEFAULT_STREAM macro needs to be defined with the -DCUDA_API_PER_THREAD_DEFAULT_STREAM=1 compiler flag. For code that is compiled using the --default-stream legacy compilation flag, the default stream is a special stream called the NULL stream and each device has a single NULL stream used for all host threads.'},\n",
       " {'id': 210,\n",
       "  'content': 'The NULL stream is special as it causes implicit synchronization as described in Implicit Synchronization . For code that is compiled without specifying a --default-stream compilation flag, --default-stream legacy is assumed as the default. 3.2.8.5.3.'},\n",
       " {'id': 211,\n",
       "  'content': 'Explicit Synchronization \\uf0c1 There are various ways to explicitly synchronize streams with each other. cudaDeviceSynchronize() waits until all preceding commands in all streams of all host threads have completed. cudaStreamSynchronize() takes a stream as a parameter and waits until all preceding commands in the given stream have completed. It can be used to synchronize the host with a specific stream, allowing other streams to continue executing on the device. cudaStreamWaitEvent() takes a stream and an event as parameters (see Events for a description of events)and makes all the commands added to the given stream after the call to cudaStreamWaitEvent() delay their execution until the given event has completed. cudaStreamQuery() provides applications with a way to know if all preceding commands in a stream have completed. 3.2.8.5.4. Implicit Synchronization \\uf0c1 Two commands from different streams cannot run concurrently if any one of the following operations is issued in-between them by the host thread: a page-locked host memory allocation, a device memory allocation, a device memory set, a memory copy between two addresses to the same device memory, any CUDA command to the NULL stream, a switch between the L1/shared memory configurations described in Compute Capability 7.x . Operations that require a dependency check include any other commands within the same stream as the launch being checked and any call to cudaStreamQuery() on that stream. Therefore, applications should follow these guidelines to improve their potential for concurrent kernel execution: All independent operations should be issued before dependent operations, Synchronization of any kind should be delayed as long as possible. 3.2.8.5.5. Overlapping Behavior \\uf0c1 The amount of execution overlap between two streams depends on the order in which the commands are issued to each stream and whether or not the device supports overlap of data transfer and kernel execution (see Overlap of Data Transfer and Kernel Execution ), concurrent kernel execution (see Concurrent Kernel Execution ), and/or concurrent data transfers (see Concurrent Data Transfers ). For example, on devices that do not support concurrent data transfers, the two streams of the code sample of Creation and Destruction do not overlap at all because the memory copy from host to device is issued to stream[1] after the memory copy from device to host is issued to stream[0], so it can only start once the memory copy from device to host issued to stream[0] has completed. If the code is rewritten the following way (and assuming the device supports overlap of data transfer and kernel execution) for ( int i = 0 ; i >> ( outputDevPtr + i * size , inputDevPtr + i * size , size ); for ( int i = 0 ; i >> ( devPtrOut [ i ], devPtrIn [ i ], size ); cudaMemcpyAsync ( hostPtr [ i ], devPtrOut [ i ], size , cudaMemcpyDeviceToHost , stream [ i ]); cudaLaunchHostFunc ( stream [ i ], MyCallback , ( void * ) i ); } The commands that are issued in a stream after a host function do not start executing before the function has completed. A host function enqueued into a stream must not make CUDA API calls (directly or indirectly), as it might end up waiting on itself if it makes such a call leading to a deadlock. 3.2.8.5.7. Stream Priorities \\uf0c1 The relative priorities of streams can be specified at creation using cudaStreamCreateWithPriority() . The range of allowable priorities, ordered as [ highest priority, lowest priority ] can be obtained using the cudaDeviceGetStreamPriorityRange() function. At runtime, pending work in higher-priority streams takes preference over pending work in low-priority streams. The following code sample obtains the allowable range of priorities for the current device, and creates streams with the highest and lowest available priorities. // get the range of stream priorities for this device int priority_high , priority_low ; cudaDeviceGetStreamPriorityRange ( & priority_low , & priority_high ); // create streams with highest and lowest available priorities cudaStream_t st_high , st_low ; cudaStreamCreateWithPriority ( & st_high , cudaStreamNonBlocking , priority_high ); cudaStreamCreateWithPriority ( & st_low , cudaStreamNonBlocking , priority_low ); 3.2.8.6. Programmatic Dependent Launch and Synchronization \\uf0c1 The Programmatic Dependent Launch mechanism allows for a dependent secondary kernel to launch before the primary kernel it depends on in the same CUDA stream has finished executing. Available starting with devices of compute capability 9.0, this technique can provide performance benefits when the secondary kernel can complete significant work that does not depend on the results of the primary kernel. 3.2.8.6.1. Background \\uf0c1 A CUDA application utilizes the GPU by launching and executing multiple kernels on it. A typical GPU activity timeline is shown in Figure 10 . Figure 10 GPU activity timeline \\uf0c1 Here, secondary_kernel is launched after primary_kernel finishes its execution. Serialized execution is usually necessary because secondary_kernel depends on result data produced by primary_kernel . If secondary_kernel has no dependency on primary_kernel , both of them can be launched concurrently by using CUDA streams . Even if secondary_kernel is dependent on primary_kernel , there is some potential for concurrent execution. For example, almost all the kernels have some sort of preamble section during which tasks such as zeroing buffers or loading constant values are performed. Figure 11 Preamble section of secondary_kernel \\uf0c1 Figure 11 demonstrates the portion of secondary_kernel that could be executed concurrently without impacting the application. Note that concurrent launch also allows us to hide the launch latency of secondary_kernel behind the execution of primary_kernel . Figure 12 Concurrent execution of primary_kernel and secondary_kernel \\uf0c1 The concurrent launch and execution of secondary_kernel shown in Figure 12 is achievable using Programmatic Dependent Launch . Programmatic Dependent Launch introduces changes to the CUDA kernel launch APIs as explained in following section. These APIs require at least compute capability 9.0 to provide overlapping execution. 3.2.8.6.2. API Description \\uf0c1 In Programmatic Dependent Launch, a primary and a secondary kernel are launched in the same CUDA stream. The primary kernel should execute cudaTriggerProgrammaticLaunchCompletion with all thread blocks when it’s ready for the secondary kernel to launch. The secondary kernel must be launched using the extensible launch API as shown. __global__ void primary_kernel () { // Initial work that should finish before starting secondary kernel // Trigger the secondary kernel cudaTriggerProgrammaticLaunchCompletion (); // Work that can coincide with the secondary kernel } __global__ void secondary_kernel () { // Independent work // Will block until all primary kernels the secondary kernel is dependent on have completed and flushed results to global memory cudaGridDependencySynchronize (); // Dependent work } cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ]. id = cudaLaunchAttributeProgrammaticStreamSerialization ; attribute [ 0 ].'},\n",
       " {'id': 212,\n",
       "  'content': 'programmaticStreamSerializationAllowed = 1 ; configSecondary . attrs = attribute ; configSecondary . numAttrs = 1 ; primary_kernel >> (); cudaLaunchKernelEx ( & configSecondary , secondary_kernel ); When the secondary kernel is launched using the cudaLaunchAttributeProgrammaticStreamSerialization attribute, the CUDA driver is safe to launch the secondary kernel early and not wait on the completion and memory flush of the primary before launching the secondary. The CUDA driver can launch the secondary kernel when all primary thread blocks have launched and executed cudaTriggerProgrammaticLaunchCompletion . If the primary kernel doesn’t execute the trigger, it implicitly occurs after all thread blocks in the primary kernel exit. In either case, the secondary thread blocks might launch before data written by the primary kernel is visible. As such, when the secondary kernel is configured with Programmatic Dependent Launch , it must always use cudaGridDependencySynchronize or other means to verify that the result data from the primary is available. Please note that these methods provide the opportunity for the primary and secondary kernels to execute concurrently, however this behavior is opportunistic and not guaranteed to lead to concurrent kernel execution. Reliance on concurrent execution in this manner is unsafe and can lead to deadlock.'},\n",
       " {'id': 213,\n",
       "  'content': \"3.2.8.6.3. Use in CUDA Graphs \\uf0c1 Programmatic Dependent Launch can be used in CUDA Graphs via stream capture or directly via edge data . To program this feature in a CUDA Graph with edge data, use a cudaGraphDependencyType value of cudaGraphDependencyTypeProgrammatic on an edge connecting two kernel nodes. This edge type makes the upstream kernel visible to a cudaGridDependencySynchronize() in the downstream kernel. This type must be used with an outgoing port of either cudaGraphKernelNodePortLaunchCompletion or cudaGraphKernelNodePortProgrammatic . The resulting graph equivalents for stream capture are as follows: Stream code (abbreviated) Resulting graph edge cudaLaunchAttribute attribute ; attribute . id = cudaLaunchAttributeProgrammaticStreamSerialization ; attribute . programmaticStreamSerializationAllowed = 1 ; cudaGraphEdgeData edgeData ; edgeData . type = cudaGraphDependencyTypeProgrammatic ; edgeData . from_port = cudaGraphKernelNodePortProgrammatic ; cudaLaunchAttribute attribute ; attribute . id = cudaLaunchAttributeProgrammaticEvent ; attribute . programmaticEvent . triggerAtBlockStart = 0 ; cudaGraphEdgeData edgeData ; edgeData . triggerAtBlockStart = 1 ; cudaGraphEdgeData edgeData ; edgeData . from_port = cudaGraphKernelNodePortLaunchCompletion ; 3.2.8.7. CUDA Graphs \\uf0c1 CUDA Graphs present a new model for work submission in CUDA. A graph is a series of operations, such as kernel launches, connected by dependencies, which is defined separately from its execution. This allows a graph to be defined once and then launched repeatedly. Separating out the definition of a graph from its execution enables a number of optimizations: first, CPU launch costs are reduced compared to streams, because much of the setup is done in advance; second, presenting the whole workflow to CUDA enables optimizations which might not be possible with the piecewise work submission mechanism of streams. To see the optimizations possible with graphs, consider what happens in a stream: when you place a kernel into a stream, the host driver performs a sequence of operations in preparation for the execution of the kernel on the GPU. These operations, necessary for setting up and launching the kernel, are an overhead cost which must be paid for each kernel that is issued. For a GPU kernel with a short execution time, this overhead cost can be a significant fraction of the overall end-to-end execution time. Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution. During the definition phase, a program creates a description of the operations in the graph along with the dependencies between them. Instantiation takes a snapshot of the graph template, validates it, and performs much of the setup and initialization of work with the aim of minimizing what needs to be done at launch. The resulting instance is known as an executable graph. An executable graph may be launched into a stream, similar to any other CUDA work. It may be launched any number of times without repeating the instantiation. 3.2.8.7.1. Graph Structure \\uf0c1 An operation forms a node in a graph. The dependencies between the operations are the edges. These dependencies constrain the execution sequence of the operations. An operation may be scheduled at any time once the nodes on which it depends are complete. Scheduling is left up to the CUDA system. 3.2.8.7.1.1. Node Types \\uf0c1 A graph node can be one of: kernel CPU function call memory copy memset empty node waiting on an event recording an event signalling an external semaphore waiting on an external semaphore conditional node child graph: To execute a separate nested graph, as shown in the following figure. Figure 13 Child Graph Example \\uf0c1 3.2.8.7.1.2. Edge Data \\uf0c1 CUDA 12.3 introduced edge data on CUDA Graphs. Edge data modifies a dependency specified by an edge and consists of three parts: an outgoing port, an incoming port, and a type. An outgoing port specifies when an associated edge is triggered. An incoming port specifies what portion of a node is dependent on an associated edge. A type modifies the relation between the endpoints. Port values are specific to node type and direction, and edge types may be restricted to specific node types. In all cases, zero-initialized edge data represents default behavior. Outgoing port 0 waits on an entire task, incoming port 0 blocks an entire task, and edge type 0 is associated with a full dependency with memory synchronizing behavior. Edge data is optionally specified in various graph APIs via a parallel array to the associated nodes. If it is omitted as an input parameter, zero-initialized data is used. If it is omitted as an output (query) parameter, the API accepts this if the edge data being ignored is all zero-initialized, and returns cudaErrorLossyQuery if the call would discard information. Edge data is also available in some stream capture APIs: cudaStreamBeginCaptureToGraph() , cudaStreamGetCaptureInfo() , and cudaStreamUpdateCaptureDependencies() . In these cases, there is not yet a downstream node. The data is associated with a dangling edge (half edge) which will either be connected to a future captured node or discarded at termination of stream capture. Note that some edge types do not wait on full completion of the upstream node. These edges are ignored when considering if a stream capture has been fully rejoined to the origin stream, and cannot be discarded at the end of capture. See Creating a Graph Using Stream Capture . Currently, no node types define additional incoming ports, and only kernel nodes define additional outgoing ports. There is one non-default dependency type, cudaGraphDependencyTypeProgrammatic , which enables Programmatic Dependent Launch between two kernel nodes. 3.2.8.7.2. Creating a Graph Using Graph APIs \\uf0c1 Graphs can be created via two mechanisms: explicit API and stream capture. The following is an example of creating and executing the below graph. Figure 14 Creating a Graph Using Graph APIs Example \\uf0c1 // Create the graph - it starts out empty cudaGraphCreate ( & graph , 0 ); // For the purpose of this example, we'll create // the nodes separately from the dependencies to // demonstrate that it can be done in two stages. // Note that dependencies can also be specified // at node creation. cudaGraphAddKernelNode ( & a , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & b , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & c , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & d , graph , NULL , 0 , & nodeParams ); // Now set up dependencies on each node cudaGraphAddDependencies ( graph , & a , & b , 1 ); // A->B cudaGraphAddDependencies ( graph , & a , & c , 1 ); // A->C cudaGraphAddDependencies ( graph , & b , & d , 1 ); // B->D cudaGraphAddDependencies ( graph , & c , & d , 1 ); // C->D 3.2.8.7.3. Creating a Graph Using Stream Capture \\uf0c1 Stream capture provides a mechanism to create a graph from existing stream-based APIs. A section of code which launches work into streams, including existing code, can be bracketed with calls to cudaStreamBeginCapture() and cudaStreamEndCapture() . See below. cudaGraph_t graph ; cudaStreamBeginCapture ( stream ); kernel_A >> (...); kernel_B >> (...); libraryCall ( stream ); kernel_C >> (...); cudaStreamEndCapture ( stream , & graph ); A call to cudaStreamBeginCapture() places a stream in capture mode. When a stream is being captured, work launched into the stream is not enqueued for execution. It is instead appended to an internal graph that is progressively being built up. This graph is then returned by calling cudaStreamEndCapture() , which also ends capture mode for the stream. A graph which is actively being constructed by stream capture is referred to as a capture graph. Stream capture can be used on any CUDA stream except cudaStreamLegacy (the “NULL stream”). Note that it can be used on cudaStreamPerThread . If a program is using the legacy stream, it may be possible to redefine stream 0 to be the per-thread stream with no functional change. See Default Stream . Whether a stream is being captured can be queried with cudaStreamIsCapturing() . Work can be captured to an existing graph using cudaStreamBeginCaptureToGraph() . Instead of capturing to an internal graph, work is captured to a graph provided by the user. 3.2.8.7.3.1. Cross-stream Dependencies and Events \\uf0c1 Stream capture can handle cross-stream dependencies expressed with cudaEventRecord() and cudaStreamWaitEvent() , provided the event being waited upon was recorded into the same capture graph. When an event is recorded in a stream that is in capture mode, it results in a captured event. A captured event represents a set of nodes in a capture graph. When a captured event is waited on by a stream, it places the stream in capture mode if it is not already, and the next item in the stream will have additional dependencies on the nodes in the captured event. The two streams are then being captured to the same capture graph. When cross-stream dependencies are present in stream capture, cudaStreamEndCapture() must still be called in the same stream where cudaStreamBeginCapture() was called; this is the origin stream . Any other streams which are being captured to the same capture graph, due to event-based dependencies, must also be joined back to the origin stream. This is illustrated below. All streams being captured to the same capture graph are taken out of capture mode upon cudaStreamEndCapture() . Failure to rejoin to the origin stream will result in failure of the overall capture operation. // stream1 is the origin stream cudaStreamBeginCapture ( stream1 ); kernel_A >> (...); // Fork into stream2 cudaEventRecord ( event1 , stream1 ); cudaStreamWaitEvent ( stream2 , event1 ); kernel_B >> (...); kernel_C >> (...); // Join stream2 back to origin stream (stream1) cudaEventRecord ( event2 , stream2 ); cudaStreamWaitEvent ( stream1 , event2 ); kernel_D >> (...); // End capture in the origin stream cudaStreamEndCapture ( stream1 , & graph ); // stream1 and stream2 no longer in capture mode Graph returned by the above code is shown in Figure 14 . Note When a stream is taken out of capture mode, the next non-captured item in the stream (if any) will still have a dependency on the most recent prior non-captured item, despite intermediate items having been removed. 3.2.8.7.3.2. Prohibited and Unhandled Operations \\uf0c1 It is invalid to synchronize or query the execution status of a stream which is being captured or a captured event, because they do not represent items scheduled for execution. It is also invalid to query the execution status of or synchronize a broader handle which encompasses an active stream capture, such as a device or context handle when any associated stream is in capture mode. When any stream in the same context is being captured, and it was not created with cudaStreamNonBlocking , any attempted use of the legacy stream is invalid. This is because the legacy stream handle at all times encompasses these other streams; enqueueing to the legacy stream would create a dependency on the streams being captured, and querying it or synchronizing it would query or synchronize the streams being captured. It is therefore also invalid to call synchronous APIs in this case. Synchronous APIs, such as cudaMemcpy() , enqueue work to the legacy stream and synchronize it before returning. Note As a general rule, when a dependency relation would connect something that is captured with something that was not captured and instead enqueued for execution, CUDA prefers to return an error rather than ignore the dependency. An exception is made for placing a stream into or out of capture mode; this severs a dependency relation between items added to the stream immediately before and after the mode transition. It is invalid to merge two separate capture graphs by waiting on a captured event from a stream which is being captured and is associated with a different capture graph than the event. It is invalid to wait on a non-captured event from a stream which is being captured without specifying the cudaEventWaitExternal flag. A small number of APIs that enqueue asynchronous operations into streams are not currently supported in graphs and will return an error if called with a stream which is being captured, such as cudaStreamAttachMemAsync() . 3.2.8.7.3.3. Invalidation \\uf0c1 When an invalid operation is attempted during stream capture, any associated capture graphs are invalidated . When a capture graph is invalidated, further use of any streams which are being captured or captured events associated with the graph is invalid and will return an error, until stream capture is ended with cudaStreamEndCapture() . This call will take the associated streams out of capture mode, but will also return an error value and a NULL graph. 3.2.8.7.4. CUDA User Objects \\uf0c1 CUDA User Objects can be used to help manage the lifetime of resources used by asynchronous work in CUDA. In particular, this feature is useful for CUDA Graphs and stream capture . Various resource management schemes are not compatible with CUDA graphs. Consider for example an event-based pool or a synchronous-create, asynchronous-destroy scheme. // Library API with pool allocation void libraryWork ( cudaStream_t stream ) { auto & resource = pool . claimTemporaryResource (); resource . waitOnReadyEventInStream ( stream ); launchWork ( stream , resource ); resource . recordReadyEvent ( stream ); } // Library API with asynchronous resource deletion void libraryWork ( cudaStream_t stream ) { Resource * resource = new Resource (...); launchWork ( stream , resource ); cudaStreamAddCallback ( stream , []( cudaStream_t , cudaError_t , void * resource ) { delete static_cast ( resource ); }, resource , 0 ); // Error handling considerations not shown } These schemes are difficult with CUDA graphs because of the non-fixed pointer or handle for the resource which requires indirection or graph update, and the synchronous CPU code needed each time the work is submitted. They also do not work with stream capture if these considerations are hidden from the caller of the library, and because of use of disallowed APIs during capture. Various solutions exist such as exposing the resource to the caller. CUDA user objects present another approach. A CUDA user object associates a user-specified destructor callback with an internal refcount, similar to C++ shared_ptr . References may be owned by user code on the CPU and by CUDA graphs. Note that for user-owned references, unlike C++ smart pointers, there is no object representing the reference; users must track user-owned references manually. A typical use case would be to immediately move the sole user-owned reference to a CUDA graph after the user object is created. When a reference is associated to a CUDA graph, CUDA will manage the graph operations automatically. A cloned cudaGraph_t retains a copy of every reference owned by the source cudaGraph_t , with the same multiplicity. An instantiated cudaGraphExec_t retains a copy of every reference in the source cudaGraph_t . When a cudaGraphExec_t is destroyed without being synchronized, the references are retained until the execution is completed. Here is an example use. cudaGraph_t graph ; // Preexisting graph Object * object = new Object ; // C++ object with possibly nontrivial destructor cudaUserObject_t cuObject ; cudaUserObjectCreate ( & cuObject , object , // Here we use a CUDA-provided template wrapper for this API, // which supplies a callback to delete the C++ object pointer 1 , // Initial refcount cudaUserObjectNoDestructorSync // Acknowledge that the callback cannot be // waited on via CUDA ); cudaGraphRetainUserObject ( graph , cuObject , 1 , // Number of references cudaGraphUserObjectMove // Transfer a reference owned by the caller (do // not modify the total reference count) ); // No more references owned by this thread; no need to call release API cudaGraphExec_t graphExec ; cudaGraphInstantiate ( & graphExec , graph , nullptr , nullptr , 0 ); // Will retain a // new reference cudaGraphDestroy ( graph ); // graphExec still owns a reference cudaGraphLaunch ( graphExec , 0 ); // Async launch has access to the user objects cudaGraphExecDestroy ( graphExec ); // Launch is not synchronized; the release // will be deferred if needed cudaStreamSynchronize ( 0 ); // After the launch is synchronized, the remaining // reference is released and the destructor will // execute. Note this happens asynchronously. // If the destructor callback had signaled a synchronization object, it would // be safe to wait on it at this point. References owned by graphs in child graph nodes are associated to the child graphs, not the parents. If a child graph is updated or deleted, the references change accordingly. If an executable graph or child graph is updated with cudaGraphExecUpdate or cudaGraphExecChildGraphNodeSetParams , the references in the new source graph are cloned and replace the references in the target graph. In either case, if previous launches are not synchronized, any references which would be released are held until the launches have finished executing. There is not currently a mechanism to wait on user object destructors via a CUDA API. Users may signal a synchronization object manually from the destructor code. In addition, it is not legal to call CUDA APIs from the destructor, similar to the restriction on cudaLaunchHostFunc . This is to avoid blocking a CUDA internal shared thread and preventing forward progress. It is legal to signal another thread to perform an API call, if the dependency is one way and the thread doing the call cannot block forward progress of CUDA work. User objects are created with cudaUserObjectCreate , which is a good starting point to browse related APIs.\"},\n",
       " {'id': 214, 'content': '3.2.8.7.5.'},\n",
       " {'id': 215,\n",
       "  'content': 'Updating Instantiated Graphs \\uf0c1 Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution. In situations where the workflow is not changing, the overhead of definition and instantiation can be amortized over many executions, and graphs provide a clear advantage over streams. A graph is a snapshot of a workflow, including kernels, parameters, and dependencies, in order to replay it as rapidly and efficiently as possible. In situations where the workflow changes the graph becomes out of date and must be modified. Major changes to graph structure such as topology or types of nodes will require re-instantiation of the source graph because various topology-related optimization techniques must be re-applied. The cost of repeated instantiation can reduce the overall performance benefit from graph execution, but it is common for only node parameters, such as kernel parameters and cudaMemcpy addresses, to change while graph topology remains the same. For this case, CUDA provides a lightweight mechanism known as “Graph Update,” which allows certain node parameters to be modified in-place without having to rebuild the entire graph. This is much more efficient than re-instantiation. Updates will take effect the next time the graph is launched, so they will not impact previous graph launches, even if they are running at the time of the update. A graph may be updated and relaunched repeatedly, so multiple updates/launches can be queued on a stream. CUDA provides two mechanisms for updating instantiated graph parameters, whole graph update and individual node update. Whole graph update allows the user to supply a topologically identical cudaGraph_t object whose nodes contain updated parameters. Individual node update allows the user to explicitly update the parameters of individual nodes. Using an updated cudaGraph_t is more convenient when a large number of nodes are being updated, or when the graph topology is unknown to the caller (i.e., The graph resulted from stream capture of a library call). Using individual node update is preferred when the number of changes is small and the user has the handles to the nodes requiring updates. Individual node update skips the topology checks and comparisons for unchanged nodes, so it can be more efficient in many cases. CUDA also provides a mechanism for enabling and disabling individual nodes without affecting their current parameters. The following sections explain each approach in more detail.'},\n",
       " {'id': 216,\n",
       "  'content': '3.2.8.7.5.1. Graph Update Limitations \\uf0c1 Kernel nodes: The owning context of the function cannot change. A node whose function originally did not use CUDA dynamic parallelism cannot be updated to a function which uses CUDA dynamic parallelism. cudaMemset and cudaMemcpy nodes: The CUDA device(s) to which the operand(s) was allocated/mapped cannot change. The source/destination memory must be allocated from the same context as the original source/destination memory. Only 1D cudaMemset / cudaMemcpy nodes can be changed. Additional memcpy node restrictions: Changing either the source or destination memory type (i.e., cudaPitchedPtr , cudaArray_t , etc. ), or the type of transfer (i.e., cudaMemcpyKind ) is not supported. External semaphore wait nodes and record nodes: Changing the number of semaphores is not supported. Conditional nodes: The order of handle creation and assignment must match between the graphs. Changing node parameters is not supported (i.e. number of graphs in the conditional, node context, etc). Changing parameters of nodes within the conditional body graph is subject to the rules above. There are no restrictions on updates to host nodes, event record nodes, or event wait nodes. 3.2.8.7.5.2. Whole Graph Update \\uf0c1 cudaGraphExecUpdate() allows an instantiated graph (the “original graph”) to be updated with the parameters from a topologically identical graph (the “updating” graph). The topology of the updating graph must be identical to the original graph used to instantiate the cudaGraphExec_t . In addition, the order in which the dependencies are specified must match. Finally, CUDA needs to consistently order the sink nodes (nodes with no dependencies). CUDA relies on the order of specific api calls to achieve consistent sink node ordering. More explicitly, following the following rules will cause cudaGraphExecUpdate() to pair the nodes in the original graph and the updating graph deterministically: For any capturing stream, the API calls operating on that stream must be made in the same order, including event wait and other api calls not directly corresponding to node creation. The API calls which directly manipulate a given graph node’s incoming edges (including captured stream APIs, node add APIs, and edge addition / removal APIs) must be made in the same order. Moreover, when dependencies are specified in arrays to these APIs, the order in which the dependencies are specified inside those arrays must match. Sink nodes must be consistently ordered. Sink nodes are nodes without dependent nodes / outgoing edges in the final graph at the time of the cudaGraphExecUpdate() invocation. The following operations affect sink node ordering (if present) and must (as a combined set) be made in the same order: Node add APIs resulting in a sink node. Edge removal resulting in a node becoming a sink node. cudaStreamUpdateCaptureDependencies() , if it removes a sink node from a capturing stream’s dependency set. cudaStreamEndCapture() . The following example shows how the API could be used to update an instantiated graph: cudaGraphExec_t graphExec = NULL ; for ( int i = 0 ; i >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 ); // Launch the host graph, which will in turn launch the device graph. cudaGraphLaunch ( gExec1 , stream ); } A graph can have up to 120 total fire-and-forget graphs during the course of its execution. This total resets between launches of the same parent graph. 3.2.8.7.7.2.1.2. Graph Execution Environments \\uf0c1 In order to fully understand the device-side synchronization model, it is first necessary to understand the concept of an execution environment. When a graph is launched from the device, it is launched into its own execution environment. The execution environment of a given graph encapsulates all work in the graph as well as all generated fire and forget work. The graph can be considered complete when it has completed execution and when all generated child work is complete. The below diagram shows the environment encapsulation that would be generated by the fire-and-forget sample code in the previous section. Figure 16 Fire and forget launch, with execution environments \\uf0c1 These environments are also hierarchical, so a graph environment can include multiple levels of child-environments from fire and forget launches. Figure 17 Nested fire and forget environments \\uf0c1 When a graph is launched from the host, there exists a stream environment that parents the execution environment of the launched graph. The stream environment encapsulates all work generated as part of the overall launch. The stream launch is complete (i.e. downstream dependent work may now run) when the overall stream environment is marked as complete. Figure 18 The stream environment, visualized \\uf0c1 3.2.8.7.7.2.1.3. Tail Launch \\uf0c1 Unlike on the host, it is not possible to synchronize with device graphs from the GPU via traditional methods such as cudaDeviceSynchronize() or cudaStreamSynchronize() . Rather, in order to enable serial work dependencies, a different launch mode - tail launch - is offered, to provide similar functionality. A tail launch executes when a graph’s environment is considered complete - ie, when the graph and all its children are complete. When a graph completes, the environment of the next graph in the tail launch list will replace the completed environment as a child of the parent environment. Like fire-and-forget launches, a graph can have multiple graphs enqueued for tail launch. Figure 19 A simple tail launch \\uf0c1 The above execution flow can be generated by the code below: __global__ void launchTailGraph ( cudaGraphExec_t graph ) { cudaGraphLaunch ( graph , cudaStreamGraphTailLaunch ); } void graphSetup () { cudaGraphExec_t gExec1 , gExec2 ; cudaGraph_t g1 , g2 ; // Create, instantiate, and upload the device graph. create_graph ( & g2 ); cudaGraphInstantiate ( & gExec2 , g2 , cudaGraphInstantiateFlagDeviceLaunch ); cudaGraphUpload ( gExec2 , stream ); // Create and instantiate the launching graph. cudaStreamBeginCapture ( stream , cudaStreamCaptureModeGlobal ); launchTailGraph >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 ); // Launch the host graph, which will in turn launch the device graph. cudaGraphLaunch ( gExec1 , stream ); } Tail launches enqueued by a given graph will execute one at a time, in order of when they were enqueued. So the first enqueued graph will run first, and then the second, and so on. Figure 20 Tail launch ordering \\uf0c1 Tail launches enqueued by a tail graph will execute before tail launches enqueued by previous graphs in the tail launch list. These new tail launches will execute in the order they are enqueued. Figure 21 Tail launch ordering when enqueued from multiple graphs \\uf0c1 A graph can have up to 255 pending tail launches. 3.2.8.7.7.2.1.3.1. Tail Self-launch \\uf0c1 It is possible for a device graph to enqueue itself for a tail launch, although a given graph can only have one self-launch enqueued at a time. In order to query the currently running device graph so that it can be relaunched, a new device-side function is added: cudaGraphExec_t cudaGetCurrentGraphExec (); This function returns the handle of the currently running graph if it is a device graph. If the currently executing kernel is not a node within a device graph, this function will return NULL. Below is sample code showing usage of this function for a relaunch loop: __device__ int relaunchCount = 0 ; __global__ void relaunchSelf () { int relaunchMax = 100 ; if ( threadIdx . x == 0 ) { if ( relaunchCount >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 ); // Launch the host graph, which will in turn launch the device graph. cudaGraphLaunch ( gExec1 , stream ); } Since sibling launches are not launched into the launching graph’s execution environment, they will not gate tail launches enqueued by the launching graph. 3.2.8.7.8.'},\n",
       " {'id': 217,\n",
       "  'content': 'Conditional Graph Nodes \\uf0c1 Conditional nodes allow conditional execution and looping of a graph contained within the conditional node. This allows dynamic and iterative workflows to be represented completely within a graph and frees up the host CPU to perform other work in parallel. Evaluation of the condition value is performed on the device when the dependencies of the conditional node have been met. Conditional nodes can be one of the following types: Conditional IF nodes execute their body graph once if the condition value is non-zero when the node is executed. Conditional WHILE nodes execute their body graph if the condition value is non-zero when the node is executed and will continue to execute their body graph until the condition value is zero. A condition value is accessed by a conditional handle , which must be created before the node. The condition value can be set by device code using cudaGraphSetConditional() . A default value, applied on each graph launch, can also be specified when the handle is created. When the conditional node is created, an empty graph is created and the handle is returned to the user so that the graph can be populated. This conditional body graph can be populated using either the graph APIs or cudaStreamBeginCaptureToGraph() . Conditional nodes can be nested. 3.2.8.7.8.1. Conditional Handles \\uf0c1 A condition value is represented by cudaGraphConditionalHandle and is created by cudaGraphConditionalHandleCreate() . The handle must be associated with a single conditional node. Handles cannot be destroyed. If cudaGraphCondAssignDefault is specified when the handle is created, the condition value will be initialized to the specified default before every graph launch. If this flag is not provided, it is up to the user to initialize the condition value in a kernel upstream of the conditional node which tests it. If the condition value is not initialized by one of these methods, its value is undefined. The default value and flags associated with a handle will be updated during whole graph update . 3.2.8.7.8.2. Condtional Node Body Graph Requirements \\uf0c1 General requirements: The graph’s nodes must all reside on a single device. The graph can only contain kernel nodes, empty nodes, memcpy nodes, memset nodes, child graph nodes, and conditional nodes. Kernel nodes: Use of CUDA Dynamic Parallelism by kernels in the graph is not permitted. Cooperative launches are permitted so long as MPS is not in use. Memcpy/Memset nodes: Only copies/memsets involving device memory and/or pinned device-mapped host memory are permitted. Copies/memsets involving CUDA arrays are not permitted. Both operands must be accessible from the current device at time of instantiation. Note that the copy operation will be performed from the device on which the graph resides, even if it is targeting memory on another device. 3.2.8.7.8.3. Conditional IF Nodes \\uf0c1 The body graph of an IF node will be executed once if the condition is non-zero when the node is executed. The following diagram depicts a 3 node graph where the middle node, B, is a conditional node: Figure 23 Conditional IF Node \\uf0c1 The following code illustrates the creation of a graph containing an IF conditional node. The default value of the condition is set using an upstream kernel. The body of the conditional is populated using the graph API . __global__ void setHandle ( cudaGraphConditionalHandle handle ) { ... cudaGraphSetConditional ( handle , value ); ... } void graphSetup () { cudaGraph_t graph ; cudaGraphExec_t graphExec ; cudaGraphNode_t node ; void * kernelArgs [ 1 ]; int value = 1 ; cudaGraphCreate ( & graph , 0 ); cudaGraphConditionalHandle handle ; cudaGraphConditionalHandleCreate ( & handle , graph ); // Use a kernel upstream of the conditional to set the handle value cudaGraphNodeParams params = { cudaGraphNodeTypeKernel }; params . kernel .'},\n",
       " {'id': 218,\n",
       "  'content': 'func = ( void * ) setHandle ; params . gridDim . x = params . y = params . z = 1 ; params .'},\n",
       " {'id': 219,\n",
       "  'content': 'blockDim . kernelParams = kernelArgs ; kernelArgs [ 0 ] = & handle ; cudaGraphAddNode ( & node , graph , NULL , 0 , & params ); cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional }; cParams . conditional . handle = handle ; cParams . type = cudaGraphCondTypeIf ; cParams . size = 1 ; cudaGraphAddNode ( & node , graph , & node , 1 , & cParams ); cudaGraph_t bodyGraph = cParams . phGraph_out [ 0 ]; // Populate the body of the conditional node ... cudaGraphAddNode ( & node , bodyGraph , NULL , 0 , & params ); cudaGraphInstantiate ( & graphExec , graph , NULL , NULL , 0 ); cudaGraphLaunch ( graphExec , 0 ); cudaDeviceSynchronize (); cudaGraphExecDestroy ( graphExec ); cudaGraphDestroy ( graph ); } 3.2.8.7.8.4. Conditional WHILE Nodes \\uf0c1 The body graph of a WHILE node will be executed until the condition is non-zero. The condition will be evaluated when the node is executed and after completion of the body graph. The following diagram depicts a 3 node graph where the middle node, B, is a conditional node: Figure 24 Conditional WHILE Node \\uf0c1 The following code illustrates the creation of a graph containing a WHILE conditional node. The handle is created using cudaGraphCondAssignDefault to avoid the need for an upstream kernel. __global__ void loopKernel ( cudaGraphConditionalHandle handle ) { static int count = 10 ; cudaGraphSetConditional ( handle , -- count ? 1 : 0 ); } void graphSetup () { cudaGraph_t graph ; cudaGraphExec_t graphExec ; cudaGraphNode_t node ; void * kernelArgs [ 1 ]; cuGraphCreate ( & graph , 0 ); cudaGraphConditionalHandle handle ; cudaGraphConditionalHandleCreate ( & handle , graph , 1 , cudaGraphCondAssignDefault ); cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional }; cParams . type = cudaGraphCondTypeWhile ; cParams . size = 1 ; cudaGraphAddNode ( & node , graph , NULL , 0 , & cParams ); cudaGraph_t bodyGraph = cParams . phGraph_out [ 0 ]; cudaGraphNodeParams params = { cudaGraphNodeTypeKernel }; params . func = ( void * ) loopKernel ; params . kernelParams = kernelArgs ; kernelArgs [ 0 ] = & handle ; cudaGraphAddNode ( & node , bodyGraph , NULL , 0 , & params ); cudaGraphInstantiate ( & graphExec , graph , NULL , NULL , 0 ); cudaGraphLaunch ( graphExec , 0 ); cudaDeviceSynchronize (); cudaGraphExecDestroy ( graphExec ); cudaGraphDestroy ( graph ); } 3.2.8.8. Events \\uf0c1 The runtime also provides a way to closely monitor the device’s progress, as well as perform accurate timing, by letting the application asynchronously record events at any point in the program, and query when these events are completed. An event has completed when all tasks - or optionally, all commands in a given stream - preceding the event have completed. Events in stream zero are completed after all preceding tasks and commands in all streams are completed. 3.2.8.8.1. Creation and Destruction of Events \\uf0c1 The following code sample creates two events: cudaEvent_t start , stop ; cudaEventCreate ( & start ); cudaEventCreate ( & stop ); They are destroyed this way: cudaEventDestroy ( start ); cudaEventDestroy ( stop ); 3.2.8.8.2. Elapsed Time \\uf0c1 The events created in Creation and Destruction can be used to time the code sample of Creation and Destruction the following way: cudaEventRecord ( start , 0 ); for ( int i = 0 ; i >> ( outputDev + i * size , inputDev + i * size , size ); cudaMemcpyAsync ( outputHost + i * size , outputDev + i * size , size , cudaMemcpyDeviceToHost , stream [ i ]); } cudaEventRecord ( stop , 0 ); cudaEventSynchronize ( stop ); float elapsedTime ; cudaEventElapsedTime ( & elapsedTime , start , stop ); 3.2.8.9. Synchronous Calls \\uf0c1 When a synchronous function is called, control is not returned to the host thread before the device has completed the requested task. Whether the host thread will then yield, block, or spin can be specified by calling cudaSetDeviceFlags() with some specific flags (see reference manual for details) before any other CUDA call is performed by the host thread. 3.2.9.'},\n",
       " {'id': 220,\n",
       "  'content': 'Multi-Device System \\uf0c1 3.2.9.1. Device Enumeration \\uf0c1 A host system can have multiple devices. The following code sample shows how to enumerate these devices, query their properties, and determine the number of CUDA-enabled devices. int deviceCount ; cudaGetDeviceCount ( & deviceCount ); int device ; for ( device = 0 ; device >> ( p0 ); // Launch kernel on device 0 cudaSetDevice ( 1 ); // Set device 1 as current float * p1 ; cudaMalloc ( & p1 , size ); // Allocate memory on device 1 MyKernel >> ( p1 ); // Launch kernel on device 1 3.2.9.3. Stream and Event Behavior \\uf0c1 A kernel launch will fail if it is issued to a stream that is not associated to the current device as illustrated in the following code sample. cudaSetDevice ( 0 ); // Set device 0 as current cudaStream_t s0 ; cudaStreamCreate ( & s0 ); // Create stream s0 on device 0 MyKernel >> (); // Launch kernel on device 0 in s0 cudaSetDevice ( 1 ); // Set device 1 as current cudaStream_t s1 ; cudaStreamCreate ( & s1 ); // Create stream s1 on device 1 MyKernel >> (); // Launch kernel on device 1 in s1 // This kernel launch will fail: MyKernel >> (); // Launch kernel on device 1 in s0 A memory copy will succeed even if it is issued to a stream that is not associated to the current device. cudaEventRecord() will fail if the input event and input stream are associated to different devices. cudaEventElapsedTime() will fail if the two input events are associated to different devices. cudaEventSynchronize() and cudaEventQuery() will succeed even if the input event is associated to a device that is different from the current device. cudaStreamWaitEvent() will succeed even if the input stream and input event are associated to different devices. cudaStreamWaitEvent() can therefore be used to synchronize multiple devices with each other. Each device has its own default stream (see Default Stream ), so commands issued to the default stream of a device may execute out of order or concurrently with respect to commands issued to the default stream of any other device. 3.2.9.4. Peer-to-Peer Memory Access \\uf0c1 Depending on the system properties, specifically the PCIe and/or NVLINK topology, devices are able to address each other’s memory (i.e., a kernel executing on one device can dereference a pointer to the memory of the other device). This peer-to-peer memory access feature is supported between two devices if cudaDeviceCanAccessPeer() returns true for these two devices. Peer-to-peer memory access is only supported in 64-bit applications and must be enabled between two devices by calling cudaDeviceEnablePeerAccess() as illustrated in the following code sample. On non-NVSwitch enabled systems, each device can support a system-wide maximum of eight peer connections. A unified address space is used for both devices (see Unified Virtual Address Space ), so the same pointer can be used to address memory from both devices as shown in the code sample below. cudaSetDevice ( 0 ); // Set device 0 as current float * p0 ; size_t size = 1024 * sizeof ( float ); cudaMalloc ( & p0 , size ); // Allocate memory on device 0 MyKernel >> ( p0 ); // Launch kernel on device 0 cudaSetDevice ( 1 ); // Set device 1 as current cudaDeviceEnablePeerAccess ( 0 , 0 ); // Enable peer-to-peer access // with device 0 // Launch kernel on device 1 // This kernel launch can access memory on device 0 at address p0 MyKernel >> ( p0 ); 3.2.9.4.1. IOMMU on Linux \\uf0c1 On Linux only, CUDA and the display driver does not support IOMMU-enabled bare-metal PCIe peer to peer memory copy. However, CUDA and the display driver does support IOMMU via VM pass through. As a consequence, users on Linux, when running on a native bare metal system, should disable the IOMMU. The IOMMU should be enabled and the VFIO driver be used as a PCIe pass through for virtual machines. On Windows the above limitation does not exist. See also Allocating DMA Buffers on 64-bit Platforms . 3.2.9.5. Peer-to-Peer Memory Copy \\uf0c1 Memory copies can be performed between the memories of two different devices. When a unified address space is used for both devices (see Unified Virtual Address Space ), this is done using the regular memory copy functions mentioned in Device Memory . Otherwise, this is done using cudaMemcpyPeer() , cudaMemcpyPeerAsync() , cudaMemcpy3DPeer() , or cudaMemcpy3DPeerAsync() as illustrated in the following code sample. cudaSetDevice ( 0 ); // Set device 0 as current float * p0 ; size_t size = 1024 * sizeof ( float ); cudaMalloc ( & p0 , size ); // Allocate memory on device 0 cudaSetDevice ( 1 ); // Set device 1 as current float * p1 ; cudaMalloc ( & p1 , size ); // Allocate memory on device 1 cudaSetDevice ( 0 ); // Set device 0 as current MyKernel >> ( p0 ); // Launch kernel on device 0 cudaSetDevice ( 1 ); // Set device 1 as current cudaMemcpyPeer ( p1 , 1 , p0 , 0 , size ); // Copy p0 to p1 MyKernel >> ( p1 ); // Launch kernel on device 1 A copy (in the implicit NULL stream) between the memories of two different devices: does not start until all commands previously issued to either device have completed and runs to completion before any commands (see Asynchronous Concurrent Execution ) issued after the copy to either device can start. Consistent with the normal behavior of streams, an asynchronous copy between the memories of two devices may overlap with copies or kernels in another stream. Note that if peer-to-peer access is enabled between two devices via cudaDeviceEnablePeerAccess() as described in Peer-to-Peer Memory Access , peer-to-peer memory copy between these two devices no longer needs to be staged through the host and is therefore faster. 3.2.10. Unified Virtual Address Space \\uf0c1 When the application is run as a 64-bit process, a single address space is used for the host and all the devices of compute capability 2.0 and higher. All host memory allocations made via CUDA API calls and all device memory allocations on supported devices are within this virtual address range. As a consequence: The location of any memory on the host allocated through CUDA, or on any of the devices which use the unified address space, can be determined from the value of the pointer using cudaPointerGetAttributes() . When copying to or from the memory of any device which uses the unified address space, the cudaMemcpyKind parameter of cudaMemcpy*() can be set to cudaMemcpyDefault to determine locations from the pointers. This also works for host pointers not allocated through CUDA, as long as the current device uses unified addressing. Allocations via cudaHostAlloc() are automatically portable (see Portable Memory ) across all the devices for which the unified address space is used, and pointers returned by cudaHostAlloc() can be used directly from within kernels running on these devices (i.e., there is no need to obtain a device pointer via cudaHostGetDevicePointer() as described in Mapped Memory . Applications may query if the unified address space is used for a particular device by checking that the unifiedAddressing device property (see Device Enumeration ) is equal to 1. 3.2.11. Interprocess Communication \\uf0c1 Any device memory pointer or event handle created by a host thread can be directly referenced by any other thread within the same process. It is not valid outside this process however, and therefore cannot be directly referenced by threads belonging to a different process. To share device memory pointers and events across processes, an application must use the Inter Process Communication API, which is described in detail in the reference manual. The IPC API is only supported for 64-bit processes on Linux and for devices of compute capability 2.0 and higher. Note that the IPC API is not supported for cudaMallocManaged allocations. Using this API, an application can get the IPC handle for a given device memory pointer using cudaIpcGetMemHandle() , pass it to another process using standard IPC mechanisms (for example, interprocess shared memory or files), and use cudaIpcOpenMemHandle() to retrieve a device pointer from the IPC handle that is a valid pointer within this other process. Event handles can be shared using similar entry points. Note that allocations made by cudaMalloc() may be sub-allocated from a larger block of memory for performance reasons. In such case, CUDA IPC APIs will share the entire underlying memory block which may cause other sub-allocations to be shared, which can potentially lead to information disclosure between processes. To prevent this behavior, it is recommended to only share allocations with a 2MiB aligned size. An example of using the IPC API is where a single primary process generates a batch of input data, making the data available to multiple secondary processes without requiring regeneration or copying. Applications using CUDA IPC to communicate with each other should be compiled, linked, and run with the same CUDA driver and runtime. Note Since CUDA 11.5, only events-sharing IPC APIs are supported on L4T and embedded Linux Tegra devices with compute capability 7.x and higher. The memory-sharing IPC APIs are still not supported on Tegra platforms. 3.2.12. Error Checking \\uf0c1 All runtime functions return an error code, but for an asynchronous function (see Asynchronous Concurrent Execution ), this error code cannot possibly report any of the asynchronous errors that could occur on the device since the function returns before the device has completed the task; the error code only reports errors that occur on the host prior to executing the task, typically related to parameter validation; if an asynchronous error occurs, it will be reported by some subsequent unrelated runtime function call. The only way to check for asynchronous errors just after some asynchronous function call is therefore to synchronize just after the call by calling cudaDeviceSynchronize() (or by using any other synchronization mechanisms described in Asynchronous Concurrent Execution ) and checking the error code returned by cudaDeviceSynchronize() . The runtime maintains an error variable for each host thread that is initialized to cudaSuccess and is overwritten by the error code every time an error occurs (be it a parameter validation error or an asynchronous error). cudaPeekAtLastError() returns this variable. cudaGetLastError() returns this variable and resets it to cudaSuccess . Kernel launches do not return any error code, so cudaPeekAtLastError() or cudaGetLastError() must be called just after the kernel launch to retrieve any pre-launch errors. To ensure that any error returned by cudaPeekAtLastError() or cudaGetLastError() does not originate from calls prior to the kernel launch, one has to make sure that the runtime error variable is set to cudaSuccess just before the kernel launch, for example, by calling cudaGetLastError() just before the kernel launch. Kernel launches are asynchronous, so to check for asynchronous errors, the application must synchronize in-between the kernel launch and the call to cudaPeekAtLastError() or cudaGetLastError() . Note that cudaErrorNotReady that may be returned by cudaStreamQuery() and cudaEventQuery() is not considered an error and is therefore not reported by cudaPeekAtLastError() or cudaGetLastError() . 3.2.13. Call Stack \\uf0c1 On devices of compute capability 2.x and higher, the size of the call stack can be queried using cudaDeviceGetLimit() and set using cudaDeviceSetLimit() . When the call stack overflows, the kernel call fails with a stack overflow error if the application is run via a CUDA debugger (CUDA-GDB, Nsight) or an unspecified launch error, otherwise. When the compiler cannot determine the stack size, it issues a warning saying Stack size cannot be statically determined. This is usually the case with recursive functions. Once this warning is issued, user will need to set stack size manually if default stack size is not sufficient. 3.2.14. Texture and Surface Memory \\uf0c1 CUDA supports a subset of the texturing hardware that the GPU uses for graphics to access texture and surface memory. Reading data from texture or surface memory instead of global memory can have several performance benefits as described in Device Memory Accesses . 3.2.14.1. Texture Memory \\uf0c1 Texture memory is read from kernels using the device functions described in Texture Functions . The process of reading a texture calling one of these functions is called a texture fetch . Each texture fetch specifies a parameter called a texture object for the texture object API. The texture object specifies: The texture , which is the piece of texture memory that is fetched. Texture objects are created at runtime and the texture is specified when creating the texture object as described in Texture Object API . Its dimensionality that specifies whether the texture is addressed as a one dimensional array using one texture coordinate, a two-dimensional array using two texture coordinates, or a three-dimensional array using three texture coordinates. Elements of the array are called texels , short for texture elements . The texture width , height , and depth refer to the size of the array in each dimension. Table 21 lists the maximum texture width, height, and depth depending on the compute capability of the device. The type of a texel, which is restricted to the basic integer and single-precision floating-point types and any of the 1-, 2-, and 4-component vector types defined in Built-in Vector Types that are derived from the basic integer and single-precision floating-point types. The read mode , which is equal to cudaReadModeNormalizedFloat or cudaReadModeElementType . If it is cudaReadModeNormalizedFloat and the type of the texel is a 16-bit or 8-bit integer type, the value returned by the texture fetch is actually returned as floating-point type and the full range of the integer type is mapped to [0.0, 1.0] for unsigned integer type and [-1.0, 1.0] for signed integer type; for example, an unsigned 8-bit texture element with the value 0xff reads as 1. If it is cudaReadModeElementType , no conversion is performed. Whether texture coordinates are normalized or not. By default, textures are referenced (by the functions of Texture Functions ) using floating-point coordinates in the range [0, N-1] where N is the size of the texture in the dimension corresponding to the coordinate. For example, a texture that is 64x32 in size will be referenced with coordinates in the range [0, 63] and [0, 31] for the x and y dimensions, respectively. Normalized texture coordinates cause the coordinates to be specified in the range [0.0, 1.0-1/N] instead of [0, N-1], so the same 64x32 texture would be addressed by normalized coordinates in the range [0, 1-1/N] in both the x and y dimensions. Normalized texture coordinates are a natural fit to some applications’ requirements, if it is preferable for the texture coordinates to be independent of the texture size. The addressing mode . It is valid to call the device functions of Section B.8 with coordinates that are out of range. The addressing mode defines what happens in that case. The default addressing mode is to clamp the coordinates to the valid range: [0, N) for non-normalized coordinates and [0.0, 1.0) for normalized coordinates. If the border mode is specified instead, texture fetches with out-of-range texture coordinates return zero. For normalized coordinates, the wrap mode and the mirror mode are also available. When using the wrap mode, each coordinate x is converted to frac(x)=x - floor(x) where floor(x) is the largest integer not greater than x . When using the mirror mode, each coordinate x is converted to frac(x) if floor(x) is even and 1-frac(x) if floor(x) is odd. The addressing mode is specified as an array of size three whose first, second, and third elements specify the addressing mode for the first, second, and third texture coordinates, respectively; the addressing mode are cudaAddressModeBorder , cudaAddressModeClamp , cudaAddressModeWrap , and cudaAddressModeMirror ; cudaAddressModeWrap and cudaAddressModeMirror are only supported for normalized texture coordinates The filtering mode which specifies how the value returned when fetching the texture is computed based on the input texture coordinates. Linear texture filtering may be done only for textures that are configured to return floating-point data. It performs low-precision interpolation between neighboring texels. When enabled, the texels surrounding a texture fetch location are read and the return value of the texture fetch is interpolated based on where the texture coordinates fell between the texels. Simple linear interpolation is performed for one-dimensional textures, bilinear interpolation for two-dimensional textures, and trilinear interpolation for three-dimensional textures. The filtering mode is equal to cudaFilterModePoint or cudaFilterModeLinear . If it is cudaFilterModePoint , the returned value is the texel whose texture coordinates are the closest to the input texture coordinates. If it is cudaFilterModeLinear , the returned value is the linear interpolation of the two (for a one-dimensional texture), four (for a two dimensional texture), or eight (for a three dimensional texture) texels whose texture coordinates are the closest to the input texture coordinates. cudaFilterModeLinear is only valid for returned values of floating-point type. Texture Object API introduces the texture object API.'},\n",
       " {'id': 221,\n",
       "  'content': '16-Bit Floating-Point Textures explains how to deal with 16-bit floating-point textures. Textures can also be layered as described in Layered Textures . Cubemap Textures and Cubemap Layered Textures describe a special type of texture, the cubemap texture. Texture Gather describes a special texture fetch, texture gather. 3.2.14.1.1. Texture Object API \\uf0c1 A texture object is created using cudaCreateTextureObject() from a resource description of type struct cudaResourceDesc , which specifies the texture, and from a texture description defined as such: struct cudaTextureDesc { enum cudaTextureAddressMode addressMode [ 3 ]; enum cudaTextureFilterMode filterMode ; enum cudaTextureReadMode readMode ; int sRGB ; int normalizedCoords ; unsigned int maxAnisotropy ; enum cudaTextureFilterMode mipmapFilterMode ; float mipmapLevelBias ; float minMipmapLevelClamp ; float maxMipmapLevelClamp ; }; addressMode specifies the addressing mode; filterMode specifies the filter mode; readMode specifies the read mode; normalizedCoords specifies whether texture coordinates are normalized or not; See reference manual for sRGB , maxAnisotropy , mipmapFilterMode , mipmapLevelBias , minMipmapLevelClamp , and maxMipmapLevelClamp . The following code sample applies some simple transformation kernel to a texture. // Simple transformation kernel __global__ void transformKernel ( float * output , cudaTextureObject_t texObj , int width , int height , float theta ) { // Calculate normalized texture coordinates unsigned int x = blockIdx . x ; unsigned int y = blockIdx . y ; float u = x / ( float ) width ; float v = y / ( float ) height ; // Transform coordinates u -= 0.5f ; v -= 0.5f ; float tu = u * cosf ( theta ) - v * sinf ( theta ) + 0.5f ; float tv = v * cosf ( theta ) + u * sinf ( theta ) + 0.5f ; // Read from texture and write to global memory output [ y * width + x ] = tex2D ( texObj , tu , tv ); } // Host code int main () { const int height = 1024 ; const int width = 1024 ; float angle = 0.5 ; // Allocate and set some host data float * h_data = ( float * ) std :: malloc ( sizeof ( float ) * width * height ); for ( int i = 0 ; i >> ( output , texObj , width , height , angle ); // Copy data from device back to host cudaMemcpy ( h_data , output , width * height * sizeof ( float ), cudaMemcpyDeviceToHost ); // Destroy texture object cudaDestroyTextureObject ( texObj ); // Free device memory cudaFreeArray ( cuArray ); cudaFree ( output ); // Free host memory free ( h_data ); return 0 ; } 3.2.14.1.2. 16-Bit Floating-Point Textures \\uf0c1 The 16-bit floating-point or half format supported by CUDA arrays is the same as the IEEE 754-2008 binary2 format.'},\n",
       " {'id': 222,\n",
       "  'content': 'CUDA C++ does not support a matching data type, but provides intrinsic functions to convert to and from the 32-bit floating-point format via the unsigned short type: __float2half_rn(float) and __half2float(unsigned short) . These functions are only supported in device code.'},\n",
       " {'id': 223,\n",
       "  'content': 'Equivalent functions for the host code can be found in the OpenEXR library, for example. 16-bit floating-point components are promoted to 32 bit float during texture fetching before any filtering is performed. A channel description for the 16-bit floating-point format can be created by calling one of the cudaCreateChannelDescHalf*() functions. 3.2.14.1.3. Layered Textures \\uf0c1 A one-dimensional or two-dimensional layered texture (also known as texture array in Direct3D and array texture in OpenGL) is a texture made up of a sequence of layers, all of which are regular textures of same dimensionality, size, and data type. A one-dimensional layered texture is addressed using an integer index and a floating-point texture coordinate; the index denotes a layer within the sequence and the coordinate addresses a texel within that layer. A two-dimensional layered texture is addressed using an integer index and two floating-point texture coordinates; the index denotes a layer within the sequence and the coordinates address a texel within that layer. A layered texture can only be a CUDA array by calling cudaMalloc3DArray() with the cudaArrayLayered flag (and a height of zero for one-dimensional layered texture). Layered textures are fetched using the device functions described in tex1DLayered() and tex2DLayered() . Texture filtering (see Texture Fetching ) is done only within a layer, not across layers. Layered textures are only supported on devices of compute capability 2.0 and higher. 3.2.14.1.4. Cubemap Textures \\uf0c1 A cubemap texture is a special type of two-dimensional layered texture that has six layers representing the faces of a cube: The width of a layer is equal to its height. The cubemap is addressed using three texture coordinates x , y , and z that are interpreted as a direction vector emanating from the center of the cube and pointing to one face of the cube and a texel within the layer corresponding to that face. More specifically, the face is selected by the coordinate with largest magnitude m and the corresponding layer is addressed using coordinates (s/m+1)/2 and (t/m+1)/2 where s and t are defined in Table 3 . Table 3 Cubemap Fetch \\uf0c1 face m s t |x| > |y| and |x| > |z| x ≥ 0 0 x -z -y x |x| and |y| > |z| y ≥ 0 2 y x z y |x| and |z| > |y| z ≥ 0 4 z x -y z >> ( inputSurfObj , outputSurfObj , width , height ); // Copy data from device back to host cudaMemcpy2DFromArray ( h_data , spitch , cuOutputArray , 0 , 0 , 4 * width * sizeof ( unsigned char ), height , cudaMemcpyDeviceToHost ); // Destroy surface objects cudaDestroySurfaceObject ( inputSurfObj ); cudaDestroySurfaceObject ( outputSurfObj ); // Free device memory cudaFreeArray ( cuInputArray ); cudaFreeArray ( cuOutputArray ); // Free host memory free ( h_data ); return 0 ; } 3.2.14.2.2. Cubemap Surfaces \\uf0c1 Cubemap surfaces are accessed using surfCubemapread() and surfCubemapwrite() ( surfCubemapread and surfCubemapwrite ) as a two-dimensional layered surface, i.e., using an integer index denoting a face and two floating-point texture coordinates addressing a texel within the layer corresponding to this face.'},\n",
       " {'id': 224,\n",
       "  'content': 'Faces are ordered as indicated in Table 3 . 3.2.14.2.3. Cubemap Layered Surfaces \\uf0c1 Cubemap layered surfaces are accessed using surfCubemapLayeredread() and surfCubemapLayeredwrite() ( surfCubemapLayeredread() and surfCubemapLayeredwrite() ) as a two-dimensional layered surface, i.e., using an integer index denoting a face of one of the cubemaps and two floating-point texture coordinates addressing a texel within the layer corresponding to this face. Faces are ordered as indicated in Table 3 , so index ((2 * 6) + 3), for example, accesses the fourth face of the third cubemap. 3.2.14.3. CUDA Arrays \\uf0c1 CUDA arrays are opaque memory layouts optimized for texture fetching. They are one dimensional, two dimensional, or three-dimensional and composed of elements, each of which has 1, 2 or 4 components that may be signed or unsigned 8-, 16-, or 32-bit integers, 16-bit floats, or 32-bit floats. CUDA arrays are only accessible by kernels through texture fetching as described in Texture Memory or surface reading and writing as described in Surface Memory . 3.2.14.4. Read/Write Coherency \\uf0c1 The texture and surface memory is cached (see Device Memory Accesses ) and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global write or a surface write in the same kernel call returns undefined data. In other words, a thread can safely read some texture or surface memory location only if this memory location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call. 3.2.15. Graphics Interoperability \\uf0c1 Some resources from OpenGL and Direct3D may be mapped into the address space of CUDA, either to enable CUDA to read data written by OpenGL or Direct3D, or to enable CUDA to write data for consumption by OpenGL or Direct3D. A resource must be registered to CUDA before it can be mapped using the functions mentioned in OpenGL Interoperability and Direct3D Interoperability . These functions return a pointer to a CUDA graphics resource of type struct cudaGraphicsResource . Registering a resource is potentially high-overhead and therefore typically called only once per resource. A CUDA graphics resource is unregistered using cudaGraphicsUnregisterResource() . Each CUDA context which intends to use the resource is required to register it separately. Once a resource is registered to CUDA, it can be mapped and unmapped as many times as necessary using cudaGraphicsMapResources() and cudaGraphicsUnmapResources() . cudaGraphicsResourceSetMapFlags() can be called to specify usage hints (write-only, read-only) that the CUDA driver can use to optimize resource management. A mapped resource can be read from or written to by kernels using the device memory address returned by cudaGraphicsResourceGetMappedPointer() for buffers and cudaGraphicsSubResourceGetMappedArray() for CUDA arrays. Accessing a resource through OpenGL, Direct3D, or another CUDA context while it is mapped produces undefined results. OpenGL Interoperability and Direct3D Interoperability give specifics for each graphics API and some code samples. SLI Interoperability gives specifics for when the system is in SLI mode. 3.2.15.1. OpenGL Interoperability \\uf0c1 The OpenGL resources that may be mapped into the address space of CUDA are OpenGL buffer, texture, and renderbuffer objects. A buffer object is registered using cudaGraphicsGLRegisterBuffer() . In CUDA, it appears as a device pointer and can therefore be read and written by kernels or via cudaMemcpy() calls. A texture or renderbuffer object is registered using cudaGraphicsGLRegisterImage() . In CUDA, it appears as a CUDA array. Kernels can read from the array by binding it to a texture or surface reference. They can also write to it via the surface write functions if the resource has been registered with the cudaGraphicsRegisterFlagsSurfaceLoadStore flag. The array can also be read and written via cudaMemcpy2D() calls. cudaGraphicsGLRegisterImage() supports all texture formats with 1, 2, or 4 components and an internal type of float (for example, GL_RGBA_FLOAT32 ), normalized integer (for example, GL_RGBA8, GL_INTENSITY16 ), and unnormalized integer (for example, GL_RGBA8UI ) (please note that since unnormalized integer formats require OpenGL 3.0, they can only be written by shaders, not the fixed function pipeline). The OpenGL context whose resources are being shared has to be current to the host thread making any OpenGL interoperability API calls. Please note: When an OpenGL texture is made bindless (say for example by requesting an image or texture handle using the glGetTextureHandle */ glGetImageHandle * APIs) it cannot be registered with CUDA. The application needs to register the texture for interop before requesting an image or texture handle. The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object: GLuint positionsVBO ; struct cudaGraphicsResource * positionsVBO_CUDA ; int main () { // Initialize OpenGL and GLUT for device 0 // and make the OpenGL context current ... glutDisplayFunc ( display ); // Explicitly set device 0 cudaSetDevice ( 0 ); // Create buffer object and register it with CUDA glGenBuffers ( 1 , & positionsVBO ); glBindBuffer ( GL_ARRAY_BUFFER , positionsVBO ); unsigned int size = width * height * 4 * sizeof ( float ); glBufferData ( GL_ARRAY_BUFFER , size , 0 , GL_DYNAMIC_DRAW ); glBindBuffer ( GL_ARRAY_BUFFER , 0 ); cudaGraphicsGLRegisterBuffer ( & positionsVBO_CUDA , positionsVBO , cudaGraphicsMapFlagsWriteDiscard ); // Launch rendering loop glutMainLoop (); ... } void display () { // Map buffer object for writing from CUDA float4 * positions ; cudaGraphicsMapResources ( 1 , & positionsVBO_CUDA , 0 ); size_t num_bytes ; cudaGraphicsResourceGetMappedPointer (( void ** ) & positions , & num_bytes , positionsVBO_CUDA )); // Execute kernel dim3 dimBlock ( 16 , 16 , 1 ); dim3 dimGrid ( width / dimBlock . x , height / dimBlock . y , 1 ); createVertices >> ( positions , time , width , height ); // Unmap buffer object cudaGraphicsUnmapResources ( 1 , & positionsVBO_CUDA , 0 ); // Render from buffer object glClear ( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT ); glBindBuffer ( GL_ARRAY_BUFFER , positionsVBO ); glVertexPointer ( 4 , GL_FLOAT , 0 , 0 ); glEnableClientState ( GL_VERTEX_ARRAY ); glDrawArrays ( GL_POINTS , 0 , width * height ); glDisableClientState ( GL_VERTEX_ARRAY ); // Swap buffers glutSwapBuffers (); glutPostRedisplay (); } void deleteVBO () { cudaGraphicsUnregisterResource ( positionsVBO_CUDA ); glDeleteBuffers ( 1 , & positionsVBO ); } __global__ void createVertices ( float4 * positions , float time , unsigned int width , unsigned int height ) { unsigned int x = blockIdx . y ; // Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ; // calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ; // Write positions positions [ y * width + x ] = make_float4 ( u , w , v , 1.0f ); } On Windows and for Quadro GPUs, cudaWGLGetDevice() can be used to retrieve the CUDA device associated to the handle returned by wglEnumGpusNV() .'},\n",
       " {'id': 225,\n",
       "  'content': 'Quadro GPUs offer higher performance OpenGL interoperability than GeForce and Tesla GPUs in a multi-GPU configuration where OpenGL rendering is performed on the Quadro GPU and CUDA computations are performed on other GPUs in the system. 3.2.15.2. Direct3D Interoperability \\uf0c1 Direct3D interoperability is supported for Direct3D 9Ex, Direct3D 10, and Direct3D 11. A CUDA context may interoperate only with Direct3D devices that fulfill the following criteria: Direct3D 9Ex devices must be created with DeviceType set to D3DDEVTYPE_HAL and BehaviorFlags with the D3DCREATE_HARDWARE_VERTEXPROCESSING flag; Direct3D 10 and Direct3D 11 devices must be created with DriverType set to D3D_DRIVER_TYPE_HARDWARE . The Direct3D resources that may be mapped into the address space of CUDA are Direct3D buffers, textures, and surfaces. These resources are registered using cudaGraphicsD3D9RegisterResource() , cudaGraphicsD3D10RegisterResource() , and cudaGraphicsD3D11RegisterResource() . The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object. 3.2.15.2.1. Direct3D 9 Version \\uf0c1 IDirect3D9 * D3D ; IDirect3DDevice9 * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; IDirect3DVertexBuffer9 * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ; // Initialize Direct3D D3D = Direct3DCreate9Ex ( D3D_SDK_VERSION ); // Get a CUDA-enabled adapter unsigned int adapter = 0 ; for (; adapter GetAdapterCount (); adapter ++ ) { D3DADAPTER_IDENTIFIER9 adapterId ; g_pD3D -> GetAdapterIdentifier ( adapter , 0 , & adapterId ); if ( cudaD3D9GetDevice ( & dev , adapterId . DeviceName ) == cudaSuccess ) break ; } // Create device ... D3D -> CreateDeviceEx ( adapter , D3DDEVTYPE_HAL , hWnd , D3DCREATE_HARDWARE_VERTEXPROCESSING , & params , NULL , & device ); // Use the same device cudaSetDevice ( dev ); // Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); device -> CreateVertexBuffer ( size , 0 , D3DFVF_CUSTOMVERTEX , D3DPOOL_DEFAULT , & positionsVB , 0 ); cudaGraphicsD3D9RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard ); // Launch rendering loop while (...) { ... Render (); ... } ... } void Render () { // Map vertex buffer for writing from CUDA float4 * positions ; cudaGraphicsMapResources ( 1 , & positionsVB_CUDA , 0 ); size_t num_bytes ; cudaGraphicsResourceGetMappedPointer (( void ** ) & positions , & num_bytes , positionsVB_CUDA )); // Execute kernel dim3 dimBlock ( 16 , 16 , 1 ); dim3 dimGrid ( width / dimBlock . y , 1 ); createVertices >> ( positions , time , width , height ); // Unmap vertex buffer cudaGraphicsUnmapResources ( 1 , & positionsVB_CUDA , 0 ); // Draw and present ... } void releaseVB () { cudaGraphicsUnregisterResource ( positionsVB_CUDA ); positionsVB -> Release (); } __global__ void createVertices ( float4 * positions , float time , unsigned int width , unsigned int height ) { unsigned int x = blockIdx . y ; // Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ; // Calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ; // Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3.2.15.2.2. Direct3D 10 Version \\uf0c1 ID3D10Device * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; ID3D10Buffer * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ; // Get a CUDA-enabled adapter IDXGIFactory * factory ; CreateDXGIFactory ( __uuidof ( IDXGIFactory ), ( void ** ) & factory ); IDXGIAdapter * adapter = 0 ; for ( unsigned int i = 0 ; ! adapter ; ++ i ) { if ( FAILED ( factory -> EnumAdapters ( i , & adapter )) break ; if ( cudaD3D10GetDevice ( & dev , adapter ) == cudaSuccess ) break ; adapter -> Release (); } factory -> Release (); // Create swap chain and device ... D3D10CreateDeviceAndSwapChain ( adapter , D3D10_DRIVER_TYPE_HARDWARE , 0 , D3D10_CREATE_DEVICE_DEBUG , D3D10_SDK_VERSION , & swapChainDesc , & swapChain , & device ); adapter -> Release (); // Use the same device cudaSetDevice ( dev ); // Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); D3D10_BUFFER_DESC bufferDesc ; bufferDesc . Usage = D3D10_USAGE_DEFAULT ; bufferDesc . ByteWidth = size ; bufferDesc . BindFlags = D3D10_BIND_VERTEX_BUFFER ; bufferDesc . CPUAccessFlags = 0 ; bufferDesc . MiscFlags = 0 ; device -> CreateBuffer ( & bufferDesc , 0 , & positionsVB ); cudaGraphicsD3D10RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard ); // Launch rendering loop while (...) { ... y ; // Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ; // Calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ; // Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3.2.15.2.3. Direct3D 11 Version \\uf0c1 ID3D11Device * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; ID3D11Buffer * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ; // Get a CUDA-enabled adapter IDXGIFactory * factory ; CreateDXGIFactory ( __uuidof ( IDXGIFactory ), ( void ** ) & factory ); IDXGIAdapter * adapter = 0 ; for ( unsigned int i = 0 ; ! adapter ; ++ i ) { if ( FAILED ( factory -> EnumAdapters ( i , & adapter )) break ; if ( cudaD3D11GetDevice ( & dev , adapter ) == cudaSuccess ) break ; adapter -> Release (); } factory -> Release (); // Create swap chain and device ... sFnPtr_D3D11CreateDeviceAndSwapChain ( adapter , D3D11_DRIVER_TYPE_HARDWARE , 0 , D3D11_CREATE_DEVICE_DEBUG , featureLevels , 3 , D3D11_SDK_VERSION , & swapChainDesc , & swapChain , & device , & featureLevel , & deviceContext ); adapter -> Release (); // Use the same device cudaSetDevice ( dev ); // Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); D3D11_BUFFER_DESC bufferDesc ; bufferDesc . Usage = D3D11_USAGE_DEFAULT ; bufferDesc . BindFlags = D3D11_BIND_VERTEX_BUFFER ; bufferDesc . MiscFlags = 0 ; device -> CreateBuffer ( & bufferDesc , 0 , & positionsVB ); cudaGraphicsD3D11RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard ); // Launch rendering loop while (...) { ... y ; // Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2.0f - 1.0f ; v = v * 2.0f - 1.0f ; // Calculate simple sine wave pattern float freq = 4.0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0.5f ; // Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3.2.15.3. SLI Interoperability \\uf0c1 In a system with multiple GPUs, all CUDA-enabled GPUs are accessible via the CUDA driver and runtime as separate devices. There are however special considerations as described below when the system is in SLI mode. First, an allocation in one CUDA device on one GPU will consume memory on other GPUs that are part of the SLI configuration of the Direct3D or OpenGL device. Because of this, allocations may fail earlier than otherwise expected. Second, applications should create multiple CUDA contexts, one for each GPU in the SLI configuration. While this is not a strict requirement, it avoids unnecessary data transfers between devices. The application can use the cudaD3D[9|10|11]GetDevices() for Direct3D and cudaGLGetDevices() for OpenGL set of calls to identify the CUDA device handle(s) for the device(s) that are performing the rendering in the current and next frame. Given this information the application will typically choose the appropriate device and map Direct3D or OpenGL resources to the CUDA device returned by cudaD3D[9|10|11]GetDevices() or cudaGLGetDevices() when the deviceList parameter is set to cudaD3D[9|10|11]DeviceListCurrentFrame or cudaGLDeviceListCurrentFrame . Please note that resource returned from cudaGraphicsD9D[9|10|11]RegisterResource and cudaGraphicsGLRegister[Buffer|Image] must be only used on device the registration happened. Therefore on SLI configurations when data for different frames is computed on different CUDA devices it is necessary to register the resources for each separately. See Direct3D Interoperability and OpenGL Interoperability for details on how the CUDA runtime interoperate with Direct3D and OpenGL, respectively. 3.2.16. External Resource Interoperability \\uf0c1 External resource interoperability allows CUDA to import certain resources that are explicitly exported by other APIs. These objects are typically exported by other APIs using handles native to the Operating System, like file descriptors on Linux or NT handles on Windows. They could also be exported using other unified interfaces such as the NVIDIA Software Communication Interface. There are two types of resources that can be imported: memory objects and synchronization objects. Memory objects can be imported into CUDA using cudaImportExternalMemory() . An imported memory object can be accessed from within kernels using device pointers mapped onto the memory object via cudaExternalMemoryGetMappedBuffer() or CUDA mipmapped arrays mapped via cudaExternalMemoryGetMappedMipmappedArray() . Depending on the type of memory object, it may be possible for more than one mapping to be setup on a single memory object. The mappings must match the mappings setup in the exporting API. Any mismatched mappings result in undefined behavior. Imported memory objects must be freed using cudaDestroyExternalMemory() . Freeing a memory object does not free any mappings to that object. Therefore, any device pointers mapped onto that object must be explicitly freed using cudaFree() and any CUDA mipmapped arrays mapped onto that object must be explicitly freed using cudaFreeMipmappedArray() . It is illegal to access mappings to an object after it has been destroyed. Synchronization objects can be imported into CUDA using cudaImportExternalSemaphore() . An imported synchronization object can then be signaled using cudaSignalExternalSemaphoresAsync() and waited on using cudaWaitExternalSemaphoresAsync() . It is illegal to issue a wait before the corresponding signal has been issued. Also, depending on the type of the imported synchronization object, there may be additional constraints imposed on how they can be signaled and waited on, as described in subsequent sections. Imported semaphore objects must be freed using cudaDestroyExternalSemaphore() . All outstanding signals and waits must have completed before the semaphore object is destroyed. 3.2.16.1.'},\n",
       " {'id': 226,\n",
       "  'content': 'Vulkan Interoperability \\uf0c1 3.2.16.1.1. Matching device UUIDs \\uf0c1 When importing memory and synchronization objects exported by Vulkan, they must be imported and mapped on the same device as they were created on. The CUDA device that corresponds to the Vulkan physical device on which the objects were created can be determined by comparing the UUID of a CUDA device with that of the Vulkan physical device, as shown in the following code sample. Note that the Vulkan physical device should not be part of a device group that contains more than one Vulkan physical device. The device group as returned by vkEnumeratePhysicalDeviceGroups that contains the given Vulkan physical device must have a physical device count of 1. int getCudaDeviceForVulkanPhysicalDevice ( VkPhysicalDevice vkPhysicalDevice ) { VkPhysicalDeviceIDProperties vkPhysicalDeviceIDProperties = {}; vkPhysicalDeviceIDProperties . sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_ID_PROPERTIES ; vkPhysicalDeviceIDProperties . pNext = NULL ; VkPhysicalDeviceProperties2 vkPhysicalDeviceProperties2 = {}; vkPhysicalDeviceProperties2 . sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_PROPERTIES_2 ; vkPhysicalDeviceProperties2 . pNext = & vkPhysicalDeviceIDProperties ; vkGetPhysicalDeviceProperties2 ( vkPhysicalDevice , & vkPhysicalDeviceProperties2 ); int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice GetAdapterLuid (); int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice QueryInterface ( __uuidof ( IDXGIDevice ), ( void ** ) & dxgiDevice ); IDXGIAdapter * dxgiAdapter ; dxgiDevice -> GetAdapter ( & dxgiAdapter ); DXGI_ADAPTER_DESC dxgiAdapterDesc ; dxgiAdapter -> GetDesc ( & dxgiAdapterDesc ); LUID d3d11Luid = dxgiAdapterDesc . AdapterLuid ; int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice ( bufattrs [ 0 ].'},\n",
       " {'id': 227,\n",
       "  'content': 'value )); // Note cache and compression are per GPU attributes, so read values for specific gpu by comparing UUID // Read cacheability granted by NvSciBuf int numGpus = bufattrs [ 1 ]. len / sizeof ( NvSciBufAttrValGpuCache ); NvSciBufAttrValGpuCache [] cacheVal = ( NvSciBufAttrValGpuCache * ) bufattrs [ 1 ]. value ; bool ret_cacheVal ; for ( int i = 0 ; i >> ( array , arrayCount ); cudaDeviceSynchronize (); // If interested, the occupancy can be calculated with // cudaOccupancyMaxActiveBlocksPerMultiprocessor return 0 ; } The following code sample shows how to use the cluster occupancy API to find the max number of active clusters of a given size. Example code below calucaltes occupancy for cluster of size 2 and 128 threads per block. Cluster size of 8 is forward compatible starting compute capability 9.0, except on GPU hardware or MIG configurations which are too small to support 8 multiprocessors in which case the maximum cluster size will be reduced. But it is recommended that the users query the maximum cluster size before launching a cluster kernel. Max cluster size can be queried using cudaOccupancyMaxPotentialClusterSize API. { cudaLaunchConfig_t config = { 0 }; config . gridDim = number_of_blocks ; config . blockDim = 128 ; // threads_per_block = 128 config . dynamicSmemBytes = dynamic_shared_memory_size ; cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ]. x = 2 ; // cluster_size = 2 attribute [ 0 ]. numAttrs = 1 ; int max_cluster_size = 0 ; cudaOccupancyMaxPotentialClusterSize ( & max_cluster_size , ( void * ) kernel , & config ); int max_active_clusters = 0 ; cudaOccupancyMaxActiveClusters ( & max_active_clusters , ( void * ) kernel , & config ); std :: cout /include/cuda_occupancy.h for any use cases that cannot depend on the CUDA software stack. The Nsight Compute version of the occupancy calculator is particularly useful as a learning tool that visualizes the impact of changes to the parameters that affect occupancy (block size, registers per thread, and shared memory per thread). 5.3.'},\n",
       " {'id': 228,\n",
       "  'content': 'Maximize Memory Throughput \\uf0c1 The first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth. That means minimizing data transfers between the host and the device, as detailed in Data Transfer between Host and Device , since these have much lower bandwidth than data transfers between global memory and the device. That also means minimizing data transfers between global memory and the device by maximizing use of on-chip memory: shared memory and caches (i.e., L1 cache and L2 cache available on devices of compute capability 2.x and higher, texture cache and constant cache available on all devices). Shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it. As illustrated in CUDA Runtime , a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have each thread of a block: Load data from device memory to shared memory, Synchronize with all the other threads of the block so that each thread can safely read shared memory locations that were populated by different threads, Process the data in shared memory, Synchronize again if necessary to make sure that shared memory has been updated with the results, Write the results back to device memory. For some applications (for example, for which global memory access patterns are data-dependent), a traditional hardware-managed cache is more appropriate to exploit data locality. As mentioned in Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 , for devices of compute capability 7.x, 8.x and 9.0, the same on-chip memory is used for both L1 and shared memory, and how much of it is dedicated to L1 versus shared memory is configurable for each kernel call. The throughput of memory accesses by a kernel can vary by an order of magnitude depending on access pattern for each type of memory. The next step in maximizing memory throughput is therefore to organize memory accesses as optimally as possible based on the optimal memory access patterns described in Device Memory Accesses . This optimization is especially important for global memory accesses as global memory bandwidth is low compared to available on-chip bandwidths and arithmetic instruction throughput, so non-optimal global memory accesses generally have a high impact on performance. 5.3.1. Data Transfer between Host and Device \\uf0c1 Applications should strive to minimize data transfer between the host and the device. One way to accomplish this is to move more code from the host to the device, even if that means running kernels that do not expose enough parallelism to execute on the device with full efficiency. Intermediate data structures may be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory. Also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always performs better than making each transfer separately. On systems with a front-side bus, higher performance for data transfers between host and device is achieved by using page-locked host memory as described in Page-Locked Host Memory . In addition, when using mapped page-locked memory ( Mapped Memory ), there is no need to allocate any device memory and explicitly copy data between device and host memory. Data transfers are implicitly performed each time the kernel accesses the mapped memory. For maximum performance, these memory accesses must be coalesced as with accesses to global memory (see Device Memory Accesses ). Assuming that they are and that the mapped memory is read or written only once, using mapped page-locked memory instead of explicit copies between device and host memory can be a win for performance. On integrated systems where device memory and host memory are physically the same, any copy between host and device memory is superfluous and mapped page-locked memory should be used instead. Applications may query a device is integrated by checking that the integrated device property (see Device Enumeration ) is equal to 1. 5.3.2. Device Memory Accesses \\uf0c1 An instruction that accesses addressable memory (i.e., global, local, shared, constant, or texture memory) might need to be re-issued multiple times depending on the distribution of the memory addresses across the threads within the warp. How the distribution affects the instruction throughput this way is specific to each type of memory and described in the following sections. For example, for global memory, as a general rule, the more scattered the addresses are, the more reduced the throughput is. Global Memory Global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions. These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple of their size) can be read or written by memory transactions. When a warp executes an instruction that accesses global memory, it coalesces the memory accesses of the threads within the warp into one or more of these memory transactions depending on the size of the word accessed by each thread and the distribution of the memory addresses across the threads. In general, the more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly. For example, if a 32-byte memory transaction is generated for each thread’s 4-byte access, throughput is divided by 8. How many transactions are necessary and how much throughput is ultimately affected varies with the compute capability of the device. Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 give more details on how global memory accesses are handled for various compute capabilities. To maximize global memory throughput, it is therefore important to maximize coalescing by: Following the most optimal access patterns based on Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 Using data types that meet the size and alignment requirement detailed in the section Size and Alignment Requirement below, Padding data in some cases, for example, when accessing a two-dimensional array as described in the section Two-Dimensional Arrays below. Size and Alignment Requirement Global memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes. Any access (via a variable or a pointer) to data residing in global memory compiles to a single global memory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is naturally aligned (i.e., its address is a multiple of that size). If this size and alignment requirement is not fulfilled, the access compiles to multiple instructions with interleaved access patterns that prevent these instructions from fully coalescing. It is therefore recommended to use types that meet this requirement for data that resides in global memory. The alignment requirement is automatically fulfilled for the Built-in Vector Types . For structures, the size and alignment requirements can be enforced by the compiler using the alignment specifiers __align__(8) or __align__(16) , such as struct __align__ ( 8 ) { float x ; float y ; }; or struct __align__ ( 16 ) { float x ; float y ; float z ; }; Any address of a variable residing in global memory or returned by one of the memory allocation routines from the driver or runtime API is always aligned to at least 256 bytes. Reading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words), so special care must be taken to maintain alignment of the starting address of any value or array of values of these types. A typical case where this might be easily overlooked is when using some custom global memory allocation scheme, whereby the allocations of multiple arrays (with multiple calls to cudaMalloc() or cuMemAlloc() ) is replaced by the allocation of a single large block of memory partitioned into multiple arrays, in which case the starting address of each array is offset from the block’s starting address. Two-Dimensional Arrays A common global memory access pattern is when each thread of index (tx,ty) uses the following address to access one element of a 2D array of width width , located at address BaseAddress of type type* (where type meets the requirement described in Maximize Utilization ): BaseAddress + width * ty + tx For these accesses to be fully coalesced, both the width of the thread block and the width of the array must be a multiple of the warp size. In particular, this means that an array whose width is not a multiple of this size will be accessed much more efficiently if it is actually allocated with a width rounded up to the closest multiple of this size and its rows padded accordingly. The cudaMallocPitch() and cuMemAllocPitch() functions and associated memory copy functions described in the reference manual enable programmers to write non-hardware-dependent code to allocate arrays that conform to these constraints. Local Memory Local memory accesses only occur for some automatic variables as mentioned in Variable Memory Space Specifiers . Automatic variables that the compiler is likely to place in local memory are: Arrays for which it cannot determine that they are indexed with constant quantities, Large structures or arrays that would consume too much register space, Any variable if the kernel uses more registers than available (this is also known as register spilling ). Inspection of the PTX assembly code (obtained by compiling with the -ptx or -keep option) will tell if a variable has been placed in local memory during the first compilation phases as it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics. Even if it has not, subsequent compilation phases might still decide otherwise though if they find it consumes too much register space for the targeted architecture: Inspection of the cubin object using cuobjdump will tell if this is the case. Also, the compiler reports total local memory usage per kernel ( lmem ) when compiling with the --ptxas-options=-v option. Note that some mathematical functions have implementation paths that might access local memory. The local memory space resides in device memory, so local memory accesses have the same high latency and low bandwidth as global memory accesses and are subject to the same requirements for memory coalescing as described in Device Memory Accesses . Local memory is however organized such that consecutive 32-bit words are accessed by consecutive thread IDs. Accesses are therefore fully coalesced as long as all threads in a warp access the same relative address (for example, same index in an array variable, same member in a structure variable). On devices of compute capability 5.x onwards, local memory accesses are always cached in L2 in the same way as global memory accesses (see Compute Capability 5.x and Compute Capability 6.x ). Shared Memory Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory. To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously. Any memory read or write request made of n addresses that fall in n distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that is n times as high as the bandwidth of a single module. However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized. The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests. If the number of separate memory requests is n , the initial memory request is said to cause n -way bank conflicts. To get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts. This is described in Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x , and Compute Capability 9.0 for devices of compute capability 5.x, 6.x, 7.x, 8.x, and 9.0 respectively. Constant Memory The constant memory space resides in device memory and is cached in the constant cache. A request is then split into as many separate requests as there are different memory addresses in the initial request, decreasing throughput by a factor equal to the number of separate requests. The resulting requests are then serviced at the throughput of the constant cache in case of a cache hit, or at the throughput of device memory otherwise. Texture and Surface Memory The texture and surface memory spaces reside in device memory and are cached in texture cache, so a texture fetch or surface read costs one memory read from device memory only on a cache miss, otherwise it just costs one read from texture cache. The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture or surface addresses that are close together in 2D will achieve best performance. Also, it is designed for streaming fetches with a constant latency; a cache hit reduces DRAM bandwidth demand but not fetch latency. Reading device memory through texture or surface fetching present some benefits that can make it an advantageous alternative to reading device memory from global or constant memory: If the memory reads do not follow the access patterns that global or constant memory reads must follow to get good performance, higher bandwidth can be achieved providing that there is locality in the texture fetches or surface reads; Addressing calculations are performed outside the kernel by dedicated units; Packed data may be broadcast to separate variables in a single operation; 8-bit and 16-bit integer input data may be optionally converted to 32 bit floating-point values in the range [0.0, 1.0] or [-1.0, 1.0] (see Texture Memory ). 5.4. Maximize Instruction Throughput \\uf0c1 To maximize instruction throughput the application should: Minimize the use of arithmetic instructions with low throughput; this includes trading precision for speed when it does not affect the end result, such as using intrinsic instead of regular functions (intrinsic functions are listed in Intrinsic Functions ), single-precision instead of double-precision, or flushing denormalized numbers to zero; Minimize divergent warps caused by control flow instructions as detailed in Control Flow Instructions Reduce the number of instructions, for example, by optimizing out synchronization points whenever possible as described in Synchronization Instruction or by using restricted pointers as described in __restrict__ . In this section, throughputs are given in number of operations per clock cycle per multiprocessor. For a warp size of 32, one instruction corresponds to 32 operations, so if N is the number of operations per clock cycle, the instruction throughput is N/32 instructions per clock cycle. All throughputs are for one multiprocessor. They must be multiplied by the number of multiprocessors in the device to get throughput for the whole device. 5.4.1. Arithmetic Instructions \\uf0c1 The following table gives the throughputs of the arithmetic instructions that are natively supported in hardware for devices of various compute capabilities. Table 4 Throughput of Native Arithmetic Instructions. (Number of Results per Clock Cycle per Multiprocessor) \\uf0c1 Compute Capability 5.0, 5.2 5.3 6.0 6.1 6.2 7.x 8.0 8.6 8.9 9.0 16-bit floating-point add, multiply, multiply-add N/A 256 128 2 256 128 256 3 128 256 32-bit floating-point add, multiply, multiply-add 128 64 128 64 128 64-bit floating-point add, multiply, multiply-add 4 32 4 32 5 32 2 64 32-bit floating-point reciprocal, reciprocal square root, base-2 logarithm ( __log2f ), base 2 exponential ( exp2f ), sine ( __sinf ), cosine ( __cosf ) 32 16 32 16 32-bit integer add, extended-precision add, subtract, extended-precision subtract 128 64 128 64 32-bit integer multiply, multiply-add, extended-precision multiply-add Multiple instruct. 64 6 24-bit integer multiply ( __[u]mul24 ) Multiple instruct. 32-bit integer shift 64 32 64 compare, minimum, maximum 64 32 64 32-bit integer bit reverse 64 32 64 16 Bit field extract/insert 64 32 64 Multiple Instruct. 64 32-bit bitwise AND, OR, XOR 128 64 128 64 count of leading zeros, most significant non-sign bit 32 16 32 16 population count 32 16 32 16 warp shuffle 32 32 8 32 warp reduce Multiple instruct. 16 warp vote 64 sum of absolute difference 64 32 64 SIMD video instructions vabsdiff2 Multiple instruct. SIMD video instructions vabsdiff4 Multiple instruct. 64 All other SIMD video instructions Multiple instruct. Type conversions from 8-bit and 16-bit integer to 32-bit integer types 32 16 32 64 Type conversions from and to 64-bit types 4 16 4 16 10 16 2 2 16 All other type conversions 32 16 32 16 16-bit DPX Multiple instruct. 128 32-bit DPX Multiple instruct. 64 Other instructions and functions are implemented on top of the native instructions. The implementation may be different for devices of different compute capabilities, and the number of native instructions after compilation may fluctuate with every compiler version. For complicated functions, there can be multiple code paths depending on input. cuobjdump can be used to inspect a particular implementation in a cubin object. The implementation of some functions are readily available on the CUDA header files ( math_functions.h , device_functions.h , …). In general, code compiled with -ftz=true (denormalized numbers are flushed to zero) tends to have higher performance than code compiled with -ftz=false . Similarly, code compiled with -prec-div=false (less precise division) tends to have higher performance code than code compiled with -prec-div=true , and code compiled with -prec-sqrt=false (less precise square root) tends to have higher performance than code compiled with -prec-sqrt=true . The nvcc user manual describes these compilation flags in more details. Single-Precision Floating-Point Division __fdividef(x, y) (see Intrinsic Functions ) provides faster single-precision floating-point division than the division operator. Single-Precision Floating-Point Reciprocal Square Root To preserve IEEE-754 semantics the compiler can optimize 1.0/sqrtf() into rsqrtf() only when both reciprocal and square root are approximate, (i.e., with -prec-div=false and -prec-sqrt=false ). It is therefore recommended to invoke rsqrtf() directly where desired. Single-Precision Floating-Point Square Root Single-precision floating-point square root is implemented as a reciprocal square root followed by a reciprocal instead of a reciprocal square root followed by a multiplication so that it gives correct results for 0 and infinity. Sine and Cosine sinf(x) , cosf(x) , tanf(x) , sincosf(x) , and corresponding double-precision instructions are much more expensive and even more so if the argument x is large in magnitude. More precisely, the argument reduction code (see Mathematical Functions for implementation) comprises two code paths referred to as the fast path and the slow path, respectively. The fast path is used for arguments sufficiently small in magnitude and essentially consists of a few multiply-add operations. The slow path is used for arguments large in magnitude and consists of lengthy computations required to achieve correct results over the entire argument range. At present, the argument reduction code for the trigonometric functions selects the fast path for arguments whose magnitude is less than 105615.0f for the single-precision functions, and less than 2147483648.0 for the double-precision functions. As the slow path requires more registers than the fast path, an attempt has been made to reduce register pressure in the slow path by storing some intermediate variables in local memory, which may affect performance because of local memory high latency and bandwidth (see Device Memory Accesses ). At present, 28 bytes of local memory are used by single-precision functions, and 44 bytes are used by double-precision functions. However, the exact amount is subject to change. Due to the lengthy computations and use of local memory in the slow path, the throughput of these trigonometric functions is lower by one order of magnitude when the slow path reduction is required as opposed to the fast path reduction. Integer Arithmetic Integer division and modulo operation are costly as they compile to up to 20 instructions. They can be replaced with bitwise operations in some cases: If n is a power of 2, ( i/n ) is equivalent to (i>>log2(n)) and (i%n) is equivalent to ( i&(n-1) ); the compiler will perform these conversions if n is literal. __brev and __popc map to a single instruction and __brevll and __popcll to a few instructions. __[u]mul24 are legacy intrinsic functions that no longer have any reason to be used. Half Precision Arithmetic In order to achieve good performance for 16-bit precision floating-point add, multiply or multiply-add, it is recommended that the half2 datatype is used for half precision and __nv_bfloat162 be used for __nv_bfloat16 precision. Vector intrinsics (for example, __hadd2 , __hsub2 , __hmul2 , __hfma2 ) can then be used to do two operations in a single instruction. Using half2 or __nv_bfloat162 in place of two calls using half or __nv_bfloat16 may also help performance of other intrinsics, such as warp shuffles. The intrinsic __halves2half2 is provided to convert two half precision values to the half2 datatype. The intrinsic __halves2bfloat162 is provided to convert two __nv_bfloat precision values to the __nv_bfloat162 datatype. Type Conversion Sometimes, the compiler must insert conversion instructions, introducing additional execution cycles. This is the case for: Functions operating on variables of type char or short whose operands generally need to be converted to int , Double-precision floating-point constants (i.e., those constants defined without any type suffix) used as input to single-precision floating-point computations (as mandated by C/C++ standards). This last case can be avoided by using single-precision floating-point constants, defined with an f suffix such as 3.141592653589793f , 1.0f , 0.5f .'},\n",
       " {'id': 229,\n",
       "  'content': '5.4.2. Control Flow Instructions \\uf0c1 Any flow control instruction ( if , switch , do , for , while ) can significantly impact the effective instruction throughput by causing threads of the same warp to diverge (i.e., to follow different execution paths). If this happens, the different executions paths have to be serialized, increasing the total number of instructions executed for this warp. To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps. This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture . A trivial example is when the controlling condition only depends on ( threadIdx / warpSize ) where warpSize is the warp size. In this case, no warp diverges since the controlling condition is perfectly aligned with the warps. Sometimes, the compiler may unroll loops or it may optimize out short if or switch blocks by using branch predication instead, as detailed below. In these cases, no warp can ever diverge. The programmer can also control loop unrolling using the #pragma unroll directive (see #pragma unroll ). When using branch predication none of the instructions whose execution depends on the controlling condition gets skipped. Instead, each of them is associated with a per-thread condition code or predicate that is set to true or false based on the controlling condition and although each of these instructions gets scheduled for execution, only the instructions with a true predicate are actually executed. Instructions with a false predicate do not write results, and also do not evaluate addresses or read operands. 5.4.3. Synchronization Instruction \\uf0c1 Throughput for __syncthreads() is 32 operations per clock cycle for devices of compute capability 6.0, 16 operations per clock cycle for devices of compute capability 7.x as well as 8.x and 64 operations per clock cycle for devices of compute capability 5.x, 6.1 and 6.2. Note that __syncthreads() can impact performance by forcing the multiprocessor to idle as detailed in Device Memory Accesses . 5.5. Minimize Memory Thrashing \\uf0c1 Applications that constantly allocate and free memory too often may find that the allocation calls tend to get slower over time up to a limit. This is typically expected due to the nature of releasing memory back to the operating system for its own use. For best performance in this regard, we recommend the following: Try to size your allocation to the problem at hand. Don’t try to allocate all available memory with cudaMalloc / cudaMallocHost / cuMemCreate , as this forces memory to be resident immediately and prevents other applications from being able to use that memory. This can put more pressure on operating system schedulers, or just prevent other applications using the same GPU from running entirely. Try to allocate memory in appropriately sized allocations early in the application and allocations only when the application does not have any use for it. Reduce the number of cudaMalloc + cudaFree calls in the application, especially in performance-critical regions. If an application cannot allocate enough device memory, consider falling back on other memory types such as cudaMallocHost or cudaMallocManaged , which may not be as performant, but will enable the application to make progress. For platforms that support the feature, cudaMallocManaged allows for oversubscription, and with the correct cudaMemAdvise policies enabled, will allow the application to retain most if not all the performance of cudaMalloc . cudaMallocManaged also won’t force an allocation to be resident until it is needed or prefetched, reducing the overall pressure on the operating system schedulers and better enabling multi-tenet use cases. 3 128 for __nv_bfloat16 4 8 for GeForce GPUs, except for Titan GPUs 5 2 for compute capability 7.5 GPUs 6 32 for extended-precision 7 32 for GeForce GPUs, except for Titan GPUs 8 16 for compute capabilities 7.5 GPUs 9 8 for GeForce GPUs, except for Titan GPUs 10 2 for compute capabilities 7.5 GPUs 6. CUDA-Enabled GPUs \\uf0c1 https://developer.nvidia.com/cuda-gpus lists all CUDA-enabled devices with their compute capability. The compute capability, number of multiprocessors, clock frequency, total amount of device memory, and other properties can be queried using the runtime (see reference manual). 7.'},\n",
       " {'id': 230,\n",
       "  'content': 'C++ Language Extensions \\uf0c1 7.1. Function Execution Space Specifiers \\uf0c1 Function execution space specifiers denote whether a function executes on the host or on the device and whether it is callable from the host or from the device. 7.1.1. __global__ \\uf0c1 The __global__ execution space specifier declares a function as being a kernel. Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 5.0 or higher (see CUDA Dynamic Parallelism for more details). A __global__ function must have void return type, and cannot be a member of a class. Any call to a __global__ function must specify its execution configuration as described in Execution Configuration . A call to a __global__ function is asynchronous, meaning it returns before the device has completed its execution. 7.1.2. __device__ \\uf0c1 The __device__ execution space specifier declares a function that is: Executed on the device, Callable from the device only. The __global__ and __device__ execution space specifiers cannot be used together. 7.1.3. __host__ \\uf0c1 The __host__ execution space specifier declares a function that is: Executed on the host, Callable from the host only. It is equivalent to declare a function with only the __host__ execution space specifier or to declare it without any of the __host__ , __device__ , or __global__ execution space specifier; in either case the function is compiled for the host only. The __global__ and __host__ execution space specifiers cannot be used together. The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device: __host__ __device__ func () { #if __CUDA_ARCH__ >= 800 // Device code path for compute capability 8.x #elif __CUDA_ARCH__ >= 700 // Device code path for compute capability 7.x #elif __CUDA_ARCH__ >= 600 // Device code path for compute capability 6.x #elif __CUDA_ARCH__ >= 500 // Device code path for compute capability 5.x #elif !defined(__CUDA_ARCH__) // Host code path #endif } 7.1.4. Undefined behavior \\uf0c1 A ‘cross-execution space’ call has undefined behavior when: __CUDA_ARCH__ is defined, a call from within a __global__ , __device__ or __host__ __device__ function to a __host__ function. __CUDA_ARCH__ is undefined, a call from within a __host__ function to a __device__ function. 9 7.1.5. __noinline__ and __forceinline__ \\uf0c1 The compiler inlines any __device__ function when deemed appropriate. The __noinline__ function qualifier can be used as a hint for the compiler not to inline the function if possible. The __forceinline__ function qualifier can be used to force the compiler to inline the function. The __noinline__ and __forceinline__ function qualifiers cannot be used together, and neither function qualifier can be applied to an inline function. 7.1.6. __inline_hint__ \\uf0c1 The __inline_hint__ qualifier enables more aggressive inlining in the compiler. Unlike __forceinline__ , it does not imply that the function is inline. It can be used to improve inlining across modules when using LTO. Neither the __noinline__ nor the __forceinline__ function qualifier can be used with the __inline_hint__ function qualifier. 7.2. Variable Memory Space Specifiers \\uf0c1 Variable memory space specifiers denote the memory location on the device of a variable. An automatic variable declared in device code without any of the __device__ , __shared__ and __constant__ memory space specifiers described in this section generally resides in a register. However in some cases the compiler might choose to place it in local memory, which can have adverse performance consequences as detailed in Device Memory Accesses . 7.2.1. __device__ \\uf0c1 The __device__ memory space specifier declares a variable that resides on the device. At most one of the other memory space specifiers defined in the next three sections may be used together with __device__ to further denote which memory space the variable belongs to. If none of them is present, the variable: Resides in global memory space, Has the lifetime of the CUDA context in which it is created, Has a distinct object per device, Is accessible from all the threads within the grid and from the host through the runtime library (cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol() ). 7.2.2. __constant__ \\uf0c1 The __constant__ memory space specifier, optionally used together with __device__ , declares a variable that: Resides in constant memory space, Has the lifetime of the CUDA context in which it is created, Has a distinct object per device, Is accessible from all the threads within the grid and from the host through the runtime library ( cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol() ). The behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point of this grid’s lifetime is undefined. 7.2.3. __shared__ \\uf0c1 The __shared__ memory space specifier, optionally used together with __device__ , declares a variable that: Resides in the shared memory space of a thread block, Has the lifetime of the block, Has a distinct object per block, Is only accessible from all the threads within the block, Does not have a constant address. When declaring a variable in shared memory as an external array such as extern __shared__ float shared []; the size of the array is determined at launch time (see Execution Configuration ). All variables declared in this fashion, start at the same address in memory, so that the layout of the variables in the array must be explicitly managed through offsets. For example, if one wants the equivalent of short array0 [ 128 ]; float array1 [ 64 ]; int array2 [ 256 ]; in dynamically allocated shared memory, one could declare and initialize the arrays the following way: extern __shared__ float array []; __device__ void func () // __device__ or __global__ function { short * array0 = ( short * ) array ; float * array1 = ( float * ) & array0 [ 128 ]; int * array2 = ( int * ) & array1 [ 64 ]; } Note that pointers need to be aligned to the type they point to, so the following code, for example, does not work since array1 is not aligned to 4 bytes. extern __shared__ float array []; __device__ void func () // __device__ or __global__ function { short * array0 = ( short * ) array ; float * array1 = ( float * ) & array0 [ 127 ]; } Alignment requirements for the built-in vector types are listed in Table 5 . 7.2.4. __grid_constant__ \\uf0c1 The __grid_constant__ annotation for compute architectures greater or equal to 7.0 annotates a const -qualified __global__ function parameter of non-reference type that: Has the lifetime of the grid, Is private to the grid, i.e., the object is not accessible to host threads and threads from other grids, including sub-grids, Has a distinct object per grid, i.e., all threads in the grid see the same address, Is read-only, i.e., modifying a __grid_constant__ object or any of its sub-objects is undefined behavior , including mutable members. Requirements: Kernel parameters annotated with __grid_constant__ must have const -qualified non-reference types. All function declarations must match with respect to any __grid_constant_ parameters. A function template specialization must match the primary template declaration with respect to any __grid_constant__ parameters. A function template instantiation directive must match the primary template declaration with respect to any __grid_constant__ parameters. If the address of a __global__ function parameter is taken, the compiler will ordinarily make a copy of the kernel parameter in thread local memory and use the address of the copy, to partially support C++ semantics, which allow each thread to modify its own local copy of function parameters. Annotating a __global__ function parameter with __grid_constant__ ensures that the compiler will not create a copy of the kernel parameter in thread local memory, but will instead use the generic address of the parameter itself. Avoiding the local copy may result in improved performance. __device__ void unknown_function ( S const & ); __global__ void kernel ( const __grid_constant__ S s ) { s . x += threadIdx . x ; // Undefined Behavior: tried to modify read-only memory // Compiler will _not_ create a per-thread thread local copy of \"s\": unknown_function ( s ); } 7.2.5. __managed__ \\uf0c1 The __managed__ memory space specifier, optionally used together with __device__ , declares a variable that: Can be referenced from both device and host code, for example, its address can be taken or it can be read or written directly from a device or host function. Has the lifetime of an application. See __managed__ Memory Space Specifier for more details.'},\n",
       " {'id': 231,\n",
       "  'content': '7.2.6. __restrict__ \\uf0c1 nvcc supports restricted pointers via the __restrict__ keyword. Restricted pointers were introduced in C99 to alleviate the aliasing problem that exists in C-type languages, and which inhibits all kind of optimization from code re-ordering to common sub-expression elimination. Here is an example subject to the aliasing issue, where use of restricted pointer can help the compiler to reduce the number of instructions: void foo ( const float * a , const float * b , float * c ) { c [ 0 ] = a [ 0 ] * b [ 0 ]; c [ 1 ] = a [ 0 ] * b [ 0 ]; c [ 2 ] = a [ 0 ] * b [ 0 ] * a [ 1 ]; c [ 3 ] = a [ 0 ] * a [ 1 ]; c [ 4 ] = a [ 0 ] * b [ 0 ]; c [ 5 ] = b [ 0 ]; ... } In C-type languages, the pointers a , b , and c may be aliased, so any write through c could modify elements of a or b . This means that to guarantee functional correctness, the compiler cannot load a[0] and b[0] into registers, multiply them, and store the result to both c[0] and c[1] , because the results would differ from the abstract execution model if, say, a[0] is really the same location as c[0] . So the compiler cannot take advantage of the common sub-expression. Likewise, the compiler cannot just reorder the computation of c[4] into the proximity of the computation of c[0] and c[1] because the preceding write to c[3] could change the inputs to the computation of c[4] . By making a , b , and c restricted pointers, the programmer asserts to the compiler that the pointers are in fact not aliased, which in this case means writes through c would never overwrite elements of a or b . This changes the function prototype as follows: void foo ( const float * __restrict__ a , const float * __restrict__ b , float * __restrict__ c ); Note that all pointer arguments need to be made restricted for the compiler optimizer to derive any benefit. With the __restrict__ keywords added, the compiler can now reorder and do common sub-expression elimination at will, while retaining functionality identical with the abstract execution model: void foo ( const float * __restrict__ a , const float * __restrict__ b , float * __restrict__ c ) { float t0 = a [ 0 ]; float t1 = b [ 0 ]; float t2 = t0 * t1 ; float t3 = a [ 1 ]; c [ 0 ] = t2 ; c [ 1 ] = t2 ; c [ 4 ] = t2 ; c [ 2 ] = t2 * t3 ; c [ 3 ] = t0 * t3 ; c [ 5 ] = t1 ; ... } The effects here are a reduced number of memory accesses and reduced number of computations. This is balanced by an increase in register pressure due to “cached” loads and common sub-expressions. Since register pressure is a critical issue in many CUDA codes, use of restricted pointers can have negative performance impact on CUDA code, due to reduced occupancy. 7.3.'},\n",
       " {'id': 232,\n",
       "  'content': 'Built-in Vector Types \\uf0c1 7.3.1. char, short, int, long, longlong, float, double \\uf0c1 These are vector types derived from the basic integer and floating-point types. They are structures and the 1st, 2nd, 3rd, and 4th components are accessible through the fields x , y , z , and w , respectively. They all come with a constructor function of the form make_ ; for example, int2 make_int2 ( int x , int y ); which creates a vector of type int2 with value (x, y) . The alignment requirements of the vector types are detailed in the following table . Table 5 Alignment Requirements \\uf0c1 Type Alignment char1, uchar1 1 char2, uchar2 2 char3, uchar3 1 char4, uchar4 4 short1, ushort1 2 short2, ushort2 4 short3, ushort3 2 short4, ushort4 8 int1, uint1 4 int2, uint2 8 int3, uint3 4 int4, uint4 16 long1, ulong1 4 if sizeof(long) is equal to sizeof(int) 8, otherwise long2, ulong2 8 if sizeof(long) is equal to sizeof(int), 16, otherwise long3, ulong3 4 if sizeof(long) is equal to sizeof(int), 8, otherwise long4, ulong4 16 longlong1, ulonglong1 8 longlong2, ulonglong2 16 longlong3, ulonglong3 8 longlong4, ulonglong4 16 float1 4 float2 8 float3 4 float4 16 double1 8 double2 16 double3 8 double4 16 7.3.2. dim3 \\uf0c1 This type is an integer vector type based on uint3 that is used to specify dimensions.'},\n",
       " {'id': 233,\n",
       "  'content': 'When defining a variable of type dim3 , any component left unspecified is initialized to 1. 7.4. Built-in Variables \\uf0c1 Built-in variables specify the grid and block dimensions and the block and thread indices. They are only valid within functions that are executed on the device. 7.4.1. gridDim \\uf0c1 This variable is of type dim3 (see dim3 ) and contains the dimensions of the grid. 7.4.2. blockIdx \\uf0c1 This variable is of type uint3 (see char, short, int, long, longlong, float, double ) and contains the block index within the grid. 7.4.3. blockDim \\uf0c1 This variable is of type dim3 (see dim3 ) and contains the dimensions of the block. 7.4.4. threadIdx \\uf0c1 This variable is of type uint3 (see char, short, int, long, longlong, float, double ) and contains the thread index within the block. 7.4.5. warpSize \\uf0c1 This variable is of type int and contains the warp size in threads (see SIMT Architecture for the definition of a warp). 7.5. Memory Fence Functions \\uf0c1 The CUDA programming model assumes a device with a weakly-ordered memory model, that is the order in which a CUDA thread writes data to shared memory, global memory, page-locked host memory, or the memory of a peer device is not necessarily the order in which the data is observed being written by another CUDA or host thread. It is undefined behavior for two threads to read from or write to the same memory location without synchronization. In the following example, thread 1 executes writeXY() , while thread 2 executes readXY() . __device__ int X = 1 , Y = 2 ; __device__ void writeXY () { X = 10 ; Y = 20 ; } __device__ void readXY () { int B = Y ; int A = X ; } The two threads read and write from the same memory locations X and Y simultaneously. Any data-race is undefined behavior, and has no defined semantics. The resulting values for A and B can be anything. Memory fence functions can be used to enforce a sequentially-consistent ordering on memory accesses. The memory fence functions differ in the scope in which the orderings are enforced but they are independent of the accessed memory space (shared memory, global memory, page-locked host memory, and the memory of a peer device). void __threadfence_block (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_block) and ensures that: All writes to all memory made by the calling thread before the call to __threadfence_block() are observed by all threads in the block of the calling thread as occurring before all writes to all memory made by the calling thread after the call to __threadfence_block() ; All reads from all memory made by the calling thread before the call to __threadfence_block() are ordered before all reads from all memory made by the calling thread after the call to __threadfence_block() . void __threadfence (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_device) and ensures that no writes to all memory made by the calling thread after the call to __threadfence() are observed by any thread in the device as occurring before any write to all memory made by the calling thread before the call to __threadfence() . void __threadfence_system (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_system) and ensures that all writes to all memory made by the calling thread before the call to __threadfence_system() are observed by all threads in the device, host threads, and all threads in peer devices as occurring before all writes to all memory made by the calling thread after the call to __threadfence_system() . __threadfence_system() is only supported by devices of compute capability 2.x and higher. In the previous code sample, we can insert fences in the codes as follows: __device__ int X = 1 , Y = 2 ; __device__ void writeXY () { X = 10 ; __threadfence (); Y = 20 ; } __device__ void readXY () { int B = Y ; __threadfence (); int A = X ; } For this code, the following outcomes can be observed: A equal to 1 and B equal to 2, A equal to 10 and B equal to 2, A equal to 10 and B equal to 20. The fourth outcome is not possible, because the first write must be visible before the second write. If thread 1 and 2 belong to the same block, it is enough to use __threadfence_block() . If thread 1 and 2 do not belong to the same block, __threadfence() must be used if they are CUDA threads from the same device and __threadfence_system() must be used if they are CUDA threads from two different devices. A common use case is when threads consume some data produced by other threads as illustrated by the following code sample of a kernel that computes the sum of an array of N numbers in one call. Each block first sums a subset of the array and stores the result in global memory. When all blocks are done, the last block done reads each of these partial sums from global memory and sums them to obtain the final result. In order to determine which block is finished last, each block atomically increments a counter to signal that it is done with computing and storing its partial sum (see Atomic Functions about atomic functions). The last block is the one that receives the counter value equal to gridDim.x-1 . If no fence is placed between storing the partial sum and incrementing the counter, the counter might increment before the partial sum is stored and therefore, might reach gridDim.x-1 and let the last block start reading partial sums before they have been actually updated in memory. Memory fence functions only affect the ordering of memory operations by a thread; they do not, by themselves, ensure that these memory operations are visible to other threads (like __syncthreads() does for threads within a block (see Synchronization Functions )). In the code sample below, the visibility of memory operations on the result variable is ensured by declaring it as volatile (see Volatile Qualifier ). __device__ unsigned int count = 0 ; __shared__ bool isLastBlockDone ; __global__ void sum ( const float * array , unsigned int N , volatile float * result ) { // Each block sums a subset of the input array. float partialSum = calculatePartialSum ( array , N ); if ( threadIdx . x == 0 ) { // Thread 0 of each block stores the partial sum // to global memory. The compiler will use // a store operation that bypasses the L1 cache // since the \"result\" variable is declared as // volatile. This ensures that the threads of // the last block will read the correct partial // sums computed by all other blocks. result [ blockIdx . x ] = partialSum ; // Thread 0 makes sure that the incrementing // of the \"count\" variable is only performed after // the partial sum has been written to global memory. __threadfence (); // Thread 0 signals that it is done. unsigned int value = atomicInc ( & count , gridDim . x ); // Thread 0 determines if its block is the last // block to be done. isLastBlockDone = ( value == ( gridDim . x - 1 )); } // Synchronize to make sure that each thread reads // the correct value of isLastBlockDone. __syncthreads (); if ( isLastBlockDone ) { // The last block sums the partial sums // stored in result[0 .. gridDim.x-1] float totalSum = calculateTotalSum ( result ); if ( threadIdx . x == 0 ) { // Thread 0 of last block stores the total sum // to global memory and resets the count // variable, so that the next kernel call // works properly. result [ 0 ] = totalSum ; count = 0 ; } } } 7.6. Synchronization Functions \\uf0c1 void __syncthreads (); waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to __syncthreads() are visible to all threads in the block. __syncthreads() is used to coordinate communication between the threads of the same block. When some threads within a block access the same addresses in shared or global memory, there are potential read-after-write, write-after-read, or write-after-write hazards for some of these memory accesses. These data hazards can be avoided by synchronizing threads in-between these accesses. __syncthreads() is allowed in conditional code but only if the conditional evaluates identically across the entire thread block, otherwise the code execution is likely to hang or produce unintended side effects. Devices of compute capability 2.x and higher support three variations of __syncthreads() described below. int __syncthreads_count ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero. int __syncthreads_and ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them. int __syncthreads_or ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them. void __syncwarp ( unsigned mask = 0xffffffff ); will cause the executing thread to wait until all warp lanes named in mask have executed a __syncwarp() (with the same mask) before resuming execution. Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute a corresponding __syncwarp() with the same mask, or the result is undefined. Executing __syncwarp() guarantees memory ordering among threads participating in the barrier. Thus, threads within a warp that wish to communicate via memory can store to memory, execute __syncwarp() , and then safely read values stored by other threads in the warp. Note For .target sm_6x or below, all threads in mask must execute the same __syncwarp() in convergence, and the union of all values in mask must be equal to the active mask. Otherwise, the behavior is undefined.'},\n",
       " {'id': 234,\n",
       "  'content': '7.7. Mathematical Functions \\uf0c1 The reference manual lists all C/C++ standard library mathematical functions that are supported in device code and all intrinsic functions that are only supported in device code. Mathematical Functions provides accuracy information for some of these functions when relevant. 7.8. Texture Functions \\uf0c1 Texture objects are described in Texture Object API Texture fetching is described in Texture Fetching . 7.8.1. Texture Object API \\uf0c1 7.8.1.1. tex1Dfetch() \\uf0c1 template T tex1Dfetch ( cudaTextureObject_t texObj , int x ); fetches from the region of linear memory specified by the one-dimensional texture object texObj using integer texture coordinate x . tex1Dfetch() only works with non-normalized coordinates, so only the border and clamp addressing modes are supported. It does not perform any texture filtering. For integer types, it may optionally promote the integer to single-precision floating point.'},\n",
       " {'id': 235,\n",
       "  'content': '7.8.1.2. tex1D() \\uf0c1 template T tex1D ( cudaTextureObject_t texObj , float x ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x . 7.8.1.3. tex1DLod() \\uf0c1 template T tex1DLod ( cudaTextureObject_t texObj , float x , float level ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x at the level-of-detail level . 7.8.1.4. tex1DGrad() \\uf0c1 template T tex1DGrad ( cudaTextureObject_t texObj , float x , float dx , float dy ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x . The level-of-detail is derived from the X-gradient dx and Y-gradient dy . 7.8.1.5. tex2D() \\uf0c1 template T tex2D ( cudaTextureObject_t texObj , float x , float y ); fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y) . 7.8.1.6. tex2D() for sparse CUDA arrays \\uf0c1 template T tex2D ( cudaTextureObject_t texObj , float x , float y , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) . Also returns whether the texel is resident in memory via isResident pointer.'},\n",
       " {'id': 236, 'content': 'If not, the values fetched will be zeros.'},\n",
       " {'id': 237,\n",
       "  'content': '7.8.1.7. tex2Dgather() \\uf0c1 template T tex2Dgather ( cudaTextureObject_t texObj , float x , float y , int comp = 0 ); fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather . 7.8.1.8. tex2Dgather() for sparse CUDA arrays \\uf0c1 template T tex2Dgather ( cudaTextureObject_t texObj , float x , float y , bool * isResident , int comp = 0 ); fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather . 7.8.1.9. tex2DGrad() \\uf0c1 template T tex2DGrad ( cudaTextureObject_t texObj , float x , float y , float2 dx , float2 dy ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) . The level-of-detail is derived from the dx and dy gradients. 7.8.1.10. tex2DGrad() for sparse CUDA arrays \\uf0c1 template T tex2DGrad ( cudaTextureObject_t texObj , float x , float y , float2 dx , float2 dy , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) . 7.8.1.11. tex2DLod() \\uf0c1 template tex2DLod ( cudaTextureObject_t texObj , float x , float y , float level ); fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level . 7.8.1.12. tex2DLod() for sparse CUDA arrays \\uf0c1 template tex2DLod ( cudaTextureObject_t texObj , float x , float y , float level , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level . 7.8.1.13. tex3D() \\uf0c1 template T tex3D ( cudaTextureObject_t texObj , float x , float y , float z ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) . 7.8.1.14. tex3D() for sparse CUDA arrays \\uf0c1 template T tex3D ( cudaTextureObject_t texObj , float x , float y , float z , bool * isResident ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) . 7.8.1.15. tex3DLod() \\uf0c1 template T tex3DLod ( cudaTextureObject_t texObj , float x , float y , float z , float level ); fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level . 7.8.1.16. tex3DLod() for sparse CUDA arrays \\uf0c1 template T tex3DLod ( cudaTextureObject_t texObj , float x , float y , float z , float level , bool * isResident ); fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level . 7.8.1.17. tex3DGrad() \\uf0c1 template T tex3DGrad ( cudaTextureObject_t texObj , float x , float y , float z , float4 dx , float4 dy ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy . 7.8.1.18. tex3DGrad() for sparse CUDA arrays \\uf0c1 template T tex3DGrad ( cudaTextureObject_t texObj , float x , float y , float z , float4 dx , float4 dy , bool * isResident ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy . 7.8.1.19. tex1DLayered() \\uf0c1 template T tex1DLayered ( cudaTextureObject_t texObj , float x , int layer ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x and index layer , as described in Layered Textures 7.8.1.20. tex1DLayeredLod() \\uf0c1 template T tex1DLayeredLod ( cudaTextureObject_t texObj , float x , int layer , float level ); fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and level-of-detail level . 7.8.1.21. tex1DLayeredGrad() \\uf0c1 template T tex1DLayeredGrad ( cudaTextureObject_t texObj , float x , int layer , float dx , float dy ); fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and a level-of-detail derived from the dx and dy gradients. 7.8.1.22. tex2DLayered() \\uf0c1 template T tex2DLayered ( cudaTextureObject_t texObj , float x , float y , int layer ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer , as described in Layered Textures . 7.8.1.23. tex2DLayered() for sparse CUDA arrays \\uf0c1 template T tex2DLayered ( cudaTextureObject_t texObj , float x , float y , int layer , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer , as described in Layered Textures . 7.8.1.24. tex2DLayeredLod() \\uf0c1 template T tex2DLayeredLod ( cudaTextureObject_t texObj , float x , float y , int layer , float level ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) . 7.8.1.25. tex2DLayeredLod() for sparse CUDA arrays \\uf0c1 template T tex2DLayeredLod ( cudaTextureObject_t texObj , float x , float y , int layer , float level , bool * isResident ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) . 7.8.1.26. tex2DLayeredGrad() \\uf0c1 template T tex2DLayeredGrad ( cudaTextureObject_t texObj , float x , float y , int layer , float2 dx , float2 dy ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients. 7.8.1.27. tex2DLayeredGrad() for sparse CUDA arrays \\uf0c1 template T tex2DLayeredGrad ( cudaTextureObject_t texObj , float x , float y , int layer , float2 dx , float2 dy , bool * isResident ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients. 7.8.1.28. texCubemap() \\uf0c1 template T texCubemap ( cudaTextureObject_t texObj , float x , float y , float z ); fetches the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) , as described in Cubemap Textures . 7.8.1.29. texCubemapGrad() \\uf0c1 template T texCubemapGrad ( cudaTextureObject_t texObj , float x , float , y , float z , float4 dx , float4 dy ); fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures . The level-of-detail used is derived from the dx and dy gradients. 7.8.1.30. texCubemapLod() \\uf0c1 template T texCubemapLod ( cudaTextureObject_t texObj , float x , float , y , float z , float level ); fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures . The level-of-detail used is given by level . 7.8.1.31. texCubemapLayered() \\uf0c1 template T texCubemapLayered ( cudaTextureObject_t texObj , float x , float y , float z , int layer ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinates (x,y,z) , and index layer , as described in Cubemap Layered Textures . 7.8.1.32. texCubemapLayeredGrad() \\uf0c1 template T texCubemapLayeredGrad ( cudaTextureObject_t texObj , float x , float y , float z , int layer , float4 dx , float4 dy ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer , as described in Cubemap Layered Textures , at level-of-detail derived from the dx and dy gradients. 7.8.1.33. texCubemapLayeredLod() \\uf0c1 template T texCubemapLayeredLod ( cudaTextureObject_t texObj , float x , float y , float z , int layer , float level ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer , as described in Cubemap Layered Textures , at level-of-detail level level . 7.9. Surface Functions \\uf0c1 Surface functions are only supported by devices of compute capability 2.0 and higher. Surface objects are described in described in Surface Object API In the sections below, boundaryMode specifies the boundary mode, that is how out-of-range surface coordinates are handled; it is equal to either cudaBoundaryModeClamp , in which case out-of-range coordinates are clamped to the valid range, or cudaBoundaryModeZero , in which case out-of-range reads return zero and out-of-range writes are ignored, or cudaBoundaryModeTrap , in which case out-of-range accesses cause the kernel execution to fail. 7.9.1. Surface Object API \\uf0c1 7.9.1.1. surf1Dread() \\uf0c1 template T surf1Dread ( cudaSurfaceObject_t surfObj , int x , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the one-dimensional surface object surfObj using byte coordinate x. 7.9.1.2. surf1Dwrite \\uf0c1 template void surf1Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the one-dimensional surface object surfObj at byte coordinate x. 7.9.1.3. surf2Dread() \\uf0c1 template T surf2Dread ( cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); template void surf2Dread ( T * data , cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the two-dimensional surface object surfObj using byte coordinates x and y. 7.9.1.4. surf2Dwrite() \\uf0c1 template void surf2Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the two-dimensional surface object surfObj at byte coordinate x and y. 7.9.1.5. surf3Dread() \\uf0c1 template T surf3Dread ( cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); template void surf3Dread ( T * data , cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the three-dimensional surface object surfObj using byte coordinates x, y, and z. 7.9.1.6. surf3Dwrite() \\uf0c1 template void surf3Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the three-dimensional object surfObj at byte coordinate x, y, and z. 7.9.1.7. surf1DLayeredread() \\uf0c1 template T surf1DLayeredread ( cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); template void surf1DLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the one-dimensional layered surface object surfObj using byte coordinate x and index layer . 7.9.1.8. surf1DLayeredwrite() \\uf0c1 template void surf1DLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the two-dimensional layered surface object surfObj at byte coordinate x and index layer . 7.9.1.9. surf2DLayeredread() \\uf0c1 template T surf2DLayeredread ( cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); template void surf2DLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the two-dimensional layered surface object surfObj using byte coordinate x and y, and index layer . 7.9.1.10. surf2DLayeredwrite() \\uf0c1 template void surf2DLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the one-dimensional layered surface object surfObj at byte coordinate x and y, and index layer . 7.9.1.11. surfCubemapread() \\uf0c1 template T surfCubemapread ( cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); template void surfCubemapread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the cubemap surface object surfObj using byte coordinate x and y, and face index face. 7.9.1.12. surfCubemapwrite() \\uf0c1 template void surfCubemapwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the cubemap object surfObj at byte coordinate x and y, and face index face. 7.9.1.13. surfCubemapLayeredread() \\uf0c1 template T surfCubemapLayeredread ( cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); template void surfCubemapLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the cubemap layered surface object surfObj using byte coordinate x and y, and index layerFace. 7.9.1.14. surfCubemapLayeredwrite() \\uf0c1 template void surfCubemapLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the cubemap layered object surfObj at byte coordinate x and y , and index layerFace . 7.10.'},\n",
       " {'id': 238,\n",
       "  'content': 'Read-Only Data Cache Load Function \\uf0c1 The read-only data cache load function is only supported by devices of compute capability 5.0 and higher. T __ldg ( const T * address ); returns the data of type T located at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2 . With the cuda_fp16.h header included, T can be __half or __half2 . Similarly, with the cuda_bf16.h header included, T can also be __nv_bfloat16 or __nv_bfloat162 . The operation is cached in the read-only data cache (see Global Memory ).'},\n",
       " {'id': 239,\n",
       "  'content': '7.11. Load Functions Using Cache Hints \\uf0c1 These load functions are only supported by devices of compute capability 5.0 and higher. T __ldcg ( const T * address ); T __ldca ( const T * address ); T __ldcs ( const T * address ); T __ldlu ( const T * address ); T __ldcv ( const T * address ); returns the data of type T located at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2 . The operation is using the corresponding cache operator (see PTX ISA ) 7.12. Store Functions Using Cache Hints \\uf0c1 These store functions are only supported by devices of compute capability 5.0 and higher. void __stwb ( T * address , T value ); void __stcg ( T * address , T value ); void __stcs ( T * address , T value ); void __stwt ( T * address , T value ); stores the value argument of type T to the location at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2 . The operation is using the corresponding cache operator (see PTX ISA ) 7.13. Time Function \\uf0c1 clock_t clock (); long long int clock64 (); when executed in device code, returns the value of a per-multiprocessor counter that is incremented every clock cycle. Sampling this counter at the beginning and at the end of a kernel, taking the difference of the two samples, and recording the result per thread provides a measure for each thread of the number of clock cycles taken by the device to completely execute the thread, but not of the number of clock cycles the device actually spent executing thread instructions. The former number is greater than the latter since threads are time sliced.'},\n",
       " {'id': 240,\n",
       "  'content': '7.14. Atomic Functions \\uf0c1 An atomic function performs a read-modify-write atomic operation on one 32-bit, 64-bit, or 128-bit word residing in global or shared memory. In the case of float2 or float4 , the read-modify-write operation is performed on each element of the vector residing in global memory. For example, atomicAdd() reads a word at some address in global or shared memory, adds a number to it, and writes the result back to the same address. Atomic functions can only be used in device functions. The atomic functions described in this section have ordering cuda::memory_order_relaxed and are only atomic at a particular scope : Atomic APIs with _system suffix (example: atomicAdd_system ) are atomic at scope cuda::thread_scope_system if they meet particular conditions . Atomic APIs without a suffix (example: atomicAdd ) are atomic at scope cuda::thread_scope_device . Atomic APIs with _block suffix (example: atomicAdd_block ) are atomic at scope cuda::thread_scope_block . In the following example both the CPU and the GPU atomically update an integer value at address addr : __global__ void mykernel ( int * addr ) { atomicAdd_system ( addr , 10 ); // only available on devices with compute capability 6.x } void foo () { int * addr ; cudaMallocManaged ( & addr , 4 ); * addr = 0 ; mykernel >> ( addr ); __sync_fetch_and_add ( addr , 10 ); // CPU atomic operation } Note that any atomic operation can be implemented based on atomicCAS() (Compare And Swap). For example, atomicAdd() for double-precision floating-point numbers is not available on devices with compute capability lower than 6.0 but it can be implemented as follows: #if __CUDA_ARCH__ T atomicExch ( T * address , T val ); reads the 128-bit word old located at the address address in global or shared memory and stores val back to memory at the same address. These two operations are performed in one atomic transaction.'},\n",
       " {'id': 241,\n",
       "  'content': 'The function returns old . The type T must meet the following requirements: sizeof ( T ) == 16 alignof ( T ) >= 16 std :: is_trivially_copyable :: value == true // for C++03 and older std :: is_default_constructible :: value == true So, T must be 128-bit and properly aligned, be trivially copyable, and on C++03 or older, it must also be default constructible. The 128-bit atomicExch() is only supported by devices of compute capability 9.x and higher. 7.14.1.4. atomicMin() \\uf0c1 int atomicMin ( int * address , int val ); unsigned int atomicMin ( unsigned int * address , unsigned int val ); unsigned long long int atomicMin ( unsigned long long int * address , unsigned long long int val ); long long int atomicMin ( long long int * address , long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes the minimum of old and val , and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The 64-bit version of atomicMin() is only supported by devices of compute capability 5.0 and higher. 7.14.1.5. atomicMax() \\uf0c1 int atomicMax ( int * address , int val ); unsigned int atomicMax ( unsigned int * address , unsigned int val ); unsigned long long int atomicMax ( unsigned long long int * address , unsigned long long int val ); long long int atomicMax ( long long int * address , long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes the maximum of old and val , and stores the result back to memory at the same address. The 64-bit version of atomicMax() is only supported by devices of compute capability 5.0 and higher. 7.14.1.6. atomicInc() \\uf0c1 unsigned int atomicInc ( unsigned int * address , unsigned int val ); reads the 32-bit word old located at the address address in global or shared memory, computes ((old >= val) ? 0 : (old+1)) , and stores the result back to memory at the same address. 7.14.1.7. atomicDec() \\uf0c1 unsigned int atomicDec ( unsigned int * address , unsigned int val ); reads the 32-bit word old located at the address address in global or shared memory, computes (((old == 0) || (old > val)) ? val : (old-1) ), and stores the result back to memory at the same address. 7.14.1.8. atomicCAS() \\uf0c1 int atomicCAS ( int * address , int compare , int val ); unsigned int atomicCAS ( unsigned int * address , unsigned int compare , unsigned int val ); unsigned long long int atomicCAS ( unsigned long long int * address , unsigned long long int compare , unsigned long long int val ); unsigned short int atomicCAS ( unsigned short int * address , unsigned short int compare , unsigned short int val ); reads the 16-bit, 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old == compare ? val : old) , and stores the result back to memory at the same address. The function returns old (Compare And Swap). template T atomicCAS ( T * address , T compare , T val ); reads the 128-bit word old located at the address address in global or shared memory, computes (old == compare ? The 128-bit atomicCAS() is only supported by devices of compute capability 9.x and higher. 7.14.2. Bitwise Functions \\uf0c1 7.14.2.1. atomicAnd() \\uf0c1 int atomicAnd ( int * address , int val ); unsigned int atomicAnd ( unsigned int * address , unsigned int val ); unsigned long long int atomicAnd ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old & val ), and stores the result back to memory at the same address. The 64-bit version of atomicAnd() is only supported by devices of compute capability 5.0 and higher. 7.14.2.2. atomicOr() \\uf0c1 int atomicOr ( int * address , int val ); unsigned int atomicOr ( unsigned int * address , unsigned int val ); unsigned long long int atomicOr ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old | val) , and stores the result back to memory at the same address. The 64-bit version of atomicOr() is only supported by devices of compute capability 5.0 and higher. 7.14.2.3. atomicXor() \\uf0c1 int atomicXor ( int * address , int val ); unsigned int atomicXor ( unsigned int * address , unsigned int val ); unsigned long long int atomicXor ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old ^ val) , and stores the result back to memory at the same address. The 64-bit version of atomicXor() is only supported by devices of compute capability 5.0 and higher. 7.15. Address Space Predicate Functions \\uf0c1 The functions described in this section have unspecified behavior if the argument is a null pointer. 7.15.1. __isGlobal() \\uf0c1 __device__ unsigned int __isGlobal ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in global memory space, otherwise returns 0. 7.15.2. __isShared() \\uf0c1 __device__ unsigned int __isShared ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in shared memory space, otherwise returns 0. 7.15.3. __isConstant() \\uf0c1 __device__ unsigned int __isConstant ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in constant memory space, otherwise returns 0. 7.15.4. __isGridConstant() \\uf0c1 __device__ unsigned int __isGridConstant ( const void * ptr ); Returns 1 if ptr contains the generic address of a kernel parameter annotated with __grid_constant__ , otherwise returns 0. Only supported for compute architectures greater than or equal to 7.x or later. 7.15.5. __isLocal() \\uf0c1 __device__ unsigned int __isLocal ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in local memory space, otherwise returns 0. 7.16. Address Space Conversion Functions \\uf0c1 7.16.1. __cvta_generic_to_global() \\uf0c1 __device__ size_t __cvta_generic_to_global ( const void * ptr ); Returns the result of executing the PTX cvta.to.global instruction on the generic address denoted by ptr . 7.16.2. __cvta_generic_to_shared() \\uf0c1 __device__ size_t __cvta_generic_to_shared ( const void * ptr ); Returns the result of executing the PTX cvta.to.shared instruction on the generic address denoted by ptr . 7.16.3. __cvta_generic_to_constant() \\uf0c1 __device__ size_t __cvta_generic_to_constant ( const void * ptr ); Returns the result of executing the PTX cvta.to.const instruction on the generic address denoted by ptr . 7.16.4. __cvta_generic_to_local() \\uf0c1 __device__ size_t __cvta_generic_to_local ( const void * ptr ); Returns the result of executing the PTX cvta.to.local instruction on the generic address denoted by ptr . 7.16.5. __cvta_global_to_generic() \\uf0c1 __device__ void * __cvta_global_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.global instruction on the value provided by rawbits . 7.16.6. __cvta_shared_to_generic() \\uf0c1 __device__ void * __cvta_shared_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.shared instruction on the value provided by rawbits . 7.16.7. __cvta_constant_to_generic() \\uf0c1 __device__ void * __cvta_constant_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.const instruction on the value provided by rawbits . 7.16.8. __cvta_local_to_generic() \\uf0c1 __device__ void * __cvta_local_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta.local instruction on the value provided by rawbits . 7.17. Alloca Function \\uf0c1 7.17.1. Synopsis \\uf0c1 __host__ __device__ void * alloca ( size_t size ); 7.17.2. Description \\uf0c1 The alloca() function allocates size bytes of memory in the stack frame of the caller. The returned value is a pointer to allocated memory, the beginning of the memory is 16 bytes aligned when the function is invoked from device code. The allocated memory is automatically freed when the caller to alloca() is returned. Note On Windows platform, must be included before using alloca() . Using alloca() may cause the stack to overflow, user needs to adjust stack size accordingly. It is supported with compute capability 5.2 or higher.'},\n",
       " {'id': 242,\n",
       "  'content': '7.17.3. Example \\uf0c1 __device__ void foo ( unsigned int num ) { int4 * ptr = ( int4 * ) alloca ( num * sizeof ( int4 )); // use of ptr ... } 7.18. Compiler Optimization Hint Functions \\uf0c1 The functions described in this section can be used to provide additional information to the compiler optimizer. 7.18.1. __builtin_assume_aligned() \\uf0c1 void * __builtin_assume_aligned ( const void * exp , size_t align ) Allows the compiler to assume that the argument pointer is aligned to at least align bytes, and returns the argument pointer. Example: void * res = __builtin_assume_aligned ( ptr , 32 ); // compiler can assume \\'res\\' is // at least 32-byte aligned Three parameter version: void * __builtin_assume_aligned ( const void * exp , size_t align , offset ) Allows the compiler to assume that (char *)exp - offset is aligned to at least align bytes, and returns the argument pointer. Example: void * res = __builtin_assume_aligned ( ptr , 32 , 8 ); // compiler can assume // \\'(char *)res - 8\\' is // at least 32-byte aligned. 7.18.2. __builtin_assume() \\uf0c1 void __builtin_assume ( bool exp ) Allows the compiler to assume that the Boolean argument is true. If the argument is not true at run time, then the behavior is undefined. Note that if the argument has side effects, the behavior is unspecified. Example: __device__ int get ( int * ptr , int idx ) { __builtin_assume ( idx __global__ void bcast ( int arg ) { int laneId = threadIdx . x & 0x1f ; int value ; if ( laneId == 0 ) // Note unused variable for value = arg ; // all threads except lane 0 value = __shfl_sync ( 0xffffffff , value , 0 ); // Synchronize all threads in warp, and get \"value\" from lane 0 if ( value != arg ) printf ( \"Thread %d failed. \\\\n \" , threadIdx . x ); } int main () { bcast >> ( 1234 ); cudaDeviceSynchronize (); return 0 ; } 7.22.3.2. Inclusive plus-scan across sub-partitions of 8 threads \\uf0c1 #include __global__ void scan4 () { int laneId = threadIdx . x & 0x1f ; // Seed sample starting value (inverse of lane ID) int value = 31 - laneId ; // Loop to accumulate scan within my partition. // Scan requires log2(n) == 3 steps for 8 threads // It works by an accumulated sum up the warp // by 1, 2, 4, 8 etc. steps. for ( int i = 1 ; i = i ) value += n ; } printf ( \"Thread %d final value = %d \\\\n \" , threadIdx . x , value ); } int main () { scan4 >> (); cudaDeviceSynchronize (); return 0 ; } 7.22.3.3. Reduction across a warp \\uf0c1 #include __global__ void warpReduce () { int laneId = threadIdx . x & 0x1f ; // Seed starting value as inverse lane ID int value = 31 - laneId ; // Use XOR mode to perform butterfly reduction for ( int i = 16 ; i >= 1 ; i /= 2 ) value += __shfl_xor_sync ( 0xffffffff , value , i , 32 ); // \"value\" now contains the sum across all threads printf ( \"Thread %d final value = %d \\\\n \" , threadIdx . x , value ); } int main () { warpReduce >> (); cudaDeviceSynchronize (); return 0 ; } 7.23. Nanosleep Function \\uf0c1 7.23.1. Synopsis \\uf0c1 void __nanosleep ( unsigned ns ); 7.23.2. Description \\uf0c1 __nanosleep(ns) suspends the thread for a sleep duration of approximately ns nanoseconds. The maximum sleep duration is approximately 1 millisecond. It is supported with compute capability 7.0 or higher.'},\n",
       " {'id': 243,\n",
       "  'content': '7.23.3. Example \\uf0c1 The following code implements a mutex with exponential back-off. __device__ void mutex_lock ( unsigned int * mutex ) { unsigned int ns = 8 ; while ( atomicCAS ( mutex , 0 , 1 ) == 1 ) { __nanosleep ( ns ); if ( ns class fragment ; void load_matrix_sync ( fragment & a , const T * mptr , unsigned ldm ); void load_matrix_sync ( fragment & a , const T * mptr , unsigned ldm , layout_t layout ); void store_matrix_sync ( T * mptr , const fragment & a , unsigned ldm , layout_t layout ); void fill_fragment ( fragment & a , const T & v ); void mma_sync ( fragment & d , const fragment & a , const fragment & b , const fragment & c , bool satf = false ); fragment An overloaded class containing a section of a matrix distributed across all threads in the warp. The mapping of matrix elements into fragment internal storage is unspecified and subject to change in future architectures. Only certain combinations of template arguments are allowed. The first template parameter specifies how the fragment will participate in the matrix operation. Acceptable values for Use are: matrix_a when the fragment is used as the first multiplicand, A , matrix_b when the fragment is used as the second multiplicand, B , or accumulator when the fragment is used as the source or destination accumulators ( C or D , respectively). The m , n and k sizes describe the shape of the warp-wide matrix tiles participating in the multiply-accumulate operation. The dimension of each tile depends on its role. For matrix_a the tile takes dimension m x k ; for matrix_b the dimension is k x n , and accumulator tiles are m x n . The data type, T , may be double , float , __half , __nv_bfloat16 , char , or unsigned char for multiplicands and double , float , int , or __half for accumulators. As documented in Element Types and Matrix Sizes , limited combinations of accumulator and multiplicand types are supported. The Layout parameter must be specified for matrix_a and matrix_b fragments. row_major or col_major indicate that elements within a matrix row or column are contiguous in memory, respectively. The Layout parameter for an accumulator matrix should retain the default value of void . A row or column layout is specified only when the accumulator is loaded or stored as described below. load_matrix_sync Waits until all warp lanes have arrived at load_matrix_sync and then loads the matrix fragment a from memory. mptr must be a 256-bit aligned pointer pointing to the first element of the matrix in memory. ldm describes the stride in elements between consecutive rows (for row major layout) or columns (for column major layout) and must be a multiple of 8 for __half element type or multiple of 4 for float element type. (i.e., multiple of 16 bytes in both cases). If the fragment is an accumulator , the layout argument must be specified as either mem_row_major or mem_col_major . For matrix_a and matrix_b fragments, the layout is inferred from the fragment’s layout parameter. The values of mptr , ldm , layout and all template parameters for a must be the same for all threads in the warp. This function must be called by all threads in the warp, or the result is undefined. store_matrix_sync Waits until all warp lanes have arrived at store_matrix_sync and then stores the matrix fragment a to memory. The layout of the output matrix must be specified as either mem_row_major or mem_col_major . fill_fragment Fill a matrix fragment with a constant value v . Because the mapping of matrix elements to each fragment is unspecified, this function is ordinarily called by all threads in the warp with a common value for v . mma_sync Waits until all warp lanes have arrived at mma_sync, and then performs the warp-synchronous matrix multiply-accumulate operation D=A*B+C . The in-place operation, C=A*B+C , is also supported. The value of satf and template parameters for each matrix fragment must be the same for all threads in the warp. Also, the template parameters m , n and k must match between fragments A , B , C and D . If satf (saturate to finite value) mode is true , the following additional numerical properties apply for the destination accumulator: If an element result is +Infinity, the corresponding accumulator will contain +MAX_NORM If an element result is -Infinity, the corresponding accumulator will contain -MAX_NORM If an element result is NaN, the corresponding accumulator will contain +0 Because the map of matrix elements into each thread’s fragment is unspecified, individual matrix elements must be accessed from memory (shared or global) after calling store_matrix_sync . In the special case where all threads in the warp will apply an element-wise operation uniformly to all fragment elements, direct element access can be implemented using the following fragment class members. enum fragment :: num_elements ; T fragment :: x [ num_elements ]; As an example, the following code scales an accumulator matrix tile by half. wmma :: fragment frag ; float alpha = 0.5f ; // Same value for all threads in warp /*...*/ for ( int t = 0 ; t =10 bits). The internal layout of this format is implementation defined. In order to use this floating point format with WMMA operations, the input matrices must be manually converted to tf32 precision. To facilitate conversion, a new intrinsic __float_to_tf32 is provided. While the input and output arguments to the intrinsic are of float type, the output will be tf32 numerically. This new precision is intended to be used with Tensor Cores only, and if mixed with other float type operations, the precision and range of the result will be undefined. Once an input matrix ( matrix_a or matrix_b ) is converted to tf32 precision, the combination of a fragment with precision::tf32 precision, and a data type of float to load_matrix_sync will take advantage of this new capability. Both the accumulator fragments must have float data types. The only supported matrix size is 16x16x8 (m-n-k). The elements of the fragment are represented as float , hence the mapping from element_type to storage_element_type is: precision :: tf32 -> float 7.24.3. Double Precision \\uf0c1 Tensor Cores support double-precision floating point operations on devices with compute capability 8.0 and higher. To use this new functionality, a fragment with the double type must be used. The mma_sync operation will be performed with the .rn (rounds to nearest even) rounding modifier. 7.24.4. Sub-byte Operations \\uf0c1 Sub-byte WMMA operations provide a way to access the low-precision capabilities of Tensor Cores. They are considered a preview feature i.e. the data structures and APIs for them are subject to change and may not be compatible with future releases. This functionality is available via the nvcuda::wmma::experimental namespace: namespace experimental { namespace precision { struct u4 ; // 4-bit unsigned struct s4 ; // 4-bit signed struct b1 ; // 1-bit } enum bmmaBitOp { bmmaBitOpXOR = 1 , // compute_75 minimum bmmaBitOpAND = 2 // compute_80 minimum }; enum bmmaAccumulateOp { bmmaAccumulateOpPOPC = 1 }; } For 4 bit precision, the APIs available remain the same, but you must specify experimental::precision::u4 or experimental::precision::s4 as the fragment data type. Since the elements of the fragment are packed together, num_storage_elements will be smaller than num_elements for that fragment. The num_elements variable for a sub-byte fragment, hence returns the number of elements of sub-byte type element_type . This is true for single bit precision as well, in which case, the mapping from element_type to storage_element_type is as follows: experimental :: precision :: u4 -> unsigned ( 8 elements in 1 storage element ) experimental :: precision :: s4 -> int ( 8 elements in 1 storage element ) experimental :: precision :: b1 -> unsigned ( 32 elements in 1 storage element ) T -> T //all other types The allowed layouts for sub-byte fragments is always row_major for matrix_a and col_major for matrix_b . For sub-byte operations the value of ldm in load_matrix_sync should be a multiple of 32 for element type experimental::precision::u4 and experimental::precision::s4 or a multiple of 128 for element type experimental::precision::b1 (i.e., multiple of 16 bytes in both cases). Note Support for the following variants for MMA instructions is deprecated and will be removed in sm_90: experimental::precision::u4 experimental::precision::s4 experimental::precision::b1 with bmmaBitOp set to bmmaBitOpXOR bmma_sync Waits until all warp lanes have executed bmma_sync, and then performs the warp-synchronous bit matrix multiply-accumulate operation D = (A op B) + C , where op consists of a logical operation bmmaBitOp followed by the accumulation defined by bmmaAccumulateOp . The available operations are: bmmaBitOpXOR , a 128-bit XOR of a row in matrix_a with the 128-bit column of matrix_b bmmaBitOpAND , a 128-bit AND of a row in matrix_a with the 128-bit column of matrix_b , available on devices with compute capability 8.0 and higher. The accumulate op is always bmmaAccumulateOpPOPC which counts the number of set bits. 7.24.5. Restrictions \\uf0c1 The special format required by tensor cores may be different for each major and minor device architecture. This is further complicated by threads holding only a fragment (opaque architecture-specific ABI data structure) of the overall matrix, with the developer not allowed to make assumptions on how the individual parameters are mapped to the registers participating in the matrix multiply-accumulate. Since fragments are architecture-specific, it is unsafe to pass them from function A to function B if the functions have been compiled for different link-compatible architectures and linked together into the same device executable. In this case, the size and layout of the fragment will be specific to one architecture and using WMMA APIs in the other will lead to incorrect results or potentially, corruption. An example of two link-compatible architectures, where the layout of the fragment differs, is sm_70 and sm_75. fragA . cu : void foo () { wmma :: fragment mat_a ; bar ( & mat_a ); } fragB . cu : void bar ( wmma :: fragment * mat_a ) { // operate on mat_a } // sm_70 fragment layout $ > nvcc - dc - arch = compute_70 - code = sm_70 fragA . cu - o fragA . o // sm_75 fragment layout $ > nvcc - dc - arch = compute_75 - code = sm_75 fragB . cu - o fragB . o // Linking the two together $ > nvcc - dlink - arch = sm_75 fragA . o fragB .'},\n",
       " {'id': 244,\n",
       "  'content': 'o - o frag . o This undefined behavior might also be undetectable at compilation time and by tools at runtime, so extra care is needed to make sure the layout of the fragments is consistent. This linking hazard is most likely to appear when linking with a legacy library that is both built for a different link-compatible architecture and expecting to be passed a WMMA fragment. Note that in the case of weak linkages (for example, a CUDA C++ inline function), the linker may choose any available function definition which may result in implicit passes between compilation units. To avoid these sorts of problems, the matrix should always be stored out to memory for transit through external interfaces (e.g. wmma::store_matrix_sync(dst, …); ) and then it can be safely passed to bar() as a pointer type [e.g. float *dst ]. Note that since sm_70 can run on sm_75, the above example sm_75 code can be changed to sm_70 and correctly work on sm_75. However, it is recommended to have sm_75 native code in your application when linking with other sm_75 separately compiled binaries. 7.24.6.'},\n",
       " {'id': 245,\n",
       "  'content': 'Element Types and Matrix Sizes \\uf0c1 Tensor Cores support a variety of element types and matrix sizes. The following table presents the various combinations of matrix_a , matrix_b and accumulator matrix supported: Matrix A Matrix B Accumulator Matrix Size (m-n-k) __half __half float 16x16x16 __half __half float 32x8x16 __half __half float 8x32x16 __half __half __half 16x16x16 __half __half __half 32x8x16 __half __half __half 8x32x16 unsigned char unsigned char int 16x16x16 unsigned char unsigned char int 32x8x16 unsigned char unsigned char int 8x32x16 signed char signed char int 16x16x16 signed char signed char int 32x8x16 signed char signed char int 8x32x16 Alternate Floating Point support: Matrix A Matrix B Accumulator Matrix Size (m-n-k) __nv_bfloat16 __nv_bfloat16 float 16x16x16 __nv_bfloat16 __nv_bfloat16 float 32x8x16 __nv_bfloat16 __nv_bfloat16 float 8x32x16 precision::tf32 precision::tf32 float 16x16x8 Double Precision Support: Matrix A Matrix B Accumulator Matrix Size (m-n-k) double double double 8x8x4 Experimental support for sub-byte operations: Matrix A Matrix B Accumulator Matrix Size (m-n-k) precision::u4 precision::u4 int 8x8x32 precision::s4 precision::s4 int 8x8x32 precision::b1 precision::b1 int 8x8x128 7.24.7. Example \\uf0c1 The following code implements a 16x16x16 matrix multiplication in a single warp.'},\n",
       " {'id': 246,\n",
       "  'content': '#include using namespace nvcuda ; __global__ void wmma_ker ( half * a , half * b , float * c ) { // Declare the fragments wmma :: fragment a_frag ; wmma :: fragment b_frag ; wmma :: fragment c_frag ; // Initialize the output to zero wmma :: fill_fragment ( c_frag , 0.0f ); // Load the inputs wmma :: load_matrix_sync ( a_frag , a , 16 ); wmma :: load_matrix_sync ( b_frag , b , 16 ); // Perform the matrix multiplication wmma :: mma_sync ( c_frag , a_frag , b_frag , c_frag ); // Store the output wmma :: store_matrix_sync ( c , c_frag , 16 , wmma :: mem_row_major ); } 7.25. DPX \\uf0c1 DPX is a set of functions that enable finding min and max values, as well as fused addition and min/max, for up to three 16 and 32-bit signed or unsigned integer parameters, with optional ReLU (clamping to zero): three parameters: __vimax3_s32 , __vimax3_s16x2 , __vimax3_u32 , __vimax3_u16x2 , __vimin3_s32 , __vimin3_s16x2 , __vimin3_u32 , __vimin3_u16x2 two parameters, with ReLU: __vimax_s32_relu , __vimax_s16x2_relu , __vimin_s32_relu , __vimin_s16x2_relu three parameters, with ReLU: __vimax3_s32_relu , __vimax3_s16x2_relu , __vimin3_s32_relu , __vimin3_s16x2_relu two parameters, also returning which parameter was smaller/larger: __vibmax_s32 , __vibmax_u32 , __vibmin_s32 , __vibmin_u32 , __vibmax_s16x2 , __vibmax_u16x2 , __vibmin_s16x2 , __vibmin_u16x2 three parameters, comparing (first + second) with the third: __viaddmax_s32 , __viaddmax_s16x2 , __viaddmax_u32 , __viaddmax_u16x2 , __viaddmin_s32 , __viaddmin_s16x2 , __viaddmin_u32 , __viaddmin_u16x2 three parameters, with ReLU, comparing (first + second) with the third and a zero: __viaddmax_s32_relu , __viaddmax_s16x2_relu , __viaddmin_s32_relu , __viaddmin_s16x2_relu These instructions are hardware-accelerated on devices with compute capability 9 and higher, and software emulation on older devices. Full API can be found in CUDA Math API documentation .'},\n",
       " {'id': 247,\n",
       "  'content': 'DPX is exceptionally useful when implementing dynamic programming algorithms, such as Smith-Waterman or Needleman–Wunsch in genomics and Floyd-Warshall in route optimization.'},\n",
       " {'id': 248,\n",
       "  'content': '7.25.1. Examples \\uf0c1 Max value of three signed 32-bit integers, with ReLU const int a = -15 ; const int b = 8 ; const int c = 5 ; int max_value_0 = __vimax3_s32_relu ( a , b , c ); // max(-15, 8, 5, 0) = 8 const int d = -2 ; const int e = -4 ; int max_value_1 = __vimax3_s32_relu ( a , d , e ); // max(-15, -2, -4, 0) = 0 Min value of the sum of two 32-bit signed integers, another 32-bit signed integer and a zero (ReLU) const int a = -5 ; const int b = 6 ; const int c = -2 ; int max_value_0 = __viaddmax_s32_relu ( a , b , c ); // max(-5 + 6, -2, 0) = max(1, -2, 0) = 1 const int d = 4 ; int max_value_1 = __viaddmax_s32_relu ( a , d , c ); // max(-5 + 4, -2, 0) = max(-1, -2, 0) = 0 Min value of two unsigned 32-bit integers and determining which value is smaller const unsigned int a = 9 ; const unsigned int b = 6 ; bool smaller_value ; unsigned int min_value = __vibmin_u32 ( a , b , & smaller_value ); // min_value is 6, smaller_value is true Max values of three pairs of unsigned 16-bit integers const unsigned a = 0x00050002 ; const unsigned b = 0x00070004 ; const unsigned c = 0x00020006 ; unsigned int max_value = __vimax3_u16x2 ( a , b , c ); // max(5, 7, 2) and max(2, 4, 6), so max_value is 0x00070006 7.26. Asynchronous Barrier \\uf0c1 The NVIDIA C++ standard library introduces a GPU implementation of std::barrier .'},\n",
       " {'id': 249,\n",
       "  'content': 'Along with the implementation of std::barrier the library provides extensions that allow users to specify the scope of barrier objects. The barrier API scopes are documented under Thread Scopes . Devices of compute capability 8.0 or higher provide hardware acceleration for barrier operations and integration of these barriers with the memcpy_async feature. On devices with compute capability below 8.0 but starting 7.0, these barriers are available without hardware acceleration. nvcuda::experimental::awbarrier is deprecated in favor of cuda::barrier . 7.26.1. Simple Synchronization Pattern \\uf0c1 Without the arrive/wait barrier, synchronization is achieved using __syncthreads() (to synchronize all threads in a block) or group.sync() when using Cooperative Groups . #include __global__ void simple_sync ( int iteration_count ) { auto block = cooperative_groups :: this_thread_block (); for ( int i = 0 ; i #include __device__ void compute ( float * data , int curr_iteration ); __global__ void split_arrive_wait ( int iteration_count , float * data ) { using barrier = cuda :: barrier ; __shared__ barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block . thread_rank () == 0 ) { init ( & bar , block . size ()); // Initialize the barrier with expected arrival count } block . sync (); for ( int curr_iter = 0 ; curr_iter #include __global__ void init_barrier () { __shared__ cuda :: barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block . size ()); // Single thread initializes the total expected arrival count. }\\nblock . sync (); } Before any thread can participate in cuda::barrier , the barrier must be initialized using init() with an expected arrival count , block.size() in this example. Initialization must happen before any thread calls bar.arrive() . This poses a bootstrapping challenge in that threads must synchronize before participating in the cuda::barrier , but threads are creating a cuda::barrier in order to synchronize. In this example, threads that will participate are part of a cooperative group and use block.sync() to bootstrap initialization. In this example a whole thread block is participating in initialization, hence __syncthreads() could also be used. The second parameter of init() is the expected arrival count , i.e., the number of times bar.arrive() will be called by participating threads before a participating thread is unblocked from its call to bar.wait(std::move(token)) . In the prior example the cuda::barrier is initialized with the number of threads in the thread block i.e., cooperative_groups::this_thread_block().size() , and all threads within the thread block participate in the barrier. A cuda::barrier is flexible in specifying how threads participate (split arrive/wait) and which threads participate. In contrast this_thread_block.sync() from cooperative groups or __syncthreads() is applicable to whole-thread-block and __syncwarp(mask) is a specified subset of a warp. If the intention of the user is to synchronize a full thread block or a full warp we recommend using __syncthreads() and __syncwarp(mask) respectively for performance reasons. 7.26.4. A Barrier’s Phase: Arrival, Countdown, Completion, and Reset \\uf0c1 A cuda::barrier counts down from the expected arrival count to zero as participating threads call bar.arrive() . When the countdown reaches zero, a cuda::barrier is complete for the current phase. When the last call to bar.arrive() causes the countdown to reach zero, the countdown is automatically and atomically reset. The reset assigns the countdown to the expected arrival count, and moves the cuda::barrier to the next phase. A token object of class cuda::barrier::arrival_token , as returned from token=bar.arrive() , is associated with the current phase of the barrier. A call to bar.wait(std::move(token)) blocks the calling thread while the cuda::barrier is in the current phase, i.e., while the phase associated with the token matches the phase of the cuda::barrier . If the phase is advanced (because the countdown reaches zero) before the call to bar.wait(std::move(token)) then the thread does not block; if the phase is advanced while the thread is blocked in bar.wait(std::move(token)) , the thread is unblocked. It is essential to know when a reset could or could not occur, especially in non-trivial arrive/wait synchronization patterns. A thread’s calls to token=bar.arrive() and bar.wait(std::move(token)) must be sequenced such that token=bar.arrive() occurs during the cuda::barrier ’s current phase, and bar.wait(std::move(token)) occurs during the same or next phase. A thread’s call to bar.arrive() must occur when the barrier’s counter is non-zero. After barrier initialization, if a thread’s call to bar.arrive() causes the countdown to reach zero then a call to bar.wait(std::move(token)) must happen before the barrier can be reused for a subsequent call to bar.arrive() . bar.wait() must only be called using a token object of the current phase or the immediately preceding phase. For any other values of the token object, the behavior is undefined. For simple arrive/wait synchronization patterns, compliance with these usage rules is straightforward.'},\n",
       " {'id': 250,\n",
       "  'content': '7.26.5. Spatial Partitioning (also known as Warp Specialization) \\uf0c1 A thread block can be spatially partitioned such that warps are specialized to perform independent computations. Spatial partitioning is used in a producer or consumer pattern, where one subset of threads produces data that is concurrently consumed by the other (disjoint) subset of threads. A producer/consumer spatial partitioning pattern requires two one sided synchronizations to manage a data buffer between the producer and consumer. Producer Consumer wait for buffer to be ready to be filled signal buffer is ready to be filled produce data and fill the buffer signal buffer is filled wait for buffer to be filled consume data in filled buffer Producer threads wait for consumer threads to signal that the buffer is ready to be filled; however, consumer threads do not wait for this signal. Consumer threads wait for producer threads to signal that the buffer is filled; however, producer threads do not wait for this signal. For full producer/consumer concurrency this pattern has (at least) double buffering where each buffer requires two cuda::barrier s. #include #include using barrier = cuda :: barrier ; __device__ void producer ( barrier ready [], barrier filled [], float * buffer , float * in , int N , int buffer_len ) { for ( int i = 0 ; i #include __device__ bool condition_check (); __global__ void early_exit_kernel ( int N ) { using barrier = cuda :: barrier ; __shared__ barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block . thread_rank () == 0 ) init ( & bar , block .'},\n",
       " {'id': 251,\n",
       "  'content': 'size ()); block . sync (); for ( int i = 0 ; i is executed once per phase, after the last thread arrives and before any thread is unblocked from the wait . Memory operations performed by the threads that arrived at the barrier during the phase are visible to the thread executing the CompletionFunction , and all memory operations performed within the CompletionFunction are visible to all threads waiting at the barrier once they are unblocked from the wait . #include #include #include namespace cg = cooperative_groups ; __device__ int divergent_compute ( int * , int ); __device__ int independent_computation ( int * , int ); __global__ void psum ( int * data , int n , int * acc ) { auto block = cg :: this_thread_block (); constexpr int BlockSize = 128 ; __shared__ int smem [ BlockSize ]; assert ( BlockSize == block . size ()); assert ( n % 128 == 0 ); auto completion_fn = [ & ] { int sum = 0 ; for ( int i = 0 ; i ; __shared__ std :: aligned_storage bar_storage ; // Initialize barrier: barrier_t * bar = ( barrier_t * ) & bar_storage ; if ( block . thread_rank () == 0 ) { assert ( * acc == 0 ); assert ( blockDim . x == blockDim .'},\n",
       " {'id': 252,\n",
       "  'content': 'y == blockDim . y == 1 ); new ( bar ) barrier_t { block . size (), completion_fn }; // equivalent to: init(bar, block.size(), completion_fn); } block . sync (); // Main loop for ( int i = 0 ; i arrive (); // We can do independent computation here bar -> wait ( std :: move ( t )); // shared-memory is safe to re-use in the next iteration // since all threads are done with it, including the one // that did the reduction } } 7.26.8. Memory Barrier Primitives Interface \\uf0c1 Memory barrier primitives are C-like interfaces to cuda::barrier functionality. These primitives are available through including the header.'},\n",
       " {'id': 253,\n",
       "  'content': \"7.26.8.1. Data Types \\uf0c1 typedef /* implementation defined */ __mbarrier_t ; typedef /* implementation defined */ __mbarrier_token_t ; 7.26.8.2. Memory Barrier Primitives API \\uf0c1 uint32_t __mbarrier_maximum_count (); void __mbarrier_init ( __mbarrier_t * bar , uint32_t expected_count ); bar must be a pointer to __shared__ memory. expected_count __device__ void compute ( int * global_out , int const * shared_in ) { // Computes using all values of current batch from shared memory. // Stores this thread's result back to global memory. }\\n__global__ void without_memcpy_async ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid . size ()); // Exposition: input size fits batch_sz * grid_size extern __shared__ int shared []; // block.size() * sizeof(int) bytes size_t local_idx = block . thread_rank (); for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_memcpy_async ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid . size ()); // Exposition: input size fits batch_sz * grid_size extern __shared__ int shared []; // block.size() * sizeof(int) bytes for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_barrier ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid . size ()); // Assume input size fits batch_sz * grid_size extern __shared__ int shared []; // block.size() * sizeof(int) bytes // Create a synchronization object (C++20 barrier) __shared__ cuda :: barrier barrier ; if ( block . thread_rank () == 0 ) { init ( & barrier , block . size ()); // Friend function initializes barrier } block . sync (); for ( size_t batch = 0 ; batch (size_t size) Shape can be used to supply a proof that both pointers passed to memcpy_async are aligned to an Align alignment boundary and that size is a multiple of Align , by passing it as an argument where the memcpy_async APIs expect a Shape : cuda :: memcpy_async ( group , dst , src , cuda :: aligned_size_t ( N * block . size ()), pipeline ); If the proof is incorrect, the behavior is undefined. 7.27.6.2. Trivially copyable \\uf0c1 On devices with compute capability 8.0, the cp.async family of instructions allows copying data from global to shared memory asynchronously. If the pointer types passed to memcpy_async do not point to TriviallyCopyable types, the copy constructor of each output element needs to be invoked, and these instructions cannot be used to accelerate memcpy_async . 7.27.6.3. Warp Entanglement - Commit \\uf0c1 The sequence of memcpy_async batches is shared across the warp. The commit operation is coalesced such that the sequence is incremented once for all converged threads that invoke the commit operation. If the warp is fully converged, the sequence is incremented by one; if the warp is fully diverged, the sequence is incremented by 32. Let PB be the warp-shared pipeline’s actual sequence of batches. PB = {BP0, BP1, BP2, …, BPL} Let TB be a thread’s perceived sequence of batches, as if the sequence were only incremented by this thread’s invocation of the commit operation. TB = {BT0, BT1, BT2, …, BTL} The pipeline::producer_commit() return value is from the thread’s perceived batch sequence. An index in a thread’s perceived sequence always aligns to an equal or larger index in the actual warp-shared sequence. The sequences are equal only when all commit operations are invoked from converged threads. BTn ≡ BPm where n () or pipeline::consumer_wait() to wait for batches in the perceived sequence TB to complete. Note that pipeline::consumer_wait() is equivalent to pipeline_consumer_wait_prior() , where N = PL . The pipeline_consumer_wait_prior() function waits for batches in the actual sequence at least up to and including PL-N . Since TL #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_single_stage ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid . size ()); // Assume input size fits batch_sz * grid_size constexpr size_t stages_count = 1 ; // Pipeline with one stage // One batch must fit in shared memory: extern __shared__ int shared []; // block.size() * sizeof(int) bytes // Allocate shared storage for a single stage cuda::pipeline: __shared__ cuda :: pipeline_shared_state shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state ); // Each thread processes `batch_sz` elements. // Compute offset of the batch `batch` of this thread block in global memory: auto block_batch = [ & ]( size_t batch ) -> int { return block . group_index ().\"},\n",
       " {'id': 254, 'content': 'x * block .'},\n",
       " {'id': 255,\n",
       "  'content': 'size () + grid . size () * batch ; }; for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_staging ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid . size ()); // Assume input size fits batch_sz * grid_size constexpr size_t stages_count = 2 ; // Pipeline with two stages // Two batches must fit in shared memory: extern __shared__ int shared []; // stages_count * block.size() * sizeof(int) bytes size_t shared_offset [ stages_count ] = { 0 , block . size () }; // Offsets to each batch // Allocate shared storage for a two-stage cuda::pipeline: __shared__ cuda :: pipeline_shared_state shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state ); // Each thread processes `batch_sz` elements. size () * batch ; }; // Initialize first pipeline stage by submitting a `memcpy_async` to fetch a whole batch for the block: if ( batch_sz == 0 ) return ; pipeline . producer_acquire (); cuda :: memcpy_async ( block , shared + shared_offset [ 0 ], global_in + block_batch ( 0 ), sizeof ( int ) * block . size (), pipeline ); pipeline . producer_commit (); // Pipelined copy/compute: for ( size_t batch = 1 ; batch encapsulates the finite resources that allow a pipeline to process up to count concurrent stages. If all resources are in use, pipeline.producer_acquire() blocks producer threads until the resources of the next pipeline stage are released by consumer threads. This example can be written in a more concise manner by merging the prolog and epilog of the loop with the loop itself as follows: template __global__ void with_staging_unified ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid . size ()); // Assume input size fits batch_sz * grid_size extern __shared__ int shared []; // stages_count * block.size() * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state ); auto block_batch = [ & ]( size_t batch ) -> int { return block . size () * batch ; }; // compute_batch: next batch to process // fetch_batch: next batch to fetch from global memory for ( size_t compute_batch = 0 , fetch_batch = 0 ; compute_batch primitive used above is very flexible, and supports two features that our examples above are not using: any arbitrary subset of threads in the block can participate in the pipeline , and from the threads that participate, any subsets can be producers, consumers, or both. In the following example, threads with an “even” thread rank are producers, while other threads are consumers: __device__ void compute ( int * global_out , int shared_in ); template __global__ void with_specialized_staging_unified ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); // In this example, threads with \"even\" thread rank are producers, while threads with \"odd\" thread rank are consumers: const cuda :: pipeline_role thread_role = block . thread_rank () % 2 == 0 ? cuda :: pipeline_role :: producer : cuda :: pipeline_role :: consumer ; // Each thread block only has half of its threads as producers: auto producer_threads = block . size () / 2 ; // Map adjacent even and odd threads to the same id: const int thread_idx = block . thread_rank () / 2 ; auto elements_per_batch = size / batch_sz ; auto elements_per_batch_per_block = elements_per_batch / grid . group_dim (). x ; extern __shared__ int shared []; // stages_count * elements_per_batch_per_block * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s shared_state ; cuda :: pipeline pipeline = cuda :: make_pipeline ( block , & shared_state , thread_role ); // Each thread block processes `batch_sz` batches. // Compute offset of the batch `batch` of this thread block in global memory: auto block_batch = [ & ]( size_t batch ) -> int { return elements_per_batch * batch + elements_per_batch_per_block * blockIdx . x ; }; for ( size_t compute_batch = 0 , fetch_batch = 0 ; compute_batch by using a pipeline combined with __syncthreads() : template __global__ void with_staging_scope_thread ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); auto thread = cooperative_groups :: this_thread (); assert ( size == batch_sz * grid . size ()); // Assume input size fits batch_sz * grid_size extern __shared__ int shared []; // stages_count * block.size() * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s pipeline = cuda :: make_pipeline (); auto block_batch = [ & ]( size_t batch ) -> int { return block . size () * batch ; }; for ( size_t compute_batch = 0 , fetch_batch = 0 ; compute_batch . For a C-like interface, when compiling without ISO C++ 2011 compatibility, see Pipeline Primitives Interface . 7.28.4. Pipeline Primitives Interface \\uf0c1 Pipeline primitives are a C-like interface for memcpy_async functionality. The pipeline primitives interface is available by including the header. When compiling without ISO C++ 2011 compatibility, include the header.'},\n",
       " {'id': 256,\n",
       "  'content': '7.28.4.1. memcpy_async Primitive \\uf0c1 void __pipeline_memcpy_async ( void * __restrict__ dst_shared , const void * __restrict__ src_global , size_t size_and_align , size_t zfill = 0 ); Request that the following operation be submitted for asynchronous evaluation: size_t i = 0 ; for (; i #include using barrier = cuda :: barrier ; namespace ptx = cuda :: ptx ; static constexpr size_t buf_len = 1024 ; __global__ void add_one_kernel ( int * data , size_t offset ) { // Shared memory buffer. The destination shared memory buffer of // a bulk operations should be 16 byte aligned. __shared__ alignas ( 16 ) int smem_data [ buf_len ]; // 1. a) Initialize shared memory barrier with the number of threads participating in the barrier. // b) Make initialized barrier visible in async proxy. #pragma nv_diag_suppress static_var_with_dynamic_init __shared__ barrier bar ; if ( threadIdx . x == 0 ) { init ( & bar , blockDim . x ); // a) ptx :: fence_proxy_async ( ptx :: space_shared ); // b) } __syncthreads (); // 2. Initiate TMA transfer to copy global to shared memory. if ( threadIdx .'},\n",
       " {'id': 257,\n",
       "  'content': 'x == 0 ) { // 3a. cuda::memcpy_async arrives on the barrier and communicates // how many bytes are expected to come in (the transaction count) cuda :: memcpy_async ( smem_data , data + offset , cuda :: aligned_size_t ( sizeof ( smem_data )), bar ); } // 3b. All threads arrive on the barrier barrier :: arrival_token token = bar . arrive (); // 3c.'},\n",
       " {'id': 258,\n",
       "  'content': 'Wait for the data to have arrived. bar . wait ( std :: move ( token )); // 4. Compute saxpy and write back to shared memory for ( int i = threadIdx . x ; i ()); } } Barrier initialization . The barrier is initialized with the number of threads participating in the block. As a result, the barrier will flip only if all threads have arrived on this barrier. Shared memory barriers are described in more detail in Asynchronous Data Copies using cuda::barrier . To make the initialized barrier visible to subsequent bulk-asynchronous copies, the fence.proxy.async.shared::cta instruction is used. This instruction ensures that subsequent bulk-asynchronous copy operations operate on the initialized barrier. TMA read . The bulk-asynchronous copy instruction directs the hardware to copy a large chunk of data into shared memory, and to update the transaction count of the shared memory barrier after completing the read. In general, issuing as few bulk copies with as big a size as possible results in the best performance. Because the copy can be performed asynchronously by the hardware, it is not necessary to split the copy into smaller chunks. The thread that initiates the bulk-asynchronous copy operation arrives at the barrier using mbarrier.expect_tx . This is automatically performed by cuda::memcpy_async . This tells the barrier that the thread has arrived and also how many bytes (tx / transactions) are expected to arrive. Only a single thread has to update the expected transaction count. If multiple threads update the transaction count, the expected transaction will be the sum of the updates. The barrier will only flip once all threads have arrived and all bytes have arrived. Once the barrier has flipped, the bytes are safe to read from shared memory, both by the threads as well as by subsequent bulk-asynchronous copies. More information about barrier transaction accounting can be found in the PTX ISA . Barrier wait . Waiting for the barrier to flip is done using mbarrier.try_wait . It can either return true, indicating that the wait is over, or return false, which may mean that the wait timed out. The while loop waits for completion, and retries on time-out. SMEM write and sync . The increment of the buffer values reads and writes to shared memory. To make the writes visible to subsequent bulk-asynchronous copies, the fence.proxy.async.shared::cta instruction is used. This orders the writes to shared memory before subsequent reads from bulk-asynchronous copy operations, which read through the async proxy. So each thread first orders the writes to objects in shared memory in the async proxy via the fence.proxy.async.shared::cta , and these operations by all threads are ordered before the async operation performed in thread 0 using __syncthreads() . TMA write and sync . The write from shared to global memory is again initiated by a single thread. The completion of the write is not tracked by a shared memory barrier. Instead, a thread-local mechanism is used. Multiple writes can be batched into a so-called bulk async-group . Afterwards, the thread can wait for all operations in this group to have completed reading from shared memory (as in the code above) or to have completed writing to global memory, making the writes visible to the initiating thread. For more information, refer to the PTX ISA documentation of cp.async.bulk.wait_group . Note that the bulk-asynchronous and non-bulk asynchronous copy instructions have different async-groups: there exist both cp.async.wait_group and cp.async.bulk.wait_group instructions. The bulk-asynchronous instructions have specific alignment requirements on their source and destination addresses. More information can be found in the table below. Table 7 Alignment requirements for one-dimensional bulk-asynchronous operations in Compute Capability 9.0. \\uf0c1 Address / Size Alignment Global memory address Must be 16 byte aligned. Shared memory address Must be 16 byte aligned. Shared memory barrier address Must be 8 byte aligned (this is guaranteed by cuda::barrier ). Size of transfer Must be a multiple of 16 bytes. 7.29.2. Using TMA to transfer multi-dimensional arrays \\uf0c1 The primary difference between the one-dimensional and multi-dimensional case is that a tensor map must be created on the host and passed to the CUDA kernel. This section describes how to create a tensor map using the CUDA driver API, how to pass it to device, and how to use it on device. Driver API . A tensor map is created using the cuTensorMapEncodeTiled driver API. This API can be accessed by linking to the driver directly ( -lcuda ) or by using the cudaGetDriverEntryPoint API. Below, we show how to get a pointer to the cuTensorMapEncodeTiled API. For more information, refer to Driver Entry Point Access . #include // PFN_cuTensorMapEncodeTiled, CUtensorMap PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled () { // Get pointer to cuGetProcAddress cudaDriverEntryPointQueryResult driver_status ; void * cuGetProcAddress_ptr = nullptr ; CUDA_CHECK ( cudaGetDriverEntryPoint ( \"cuGetProcAddress\" , & cuGetProcAddress_ptr , cudaEnableDefault , & driver_status )); assert ( driver_status == cudaDriverEntryPointSuccess ); PFN_cuGetProcAddress_v12000 cuGetProcAddress = reinterpret_cast ( cuGetProcAddress_ptr ); // Use cuGetProcAddress to get a pointer to the CTK 12.0 version of cuTensorMapEncodeTiled CUdriverProcAddressQueryResult symbol_status ; void * cuTensorMapEncodeTiled_ptr = nullptr ; CUresult res = cuGetProcAddress ( \"cuTensorMapEncodeTiled\" , & cuTensorMapEncodeTiled_ptr , 12000 , CU_GET_PROC_ADDRESS_DEFAULT , & symbol_status ); assert ( res == CUDA_SUCCESS && symbol_status == CU_GET_PROC_ADDRESS_SUCCESS ); return reinterpret_cast ( cuTensorMapEncodeTiled_ptr ); } Creation . Creating a tensor map requires many parameters.'},\n",
       " {'id': 259,\n",
       "  'content': 'Among them are the base pointer to an array in global memory, the size of the array (in number of elements), the stride from one row to the next (in bytes), the size of the shared memory buffer (in number of elements). The code below creates a tensor map to describe a two-dimensional row-major array of size GMEM_HEIGHT x GMEM_WIDTH . Note the order of the parameters: the fastest moving dimension comes first. CUtensorMap tensor_map {}; // rank is the number of dimensions of the array. constexpr uint32_t rank = 2 ; uint64_t size [ rank ] = { GMEM_WIDTH , GMEM_HEIGHT }; // The stride is the number of bytes to traverse from the first element of one row to the next. // It must be a multiple of 16. uint64_t stride [ rank - 1 ] = { GMEM_WIDTH * sizeof ( int )}; // The box_size is the size of the shared memory buffer that is used as the // destination of a TMA transfer. uint32_t box_size [ rank ] = { SMEM_WIDTH , SMEM_HEIGHT }; // The distance between elements in units of sizeof(element). A stride of 2 // can be used to load only the real component of a complex-valued tensor, for instance. uint32_t elem_stride [ rank ] = { 1 , 1 }; // Get a function pointer to the cuTensorMapEncodeTiled driver API. auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled (); // Create the tensor descriptor. CUresult res = cuTensorMapEncodeTiled ( & tensor_map , // CUtensorMap *tensorMap, CUtensorMapDataType :: CU_TENSOR_MAP_DATA_TYPE_INT32 , rank , // cuuint32_t tensorRank, tensor_ptr , // void *globalAddress, size , // const cuuint64_t *globalDim, stride , // const cuuint64_t *globalStrides, box_size , // const cuuint32_t *boxDim, elem_stride , // const cuuint32_t *elementStrides, // Interleave patterns can be used to accelerate loading of values that // are less than 4 bytes long. CUtensorMapInterleave :: CU_TENSOR_MAP_INTERLEAVE_NONE , // Swizzling can be used to avoid shared memory bank conflicts. CUtensorMapSwizzle :: CU_TENSOR_MAP_SWIZZLE_NONE , // L2 Promotion can be used to widen the effect of a cache-policy to a wider // set of L2 cache lines. CUtensorMapL2promotion :: CU_TENSOR_MAP_L2_PROMOTION_NONE , // Any element that is outside of bounds will be set to zero by the TMA transfer. CUtensorMapFloatOOBfill :: CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE ); Host-to-device transfer . A bulk tensor asynchronous operations require the tensor map to be in immutable memory. This can be achieved by using constant memory or by passing the tensor map as a const __grid_constant__ parameter to a kernel. When passing the tensor map as a parameter, some versions of the GCC C++ compiler issue the warning “the ABI for passing parameters with 64-byte alignment has changed in GCC 4.6”. This warning can be ignored. __global__ void kernel ( const __grid_constant__ CUtensorMap tensor_map ) { // Use tensor_map here. }\\nint main () { CUtensorMap map ; // [ ..Initialize map.. ] kernel >> ( map ); } As an alternative to the __grid_constant__ kernel parameter, a global constant variable can be used. An example is included below. __constant__ CUtensorMap global_tensor_map ; __global__ void kernel () { // Use global_tensor_map here. }\\nint main () { CUtensorMap local_tensor_map ; // [ ..Initialize map.. ] cudaMemcpyToSymbol ( global_tensor_map , & local_tensor_map , sizeof ( CUtensorMap )); kernel >> (); } The following example copies the tensor map to global device memory. Using a pointer to a tensor map in global device memory is undefined behavior and will lead to silent and difficult to track down bugs. __device__ CUtensorMap global_tensor_map ; __global__ void kernel ( CUtensorMap * tensor_map ) { // Do *not* use tensor_map here. Using a global memory pointer is // undefined behavior and can fail silently and unreliably. }\\nint main () { CUtensorMap local_tensor_map ; // [ ..Initialize map.. ] cudaMemcpy ( global_tensor_map , & local_tensor_map , sizeof ( CUtensorMap )); kernel >> ( global_tensor_map ); } Use . The kernel below loads a 2D tile of size SMEM_HEIGHT x SMEM_WIDTH from a larger 2D array. The top-left corner of the tile is indicated by the indices x and y . The tile is loaded into shared memory, modified, and written back to global memory. #include // CUtensormap #include using barrier = cuda :: barrier ; namespace cde = cuda :: device :: experimental ; __global__ void kernel ( const __grid_constant__ CUtensorMap tensor_map , int x , int y ) { // The destination shared memory buffer of a bulk tensor operation should be // 128 byte aligned. __shared__ alignas ( 128 ) int smem_buffer [ SMEM_HEIGHT ][ SMEM_WIDTH ]; // Initialize shared memory barrier with the number of threads participating in the barrier. x == 0 ) { // Initialize barrier. All `blockDim.x` threads in block participate. init ( & bar , blockDim . x ); // Make initialized barrier visible in async proxy. cde :: fence_proxy_async_shared_cta (); } // Syncthreads so initialized barrier is visible to all threads. __syncthreads (); barrier :: arrival_token token ; if ( threadIdx . x == 0 ) { // Initiate bulk tensor copy. cde :: cp_async_bulk_tensor_2d_global_to_shared ( & smem_buffer , & tensor_map , x , y , bar ); // Arrive on the barrier and tell how many bytes are expected to come in. token = cuda :: device :: barrier_arrive_tx ( bar , 1 , sizeof ( smem_buffer )); } else { // Other threads just arrive. token = bar . arrive (); } // Wait for the data to have arrived. wait ( std :: move ( token )); // Symbolically modify a value in shared memory. smem_buffer [ 0 ][ threadIdx . x ] += threadIdx . x ; // Wait for shared memory writes to be visible to TMA engine. cde :: fence_proxy_async_shared_cta (); __syncthreads (); // After syncthreads, writes by all threads are visible to TMA engine. // Initiate TMA transfer to copy shared memory to global memory if ( threadIdx . x == 0 ) { cde :: cp_async_bulk_tensor_2d_shared_to_global ( & tensor_map , x , y , & smem_buffer ); // Wait for TMA transfer to have finished reading shared memory. // Create a \"bulk async-group\" out of the previous bulk copy operation. cde :: cp_async_bulk_commit_group (); // Wait for the group to have completed reading from shared memory. cde :: cp_async_bulk_wait_group_read (); } // Destroy barrier. This invalidates the memory region of the barrier. If // further computations were to take place in the kernel, this allows the // memory location of the shared memory barrier to be reused. x == 0 ) { ( & bar ) ->~ barrier (); } } Negative indices and out of bounds . When part of the tile that is being read from global to shared memory is out of bounds, the shared memory that corresponds to the out of bounds area is zero-filled. The top-left corner indices of the tile may also be negative. When writing from shared to global memory, parts of the tile may be out of bounds, but the top left corner cannot have any negative indices. Size and stride . The size of a tensor is the number of elements along one dimension. All sizes must be greater than one. The stride is the number of bytes between elements of the same dimension. For instance, a 4 x 4 matrix of integers has sizes 4 and 4. Since it has 4 bytes per element, the strides are 4 and 16 bytes. Due to alignment requirements, a 4 x 3 row-major matrix of integers must have strides of 4 and 16 bytes as well. Each row is padded with 4 extra bytes to ensure that the start of the next row is aligned to 16 bytes. For more information regarding alignment, refer to Table Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9.0. . Table 8 Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9.0. Global memory sizes Must be greater than or equal to one.'},\n",
       " {'id': 260,\n",
       "  'content': 'Does not have to be a multiple of 16 bytes. Global memory strides Must be multiples of 16 bytes. Shared memory address Must be 128 byte aligned. 7.29.2.1. Multi-dimensional TMA PTX wrappers \\uf0c1 Below, the PTX instructions are ordered by their use in the example code above. The cp.async.bulk.tensor instructions initiate a bulk tensor asynchronous copy between global and shared memory. The wrappers below read from global to shared memory and write from shared to global memory. // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_1d_global_to_shared ( void * dest , const CUtensorMap * tensor_map , int c0 , cuda :: barrier & bar ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_2d_global_to_shared ( void * dest , const CUtensorMap * tensor_map , int c0 , int c1 , cuda :: barrier & bar ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_3d_global_to_shared ( void * dest , const CUtensorMap * tensor_map , int c0 , int c1 , int c2 , cuda :: barrier & bar ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_4d_global_to_shared ( void * dest , const CUtensorMap * tensor_map , int c0 , int c1 , int c2 , int c3 , cuda :: barrier & bar ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_5d_global_to_shared ( void * dest , const CUtensorMap * tensor_map , int c0 , int c1 , int c2 , int c3 , int c4 , cuda :: barrier & bar ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_1d_shared_to_global ( const CUtensorMap * tensor_map , int c0 , const void * src ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_2d_shared_to_global ( const CUtensorMap * tensor_map , int c0 , int c1 , const void * src ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_3d_shared_to_global ( const CUtensorMap * tensor_map , int c0 , int c1 , int c2 , const void * src ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_4d_shared_to_global ( const CUtensorMap * tensor_map , int c0 , int c1 , int c2 , int c3 , const void * src ); // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor inline __device__ void cuda :: device :: experimental :: cp_async_bulk_tensor_5d_shared_to_global ( const CUtensorMap * tensor_map , int c0 , int c1 , int c2 , int c3 , int c4 , const void * src ); 7.30. Profiler Counter Function \\uf0c1 Each multiprocessor has a set of sixteen hardware counters that an application can increment with a single instruction by calling the __prof_trigger() function.'},\n",
       " {'id': 261,\n",
       "  'content': 'void __prof_trigger ( int counter ); increments by one per warp the per-multiprocessor hardware counter of index counter . Counters 8 to 15 are reserved and should not be used by applications. The value of counters 0, 1, …, 7 can be obtained via nvprof by nvprof --events prof_trigger_0x where x is 0, 1, …, 7. All counters are reset before each kernel launch (note that when collecting counters, kernel launches are synchronous as mentioned in Concurrent Execution between Host and Device ). 7.31. Assertion \\uf0c1 Assertion is only supported by devices of compute capability 2.x and higher. void assert ( int expression ); stops the kernel execution if expression is equal to zero. If the program is run within a debugger, this triggers a breakpoint and the debugger can be used to inspect the current state of the device. Otherwise, each thread for which expression is equal to zero prints a message to stderr after synchronization with the host via cudaDeviceSynchronize() , cudaStreamSynchronize() , or cudaEventSynchronize() . The format of this message is as follows: ::: block: [blockId.x,blockId.x,blockIdx.z], thread: [threadIdx.x,threadIdx.y,threadIdx.z] Assertion `` failed. Any subsequent host-side synchronization calls made for the same device will return cudaErrorAssert . No more commands can be sent to this device until cudaDeviceReset() is called to reinitialize the device. If expression is different from zero, the kernel execution is unaffected. For example, the following program from source file test.cu #include __global__ void testAssert ( void ) { int is_one = 1 ; int should_be_one = 0 ; // This will have no effect assert ( is_one ); // This will halt kernel execution assert ( should_be_one ); } int main ( int argc , char * argv []) { testAssert >> (); cudaDeviceSynchronize (); return 0 ; } will output: test.cu:19: void testAssert(): block: [0,0,0], thread: [0,0,0] Assertion `should_be_one` failed. Assertions are for debugging purposes. They can affect performance and it is therefore recommended to disable them in production code. They can be disabled at compile time by defining the NDEBUG preprocessor macro before including assert.h . Note that expression should not be an expression with side effects (something like (++i > 0) , for example), otherwise disabling the assertion will affect the functionality of the code.'},\n",
       " {'id': 262,\n",
       "  'content': '7.32. Trap function \\uf0c1 A trap operation can be initiated by calling the __trap() function from any device thread. void __trap (); The execution of the kernel is aborted and an interrupt is raised in the host program. 7.33. Breakpoint Function \\uf0c1 Execution of a kernel function can be suspended by calling the __brkpt() function from any device thread. void __brkpt (); 7.34. Formatted Output \\uf0c1 Formatted output is only supported by devices of compute capability 2.x and higher. int printf ( const char * format [, arg , ...]); prints formatted output from a kernel to a host-side output stream. The in-kernel printf() function behaves in a similar way to the standard C-library printf() function, and the user is referred to the host system’s manual pages for a complete description of printf() behavior. In essence, the string passed in as format is output to a stream on the host, with substitutions made from the argument list wherever a format specifier is encountered. Supported format specifiers are listed below. The printf() command is executed as any other device-side function: per-thread, and in the context of the calling thread. From a multi-threaded kernel, this means that a straightforward call to printf() will be executed by every thread, using that thread’s data as specified. Multiple versions of the output string will then appear at the host stream, once for each thread which encountered the printf() . It is up to the programmer to limit the output to a single thread if only a single output string is desired (see Examples for an illustrative example). Unlike the C-standard printf() , which returns the number of characters printed, CUDA’s printf() returns the number of arguments parsed. If no arguments follow the format string, 0 is returned.'},\n",
       " {'id': 263,\n",
       "  'content': 'If the format string is NULL, -1 is returned. If an internal error occurs, -2 is returned.'},\n",
       " {'id': 264,\n",
       "  'content': '7.34.1. Format Specifiers \\uf0c1 As for standard printf() , format specifiers take the form: %[flags][width][.precision][size]type The following fields are supported (see widely-available documentation for a complete description of all behaviors): Flags: \\'#\\' \\' \\' \\'0\\' \\'+\\' \\'-\\' Width: \\'*\\' \\'0-9\\' Precision: \\'0-9\\' Size: \\'h\\' \\'l\\' \\'ll\\' Type: \"%cdiouxXpeEfgGaAs\" Note that CUDA’s printf() will accept any combination of flag, width, precision, size and type, whether or not overall they form a valid format specifier. In other words, “ %hd ” will be accepted and printf will expect a double-precision variable in the corresponding location in the argument list. 7.34.2. Limitations \\uf0c1 Final formatting of the printf() output takes place on the host system. This means that the format string must be understood by the host-system’s compiler and C library. Every effort has been made to ensure that the format specifiers supported by CUDA’s printf function form a universal subset from the most common host compilers, but exact behavior will be host-OS-dependent. As described in Format Specifiers , printf() will accept all combinations of valid flags and types. This is because it cannot determine what will and will not be valid on the host system where the final output is formatted. The effect of this is that output may be undefined if the program emits a format string which contains invalid combinations. The printf() command can accept at most 32 arguments in addition to the format string. Additional arguments beyond this will be ignored, and the format specifier output as-is. Owing to the differing size of the long type on 64-bit Windows platforms (four bytes on 64-bit Windows platforms, eight bytes on other 64-bit platforms), a kernel which is compiled on a non-Windows 64-bit machine but then run on a win64 machine will see corrupted output for all format strings which include “ %ld ”. It is recommended that the compilation platform matches the execution platform to ensure safety. The output buffer for printf() is set to a fixed size before kernel launch (see Associated Host-Side API ). It is circular and if more output is produced during kernel execution than can fit in the buffer, older output is overwritten. It is flushed only when one of these actions is performed: Kernel launch via >> or cuLaunchKernel() (at the start of the launch, and if the CUDA_LAUNCH_BLOCKING environment variable is set to 1, at the end of the launch as well), Synchronization via cudaDeviceSynchronize() , cuCtxSynchronize() , cudaStreamSynchronize() , cuStreamSynchronize() , cudaEventSynchronize() , or cuEventSynchronize() , Memory copies via any blocking version of cudaMemcpy*() or cuMemcpy*() , Module loading/unloading via cuModuleLoad() or cuModuleUnload() , Context destruction via cudaDeviceReset() or cuCtxDestroy() . Prior to executing a stream callback added by cudaStreamAddCallback or cuStreamAddCallback . Note that the buffer is not flushed automatically when the program exits. The user must call cudaDeviceReset() or cuCtxDestroy() explicitly, as shown in the examples below. Internally printf() uses a shared data structure and so it is possible that calling printf() might change the order of execution of threads. In particular, a thread which calls printf() might take a longer execution path than one which does not call printf() , and that path length is dependent upon the parameters of the printf() . Note, however, that CUDA makes no guarantees of thread execution order except at explicit __syncthreads() barriers, so it is impossible to tell whether execution order has been modified by printf() or by other scheduling behavior in the hardware. 7.34.3. Associated Host-Side API \\uf0c1 The following API functions get and set the size of the buffer used to transfer the printf() arguments and internal metadata to the host (default is 1 megabyte): cudaDeviceGetLimit(size_t* size,cudaLimitPrintfFifoSize) cudaDeviceSetLimit(cudaLimitPrintfFifoSize, size_t size) 7.34.4. Examples \\uf0c1 The following code sample: #include __global__ void helloCUDA ( float f ) { printf ( \"Hello thread %d, f=%f \\\\n \" , threadIdx . x , f ); } int main () { helloCUDA >> ( 1.2345f ); cudaDeviceSynchronize (); return 0 ; } will output: Hello thread 2, f=1.2345 Hello thread 1, f=1.2345 Hello thread 4, f=1.2345 Hello thread 0, f=1.2345 Hello thread 3, f=1.2345 Notice how each thread encounters the printf() command, so there are as many lines of output as there were threads launched in the grid. As expected, global values (i.e., float f ) are common between all threads, and local values (i.e., threadIdx.x ) are distinct per-thread. The following code sample: #include __global__ void helloCUDA ( float f ) { if ( threadIdx . x == 0 ) printf ( \"Hello thread %d, f=%f \\\\n \" , threadIdx . x , f ) ; } int main () { helloCUDA >> ( 1.2345f ); cudaDeviceSynchronize (); return 0 ; } will output: Hello thread 0, f=1.2345 Self-evidently, the if() statement limits which threads will call printf , so that only a single line of output is seen. 7.35.'},\n",
       " {'id': 265,\n",
       "  'content': 'Dynamic Global Memory Allocation and Operations \\uf0c1 Dynamic global memory allocation and operations are only supported by devices of compute capability 2.x and higher. __host__ __device__ void * malloc ( size_t size ); __device__ void * __nv_aligned_device_malloc ( size_t size , size_t align ); __host__ __device__ void free ( void * ptr ); allocate and free memory dynamically from a fixed-size heap in global memory. __host__ __device__ void * memcpy ( void * dest , const void * src , size_t size ); copy size bytes from the memory location pointed by src to the memory location pointed by dest . __host__ __device__ void * memset ( void * ptr , int value , size_t size ); set size bytes of memory block pointed by ptr to value (interpreted as an unsigned char). The CUDA in-kernel malloc() function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the request. The returned pointer is guaranteed to be aligned to a 16-byte boundary. The CUDA in-kernel __nv_aligned_device_malloc() function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the requested size or alignment. The address of the allocated memory will be a multiple of align . align must be a non-zero power of 2. The CUDA in-kernel free() function deallocates the memory pointed to by ptr , which must have been returned by a previous call to malloc() or __nv_aligned_device_malloc() . If ptr is NULL, the call to free() is ignored. Repeated calls to free() with the same ptr has undefined behavior. The memory allocated by a given CUDA thread via malloc() or __nv_aligned_device_malloc() remains allocated for the lifetime of the CUDA context, or until it is explicitly released by a call to free() . It can be used by any other CUDA threads even from subsequent kernel launches. Any CUDA thread may free memory allocated by another thread, but care should be taken to ensure that the same pointer is not freed more than once. 7.35.1. Heap Memory Allocation \\uf0c1 The device memory heap has a fixed size that must be specified before any program using malloc() , __nv_aligned_device_malloc() or free() is loaded into the context. A default heap of eight megabytes is allocated if any program uses malloc() or __nv_aligned_device_malloc() without explicitly specifying the heap size. The following API functions get and set the heap size: cudaDeviceGetLimit(size_t* size, cudaLimitMallocHeapSize) cudaDeviceSetLimit(cudaLimitMallocHeapSize, size_t size) The heap size granted will be at least size bytes. cuCtxGetLimit() and cudaDeviceGetLimit() return the currently requested heap size. The actual memory allocation for the heap occurs when a module is loaded into the context, either explicitly via the CUDA driver API (see Module ), or implicitly via the CUDA runtime API (see CUDA Runtime ). If the memory allocation fails, the module load will generate a CUDA_ERROR_SHARED_OBJECT_INIT_FAILED error. Heap size cannot be changed once a module load has occurred and it does not resize dynamically according to need. Memory reserved for the device heap is in addition to memory allocated through host-side CUDA API calls such as cudaMalloc() . 7.35.2. Interoperability with Host Memory API \\uf0c1 Memory allocated via device malloc() or __nv_aligned_device_malloc() cannot be freed using the runtime (i.e., by calling any of the free memory functions from Device Memory ). Similarly, memory allocated via the runtime (i.e., by calling any of the memory allocation functions from Device Memory ) cannot be freed via free() . In addition, memory allocated by a call to malloc() or __nv_aligned_device_malloc() in device code cannot be used in any runtime or driver API calls (i.e. cudaMemcpy, cudaMemset, etc).'},\n",
       " {'id': 266, 'content': '7.35.3.'},\n",
       " {'id': 267,\n",
       "  'content': 'Examples \\uf0c1 7.35.3.1. Per Thread Allocation \\uf0c1 The following code sample: #include #include __global__ void mallocTest () { size_t size = 123 ; char * ptr = ( char * ) malloc ( size ); memset ( ptr , 0 , size ); printf ( \"Thread %d got pointer: %p \\\\n \" , threadIdx . x , ptr ); free ( ptr ); } int main () { // Set a heap size of 128 megabytes. Note that this must // be done before any kernel is launched. cudaDeviceSetLimit ( cudaLimitMallocHeapSize , 128 * 1024 * 1024 ); mallocTest >> (); cudaDeviceSynchronize (); return 0 ; } will output: Thread 0 got pointer : 00057020 Thread 1 got pointer : 000570 8 c Thread 2 got pointer : 000570f 8 Thread 3 got pointer : 00057164 Thread 4 got pointer : 000571 d0 Notice how each thread encounters the malloc() and memset() commands and so receives and initializes its own allocation. (Exact pointer values will vary: these are illustrative.)\\n7.35.3.2. Per Thread Block Allocation \\uf0c1 #include __global__ void mallocTest () { __shared__ int * data ; // The first thread in the block does the allocation and then // shares the pointer with all other threads through shared memory, // so that access can easily be coalesced. // 64 bytes per thread are allocated. x == 0 ) { size_t size = blockDim . x * 64 ; data = ( int * ) malloc ( size ); } __syncthreads (); // Check for failure if ( data == NULL ) return ; // Threads index into the memory, ensuring coalescence int * ptr = data ; for ( int i = 0 ; i >> (); cudaDeviceSynchronize (); return 0 ; } 7.35.3.3. Allocation Persisting Between Kernel Launches \\uf0c1 #include #include #define NUM_BLOCKS 20 __device__ int * dataptr [ NUM_BLOCKS ]; // Per-block pointer __global__ void allocmem () { // Only the first thread in the block does the allocation // since we want only one allocation per block. x == 0 ) dataptr [ blockIdx . x ] = ( int * ) malloc ( blockDim . x * 4 ); __syncthreads (); // Check for failure if ( dataptr [ blockIdx . x ] == NULL ) return ; // Zero the data with all threads in parallel dataptr [ blockIdx . x ][ threadIdx . x ] = 0 ; } // Simple example: store thread ID into each element __global__ void usemem () { int * ptr = dataptr [ blockIdx . x ]; if ( ptr != NULL ) ptr [ threadIdx . x ; } // Print the content of the buffer before freeing it __global__ void freemem () { int * ptr = dataptr [ blockIdx . x ]; if ( ptr != NULL ) printf ( \"Block %d, Thread %d: final value = %d \\\\n \" , blockIdx . x , threadIdx . x , ptr [ threadIdx . x ]); // Only free from one thread! x == 0 ) free ( ptr ); } int main () { cudaDeviceSetLimit ( cudaLimitMallocHeapSize , 128 * 1024 * 1024 ); // Allocate memory allocmem >> (); // Use memory usemem >> (); usemem >> (); usemem >> (); // Free memory freemem >> (); cudaDeviceSynchronize (); return 0 ; } 7.36. Execution Configuration \\uf0c1 Any call to a __global__ function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams). The execution configuration is specified by inserting an expression of the form >> between the function name and the parenthesized argument list, where: Dg is of type dim3 (see dim3 ) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched; Db is of type dim3 (see dim3 ) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block; Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in __shared__ ; Ns is an optional argument which defaults to 0; S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0. As an example, a function declared as __global__ void Func ( float * parameter ); must be called like this: Func >> ( parameter ); The arguments to the execution configuration are evaluated before the actual function arguments. The function call will fail if Dg or Db are greater than the maximum sizes allowed for the device as specified in Compute Capabilities , or if Ns is greater than the maximum amount of shared memory available on the device, minus the amount of shared memory required for static allocation. Compute capability 9.0 and above allows users to specify compile time thread block cluster dimensions, so that the kernel can use the cluster hierarchy in CUDA. Compile time cluster dimension can be specified using __cluster_dims__([x, [y, [z]]]) . The example below shows compile time cluster size of 2 in X dimension and 1 in Y and Z dimension. __global__ void __cluster_dims__ ( 2 , 1 , 1 ) Func ( float * parameter ); Thread block cluster dimensions can also be specified at runtime and kernel with the cluster can be launched using cudaLaunchKernelEx API. The API takes a configuration arugument of type cudaLaunchConfig_t , kernel function pointer and kernel arguments. Runtime kernel configuration is shown in the example below. __global__ void Func ( float * parameter ); // Kernel invocation with runtime cluster size { cudaLaunchConfig_t config = { 0 }; // The grid dimension is not affected by cluster launch, and is still enumerated // using number of blocks. gridDim = Dg ; config .'},\n",
       " {'id': 268,\n",
       "  'content': 'blockDim = Db ; config . dynamicSmemBytes = Ns ; cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ]. numAttrs = 1 ; float * parameter ; cudaLaunchKernelEx ( & config , Func , parameter ); } 7.37. Launch Bounds \\uf0c1 As discussed in detail in Multiprocessor Level , the fewer registers a kernel uses, the more threads and thread blocks are likely to reside on a multiprocessor, which can improve performance. Therefore, the compiler uses heuristics to minimize register usage while keeping register spilling (see Device Memory Accesses ) and instruction count to a minimum. An application can optionally aid these heuristics by providing additional information to the compiler in the form of launch bounds that are specified using the __launch_bounds__() qualifier in the definition of a __global__ function: __global__ void __launch_bounds__ ( maxThreadsPerBlock , minBlocksPerMultiprocessor , maxBlocksPerCluster ) MyKernel (...) { ... } maxThreadsPerBlock specifies the maximum number of threads per block with which the application will ever launch MyKernel() ; it compiles to the .maxntid PTX directive. minBlocksPerMultiprocessor is optional and specifies the desired minimum number of resident blocks per multiprocessor; it compiles to the .minnctapersm PTX directive. maxBlocksPerCluster is optional and specifies the desired maximum number thread blocks per cluster with which the application will ever launch MyKernel() ; it compiles to the .maxclusterrank PTX directive. If launch bounds are specified, the compiler first derives from them the upper limit L on the number of registers the kernel should use to ensure that minBlocksPerMultiprocessor blocks (or a single block if minBlocksPerMultiprocessor is not specified) of maxThreadsPerBlock threads can reside on the multiprocessor (see Hardware Multithreading for the relationship between the number of registers used by a kernel and the number of registers allocated per block). The compiler then optimizes register usage in the following way: If the initial register usage is higher than L , the compiler reduces it further until it becomes less or equal to L , usually at the expense of more local memory usage and/or higher number of instructions; If the initial register usage is lower than L If maxThreadsPerBlock is specified and minBlocksPerMultiprocessor is not, the compiler uses maxThreadsPerBlock to determine the register usage thresholds for the transitions between n and n+1 resident blocks (i.e., when using one less register makes room for an additional resident block as in the example of Multiprocessor Level ) and then applies similar heuristics as when no launch bounds are specified; If both minBlocksPerMultiprocessor and maxThreadsPerBlock are specified, the compiler may increase register usage as high as L to reduce the number of instructions and better hide single thread instruction latency. A kernel will fail to launch if it is executed with more threads per block than its launch bound maxThreadsPerBlock .'},\n",
       " {'id': 269,\n",
       "  'content': 'A kernel will fail to launch if it is executed with more thread blocks per cluster than its launch bound maxBlocksPerCluster . Per thread resources required by a CUDA kernel might limit the maximum block size in an unwanted way. In order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an SM, developers should include the single argument __launch_bounds__(maxThreadsPerBlock) which specifies the largest block size that the kernel will be launched with. Failure to do so could lead to “too many resources requested for launch” errors. Providing the two argument version of __launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor) can improve performance in some cases. The right value for minBlocksPerMultiprocessor should be determined using a detailed per kernel analysis. Optimal launch bounds for a given kernel will usually differ across major architecture revisions. The sample code below shows how this is typically handled in device code using the __CUDA_ARCH__ macro introduced in Application Compatibility #define THREADS_PER_BLOCK 256 #if __CUDA_ARCH__ >= 200 #define MY_KERNEL_MAX_THREADS (2 * THREADS_PER_BLOCK) #define MY_KERNEL_MIN_BLOCKS 3 #else #define MY_KERNEL_MAX_THREADS THREADS_PER_BLOCK #define MY_KERNEL_MIN_BLOCKS 2 #endif // Device code __global__ void __launch_bounds__ ( MY_KERNEL_MAX_THREADS , MY_KERNEL_MIN_BLOCKS ) MyKernel (...) { ... } In the common case where MyKernel is invoked with the maximum number of threads per block (specified as the first parameter of __launch_bounds__() ), it is tempting to use MY_KERNEL_MAX_THREADS as the number of threads per block in the execution configuration: // Host code MyKernel >> (...); This will not work however since __CUDA_ARCH__ is undefined in host code as mentioned in Application Compatibility , so MyKernel will launch with 256 threads per block even when __CUDA_ARCH__ is greater or equal to 200. Instead the number of threads per block should be determined: Either at compile time using a macro that does not depend on __CUDA_ARCH__ , for example // Host code MyKernel >> (...); Or at runtime based on the compute capability // Host code cudaGetDeviceProperties ( & deviceProp , device ); int threadsPerBlock = ( deviceProp . major >= 2 ? 2 * THREADS_PER_BLOCK : THREADS_PER_BLOCK ); MyKernel >> (...); Register usage is reported by the --ptxas-options=-v compiler option. The number of resident blocks can be derived from the occupancy reported by the CUDA profiler (see Device Memory Accesses for a definition of occupancy). The __launch_bounds__() and __maxnreg__() qualifiers cannot be applied to the same kernel. Register usage can also be controlled for all __global__ functions in a file using the maxrregcount compiler option. The value of maxrregcount is ignored for functions with launch bounds. 7.38. Maximum Number of Registers per Thread \\uf0c1 To provide a mechanism for low-level performance tuning, CUDA C++ provides the __maxnreg()__ function qualifier to pass performance tuning information to the backend optimizing compiler. The __maxnreg__() qualifier specifies the maximum number of registers to be allocated to a single thread in a thread block. In the definition of a __global__ function: __global__ void __maxnreg__ ( maxNumberRegistersPerThread ) MyKernel (...) { ... } maxNumberRegistersPerThread specifies the maximum number of registers to be allocated to a single thread in a thread block of the kernel MyKernel() ; it compiles to the .maxnreg PTX directive. The value of maxrregcount is ignored for functions with the __maxnreg__ qualifier.'},\n",
       " {'id': 270,\n",
       "  'content': '7.39. #pragma unroll \\uf0c1 By default, the compiler unrolls small loops with a known trip count. The #pragma unroll directive however can be used to control unrolling of any given loop. It must be placed immediately before the loop and only applies to that loop. It is optionally followed by an integral constant expression (ICE) 13 . If the ICE is absent, the loop will be completely unrolled if its trip count is constant. If the ICE evaluates to 1, the compiler will not unroll the loop. The pragma will be ignored if the ICE evaluates to a non-positive integer or to an integer greater than the maximum value representable by the int data type. Examples: struct S1_t { static const int value = 4 ; }; template __device__ void foo ( int * p1 , int * p2 ) { // no argument specified, loop will be completely unrolled #pragma unroll for ( int i = 0 ; i ( p1 , p2 ); } 7.40. SIMD Video Instructions \\uf0c1 PTX ISA version 3.0 includes SIMD (Single Instruction, Multiple Data) video instructions which operate on pairs of 16-bit values and quads of 8-bit values. These are available on devices of compute capability 3.0. The SIMD video instructions are: vadd2, vadd4 vsub2, vsub4 vavrg2, vavrg4 vabsdiff2, vabsdiff4 vmin2, vmin4 vmax2, vmax4 vset2, vset4 PTX instructions, such as the SIMD video instructions, can be included in CUDA programs by way of the assembler, asm() , statement. The basic syntax of an asm() statement is: asm ( \"template-string\" : \"constraint\" ( output ) : \"constraint\" ( input ) \")); An example of using the vabsdiff4 PTX instruction is: asm ( \"vabsdiff4.u32.u32.u32.add\" \" %0, %1, %2, %3;\" : \"=r\" ( result ) : \"r\" ( A ), \"r\" ( B ), \"r\" ( C )); This uses the vabsdiff4 instruction to compute an integer quad byte SIMD sum of absolute differences. The absolute difference value is computed for each byte of the unsigned integers A and B in SIMD fashion. The optional accumulate operation ( .add ) is specified to sum these differences. Refer to the document “Using Inline PTX Assembly in CUDA” for details on using the assembly statement in your code. Refer to the PTX ISA documentation (“Parallel Thread Execution ISA Version 3.0” for example) for details on the PTX instructions for the version of PTX that you are using. 7.41.'},\n",
       " {'id': 271,\n",
       "  'content': 'Diagnostic Pragmas \\uf0c1 The following pragmas may be used to control the error severity used when a given diagnostic message is issued. #pragma nv_diag_suppress #pragma nv_diag_warning #pragma nv_diag_error #pragma nv_diag_default #pragma nv_diag_once Uses of these pragmas have the following form: #pragma nv_diag_xxx error_number, error_number ... The diagnostic affected is specified using an error number showed in a warning message. Any diagnostic may be overridden to be an error, but only warnings may have their severity suppressed or be restored to a warning after being promoted to an error. The nv_diag_default pragma is used to return the severity of a diagnostic to the one that was in effect before any pragmas were issued (i.e., the normal severity of the message as modified by any command-line options). The following example suppresses the \"declared but never referenced\" warning on the declaration of foo : #pragma nv_diag_suppress 177 void foo () { int i = 0 ; } #pragma nv_diag_default 177 void bar () { int i = 0 ; } The following pragmas may be used to save and restore the current diagnostic pragma state: #pragma nv_diagnostic push #pragma nv_diagnostic pop Examples: #pragma nv_diagnostic push #pragma nv_diag_suppress 177 void foo () { int i = 0 ; } #pragma nv_diagnostic pop void bar () { int i = 0 ; } Note that the pragmas only affect the nvcc CUDA frontend compiler; they have no effect on the host compiler. Removal Notice: The support of diagnostic pragmas without nv_ prefix are removed from CUDA 12.0, if the pragmas are inside the device code, warning unrecognized #pragma in device code will be emitted, otherwise they will be passed to the host compiler. If they are intended for CUDA code, use the pragmas with nv_ prefix instead. 11 When the enclosing __host__ function is a template, nvcc may currently fail to issue a diagnostic message in some cases; this behavior may change in the future. 12 The intent is to prevent the host compiler from encountering the call to the function if the host compiler does not support it. 13 ( 1 , 2 ) See the C++ Standard for definition of integral constant expression. 8.'},\n",
       " {'id': 272,\n",
       "  'content': 'Cooperative Groups \\uf0c1 8.1. Introduction \\uf0c1 Cooperative Groups is an extension to the CUDA programming model, introduced in CUDA 9, for organizing groups of communicating threads. Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions. Historically, the CUDA programming model has provided a single, simple construct for synchronizing cooperating threads: a barrier across all threads of a thread block, as implemented with the __syncthreads() intrinsic function. However, programmers would like to define and synchronize groups of threads at other granularities to enable greater performance, design flexibility, and software reuse in the form of “collective” group-wide function interfaces. In an effort to express broader patterns of parallel interaction, many performance-oriented programmers have resorted to writing their own ad hoc and unsafe primitives for synchronizing threads within a single warp, or across sets of thread blocks running on a single GPU. Whilst the performance improvements achieved have often been valuable, this has resulted in an ever-growing collection of brittle code that is expensive to write, tune, and maintain over time and across GPU generations. Cooperative Groups addresses this by providing a safe and future-proof mechanism to enable performant code. 8.2. What’s New in Cooperative Groups \\uf0c1 8.2.1. CUDA 12.2 \\uf0c1 barrier_arrive and barrier_wait member functions were added for grid_group and thread_block . Description of the API is available here .'},\n",
       " {'id': 273,\n",
       "  'content': '8.2.2. CUDA 12.1 \\uf0c1 invoke_one and invoke_one_broadcast APIs were added. 8.2.3. CUDA 12.0 \\uf0c1 The following experimental APIs are now moved to the main namespace: asynchronous reduce and scan update added in CUDA 11.7 thread_block_tile larger than 32 added in CUDA 11.1 It is no longer required to provide memory using the block_tile_memory object in order to create these large tiles on Compute Capability 8.0 or higher. 8.3. Programming Model Concept \\uf0c1 The Cooperative Groups programming model describes synchronization patterns both within and across CUDA thread blocks. It provides both the means for applications to define their own groups of threads, and the interfaces to synchronize them. It also provides new launch APIs that enforce certain restrictions and therefore can guarantee the synchronization will work. These primitives enable new patterns of cooperative parallelism within CUDA, including producer-consumer parallelism, opportunistic parallelism, and global synchronization across the entire Grid. The Cooperative Groups programming model consists of the following elements: Data types for representing groups of cooperating threads; Operations to obtain implicit groups defined by the CUDA launch API (e.g., thread blocks); Collectives for partitioning existing groups into new groups; Collective Algorithms for data movement and manipulation (e.g. memcpy_async, reduce, scan); An operation to synchronize all threads within the group; Operations to inspect the group properties; Collectives that expose low-level, group-specific and often HW accelerated, operations. The main concept in Cooperative Groups is that of objects naming the set of threads that are part of it. This expression of groups as first-class program objects improves software composition, since collective functions can receive an explicit object representing the group of participating threads. This object also makes programmer intent explicit, which eliminates unsound architectural assumptions that result in brittle code, undesirable restrictions upon compiler optimizations, and better compatibility with new GPU generations. To write efficient code, its best to use specialized groups (going generic loses a lot of compile time optimizations), and pass these group objects by reference to functions that intend to use these threads in some cooperative fashion. Cooperative Groups requires CUDA 9.0 or later. To use Cooperative Groups, include the header file: // Primary header is compatible with pre-C++11, collective algorithm headers require C++11 #include // Optionally include for memcpy_async() collective #include // Optionally include for reduce() collective #include // Optionally include for inclusive_scan() and exclusive_scan() collectives #include and use the Cooperative Groups namespace: using namespace cooperative_groups ; // Alternatively use an alias to avoid polluting the namespace with collective algorithms namespace cg = cooperative_groups ; The code can be compiled in a normal way using nvcc, however if you wish to use memcpy_async, reduce or scan functionality and your host compiler’s default dialect is not C++11 or higher, then you must add --std=c++11 to the command line. 8.3.1.'},\n",
       " {'id': 274,\n",
       "  'content': 'Composition Example \\uf0c1 To illustrate the concept of groups, this example attempts to perform a block-wide sum reduction. Previously, there were hidden constraints on the implementation when writing this code: __device__ int sum ( int * x , int n ) { // ... __syncthreads (); return total ; } __global__ void parallel_kernel ( float * x ) { // ... // Entire thread block must call sum sum ( x , n ); } All threads in the thread block must arrive at the __syncthreads() barrier, however, this constraint is hidden from the developer who might want to use sum(…) . With Cooperative Groups, a better way of writing this would be: __device__ int sum ( const thread_block & g , int * x , int n ) { // ... g . sync () return total ; } __global__ void parallel_kernel (...) { // ... // Entire thread block must call sum thread_block tb = this_thread_block (); sum ( tb , x , n ); // ... } 8.4.'},\n",
       " {'id': 275,\n",
       "  'content': 'Group Types \\uf0c1 8.4.1. Implicit Groups \\uf0c1 Implicit groups represent the launch configuration of the kernel. Regardless of how your kernel is written, it always has a set number of threads, blocks and block dimensions, a single grid and grid dimensions. In addition, if the multi-device cooperative launch API is used, it can have multiple grids (single grid per device). These groups provide a starting point for decomposition into finer grained groups which are typically HW accelerated and are more specialized for the problem the developer is solving. Although you can create an implicit group anywhere in the code, it is dangerous to do so. Creating a handle for an implicit group is a collective operation—all threads in the group must participate. If the group was created in a conditional branch that not all threads reach, this can lead to deadlocks or data corruption. For this reason, it is recommended that you create a handle for the implicit group upfront (as early as possible, before any branching has occurred) and use that handle throughout the kernel. Group handles must be initialized at declaration time (there is no default constructor) for the same reason and copy-constructing them is discouraged. 8.4.1.1. Thread Block Group \\uf0c1 Any CUDA programmer is already familiar with a certain group of threads: the thread block. The Cooperative Groups extension introduces a new datatype, thread_block , to explicitly represent this concept within the kernel. class thread_block; Constructed via: thread_block g = this_thread_block (); Public Member Functions: static void sync() : Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive()) thread_block::arrival_token barrier_arrive() : Arrive on the thread_block barrier, returns a token that needs to be passed into barrier_wait() . More details here void barrier_wait(thread_block::arrival_token&& t) : Wait on the thread_block barrier, takes arrival token returned from barrier_arrive() as a rvalue reference. More details here static unsigned int thread_rank() : Rank of the calling thread within [0, num_threads) static dim3 group_index() : 3-Dimensional index of the block within the launched grid static dim3 thread_index() : 3-Dimensional index of the thread within the launched block static dim3 dim_threads() : Dimensions of the launched block in units of threads static unsigned int num_threads() : Total number of threads in the group Legacy member functions (aliases): static unsigned int size() : Total number of threads in the group (alias of num_threads() ) static dim3 group_dim() : Dimensions of the launched block (alias of dim_threads() ) Example: /// Loading an integer from global into shared memory __global__ void kernel ( int * globalInput ) { __shared__ int x ; thread_block g = this_thread_block (); // Choose a leader in the thread block if ( g . thread_rank () == 0 ) { // load from global into shared for all threads to work with x = ( * globalInput ); } // After loading data into shared memory, you want to synchronize // if all threads in your thread block need to see it g . sync (); // equivalent to __syncthreads(); } Note: that all threads in the group must participate in collective operations, or the behavior is undefined. Related: The thread_block datatype is derived from the more generic thread_group datatype, which can be used to represent a wider class of groups. 8.4.1.2. Cluster Group \\uf0c1 This group object represents all the threads launched in a single cluster. Refer to Thread Block Clusters . The APIs are available on all hardware with Compute Capability 9.0+. In such cases, when a non-cluster grid is launched, the APIs assume a 1x1x1 cluster. class cluster_group; Constructed via: cluster_group g = this_cluster (); Public Member Functions: static void sync() : Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive()) static cluster_group::arrival_token barrier_arrive() : Arrive on the cluster barrier, returns a token that needs to be passed into barrier_wait() . More details here static void barrier_wait(cluster_group::arrival_token&& t) : Wait on the cluster barrier, takes arrival token returned from barrier_arrive() as a rvalue reference. More details here static unsigned int thread_rank() : Rank of the calling thread within [0, num_threads) static unsigned int block_rank() : Rank of the calling block within [0, num_blocks) static unsigned int num_threads() : Total number of threads in the group static unsigned int num_blocks() : Total number of blocks in the group static dim3 dim_threads() : Dimensions of the launched cluster in units of threads static dim3 dim_blocks() : Dimensions of the launched cluster in units of blocks static dim3 block_index() : 3-Dimensional index of the calling block within the launched cluster static unsigned int query_shared_rank(const void *addr) : Obtain the block rank to which a shared memory address belongs static T* map_shared_rank(T *addr, int rank) : Obtain the address of a shared memory variable of another block in the cluster Legacy member functions (aliases): static unsigned int size() : Total number of threads in the group (alias of num_threads() ) 8.4.1.3. Grid Group \\uf0c1 This group object represents all the threads launched in a single grid.'},\n",
       " {'id': 276,\n",
       "  'content': 'APIs other than sync() are available at all times, but to be able to synchronize across the grid, you need to use the cooperative launch API. class grid_group; Constructed via: grid_group g = this_grid (); Public Member Functions: bool is_valid() const : Returns whether the grid_group can synchronize void sync() const : Synchronize the threads named in the group, equivalent to g.barrier_wait(g.barrier_arrive()) grid_group::arrival_token barrier_arrive() : Arrive on the grid barrier, returns a token that needs to be passed into barrier_wait() . More details here void barrier_wait(grid_group::arrival_token&& t) : Wait on the grid barrier, takes arrival token returned from barrier_arrive() as a rvalue reference. More details here static unsigned long long thread_rank() : Rank of the calling thread within [0, num_threads) static unsigned long long block_rank() : Rank of the calling block within [0, num_blocks) static unsigned long long cluster_rank() : Rank of the calling cluster within [0, num_clusters) static unsigned long long num_threads() : Total number of threads in the group static unsigned long long num_blocks() : Total number of blocks in the group static unsigned long long num_clusters() : Total number of clusters in the group static dim3 dim_blocks() : Dimensions of the launched grid in units of blocks static dim3 dim_clusters() : Dimensions of the launched grid in units of clusters static dim3 block_index() : 3-Dimensional index of the block within the launched grid static dim3 cluster_index() : 3-Dimensional index of the cluster within the launched grid Legacy member functions (aliases): static unsigned long long size() : Total number of threads in the group (alias of num_threads() ) static dim3 group_dim() : Dimensions of the launched grid (alias of dim_blocks() ) 8.4.1.4. Multi Grid Group \\uf0c1 This group object represents all the threads launched across all devices of a multi-device cooperative launch. Unlike the grid.group, all the APIs require that you have used the appropriate launch API. class multi_grid_group; Constructed via: // Kernel must be launched with the cooperative multi-device API multi_grid_group g = this_multi_grid (); Public Member Functions: bool is_valid() const : Returns whether the multi_grid_group can be used void sync() const : Synchronize the threads named in the group unsigned long long num_threads() const : Total number of threads in the group unsigned long long thread_rank() const : Rank of the calling thread within [0, num_threads) unsigned int grid_rank() const : Rank of the grid within [0,num_grids] unsigned int num_grids() const : Total number of grids launched Legacy member functions (aliases): unsigned long long size() const : Total number of threads in the group (alias of num_threads() ) Deprecation Notice: multi_grid_group has been deprecated in CUDA 11.3 for all devices. 8.4.2.'},\n",
       " {'id': 277,\n",
       "  'content': 'Explicit Groups \\uf0c1 8.4.2.1. Thread Block Tile \\uf0c1 A templated version of a tiled group, where a template parameter is used to specify the size of the tile - with this known at compile time there is the potential for more optimal execution. template class thread_block_tile ; Constructed via: template _CG_QUALIFIER thread_block_tile tiled_partition ( const ParentT & g ) Size must be a power of 2 and less than or equal to 1024. Notes section describes extra steps needed to create tiles of size larger than 32 on hardware with Compute Capability 7.5 or lower. ParentT is the parent-type from which this group was partitioned. It is automatically inferred, but a value of void will store this information in the group handle rather than in the type. Public Member Functions: void sync() const : Synchronize the threads named in the group unsigned long long num_threads() const : Total number of threads in the group unsigned long long thread_rank() const : Rank of the calling thread within [0, num_threads) unsigned long long meta_group_size() const : Returns the number of groups created when the parent group was partitioned. unsigned long long meta_group_rank() const : Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size) T shfl(T var, unsigned int src_rank) const : Refer to Warp Shuffle Functions , Note: For sizes larger than 32 all threads in the group have to specify the same src_rank, otherwise the behavior is undefined. T shfl_up(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32. T shfl_down(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32. T shfl_xor(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32. T any(int predicate) const : Refer to Warp Vote Functions T all(int predicate) const : Refer to Warp Vote Functions T ballot(int predicate) const : Refer to Warp Vote Functions , available only for sizes lower or equal to 32. unsigned int match_any(T val) const : Refer to Warp Match Functions , available only for sizes lower or equal to 32. unsigned int match_all(T val, int &pred) const : Refer to Warp Match Functions , available only for sizes lower or equal to 32. Legacy member functions (aliases): unsigned long long size() const : Total number of threads in the group (alias of num_threads() ) Notes: thread_block_tile templated data structure is being used here, the size of the group is passed to the tiled_partition call as a template parameter rather than an argument. shfl, shfl_up, shfl_down, and shfl_xor functions accept objects of any type when compiled with C++11 or later. This means it’s possible to shuffle non-integral types as long as they satisfy the below constraints: Qualifies as trivially copyable i.e., is_trivially_copyable::value == true sizeof(T) struct block_tile_memory ; MaxBlockSize Specifies the maximal number of threads in the current thread block. This parameter can be used to minimize the shared memory usage of block_tile_memory in kernels launched only with smaller thread counts. This block_tile_memory needs be then passed into cooperative_groups::this_thread_block , allowing the resulting thread_block to be partitioned into tiles of sizes larger than 32. Overload of this_thread_block accepting block_tile_memory argument is a collective operation and has to be called with all threads in the thread_block . block_tile_memory can be used on hardware with Compute Capability 8.0 or higher in order to be able to write one source targeting multiple different Compute Capabilities. It should consume no memory when instantiated in shared memory in cases where its not required. Examples: /// The following code will create two sets of tiled groups, of size 32 and 4 respectively: /// The latter has the provenance encoded in the type, while the first stores it in the handle thread_block block = this_thread_block (); thread_block_tile tile32 = tiled_partition ( block ); thread_block_tile tile4 = tiled_partition ( block ); /// The following code will create tiles of size 128 on all Compute Capabilities. /// block_tile_memory can be omitted on Compute Capability 8.0 or higher. __global__ void kernel (...) { // reserve shared memory for thread_block_tile usage, // specify that block size will be at most 256 threads. __shared__ block_tile_memory shared ; thread_block thb = this_thread_block ( shared ); // Create tiles with 128 threads. auto tile = tiled_partition ( thb ); // ... } 8.4.2.1.1. Warp-Synchronous Code Pattern \\uf0c1 Developers might have had warp-synchronous codes that they previously made implicit assumptions about the warp size and would code around that number. Now this needs to be specified explicitly. __global__ void cooperative_kernel (...) { // obtain default \"current thread block\" group thread_block my_block = this_thread_block (); // subdivide into 32-thread, tiled subgroups // Tiled subgroups evenly partition a parent group into // adjacent sets of threads - in this case each one warp in size auto my_tile = tiled_partition ( my_block ); // This operation will be performed by only the // first 32-thread tile of each block if ( my_tile . meta_group_rank () == 0 ) { // ...'},\n",
       " {'id': 278, 'content': 'my_tile .'},\n",
       " {'id': 279,\n",
       "  'content': 'sync (); } } 8.4.2.1.2. Single thread group \\uf0c1 Group representing the current thread can be obtained from this_thread function: thread_block_tile this_thread (); The following memcpy_async API uses a thread_group , to copy an int element from source to destination: #include #include cooperative_groups :: memcpy_async ( cooperative_groups :: this_thread (), dest , src , sizeof ( int )); More detailed examples of using this_thread to perform asynchronous copies can be found in the Single-Stage Asynchronous Data Copies using cuda::pipeline and Multi-Stage Asynchronous Data Copies using cuda::pipeline sections. 8.4.2.2. Coalesced Groups \\uf0c1 In CUDA’s SIMT architecture, at the hardware level the multiprocessor executes threads in groups of 32 called warps. If there exists a data-dependent conditional branch in the application code such that threads within a warp diverge, then the warp serially executes each branch disabling threads not on that path. The threads that remain active on the path are referred to as coalesced. Cooperative Groups has functionality to discover, and create, a group containing all coalesced threads. Constructing the group handle via coalesced_threads() is opportunistic. It returns the set of active threads at that point in time, and makes no guarantee about which threads are returned (as long as they are active) or that they will stay coalesced throughout execution (they will be brought back together for the execution of a collective but can diverge again afterwards). class coalesced_group; Constructed via: coalesced_group active = coalesced_threads (); Public Member Functions: void sync() const : Synchronize the threads named in the group unsigned long long num_threads() const : Total number of threads in the group unsigned long long thread_rank() const : Rank of the calling thread within [0, num_threads) unsigned long long meta_group_size() const : Returns the number of groups created when the parent group was partitioned. If this group was created by querying the set of active threads, e.g. coalesced_threads() the value of meta_group_size() will be 1. unsigned long long meta_group_rank() const : Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size). coalesced_threads() the value of meta_group_rank() will always be 0. T shfl(T var, unsigned int src_rank) const : Refer to Warp Shuffle Functions T shfl_up(T var, int delta) const : Refer to Warp Shuffle Functions T shfl_down(T var, int delta) const : Refer to Warp Shuffle Functions T any(int predicate) const : Refer to Warp Vote Functions T all(int predicate) const : Refer to Warp Vote Functions T ballot(int predicate) const : Refer to Warp Vote Functions unsigned int match_any(T val) const : Refer to Warp Match Functions unsigned int match_all(T val, int &pred) const : Refer to Warp Match Functions Legacy member functions (aliases): unsigned long long size() const : Total number of threads in the group (alias of num_threads() ) Notes: shfl, shfl_up, and shfl_down functions accept objects of any type when compiled with C++11 or later. This means it’s possible to shuffle non-integral types as long as they satisfy the below constraints: Qualifies as trivially copyable i.e. is_trivially_copyable::value == true sizeof(T) thread_block_tile tiled_partition ( const ParentT & g ); thread_group tiled_partition ( const thread_group & parent , unsigned int tilesz ); The tiled_partition method is a collective operation that partitions the parent group into a one-dimensional, row-major, tiling of subgroups. A total of ((size(parent)/tilesz) subgroups will be created, therefore the parent group size must be evenly divisible by the Size . The allowed parent groups are thread_block or thread_block_tile . The implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution. Functionality is limited to native hardware sizes, 1/2/4/8/16/32 and the cg::size(parent) must be greater than the Size parameter. The templated version of tiled_partition supports 64/128/256/512 sizes as well, but some additional steps are required on Compute Capability 7.5 or lower, refer to Thread Block Tile for details. Codegen Requirements: Compute Capability 5.0 minimum, C++11 for sizes larger than 32 Example: /// The following code will create a 32-thread tile thread_block block = this_thread_block (); thread_block_tile tile32 = tiled_partition ( block ); We can partition each of these groups into even smaller groups, each of size 4 threads: auto tile4 = tiled_partition ( tile32 ); // or using a general group // thread_group tile4 = tiled_partition(tile32, 4); If, for instance, if we were to then include the following line of code: if ( tile4 . thread_rank () == 0 ) printf ( \"Hello from tile4 rank 0 \\\\n \" ); then the statement would be printed by every fourth thread in the block: the threads of rank 0 in each tile4 group, which correspond to those threads with ranks 0,4,8,12,etc. in the block group.'},\n",
       " {'id': 280,\n",
       "  'content': '8.5.2. labeled_partition \\uf0c1 template coalesced_group labeled_partition ( const coalesced_group & g , Label label ); template coalesced_group labeled_partition ( const thread_block_tile & g , Label label ); The labeled_partition method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced. The implementation will evaluate a condition label and assign threads that have the same value for label into the same group. Label can be any integral type.'},\n",
       " {'id': 281,\n",
       "  'content': 'Note: This functionality is still being evaluated and may slightly change in the future.'},\n",
       " {'id': 282,\n",
       "  'content': 'Codegen Requirements: Compute Capability 7.0 minimum, C++11 8.5.3. binary_partition \\uf0c1 coalesced_group binary_partition ( const coalesced_group & g , bool pred ); template coalesced_group binary_partition ( const thread_block_tile & g , bool pred ); The binary_partition() method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced. The implementation will evaluate a predicate and assign threads that have the same value into the same group. This is a specialized form of labeled_partition() , where the label can only be 0 or 1. Codegen Requirements: Compute Capability 7.0 minimum, C++11 Example: /// This example divides a 32-sized tile into a group with odd /// numbers and a group with even numbers _global__ void oddEven ( int * inputArr ) { auto block = cg :: this_thread_block (); auto tile32 = cg :: tiled_partition ( block ); // inputArr contains random integers int elem = inputArr [ block . thread_rank ()]; // after this, tile32 is split into 2 groups, // a subtile where elem&1 is true and one where its false auto subtile = cg :: binary_partition ( tile32 , ( elem & 1 )); } 8.6. Group Collectives \\uf0c1 Cooperative Groups library provides a set of collective operations that can be performed by a group of threads. These operations require participation of all threads in the specified group in order to complete the operation. All threads in the group need to pass the same values for corresponding arguments to each collective call, unless different values are explicitly allowed in the argument description. Otherwise the behavior of the call is undefined.'},\n",
       " {'id': 283, 'content': '8.6.1.'},\n",
       " {'id': 284,\n",
       "  'content': 'Synchronization \\uf0c1 8.6.1.1. barrier_arrive and barrier_wait \\uf0c1 T :: arrival_token T::barrier_arrive (); void T::barrier_wait ( T :: arrival_token && ); barrier_arrive and barrier_wait member functions provide a synchronization API similar to cuda::barrier (read more) . Cooperative Groups automatically initializes the group barrier, but arrive and wait operations have an additional restriction resulting from collective nature of those operations: All threads in the group must arrive and wait at the barrier once per phase. When barrier_arrive is called with a group, result of calling any collective operation or another barrier arrival with that group is undefined until completion of the barrier phase is observed with barrier_wait call. Threads blocked on barrier_wait might be released from the synchronization before other threads call barrier_wait , but only after all threads in the group called barrier_arrive . Group type T can be any of the implicit groups .This allows threads to do independent work after they arrive and before they wait for the synchronization to resolve, allowing to hide some of the synchronization latency. barrier_arrive returns an arrival_token object that must be passed into the corresponding barrier_wait . Token is consumed this way and can not be used for another barrier_wait call. Example of barrier_arrive and barrier_wait used to synchronize initization of shared memory across the cluster: #include using namespace cooperative_groups ; void __device__ init_shared_data ( const thread_block & block , int * data ); void __device__ local_processing ( const thread_block & block ); void __device__ process_shared_data ( const thread_block & block , int * data ); __global__ void cluster_kernel () { extern __shared__ int array []; auto cluster = this_cluster (); auto block = this_thread_block (); // Use this thread block to initialize some shared state init_shared_data ( block , & array [ 0 ]); auto token = cluster . barrier_arrive (); // Let other blocks know this block is running and data was initialized // Do some local processing to hide the synchronization latency local_processing ( block ); // Map data in shared memory from the next block in the cluster int * dsmem = cluster . map_shared_rank ( & array [ 0 ], ( cluster . block_rank () + 1 ) % cluster . num_blocks ()); // Make sure all other blocks in the cluster are running and initialized shared data before accessing dsmem cluster . barrier_wait ( std :: move ( token )); // Consume data in distributed shared memory process_shared_data ( block , dsmem ); cluster . sync (); } 8.6.1.2. sync \\uf0c1 static void T::sync (); template void sync ( T & group ); sync synchronizes the threads named in the group. Group type T can be any of the existing group types, as all of them support synchronization. Its available as a member function in every group type or as a free function taking a group as parameter. If the group is a grid_group or a multi_grid_group the kernel must have been launched using the appropriate cooperative launch APIs. Equivalent to T.barrier_wait(T.barrier_arrive()) .'},\n",
       " {'id': 285, 'content': '8.6.2.'},\n",
       " {'id': 286,\n",
       "  'content': 'Data Transfer \\uf0c1 8.6.2.1. memcpy_async \\uf0c1 memcpy_async is a group-wide collective memcpy that utilizes hardware accelerated support for non-blocking memory transactions from global to shared memory. Given a set of threads named in the group, memcpy_async will move specified amount of bytes or elements of the input type through a single pipeline stage. Additionally for achieving best performance when using the memcpy_async API, an alignment of 16 bytes for both shared memory and global memory is required. It is important to note that while this is a memcpy in the general case, it is only asynchronous if the source is global memory and the destination is shared memory and both can be addressed with 16, 8, or 4 byte alignments. Asynchronously copied data should only be read following a call to wait or wait_prior which signals that the corresponding stage has completed moving data to shared memory. Having to wait on all outstanding requests can lose some flexibility (but gain simplicity). In order to efficiently overlap data transfer and execution, its important to be able to kick off an N+1 memcpy_async request while waiting on and operating on request N . To do so, use memcpy_async and wait on it using the collective stage-based wait_prior API. See wait and wait_prior for more details. Usage 1 template void memcpy_async ( const TyGroup & group , TyElem * __restrict__ _dst , const TyElem * __restrict__ _src , const TyShape & shape ); Performs a copy of ``shape`` bytes . Usage 2 template void memcpy_async ( const TyGroup & group , TyElem * __restrict__ dst , const TyDstLayout & dstLayout , const TyElem * __restrict__ src , const TySrcLayout & srcLayout ); Performs a copy of ``min(dstLayout, srcLayout)`` elements . If layouts are of type cuda::aligned_size_t , both must specify the same alignment. Errata The memcpy_async API introduced in CUDA 11.1 with both src and dst input layouts, expects the layout to be provided in elements rather than bytes. The element type is inferred from TyElem and has the size sizeof(TyElem) . If cuda::aligned_size_t type is used as the layout, the number of elements specified times sizeof(TyElem) must be a multiple of N and it is recommended to use std::byte or char as the element type. If specified shape or layout of the copy is of type cuda::aligned_size_t , alignment will be guaranteed to be at least min(16, N) . In that case both dst and src pointers need to be aligned to N bytes and the number of bytes copied needs to be a multiple of N. Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for asynchronicity, C++11 cooperative_groups/memcpy_async.h header needs to be included. Example: /// This example streams elementsPerThreadBlock worth of data from global memory /// into a limited sized shared memory (elementsInShared) block to operate on. #include #include namespace cg = cooperative_groups ; __global__ void kernel ( int * global_data ) { cg :: thread_block tb = cg :: this_thread_block (); const size_t elementsPerThreadBlock = 16 * 1024 ; const size_t elementsInShared = 128 ; __shared__ int local_smem [ elementsInShared ]; size_t copy_count ; size_t index = 0 ; while ( index void wait ( TyGroup & group ); template void wait_prior ( TyGroup & group ); wait and wait_prior collectives allow to wait for memcpy_async copies to complete. wait blocks calling threads until all previous copies are done. wait_prior allows that the latest NumStages are still not done and waits for all the previous requests. So with N total copies requested, it waits until the first N-NumStages are done and the last NumStages might still be in progress. Both wait and wait_prior will synchronize the named group. Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for asynchronicity, C++11 cooperative_groups/memcpy_async.h header needs to be included. Example: /// This example streams elementsPerThreadBlock worth of data from global memory /// into a limited sized shared memory (elementsInShared) block to operate on in /// multiple (two) stages. As stage N is kicked off, we can wait on and operate on stage N-1. #include #include namespace cg = cooperative_groups ; __global__ void kernel ( int * global_data ) { cg :: thread_block tb = cg :: this_thread_block (); const size_t elementsPerThreadBlock = 16 * 1024 + 64 ; const size_t elementsInShared = 128 ; __align__ ( 16 ) __shared__ int local_smem [ 2 ][ elementsInShared ]; int stage = 0 ; // First kick off an extra request size_t copy_count = elementsInShared ; size_t index = copy_count ; cg :: memcpy_async ( tb , local_smem [ stage ], elementsInShared , global_data , elementsPerThreadBlock - index ); while ( index ( tb ); // Its now available and we can work with local_smem[stage] here // (...) // // Calculate the amount fo data that was actually copied, for the next iteration. copy_count = min ( elementsInShared , elementsPerThreadBlock - index ); index += copy_count ; // A cg::sync(tb) might be needed here depending on whether // the work done with local_smem[stage] can release threads to race ahead or not // Wrap to the next stage stage ^= 1 ; } cg :: wait ( tb ); // The last local_smem[stage] can be handled here } 8.6.3. Data Manipulation \\uf0c1 8.6.3.1. reduce \\uf0c1 template auto reduce ( const TyGroup & group , TyArg && val , TyOp && op ) -> decltype ( op ( val , val )); reduce performs a reduction operation on the data provided by each thread named in the group passed in. This takes advantage of hardware acceleration (on compute 80 and higher devices) for the arithmetic add, min, or max operations and the logical AND, OR, or XOR, as well as providing a software fallback on older generation hardware. Only 4B types are accelerated by hardware. group : Valid group types are coalesced_group and thread_block_tile . val : Any type that satisfies the below requirements: Qualifies as trivially copyable i.e.'},\n",
       " {'id': 287,\n",
       "  'content': 'is_trivially_copyable::value == true sizeof(T) () . Reduce also supports lambdas and other function objects that can be invoked using operator() Asynchronous reduce template void reduce_update_async ( const TyGroup & group , TyAtomic & atomic , TyArg && val , TyOp && op ); template void reduce_store_async ( const TyGroup & group , TyAtomic & atomic , TyArg && val , TyOp && op ); template void reduce_store_async ( const TyGroup & group , TyArg * ptr , TyArg && val , TyOp && op ); *_async variants of the API are asynchronously calculating the result to either store to or update a specified destination by one of the participating threads, instead of returning it by each thread. To observe the effect of these asynchronous calls, calling group of threads or a larger group containing them need to be synchronized. In case of the atomic store or update variant, atomic argument can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library . This variant of the API is available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library. Result of the reduction is used to atomically update the atomic according to the specified op , eg. the result is atomically added to the atomic in case of cg::plus() . Type held by the atomic must match the type of TyArg . Scope of the atomic must include all the threads in the group and if multiple groups are using the same atomic concurrently, scope must include all threads in all groups using it. Atomic update is performed with relaxed memory ordering. In case of the pointer store variant, result of the reduction will be weakly stored into the dst pointer. Codegen Requirements: Compute Capability 5.0 minimum, Compute Capability 8.0 for HW acceleration, C++11.'},\n",
       " {'id': 288,\n",
       "  'content': 'cooperative_groups/reduce.h header needs to be included. Example of approximate standard deviation for integer vector: #include #include namespace cg = cooperative_groups ; /// Calculate approximate standard deviation of integers in vec __device__ int std_dev ( const cg :: thread_block_tile & tile , int * vec , int length ) { int thread_sum = 0 ; // calculate average first for ( int i = tile . thread_rank (); i allows cg::reduce() to know it can use hardware acceleration for addition int avg = cg :: reduce ( tile , thread_sum , cg :: plus ()) / length ; int thread_diffs_sum = 0 ; for ( int i = tile . thread_rank (); i ( cg :: reduce ( tile , thread_diffs_sum , cg :: plus ())) / length ; return static_cast ( sqrtf ( diff_sum )); } Example of block wide reduction: #include #include namespace cg = cooperative_groups ; /// The following example accepts input in *A and outputs a result into *sum /// It spreads the data equally within the block __device__ void block_reduce ( const int * A , int count , cuda :: atomic & total_sum ) { auto block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( block ); int thread_sum = 0 ; // Stride loop over all values, each thread accumulates its part of the array. for ( int i = block . thread_rank (); i allows cg::reduce() to know it can use hardware acceleration for addition cg :: reduce_update_async ( tile , total_sum , thread_sum , cg :: plus ()); // synchronize the block, to ensure all async reductions are ready block . sync (); } 8.6.3.2. Reduce Operators \\uf0c1 Below are the prototypes of function objects for some of the basic operations that can be done with reduce namespace cooperative_groups { template struct cg :: plus ; template struct cg :: less ; template struct cg :: greater ; template struct cg :: bit_and ; template struct cg :: bit_xor ; template struct cg :: bit_or ; } Reduce is limited to the information available to the implementation at compile time. Thus in order to make use of intrinsics introduced in CC 8.0, the cg:: namespace exposes several functional objects that mirror the hardware. These objects appear similar to those presented in the C++ STL, with the exception of less/greater . The reason for any difference from the STL is that these function objects are designed to actually mirror the operation of the hardware intrinsics. Functional description: cg::plus: Accepts two values and returns the sum of both using operator+. cg::less: Accepts two values and returns the lesser using operator is specialized within cg::reduce and calls __reduce_add_sync(...) on CC 8.0+ cg :: reduce ( tile , ( int ) val , cg :: plus ()); // cg::plus fails to match with an accelerator and instead performs a standard shuffle based reduction cg :: reduce ( tile , ( float ) val , cg :: plus ()); // While individual components of a vector are supported, reduce will not use hardware intrinsics for the following // It will also be necessary to define a corresponding operator for vector and any custom types that may be used int4 vec = {...}; cg :: reduce ( tile , vec , cg :: plus ()) // Finally lambdas and other function objects cannot be inspected for dispatch // and will instead perform shuffle based reductions using the provided function object. cg :: reduce ( tile , ( int ) val , []( int l , int r ) -> int { return l + r ;}); } 8.6.3.3. inclusive_scan and exclusive_scan \\uf0c1 template auto inclusive_scan ( const TyGroup & group , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal inclusive_scan ( const TyGroup & group , TyVal && val ); template auto exclusive_scan ( const TyGroup & group , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal exclusive_scan ( const TyGroup & group , TyVal && val ); inclusive_scan and exclusive_scan performs a scan operation on the data provided by each thread named in the group passed in. Result for each thread is a reduction of data from threads with lower thread_rank than that thread in case of exclusive_scan . inclusive_scan result also includes the calling thread data in the reduction. inclusive_scan and exclusive_scan also supports lambdas and other function objects that can be invoked using operator() . Overloads without this argument use cg::plus() . Scan update template auto inclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal inclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val ); template auto exclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal exclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val ); *_scan_update collectives take an additional argument atomic that can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library . These variants of the API are available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library. These variants will perform an update to the atomic according to op with value of the sum of input values of all threads in the group. Previous value of the atomic will be combined with the result of scan by each thread and returned. Type held by the atomic must match the type of TyVal . Following pseudocode illustrates how the update variant of scan works: /* inclusive_scan_update behaves as the following block, except both reduce and inclusive_scan is calculated simultaneously. auto total = reduce(group, val, op); TyVal old; if (group.thread_rank() == selected_thread) { atomicaly { old = atomic.load(); atomic.store(op(old, total)); } } old = group.shfl(old, selected_thread); return op(inclusive_scan(group, val, op), old); */ Codegen Requirements: Compute Capability 5.0 minimum, C++11. cooperative_groups/scan.h header needs to be included. Example: #include #include #include namespace cg = cooperative_groups ; __global__ void kernel () { auto thread_block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( thread_block ); unsigned int val = cg :: inclusive_scan ( tile , tile . thread_rank ()); printf ( \"%u: %u \\\\n \" , tile . thread_rank (), val ); } /* prints for each group: 0: 0 1: 1 2: 3 3: 6 4: 10 5: 15 6: 21 7: 28 */ Example of stream compaction using exclusive_scan: #include #include namespace cg = cooperative_groups ; // put data from input into output only if it passes test_fn predicate template __device__ int stream_compaction ( Group & g , Data * input , int count , TyFn && test_fn , Data * output ) { int per_thread = count / g . num_threads (); int thread_start = min ( g . thread_rank () * per_thread , count ); int my_count = min ( per_thread , count - thread_start ); // get all passing items from my part of the input // into a contagious part of the array and count them. int i = thread_start ; while ( i #include namespace cg = cooperative_groups ; // Buffer partitioning is static to make the example easier to follow, // but any arbitrary dynamic allocation scheme can be implemented by replacing this function. __device__ int calculate_buffer_space_needed ( cg :: thread_block_tile & tile ) { return tile . thread_rank () % 2 + 1 ; } __device__ int my_thread_data ( int i ) { return i ; } __global__ void kernel () { __shared__ extern int buffer []; __shared__ cuda :: atomic buffer_used ; auto block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( block ); buffer_used = 0 ; block . sync (); // each thread calculates buffer size it needs int buf_needed = calculate_buffer_space_needed ( tile ); // scan over the needs of each thread, result for each thread is an offset // of that thread’s part of the buffer. buffer_used is atomically updated with // the sum of all thread\\'s inputs, to correctly offset other tile’s allocations int buf_offset = cg :: exclusive_scan_update ( tile , buffer_used , buf_needed ); // each thread fills its own part of the buffer with thread specific data for ( int i = 0 ; i void invoke_one ( const Group & group , Fn && fn , Args && ... args ); template auto invoke_one_broadcast ( const Group & group , Fn && fn , Args && ... args ) -> decltype ( fn ( args ...)); invoke_one selects a single arbitrary thread from the calling group and uses that thread to call the supplied invocable fn with the supplied arguments args . In case of invoke_one_broadcast the result of the call is also distributed to all threads in the group and returned from this collective. Calling group can be synchronized with the selected thread before and/or after it calls the supplied invocable. It means that communication within the calling group is not allowed inside the supplied invocable body, otherwise forward progress is not guaranteed. Communication with threads outside of the calling group is allowed in the body of the supplied invocable. Thread selection mechanism is not guranteed to be deterministic. On devices with Compute Capability 9.0 or higher hardware acceleration might be used to select the thread when called with explicit group types . group : All group types are valid for invoke_one , coalesced_group and thread_block_tile are valid for invoke_one_broadcast . fn : Function or object that can be invoked using operator() . args : Parameter pack of types matching types of parameters of the supplied invocable fn . In case of invoke_one_broadcast the return type of the supplied invocable fn must satisfy the below requirements: Qualifies as trivially copyable i.e. is_trivially_copyable::value == true sizeof(T) #include namespace cg = cooperative_groups ; template __device__ unsigned int atomicAddOneRelaxed ( cuda :: atomic & atomic ) { auto g = cg :: coalesced_threads (); auto prev = cg :: invoke_one_broadcast ( g , [ & ] () { return atomic . fetch_add ( g . num_threads (), cuda :: memory_order_relaxed ); }); return prev + g . thread_rank (); } 8.7. Grid Synchronization \\uf0c1 Prior to the introduction of Cooperative Groups, the CUDA programming model only allowed synchronization between thread blocks at a kernel completion boundary. The kernel boundary carries with it an implicit invalidation of state, and with it, potential performance implications. For example, in certain use cases, applications have a large number of small kernels, with each kernel representing a stage in a processing pipeline. The presence of these kernels is required by the current CUDA programming model to ensure that the thread blocks operating on one pipeline stage have produced data before the thread block operating on the next pipeline stage is ready to consume it. In such cases, the ability to provide global inter thread block synchronization would allow the application to be restructured to have persistent thread blocks, which are able to synchronize on the device when a given stage is complete. To synchronize across the grid, from within a kernel, you would simply use the grid.sync() function: grid_group grid = this_grid (); grid . sync (); And when launching the kernel it is necessary to use, instead of the >> execution configuration syntax, the cudaLaunchCooperativeKernel CUDA runtime launch API or the CUDA driver equivalent . Example: To guarantee co-residency of the thread blocks on the GPU, the number of blocks launched needs to be carefully considered. For example, as many blocks as there are SMs can be launched as follows: int dev = 0 ; cudaDeviceProp deviceProp ; cudaGetDeviceProperties ( & deviceProp , dev ); // initialize, then launch cudaLaunchCooperativeKernel (( void * ) my_kernel , deviceProp . multiProcessorCount , numThreads , args ); Alternatively, you can maximize the exposed parallelism by calculating how many blocks can fit simultaneously per-SM using the occupancy calculator as follows: /// This will launch a grid that can maximally fill the GPU, on the default stream with kernel arguments int numBlocksPerSm = 0 ; // Number of threads my_kernel will be launched with int numThreads = 128 ; cudaDeviceProp deviceProp ; cudaGetDeviceProperties ( & deviceProp , dev ); cudaOccupancyMaxActiveBlocksPerMultiprocessor ( & numBlocksPerSm , my_kernel , numThreads , 0 ); // launch void * kernelArgs [] = { /* add kernel args */ }; dim3 dimBlock ( numThreads , 1 , 1 ); dim3 dimGrid ( deviceProp . multiProcessorCount * numBlocksPerSm , 1 , 1 ); cudaLaunchCooperativeKernel (( void * ) my_kernel , dimGrid , dimBlock , kernelArgs ); It is good practice to first ensure the device supports cooperative launches by querying the device attribute cudaDevAttrCooperativeLaunch : int dev = 0 ; int supportsCoopLaunch = 0 ; cudaDeviceGetAttribute ( & supportsCoopLaunch , cudaDevAttrCooperativeLaunch , dev ); which will set supportsCoopLaunch to 1 if the property is supported on device 0. Only devices with compute capability of 6.0 and higher are supported. In addition, you need to be running on either of these: The Linux platform without MPS The Linux platform with MPS and on a device with compute capability 7.0 or higher The latest Windows platform 8.8. Multi-Device Synchronization \\uf0c1 In order to enable synchronization across multiple devices with Cooperative Groups, use of the cudaLaunchCooperativeKernelMultiDevice CUDA API is required. This, a significant departure from existing CUDA APIs, will allow a single host thread to launch a kernel across multiple devices. In addition to the constraints and guarantees made by cudaLaunchCooperativeKernel , this API has additional semantics: This API will ensure that a launch is atomic, i.e. if the API call succeeds, then the provided number of thread blocks will launch on all specified devices. The functions launched via this API must be identical. No explicit checks are done by the driver in this regard because it is largely not feasible. It is up to the application to ensure this. No two entries in the provided cudaLaunchParams may map to the same device. All devices being targeted by this launch must be of the same compute capability - major and minor versions. The block size, grid size and amount of shared memory per grid must be the same across all devices. Note that this means the maximum number of blocks that can be launched per device will be limited by the device with the least number of SMs. Any user defined __device__ , __constant__ or __managed__ device global variables present in the module that owns the CUfunction being launched are independently instantiated on every device. The user is responsible for initializing such device global variables appropriately. Deprecation Notice: cudaLaunchCooperativeKernelMultiDevice has been deprecated in CUDA 11.3 for all devices. Example of an alternative approach can be found in the multi device conjugate gradient sample. Optimal performance in multi-device synchronization is achieved by enabling peer access via cuCtxEnablePeerAccess or cudaDeviceEnablePeerAccess for all participating devices. The launch parameters should be defined using an array of structs (one per device), and launched with cudaLaunchCooperativeKernelMultiDevice Example: cudaDeviceProp deviceProp ; cudaGetDeviceCount ( & numGpus ); // Per device launch parameters cudaLaunchParams * launchParams = ( cudaLaunchParams * ) malloc ( sizeof ( cudaLaunchParams ) * numGpus ); cudaStream_t * streams = ( cudaStream_t * ) malloc ( sizeof ( cudaStream_t ) * numGpus ); // The kernel arguments are copied over during launch // Its also possible to have individual copies of kernel arguments per device, but // the signature and name of the function/kernel must be the same. void * kernelArgs [] = { /* Add kernel arguments */ }; for ( int i = 0 ; i >> ( data ); tail_launch >> ( data ); } } void host_launch ( int * data ) { parent_launch >> ( data ); } 9.2.2.1.2.'},\n",
       " {'id': 289,\n",
       "  'content': 'Zero Copy Memory \\uf0c1 Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above. A kernel may not allocate or free zero-copy memory, but may use pointers to zero-copy passed in from the host program. 9.2.2.1.3. Constant Memory \\uf0c1 Constants may not be modified from the device. They may only be modified from the host, but the behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point during its lifetime is undefined. 9.2.2.1.4. Shared and Local Memory \\uf0c1 Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child. Behavior is undefined when an object in one of these locations is referenced outside of the scope within which it belongs, and may cause an error. The NVIDIA compiler will attempt to warn if it can detect that a pointer to local or shared memory is being passed as an argument to a kernel launch. At runtime, the programmer may use the __isGlobal() intrinsic to determine whether a pointer references global memory and so may safely be passed to a child launch. Note that calls to cudaMemcpy*Async() or cudaMemset*Async() may invoke new child kernels on the device in order to preserve stream semantics. As such, passing shared or local memory pointers to these APIs is illegal and will return an error. 9.2.2.1.5. Local Memory \\uf0c1 Local memory is private storage for an executing thread, and is not visible outside of that thread. It is illegal to pass a pointer to local memory as a launch argument when launching a child kernel. The result of dereferencing such a local memory pointer from a child will be undefined. For example the following is illegal, with undefined behavior if x_array is accessed by child_launch : int x_array [ 10 ]; // Creates x_array in parent\\'s local memory child_launch >> ( x_array ); It is sometimes difficult for a programmer to be aware of when a variable is placed into local memory by the compiler. As a general rule, all storage passed to a child kernel should be allocated explicitly from the global-memory heap, either with cudaMalloc() , new() or by declaring __device__ storage at global scope. For example: // Correct - \"value\" is global storage __device__ int value ; __device__ void x () { value = 5 ; child >> ( & value ); } // Invalid - \"value\" is local storage __device__ void y () { int value = 5 ; child >> ( & value ); } 9.2.2.1.6. Texture Memory \\uf0c1 Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses. Coherence for texture memory is enforced at the invocation of a child grid and when a child grid completes. This means that writes to memory prior to a child kernel launch are reflected in texture memory accesses of the child. Similarly to Global Memory above, writes to memory by a child are never guaranteed to be reflected in the texture memory accesses by a parent. The only way to access the modifications made by the threads in the child grid before the parent grid exits is via a kernel launched into the cudaStreamTailLaunch stream. Concurrent accesses by parent and child may result in inconsistent data.'},\n",
       " {'id': 290, 'content': '9.3.'},\n",
       " {'id': 291,\n",
       "  'content': \"Programming Interface \\uf0c1 9.3.1. CUDA C++ Reference \\uf0c1 This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism . The language interface and API available to CUDA kernels using CUDA C++ for Dynamic Parallelism, referred to as the Device Runtime , is substantially like that of the CUDA Runtime API available on the host. Where possible the syntax and semantics of the CUDA Runtime API have been retained in order to facilitate ease of code reuse for routines that may run in either the host or device environments. As with all code in CUDA C++, the APIs and code outlined here is per-thread code. This enables each thread to make unique, dynamic decisions regarding what kernel or operation to execute next. There are no synchronization requirements between threads within a block to execute any of the provided device runtime APIs, which enables the device runtime API functions to be called in arbitrarily divergent kernel code without deadlock. 9.3.1.1. Device-Side Kernel Launch \\uf0c1 Kernels may be launched from the device using the standard CUDA >> syntax: kernel_name >> ([ kernel arguments ]); Dg is of type dim3 and specifies the dimensions and size of the grid Db is of type dim3 and specifies the dimensions and size of each thread block Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per thread block for this call in addition to statically allocated memory. Ns is an optional argument that defaults to 0. S is of type cudaStream_t and specifies the stream associated with this call. The stream must have been allocated in the same grid where the call is being made. S is an optional argument that defaults to the NULL stream. 9.3.1.1.1. Launches are Asynchronous \\uf0c1 Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread. That is to say, the >> launch command will return immediately and the launching thread will continue to execute until it hits an implicit launch-synchronization point (such as at a kernel launched into the cudaStreamTailLaunch stream). The child grid launch is posted to the device and will execute independently of the parent thread. The child grid may begin execution at any time after launch, but is not guaranteed to begin execution until the launching thread reaches an implicit launch-synchronization point. 9.3.1.1.2. Launch Environment Configuration \\uf0c1 All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig() , and device limits returned from cudaDeviceGetLimit() ) will be inherited from the parent. Likewise, device limits such as stack size will remain as-configured. For host-launched kernels, per-kernel configurations set from the host will take precedence over the global setting. These configurations will be used when the kernel is launched from the device as well. It is not possible to reconfigure a kernel’s environment from the device. 9.3.1.2. Streams \\uf0c1 Both named and unnamed (NULL) streams are available from the device runtime. Named streams may be used by any thread within a grid, but stream handles may not be passed to other child/parent kernels. In other words, a stream should be treated as private to the grid in which it is created. Similar to host-side launch, work launched into separate streams may run concurrently, but actual concurrency is not guaranteed. Programs that depend upon concurrency between child kernels are not supported by the CUDA programming model and will have undefined behavior. The host-side NULL stream’s cross-stream barrier semantic is not supported on the device (see below for details). In order to retain semantic compatibility with the host runtime, all device streams must be created using the cudaStreamCreateWithFlags() API, passing the cudaStreamNonBlocking flag. The cudaStreamCreate() call is a host-runtime- only API and will fail to compile for the device. As cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, a kernel launched into the cudaStreamTailLaunch stream should be used instead when the application needs to know that stream-launched child kernels have completed. 9.3.1.2.1. The Implicit (NULL) Stream \\uf0c1 Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details). The device runtime offers a single implicit, unnamed stream shared between all threads in a thread block, but as all named streams must be created with the cudaStreamNonBlocking flag, work launched into the NULL stream will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks). 9.3.1.2.2. The Fire-and-Forget Stream \\uf0c1 The fire-and-forget named stream ( cudaStreamFireAndForget ) allows the user to launch fire-and-forget work with less boilerplate and without stream tracking overhead. It is functionally identical to, but faster than, creating a new stream per launch, and launching into that stream. Fire-and-forget launches are immediately scheduled for launch without any dependency on the completion of previously launched grids. No other grid launches can depend on the completion of a fire-and-forget launch, except through the implicit synchronization at the end of the parent grid. So a tail launch or the next grid in parent grid’s stream won’t launch before a parent grid’s fire-and-forget work has completed. // In this example, C2's launch will not wait for C1's completion __global__ void P ( ... ) { C1 >> ( ...\"},\n",
       " {'id': 292,\n",
       "  'content': '); C2 >> ( ... ); } The fire-and-forget stream cannot be used to record or wait on events. Attempting to do so results in cudaErrorInvalidValue . The fire-and-forget stream is not supported when compiled with CUDA_FORCE_CDP1_IF_SUPPORTED defined. Fire-and-forget stream usage requires compilation to be in 64-bit mode. 9.3.1.2.3. The Tail Launch Stream \\uf0c1 The tail launch named stream ( cudaStreamTailLaunch ) allows a grid to schedule a new grid for launch after its completion. It should be possible to to use a tail launch to achieve the same functionality as a cudaDeviceSynchronize() in most cases. Each grid has its own tail launch stream. All non-tail launch work launched by a grid is implicitly synchronized before the tail stream is kicked off. I.e. A parent grid’s tail launch does not launch until the parent grid and all work launched by the parent grid to ordinary streams or per-thread or fire-and-forget streams have completed. If two grids are launched to the same grid’s tail launch stream, the later grid does not launch until the earlier grid and all its descendent work has completed. // In this example, C2 will only launch after C1 completes. __global__ void P ( ... ) { C1 >> ( ...'},\n",
       " {'id': 293,\n",
       "  'content': '); C2 >> ( ... ); } Grids launched into the tail launch stream will not launch until the completion of all work by the parent grid, including all other grids (and their descendants) launched by the parent in all non-tail launched streams, including work executed or launched after the tail launch. // In this example, C will only launch after all X, F and P complete. __global__ void P ( ... ) { C >> ( ... ); X >> ( ...'},\n",
       " {'id': 294,\n",
       "  'content': '); F >> ( ... ) } The next grid in the parent grid’s stream will not be launched before a parent grid’s tail launch work has completed. In other words, the tail launch stream behaves as if it were inserted between its parent grid and the next grid in its parent grid’s stream. // In this example, P2 will only launch after C completes. __global__ void P1 ( ... ) { C >> ( ... ); } __global__ void P2 ( ... ) { } int main ( ... ) { ... P1 >> ( ... ); P2 >> ( ...'},\n",
       " {'id': 295,\n",
       "  'content': \"); ... } Each grid only gets one tail launch stream. To tail launch concurrent grids, it can be done like the example below. // In this example, C1 and C2 will launch concurrently after P's completion __global__ void T ( ... ) { C1 >> ( ... ); C2 >> ( ... ); } __global__ void P ( ... ) { ...\"},\n",
       " {'id': 296,\n",
       "  'content': 'T >> ( ... ); } The tail launch stream cannot be used to record or wait on events. The tail launch stream is not supported when compiled with CUDA_FORCE_CDP1_IF_SUPPORTED defined. Tail launch stream usage requires compilation to be in 64-bit mode. 9.3.1.3. Events \\uf0c1 Only the inter-stream synchronization capabilities of CUDA events are supported. This means that cudaStreamWaitEvent() is supported, but cudaEventSynchronize() , cudaEventElapsedTime() , and cudaEventQuery() are not. As cudaEventElapsedTime() is not supported, cudaEvents must be created via cudaEventCreateWithFlags() , passing the cudaEventDisableTiming flag. As with named streams, event objects may be shared between all threads within the grid which created them but are local to that grid and may not be passed to other kernels. Event handles are not guaranteed to be unique between grids, so using an event handle within a grid that did not create it will result in undefined behavior. 9.3.1.4. Synchronization \\uf0c1 It is up to the program to perform sufficient inter-thread synchronization, for example via a CUDA Event, if the calling thread is intended to synchronize with child grids invoked from other threads. As it is not possible to explicitly synchronize child work from a parent thread, there is no way to guarantee that changes occuring in child grids are visible to threads within the parent grid. 9.3.1.5.'},\n",
       " {'id': 297,\n",
       "  'content': 'Device Management \\uf0c1 Only the device on which a kernel is running will be controllable from that kernel. This means that device APIs such as cudaSetDevice() are not supported by the device runtime. The active device as seen from the GPU (returned from cudaGetDevice() ) will have the same device number as seen from the host system. The cudaDeviceGetAttribute() call may request information about another device as this API allows specification of a device ID as a parameter of the call. Note that the catch-all cudaGetDeviceProperties() API is not offered by the device runtime - properties must be queried individually. 9.3.1.6.'},\n",
       " {'id': 298,\n",
       "  'content': 'Memory Declarations \\uf0c1 9.3.1.6.1. Device and Constant Memory \\uf0c1 Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime. All kernels may read or write device variables, whether the kernel was initially launched by the host or device runtime. Equivalently, all kernels will have the same view of __constant__ s as declared at the module scope. 9.3.1.6.2. Textures and Surfaces \\uf0c1 CUDA supports dynamically created texture and surface objects 14 , where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host. The device runtime does not allow creation or destruction of texture or surface objects from within device code, but texture and surface objects created from the host may be used and passed around freely on the device. Regardless of where they are created, dynamically created texture objects are always valid and may be passed to child kernels from a parent. Note The device runtime does not support legacy module-scope (i.e., Fermi-style) textures and surfaces within a kernel launched from the device. Module-scope (legacy) textures may be created from the host and used in device code as for any kernel, but may only be used by a top-level kernel (i.e., the one which is launched from the host). 9.3.1.6.3. Shared Memory Variable Declarations \\uf0c1 In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernel’s caller via a launch configuration argument. Both types of declarations are valid under the device runtime. __global__ void permute ( int n , int * data ) { extern __shared__ int smem []; if ( n >> ( n / 2 , data ); permute >> ( n / 2 , data + n / 2 ); } } void host_launch ( int * data ) { permute >> ( 256 , data ); } 9.3.1.6.4. Symbol Addresses \\uf0c1 Device-side symbols (i.e., those marked __device__ ) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernel’s visible address space. This also applies to __constant__ symbols, although in this case the pointer will reference read-only data. Given that device-side symbols can be referenced directly, those CUDA runtime APIs which reference symbols (e.g., cudaMemcpyToSymbol() or cudaGetSymbolAddress() ) are redundant and hence not supported by the device runtime. Note this implies that constant data cannot be altered from within a running kernel, even ahead of a child kernel launch, as references to __constant__ space are read-only. 9.3.1.7. API Errors and Launch Failures \\uf0c1 As usual for the CUDA runtime, any function may return an error code. The last error code returned is recorded and may be retrieved via the cudaGetLastError() call. Errors are recorded per-thread, so that each thread can identify the most recent error that it has generated. The error code is of type cudaError_t . Similar to a host-side launch, device-side launches may fail for many reasons (invalid arguments, etc). The user must call cudaGetLastError() to determine if a launch generated an error, however lack of an error after launch does not imply the child kernel completed successfully. For device-side exceptions, e.g., access to an invalid address, an error in a child grid will be returned to the host. 9.3.1.7.1. Launch Setup APIs \\uf0c1 Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs. It is permitted for a CUDA application to call these APIs itself, with the same requirements as for PTX. In both cases, the user is then responsible for correctly populating all necessary data structures in the correct format according to specification.'},\n",
       " {'id': 299,\n",
       "  'content': 'Backwards compatibility is guaranteed in these data structures. As with host-side launch, the device-side operator >> maps to underlying kernel launch APIs. This is so that users targeting PTX will be able to enact a launch, and so that the compiler front-end can translate >> into these calls. Table 9 New Device-only Launch Implementation Functions \\uf0c1 Runtime API Launch Functions Description of Difference From Host Runtime Behaviour (behavior is identical if no description) cudaGetParameterBuffer Generated automatically from >> . Note different API to host equivalent. cudaLaunchDevice Generated automatically from >> . The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows: extern device cudaError_t cudaGetParameterBuffer ( void ** params ); extern __device__ cudaError_t cudaLaunchDevice ( void * kernel , void * params , dim3 gridDim , dim3 blockDim , unsigned int sharedMemSize = 0 , cudaStream_t stream = 0 ); 9.3.1.8. API Reference \\uf0c1 The portions of the CUDA Runtime API supported in the device runtime are detailed here. Host and device runtime APIs have identical syntax; semantics are the same except where indicated. The following table provides an overview of the API relative to the version available from the host. Table 10 Supported API Functions \\uf0c1 Runtime API Functions Details cudaDeviceGetCacheConfig cudaDeviceGetLimit cudaGetLastError Last error is per-thread state, not per-block state cudaPeekAtLastError cudaGetErrorString cudaGetDeviceCount cudaDeviceGetAttribute Will return attributes for any device cudaGetDevice Always returns current device ID as would be seen from host cudaStreamCreateWithFlags Must pass cudaStreamNonBlocking flag cudaStreamDestroy cudaStreamWaitEvent cudaEventCreateWithFlags Must pass cudaEventDisableTiming flag cudaEventRecord cudaEventDestroy cudaFuncGetAttributes cudaMemcpyAsync Notes about all memcpy/memset functions: Only async memcpy/set functions are supported Only device-to-device memcpy is permitted May not pass in local or shared memory pointers cudaMemcpy2DAsync cudaMemcpy3DAsync cudaMemsetAsync cudaMemset2DAsync cudaMemset3DAsync cudaRuntimeGetVersion cudaMalloc May not call cudaFree on the device on a pointer created on the host, and vice-versa cudaFree cudaOccupancyMaxActiveBlocksPerMultiprocessor cudaOccupancyMaxPotentialBlockSize cudaOccupancyMaxPotentialBlockSizeVariableSMem 9.3.2. Device-side Launch from PTX \\uf0c1 This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language.'},\n",
       " {'id': 300,\n",
       "  'content': 'It provides the low-level details related to supporting kernel launches at the PTX level. 9.3.2.1. Kernel Launch APIs \\uf0c1 Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer() . cudaLaunchDevice() launches the specified kernel with the parameter buffer that is obtained by calling cudaGetParameterBuffer() and filled with the parameters to the launched kernel. The parameter buffer can be NULL, i.e., no need to invoke cudaGetParameterBuffer() , if the launched kernel does not take any parameters. 9.3.2.1.1. cudaLaunchDevice \\uf0c1 At the PTX level, cudaLaunchDevice() needs to be declared in one of the two forms shown below before it is used. // PTX-level Declaration of cudaLaunchDevice() when .address_size is 64 . extern .'},\n",
       " {'id': 301, 'content': 'func (.'},\n",
       " {'id': 302,\n",
       "  'content': 'param . b32 func_retval0 ) cudaLaunchDevice ( . b64 func , . b64 parameterBuffer , . align 4 . b8 gridDimension [ 12 ], . b8 blockDimension [ 12 ], . b32 sharedMemSize , . b64 stream ) ; The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api.h . The function is defined in the cudadevrt system library, which must be linked with a program in order to use device-side kernel launch functionality. // CUDA-level declaration of cudaLaunchDevice() extern \"C\" __device__ cudaError_t cudaLaunchDevice ( void * func , void * parameterBuffer , dim3 gridDimension , dim3 blockDimension , unsigned int sharedMemSize , cudaStream_t stream ); The first parameter is a pointer to the kernel to be is launched, and the second parameter is the parameter buffer that holds the actual parameters to the launched kernel. The layout of the parameter buffer is explained in Parameter Buffer Layout , below. Other parameters specify the launch configuration, i.e., as grid dimension, block dimension, shared memory size, and the stream associated with the launch (please refer to Execution Configuration for the detailed description of launch configuration. 9.3.2.1.2. cudaGetParameterBuffer \\uf0c1 cudaGetParameterBuffer() needs to be declared at the PTX level before it’s used. The PTX-level declaration must be in one of the two forms given below, depending on address size: // PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64 . b64 func_retval0 ) cudaGetParameterBuffer ( . b64 alignment , . b64 size ) ; The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration: // CUDA-level Declaration of cudaGetParameterBuffer() extern \"C\" __device__ void * cudaGetParameterBuffer ( size_t alignment , size_t size ); The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement in bytes. In the current implementation, the parameter buffer returned by cudaGetParameterBuffer() is always guaranteed to be 64- byte aligned, and the alignment requirement parameter is ignored. However, it is recommended to pass the correct alignment requirement value - which is the largest alignment of any parameter to be placed in the parameter buffer - to cudaGetParameterBuffer() to ensure portability in the future. 9.3.2.2. Parameter Buffer Layout \\uf0c1 Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned. That is, each parameter must be placed at the n th byte in the parameter buffer, where n is the smallest multiple of the parameter size that is greater than the offset of the last byte taken by the preceding parameter. The maximum size of the parameter buffer is 4KB. For a more detailed description of PTX code generated by the CUDA compiler, please refer to the PTX-3.5 specification. 9.3.3. Toolkit Support for Dynamic Parallelism \\uf0c1 9.3.3.1. Including Device Runtime API in CUDA Code \\uf0c1 Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation. There is no need to include cuda_device_runtime_api.h explicitly. 9.3.3.2. Compiling and Linking \\uf0c1 When compiling and linking CUDA programs using dynamic parallelism with nvcc , the program will automatically link against the static device runtime library libcudadevrt . The device runtime is offered as a static library ( cudadevrt.lib on Windows, libcudadevrt.a under Linux), against which a GPU application that uses the device runtime must be linked. Linking of device libraries can be accomplished through nvcc and/or nvlink . Two simple examples are shown below. A device runtime program may be compiled and linked in a single step, if all required source files can be specified from the command line: $ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt It is also possible to compile CUDA .cu source files first to object files, and then link these together in a two-stage process: $ nvcc -arch=sm_75 -dc hello_world.cu -o hello_world.o $ nvcc -arch=sm_75 -rdc=true hello_world.o -o hello -lcudadevrt Please see the Using Separate Compilation section of The CUDA Driver Compiler NVCC guide for more details. 9.4.'},\n",
       " {'id': 303,\n",
       "  'content': 'Programming Guidelines \\uf0c1 9.4.1. Basics \\uf0c1 The device runtime is a functional subset of the host runtime. API level device management, kernel launching, device memcpy, stream management, and event management are exposed from the device runtime. Programming for the device runtime should be familiar to someone who already has experience with CUDA. Device runtime syntax and semantics are largely the same as that of the host API, with any exceptions detailed earlier in this document. The following example shows a simple Hello World program incorporating dynamic parallelism: #include __global__ void childKernel () { printf ( \"Hello \" ); } __global__ void tailKernel () { printf ( \"World! \\\\n \" ); } __global__ void parentKernel () { // launch child childKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return ; } // launch tail into cudaStreamTailLaunch stream // implicitly synchronizes: waits for child to complete tailKernel >> (); } int main ( int argc , char * argv []) { // launch parent parentKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return 1 ; } // wait for parent to complete if ( cudaSuccess != cudaDeviceSynchronize ()) { return 2 ; } return 0 ; } This program may be built in a single step from the command line as follows: $ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt 9.4.2. Performance \\uf0c1 9.4.2.1.'},\n",
       " {'id': 304,\n",
       "  'content': 'Dynamic-parallelism-enabled Kernel Overhead \\uf0c1 System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own. This overhead arises from the device runtime’s execution tracking and management software and may result in decreased performance. This overhead is, in general, incurred for applications that link against the device runtime library. 9.4.3. Implementation Restrictions and Limitations \\uf0c1 Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime. 9.4.3.1. Runtime \\uf0c1 9.4.3.1.1. Memory Footprint \\uf0c1 The device runtime system software reserves memory for various management purposes, in particular a reservation for tracking pending grid launches. Configuration controls are available to reduce the size of this reservation in exchange for certain launch limitations.'},\n",
       " {'id': 305, 'content': 'See Configuration Options , below, for details.'},\n",
       " {'id': 306,\n",
       "  'content': '9.4.3.1.2. Pending Kernel Launches \\uf0c1 When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes. This data is stored within a system-managed launch pool. The size of the fixed-size launch pool is configurable by calling cudaDeviceSetLimit() from the host and specifying cudaLimitDevRuntimePendingLaunchCount . 9.4.3.1.3. Configuration Options \\uf0c1 Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program. Limits must be set before any kernel is launched, and may not be changed while the GPU is actively running programs. The following named limits may be set: Limit Behavior cudaLimitDevRuntimePendingLaunchCount Controls the amount of memory set aside for buffering kernel launches and events which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources. When the buffer is full, an attempt to allocate a launch slot during a device side kernel launch will fail and return cudaErrorLaunchOutOfResources , while an attempt to allocate an event slot will fail and return cudaErrorMemoryAllocation . The default number of launch slots is 2048. Applications may increase the number of launch and/or event slots by setting cudaLimitDevRuntimePendingLaunchCount . The number of event slots allocated is twice the value of that limit. cudaLimitStackSize Controls the stack size in bytes of each GPU thread. The CUDA driver automatically increases the per-thread stack size for each kernel launch as needed. This size isn’t reset back to the original value after each launch. To set the per-thread stack size to a different value, cudaDeviceSetLimit() can be called to set this limit. The stack will be immediately resized, and if necessary, the device will block until all preceding requested tasks are complete. cudaDeviceGetLimit() can be called to get the current per-thread stack size. 9.4.3.1.4. Memory Allocation and Lifetime \\uf0c1 cudaMalloc() and cudaFree() have distinct semantics between the host and device environments. When invoked from the host, cudaMalloc() allocates a new region from unused device memory. When invoked from the device runtime these functions map to device-side malloc() and free() . This implies that within the device environment the total allocatable memory is limited to the device malloc() heap size, which may be smaller than the available unused device memory. Also, it is an error to invoke cudaFree() from the host program on a pointer which was allocated by cudaMalloc() on the device or vice-versa. cudaMalloc() on Host cudaMalloc() on Device cudaFree() on Host Supported Not Supported cudaFree() on Device Not Supported Supported Allocation limit Free device memory cudaLimitMallocHeapSize 9.4.3.1.5. SM Id and Warp Id \\uf0c1 Note that in PTX %smid and %warpid are defined as volatile values. The device runtime may reschedule thread blocks onto different SMs in order to more efficiently manage resources. As such, it is unsafe to rely upon %smid or %warpid remaining unchanged across the lifetime of a thread or thread block. 9.4.3.1.6. ECC Errors \\uf0c1 No notification of ECC errors is available to code within a CUDA kernel. ECC errors are reported at the host side once the entire launch tree has completed. Any ECC errors which arise during execution of a nested program will either generate an exception or continue execution (depending upon error and configuration).'},\n",
       " {'id': 307,\n",
       "  'content': '9.5. CDP2 vs CDP1 \\uf0c1 This section summarises the differences between, and the compatibility and interoperability of, the new (CDP2) and legacy (CDP1) CUDA Dynamic Parallelism interfaces. It also shows how to opt-out of the CDP2 interface on devices of compute capability less than 9.0. 9.5.1. Differences Between CDP1 and CDP2 \\uf0c1 Explicit device-side synchronization is no longer possible with CDP2 or on devices of compute capability 9.0 or higher. Implicit synchronization (such as tail launches) must be used instead. Attempting to query or set cudaLimitDevRuntimeSyncDepth (or CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH ) with CDP2 or on devices of compute capability 9.0 or higher results in cudaErrorUnsupportedLimit . CDP2 no longer has a virtualized pool for pending launches that don’t fit in the fixed-sized pool. cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of launch slots. For CDP2, there is a limit to the total number of events existing at once (note that events are destroyed only after a launch completes), equal to twice the pending launch count. cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of event slots. Streams are tracked per grid with CDP2 or on devices of compute capability 9.0 or higher, not per thread block. This allows work to be launched into a stream created by another thread block. Attempting to do so with the CDP1 results in cudaErrorInvalidValue . CDP2 introduces the tail launch ( cudaStreamTailLaunch ) and fire-and-forget ( cudaStreamFireAndForget ) named streams. CDP2 is supported only under 64-bit compilation mode. 9.5.2. Compatibility and Interoperability \\uf0c1 CDP2 is the default. Functions can be compiled with -DCUDA_FORCE_CDP1_IF_SUPPORTED to opt-out of using CDP2 on devices of compute capability less than 9.0. Function compiler with CUDA 12.0 and newer (default) Function compiled with pre-CUDA 12.0 or with CUDA 12.0 and newer with -DCUDA_FORCE_CDP1_IF_SUPPORTED specified Compilation Compile error if device code references cudaDeviceSynchronize . Compile error if code references cudaStreamTailLaunch or cudaStreamFireAndForget . Compile error if device code references cudaDeviceSynchronize and code is compiled for sm_90 or newer. Compute capability >> ( data ); cudaDeviceSynchronize (); } __syncthreads (); } void host_launch ( int * data ) { parent_launch >> ( data ); } 9.6.1.2.1.2. Zero Copy Memory (CDP1) \\uf0c1 See Zero Copy Memory , above, for CDP2 version of document. Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above. 9.6.1.2.1.3. Constant Memory (CDP1) \\uf0c1 See Constant Memory , above, for CDP2 version of document. Constants are immutable and may not be modified from the device, even between parent and child launches. That is to say, the value of all __constant__ variables must be set from the host prior to launch. Constant memory is inherited automatically by all child kernels from their respective parents. Taking the address of a constant memory object from within a kernel thread has the same semantics as for all CUDA programs, and passing that pointer from parent to child or from a child to parent is naturally supported. 9.6.1.2.1.4. Shared and Local Memory (CDP1) \\uf0c1 See Shared and Local Memory , above, for CDP2 version of document. Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child. 9.6.1.2.1.5. Local Memory (CDP1) \\uf0c1 See Local Memory , above, for CDP2 version of document. Local memory is private storage for an executing thread, and is not visible outside of that thread. For example: // Correct - \"value\" is global storage __device__ int value ; __device__ void x () { value = 5 ; child >> ( & value ); } // Invalid - \"value\" is local storage __device__ void y () { int value = 5 ; child >> ( & value ); } 9.6.1.2.1.6. Texture Memory (CDP1) \\uf0c1 See Texture Memory , above, for CDP2 version of document. Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses. Similarly, writes to memory by a child will be reflected in the texture memory accesses by a parent, but only after the parent synchronizes on the child’s completion. Warning Explicit synchronization with child kernels from a parent block (i.e. using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release. 9.6.2.'},\n",
       " {'id': 308,\n",
       "  'content': 'Programming Interface (CDP1) \\uf0c1 See Programming Interface , above, for CDP2 version of document. 9.6.2.1. CUDA C++ Reference (CDP1) \\uf0c1 See CUDA C++ Reference , above, for CDP2 version of document. This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism . 9.6.2.1.1. Device-Side Kernel Launch (CDP1) \\uf0c1 See Device-Side Kernel Launch , above, for CDP2 version of document. Kernels may be launched from the device using the standard CUDA >> syntax: kernel_name >> ([ kernel arguments ]); Dg is of type dim3 and specifies the dimensions and size of the grid Db is of type dim3 and specifies the dimensions and size of each thread block Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per thread block for this call and addition to statically allocated memory. The stream must have been allocated in the same thread block where the call is being made.'},\n",
       " {'id': 309, 'content': 'S is an optional argument that defaults to 0.'},\n",
       " {'id': 310,\n",
       "  'content': '9.6.2.1.1.1. Launches are Asynchronous (CDP1) \\uf0c1 See Launches are Asynchronous , above, for CDP2 version of document. Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread. That is to say, the >> launch command will return immediately and the launching thread will continue to execute until it hits an explicit launch-synchronization point such as cudaDeviceSynchronize() . The grid launch is posted to the device and will execute independently of the parent thread. The child grid may begin execution at any time after launch, but is not guaranteed to begin execution until the launching thread reaches an explicit launch-synchronization point. 9.6.2.1.1.2.'},\n",
       " {'id': 311,\n",
       "  'content': 'Launch Environment Configuration (CDP1) \\uf0c1 See Launch Environment Configuration , above, for CDP2 version of document. All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig() , and device limits returned from cudaDeviceGetLimit() ) will be inherited from the parent. 9.6.2.1.2.'},\n",
       " {'id': 312,\n",
       "  'content': 'Streams (CDP1) \\uf0c1 See Streams , above, for CDP2 version of document. Both named and unnamed (NULL) streams are available from the device runtime. Named streams may be used by any thread within a thread-block, but stream handles may not be passed to other blocks or child/parent kernels. In other words, a stream should be treated as private to the block in which it is created. Stream handles are not guaranteed to be unique between blocks, so using a stream handle within a block that did not allocate it will result in undefined behavior. As cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, cudaDeviceSynchronize() should be used instead when the application needs to know that stream-launched child kernels have completed. 9.6.2.1.2.1. The Implicit (NULL) Stream (CDP1) \\uf0c1 See The Implicit (NULL) Stream , above, for CDP2 version of document. Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details). The device runtime offers a single implicit, unnamed stream shared between all threads in a block, but as all named streams must be created with the cudaStreamNonBlocking flag, work launched into the NULL stream will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks). 9.6.2.1.3. Events (CDP1) \\uf0c1 See Events , above, for CDP2 version of document. Only the inter-stream synchronization capabilities of CUDA events are supported. As for all device runtime objects, event objects may be shared between all threads within the thread-block which created them but are local to that block and may not be passed to other kernels, or between blocks within the same kernel. Event handles are not guaranteed to be unique between blocks, so using an event handle within a block that did not create it will result in undefined behavior. 9.6.2.1.4.'},\n",
       " {'id': 313,\n",
       "  'content': 'Synchronization (CDP1) \\uf0c1 See Synchronization , above, for CDP2 version of document. The cudaDeviceSynchronize() function will synchronize on all work launched by any thread in the thread-block up to the point where cudaDeviceSynchronize() was called. Note that cudaDeviceSynchronize() may be called from within divergent code (see Block Wide Synchronization (CDP1) ). It is up to the program to perform sufficient additional inter-thread synchronization, for example via a call to __syncthreads() , if the calling thread is intended to synchronize with child grids invoked from other threads. 9.6.2.1.4.1. Block Wide Synchronization (CDP1) \\uf0c1 See CUDA Dynamic Parallelism , above, for CDP2 version of document. The cudaDeviceSynchronize() function does not imply intra-block synchronization. In particular, without explicit synchronization via a __syncthreads() directive the calling thread can make no assumptions about what work has been launched by any thread other than itself. For example if multiple threads within a block are each launching work and synchronization is desired for all this work at once (perhaps because of event-based dependencies), it is up to the program to guarantee that this work is submitted by all threads before calling cudaDeviceSynchronize() . Because the implementation is permitted to synchronize on launches from any thread in the block, it is quite possible that simultaneous calls to cudaDeviceSynchronize() by multiple threads will drain all work in the first call and then have no effect for the later calls. 9.6.2.1.5.'},\n",
       " {'id': 314,\n",
       "  'content': 'Device Management (CDP1) \\uf0c1 See Device Management , above, for CDP2 version of document. Only the device on which a kernel is running will be controllable from that kernel. 9.6.2.1.6. Memory Declarations (CDP1) \\uf0c1 See Memory Declarations , above, for CDP2 version of document. 9.6.2.1.6.1. Device and Constant Memory (CDP1) \\uf0c1 See Device and Constant Memory , above, for CDP2 version of document. Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime. 9.6.2.1.6.2. Textures and Surfaces (CDP1) \\uf0c1 See Textures and Surfaces , above, for CDP2 version of document. CUDA supports dynamically created texture and surface objects 14 , where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host. 9.6.2.1.6.3. Shared Memory Variable Declarations (CDP1) \\uf0c1 See Shared Memory Variable Declarations , above, for CDP2 version of document. In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernel’s caller via a launch configuration argument. __global__ void permute ( int n , int * data ) { extern __shared__ int smem []; if ( n >> ( n / 2 , data ); permute >> ( n / 2 , data + n / 2 ); } } void host_launch ( int * data ) { permute >> ( 256 , data ); } 9.6.2.1.6.4. Symbol Addresses (CDP1) \\uf0c1 See Symbol Addresses , above, for CDP2 version of document. Device-side symbols (i.e., those marked __device__ ) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernel’s visible address space. 9.6.2.1.7.'},\n",
       " {'id': 315,\n",
       "  'content': 'API Errors and Launch Failures (CDP1) \\uf0c1 See API Errors and Launch Failures , above, for CDP2 version of document. As usual for the CUDA runtime, any function may return an error code. For device-side exceptions, e.g., access to an invalid address, an error in a child grid will be returned to the host instead of being returned by the parent’s call to cudaDeviceSynchronize() . 9.6.2.1.7.1.'},\n",
       " {'id': 316,\n",
       "  'content': 'Launch Setup APIs (CDP1) \\uf0c1 See Launch Setup APIs , above, for CDP2 version of document. Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs. Table 11 New Device-only Launch Implementation Functions \\uf0c1 Runtime API Launch Functions Description of Difference From Host Runtime Behaviour (behavior is identical if no description) cudaGetParameterBuffer Generated automatically from >> . The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows: extern device cudaError_t cudaGetParameterBuffer ( void ** params ); extern __device__ cudaError_t cudaLaunchDevice ( void * kernel , void * params , dim3 gridDim , dim3 blockDim , unsigned int sharedMemSize = 0 , cudaStream_t stream = 0 ); 9.6.2.1.8. API Reference (CDP1) \\uf0c1 See API Reference , above, for CDP2 version of document. The portions of the CUDA Runtime API supported in the device runtime are detailed here. The table below provides an overview of the API relative to the version available from the host. Table 12 Supported API Functions \\uf0c1 Runtime API Functions Details cudaDeviceSynchronize Synchronizes on work launched from thread’s own block only. Warning: Note that calling this API from device code is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release. cudaDeviceGetCacheConfig cudaDeviceGetLimit cudaGetLastError Last error is per-thread state, not per-block state cudaPeekAtLastError cudaGetErrorString cudaGetDeviceCount cudaDeviceGetAttribute Will return attributes for any device cudaGetDevice Always returns current device ID as would be seen from host cudaStreamCreateWithFlags Must pass cudaStreamNonBlocking flag cudaStreamDestroy cudaStreamWaitEvent cudaEventCreateWithFlags Must pass cudaEventDisableTiming flag cudaEventRecord cudaEventDestroy cudaFuncGetAttributes cudaMemcpyAsync Notes about all memcpy/memset functions: Only async memcpy/set functions are supported Only device-to-device memcpy is permitted May not pass in local or shared memory pointers cudaMemcpy2DAsync cudaMemcpy3DAsync cudaMemsetAsync cudaMemset2DAsync cudaMemset3DAsync cudaRuntimeGetVersion cudaMalloc May not call cudaFree on the device on a pointer created on the host, and vice-versa cudaFree cudaOccupancyMaxActiveBlocksPerMultiprocessor cudaOccupancyMaxPotentialBlockSize cudaOccupancyMaxPotentialBlockSizeVariableSMem 9.6.2.2. Device-side Launch from PTX (CDP1) \\uf0c1 See Device-side Launch from PTX , above, for CDP2 version of document.'},\n",
       " {'id': 317,\n",
       "  'content': 'This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language. 9.6.2.2.1. Kernel Launch APIs (CDP1) \\uf0c1 See Kernel Launch APIs , above, for CDP2 version of document. Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer() . 9.6.2.2.1.1. cudaLaunchDevice (CDP1) \\uf0c1 See cudaLaunchDevice , above, for CDP2 version of document. At the PTX level, cudaLaunchDevice() needs to be declared in one of the two forms shown below before it is used. b64 stream ) ; // PTX-level Declaration of cudaLaunchDevice() when .address_size is 32 . b32 func , . b32 parameterBuffer , . b32 stream ) ; The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api.h . The layout of the parameter buffer is explained in Parameter Buffer Layout (CDP1) , below. 9.6.2.2.1.2. cudaGetParameterBuffer (CDP1) \\uf0c1 See cudaGetParameterBuffer , above, for CDP2 version of document. cudaGetParameterBuffer() needs to be declared at the PTX level before it’s used. The PTX-level declaration must be in one of the two forms given below, depending on address size: // PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64 // When .address_size is 64 . b64 size ) ; // PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 32 . b32 func_retval0 ) cudaGetParameterBuffer ( . b32 alignment , . b32 size ) ; The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration: // CUDA-level Declaration of cudaGetParameterBuffer() extern \"C\" __device__ void * cudaGetParameterBuffer ( size_t alignment , size_t size ); The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement in bytes. 9.6.2.2.2. Parameter Buffer Layout (CDP1) \\uf0c1 See Parameter Buffer Layout , above, for CDP2 version of document. Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned. 9.6.2.3. Toolkit Support for Dynamic Parallelism (CDP1) \\uf0c1 See Toolkit Support for Dynamic Parallelism , above, for CDP2 version of document. 9.6.2.3.1. Including Device Runtime API in CUDA Code (CDP1) \\uf0c1 See Including Device Runtime API in CUDA Code , above, for CDP2 version of document. Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation. 9.6.2.3.2. Compiling and Linking (CDP1) \\uf0c1 See Compiling and Linking , above, for CDP2 version of document. When compiling and linking CUDA programs using dynamic parallelism with nvcc , the program will automatically link against the static device runtime library libcudadevrt . 9.6.3.'},\n",
       " {'id': 318,\n",
       "  'content': 'Programming Guidelines (CDP1) \\uf0c1 See Programming Guidelines , above, for CDP2 version of document. 9.6.3.1. Basics (CDP1) \\uf0c1 See Basics , above, for CDP2 version of document. The device runtime is a functional subset of the host runtime. The following example shows a simple Hello World program incorporating dynamic parallelism: #include __global__ void childKernel () { printf ( \"Hello \" ); } __global__ void parentKernel () { // launch child childKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return ; } // wait for child to complete if ( cudaSuccess != cudaDeviceSynchronize ()) { return ; } printf ( \"World! \\\\n \" ); } int main ( int argc , char * argv []) { // launch parent parentKernel >> (); if ( cudaSuccess != cudaGetLastError ()) { return 1 ; } // wait for parent to complete if ( cudaSuccess != cudaDeviceSynchronize ()) { return 2 ; } return 0 ; } This program may be built in a single step from the command line as follows: $ nvcc -arch=sm_75 -rdc=true hello_world.cu -o hello -lcudadevrt 9.6.3.2. Performance (CDP1) \\uf0c1 See Performance , above, for CDP2 version of document. 9.6.3.2.1. Synchronization (CDP1) \\uf0c1 See CUDA Dynamic Parallelism , above, for CDP2 version of document. Warning Explicit synchronization with child kernels from a parent block (such as using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11.6, removed for compute_90+ compilation, and is slated for full removal in a future CUDA release. Synchronization by one thread may impact the performance of other threads in the same Thread Block , even when those other threads do not call cudaDeviceSynchronize() themselves. This impact will depend upon the underlying implementation. In general the implicit synchronization of child kernels done when a thread block ends is more efficient compared to calling cudaDeviceSynchronize() explicitly. It is therefore recommended to only call cudaDeviceSynchronize() if it is needed to synchronize with a child kernel before a thread block ends. 9.6.3.2.2. Dynamic-parallelism-enabled Kernel Overhead (CDP1) \\uf0c1 See Dynamic-parallelism-enabled Kernel Overhead , above, for CDP2 version of document. System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own. This overhead arises from the device runtime’s execution tracking and management software and may result in decreased performance for example, library calls when made from the device compared to from the host side. 9.6.3.3. Implementation Restrictions and Limitations (CDP1) \\uf0c1 See Implementation Restrictions and Limitations , above, for CDP2 version of document. Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime. 9.6.3.3.1. Runtime (CDP1) \\uf0c1 See Runtime , above, for CDP2 version of document. 9.6.3.3.1.1. Memory Footprint (CDP1) \\uf0c1 See Memory Footprint , above, for CDP2 version of document. The device runtime system software reserves memory for various management purposes, in particular one reservation which is used for saving parent-grid state during synchronization, and a second reservation for tracking pending grid launches. Configuration controls are available to reduce the size of these reservations in exchange for certain launch limitations. See Configuration Options (CDP1) , below, for details. The majority of reserved memory is allocated as backing-store for parent kernel state, for use when synchronizing on a child launch. Conservatively, this memory must support storing of state for the maximum number of live threads possible on the device. This means that each parent generation at which cudaDeviceSynchronize() is callable may require up to 860MB of device memory, depending on the device configuration, which will be unavailable for program use even if it is not all consumed. 9.6.3.3.1.2. Nesting and Synchronization Depth (CDP1) \\uf0c1 See CUDA Dynamic Parallelism , above, for CDP2 version of document. Using the device runtime, one kernel may launch another kernel, and that kernel may launch another, and so on. Each subordinate launch is considered a new nesting level , and the total number of levels is the nesting depth of the program. The synchronization depth is defined as the deepest level at which the program will explicitly synchronize on a child launch. Typically this is one less than the nesting depth of the program, but if the program does not need to call cudaDeviceSynchronize() at all levels then the synchronization depth might be substantially different to the nesting depth. The overall maximum nesting depth is limited to 24, but practically speaking the real limit will be the amount of memory required by the system for each new level (see Memory Footprint (CDP1) above). Any launch which would result in a kernel at a deeper level than the maximum will fail. Note that this may also apply to cudaMemcpyAsync() , which might itself generate a kernel launch. See Configuration Options (CDP1) for details. By default, sufficient storage is reserved for two levels of synchronization. This maximum synchronization depth (and hence reserved storage) may be controlled by calling cudaDeviceSetLimit() and specifying cudaLimitDevRuntimeSyncDepth . The number of levels to be supported must be configured before the top-level kernel is launched from the host, in order to guarantee successful execution of a nested program. Calling cudaDeviceSynchronize() at a depth greater than the specified maximum synchronization depth will return an error. An optimization is permitted where the system detects that it need not reserve space for the parent’s state in cases where the parent kernel never calls cudaDeviceSynchronize() . In this case, because explicit parent/child synchronization never occurs, the memory footprint required for a program will be much less than the conservative maximum. Such a program could specify a shallower maximum synchronization depth to avoid over-allocation of backing store. 9.6.3.3.1.3. Pending Kernel Launches (CDP1) \\uf0c1 See Pending Kernel Launches , above, for CDP2 version of document. When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes. The launch pool is divided into a fixed-size pool and a virtualized pool with lower performance. The device runtime system software will try to track launch data in the fixed-size pool first. The virtualized pool will be used to track new launches when the fixed-size pool is full. 9.6.3.3.1.4. Configuration Options (CDP1) \\uf0c1 See Configuration Options , above, for CDP2 version of document. Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program. The following named limits may be set: Limit Behavior cudaLimitDevRuntimeSyncDepth Sets the maximum depth at which cudaDeviceSynchronize() may be called. Launches may be performed deeper than this, but explicit synchronization deeper than this limit will return the cudaErrorLaunchMaxDepthExceeded . The default maximum sync depth is 2. cudaLimitDevRuntimePendingLaunchCount Controls the amount of memory set aside for buffering kernel launches which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources. When the buffer is full, the device runtime system software will attempt to track new pending launches in a lower performance virtualized buffer. If the virtualized buffer is also full, i.e. when all available heap space is consumed, launches will not occur, and the thread’s last error will be set to cudaErrorLaunchPendingCountExceeded . The default pending launch count is 2048 launches. 9.6.3.3.1.5. Memory Allocation and Lifetime (CDP1) \\uf0c1 See Memory Allocation and Lifetime , above, for CDP2 version of document. cudaMalloc() and cudaFree() have distinct semantics between the host and device environments. cudaMalloc() on Host cudaMalloc() on Device cudaFree() on Host Supported Not Supported cudaFree() on Device Not Supported Supported Allocation limit Free device memory cudaLimitMallocHeapSize 9.6.3.3.1.6. SM Id and Warp Id (CDP1) \\uf0c1 See SM Id and Warp Id , above, for CDP2 version of document. Note that in PTX %smid and %warpid are defined as volatile values. 9.6.3.3.1.7. ECC Errors (CDP1) \\uf0c1 See ECC Errors , above, for CDP2 version of document. No notification of ECC errors is available to code within a CUDA kernel. 14 ( 1 , 2 , 3 ) Dynamically created texture and surface objects are an addition to the CUDA memory model introduced with CUDA 5.0. Please see the CUDA Programming Guide for details.'},\n",
       " {'id': 319,\n",
       "  'content': '10. Virtual Memory Management \\uf0c1 10.1. Introduction \\uf0c1 The Virtual Memory Management APIs provide a way for the application to directly manage the unified virtual address space that CUDA provides to map physical memory to virtual addresses accessible by the GPU. Introduced in CUDA 10.2, these APIs additionally provide a new way to interop with other processes and graphics APIs like OpenGL and Vulkan, as well as provide newer memory attributes that a user can tune to fit their applications. Historically, memory allocation calls (such as cudaMalloc() ) in the CUDA programming model have returned a memory address that points to the GPU memory. The address thus obtained could be used with any CUDA API or inside a device kernel. However, the memory allocated could not be resized depending on the user’s memory needs. In order to increase an allocation’s size, the user had to explicitly allocate a larger buffer, copy data from the initial allocation, free it and then continue to keep track of the newer allocation’s address. This often leads to lower performance and higher peak memory utilization for applications. Essentially, users had a malloc-like interface for allocating GPU memory, but did not have a corresponding realloc to complement it. The Virtual Memory Management APIs decouple the idea of an address and memory and allow the application to handle them separately. The APIs allow applications to map and unmap memory from a virtual address range as they see fit. In the case of enabling peer device access to memory allocations by using cudaEnablePeerAccess , all past and future user allocations are mapped to the target peer device. This lead to users unwittingly paying runtime cost of mapping all cudaMalloc allocations to peer devices. However, in most situations applications communicate by sharing only a few allocations with another device and not all allocations are required to be mapped to all the devices. With Virtual Memory Management, applications can specifically choose certain allocations to be accessible from target devices. The CUDA Virtual Memory Management APIs expose fine grained control to the user for managing the GPU memory in applications. It provides APIs that let users: Place memory allocated on different devices into a contiguous VA range. Perform interprocess communication for memory sharing using platform-specific mechanisms. Opt into newer memory types on the devices that support them. In order to allocate memory, the Virtual Memory Management programming model exposes the following functionality: Allocating physical memory. Reserving a VA range. Mapping allocated memory to the VA range. Controlling access rights on the mapped range. Note that the suite of APIs described in this section require a system that supports UVA.'},\n",
       " {'id': 320,\n",
       "  'content': '10.2. Query for Support \\uf0c1 Before attempting to use Virtual Memory Management APIs, applications must ensure that the devices they want to use support CUDA Virtual Memory Management. The following code sample shows querying for Virtual Memory Management support: int deviceSupportsVmm ; CUresult result = cuDeviceGetAttribute ( & deviceSupportsVmm , CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED , device ); if ( deviceSupportsVmm != 0 ) { // `device` supports Virtual Memory Management } 10.3. Allocating Physical Memory \\uf0c1 The first step in memory allocation using Virtual Memory Management APIs is to create a physical memory chunk that will provide a backing for the allocation. In order to allocate physical memory, applications must use the cuMemCreate API. The allocation created by this function does not have any device or host mappings. The function argument CUmemGenericAllocationHandle describes the properties of the memory to allocate such as the location of the allocation, if the allocation is going to be shared to another process (or other Graphics APIs), or the physical attributes of the memory to be allocated. Users must ensure the requested allocation’s size must be aligned to appropriate granularity. Information regarding an allocation’s granularity requirements can be queried using cuMemGetAllocationGranularity . The following code snippet shows allocating physical memory with cuMemCreate : CUmemGenericAllocationHandle allocatePhysicalMemory ( int device , size_t size ) { CUmemAllocationProp prop = {}; prop . type = CU_MEM_ALLOCATION_TYPE_PINNED ; prop . location . type = CU_MEM_LOCATION_TYPE_DEVICE ; prop . id = device ; cuMemGetAllocationGranularity ( & granularity , & prop , CU_MEM_ALLOC_GRANULARITY_MINIMUM ); // Ensure size matches granularity requirements for the allocation size_t padded_size = ROUND_UP ( size , granularity ); // Allocate physical memory CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 ); return allocHandle ; } The memory allocated by cuMemCreate is referenced by the CUmemGenericAllocationHandle it returns. This is a departure from the cudaMalloc-style of allocation, which returns a pointer to the GPU memory, which was directly accessible by CUDA kernel executing on the device. The memory allocated cannot be used for any operations other than querying properties using cuMemGetAllocationPropertiesFromHandle . In order to make this memory accessible, applications must map this memory into a VA range reserved by cuMemAddressReserve and provide suitable access rights to it. Applications must free the allocated memory using the cuMemRelease API. 10.3.1. Shareable Memory Allocations \\uf0c1 With cuMemCreate users now have the facility to indicate to CUDA, at allocation time, that they have earmarked a particular allocation for Inter process communication and graphics interop purposes. Applications can do this by setting CUmemAllocationProp::requestedHandleTypes to a platform-specific field. On Windows, when CUmemAllocationProp::requestedHandleTypes is set to CU_MEM_HANDLE_TYPE_WIN32 applications must also specify an LPSECURITYATTRIBUTES attribute in CUmemAllocationProp::win32HandleMetaData . This security attribute defines the scope of which exported allocations may be transferred to other processes. The CUDA Virtual Memory Management API functions do not support the legacy interprocess communication functions with their memory. Instead, they expose a new mechanism for interprocess communication that uses OS-specific handles. Applications can obtain these OS-specific handles corresponding to the allocations by using cuMemExportToShareableHandle . The handles thus obtained can be transferred by using the usual OS native mechanisms for inter process communication. The recipient process should import the allocation by using cuMemImportFromShareableHandle . Users must ensure they query for support of the requested handle type before attempting to export memory allocated with cuMemCreate . The following code snippet illustrates query for handle type support in a platform-specific way. int deviceSupportsIpcHandle ; #if defined(__linux__) cuDeviceGetAttribute ( & deviceSupportsIpcHandle , CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED , device )); #else cuDeviceGetAttribute ( & deviceSupportsIpcHandle , CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED , device )); #endif Users should set the CUmemAllocationProp::requestedHandleTypes appropriately as shown below: #if defined(__linux__) prop . requestedHandleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR ; #else prop . requestedHandleTypes = CU_MEM_HANDLE_TYPE_WIN32 ; prop . win32HandleMetaData = // Windows specific LPSECURITYATTRIBUTES attribute. #endif The memMapIpcDrv sample can be used as an example for using IPC with Virtual Memory Management allocations. 10.3.2. Memory Type \\uf0c1 Before CUDA 10.2, applications had no user-controlled way of allocating any special type of memory that certain devices may support. With cuMemCreate , applications can additionally specify memory type requirements using the CUmemAllocationProp::allocFlags to opt into any specific memory features. Applications must also ensure that the requested memory type is supported on the device of allocation.'},\n",
       " {'id': 321,\n",
       "  'content': '10.3.2.1. Compressible Memory \\uf0c1 Compressible memory can be used to accelerate accesses to data with unstructured sparsity and other compressible data patterns. Compression can save DRAM bandwidth, L2 read bandwidth and L2 capacity depending on the data being operated on. Applications that want to allocate compressible memory on devices that support Compute Data Compression can do so by setting CUmemAllocationProp::allocFlags::compressionType to CU_MEM_ALLOCATION_COMP_GENERIC . Users must query if device supports Compute Data Compression by using CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED . The following code snippet illustrates querying compressible memory support cuDeviceGetAttribute . int compressionSupported = 0 ; cuDeviceGetAttribute ( & compressionSupported , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , device ); On devices that support Compute Data Compression, users must opt in at allocation time as shown below: prop . allocFlags . compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; Due to various reasons such as limited HW resources, the allocation may not have compression attributes, the user is expected to query back the properties of the allocated memory using cuMemGetAllocationPropertiesFromHandle and check for compression attribute. CUmemAllocationPropPrivate allocationProp = {}; cuMemGetAllocationPropertiesFromHandle ( & allocationProp , allocationHandle ); if ( allocationProp . compressionType == CU_MEM_ALLOCATION_COMP_GENERIC ) { // Obtained compressible memory allocation } 10.4. Reserving a Virtual Address Range \\uf0c1 Since with Virtual Memory Management the notions of address and memory are distinct, applications must carve out an address range that can hold the memory allocations made by cuMemCreate . The address range reserved must be at least as large as the sum of the sizes of all the physical memory allocations the user plans to place in them. Applications can reserve a virtual address range by passing appropriate parameters to cuMemAddressReserve . The address range obtained will not have any device or host physical memory associated with it. The reserved virtual address range can be mapped to memory chunks belonging to any device in the system, thus providing the application a continuous VA range backed and mapped by memory belonging to different devices. Applications are expected to return the virtual address range back to CUDA using cuMemAddressFree . Users must ensure that the entire VA range is unmapped before calling cuMemAddressFree . These functions are conceptually similar to mmap/munmap (on Linux) or VirtualAlloc/VirtualFree (on Windows) functions. The following code snippet illustrates the usage for the function: CUdeviceptr ptr ; // `ptr` holds the returned start of virtual address range reserved. CUresult result = cuMemAddressReserve ( & ptr , size , 0 , 0 , 0 ); // alignment = 0 for default alignment 10.5. Virtual Aliasing Support \\uf0c1 The Virtual Memory Management APIs provide a way to create multiple virtual memory mappings or “proxies” to the same allocation using multiple calls to cuMemMap with different virtual addresses, so-called virtual aliasing. Unless otherwise noted in the PTX ISA, writes to one proxy of the allocation are considered inconsistent and incoherent with any other proxy of the same memory until the writing device operation (grid launch, memcpy, memset, and so on) completes. Grids present on the GPU prior to a writing device operation but reading after the writing device operation completes are also considered to have inconsistent and incoherent proxies. For example, the following snippet is considered undefined, assuming device pointers A and B are virtual aliases of the same memory allocation: __global__ void foo ( char * A , char * B ) { * A = 0x1 ; printf ( \"%d \\\\n \" , * B ); // Undefined behavior! *B can take on either // the previous value or some value in-between. }\\nThe following is defined behavior, assuming these two kernels are ordered monotonically (by streams or events). __global__ void foo1 ( char * A ) { * A = 0x1 ; } __global__ void foo2 ( char * B ) { printf ( \"%d \\\\n \" , * B ); // *B == *A == 0x1 assuming foo2 waits for foo1 // to complete before launching } cudaMemcpyAsync ( B , input , size , stream1 ); // Aliases are allowed at // operation boundaries foo1 >> ( A ); // allowing foo1 to access A. cudaEventRecord ( event , stream1 ); cudaStreamWaitEvent ( stream2 , event ); foo2 >> ( B ); cudaStreamWaitEvent ( stream3 , event ); cudaMemcpyAsync ( output , B , size , stream3 ); // Both launches of foo2 and // cudaMemcpy (which both // read) wait for foo1 (which writes) // to complete before proceeding 10.6. Mapping Memory \\uf0c1 The allocated physical memory and the carved out virtual address space from the previous two sections represent the memory and address distinction introduced by the Virtual Memory Management APIs. For the allocated memory to be useable, the user must first place the memory in the address space. The address range obtained from cuMemAddressReserve and the physical allocation obtained from cuMemCreate or cuMemImportFromShareableHandle must be associated with each other by using cuMemMap . Users can associate allocations from multiple devices to reside in contiguous virtual address ranges as long as they have carved out enough address space. In order to decouple the physical allocation and the address range, users must unmap the address of the mapping by using cuMemUnmap . Users can map and unmap memory to the same address range as many times as they want, as long as they ensure that they don’t attempt to create mappings on VA range reservations that are already mapped. The following code snippet illustrates the usage for the function: CUdeviceptr ptr ; // `ptr`: address in the address range previously reserved by cuMemAddressReserve. // `allocHandle`: CUmemGenericAllocationHandle obtained by a previous call to cuMemCreate. CUresult result = cuMemMap ( ptr , size , 0 , allocHandle , 0 ); 10.7. Controlling Access Rights \\uf0c1 The Virtual Memory Management APIs enable applications to explicitly protect their VA ranges with access control mechanisms. Mapping the allocation to a region of the address range using cuMemMap does not make the address accessible, and would result in a program crash if accessed by a CUDA kernel. Users must specifically select access control using the cuMemSetAccess function, which allows or restricts access for specific devices to a mapped address range. The following code snippet illustrates the usage for the function: void setAccessOnDevice ( int device , CUdeviceptr ptr , size_t size ) { CUmemAccessDesc accessDesc = {}; accessDesc . type = CU_MEM_LOCATION_TYPE_DEVICE ; accessDesc . id = device ; accessDesc . flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; // Make the address accessible cuMemSetAccess ( ptr , size , & accessDesc , 1 ); } The access control mechanism exposed with Virtual Memory Management allows users to be explicit about which allocations they want to share with other peer devices on the system. As specified earlier, cudaEnablePeerAccess forces all prior and future cudaMalloc’d allocations to be mapped to the target peer device. This can be convenient in many cases as user doesn’t have to worry about tracking the mapping state of every allocation to every device in the system. But for users concerned with performance of their applications this approach has performance implications . With access control at allocation granularity Virtual Memory Management exposes a mechanism to have peer mappings with minimal overhead. The vectorAddMMAP sample can be used as an example for using the Virtual Memory Management APIs. 11.'},\n",
       " {'id': 322,\n",
       "  'content': 'Stream Ordered Memory Allocator \\uf0c1 11.1. Introduction \\uf0c1 Managing memory allocations using cudaMalloc and cudaFree causes GPU to synchronize across all executing CUDA streams. The Stream Order Memory Allocator enables applications to order memory allocation and deallocation with other work launched into a CUDA stream such as kernel launches and asynchronous copies. This improves application memory use by taking advantage of stream-ordering semantics to reuse memory allocations. The allocator also allows applications to control the allocator’s memory caching behavior. When set up with an appropriate release threshold, the caching behavior allows the allocator to avoid expensive calls into the OS when the application indicates it is willing to accept a bigger memory footprint. The allocator also supports the easy and secure sharing of allocations between processes. For many applications, the Stream Ordered Memory Allocator reduces the need for custom memory management abstractions, and makes it easier to create high-performance custom memory management for applications that need it. For applications and libraries that already have custom memory allocators, adopting the Stream Ordered Memory Allocator enables multiple libraries to share a common pool of memory managed by the driver, thus reducing excess memory consumption. Additionally, the driver can perform optimizations based on its awareness of the allocator and other stream management APIs. Finally, Nsight Compute and the Next-Gen CUDA debugger is aware of the allocator as part of their CUDA 11.3 toolkit support. 11.2. Query for Support \\uf0c1 The user can determine whether or not a device supports the stream ordered memory allocator by calling cudaDeviceGetAttribute() with the device attribute cudaDevAttrMemoryPoolsSupported . Starting with CUDA 11.3, IPC memory pool support can be queried with the cudaDevAttrMemoryPoolSupportedHandleTypes device attribute. Previous drivers will return cudaErrorInvalidValue as those drivers are unaware of the attribute enum. int driverVersion = 0 ; int deviceSupportsMemoryPools = 0 ; int poolSupportedHandleTypes = 0 ; cudaDriverGetVersion ( & driverVersion ); if ( driverVersion >= 11020 ) { cudaDeviceGetAttribute ( & deviceSupportsMemoryPools , cudaDevAttrMemoryPoolsSupported , device ); } if ( deviceSupportsMemoryPools != 0 ) { // `device` supports the Stream Ordered Memory Allocator } if ( driverVersion >= 11030 ) { cudaDeviceGetAttribute ( & poolSupportedHandleTypes , cudaDevAttrMemoryPoolSupportedHandleTypes , device ); } if ( poolSupportedHandleTypes & cudaMemHandleTypePosixFileDescriptor ) { // Pools on the specified device can be created with posix file descriptor-based IPC } Performing the driver version check before the query avoids hitting a cudaErrorInvalidValue error on drivers where the attribute was not yet defined. One can use cudaGetLastError to clear the error instead of avoiding it.'},\n",
       " {'id': 323,\n",
       "  'content': '11.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync) \\uf0c1 The APIs cudaMallocAsync and cudaFreeAsync form the core of the allocator. cudaMallocAsync returns an allocation and cudaFreeAsync frees an allocation. Both APIs accept stream arguments to define when the allocation will become and stop being available for use. The pointer value returned by cudaMallocAsync is determined synchronously and is available for constructing future work. It is important to note that cudaMallocAsync ignores the current device/context when determining where the allocation will reside. Instead, cudaMallocAsync determines the resident device based on the specified memory pool or the supplied stream. The simplest use pattern is when the memory is allocated, used, and freed back into the same stream. void * ptr ; size_t size = 512 ; cudaMallocAsync ( & ptr , size , cudaStreamPerThread ); // do work using the allocation kernel >> ( ptr , ...); // An asynchronous free can be specified without synchronizing the cpu and GPU cudaFreeAsync ( ptr , cudaStreamPerThread ); When using an allocation in a stream other than the allocating stream, the user must guarantee that the access will happen after the allocation operation, otherwise the behavior is undefined. The user may make this guarantee either by synchronizing the allocating stream, or by using CUDA events to synchronize the producing and consuming streams. cudaFreeAsync() inserts a free operation into the stream. The user must guarantee that the free operation happens after the allocation operation and any use of the allocation. Also, any use of the allocation after the free operation starts results in undefined behavior. Events and/or stream synchronizing operations should be used to guarantee any access to the allocation on other streams is complete before the freeing stream begins the free operation. cudaMallocAsync ( & ptr , size , stream1 ); cudaEventRecord ( event1 , stream1 ); //stream2 must wait for the allocation to be ready before accessing cudaStreamWaitEvent ( stream2 , event1 ); kernel >> ( ptr , ...); cudaEventRecord ( event2 , stream2 ); // stream3 must wait for stream2 to finish accessing the allocation before // freeing the allocation cudaStreamWaitEvent ( stream3 , event2 ); cudaFreeAsync ( ptr , stream3 ); The user can free allocations allocated with cudaMalloc() with cudaFreeAsync() . The user must make the same guarantees about accesses being complete before the free operation begins. cudaMalloc ( & ptr , size ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , stream ); The user can free memory allocated with cudaMallocAsync with cudaFree() . When freeing such allocations through the cudaFree() API, the driver assumes that all accesses to the allocation are complete and performs no further synchronization. The user can use cudaStreamQuery / cudaStreamSynchronize / cudaEventQuery / cudaEventSynchronize / cudaDeviceSynchronize to guarantee that the appropriate asynchronous work is complete and that the GPU will not try to access the allocation. cudaMallocAsync ( & ptr , size , stream ); kernel >> ( ptr , ...); // synchronize is needed to avoid prematurely freeing the memory cudaStreamSynchronize ( stream ); cudaFree ( ptr ); 11.4. Memory Pools and the cudaMemPool_t \\uf0c1 Memory pools encapsulate virtual address and physical memory resources that are allocated and managed according to the pools attributes and properties. The primary aspect of a memory pool is the kind and location of memory it manages. All calls to cudaMallocAsync use the resources of a memory pool. In the absence of a specified memory pool, cudaMallocAsync uses the current memory pool of the supplied stream’s device. The current memory pool for a device may be set with cudaDeviceSetMempool and queried with cudaDeviceGetMempool . By default (in the absence of a cudaDeviceSetMempool call), the current memory pool is the default memory pool of a device. The API cudaMallocFromPoolAsync and c++ overloads of cudaMallocAsync allow a user to specify the pool to be used for an allocation without setting it as the current pool. The APIs cudaDeviceGetDefaultMempool and cudaMemPoolCreate give users handles to memory pools. Note The mempool current to a device will be local to that device. So allocating without specifying a memory pool will always yield an allocation local to the stream’s device. Note cudaMemPoolSetAttribute and cudaMemPoolGetAttribute control the attributes of the memory pools. 11.5. Default/Implicit Pools \\uf0c1 The default memory pool of a device may be retrieved with the cudaDeviceGetDefaultMempool API. Allocations from the default memory pool of a device are non-migratable device allocation located on that device. These allocations will always be accessible from that device. The accessibility of the default memory pool may be modified with cudaMemPoolSetAccess and queried by cudaMemPoolGetAccess . Since the default pools do not need to be explicitly created, they are sometimes referred to as implicit pools. The default memory pool of a device does not support IPC. 11.6. Explicit Pools \\uf0c1 The API cudaMemPoolCreate creates an explicit pool. This allows applications to request properties for their allocation beyond what is provided by the default/implict pools. These include properties such as IPC capability, maximum pool size, allocations resident on a specific CPU NUMA node on supported platforms etc. // create a pool similar to the implicit pool on device 0 int device = 0 ; cudaMemPoolProps poolProps = { }; poolProps . allocType = cudaMemAllocationTypePinned ; poolProps . id = device ; poolProps . type = cudaMemLocationTypeDevice ; cudaMemPoolCreate ( & memPool , & poolProps )); The following code snippet illustrates an example of creating an IPC capable memory pool on a valid CPU NUMA node. // create a pool resident on a CPU NUMA node that is capable of IPC sharing (via a file descriptor). int cpu_numa_id = 0 ; cudaMemPoolProps poolProps = { }; poolProps . id = cpu_numa_id ; poolProps . type = cudaMemLocationTypeHostNuma ; poolProps . handleType = cudaMemHandleTypePosixFileDescriptor ; cudaMemPoolCreate ( & ipcMemPool , & poolProps )); 11.7. Physical Page Caching Behavior \\uf0c1 By default, the allocator tries to minimize the physical memory owned by a pool. To minimize the OS calls to allocate and free physical memory, applications must configure a memory footprint for each pool. Applications can do this with the release threshold attribute ( cudaMemPoolAttrReleaseThreshold ). The release threshold is the amount of memory in bytes a pool should hold onto before trying to release memory back to the OS. When more than the release threshold bytes of memory are held by the memory pool, the allocator will try to release memory back to the OS on the next call to stream, event or device synchronize. Setting the release threshold to UINT64_MAX will prevent the driver from attempting to shrink the pool after every synchronization. Cuuint64_t setVal = UINT64_MAX ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReleaseThreshold , & setVal ); Applications that set cudaMemPoolAttrReleaseThreshold high enough to effectively disable memory pool shrinking may wish to explicitly shrink a memory pool’s memory footprint. cudaMemPoolTrimTo allows such applications to do so. When trimming a memory pool’s footprint, the minBytesToKeep parameter allows an application to hold onto an amount of memory it expects to need in a subsequent phase of execution. Cuuint64_t setVal = UINT64_MAX ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReleaseThreshold , & setVal ); // application phase needing a lot of memory from the stream ordered allocator for ( i = 0 ; i >> ( ptrs ,...); for ( j = 0 ; j reserved ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrReservedMemHigh , statistics -> reservedHigh ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrUsedMemCurrent , statistics -> used ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrUsedMemHigh , statistics -> usedHigh ); } // resetting the watermarks will make them take on the current value. void resetStatistics ( cudaMemoryPool_t memPool ) { cuuint64_t value = 0 ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReservedMemHigh , & value ); cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrUsedMemHigh , & value ); } 11.9. Memory Reuse Policies \\uf0c1 In order to service an allocation request, the driver attempts to reuse memory that was previously freed via cudaFreeAsync() before attempting to allocate more memory from the OS. For example, memory freed in a stream can immediately be reused for a subsequent allocation request in the same stream. Similarly, when a stream is synchronized with the CPU, the memory that was previously freed in that stream becomes available for reuse for an allocation in any stream. The stream ordered allocator has a few controllable allocation policies. The pool attributes cudaMemPoolReuseFollowEventDependencies , cudaMemPoolReuseAllowOpportunistic , and cudaMemPoolReuseAllowInternalDependencies control these policies. Upgrading to a newer CUDA driver may change, enhance, augment and/or reorder the reuse policies. 11.9.1. cudaMemPoolReuseFollowEventDependencies \\uf0c1 Before allocating more physical GPU memory, the allocator examines dependency information established by CUDA events and tries to allocate from memory freed in another stream. cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , originalStream ); cudaEventRecord ( event , originalStream ); // waiting on the event that captures the free in another stream // allows the allocator to reuse the memory to satisfy // a new allocation request in the other stream when // cudaMemPoolReuseFollowEventDependencies is enabled. cudaStreamWaitEvent ( otherStream , event ); cudaMallocAsync ( & ptr2 , size , otherStream ); 11.9.2. cudaMemPoolReuseAllowOpportunistic \\uf0c1 According to the cudaMemPoolReuseAllowOpportunistic policy, the allocator examines freed allocations to see if the free’s stream order semantic has been met (such as the stream has passed the point of execution indicated by the free). When this is disabled, the allocator will still reuse memory made available when a stream is synchronized with the CPU. Disabling this policy does not stop the cudaMemPoolReuseFollowEventDependencies from applying. cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , originalStream ); // after some time, the kernel finishes running wait ( 10 ); // When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request // can be fulfilled with the prior allocation based on the progress of originalStream. cudaMallocAsync ( & ptr2 , size , otherStream ); 11.9.3. cudaMemPoolReuseAllowInternalDependencies \\uf0c1 Failing to allocate and map more physical memory from the OS, the driver will look for memory whose availability depends on another stream’s pending progress. If such memory is found, the driver will insert the required dependency into the allocating stream and reuse the memory. cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ...); cudaFreeAsync ( ptr , originalStream ); // When cudaMemPoolReuseAllowInternalDependencies is enabled // and the driver fails to allocate more physical memory, the driver may // effectively perform a cudaStreamWaitEvent in the allocating stream // to make sure that future work in ‘otherStream’ happens after the work // in the original stream that would be allowed to access the original allocation. cudaMallocAsync ( & ptr2 , size , otherStream ); 11.9.4. Disabling Reuse Policies \\uf0c1 While the controllable reuse policies improve memory reuse, users may want to disable them. Allowing opportunistic reuse (such as cudaMemPoolReuseAllowOpportunistic ) introduces run to run variance in allocation patterns based on the interleaving of CPU and GPU execution. Internal dependency insertion (such as cudaMemPoolReuseAllowInternalDependencies ) can serialize work in unexpected and potentially non-deterministic ways when the user would rather explicitly synchronize an event or stream on allocation failure. 11.10. Device Accessibility for Multi-GPU Support \\uf0c1 Just like allocation accessibility controlled through the virtual memory management APIs, memory pool allocation accessibility does not follow cudaDeviceEnablePeerAccess or cuCtxEnablePeerAccess . Instead, the API cudaMemPoolSetAccess modifies what devices can access allocations from a pool. By default, allocations are accessible from the device where the allocations are located. This access cannot be revoked. To enable access from other devices, the accessing device must be peer capable with the memory pool’s device; check with cudaDeviceCanAccessPeer . If the peer capability is not checked, the set access may fail with cudaErrorInvalidDevice . If no allocations had been made from the pool, the cudaMemPoolSetAccess call may succeed even when the devices are not peer capable; in this case, the next allocation from the pool will fail. It is worth noting that cudaMemPoolSetAccess affects all allocations from the memory pool, not just future ones. Also the accessibility reported by cudaMemPoolGetAccess applies to all allocations from the pool, not just future ones. It is recommended that the accessibility settings of a pool for a given GPU not be changed frequently; once a pool is made accessible from a given GPU, it should remain accessible from that GPU for the lifetime of the pool. // snippet showing usage of cudaMemPoolSetAccess: cudaError_t setAccessOnDevice ( cudaMemPool_t memPool , int residentDevice , int accessingDevice ) { cudaMemAccessDesc accessDesc = {}; accessDesc . type = cudaMemLocationTypeDevice ; accessDesc . id = accessingDevice ; accessDesc . flags = cudaMemAccessFlagsProtReadWrite ; int canAccess = 0 ; cudaError_t error = cudaDeviceCanAccessPeer ( & canAccess , accessingDevice , residentDevice ); if ( error != cudaSuccess ) { return error ; } else if ( canAccess == 0 ) { return cudaErrorPeerAccessUnsupported ; } // Make the address accessible return cudaMemPoolSetAccess ( memPool , & accessDesc , 1 ); } 11.11. IPC Memory Pools \\uf0c1 IPC capable memory pools allow easy, efficient and secure sharing of GPU memory between processes. CUDA’s IPC memory pools provide the same security benefits as CUDA’s virtual memory management APIs. There are two phases to sharing memory between processes with memory pools. The processes first need to share access to the pool, then share specific allocations from that pool. The first phase establishes and enforces security. The second phase coordinates what virtual addresses are used in each process and when mappings need to be valid in the importing process. 11.11.1. Creating and Sharing IPC Memory Pools \\uf0c1 Sharing access to a pool involves retrieving an OS native handle to the pool (with the cudaMemPoolExportToShareableHandle() API), transferring the handle to the importing process using the usual OS native IPC mechanisms, and creating an imported memory pool (with the cudaMemPoolImportFromShareableHandle() API). For cudaMemPoolExportToShareableHandle to succeed, the memory pool had to be created with the requested handle type specified in the pool properties structure. Please reference samples for the appropriate IPC mechanisms to transfer the OS native handle between processes. The rest of the procedure can be found in the following code snippets. // in exporting process // create an exportable IPC capable pool on device 0 cudaMemPoolProps poolProps = { }; poolProps . id = 0 ; poolProps . type = cudaMemLocationTypeDevice ; // Setting handleTypes to a non zero value will make the pool exportable (IPC capable) poolProps . handleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR ; cudaMemPoolCreate ( & memPool , & poolProps )); // FD based handles are integer types int fdHandle = 0 ; // Retrieve an OS native handle to the pool. // Note that a pointer to the handle memory is passed in here. cudaMemPoolExportToShareableHandle ( & fdHandle , memPool , CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR , 0 ); // The handle must be sent to the importing process with the appropriate // OS specific APIs. // in importing process int fdHandle ; // The handle needs to be retrieved from the exporting process with the // appropriate OS specific APIs. // Create an imported pool from the shareable handle. // Note that the handle is passed by value here. cudaMemPoolImportFromShareableHandle ( & importedMemPool , ( void * ) fdHandle , CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR , 0 ); 11.11.2. Set Access in the Importing Process \\uf0c1 Imported memory pools are initially only accessible from their resident device. The imported memory pool does not inherit any accessibility set by the exporting process. The importing process needs to enable access (with cudaMemPoolSetAccess ) from any GPU it plans to access the memory from. If the imported memory pool belongs to a non-visible device in the importing process, the user must use the cudaMemPoolSetAccess API to enable access from the GPUs the allocations will be used on. 11.11.3. Creating and Sharing Allocations from an Exported Pool \\uf0c1 Once the pool has been shared, allocations made with cudaMallocAsync() from the pool in the exporting process can be shared with other processes that have imported the pool. Since the pool’s security policy is established and verified at the pool level, the OS does not need extra bookkeeping to provide security for specific pool allocations; In other words, the opaque cudaMemPoolPtrExportData required to import a pool allocation may be sent to the importing process using any mechanism. While allocations may be exported and even imported without synchronizing with the allocating stream in any way, the importing process must follow the same rules as the exporting process when accessing the allocation. Namely, access to the allocation must happen after the stream ordering of the allocation operation in the allocating stream. The two following code snippets show cudaMemPoolExportPointer() and cudaMemPoolImportPointer() sharing the allocation with an IPC event used to guarantee that the allocation isn’t accessed in the importing process before the allocation is ready. // preparing an allocation in the exporting process cudaMemPoolPtrExportData exportData ; cudaEvent_t readyIpcEvent ; cudaIpcEventHandle_t readyIpcEventHandle ; // ipc event for coordinating between processes // cudaEventInterprocess flag makes the event an ipc event // cudaEventDisableTiming is set for performance reasons cudaEventCreate ( & readyIpcEvent , cudaEventDisableTiming | cudaEventInterprocess ) // allocate from the exporting mem pool cudaMallocAsync ( & ptr , size , exportMemPool , stream ); // event for sharing when the allocation is ready. cudaEventRecord ( readyIpcEvent , stream ); cudaMemPoolExportPointer ( & exportData , ptr ); cudaIpcGetEventHandle ( & readyIpcEventHandle , readyIpcEvent ); // Share IPC event and pointer export data with the importing process using // any mechanism. Here we copy the data into shared memory shmem -> ptrData = exportData ; shmem -> readyIpcEventHandle = readyIpcEventHandle ; // signal consumers data is ready // Importing an allocation cudaMemPoolPtrExportData * importData = & shmem -> prtData ; cudaEvent_t readyIpcEvent ; cudaIpcEventHandle_t * readyIpcEventHandle = & shmem -> readyIpcEventHandle ; // Need to retrieve the ipc event handle and the export data from the // exporting process using any mechanism. Here we are using shmem and just // need synchronization to make sure the shared memory is filled in. cudaIpcOpenEventHandle ( & readyIpcEvent , readyIpcEventHandle ); // import the allocation. The operation does not block on the allocation being ready. cudaMemPoolImportPointer ( & ptr , importedMemPool , importData ); // Wait for the prior stream operations in the allocating stream to complete before // using the allocation in the importing process. cudaStreamWaitEvent ( stream , readyIpcEvent ); kernel >> ( ptr , ...); When freeing the allocation, the allocation needs to be freed in the importing process before it is freed in the exporting process. The following code snippet demonstrates the use of CUDA IPC events to provide the required synchronization between the cudaFreeAsync operations in both processes. Access to the allocation from the importing process is obviously restricted by the free operation in the importing process side. It is worth noting that cudaFree can be used to free the allocation in both processes and that other stream synchronization APIs may be used instead of CUDA IPC events. // The free must happen in importing process before the exporting process kernel >> ( ptr , ...); // Last access in importing process cudaFreeAsync ( ptr , stream ); // Access not allowed in the importing process after the free cudaIpcEventRecord ( finishedIpcEvent , stream ); // Exporting process // The exporting process needs to coordinate its free with the stream order // of the importing process’s free. cudaStreamWaitEvent ( stream , finishedIpcEvent ); kernel >> ( ptrInExportingProcess , ...); // The free in the importing process doesn’t stop the exporting process // from using the allocation. cudFreeAsync ( ptrInExportingProcess , stream ); 11.11.4. IPC Export Pool Limitations \\uf0c1 IPC pools currently do not support releasing physical blocks back to the OS. As a result the cudaMemPoolTrimTo API acts as a no-op and the cudaMemPoolAttrReleaseThreshold effectively gets ignored. This behavior is controlled by the driver, not the runtime and may change in a future driver update. 11.11.5. IPC Import Pool Limitations \\uf0c1 Allocating from an import pool is not allowed; specifically, import pools cannot be set current and cannot be used in the cudaMallocFromPoolAsync API. As such, the allocation reuse policy attributes are meaningless for these pools. IPC pools currently do not support releasing physical blocks back to the OS. The resource usage stat attribute queries only reflect the allocations imported into the process and the associated physical memory.'},\n",
       " {'id': 324,\n",
       "  'content': '11.12. Synchronization API Actions \\uf0c1 One of the optimizations that comes with the allocator being part of the CUDA driver is integration with the synchronize APIs. When the user requests that the CUDA driver synchronize, the driver waits for asynchronous work to complete. Before returning, the driver will determine what frees the synchronization guaranteed to be completed. These allocations are made available for allocation regardless of specified stream or disabled allocation policies. The driver also checks cudaMemPoolAttrReleaseThreshold here and releases any excess physical memory that it can. 11.13. Addendums \\uf0c1 11.13.1. cudaMemcpyAsync Current Context/Device Sensitivity \\uf0c1 In the current CUDA driver, any async memcpy involving memory from cudaMallocAsync should be done using the specified stream’s context as the calling thread’s current context. This is not necessary for cudaMemcpyPeerAsync , as the device primary contexts specified in the API are referenced instead of the current context. 11.13.2. cuPointerGetAttribute Query \\uf0c1 Invoking cuPointerGetAttribute on an allocation after invoking cudaFreeAsync on it results in undefined behavior. Specifically, it does not matter if an allocation is still accessible from a given stream: the behavior is still undefined. 11.13.3. cuGraphAddMemsetNode \\uf0c1 cuGraphAddMemsetNode does not work with memory allocated via the stream ordered allocator. However, memsets of the allocations can be stream captured. 11.13.4. Pointer Attributes \\uf0c1 The cuPointerGetAttributes query works on stream ordered allocations. Since stream ordered allocations are not context associated, querying CU_POINTER_ATTRIBUTE_CONTEXT will succeed but return NULL in *data . The attribute CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL can be used to determine the location of the allocation: this can be useful when selecting a context for making p2h2p copies using cudaMemcpyPeerAsync . The attribute CU_POINTER_ATTRIBUTE_MEMPOOL_HANDLE was added in CUDA 11.3 and can be useful for debugging and for confirming which pool an allocation comes from before doing IPC. 12.'},\n",
       " {'id': 325,\n",
       "  'content': 'Graph Memory Nodes \\uf0c1 12.1. Introduction \\uf0c1 Graph memory nodes allow graphs to create and own memory allocations. Graph memory nodes have GPU ordered lifetime semantics, which dictate when memory is allowed to be accessed on the device. These GPU ordered lifetime semantics enable driver-managed memory reuse, and match those of the stream ordered allocation APIs cudaMallocAsync and cudaFreeAsync , which may be captured when creating a graph. Graph allocations have fixed addresses over the life of a graph including repeated instantiations and launches. This allows the memory to be directly referenced by other operations within the graph without the need of a graph update, even when CUDA changes the backing physical memory. Within a graph, allocations whose graph ordered lifetimes do not overlap may use the same underlying physical memory. CUDA may reuse the same physical memory for allocations across multiple graphs, aliasing virtual address mappings according to the GPU ordered lifetime semantics. For example when different graphs are launched into the same stream, CUDA may virtually alias the same physical memory to satisfy the needs of allocations which have single-graph lifetimes. 12.2. Support and Compatibility \\uf0c1 Graph memory nodes require an 11.4 capable CUDA driver and support for the stream ordered allocator on the GPU. The following snippet shows how to check for support on a given device. int driverVersion = 0 ; int deviceSupportsMemoryPools = 0 ; int deviceSupportsMemoryNodes = 0 ; cudaDriverGetVersion ( & driverVersion ); if ( driverVersion >= 11020 ) { // avoid invalid value error in cudaDeviceGetAttribute cudaDeviceGetAttribute ( & deviceSupportsMemoryPools , cudaDevAttrMemoryPoolsSupported , device ); } deviceSupportsMemoryNodes = ( driverVersion >= 11040 ) && ( deviceSupportsMemoryPools != 0 ); Doing the attribute query inside the driver version check avoids an invalid value return code on 11.0 and 11.1 drivers. Be aware that the compute sanitizer emits warnings when it detects CUDA returning error codes, and a version check before reading the attribute will avoid this. Graph memory nodes are only supported on driver versions 11.4 and newer. 12.3. API Fundamentals \\uf0c1 Graph memory nodes are graph nodes representing either memory allocation or free actions. As a shorthand, nodes that allocate memory are called allocation nodes. Likewise, nodes that free memory are called free nodes. Allocations created by allocation nodes are called graph allocations. CUDA assigns virtual addresses for the graph allocation at node creation time. While these virtual addresses are fixed for the lifetime of the allocation node, the allocation contents are not persistent past the freeing operation and may be overwritten by accesses referring to a different allocation. Graph allocations are considered recreated every time a graph runs. A graph allocation’s lifetime, which differs from the node’s lifetime, begins when GPU execution reaches the allocating graph node and ends when one of the following occurs: GPU execution reaches the freeing graph node GPU execution reaches the freeing cudaFreeAsync() stream call immediately upon the freeing call to cudaFree() Note Graph destruction does not automatically free any live graph-allocated memory, even though it ends the lifetime of the allocation node. The allocation must subsequently be freed in another graph, or using cudaFreeAsync() /cudaFree() . Just like other graph nodes , graph memory nodes are ordered within a graph by dependency edges. A program must guarantee that operations accessing graph memory: are ordered after the allocation node are ordered before the operation freeing the memory Graph allocation lifetimes begin and usually end according to GPU execution (as opposed to API invocation). GPU ordering is the order that work runs on the GPU as opposed to the order that the work is enqueued or described. Thus, graph allocations are considered ‘GPU ordered.’ 12.3.1. Graph Node APIs \\uf0c1 Graph memory nodes may be explicitly created with the memory node creation APIs, cudaGraphAddMemAllocNode and cudaGraphAddMemFreeNode . The address allocated by cudaGraphAddMemAllocNode is returned to the user in the dptr field of the passed CUDA_MEM_ALLOC_NODE_PARAMS structure. All operations using graph allocations inside the allocating graph must be ordered after the allocating node. Similarly, any free nodes must be ordered after all uses of the allocation within the graph. cudaGraphAddMemFreeNode creates free nodes. In the following figure, there is an example graph with an alloc and a free node. Kernel nodes a , b , and c are ordered after the allocation node and before the free node such that the kernels can access the allocation. Kernel node e is not ordered after the alloc node and therefore cannot safely access the memory. Kernel node d is not ordered before the free node, therefore it cannot safely access the memory. Figure 28 Kernel Nodes \\uf0c1 The following code snippet establishes the graph in this figure: // Create the graph - it starts out empty cudaGraphCreate(&graph, 0); // parameters for a basic allocation cudaMemAllocNodeParams params = {}; params.poolProps.allocType = cudaMemAllocationTypePinned; params.poolProps.location.type = cudaMemLocationTypeDevice; // specify device 0 as the resident device params.poolProps.location.id = 0; params.bytesize = size; cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, ¶ms); nodeParams->kernelParams[0] = params.dptr; cudaGraphAddKernelNode(&a, graph, &allocNode, 1, &nodeParams); cudaGraphAddKernelNode(&b, graph, &a, 1, &nodeParams); cudaGraphAddKernelNode(&c, graph, &a, 1, &nodeParams); cudaGraphNode_t dependencies[2]; // kernel nodes b and c are using the graph allocation, so the freeing node must depend on them. Since the dependency of node b on node a establishes an indirect dependency, the free node does not need to explicitly depend on node a. dependencies[0] = b; dependencies[1] = c; cudaGraphAddMemFreeNode(&freeNode, graph, dependencies, 2, params.dptr); // free node does not depend on kernel node d, so it must not access the freed graph allocation. cudaGraphAddKernelNode(&d, graph, &c, 1, &nodeParams); // node e does not depend on the allocation node, so it must not access the allocation. This would be true even if the freeNode depended on kernel node e. cudaGraphAddKernelNode(&e, graph, NULL, 0, &nodeParams); 12.3.2. Stream Capture \\uf0c1 Graph memory nodes can be created by capturing the corresponding stream ordered allocation and free calls cudaMallocAsync and cudaFreeAsync . In this case, the virtual addresses returned by the captured allocation API can be used by other operations inside the graph. Since the stream ordered dependencies will be captured into the graph, the ordering requirements of the stream ordered allocation APIs guarantee that the graph memory nodes will be properly ordered with respect to the captured stream operations (for correctly written stream code). Ignoring kernel nodes d and e , for clarity, the following code snippet shows how to use stream capture to create the graph from the previous figure: cudaMallocAsync(&dptr, size, stream1); kernel_A>>(dptr, ...); // Fork into stream2 cudaEventRecord(event1, stream1); cudaStreamWaitEvent(stream2, event1); kernel_B>>(dptr, ...); // event dependencies translated into graph dependencies, so the kernel node created by the capture of kernel C will depend on the allocation node created by capturing the cudaMallocAsync call. kernel_C>>(dptr, ...); // Join stream2 back to origin stream (stream1) cudaEventRecord(event2, stream2); cudaStreamWaitEvent(stream1, event2); // Free depends on all work accessing the memory. cudaFreeAsync(dptr, stream1); // End capture in the origin stream cudaStreamEndCapture(stream1, &graph); 12.3.3. Accessing and Freeing Graph Memory Outside of the Allocating Graph \\uf0c1 Graph allocations do not have to be freed by the allocating graph. When a graph does not free an allocation, that allocation persists beyond the execution of the graph and can be accessed by subsequent CUDA operations. These allocations may be accessed in another graph or directly using a stream operation as long as the accessing operation is ordered after the allocation through CUDA events and other stream ordering mechanisms. An allocation may subsequently be freed by regular calls to cudaFree , cudaFreeAsync , or by the launch of another graph with a corresponding free node, or a subsequent launch of the allocating graph (if it was instantiated with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag). It is illegal to access memory after it has been freed - the free operation must be ordered after all operations accessing the memory using graph dependencies, CUDA events, and other stream ordering mechanisms. Note Because graph allocations may share underlying physical memory with each other, the Virtual Aliasing Support rules relating to consistency and coherency must be considered. Simply put, the free operation must be ordered after the full device operation (for example, compute kernel / memcpy) completes. Specifically, out of band synchronization - for example a handshake through memory as part of a compute kernel that accesses the graph-allocated memory - is not sufficient for providing ordering guarantees between the memory writes to graph memory and the free operation of that graph memory. The following code snippets demonstrate accessing graph allocations outside of the allocating graph with ordering properly established by: using a single stream, using events between streams, and using events baked into the allocating and freeing graph. Ordering established by using a single stream: void *dptr; cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, ¶ms); dptr = params.dptr; cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0); cudaGraphLaunch(allocGraphExec, stream); kernel>>(dptr, …); cudaFreeAsync(dptr, stream); Ordering established by recording and waiting on CUDA events: void *dptr; // Contents of allocating graph cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, ¶ms); dptr = params.dptr; // contents of consuming/freeing graph nodeParams->kernelParams[0] = params.dptr; cudaGraphAddKernelNode(&a, graph, NULL, 0, &nodeParams); cudaGraphAddMemFreeNode(&freeNode, freeGraph, &a, 1, dptr); cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0); cudaGraphInstantiate(&freeGraphExec, freeGraph, NULL, NULL, 0); cudaGraphLaunch(allocGraphExec, allocStream); // establish the dependency of stream2 on the allocation node // note: the dependency could also have been established with a stream synchronize operation cudaEventRecord(allocEvent, allocStream) cudaStreamWaitEvent(stream2, allocEvent); kernel>> (dptr, …); // establish the dependency between the stream 3 and the allocation use cudaStreamRecordEvent(streamUseDoneEvent, stream2); cudaStreamWaitEvent(stream3, streamUseDoneEvent); // it is now safe to launch the freeing graph, which may also access the memory cudaGraphLaunch(freeGraphExec, stream3); Ordering established by using graph external event nodes: void *dptr; cudaEvent_t allocEvent; // event indicating when the allocation will be ready for use. cudaEvent_t streamUseDoneEvent; // event indicating when the stream operations are done with the allocation. // Contents of allocating graph with event record node cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, ¶ms); dptr = params.dptr; // note: this event record node depends on the alloc node cudaGraphAddEventRecordNode(&recordNode, allocGraph, &allocNode, 1, allocEvent); cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0); // contents of consuming/freeing graph with event wait nodes cudaGraphAddEventWaitNode(&streamUseDoneEventNode, waitAndFreeGraph, NULL, 0, streamUseDoneEvent); cudaGraphAddEventWaitNode(&allocReadyEventNode, waitAndFreeGraph, NULL, 0, allocEvent); nodeParams->kernelParams[0] = params.dptr; // The allocReadyEventNode provides ordering with the alloc node for use in a consuming graph. cudaGraphAddKernelNode(&kernelNode, waitAndFreeGraph, &allocReadyEventNode, 1, &nodeParams); // The free node has to be ordered after both external and internal users. // Thus the node must depend on both the kernelNode and the // streamUseDoneEventNode. dependencies[0] = kernelNode; dependencies[1] = streamUseDoneEventNode; cudaGraphAddMemFreeNode(&freeNode, waitAndFreeGraph, &dependencies, 2, dptr); cudaGraphInstantiate(&waitAndFreeGraphExec, waitAndFreeGraph, NULL, NULL, 0); cudaGraphLaunch(allocGraphExec, allocStream); // establish the dependency of stream2 on the event node satisfies the ordering requirement cudaStreamWaitEvent(stream2, allocEvent); kernel>> (dptr, …); cudaStreamRecordEvent(streamUseDoneEvent, stream2); // the event wait node in the waitAndFreeGraphExec establishes the dependency on the “readyForFreeEvent” that is needed to prevent the kernel running in stream two from accessing the allocation after the free node in execution order. cudaGraphLaunch(waitAndFreeGraphExec, stream3); 12.3.4. cudaGraphInstantiateFlagAutoFreeOnLaunch \\uf0c1 Under normal circumstances, CUDA will prevent a graph from being relaunched if it has unfreed memory allocations because multiple allocations at the same address will leak memory. Instantiating a graph with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag allows the graph to be relaunched while it still has unfreed allocations. In this case, the launch automatically inserts an asynchronous free of the unfreed allocations. Auto free on launch is useful for single-producer multiple-consumer algorithms. At each iteration, a producer graph creates several allocations, and, depending on runtime conditions, a varying set of consumers accesses those allocations. This type of variable execution sequence means that consumers cannot free the allocations because a subsequent consumer may require access. Auto free on launch means that the launch loop does not need to track the producer’s allocations - instead, that information remains isolated to the producer’s creation and destruction logic. In general, auto free on launch simplifies an algorithm which would otherwise need to free all the allocations owned by a graph before each relaunch. Note The cudaGraphInstantiateFlagAutoFreeOnLaunch flag does not change the behavior of graph destruction. The application must explicitly free the unfreed memory in order to avoid memory leaks, even for graphs instantiated with the flag. The following code shows the use of cudaGraphInstantiateFlagAutoFreeOnLaunch to simplify a single-producer / multiple-consumer algorithm: // Create producer graph which allocates memory and populates it with data cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); cudaMallocAsync ( & data1 , blocks * threads , cudaStreamPerThread ); cudaMallocAsync ( & data2 , blocks * threads , cudaStreamPerThread ); produce >> ( data1 , data2 ); ... cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & producer , graph , cudaGraphInstantiateFlagAutoFreeOnLaunch ); cudaGraphDestroy ( graph ); // Create first consumer graph by capturing an asynchronous library call cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); consumerFromLibrary ( data1 , cudaStreamPerThread ); cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & consumer1 , graph , 0 ); //regular instantiation cudaGraphDestroy ( graph ); // Create second consumer graph cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); consume2 >> ( data2 ); ... cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & consumer2 , graph , 0 ); cudaGraphDestroy ( graph ); // Launch in a loop bool launchConsumer2 = false ; do { cudaGraphLaunch ( producer , myStream ); cudaGraphLaunch ( consumer1 , myStream ); if ( launchConsumer2 ) { cudaGraphLaunch ( consumer2 , myStream ); } } while ( determineAction ( & launchConsumer2 )); cudaFreeAsync ( data1 , myStream ); cudaFreeAsync ( data2 , myStream ); cudaGraphExecDestroy ( producer ); cudaGraphExecDestroy ( consumer1 ); cudaGraphExecDestroy ( consumer2 ); 12.4. Optimized Memory Reuse \\uf0c1 CUDA reuses memory in two ways: Virtual and physical memory reuse within a graph is based on virtual address assignment, like in the stream ordered allocator. Physical memory reuse between graphs is done with virtual aliasing: different graphs can map the same physical memory to their unique virtual addresses. 12.4.1. Address Reuse within a Graph \\uf0c1 CUDA may reuse memory within a graph by assigning the same virtual address ranges to different allocations whose lifetimes do not overlap. Since virtual addresses may be reused, pointers to different allocations with disjoint lifetimes are not guaranteed to be unique. The following figure shows adding a new allocation node (2) that can reuse the address freed by a dependent node (1). Figure 29 Adding New Alloc Node 2 \\uf0c1 The following figure shows adding a new alloc node (4). The new alloc node is not dependent on the free node (2) so cannot reuse the address from the associated alloc node (2). If the alloc node (2) used the address freed by free node (1), the new alloc node 3 would need a new address. Figure 30 Adding New Alloc Node 3 \\uf0c1 12.4.2. Physical Memory Management and Sharing \\uf0c1 CUDA is responsible for mapping physical memory to the virtual address before the allocating node is reached in GPU order. As an optimization for memory footprint and mapping overhead, multiple graphs may use the same physical memory for distinct allocations if they will not run simultaneously; however, physical pages cannot be reused if they are bound to more than one executing graph at the same time, or to a graph allocation which remains unfreed. CUDA may update physical memory mappings at any time during graph instantiation, launch, or execution. CUDA may also introduce synchronization between future graph launches in order to prevent live graph allocations from referring to the same physical memory. As for any allocate-free-allocate pattern, if a program accesses a pointer outside of an allocation’s lifetime, the erroneous access may silently read or write live data owned by another allocation (even if the virtual address of the allocation is unique). Use of compute sanitizer tools can catch this error. The following figure shows graphs sequentially launched in the same stream. In this example, each graph frees all the memory it allocates. Since the graphs in the same stream never run concurrently, CUDA can and should use the same physical memory to satisfy all the allocations. Figure 31 Sequentially Launched Graphs \\uf0c1 12.5. Performance Considerations \\uf0c1 When multiple graphs are launched into the same stream, CUDA attempts to allocate the same physical memory to them because the execution of these graphs cannot overlap. Physical mappings for a graph are retained between launches as an optimization to avoid the cost of remapping. If, at a later time, one of the graphs is launched such that its execution may overlap with the others (for example if it is launched into a different stream) then CUDA must perform some remapping because concurrent graphs require distinct memory to avoid data corruption. In general, remapping of graph memory in CUDA is likely caused by these operations: Changing the stream into which a graph is launched A trim operation on the graph memory pool, which explicitly frees unused memory (discussed in Physical Memory Footprint ) Relaunching a graph while an unfreed allocation from another graph is mapped to the same memory will cause a remap of memory before relaunch Remapping must happen in execution order, but after any previous execution of that graph is complete (otherwise memory that is still in use could be unmapped). Due to this ordering dependency, as well as because mapping operations are OS calls, mapping operations can be relatively expensive. Applications can avoid this cost by launching graphs containing allocation memory nodes consistently into the same stream. 12.5.1. First Launch / cudaGraphUpload \\uf0c1 Physical memory cannot be allocated or mapped during graph instantiation because the stream in which the graph will execute is unknown. Mapping is done instead during graph launch. Calling cudaGraphUpload can separate out the cost of allocation from the launch by performing all mappings for that graph immediately and associating the graph with the upload stream. If the graph is then launched into the same stream, it will launch without any additional remapping. Using different streams for graph upload and graph launch behaves similarly to switching streams, likely resulting in remap operations. In addition, unrelated memory pool management is permitted to pull memory from an idle stream, which could negate the impact of the uploads. 12.6. Physical Memory Footprint \\uf0c1 The pool-management behavior of asynchronous allocation means that destroying a graph which contains memory nodes (even if their allocations are free) will not immediately return physical memory to the OS for use by other processes. To explicitly release memory back to the OS, an application should use the cudaDeviceGraphMemTrim API. cudaDeviceGraphMemTrim will unmap and release any physical memory reserved by graph memory nodes that is not actively in use. Allocations that have not been freed and graphs that are scheduled or running are considered to be actively using the physical memory and will not be impacted. Use of the trim API will make physical memory available to other allocation APIs and other applications or processes, but will cause CUDA to reallocate and remap memory when the trimmed graphs are next launched. Note that cudaDeviceGraphMemTrim operates on a different pool from cudaMemPoolTrimTo() . The graph memory pool is not exposed to the steam ordered memory allocator. CUDA allows applications to query their graph memory footprint through the cudaDeviceGetGraphMemAttribute API. Querying the attribute cudaGraphMemAttrReservedMemCurrent returns the amount of physical memory reserved by the driver for graph allocations in the current process. Querying cudaGraphMemAttrUsedMemCurrent returns the amount of physical memory currently mapped by at least one graph. Either of these attributes can be used to track when new physical memory is acquired by CUDA for the sake of an allocating graph. Both of these attributes are useful for examining how much memory is saved by the sharing mechanism. 12.7. Peer Access \\uf0c1 Graph allocations can be configured for access from multiple GPUs, in which case CUDA will map the allocations onto the peer GPUs as required. CUDA allows graph allocations requiring different mappings to reuse the same virtual address. When this occurs, the address range is mapped onto all GPUs required by the different allocations. This means an allocation may sometimes allow more peer access than was requested during its creation; however, relying on these extra mappings is still an error. 12.7.1. Peer Access with Graph Node APIs \\uf0c1 The cudaGraphAddMemAllocNode API accepts mapping requests in the accessDescs array field of the node parameters structures. The poolProps.location embedded structure specifies the resident device for the allocation. Access from the allocating GPU is assumed to be needed, thus the application does not need to specify an entry for the resident device in the accessDescs array. cudaMemAllocNodeParams params = {}; params.poolProps.allocType = cudaMemAllocationTypePinned; params.poolProps.location.type = cudaMemLocationTypeDevice; // specify device 1 as the resident device params.poolProps.location.id = 1; params.bytesize = size; // allocate an allocation resident on device 1 accessible from device 1 cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, ¶ms); accessDescs[2]; // boilerplate for the access descs (only ReadWrite and Device access supported by the add node api) accessDescs[0].flags = cudaMemAccessFlagsProtReadWrite; accessDescs[0].location.type = cudaMemLocationTypeDevice; accessDescs[1].flags = cudaMemAccessFlagsProtReadWrite; accessDescs[1].location.type = cudaMemLocationTypeDevice; // access being requested for device 0 & 2. Device 1 access requirement left implicit. accessDescs[0].location.id = 0; accessDescs[1].location.id = 2; // access request array has 2 entries. params.accessDescCount = 2; params.accessDescs = accessDescs; // allocate an allocation resident on device 1 accessible from devices 0, 1 and 2. (0 & 2 from the descriptors, 1 from it being the resident device). cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, ¶ms); 12.7.2. Peer Access with Stream Capture \\uf0c1 For stream capture, the allocation node records the peer accessibility of the allocating pool at the time of the capture. Altering the peer accessibility of the allocating pool after a cudaMallocFromPoolAsync call is captured does not affect the mappings that the graph will make for the allocation. // boilerplate for the access descs (only ReadWrite and Device access supported by the add node api) accessDesc.flags = cudaMemAccessFlagsProtReadWrite; accessDesc.location.type = cudaMemLocationTypeDevice; accessDesc.location.id = 1; // let memPool be resident and accessible on device 0 cudaStreamBeginCapture(stream); cudaMallocAsync(&dptr1, size, memPool, stream); cudaStreamEndCapture(stream, &graph1); cudaMemPoolSetAccess(memPool, &accessDesc, 1); cudaStreamBeginCapture(stream); cudaMallocAsync(&dptr2, size, memPool, stream); cudaStreamEndCapture(stream, &graph2); //The graph node allocating dptr1 would only have the device 0 accessibility even though memPool now has device 1 accessibility. //The graph node allocating dptr2 will have device 0 and device 1 accessibility, since that was the pool accessibility at the time of the cudaMallocAsync call. 13. Mathematical Functions \\uf0c1 The reference manual lists, along with their description, all the functions of the C/C++ standard library mathematical functions that are supported in device code, as well as all intrinsic functions (that are only supported in device code). This section provides accuracy information for some of these functions when applicable. It uses ULP for quantification. For further information on the definition of the Unit in the Last Place (ULP), please see Jean-Michel Muller’s paper On the definition of ulp(x) , RR-5504, LIP RR-2005-09, INRIA, LIP. 2005, pp.16 at https://hal.inria.fr/inria-00070503/document . Mathematical functions supported in device code do not set the global errno variable, nor report any floating-point exceptions to indicate errors; thus, if error diagnostic mechanisms are required, the user should implement additional screening for inputs and outputs of the functions. The user is responsible for the validity of pointer arguments. The user must not pass uninitialized parameters to the Mathematical functions as this may result in undefined behavior: functions are inlined in the user program and thus are subject to compiler optimizations. 13.1. Standard Functions \\uf0c1 The functions from this section can be used in both host and device code. This section specifies the error bounds of each function when executed on the device and also when executed on the host in the case where the host does not supply the function. The error bounds are generated from extensive but not exhaustive tests, so they are not guaranteed bounds. Single-Precision Floating-Point Functions Addition and multiplication are IEEE-compliant, so have a maximum error of 0.5 ulp. The recommended way to round a single-precision floating-point operand to an integer, with the result being a single-precision floating-point number is rintf() , not roundf() . The reason is that roundf() maps to a 4-instruction sequence on the device, whereas rintf() maps to a single instruction. truncf() , ceilf() , and floorf() each map to a single instruction as well. Table 13 Single-Precision Mathematical Standard Library Functions with Maximum ULP Error. The maximum error is stated as the absolute value of the difference in ulps between the result returned by the CUDA library function and a correctly rounded single-precision result obtained according to the round-to-nearest ties-to-even rounding mode. \\uf0c1 Function Maximum ulp error x+y 0 (IEEE-754 round-to-nearest-even) x*y 0 (IEEE-754 round-to-nearest-even) x/y 0 for compute capability \\\\(\\\\ge 2\\\\) when compiled with -prec-div=true 2 (full range), otherwise 1/x 0 for compute capability \\\\(\\\\ge 2\\\\) when compiled with -prec-div=true 1 (full range), otherwise rsqrtf(x) 1/sqrtf(x) 2 (full range) Applies to 1/sqrtf(x) only when it is converted to rsqrtf(x) by the compiler. sqrtf(x) 0 when compiled with -prec-sqrt=true Otherwise 1 for compute capability \\\\(\\\\ge 5.2\\\\) and 3 for older architectures cbrtf(x) 1 (full range) rcbrtf(x) 1 (full range) hypotf(x,y) 3 (full range) rhypotf(x,y) 2 (full range) norm3df(x,y,z) 3 (full range) rnorm3df(x,y,z) 2 (full range) norm4df(x,y,z,t) 3 (full range) rnorm4df(x,y,z,t) 2 (full range) normf(dim,arr) An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off. . rnormf(dim,arr) An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off. expf(x) 2 (full range) exp2f(x) 2 (full range) exp10f(x) 2 (full range) expm1f(x) 1 (full range) logf(x) 1 (full range) log2f(x) 1 (full range) log10f(x) 2 (full range) log1pf(x) 1 (full range) sinf(x) 2 (full range) cosf(x) 2 (full range) tanf(x) 4 (full range) sincosf(x,sptr,cptr) 2 (full range) sinpif(x) 1 (full range) cospif(x) 1 (full range) sincospif(x,sptr,cptr) 1 (full range) asinf(x) 2 (full range) acosf(x) 2 (full range) atanf(x) 2 (full range) atan2f(y,x) 3 (full range) sinhf(x) 3 (full range) coshf(x) 2 (full range) tanhf(x) 2 (full range) asinhf(x) 3 (full range) acoshf(x) 4 (full range) atanhf(x) 3 (full range) powf(x,y) 4 (full range) erff(x) 2 (full range) erfcf(x) 4 (full range) erfinvf(x) 2 (full range) erfcinvf(x) 4 (full range) erfcxf(x) 4 (full range) normcdff(x) 5 (full range) normcdfinvf(x) 5 (full range) lgammaf(x) 6 (outside interval -10.001 … -2.264; larger inside) tgammaf(x) 5 (full range) fmaf(x,y,z) 0 (full range) frexpf(x,exp) 0 (full range) ldexpf(x,exp) 0 (full range) scalbnf(x,n) 0 (full range) scalblnf(x,l) 0 (full range) logbf(x) 0 (full range) ilogbf(x) 0 (full range) j0f(x) 9 for |x| 1.5n, the maximum absolute error is 5 x 10 -12 cyl_bessel_i0(x) 6 (full range) cyl_bessel_i1(x) 6 (full range) fmod(x,y) 0 (full range) remainder(x,y) 0 (full range) remquo(x,y,iptr) 0 (full range) modf(x,iptr) 0 (full range) fdim(x,y) 0 (full range) trunc(x) 0 (full range) round(x) 0 (full range) rint(x) 0 (full range) nearbyint(x) 0 (full range) ceil(x) 0 (full range) floor(x) 0 (full range) lrint(x) 0 (full range) lround(x) 0 (full range) llrint(x) 0 (full range) llround(x) 0 (full range) 13.2. Intrinsic Functions \\uf0c1 The functions from this section can only be used in device code.'},\n",
       " {'id': 326,\n",
       "  'content': 'Among these functions are the less accurate, but faster versions of some of the functions of Standard Functions . They have the same name prefixed with __ (such as __sinf(x) ). They are faster as they map to fewer native instructions. The compiler has an option ( -use_fast_math ) that forces each function in Table 15 to compile to its intrinsic counterpart. In addition to reducing the accuracy of the affected functions, it may also cause some differences in special case handling. A more robust approach is to selectively replace mathematical function calls by calls to intrinsic functions only where it is merited by the performance gains and where changed properties such as reduced accuracy and different special case handling can be tolerated. Table 15 Functions Affected by -use_fast_math \\uf0c1 Operator/Function Device Function x/y __fdividef(x,y) sinf(x) __sinf(x) cosf(x) __cosf(x) tanf(x) __tanf(x) sincosf(x,sptr,cptr) __sincosf(x,sptr,cptr) logf(x) __logf(x) log2f(x) __log2f(x) log10f(x) __log10f(x) expf(x) __expf(x) exp10f(x) __exp10f(x) powf(x,y) __powf(x,y) Single-Precision Floating-Point Functions __fadd_[rn,rz,ru,rd]() and __fmul_[rn,rz,ru,rd]() map to addition and multiplication operations that the compiler never merges into FMADs. By contrast, additions and multiplications generated from the ‘*’ and ‘+’ operators will frequently be combined into FMADs. Functions suffixed with _rn operate using the round to nearest even rounding mode. Functions suffixed with _rz operate using the round towards zero rounding mode. Functions suffixed with _ru operate using the round up (to positive infinity) rounding mode. Functions suffixed with _rd operate using the round down (to negative infinity) rounding mode. The accuracy of floating-point division varies depending on whether the code is compiled with -prec-div=false or -prec-div=true . When the code is compiled with -prec-div=false , both the regular division / operator and __fdividef(x,y) have the same accuracy, but for 2 126 2. __drcp_[rn,rz,ru,rd](x) IEEE-compliant. Requires compute capability > 2. __dsqrt_[rn,rz,ru,rd](x) IEEE-compliant. 14. C++ Language Support \\uf0c1 As described in Compilation with NVCC , CUDA source files compiled with nvcc can include a mix of host code and device code. The CUDA front-end compiler aims to emulate the host compiler behavior with respect to C++ input code. The input source code is processed according to the C++ ISO/IEC 14882:2003, C++ ISO/IEC 14882:2011, C++ ISO/IEC 14882:2014 or C++ ISO/IEC 14882:2017 specifications, and the CUDA front-end compiler aims to emulate any host compiler divergences from the ISO specification. In addition, the supported language is extended with CUDA-specific constructs described in this document 13 , and is subject to the restrictions described below. C++11 Language Features , C++14 Language Features and C++17 Language Features provide support matrices for the C++11, C++14, C++17 and C++20 features, respectively. Restrictions lists the language restrictions. Polymorphic Function Wrappers and Extended Lambdas describe additional features. Code Samples gives code samples.'},\n",
       " {'id': 327,\n",
       "  'content': '14.1. C++11 Language Features \\uf0c1 The following table lists new language features that have been accepted into the C++11 standard. The “Proposal” column provides a link to the ISO C++ committee proposal that describes the feature, while the “Available in nvcc (device code)” column indicates the first version of nvcc that contains an implementation of this feature (if it has been implemented) for device code. Table 18 C++11 Language Features \\uf0c1 Language Feature C++11 Proposal Available in nvcc (device code) Rvalue references N2118 7.0 Rvalue references for *this N2439 7.0 Initialization of class objects by rvalues N1610 7.0 Non-static data member initializers N2756 7.0 Variadic templates N2242 7.0 Extending variadic template template parameters N2555 7.0 Initializer lists N2672 7.0 Static assertions N1720 7.0 auto -typed variables N1984 7.0 Multi-declarator auto N1737 7.0 Removal of auto as a storage-class specifier N2546 7.0 New function declarator syntax N2541 7.0 Lambda expressions N2927 7.0 Declared type of an expression N2343 7.0 Incomplete return types N3276 7.0 Right angle brackets N1757 7.0 Default template arguments for function templates DR226 7.0 Solving the SFINAE problem for expressions DR339 7.0 Alias templates N2258 7.0 Extern templates N1987 7.0 Null pointer constant N2431 7.0 Strongly-typed enums N2347 7.0 Forward declarations for enums N2764 DR1206 7.0 Standardized attribute syntax N2761 7.0 Generalized constant expressions N2235 7.0 Alignment support N2341 7.0 Conditionally-support behavior N1627 7.0 Changing undefined behavior into diagnosable errors N1727 7.0 Delegating constructors N1986 7.0 Inheriting constructors N2540 7.0 Explicit conversion operators N2437 7.0 New character types N2249 7.0 Unicode string literals N2442 7.0 Raw string literals N2442 7.0 Universal character names in literals N2170 7.0 User-defined literals N2765 7.0 Standard Layout Types N2342 7.0 Defaulted functions N2346 7.0 Deleted functions N2346 7.0 Extended friend declarations N1791 7.0 Extending sizeof N2253 DR850 7.0 Inline namespaces N2535 7.0 Unrestricted unions N2544 7.0 Local and unnamed types as template arguments N2657 7.0 Range-based for N2930 7.0 Explicit virtual overrides N2928 N3206 N3272 7.0 Minimal support for garbage collection and reachability-based leak detection N2670 N/A (see Restrictions ) Allowing move constructors to throw [noexcept] N3050 7.0 Defining move special member functions N3053 7.0 Concurrency Sequence points N2239 Atomic operations N2427 Strong Compare and Exchange N2748 Bidirectional Fences N2752 Memory model N2429 Data-dependency ordering: atomics and memory model N2664 Propagating exceptions N2179 Allow atomics use in signal handlers N2547 Thread-local storage N2659 Dynamic initialization and destruction with concurrency N2660 C99 Features in C++11 __func__ predefined identifier N2340 7.0 C99 preprocessor N1653 7.0 long long N1811 7.0 Extended integral types N1988 14.2. C++14 Language Features \\uf0c1 The following table lists new language features that have been accepted into the C++14 standard. Table 19 C++14 Language Features \\uf0c1 Language Feature C++14 Proposal Available in nvcc (device code) Tweak to certain C++ contextual conversions N3323 9.0 Binary literals N3472 9.0 Functions with deduced return type N3638 9.0 Generalized lambda capture (init-capture) N3648 9.0 Generic (polymorphic) lambda expressions N3649 9.0 Variable templates N3651 9.0 Relaxing requirements on constexpr functions N3652 9.0 Member initializers and aggregates N3653 9.0 Clarifying memory allocation N3664 Sized deallocation N3778 [[deprecated]] attribute N3760 9.0 Single-quotation-mark as a digit separator N3781 9.0 14.3. C++17 Language Features \\uf0c1 All C++17 language features are supported in nvcc version 11.0 and later, subject to restrictions described here . 14.4. C++20 Language Features \\uf0c1 All C++20 language features are supported in nvcc version 12.0 and later, subject to restrictions described here . 14.5.'},\n",
       " {'id': 328,\n",
       "  'content': 'Restrictions \\uf0c1 14.5.1. Host Compiler Extensions \\uf0c1 Host compiler specific language extensions are not supported in device code. __Complex types are only supported in host code. __int128 type is supported in device code when compiled in conjunction with a host compiler that supports it. __float128 type is only supported in host code on 64-bit x86 Linux platforms. A constant expression of __float128 type may be processed by the compiler in a floating point representation with lower precision. 14.5.2.'},\n",
       " {'id': 329,\n",
       "  'content': 'Preprocessor Symbols \\uf0c1 14.5.2.1. __CUDA_ARCH__ \\uf0c1 The type signature of the following entities shall not depend on whether __CUDA_ARCH__ is defined or not, or on a particular value of __CUDA_ARCH__ : __global__ functions and function templates __device__ and __constant__ variables textures and surfaces Example: #if !defined(__CUDA_ARCH__) typedef int mytype ; #else typedef double mytype ; #endif __device__ mytype xxx ; // error: xxx\\'s type depends on __CUDA_ARCH__ __global__ void foo ( mytype in , // error: foo\\'s type depends on __CUDA_ARCH__ mytype * ptr ) { * ptr = in ; } If a __global__ function template is instantiated and launched from the host, then the function template must be instantiated with the same template arguments irrespective of whether __CUDA_ARCH__ is defined and regardless of the value of __CUDA_ARCH__ . Example: __device__ int result ; template __global__ void kern ( T in ) { result = in ; } __host__ __device__ void foo ( void ) { #if !defined(__CUDA_ARCH__) kern >> ( 1 ); // error: \"kern\" instantiation only // when __CUDA_ARCH__ is undefined! #endif } int main ( void ) { foo (); cudaDeviceSynchronize (); return 0 ; } In separate compilation mode, the presence or absence of a definition of a function or variable with external linkage shall not depend on whether __CUDA_ARCH__ is defined or on a particular value of __CUDA_ARCH__ 14 . Example: #if !defined(__CUDA_ARCH__) void foo ( void ) { } // error: The definition of foo() // is only present when __CUDA_ARCH__ // is undefined #endif In separate compilation, __CUDA_ARCH__ must not be used in headers such that different objects could contain different behavior. Or, it must be guaranteed that all objects will compile for the same compute_arch. If a weak function or template function is defined in a header and its behavior depends on __CUDA_ARCH__ , then the instances of that function in the objects could conflict if the objects are compiled for different compute arch. For example, if an a.h contains: template __device__ T * getptr ( void ) { #if __CUDA_ARCH__ == 700 return NULL ; /* no address */ #else __shared__ T arr [ 256 ]; return arr ; #endif } Then if a.cu and b.cu both include a.h and instantiate getptr for the same type, and b.cu expects a non-NULL address, and compile with: nvcc –arch=compute_70 –dc a.cu nvcc –arch=compute_80 –dc b.cu nvcc –arch=sm_80 a.o b.o At link time only one version of the getptr is used, so the behavior would depend on which version is chosen. To avoid this, either a.cu and b.cu must be compiled for the same compute arch, or __CUDA_ARCH__ should not be used in the shared header function. The compiler does not guarantee that a diagnostic will be generated for the unsupported uses of __CUDA_ARCH__ described above. 14.5.3.'},\n",
       " {'id': 330,\n",
       "  'content': 'Qualifiers \\uf0c1 14.5.3.1. Device Memory Space Specifiers \\uf0c1 The __device__ , __shared__ , __managed__ and __constant__ memory space specifiers are not allowed on: class , struct , and union data members, formal parameters, non-extern variable declarations within a function that executes on the host. The __device__ , __constant__ and __managed__ memory space specifiers are not allowed on variable declarations that are neither extern nor static within a function that executes on the device. A __device__ , __constant__ , __managed__ or __shared__ variable definition cannot have a class type with a non-empty constructor or a non-empty destructor. A constructor for a class type is considered empty at a point in the translation unit, if it is either a trivial constructor or it satisfies all of the following conditions: The constructor function has been defined. The constructor function has no parameters, the initializer list is empty and the function body is an empty compound statement. Its class has no virtual functions, no virtual base classes and no non-static data member initializers. The default constructors of all base classes of its class can be considered empty. For all the nonstatic data members of its class that are of class type (or array thereof), the default constructors can be considered empty. A destructor for a class is considered empty at a point in the translation unit, if it is either a trivial destructor or it satisfies all of the following conditions: The destructor function has been defined. The destructor function body is an empty compound statement. Its class has no virtual functions and no virtual base classes. The destructors of all base classes of its class can be considered empty. For all the nonstatic data members of its class that are of class type (or array thereof), the destructor can be considered empty. When compiling in the whole program compilation mode (see the nvcc user manual for a description of this mode), __device__ , __shared__ , __managed__ and __constant__ variables cannot be defined as external using the extern keyword. The only exception is for dynamically allocated __shared__ variables as described in index.html#__shared__ . When compiling in the separate compilation mode (see the nvcc user manual for a description of this mode), __device__ , __shared__ , __managed__ and __constant__ variables can be defined as external using the extern keyword. nvlink will generate an error when it cannot find a definition for an external variable (unless it is a dynamically allocated __shared__ variable). 14.5.3.2. __managed__ Memory Space Specifier \\uf0c1 Variables marked with the __managed__ memory space specifier (“managed” variables) have the following restrictions: The address of a managed variable is not a constant expression. A managed variable shall not have a const qualified type. A managed variable shall not have a reference type. The address or value of a managed variable shall not be used when the CUDA runtime may not be in a valid state, including the following cases: In static/dynamic initialization or destruction of an object with static or thread local storage duration. In code that executes after exit() has been called (for example, a function marked with gcc’s “ __attribute__((destructor)) ”). In code that executes when CUDA runtime may not be initialized (for example, a function marked with gcc’s “ __attribute__((constructor)) ”). A managed variable cannot be used as an unparenthesized id-expression argument to a decltype() expression. Managed variables have the same coherence and consistency behavior as specified for dynamically allocated managed memory. When a CUDA program containing managed variables is run on an execution platform with multiple GPUs, the variables are allocated only once, and not per GPU. A managed variable declaration without the extern linkage is not allowed within a function that executes on the host. A managed variable declaration without the extern or static linkage is not allowed within a function that executes on the device. Here are examples of legal and illegal uses of managed variables: __device__ __managed__ int xxx = 10 ; // OK int * ptr = & xxx ; // error: use of managed variable // (xxx) in static initialization struct S1_t { int field ; S1_t ( void ) : field ( xxx ) { }; }; struct S2_t { ~ S2_t ( void ) { xxx = 10 ; } }; S1_t temp1 ; // error: use of managed variable // (xxx) in dynamic initialization S2_t temp2 ; // error: use of managed variable // (xxx) in the destructor of // object with static storage // duration __device__ __managed__ const int yyy = 10 ; // error: const qualified type __device__ __managed__ int & zzz = xxx ; // error: reference type template struct S3_t { }; S3_t temp ; // error: address of managed // variable(xxx) not a // constant expression __global__ void kern ( int * ptr ) { assert ( ptr == & xxx ); // OK xxx = 20 ; // OK } int main ( void ) { int * ptr = & xxx ; // OK kern >> ( ptr ); cudaDeviceSynchronize (); xxx ++ ; // OK decltype ( xxx ) qqq ; // error: managed variable(xxx) used // as unparenthized argument to // decltype decltype (( xxx )) zzz = yyy ; // OK } 14.5.3.3. Volatile Qualifier \\uf0c1 The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers or L1 cache) as long as it respects the memory ordering semantics of memory fence functions ( Memory Fence Functions ) and memory visibility semantics of synchronization functions ( Synchronization Functions ).'},\n",
       " {'id': 331,\n",
       "  'content': 'These optimizations can be disabled using the volatile keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory read or write instruction. 14.5.4. Pointers \\uf0c1 Dereferencing a pointer either to global or shared memory in code that is executed on the host, or to host memory in code that is executed on the device results in an undefined behavior, most often in a segmentation fault and application termination. The address obtained by taking the address of a __device__ , __shared__ or __constant__ variable can only be used in device code. The address of a __device__ or __constant__ variable obtained through cudaGetSymbolAddress() as described in Device Memory can only be used in host code. 14.5.5. Operators \\uf0c1 14.5.5.1. Assignment Operator \\uf0c1 __constant__ variables can only be assigned from the host code through runtime functions ( Device Memory ); they cannot be assigned from the device code. __shared__ variables cannot have an initialization as part of their declaration. It is not allowed to assign values to any of the built-in variables defined in Built-in Variables . 14.5.5.2. Address Operator \\uf0c1 It is not allowed to take the address of any of the built-in variables defined in Built-in Variables . 14.5.6. Run Time Type Information (RTTI) \\uf0c1 The following RTTI-related features are supported in host code, but not in device code. typeid operator std::type_info dynamic_cast operator 14.5.7. Exception Handling \\uf0c1 Exception handling is only supported in host code, but not in device code. Exception specification is not supported for __global__ functions. 14.5.8. Standard Library \\uf0c1 Standard libraries are only supported in host code, but not in device code, unless specified otherwise.'},\n",
       " {'id': 332,\n",
       "  'content': '14.5.9. Namespace Reservations \\uf0c1 Unless an exception is otherwise noted, it is undefined behavior to add any declarations or definitions to cuda:: , nv:: , cooperative_groups:: or any namespace nested within. Examples: namespace cuda { // Bad: class declaration added to namespace cuda struct foo {}; // Bad: function definition added to namespace cuda cudaStream_t make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } } // namespace cuda namespace cuda { namespace utils { // Bad: function definition added to namespace nested within cuda cudaStream_t make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } } // namespace utils } // namespace cuda namespace utils { namespace cuda { // Okay: namespace cuda may be used nested within a non-reserved namespace cudaStream_t make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } } // namespace cuda } // namespace utils // Bad: Equivalent to adding symbols to namespace cuda at global scope using namespace utils ; 14.5.10. Functions \\uf0c1 14.5.10.1.'},\n",
       " {'id': 333,\n",
       "  'content': 'External Linkage \\uf0c1 A call within some device code of a function declared with the extern qualifier is only allowed if the function is defined within the same compilation unit as the device code, i.e., a single file or several files linked together with relocatable device code and nvlink. 14.5.10.2. Implicitly-declared and explicitly-defaulted functions \\uf0c1 Let F denote a function that is either implicitly-declared or is explicitly-defaulted on its first declaration The execution space specifiers ( __host__ , __device__ ) for F are the union of the execution space specifiers of all the functions that invoke it (note that a __global__ caller will be treated as a __device__ caller for this analysis). For example: class Base { int x ; public : __host__ __device__ Base ( void ) : x ( 10 ) {} }; class Derived : public Base { int y ; }; class Other : public Base { int z ; }; __device__ void foo ( void ) { Derived D1 ; Other D2 ; } __host__ void bar ( void ) { Other D3 ; } Here, the implicitly-declared constructor function “Derived::Derived” will be treated as a __device__ function, since it is invoked only from the __device__ function “foo”. The implicitly-declared constructor function “Other::Other” will be treated as a __host__ __device__ function, since it is invoked both from a __device__ function “foo” and a __host__ function “bar”. In addition, if F is a virtual destructor, then the execution spaces of each virtual destructor D overridden by F are added to the set of execution spaces for F , if D is either not implicitly defined or is explicitly defaulted on a declaration other than its first declaration. For example: struct Base1 { virtual __host__ __device__ ~ Base1 () { } }; struct Derived1 : Base1 { }; // implicitly-declared virtual destructor // ~Derived1 has __host__ __device__ // execution space specifiers struct Base2 { virtual __device__ ~ Base2 (); }; __device__ Base2 ::~ Base2 () = default ; struct Derived2 : Base2 { }; // implicitly-declared virtual destructor // ~Derived2 has __device__ execution // space specifiers 14.5.10.3. Function Parameters \\uf0c1 __global__ function parameters are passed to the device via constant memory and are limited to 32,764 bytes starting with Volta, and 4 KB on older architectures. __global__ functions cannot have a variable number of arguments. __global__ function parameters cannot be pass-by-reference. In separate compilation mode, if a __device__ or __global__ function is ODR-used in a particular translation unit, then the parameter and return types of the function must be complete in that translation unit. Example: //first.cu: struct S ; __device__ void foo ( S ); // error: type \\'S\\' is incomplete __device__ auto * ptr = foo ; int main () { } //second.cu: struct S { int x ; }; __device__ void foo ( S ) { } //compiler invocation $nvcc -std=c++14 -rdc=true first.cu second.cu -o first nvlink error : Prototype doesn\\'t match for \\'_Z3foo1S\\' in \\'/tmp/tmpxft_00005c8c_00000000-18_second.o\\', first defined in \\'/tmp/tmpxft_00005c8c_00000000-18_second.o\\' nvlink fatal : merge_elf failed 14.5.10.3.1. __global__ Function Argument Processing \\uf0c1 When a __global__ function is launched from device code, each argument must be trivially copyable and trivially destructible. When a __global__ function is launched from host code, each argument type is allowed to be non-trivially copyable or non-trivially-destructible, but the processing for such types does not follow the standard C++ model, as described below. User code must ensure that this workflow does not affect program correctness. The workflow diverges from standard C++ in two areas: Memcpy instead of copy constructor invocation When lowering a __global__ function launch from host code, the compiler generates stub functions that copy the parameters one or more times by value, before eventually using memcpy to copy the arguments to the __global__ function’s parameter memory on the device. This occurs even if an argument was non-trivially-copyable, and therefore may break programs where the copy constructor has side effects. Example: #include struct S { int x ; int * ptr ; __host__ __device__ S () { } __host__ __device__ S ( const S & ) { ptr = & x ; } }; __global__ void foo ( S in ) { // this assert may fail, because the compiler // generated code will memcpy the contents of \"in\" // from host to kernel parameter memory, so the // \"in.ptr\" is not initialized to \"&in.x\" because // the copy constructor is skipped. assert ( in . ptr == & in . x ); } int main () { S tmp ; foo >> ( tmp ); cudaDeviceSynchronize (); } Example: #include __managed__ int counter ; struct S1 { S1 () { } S1 ( const S1 & ) { ++ counter ; } }; __global__ void foo ( S1 ) { /* this assertion may fail, because the compiler generates stub functions on the host for a kernel launch, and they may copy the argument by value more than once. */ assert ( counter == 1 ); } int main () { S1 V ; foo >> ( V ); cudaDeviceSynchronize (); } Destructor may be invoked before the ``__global__`` function has finished Kernel launches are asynchronous with host execution. As a result, if a __global__ function argument has a non-trivial destructor, the destructor may execute in host code even before the __global__ function has finished execution. This may break programs where the destructor has side effects. Example: struct S { int * ptr ; S () : ptr ( nullptr ) { } S ( const S & ) { cudaMallocManaged ( & ptr , sizeof ( int )); } ~ S () { cudaFree ( ptr ); } }; __global__ void foo ( S in ) { //error: This store may write to memory that has already been // freed (see below). * ( in . ptr ) = 4 ; } int main () { S V ; /* The object \\'V\\' is first copied by value to a compiler-generated * stub function that does the kernel launch, and the stub function * bitwise copies the contents of the argument to kernel parameter * memory. * However, GPU kernel execution is asynchronous with host * execution. * As a result, S::~S() will execute when the stub function returns, releasing allocated memory, even though the kernel may not have finished execution. */ foo >> ( V ); cudaDeviceSynchronize (); } 14.5.10.3.2. Toolkit and Driver Compatibility \\uf0c1 Developers must use the 12.1 Toolkit and r530 driver or higher to compile, launch, and debug kernels that accept parameters larger than 4KB. If such kernels are launched on older drivers, CUDA will issue the error CUDA_ERROR_NOT_SUPPORTED . 14.5.10.3.3. Link Compatibility across Toolkit Revisions \\uf0c1 When linking device objects, if at least one device object contains a kernel with a parameter larger than 4KB, the developer must recompile all objects from their respective device sources with the 12.1 toolkit or higher before linking them together. Failure to do so will result in a linker error.'},\n",
       " {'id': 334,\n",
       "  'content': '14.5.10.4. Static Variables within Function \\uf0c1 Variable memory space specifiers are allowed in the declaration of a static variable V within the immediate or nested block scope of a function F where: F is a __global__ or __device__ -only function. F is a __host__ __device__ function and __CUDA_ARCH__ is defined 17 . If no explicit memory space specifier is present in the declaration of V , an implicit __device__ specifier is assumed during device compilation. V has the same initialization restrictions as a variable with the same memory space specifiers declared in namespace scope for example a __device__ variable cannot have a ‘non-empty’ constructor (see Device Memory Space Specifiers ). Examples of legal and illegal uses of function-scope static variables are shown below. struct S1_t { int x ; }; struct S2_t { int x ; __device__ S2_t ( void ) { x = 10 ; } }; struct S3_t { int x ; __device__ S3_t ( int p ) : x ( p ) { } }; __device__ void f1 () { static int i1 ; // OK, implicit __device__ memory space specifier static int i2 = 11 ; // OK, implicit __device__ memory space specifier static __managed__ int m1 ; // OK static __device__ int d1 ; // OK static __constant__ int c1 ; // OK static S1_t i3 ; // OK, implicit __device__ memory space specifier static S1_t i4 = { 22 }; // OK, implicit __device__ memory space specifier static __shared__ int i5 ; // OK int x = 33 ; static int i6 = x ; // error: dynamic initialization is not allowed static S1_t i7 = { x }; // error: dynamic initialization is not allowed static S2_t i8 ; // error: dynamic initialization is not allowed static S3_t i9 ( 44 ); // error: dynamic initialization is not allowed } __host__ __device__ void f2 () { static int i1 ; // OK, implicit __device__ memory space specifier // during device compilation. #ifdef __CUDA_ARCH__ static __device__ int d1 ; // OK, declaration is only visible during device // compilation (__CUDA_ARCH__ is defined) #else static int d0 ; // OK, declaration is only visible during host // compilation (__CUDA_ARCH__ is not defined) #endif static __device__ int d2 ; // error: __device__ variable inside // a host function during host compilation // i.e. when __CUDA_ARCH__ is not defined static __shared__ int i2 ; // error: __shared__ variable inside // a host function during host compilation // i.e. when __CUDA_ARCH__ is not defined } 14.5.10.5. Function Pointers \\uf0c1 The address of a __global__ function taken in host code cannot be used in device code (e.g. to launch the kernel). Similarly, the address of a __global__ function taken in device code cannot be used in host code. It is not allowed to take the address of a __device__ function in host code. 14.5.10.6. Function Recursion \\uf0c1 __global__ functions do not support recursion. 14.5.10.7. Friend Functions \\uf0c1 A __global__ function or function template cannot be defined in a friend declaration. Example: struct S1_t { friend __global__ void foo1 ( void ); // OK: not a definition template friend __global__ void foo2 ( void ); // OK: not a definition friend __global__ void foo3 ( void ) { } // error: definition in friend declaration template friend __global__ void foo4 ( void ) { } // error: definition in friend declaration }; 14.5.10.8. Operator Function \\uf0c1 An operator function cannot be a __global__ function. 14.5.10.9. Allocation and Deallocation Functions \\uf0c1 A user-defined operator new , operator new[] , operator delete , or operator delete[] cannot be used to replace the corresponding __host__ or __device__ builtins provided by the compiler. 14.5.11.'},\n",
       " {'id': 335,\n",
       "  'content': 'Classes \\uf0c1 14.5.11.1. Data Members \\uf0c1 Static data members are not supported except for those that are also const-qualified (see Const-qualified variables ). 14.5.11.2. Function Members \\uf0c1 Static member functions cannot be __global__ functions. 14.5.11.3. Virtual Functions \\uf0c1 When a function in a derived class overrides a virtual function in a base class, the execution space specifiers (i.e., __host__ , __device__ ) on the overridden and overriding functions must match. It is not allowed to pass as an argument to a __global__ function an object of a class with virtual functions. If an object is created in host code, invoking a virtual function for that object in device code has undefined behavior. If an object is created in device code, invoking a virtual function for that object in host code has undefined behavior. See Windows-Specific for additional constraints when using the Microsoft host compiler. Example: struct S1 { virtual __host__ __device__ void foo () { } }; __managed__ S1 * ptr1 , * ptr2 ; __managed__ __align__ ( 16 ) char buf1 [ 128 ]; __global__ void kern () { ptr1 -> foo (); // error: virtual function call on a object // created in host code. ptr2 = new ( buf1 ) S1 (); } int main ( void ) { void * buf ; cudaMallocManaged ( & buf , sizeof ( S1 ), cudaMemAttachGlobal ); ptr1 = new ( buf ) S1 (); kern >> (); cudaDeviceSynchronize (); ptr2 -> foo (); // error: virtual function call on an object // created in device code. }\\n14.5.11.4. Virtual Base Classes \\uf0c1 It is not allowed to pass as an argument to a __global__ function an object of a class derived from virtual base classes. 14.5.11.5. Anonymous Unions \\uf0c1 Member variables of a namespace scope anonymous union cannot be referenced in a __global__ or __device__ function. 14.5.11.6. Windows-Specific \\uf0c1 The CUDA compiler follows the IA64 ABI for class layout, while the Microsoft host compiler does not. Let T denote a pointer to member type, or a class type that satisfies any of the following conditions: T has virtual functions.'},\n",
       " {'id': 336,\n",
       "  'content': 'T has a virtual base class. T has multiple inheritance with more than one direct or indirect empty base class. All direct and indirect base classes B of T are empty and the type of the first field F of T uses B in its definition, such that B is laid out at offset 0 in the definition of F . Let C denote T or a class type that has T as a field type or as a base class type. The CUDA compiler may compute the class layout and size differently than the Microsoft host compiler for the type C . As long as the type C is used exclusively in host or device code, the program should work correctly. Passing an object of type C between host and device code has undefined behavior, for example, as an argument to a __global__ function or through cudaMemcpy*() calls. Accessing an object of type C or any subobject in device code, or invoking a member function in device code, has undefined behavior if the object is created in host code. Accessing an object of type C or any subobject in host code, or invoking a member function in host code, has undefined behavior if the object is created in device code 18 . 14.5.12. Templates \\uf0c1 A type or template cannot be used in the type, non-type or template template argument of a __global__ function template instantiation or a __device__/__constant__ variable instantiation if either: The type or template is defined within a __host__ or __host__ __device__ . The type or template is a class member with private or protected access and its parent class is not defined within a __device__ or __global__ function. The type is unnamed. The type is compounded from any of the types above. Example: template __global__ void myKernel ( void ) { } class myClass { private : struct inner_t { }; public : static void launch ( void ) { // error: inner_t is used in template argument // but it is private myKernel >> (); } }; // C++14 only template __device__ T d1 ; template __device__ T1 d2 ; void fn () { struct S1_t { }; // error (C++14 only): S1_t is local to the function fn d1 = {}; auto lam1 = [] { }; // error (C++14 only): a closure type cannot be used for // instantiating a variable template d2 = 10 ; } 14.5.13. Trigraphs and Digraphs \\uf0c1 Trigraphs are not supported on any platform.'},\n",
       " {'id': 337,\n",
       "  'content': 'Digraphs are not supported on Windows. 14.5.14. Const-qualified variables \\uf0c1 Let ‘V’ denote a namespace scope variable or a class static member variable that has const qualified type and does not have execution space annotations (for example, __device__, __constant__, __shared__ ). V is considered to be a host code variable. The value of V may be directly used in device code, if V has been initialized with a constant expression before the point of use, the type of V is not volatile-qualified, and it has one of the following types: built-in floating point type except when the Microsoft compiler is used as the host compiler, built-in integral type. Device source code cannot contain a reference to V or take the address of V. Example: const int xxx = 10 ; struct S1_t { static const int yyy = 20 ; }; extern const int zzz ; const float www = 5.0 ; __device__ void foo ( void ) { int local1 [ xxx ]; // OK int local2 [ S1_t :: yyy ]; // OK int val1 = xxx ; // OK int val2 = S1_t :: yyy ; // OK int val3 = zzz ; // error: zzz not initialized with constant // expression at the point of use. const int & val3 = xxx ; // error: reference to host variable const int * val4 = & xxx ; // error: address of host variable const float val5 = www ; // OK except when the Microsoft compiler is used as // the host compiler. }\\nconst int zzz = 20 ; 14.5.15. Long Double \\uf0c1 The use of long double type is not supported in device code. 14.5.16. Deprecation Annotation \\uf0c1 nvcc supports the use of deprecated attribute when using gcc , clang , xlC , icc or pgcc host compilers, and the use of deprecated declspec when using the cl.exe host compiler. It also supports the [[deprecated]] standard attribute when the C++14 dialect has been enabled. The CUDA frontend compiler will generate a deprecation diagnostic for a reference to a deprecated entity from within the body of a __device__ , __global__ or __host__ __device__ function when __CUDA_ARCH__ is defined (i.e., during device compilation phase). Other references to deprecated entities will be handled by the host compiler, e.g., a reference from within a __host__ function. The CUDA frontend compiler does not support the #pragma gcc diagnostic or #pragma warning mechanisms supported by various host compilers. Therefore, deprecation diagnostics generated by the CUDA frontend compiler are not affected by these pragmas, but diagnostics generated by the host compiler will be affected. To suppress the warning for device-code, user can use NVIDIA specific pragma #pragma nv_diag_suppress . The nvcc flag -Wno-deprecated-declarations can be used to suppress all deprecation warnings, and the flag -Werror=deprecated-declarations can be used to turn deprecation warnings into errors. 14.5.17. Noreturn Annotation \\uf0c1 nvcc supports the use of noreturn attribute when using gcc , clang , xlC , icc or pgcc host compilers, and the use of noreturn declspec when using the cl.exe host compiler. It also supports the [[noreturn]] standard attribute when the C++11 dialect has been enabled. The attribute/declspec can be used in both host and device code. 14.5.18. [[likely]] / [[unlikely]] Standard Attributes \\uf0c1 These attributes are accepted in all configurations that support the C++ standard attribute syntax. The attributes can be used to hint to the device compiler optimizer whether a statement is more or less likely to be executed compared to any alternative path that does not include the statement. Example: __device__ int foo ( int x ) { if ( i __global__ void foo ( T in ) { }; template struct S1_t { }; void bar ( void ) { auto temp1 = [] { }; foo >> ( temp1 ); // error: lambda closure type used in // template type argument foo >> ( S1_t ()); // error: lambda closure type used in // template type argument } 14.5.22.2. std::initializer_list \\uf0c1 By default, the CUDA compiler will implicitly consider the member functions of std::initializer_list to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code. The nvcc flag --no-host-device-initializer-list will disable this behavior; member functions of std::initializer_list will then be considered as __host__ functions and will not be directly invokable from device code. Example: #include __device__ int foo ( std :: initializer_list in ); __device__ void bar ( void ) { foo ({ 4 , 5 , 6 }); // (a) initializer list containing only // constant expressions. int i = 4 ; foo ({ i , 5 , 6 }); // (b) initializer list with at least one // non-constant element. // This form may have better performance than (a).'},\n",
       " {'id': 338,\n",
       "  'content': \"}\\n14.5.22.3. Rvalue references \\uf0c1 By default, the CUDA compiler will implicitly consider std::move and std::forward function templates to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code. The nvcc flag --no-host-device-move-forward will disable this behavior; std::move and std::forward will then be considered as __host__ functions and will not be directly invokable from device code. 14.5.22.4. Constexpr functions and function templates \\uf0c1 By default, a constexpr function cannot be called from a function with incompatible execution space 22 . The experimental nvcc flag --expt-relaxed-constexpr removes this restriction 23 . When this flag is specified, host code can invoke a __device__ constexpr function and device code can invoke a __host__ constexpr function. nvcc will define the macro __CUDACC_RELAXED_CONSTEXPR__ when --expt-relaxed-constexpr has been specified. Note that a function template instantiation may not be a constexpr function even if the corresponding template is marked with the keyword constexpr (C++11 Standard Section [dcl.constexpr.p6] ). 14.5.22.5. Constexpr variables \\uf0c1 Let ‘V’ denote a namespace scope variable or a class static member variable that has been marked constexpr and that does not have execution space annotations (e.g., __device__, __constant__, __shared__ ). If V is of scalar type 24 other than long double and the type is not volatile-qualified, the value of V can be directly used in device code. In addition, if V is of a non-scalar type then scalar elements of V can be used inside a constexpr __device__ or __host__ __device__ function, if the call to the function is a constant expression 25 . Device source code cannot contain a reference to V or take the address of V. Example: constexpr int xxx = 10 ; constexpr int yyy = xxx + 4 ; struct S1_t { static constexpr int qqq = 100 ; }; constexpr int host_arr [] = { 1 , 2 , 3 }; constexpr __device__ int get ( int idx ) { return host_arr [ idx ]; } __device__ int foo ( int idx ) { int v1 = xxx + yyy + S1_t :: qqq ; // OK const int & v2 = xxx ; // error: reference to host constexpr // variable const int * v3 = & xxx ; // error: address of host constexpr // variable const int & v4 = S1_t :: qqq ; // error: reference to host constexpr // variable const int * v5 = & S1_t :: qqq ; // error: address of host constexpr // variable v1 += get ( 2 ); // OK: 'get(2)' is a constant // expression. v1 += get ( idx ); // error: 'get(idx)' is not a constant // expression v1 += host_arr [ 2 ]; // error: 'host_arr' does not have // scalar type.\"},\n",
       " {'id': 339,\n",
       "  'content': 'return v1 ; } 14.5.22.6. Inline namespaces \\uf0c1 For an input CUDA translation unit, the CUDA compiler may invoke the host compiler for compiling the host code within the translation unit. In the code passed to the host compiler, the CUDA compiler will inject additional compiler generated code, if the input CUDA translation unit contained a definition of any of the following entities: __global__ function or function template instantiation __device__ , __constant__ variables with surface or texture type The compiler generated code contains a reference to the defined entity. If the entity is defined within an inline namespace and another entity of the same name and type signature is defined in an enclosing namespace, this reference may be considered ambiguous by the host compiler and host compilation will fail. This limitation can be avoided by using unique names for such entities defined within an inline namespace. Example: __device__ int Gvar ; inline namespace N1 { __device__ int Gvar ; } // __global__ void foo ( void ); // error __global__ void bar ( void ) { } // error template <> __global__ void foo ( void ) { } // error __device__ int x1b ; // error __constant__ int x2b ; // error __shared__ int x3b ; // error texture q2 ; // error surface s2 ; // error } }; 14.5.22.7. thread_local \\uf0c1 The thread_local storage specifier is not allowed in device code. 14.5.22.8. __global__ functions and function templates \\uf0c1 If the closure type associated with a lambda expression is used in a template argument of a __global__ function template instantiation, the lambda expression must either be defined in the immediate or nested block scope of a __device__ or __global__ function, or must be an extended lambda . Example: template __global__ void kernel ( T in ) { } __device__ void foo_device ( void ) { // All kernel instantiations in this function // are valid, since the lambdas are defined inside // a __device__ function. kernel >> ( [] __device__ { } ); kernel >> ( [] __host__ __device__ { } ); kernel >> ( [] { } ); } auto lam1 = [] { }; auto lam2 = [] __host__ __device__ { }; void foo_host ( void ) { // OK: instantiated with closure type of an extended __device__ lambda kernel >> ( [] __device__ { } ); // OK: instantiated with closure type of an extended __host__ __device__ // lambda kernel >> ( [] __host__ __device__ { } ); // error: unsupported: instantiated with closure type of a lambda // that is not an extended lambda kernel >> ( [] { } ); // error: unsupported: instantiated with closure type of a lambda // that is not an extended lambda kernel >> ( lam1 ); // error: unsupported: instantiated with closure type of a lambda // that is not an extended lambda kernel >> ( lam2 ); } A __global__ function or function template cannot be declared as constexpr . A __global__ function or function template cannot have a parameter of type std::initializer_list or va_list .'},\n",
       " {'id': 340,\n",
       "  'content': \"A __global__ function cannot have a parameter of rvalue reference type. A variadic __global__ function template has the following restrictions: Only a single pack parameter is allowed. The pack parameter must be listed last in the template parameter list. Example: // ok template class Wrapper , typename ... Pack > __global__ void foo1 ( Wrapper ); // error: pack parameter is not last in parameter list template class Wrapper > __global__ void foo2 ( Wrapper ); // error: multiple parameter packs template class Wrapper1 , template class Wrapper2 > __global__ void foo3 ( Wrapper1 , Wrapper2 ); 14.5.22.9. __managed__ and __shared__ variables \\uf0c1 `__managed__ and __shared__ variables cannot be marked with the keyword constexpr . 14.5.22.10. Defaulted functions \\uf0c1 Execution space specifiers on a function that is explicitly-defaulted on its first declaration are ignored by the CUDA compiler. Instead, the CUDA compiler will infer the execution space specifiers as described in Implicitly-declared and explicitly-defaulted functions . Execution space specifiers are not ignored if the function is explicitly-defaulted, but not on its first declaration. Example: struct S1 { // warning: __host__ annotation is ignored on a function that // is explicitly-defaulted on its first declaration __host__ S1 () = default ; }; __device__ void foo1 () { //note: __device__ execution space is derived for S1::S1 // based on implicit call from within __device__ function // foo1 S1 s1 ; } struct S2 { __host__ S2 (); }; //note: S2::S2 is not defaulted on its first declaration, and // its execution space is fixed to __host__ based on its // first declaration. S2 :: S2 () = default ; __device__ void foo2 () { // error: call from __device__ function 'foo2' to // __host__ function 'S2::S2' S2 s2 ; } 14.5.23. C++14 Features \\uf0c1 C++14 features enabled by default by the host compiler are also supported by nvcc. Passing nvcc -std=c++14 flag turns on all C++14 features and also invokes the host preprocessor, compiler and linker with the corresponding C++14 dialect option 26 . This section describes the restrictions on the supported C++14 features. 14.5.23.1. Functions with deduced return type \\uf0c1 A __global__ function cannot have a deduced return type. If a __device__ function has deduced return type, the CUDA frontend compiler will change the function declaration to have a void return type, before invoking the host compiler. This may cause issues for introspecting the deduced return type of the __device__ function in host code. Thus, the CUDA compiler will issue compile-time errors for referencing such deduced return type outside device function bodies, except if the reference is absent when __CUDA_ARCH__ is undefined. Examples: __device__ auto fn1 ( int x ) { return x ; } __device__ decltype ( auto ) fn2 ( int x ) { return x ; } __device__ void device_fn1 () { // OK int ( * p1 )( int ) = fn1 ; } // error: referenced outside device function bodies decltype ( fn1 ( 10 )) g1 ; void host_fn1 () { // error: referenced outside device function bodies int ( * p1 )( int ) = fn1 ; struct S_local_t { // error: referenced outside device function bodies decltype ( fn2 ( 10 )) m1 ; S_local_t () : m1 ( 10 ) { } }; } // error: referenced outside device function bodies template void host_fn2 () { } template struct S1_t { }; // error: referenced outside device function bodies struct S1_derived_t : S1_t { }; 14.5.23.2. Variable templates \\uf0c1 A __device__/__constant__ variable template cannot have a const qualified type when using the Microsoft host compiler. Examples: // error: a __device__ variable template cannot // have a const qualified type on Windows template __device__ const T d1 ( 2 ); int * const x = nullptr ; // error: a __device__ variable template cannot // have a const qualified type on Windows template __device__ T * const d2 ( x ); // OK template __device__ const T * d3 ; __device__ void fn () { int t1 = d1 ; int * const t2 = d2 ; const int * t3 = d3 ; } 14.5.24. C++17 Features \\uf0c1 C++17 features enabled by default by the host compiler are also supported by nvcc. Passing nvcc -std=c++17 flag turns on all C++17 features and also invokes the host preprocessor, compiler and linker with the corresponding C++17 dialect option 27 . This section describes the restrictions on the supported C++17 features. 14.5.24.1. Inline Variable \\uf0c1 A namespace scope inline variable declared with __device__ or __constant__ or __managed__ memory space specifier must have internal linkage, if the code is compiled with nvcc in whole program compilation mode. Examples: inline __device__ int xxx ; //error when compiled with nvcc in //whole program compilation mode. //ok when compiled with nvcc in //separate compilation mode. inline __shared__ int yyy0 ; // ok. static inline __device__ int yyy ; // ok: internal linkage namespace { inline __device__ int zzz ; // ok: internal linkage } When using g++ host compiler, an inline variable declared with __managed__ memory space specifier may not be visible to the debugger. 14.5.24.2. Structured Binding \\uf0c1 A structured binding cannot be declared with a variable memory space specifier. Example: struct S { int x ; int y ; }; __device__ auto [ a1 , b1 ] = S { 4 , 5 }; // error 14.5.25. C++20 Features \\uf0c1 C++20 features enabled by default by the host compiler are also supported by nvcc. Passing nvcc -std=c++20 flag turns on all C++20 features and also invokes the host preprocessor, compiler and linker with the corresponding C++20 dialect option 28 . This section describes the restrictions on the supported C++20 features. 14.5.25.1. Module support \\uf0c1 Modules are not supported in CUDA C++, in either host or device code. Uses of the module , export and import keywords are diagnosed as errors.\"},\n",
       " {'id': 341,\n",
       "  'content': \"14.5.25.2. Coroutine support \\uf0c1 Coroutines are not supported in device code. Uses of the co_await , co_yield and co_return keywords in the scope of a device function are diagnosed as error during device compilation. 14.5.25.3. Three-way comparison operator \\uf0c1 The three-way comparison operator is supported in both host and device code, but some uses implicitly rely on functionality from the Standard Template Library provided by the host implementation. Uses of those operators may require specifying the flag --expt-relaxed-constexpr to silence warnings and the functionality requires that the host implementation satisfies the requirements of device code. Example: #include struct S { int x , y , z ; auto operator ( const S & rhs ) const = default ; __host__ __device__ bool operator ( int rhs ) const { return false ; } }; __host__ __device__ bool f ( S a , S b ) { if ( a 1 ) // ok, calls a user-defined host-device overload return true ; return a __device__ int foo_d () { return 1 ; } __host__ __device__ int foo_hd () { return 2 ; } __host__ int foo_h () { return 3 ; } __global__ void kernel ( int * result ) { nvstd :: function fn1 = foo_d ; nvstd :: function fn2 = foo_hd ; nvstd :: function fn3 = []() { return 10 ; }; * result = fn1 () + fn2 () + fn3 (); } __host__ __device__ void hostdevice_func ( int * result ) { nvstd :: function fn1 = foo_hd ; nvstd :: function fn2 = []() { return 10 ; }; * result = fn1 () + fn2 (); } __host__ void host_func ( int * result ) { nvstd :: function fn1 = foo_h ; nvstd :: function fn2 = foo_hd ; nvstd :: function fn3 = []() { return 10 ; }; * result = fn1 () + fn2 () + fn3 (); } Instances of nvstd::function in host code cannot be initialized with the address of a __device__ function or with a functor whose operator() is a __device__ function. Instances of nvstd::function in device code cannot be initialized with the address of a __host__ function or with a functor whose operator() is a __host__ function. nvstd::function instances cannot be passed from host code to device code (and vice versa) at run time. nvstd::function cannot be used in the parameter type of a __global__ function, if the __global__ function is launched from host code. Example: #include __device__ int foo_d () { return 1 ; } __host__ int foo_h () { return 3 ; } auto lam_h = [] { return 0 ; }; __global__ void k ( void ) { // error: initialized with address of __host__ function nvstd :: function fn1 = foo_h ; // error: initialized with address of functor with // __host__ operator() function nvstd :: function fn2 = lam_h ; } __global__ void kern ( nvstd :: function f1 ) { } void foo ( void ) { // error: initialized with address of __device__ function nvstd :: function fn1 = foo_d ; auto lam_d = [ = ] __device__ { return 1 ; }; // error: initialized with address of functor with // __device__ operator() function nvstd :: function fn2 = lam_d ; // error: passing nvstd::function from host to device kern >> ( fn2 ); } nvstd::function is defined in the nvfunctional header as follows: namespace nvstd { template class function { public : // constructors __device__ __host__ function () noexcept ; __device__ __host__ function ( nullptr_t ) noexcept ; __device__ __host__ function ( const function & ); __device__ __host__ function ( function && ); template __device__ __host__ function ( _F ); // destructor __device__ __host__ ~ function (); // assignment operators __device__ __host__ function & operator = ( const function & ); __device__ __host__ function & operator = ( function && ); __device__ __host__ function & operator = ( nullptr_t ); __device__ __host__ function & operator = ( _F && ); // swap __device__ __host__ void swap ( function & ) noexcept ; // function capacity __device__ __host__ explicit operator bool () const noexcept ; // function invocation __device__ _RetType operator ()( _ArgTypes ...) const ; }; // null pointer comparisons template __device__ __host__ bool operator == ( const function & , nullptr_t ) noexcept ; template __device__ __host__ bool operator == ( nullptr_t , const function & ) noexcept ; template __device__ __host__ bool operator != ( const function & , nullptr_t ) noexcept ; template __device__ __host__ bool operator != ( nullptr_t , const function & ) noexcept ; // specialized algorithms template __device__ __host__ void swap ( function & , function & ); } 14.7. Extended Lambdas \\uf0c1 The nvcc flag '--extended-lambda' allows explicit execution space annotations in a lambda expression 29 .\"},\n",
       " {'id': 342,\n",
       "  'content': 'The execution space annotations should be present after the ‘lambda-introducer’ and before the optional ‘lambda-declarator’. nvcc will define the macro __CUDACC_EXTENDED_LAMBDA__ when the \\'--extended-lambda\\' flag has been specified. An ‘extended __device__ lambda’ is a lambda expression that is annotated explicitly with ‘ __device__ ’, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__ function. An ‘extended __host__ __device__ lambda’ is a lambda expression that is annotated explicitly with both ‘ __host__ ’ and ‘ __device__ ’, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__ function. An ‘extended lambda’ denotes either an extended __device__ lambda or an extended __host__ __device__ lambda. Extended lambdas can be used in the type arguments of __global__ function template instantiation . If the execution space annotations are not explicitly specified, they are computed based on the scopes enclosing the closure class associated with the lambda, as described in the section on C++11 support. The execution space annotations are applied to all methods of the closure class associated with the lambda. Example: void foo_host ( void ) { // not an extended lambda: no explicit execution space annotations auto lam1 = [] { }; // extended __device__ lambda auto lam2 = [] __device__ { }; // extended __host__ __device__ lambda auto lam3 = [] __host__ __device__ { }; // not an extended lambda: explicitly annotated with only \\'__host__\\' auto lam4 = [] __host__ { }; } __host__ __device__ void foo_host_device ( void ) { // not an extended lambda: no explicit execution space annotations auto lam1 = [] { }; // extended __device__ lambda auto lam2 = [] __device__ { }; // extended __host__ __device__ lambda auto lam3 = [] __host__ __device__ { }; // not an extended lambda: explicitly annotated with only \\'__host__\\' auto lam4 = [] __host__ { }; } __device__ void foo_device ( void ) { // none of the lambdas within this function are extended lambdas, // because the enclosing function is not a __host__ or __host__ __device__ // function. auto lam1 = [] { }; auto lam2 = [] __device__ { }; auto lam3 = [] __host__ __device__ { }; auto lam4 = [] __host__ { }; } // lam1 and lam2 are not extended lambdas because they are not defined // within a __host__ or __host__ __device__ function. auto lam1 = [] { }; auto lam2 = [] __host__ __device__ { }; 14.7.1. Extended Lambda Type Traits \\uf0c1 The compiler provides type traits to detect closure types for extended lambdas at compile time: __nv_is_extended_device_lambda_closure_type(type) : If ‘type’ is the closure class created for an extended __device__ lambda, then the trait is true, otherwise it is false. __nv_is_extended_device_lambda_with_preserved_return_type(type) : If ‘type’ is the closure class created for an extended __device__ lambda and the lambda is defined with trailing return type (with restriction), then the trait is true, otherwise it is false. If the trailing return type definition refers to any lambda parameter name, the return type is not preserved. __nv_is_extended_host_device_lambda_closure_type(type) : If ‘type’ is the closure class created for an extended __host__ __device__ lambda, then the trait is true, otherwise it is false. These traits can be used in all compilation modes, irrespective of whether lambdas or extended lambdas are enabled 30 . Example: #define IS_D_LAMBDA(X) __nv_is_extended_device_lambda_closure_type(X) #define IS_DPRT_LAMBDA(X) __nv_is_extended_device_lambda_with_preserved_return_type(X) #define IS_HD_LAMBDA(X) __nv_is_extended_host_device_lambda_closure_type(X) auto lam0 = [] __host__ __device__ { }; void foo ( void ) { auto lam1 = [] { }; auto lam2 = [] __device__ { }; auto lam3 = [] __host__ __device__ { }; auto lam4 = [] __device__ () --> double { return 3.14 ; } auto lam5 = [] __device__ ( int x ) --> decltype ( & x ) { return 0 ; } // lam0 is not an extended lambda (since defined outside function scope) static_assert ( ! IS_D_LAMBDA ( decltype ( lam0 )), \"\" ); static_assert ( ! IS_DPRT_LAMBDA ( decltype ( lam0 )), \"\" ); static_assert ( ! IS_HD_LAMBDA ( decltype ( lam0 )), \"\" ); // lam1 is not an extended lambda (since no execution space annotations) static_assert ( ! IS_D_LAMBDA ( decltype ( lam1 )), \"\" ); static_assert ( ! IS_DPRT_LAMBDA ( decltype ( lam1 )), \"\" ); static_assert ( ! IS_HD_LAMBDA ( decltype ( lam1 )), \"\" ); // lam2 is an extended __device__ lambda static_assert ( IS_D_LAMBDA ( decltype ( lam2 )), \"\" ); static_assert ( ! IS_DPRT_LAMBDA ( decltype ( lam2 )), \"\" ); static_assert ( ! IS_HD_LAMBDA ( decltype ( lam2 )), \"\" ); // lam3 is an extended __host__ __device__ lambda static_assert ( ! IS_D_LAMBDA ( decltype ( lam3 )), \"\" ); static_assert ( ! IS_DPRT_LAMBDA ( decltype ( lam3 )), \"\" ); static_assert ( IS_HD_LAMBDA ( decltype ( lam3 )), \"\" ); // lam4 is an extended __device__ lambda with preserved return type static_assert ( IS_D_LAMBDA ( decltype ( lam4 )), \"\" ); static_assert ( IS_DPRT_LAMBDA ( decltype ( lam4 )), \"\" ); static_assert ( ! IS_HD_LAMBDA ( decltype ( lam4 )), \"\" ); // lam5 is not an extended __device__ lambda with preserved return type // because it references the operator()\\'s parameter types in the trailing return type. static_assert ( IS_D_LAMBDA ( decltype ( lam5 )), \"\" ); static_assert ( ! IS_DPRT_LAMBDA ( decltype ( lam5 )), \"\" ); static_assert ( ! IS_HD_LAMBDA ( decltype ( lam5 )), \"\" ); } 14.7.2. Extended Lambda Restrictions \\uf0c1 The CUDA compiler will replace an extended lambda expression with an instance of a placeholder type defined in namespace scope, before invoking the host compiler. The template argument of the placeholder type requires taking the address of a function enclosing the original extended lambda expression. This is required for the correct execution of any __global__ function template whose template argument involves the closure type of an extended lambda. The enclosing function is computed as follows. By definition, the extended lambda is present within the immediate or nested block scope of a __host__ or __host__ __device__ function. If this function is not the operator() of a lambda expression, then it is considered the enclosing function for the extended lambda. Otherwise, the extended lambda is defined within the immediate or nested block scope of the operator() of one or more enclosing lambda expressions. If the outermost such lambda expression is defined in the immediate or nested block scope of a function F , then F is the computed enclosing function, else the enclosing function does not exist. Example: void foo ( void ) { // enclosing function for lam1 is \"foo\" auto lam1 = [] __device__ { }; auto lam2 = [] { auto lam3 = [] { // enclosing function for lam4 is \"foo\" auto lam4 = [] __host__ __device__ { }; }; }; } auto lam6 = [] { // enclosing function for lam7 does not exist auto lam7 = [] __host__ __device__ { }; }; Here are the restrictions on extended lambdas: An extended lambda cannot be defined inside another extended lambda expression. Example: void foo ( void ) { auto lam1 = [] __host__ __device__ { // error: extended lambda defined within another extended lambda auto lam2 = [] __host__ __device__ { }; }; } An extended lambda cannot be defined inside a generic lambda expression. Example: void foo ( void ) { auto lam1 = [] ( auto ) { // error: extended lambda defined within a generic lambda auto lam2 = [] __host__ __device__ { }; }; } If an extended lambda is defined within the immediate or nested block scope of one or more nested lambda expression, the outermost such lambda expression must be defined inside the immediate or nested block scope of a function. Example: auto lam1 = [] { // error: outer enclosing lambda is not defined within a // non-lambda-operator() function. auto lam2 = [] __host__ __device__ { }; }; The enclosing function for the extended lambda must be named and its address can be taken. If the enclosing function is a class member, then the following conditions must be satisfied: All classes enclosing the member function must have a name.'},\n",
       " {'id': 343,\n",
       "  'content': \"The member function must not have private or protected access within its parent class. All enclosing classes must not have private or protected access within their respective parent classes. Example: void foo ( void ) { // OK auto lam1 = [] __device__ { return 0 ; }; { // OK auto lam2 = [] __device__ { return 0 ; }; // OK auto lam3 = [] __device__ __host__ { return 0 ; }; } } struct S1_t { S1_t ( void ) { // Error: cannot take address of enclosing function auto lam4 = [] __device__ { return 0 ; }; } }; class C0_t { void foo ( void ) { // Error: enclosing function has private access in parent class auto temp1 = [] __device__ { return 10 ; }; } struct S2_t { void foo ( void ) { // Error: enclosing class S2_t has private access in its // parent class auto temp1 = [] __device__ { return 10 ; }; } }; }; It must be possible to take the address of the enclosing routine unambiguously, at the point where the extended lambda has been defined. This may not be feasible in some cases e.g. when a class typedef shadows a template type argument of the same name. Example: template struct A { typedef void Bar ; void test (); }; template <> struct A { }; template void A :: test () { /* In code sent to host compiler, nvcc will inject an address expression here, of the form: (void (A ::*)(void))(&A::test)) However, the class typedef 'Bar' (to void) shadows the template argument 'Bar', causing the address expression in A::test to actually refer to: (void (A ::*)(void))(&A::test)) ..which doesn't take the address of the enclosing routine 'A::test' correctly. */ auto lam1 = [] __host__ __device__ { return 4 ; }; } int main () { A xxx ; xxx . test (); } An extended lambda cannot be defined in a class that is local to a function. Example: void foo ( void ) { struct S1_t { void bar ( void ) { // Error: bar is member of a class that is local to a function. auto lam4 = [] __host__ __device__ { return 0 ; }; } }; } The enclosing function for an extended lambda cannot have deduced return type. Example: auto foo ( void ) { // Error: the return type of foo is deduced. auto lam1 = [] __host__ __device__ { return 0 ; }; } __host__ __device__ extended lambdas cannot be generic lambdas. Example: void foo ( void ) { // Error: __host__ __device__ extended lambdas cannot be // generic lambdas. auto lam1 = [] __host__ __device__ ( auto i ) { return i ; }; // Error: __host__ __device__ extended lambdas cannot be // generic lambdas. auto lam2 = [] __host__ __device__ ( auto ... i ) { return sizeof ...( i ); }; } If the enclosing function is an instantiation of a function template or a member function template, and/or the function is a member of a class template, the template(s) must satisfy the following constraints: The template must have at most one variadic parameter, and it must be listed last in the template parameter list. The template parameters must be named. The template instantiation argument types cannot involve types that are either local to a function (except for closure types for extended lambdas), or are private or protected class members. Example: template __global__ void kern ( T in ) { in (); } template struct foo {}; template class T , typename ... P1 , typename ... P2 > void bar1 ( const T , const T ) { // Error: enclosing function has multiple parameter packs auto lam1 = [] __device__ { return 10 ; }; } template class T , typename ... P1 , typename T2 > void bar2 ( const T , T2 ) { // Error: for enclosing function, the // parameter pack is not last in the template parameter list. auto lam1 = [] __device__ { return 10 ; }; } template void bar3 ( void ) { // Error: for enclosing function, the second template // parameter is not named. auto lam1 = [] __device__ { return 10 ; }; } int main () { foo f1 ; foo f2 ; bar1 ( f1 , f2 ); bar2 ( f1 , 10 ); bar3 (); } Example: template __global__ void kern ( T in ) { in (); } template void bar4 ( void ) { auto lam1 = [] __device__ { return 10 ; }; kern >> ( lam1 ); } struct C1_t { struct S1_t { }; friend int main ( void ); }; int main () { struct S1_t { }; // Error: enclosing function for device lambda in bar4 // is instantiated with a type local to main. bar4 (); // Error: enclosing function for device lambda in bar4 // is instantiated with a type that is a private member // of a class. bar4 (); } With Visual Studio host compilers, the enclosing function must have external linkage. The restriction is present because this host compiler does not support using the address of non-extern linkage functions as template arguments, which is needed by the CUDA compiler transformations to support extended lambdas. With Visual Studio host compilers, an extended lambda shall not be defined within the body of an ‘if-constexpr’ block. An extended lambda has the following restrictions on captured variables: In the code sent to the host compiler, the variable may be passed by value to a sequence of helper functions before being used to direct-initialize the field of the class type used to represent the closure type for the extended lambda 31 . A variable can only be captured by value. A variable of array type cannot be captured if the number of array dimensions is greater than 7. For a variable of array type, in the code sent to the host compiler, the closure type’s array field is first default-initialized, and then each element of the array field is copy-assigned from the corresponding element of the captured array variable. Therefore, the array element type must be default-constructible and copy-assignable in host code. A function parameter that is an element of a variadic argument pack cannot be captured. The type of the captured variable cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members. For a __host__ __device__ extended lambda, the types used in the return or parameter types of the lambda expression’s operator() cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members. Init-capture is not supported for __host__ __device__ extended lambdas. Init-capture is supported for __device__ extended lambdas, except when the init-capture is of array type or of type std::initializer_list . The function call operator for an extended lambda is not constexpr. The closure type for an extended lambda is not a literal type. The constexpr and consteval specifier cannot be used in the declaration of an extended lambda. A variable cannot be implicitly captured inside an if-constexpr block lexically nested inside an extended lambda, unless it has already been implicitly captured earlier outside the if-constexpr block or appears in the explicit capture list for the extended lambda (see example below). Example void foo ( void ) { // OK: an init-capture is allowed for an // extended __device__ lambda. auto lam1 = [ x = 1 ] __device__ () { return x ; }; // Error: an init-capture is not allowed for // an extended __host__ __device__ lambda. auto lam2 = [ x = 1 ] __host__ __device__ () { return x ; }; int a = 1 ; // Error: an extended __device__ lambda cannot capture // variables by reference. auto lam3 = [ & a ] __device__ () { return a ; }; // Error: by-reference capture is not allowed // for an extended __device__ lambda. auto lam4 = [ & x = a ] __device__ () { return x ; }; struct S1_t { }; S1_t s1 ; // Error: a type local to a function cannot be used in the type // of a captured variable. auto lam6 = [ s1 ] __device__ () { }; // Error: an init-capture cannot be of type std::initializer_list. auto lam7 = [ x = { 11 }] __device__ () { }; std :: initializer_list b = { 11 , 22 , 33 }; // Error: an init-capture cannot be of type std::initializer_list. auto lam8 = [ x = b ] __device__ () { }; // Error scenario (lam9) and supported scenarios (lam10, lam11) // for capture within 'if-constexpr' block int yyy = 4 ; auto lam9 = [ = ] __device__ { int result = 0 ; if constexpr ( false ) { //Error: An extended __device__ lambda cannot first-capture // 'yyy' in constexpr-if context result += yyy ; } return result ; }; auto lam10 = [ yyy ] __device__ { int result = 0 ; if constexpr ( false ) { //OK: 'yyy' already listed in explicit capture list for the extended lambda result += yyy ; } return result ; }; auto lam11 = [ = ] __device__ { int result = yyy ; if constexpr ( false ) { //OK: 'yyy' already implicit captured outside the 'if-constexpr' block result += yyy ; } return result ; }; } When parsing a function, the CUDA compiler assigns a counter value to each extended lambda within that function. This counter value is used in the substituted named type passed to the host compiler.\"},\n",
       " {'id': 344,\n",
       "  'content': 'Hence, whether or not an extended lambda is defined within a function should not depend on a particular value of __CUDA_ARCH__ , or on __CUDA_ARCH__ being undefined. Example template __global__ void kernel ( T in ) { in (); } __host__ __device__ void foo ( void ) { // Error: the number and relative declaration // order of extended lambdas depends on // __CUDA_ARCH__ #if defined(__CUDA_ARCH__) auto lam1 = [] __device__ { return 0 ; }; auto lam1b = [] __host___ __device__ { return 10 ; }; #endif auto lam2 = [] __device__ { return 4 ; }; kernel >> ( lam2 ); } As described above, the CUDA compiler replaces a __device__ extended lambda defined in a host function with a placeholder type defined in namespace scope. Unless the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true for the closure type of the extended lambda, the placeholder type does not define a operator() function equivalent to the original lambda declaration. An attempt to determine the return type or parameter types of the operator() function of such a lambda may therefore work incorrectly in host code, as the code processed by the host compiler will be semantically different than the input code processed by the CUDA compiler. However, it is OK to introspect the return type or parameter types of the operator() function within device code. Note that this restriction does not apply to __host__ __device__ extended lambdas, or to __device__ extended lambdas for which the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true. Example #include const char & getRef ( const char * p ) { return * p ; } void foo ( void ) { auto lam1 = [] __device__ { return \"10\" ; }; // Error: attempt to extract the return type // of a __device__ lambda in host code std :: result_of :: type xx1 = \"abc\" ; auto lam2 = [] __host__ __device__ { return \"10\" ; }; // OK : lam2 represents a __host__ __device__ extended lambda std :: result_of :: type xx2 = \"abc\" ; auto lam3 = [] __device__ () -> const char * { return \"10\" ; }; // OK : lam3 represents a __device__ extended lambda with preserved return type std :: result_of :: type xx2 = \"abc\" ; static_assert ( std :: is_same_v :: type , const char *> ); auto lam4 = [] __device__ ( char x ) -> decltype ( getRef ( & x )) { return 0 ; }; // lam4\\'s return type is not preserved because it references the operator()\\'s // parameter types in the trailing return type. static_assert ( ! __nv_is_extended_device_lambda_with_preserved_return_type ( decltype ( lam4 )), \"\" ); } For an extended device lambda: - Introspecting the parameter type of operator() is only supported in device code. - Introspecting the return type of operator() is supported only in device code, unless the trait function __nv_is_extended_device_lambda_with_preserved_return_type() returns true. If the functor object represented by an extended lambda is passed from host to device code (e.g., as the argument of a __global__ function), then any expression in the body of the lambda expression that captures variables must be remain unchanged irrespective of whether the __CUDA_ARCH__ macro is defined, and whether the macro has a particular value. This restriction arises because the lambda’s closure class layout depends on the order in which captured variables are encountered when the compiler processes the lambda expression; the program may execute incorrectly if the closure class layout differs in device and host compilation. Example __device__ int result ; template __global__ void kernel ( T in ) { result = in (); } void foo ( void ) { int x1 = 1 ; auto lam1 = [ = ] __host__ __device__ { // Error: \"x1\" is only captured when __CUDA_ARCH__ is defined. #ifdef __CUDA_ARCH__ return x1 + 1 ; #else return 10 ; #endif }; kernel >> ( lam1 ); } As described previously, the CUDA compiler replaces an extended __device__ lambda expression with an instance of a placeholder type in the code sent to the host compiler. This placeholder type does not define a pointer-to-function conversion operator in host code, however the conversion operator is provided in device code. Note that this restriction does not apply to __host__ __device__ extended lambdas. Example template __global__ void kern ( T in ) { int ( * fp )( double ) = in ; // OK: conversion in device code is supported fp ( 0 ); auto lam1 = []( double ) { return 1 ; }; // OK: conversion in device code is supported fp = lam1 ; fp ( 0 ); } void foo ( void ) { auto lam_d = [] __device__ ( double ) { return 1 ; }; auto lam_hd = [] __host__ __device__ ( double ) { return 1 ; }; kern >> ( lam_d ); kern >> ( lam_hd ); // OK : conversion for __host__ __device__ lambda is supported // in host code int ( * fp )( double ) = lam_hd ; // Error: conversion for __device__ lambda is not supported in // host code. int ( * fp2 )( double ) = lam_d ; } As described previously, the CUDA compiler replaces an extended __device__ or __host__ __device__ lambda expression with an instance of a placeholder type in the code sent to the host compiler. This placeholder type may define C++ special member functions (e.g. constructor, destructor). As a result, some standard C++ type traits may return different results for the closure type of the extended lambda, in the CUDA frontend compiler versus the host compiler. The following type traits are affected: std::is_trivially_copyable , std::is_trivially_constructible , std::is_trivially_copy_constructible , std::is_trivially_move_constructible , std::is_trivially_destructible . Care must be taken that the results of these type traits are not used in __global__ function template instantiation or in __device__ / __constant__ / __managed__ variable template instantiation. Example template void __global__ foo () { printf ( \"hi\" ); } template void dolaunch () { // ERROR: this kernel launch may fail, because CUDA frontend compiler // and host compiler may disagree on the result of // std::is_trivially_copyable() trait on the closure type of the // extended lambda foo :: value >>> (); cudaDeviceSynchronize (); } int main () { int x = 0 ; auto lam1 = [ = ] __host__ __device__ () { return x ; }; dolaunch (); } The CUDA compiler will generate compiler diagnostics for a subset of cases described in 1-12; no diagnostic will be generated for cases 13-17, but the host compiler may fail to compile the generated code. 14.7.3. Notes on __host__ __device__ lambdas \\uf0c1 Unlike __device__ lambdas, __host__ __device__ lambdas can be called from host code. As described earlier, the CUDA compiler replaces an extended lambda expression defined in host code with an instance of a named placeholder type. The placeholder type for an extended __host__ __device__ lambda invokes the original lambda’s operator() with an indirect function call 30 . The presence of the indirect function call may cause an extended __host__ __device__ lambda to be less optimized by the host compiler than lambdas that are implicitly or explicitly __host__ only. In the latter case, the host compiler can easily inline the body of the lambda into the calling context. But in case of an extended __host__ __device__ lambda, the host compiler encounters the indirect function call and may not be able to easily inline the original __host__ __device__ lambda body. 14.7.4. *this Capture By Value \\uf0c1 When a lambda is defined within a non-static class member function, and the body of the lambda refers to a class member variable, C++11/C++14 rules require that the this pointer of the class is captured by value, instead of the referenced member variable. If the lambda is an extended __device__ or __host__ __device__ lambda defined in a host function, and the lambda is executed on the GPU, accessing the referenced member variable on the GPU will cause a run time error if the this pointer points to host memory. Example: #include template __global__ void foo ( T in ) { printf ( \" \\\\n value = %d\" , in ()); } struct S1_t { int xxx ; __host__ __device__ S1_t ( void ) : xxx ( 10 ) { }; void doit ( void ) { auto lam1 = [ = ] __device__ { // reference to \"xxx\" causes // the \\'this\\' pointer (S1_t*) to be captured by value return xxx + 1 ; }; // Kernel launch fails at run time because \\'this->xxx\\' // is not accessible from the GPU foo >> ( lam1 ); cudaDeviceSynchronize (); } }; int main ( void ) { S1_t s1 ; s1 . doit (); } C++17 solves this problem by adding a new “*this” capture mode. In this mode, the compiler makes a copy of the object denoted by “*this” instead of capturing the pointer this by value. The “*this” capture mode is described in more detail here: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html . The CUDA compiler supports the “*this” capture mode for lambdas defined within __device__ and __global__ functions and for extended __device__ lambdas defined in host code, when the --extended-lambda nvcc flag is used. Here’s the above example modified to use “*this” capture mode: #include template __global__ void foo ( T in ) { printf ( \" \\\\n value = %d\" , in ()); } struct S1_t { int xxx ; __host__ __device__ S1_t ( void ) : xxx ( 10 ) { }; void doit ( void ) { // note the \"*this\" capture specification auto lam1 = [ = , * this ] __device__ { // reference to \"xxx\" causes // the object denoted by \\'*this\\' to be captured by // value, and the GPU code will access copy_of_star_this->xxx return xxx + 1 ; }; // Kernel launch succeeds foo >> ( lam1 ); cudaDeviceSynchronize (); } }; int main ( void ) { S1_t s1 ; s1 . doit (); } “*this” capture mode is not allowed for unannotated lambdas defined in host code, or for extended __host__ __device__ lambdas. Examples of supported and unsupported usage: struct S1_t { int xxx ; __host__ __device__ S1_t ( void ) : xxx ( 10 ) { }; void host_func ( void ) { // OK: use in an extended __device__ lambda auto lam1 = [ = , * this ] __device__ { return xxx ; }; // Error: use in an extended __host__ __device__ lambda auto lam2 = [ = , * this ] __host__ __device__ { return xxx ; }; // Error: use in an unannotated lambda in host function auto lam3 = [ = , * this ] { return xxx ; }; } __device__ void device_func ( void ) { // OK: use in a lambda defined in a __device__ function auto lam1 = [ = , * this ] __device__ { return xxx ; }; // OK: use in a lambda defined in a __device__ function auto lam2 = [ = , * this ] __host__ __device__ { return xxx ; }; // OK: use in a lambda defined in a __device__ function auto lam3 = [ = , * this ] { return xxx ; }; } __host__ __device__ void host_device_func ( void ) { // OK: use in an extended __device__ lambda auto lam1 = [ = , * this ] __device__ { return xxx ; }; // Error: use in an extended __host__ __device__ lambda auto lam2 = [ = , * this ] __host__ __device__ { return xxx ; }; // Error: use in an unannotated lambda in a __host__ __device__ function auto lam3 = [ = , * this ] { return xxx ; }; } }; 14.7.5. Additional Notes \\uf0c1 ADL Lookup : As described earlier, the CUDA compiler will replace an extended lambda expression with an instance of a placeholder type, before invoking the host compiler.'},\n",
       " {'id': 345,\n",
       "  'content': 'One template argument of the placeholder type uses the address of the function enclosing the original lambda expression. This may cause additional namespaces to participate in argument dependent lookup (ADL), for any host function call whose argument types involve the closure type of the extended lambda expression. This may cause an incorrect function to be selected by the host compiler. Example: namespace N1 { struct S1_t { }; template void foo ( T ); }; namespace N2 { template int foo ( T ); template void doit ( T in ) { foo ( in ); } } void bar ( N1 :: S1_t in ) { /* extended __device__ lambda. In the code sent to the host compiler, this is replaced with the placeholder type instantiation expression \\' __nv_dl_wrapper_t > { }\\' As a result, the namespace \\'N1\\' participates in ADL lookup of the call to \"foo\" in the body of N2::doit, causing ambiguity. */ auto lam1 = [ = ] __device__ { }; N2 :: doit ( lam1 ); } In the example above, the CUDA compiler replaced the extended lambda with a placeholder type that involves the N1 namespace. As a result, the namespace N1 participates in the ADL lookup for foo(in) in the body of N2::doit , and host compilation fails because multiple overload candidates N1::foo and N2::foo are found. 14.8.'},\n",
       " {'id': 346,\n",
       "  'content': 'Code Samples \\uf0c1 14.8.1. Data Aggregation Class \\uf0c1 class PixelRGBA { public : __device__ PixelRGBA () : r_ ( 0 ), g_ ( 0 ), b_ ( 0 ), a_ ( 0 ) { } __device__ PixelRGBA ( unsigned char r , unsigned char g , unsigned char b , unsigned char a = 255 ) : r_ ( r ), g_ ( g ), b_ ( b ), a_ ( a ) { } private : unsigned char r_ , g_ , b_ , a_ ; friend PixelRGBA operator + ( const PixelRGBA & , const PixelRGBA & ); }; __device__ PixelRGBA operator + ( const PixelRGBA & p1 , const PixelRGBA & p2 ) { return PixelRGBA ( p1 . r_ + p2 .'},\n",
       " {'id': 347, 'content': 'r_ , p1 . g_ + p2 . g_ , p1 . b_ + p2 . b_ , p1 .'},\n",
       " {'id': 348,\n",
       "  'content': 'a_ + p2 . a_ ); } __device__ void func ( void ) { PixelRGBA p1 , p2 ; // ... // Initialization of p1 and p2 here PixelRGBA p3 = p1 + p2 ; } 14.8.2. Derived Class \\uf0c1 __device__ void * operator new ( size_t bytes , MemoryPool & p ); __device__ void operator delete ( void * , MemoryPool & p ); class Shape { public : __device__ Shape ( void ) { } __device__ void putThis ( PrintBuffer * p ) const ; __device__ virtual void Draw ( PrintBuffer * p ) const { p -> put ( \"Shapeless\" ); } __device__ virtual ~ Shape () {} }; class Point : public Shape { public : __device__ Point () : x ( 0 ), y ( 0 ) {} __device__ Point ( int ix , int iy ) : x ( ix ), y ( iy ) { } __device__ void PutCoord ( PrintBuffer * p ) const ; __device__ void Draw ( PrintBuffer * p ) const ; __device__ ~ Point () {} private : int x , y ; }; __device__ Shape * GetPointObj ( MemoryPool & pool ) { Shape * shape = new ( pool ) Point ( rand ( -20 , 10 ), rand ( -100 , -20 )); return shape ; } 14.8.3. Class Template \\uf0c1 template class myValues { T values [ MAX_VALUES ]; public : __device__ myValues ( T clear ) { ...'},\n",
       " {'id': 349,\n",
       "  'content': '} __device__ void setValue ( int Idx , T value ) { ... } __device__ void putToMemory ( T * valueLocation ) { ... } }; template void __global__ useValues ( T * memoryBuffer ) { myValues myLocation ( 0 ); ... } __device__ void * buffer ; int main () { ... useValues >> ( buffer ); ... } 14.8.4. Function Template \\uf0c1 template __device__ bool func ( T x ) { ... return (...); } template <> __device__ bool func ( T x ) // Specialization { return true ; } // Explicit argument specification bool result = func ( 0.5 ); // Implicit argument deduction int x = 1 ; bool result = func ( x ); 14.8.5. Functor Class \\uf0c1 class Add { public : __device__ float operator () ( float a , float b ) const { return a + b ; } }; class Sub { public : __device__ float operator () ( float a , float b ) const { return a - b ; } }; // Device code template __global__ void VectorOperation ( const float * A , const float * B , float * C , unsigned int N , O op ) { unsigned int iElement = blockDim . x ; if ( iElement >> ( v1 , v2 , v3 , N , Add ()); ...'},\n",
       " {'id': 350,\n",
       "  'content': '} 15 e.g., the >> syntax for launching kernels. 16 This does not apply to entities that may be defined in more than one translation unit, such as compiler generated template instantiations. 17 The intent is to allow variable memory space specifiers for static variables in a __host__ __device__ function during device compilation, but disallow it during host compilation 18 One way to debug suspected layout mismatch of a type C is to use printf to output the values of sizeof(C) and offsetof(C, field) in host and device code. 19 Note that this may negatively impact compile time due to presence of extra declarations. 20 At present, the -std=c++11 flag is supported only for the following host compilers : gcc version >= 4.7, clang, icc >= 15, and xlc >= 13.1 21 including operator() 22 The restrictions are the same as with a non-constexpr callee function. 23 Note that the behavior of experimental flags may change in future compiler releases. 24 C++ Standard Section [basic.types] 25 C++ Standard Section [expr.const] 26 At present, the -std=c++14 flag is supported only for the following host compilers : gcc version >= 5.1, clang version >= 3.7 and icc version >= 17 27 At present, the -std=c++17 flag is supported only for the following host compilers : gcc version >= 7.0, clang version >= 8.0, Visual Studio version >= 2017, pgi compiler version >= 19.0, icc compiler version >= 19.0 28 At present, the -std=c++20 flag is supported only for the following host compilers : gcc version >= 10.0, clang version >= 10.0, Visual Studio Version >= 2022 and nvc++ version >= 20.7. 29 When using the icc host compiler, this flag is only supported for icc >= 1800. 30 ( 1 , 2 ) The traits will always return false if extended lambda mode is not active. 31 In contrast, the C++ standard specifies that the captured variable is used to direct-initialize the field of the closure type.'},\n",
       " {'id': 351,\n",
       "  'content': '15. Texture Fetching \\uf0c1 This section gives the formula used to compute the value returned by the texture functions of Texture Functions depending on the various attributes of the texture object (see Texture and Surface Memory ). The texture bound to the texture object is represented as an array T of N texels for a one-dimensional texture, N x M texels for a two-dimensional texture, N x M x L texels for a three-dimensional texture. It is fetched using non-normalized texture coordinates x , y , and z , or the normalized texture coordinates x/N , y/M , and z/L as described in Texture Memory . In this section, the coordinates are assumed to be in the valid range. Texture Memory explained how out-of-range coordinates are remapped to the valid range based on the addressing mode. 15.1. Nearest-Point Sampling \\uf0c1 In this filtering mode, the value returned by the texture fetch is tex(x)=T[i] for a one-dimensional texture, tex(x,y)=T[i,j] for a two-dimensional texture, tex(x,y,z)=T[i,j,k] for a three-dimensional texture, where i=floor(x) , j=floor(y) , and k=floor(z) . FIgure 32 illustrates nearest-point sampling for a one-dimensional texture with N=4 . Figure 32 Nearest-Point Sampling Filtering Mode \\uf0c1 For integer textures, the value returned by the texture fetch can be optionally remapped to [0.0, 1.0] (see Texture Memory ). 15.2. Linear Filtering \\uf0c1 In this filtering mode, which is only available for floating-point textures, the value returned by the texture fetch is \\\\(tex(x)=(1-\\\\alpha)T[i]+{\\\\alpha}T[i+1]\\\\) for a one-dimensional texture, \\\\(tex(x)=(1-\\\\alpha)T[i]+{\\\\alpha}T[i+1]\\\\) for a one-dimensional texture, \\\\(tex(x,y)=(1-\\\\alpha)(1-\\\\beta)T[i,j]+\\\\alpha(1-\\\\beta)T[i+1,j]+(1-\\\\alpha){\\\\beta}T[i,j+1]+\\\\alpha{\\\\beta}T[i+1,j+1]\\\\) for a two-dimensional texture, \\\\(tex(x,y,z)\\\\) = \\\\((1-\\\\alpha)(1-\\\\beta)(1-\\\\gamma)T[i,j,k]+\\\\alpha(1-\\\\beta)(1-\\\\gamma)T[i+1,j,k]+\\\\) \\\\((1-\\\\alpha)\\\\beta(1-\\\\gamma)T[i,j+1,k]+\\\\alpha\\\\beta(1-\\\\gamma)T[i+1,j+1,k]+\\\\) \\\\((1-\\\\alpha)(1-\\\\beta){\\\\gamma}T[i,j,k+1]+\\\\alpha(1-\\\\beta){\\\\gamma}T[i+1,j,k+1]+\\\\) \\\\((1-\\\\alpha)\\\\beta{\\\\gamma}T[i,j+1,k+1]+\\\\alpha\\\\beta{\\\\gamma}T[i+1,j+1,k+1]\\\\) for a three-dimensional texture, where: \\\\(i=floor(x\\\\ B)*, \\\\alpha=frac(x\\\\ B)*, *x\\\\ B\\\\ =x-0.5,\\\\) \\\\(j=floor(y\\\\ B)*, \\\\beta=frac(y\\\\ B)*, *y\\\\ B\\\\ =y-0.5,\\\\) \\\\(k=floor(z\\\\ B)*, \\\\gamma=frac(z\\\\ B)*, *z\\\\ B\\\\ = z-0.5,\\\\) \\\\(\\\\alpha\\\\) , \\\\(\\\\beta\\\\) , and \\\\(\\\\gamma\\\\) are stored in 9-bit fixed point format with 8 bits of fractional value (so 1.0 is exactly represented). Figure 33 illustrates linear filtering of a one-dimensional texture with N=4 . Figure 33 Linear Filtering Mode \\uf0c1 15.3. Table Lookup \\uf0c1 A table lookup TL(x) where x spans the interval [0,R] can be implemented as TL(x)=tex((N-1)/R)x+0.5) in order to ensure that TL(0)=T[0] and TL(R)=T[N-1] . Figure 34 illustrates the use of texture filtering to implement a table lookup with R=4 or R=1 from a one-dimensional texture with N=4 . Figure 34 One-Dimensional Table Lookup Using Linear Filtering \\uf0c1 16. Compute Capabilities \\uf0c1 The general specifications and features of a compute device depend on its compute capability (see Compute Capability ). Table 20 and Table 21 show the features and technical specifications associated with each compute capability that is currently supported. Floating-Point Standard reviews the compliance with the IEEE floating-point standard. Sections Compute Capability 5.x , Compute Capability 6.x , Compute Capability 7.x , Compute Capability 8.x and Compute Capability 9.0 give more details on the architecture of devices of compute capabilities 5.x, 6.x, 7.x, 8.x and 9.0 respectively. 16.1. Feature Availability \\uf0c1 A compute feature is introduced with a compute architecture with the intention that the feature will be available on all subsequent architectures. This is shown in Table 20 by the “yes” for availability of a feature on compute capabilities subsequent to its introduction. Highly specialized compute features that are introduced with an architecture may not be guaranteed to be available on all subsequent compute capabilities. These features target acceleration of specialized operations which are not intended for all classes of compute capabilities (denoted by the compute capability’s minor number) or are likely to significantly change on future generations (denoted by the compute capability’s major number). There are potentially two sets of compute features for a given compute capability: Compute Capability #.# : The predominant set of compute features that are introduced with the intent to be available for subsequent compute architectures. These features and their availability are summarized in Table 20. Compute Capability #.#a : A small and highly specialized set of features that are introduced to accelerate specialized operations, which are not guaranteed to be available or might change significantly on subsequent compute architecture. These features are summarized in the respective “Compute Capability #.#”” subsection. Compilation of device code targets a particular compute capability. A feature which appears in device code must be available for the targeted compute capability. For example: The compute_90 compilation target allows use of Compute Capability 9.0 features but does not allow use of Compute Capability 9.0a features. The compute_90a compilation target allows use of the complete set of compute device features, both 9.0a features and 9.0 features. 16.2. Features and Technical Specifications \\uf0c1 Table 20 Feature Support per Compute Capability \\uf0c1 Feature Support Compute Capability (Unlisted features are supported for all compute capabilities) 5.0, 5.2 5.3 6.x 7.x 8.x 9.0 Atomic functions operating on 32-bit integer values in global memory ( Atomic Functions ) Yes Atomic functions operating on 32-bit integer values in shared memory ( Atomic Functions ) Yes Atomic functions operating on 64-bit integer values in global memory ( Atomic Functions ) Yes Atomic functions operating on 64-bit integer values in shared memory ( Atomic Functions ) Yes Atomic functions operating on 128-bit integer values in global memory ( Atomic Functions ) No Yes Atomic functions operating on 128-bit integer values in shared memory ( Atomic Functions ) No Yes Atomic addition operating on 32-bit floating point values in global and shared memory ( atomicAdd() ) Yes Atomic addition operating on 64-bit floating point values in global memory and shared memory ( atomicAdd() ) No Yes Atomic addition operating on float2 and float4 floating point vectors in global memory ( atomicAdd() ) No Yes Warp vote functions ( Warp Vote Functions ) Yes Memory fence functions ( Memory Fence Functions ) Yes Synchronization functions ( Synchronization Functions ) Yes Surface functions ( Surface Functions ) Yes Unified Memory Programming ( Unified Memory Programming ) Yes Dynamic Parallelism ( CUDA Dynamic Parallelism ) Yes Half-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion No Yes Bfloat16-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion No Yes Tensor Cores No Yes Mixed Precision Warp-Matrix Functions ( Warp matrix functions ) No Yes Hardware-accelerated memcpy_async ( Asynchronous Data Copies using cuda::pipeline ) No Yes Hardware-accelerated Split Arrive/Wait Barrier ( Asynchronous Barrier ) No Yes L2 Cache Residency Management ( Device Memory L2 Access Management ) No Yes DPX Instructions for Accelerated Dynamic Programming No Yes Distributed Shared Memory No Yes Thread Block Cluster No Yes Tensor Memory Accelerator (TMA) unit No Yes Note that the KB and K units used in the following table correspond to 1024 bytes (i.e., a KiB) and 1024 respectively. Table 21 Technical Specifications per Compute Capability \\uf0c1 Compute Capability Technical Specifications 5.0 5.2 5.3 6.0 6.1 6.2 7.0 7.2 7.5 8.0 8.6 8.7 8.9 9.0 Maximum number of resident grids per device (Concurrent Kernel Execution) 32 16 128 32 16 128 16 128 Maximum dimensionality of grid of thread blocks 3 Maximum x -dimension of a grid of thread blocks [thread blocks] 2 31 -1 Maximum y- or z-dimension of a grid of thread blocks 65535 Maximum dimensionality of thread block 3 Maximum x- or y-dimensionality of a block 1024 Maximum z-dimension of a block 64 Maximum number of threads per block 1024 Warp size 32 Maximum number of resident blocks per SM 32 16 32 16 24 32 Maximum number of resident warps per SM 64 32 64 48 64 Maximum number of resident threads per SM 2048 1024 2048 1536 2048 Number of 32-bit registers per SM 64 K Maximum number of 32-bit registers per thread block 64 K 32 K 64 K 32 K 64 K Maximum number of 32-bit registers per thread 255 Maximum amount of shared memory per SM 64 KB 96 KB 64 KB 96 KB 64 KB 96 KB 64 KB 164 KB 100 KB 164 KB 100 KB 228 KB Maximum amount of shared memory per thread block 32 48 KB 96 KB 96 KB 64 KB 163 KB 99 KB 163 KB 99 KB 227 KB Number of shared memory banks 32 Maximum amount of local memory per thread 512 KB Constant memory size 64 KB Cache working set per SM for constant memory 8 KB 4 KB 8 KB Cache working set per SM for texture memory Between 12 KB and 48 KB Between 24 KB and 48 KB 32 ~ 128 KB 32 or 64 KB 28 KB ~ 192 KB 28 KB ~ 128 KB 28 KB ~ 192 KB 28 KB ~ 128 KB 28 KB ~ 256 KB Maximum width for a 1D texture object using a CUDA array 65536 131072 Maximum width for a 1D texture object using linear memory 2 27 2 28 2 27 2 28 2 27 2 28 Maximum width and number of layers for a 1D layered texture object 16384 x 2048 32768 x 2048 Maximum width and height for a 2D texture object using a CUDA array 65536 x 65536 131072 x 65536 Maximum width and height for a 2D texture object using linear memory 65536 x 65536 131072 x 65000 Maximum width and height for a 2D texture object using a CUDA array supporting texture gather 16384 x 16384 32768 x 32768 Maximum width, height, and number of layers for a 2D layered texture object 16384 x 16384 x 2048 32768 x 32768 x 2048 Maximum width, height, and depth for a 3D texture object using to a CUDA array 4096 x 4096 x 4096 16384 x 16384 x 16384 Maximum width (and height) for a cubemap texture object 16384 32768 Maximum width (and height) and number of layers for a cubemap layered texture object 16384 x 2046 32768 x 2046 Maximum number of textures that can be bound to a kernel 256 Maximum width for a 1D surface object using a CUDA array 16384 32768 Maximum width and number of layers for a 1D layered surface object 16384 x 2048 32768 x 2048 Maximum width and height for a 2D surface object using a CUDA array 65536 x 65536 1 31072 x 65536 Maximum width, height, and number of layers for a 2D layered surface object 16384 x 16384 x 2048 32768 x 32768 x 1048 Maximum width, height, and depth for a 3D surface object using a CUDA array 4096 x 4096 x 4096 16384 x 16384 x 16384 Maximum width (and height) for a cubemap surface object using a CUDA array 16384 32768 Maximum width (and height) and number of layers for a cubemap layered surface object 16384 x 2046 32768 x 2046 Maximum number of surfaces that can use a kernel 16 32 16.3. Floating-Point Standard \\uf0c1 All compute devices follow the IEEE 754-2008 standard for binary floating-point arithmetic with the following deviations: There is no dynamically configurable rounding mode; however, most of the operations support multiple IEEE rounding modes, exposed via device intrinsics.'},\n",
       " {'id': 352,\n",
       "  'content': 'There is no mechanism for detecting that a floating-point exception has occurred and all operations behave as if the IEEE-754 exceptions are always masked, and deliver the masked response as defined by IEEE-754 if there is an exceptional event. For the same reason, while SNaN encodings are supported, they are not signaling and are handled as quiet. The result of a single-precision floating-point operation involving one or more input NaNs is the quiet NaN of bit pattern 0x7fffffff. Double-precision floating-point absolute value and negation are not compliant with IEEE-754 with respect to NaNs; these are passed through unchanged. Code must be compiled with -ftz=false , -prec-div=true , and -prec-sqrt=true to ensure IEEE compliance (this is the default setting; see the nvcc user manual for description of these compilation flags). Regardless of the setting of the compiler flag -ftz , atomic single-precision floating-point adds on global memory always operate in flush-to-zero mode, i.e., behave equivalent to FADD.F32.FTZ.RN , atomic single-precision floating-point adds on shared memory always operate with denormal support, i.e., behave equivalent to FADD.F32.RN . In accordance to the IEEE-754R standard, if one of the input parameters to fminf() , fmin() , fmaxf() , or fmax() is NaN, but not the other, the result is the non-NaN parameter. The conversion of a floating-point value to an integer value in the case where the floating-point value falls outside the range of the integer format is left undefined by IEEE-754. For compute devices, the behavior is to clamp to the end of the supported range. This is unlike the x86 architecture behavior. The behavior of integer division by zero and integer overflow is left undefined by IEEE-754. For compute devices, there is no mechanism for detecting that such integer operation exceptions have occurred. Integer division by zero yields an unspecified, machine-specific value. https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus includes more information on the floating point accuracy and compliance of NVIDIA GPUs. 16.4.'},\n",
       " {'id': 353,\n",
       "  'content': 'Compute Capability 5.x \\uf0c1 16.4.1. Architecture \\uf0c1 An SM consists of: 128 CUDA cores for arithmetic operations (see Arithmetic Instructions for throughputs of arithmetic operations), 32 special function units for single-precision floating-point transcendental functions, 4 warp schedulers. When an SM is given warps to execute, it first distributes them among the four schedulers. Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any. An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified L1/texture cache of 24 KB used to cache reads from global memory, 64 KB of shared memory for devices of compute capability 5.0 or 96 KB of shared memory for devices of compute capability 5.2. The unified L1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering mentioned in Texture and Surface Memory . There is also an L2 cache shared by all SMs that is used to cache accesses to local or global memory, including temporary register spills. Applications may query the L2 cache size by checking the l2CacheSize device property (see Device Enumeration ). The cache behavior (e.g., whether reads are cached in both the unified L1/texture cache and L2 or in L2 only) can be partially configured on a per-access basis using modifiers to the load instruction. 16.4.2. Global Memory \\uf0c1 Global memory accesses are always cached in L2. Data that is read-only for the entire lifetime of the kernel can also be cached in the unified L1/texture cache described in the previous section by reading it using the __ldg() function (see Read-Only Data Cache Load Function ). When the compiler detects that the read-only condition is satisfied for some data, it will use __ldg() to read it. The compiler might not always be able to detect that the read-only condition is satisfied for some data. Marking pointers used for loading such data with both the const and __restrict__ qualifiers increases the likelihood that the compiler will detect the read-only condition. Data that is not read-only for the entire lifetime of the kernel cannot be cached in the unified L1/texture cache for devices of compute capability 5.0. For devices of compute capability 5.2, it is, by default, not cached in the unified L1/texture cache, but caching may be enabled using the following mechanisms: Perform the read using inline assembly with the appropriate modifier as described in the PTX reference manual; Compile with the -Xptxas -dlcm=ca compilation flag, in which case all reads are cached, except reads that are performed using inline assembly with a modifier that disables caching; Compile with the -Xptxas -fscm=ca compilation flag, in which case all reads are cached, including reads that are performed using inline assembly regardless of the modifier used. When caching is enabled using one of the three mechanisms listed above, devices of compute capability 5.2 will cache global memory reads in the unified L1/texture cache for all kernel launches except for the kernel launches for which thread blocks consume too much of the SM’s register file. These exceptions are reported by the profiler.'},\n",
       " {'id': 354,\n",
       "  'content': '16.4.3. Shared Memory \\uf0c1 Shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks. Each bank has a bandwidth of 32 bits per clock cycle. A shared memory request for a warp does not generate a bank conflict between two threads that access any address within the same 32-bit word (even though the two addresses fall in the same bank). In that case, for read accesses, the word is broadcast to the requesting threads and for write accesses, each address is written by only one of the threads (which thread performs the write is undefined). Figure 22 shows some examples of strided access. Figure 23 shows some examples of memory read accesses that involve the broadcast mechanism. Figure 35 Strided Shared Memory Accesses in 32 bit bank size mode. \\uf0c1 Left Linear addressing with a stride of one 32-bit word (no bank conflict). Middle Linear addressing with a stride of two 32-bit words (two-way bank conflict). Right Linear addressing with a stride of three 32-bit words (no bank conflict). Figure 36 Irregular Shared Memory Accesses. \\uf0c1 Left Conflict-free access via random permutation. Middle Conflict-free access since threads 3, 4, 6, 7, and 9 access the same word within bank 5. Right Conflict-free broadcast access (threads access the same word within a bank). 16.5.'},\n",
       " {'id': 355,\n",
       "  'content': 'Compute Capability 6.x \\uf0c1 16.5.1. Architecture \\uf0c1 An SM consists of: 64 (compute capability 6.0) or 128 (6.1 and 6.2) CUDA cores for arithmetic operations, 16 (6.0) or 32 (6.1 and 6.2) special function units for single-precision floating-point transcendental functions, 2 (6.0) or 4 (6.1 and 6.2) warp schedulers. When an SM is given warps to execute, it first distributes them among its schedulers. An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified L1/texture cache for reads from global memory of size 24 KB (6.0 and 6.2) or 48 KB (6.1), a shared memory of size 64 KB (6.0 and 6.2) or 96 KB (6.1). 16.5.2. Global Memory \\uf0c1 Global memory behaves the same way as in devices of compute capability 5.x (See Global Memory ). 16.5.3. Shared Memory \\uf0c1 Shared memory behaves the same way as in devices of compute capability 5.x (See Shared Memory ). 16.6.'},\n",
       " {'id': 356,\n",
       "  'content': 'Compute Capability 7.x \\uf0c1 16.6.1. Architecture \\uf0c1 An SM consists of: 64 FP32 cores for single-precision arithmetic operations, 32 FP64 cores for double-precision arithmetic operations, 34 64 INT32 cores for integer math, 8 mixed-precision Tensor Cores for deep learning matrix arithmetic 16 special function units for single-precision floating-point transcendental functions, 4 warp schedulers. An SM statically distributes its warps among its schedulers. An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 128 KB ( Volta ) or 96 KB ( Turing ). Shared memory is partitioned out of unified data cache, and can be configured to various sizes (See Shared Memory .)\\nThe remaining data cache serves as an L1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned in Texture and Surface Memory . 16.6.2. Independent Thread Scheduling \\uf0c1 The Volta architecture introduces Independent Thread Scheduling among threads in a warp, enabling intra-warp synchronization patterns previously unavailable and simplifying code changes when porting CPU code. However, this can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures. Below are code patterns of concern and suggested corrective actions for Volta-safe code. For applications using warp intrinsics ( __shfl* , __any , __all , __ballot ), it is necessary that developers port their code to the new, safe, synchronizing counterpart, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. See Warp Vote Functions and Warp Shuffle Functions for details. Since the intrinsics are available with CUDA 9.0+, (if necessary) code can be executed conditionally with the following preprocessor macro: #if defined(CUDART_VERSION) && CUDART_VERSION >= 9000 // *_sync intrinsic #endif These intrinsics are available on all architectures, not just Volta or Turing , and in most cases a single code-base will suffice for all architectures. Note, however, that for Pascal and earlier architectures, all threads in mask must execute the same warp intrinsic instruction in convergence, and the union of all values in mask must be equal to the warp’s active mask. The following code pattern is valid on Volta , but not on Pascal or earlier architectures. if ( tid % warpSize threshold ); if ( warpLane == 0 ) { output [ i / 32 ] = bitPack ; } } This code is invalid because CUDA does not guarantee that the warp will diverge ONLY at the loop condition. When divergence happens for other reasons, conflicting results will be computed for the same 32-bit output element by different subsets of threads in the warp. A correct code might use a non-divergent loop condition together with __ballot_sync() to safely enumerate the set of threads in the warp participating in the threshold calculation as follows. for ( int i = warpLane ; i - warpLane threshold ); if ( warpLane == 0 ) { output [ i / 32 ] = bitPack ; } } } Discovery Pattern demonstrates a valid use case for __activemask() . If applications have warp-synchronous codes, they will need to insert the new __syncwarp() warp-wide barrier synchronization instruction between any steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid. __shared__ float s_buff [ BLOCK_SIZE ]; s_buff [ tid ] = val ; __syncthreads (); // Inter-warp reduction for ( int i = BLOCK_SIZE / 2 ; i >= 32 ; i /= 2 ) { if ( tid >> (...); In addition to an integer percentage, several convenience enums are provided as listed in the code comments above. Where a chosen integer percentage does not map exactly to a supported capacity (SM 7.0 devices support shared capacities of 0, 8, 16, 32, 64, or 96 KB), the next larger capacity is used. For instance, in the example above, 50% of the 96 KB maximum is 48 KB, which is not a supported shared memory capacity. Thus, the preference is rounded up to 64 KB. Compute capability 7.x devices allow a single thread block to address the full capacity of shared memory: 96 KB on Volta , 64 KB on Turing . Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, as such they must use dynamic shared memory (rather than statically sized arrays) and require an explicit opt-in using cudaFuncSetAttribute() as follows. // Device code __global__ void MyKernel (...) { extern __shared__ float buffer []; ... } // Host code int maxbytes = 98304 ; // 96 KB cudaFuncSetAttribute ( MyKernel , cudaFuncAttributeMaxDynamicSharedMemorySize , maxbytes ); MyKernel >> (...); Otherwise, shared memory behaves the same way as for devices of compute capability 5.x (See Shared Memory ). 16.7.'},\n",
       " {'id': 357,\n",
       "  'content': 'Compute Capability 8.x \\uf0c1 16.7.1. Architecture \\uf0c1 A Streaming Multiprocessor (SM) consists of: 64 FP32 cores for single-precision arithmetic operations in devices of compute capability 8.0 and 128 FP32 cores in devices of compute capability 8.6, 8.7 and 8.9, 32 FP64 cores for double-precision arithmetic operations in devices of compute capability 8.0 and 2 FP64 cores in devices of compute capability 8.6, 8.7 and 8.9 64 INT32 cores for integer math, 4 mixed-precision Third-Generation Tensor Cores supporting half-precision (fp16), __nv_bfloat16 , tf32 , sub-byte and double precision (fp64) matrix arithmetic for compute capabilities 8.0, 8.6 and 8.7 (see Warp matrix functions for details), 4 mixed-precision Fourth-Generation Tensor Cores supporting fp8 , fp16 , __nv_bfloat16 , tf32 , sub-byte and fp64 for compute capability 8.9 (see Warp matrix functions for details), 16 special function units for single-precision floating-point transcendental functions, 4 warp schedulers. An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 192 KB for devices of compute capability 8.0 and 8.7 (1.5x Volta ’s 128 KB capacity) and 128 KB for devices of compute capabilities 8.6 and 8.9.'},\n",
       " {'id': 358,\n",
       "  'content': 'Shared memory is partitioned out of the unified data cache, and can be configured to various sizes (see Shared Memory section). 16.7.2. Global Memory \\uf0c1 Global memory behaves the same way as for devices of compute capability 5.x (See Global Memory ). 16.7.3. Shared Memory \\uf0c1 Similar to the Volta architecture , the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. For the NVIDIA Ampere GPU architecture , the unified data cache has a size of 192 KB for devices of compute capability 8.0 and 8.7 and 128 KB for devices of compute capabilities 8.6 and 8.9. The shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132 or 164 KB for devices of compute capability 8.0 and 8.7, and to 0, 8, 16, 32, 64 or 100 KB for devices of compute capabilities 8.6 and 8.9. An application can set the carveout , i.e., the preferred shared memory capacity, with the cudaFuncSetAttribute() . cudaFuncSetAttribute ( kernel_name , cudaFuncAttributePreferredSharedMemoryCarveout , carveout ); The API can specify the carveout either as an integer percentage of the maximum supported shared memory capacity of 164 KB for devices of compute capability 8.0 and 8.7 and 100 KB for devices of compute capabilities 8.6 and 8.9 respectively, or as one of the following values: {cudaSharedmemCarveoutDefault , cudaSharedmemCarveoutMaxL1 , or cudaSharedmemCarveoutMaxShared . When using a percentage, the carveout is rounded up to the nearest supported shared memory capacity. For example, for devices of compute capability 8.0, 50% will map to a 100 KB carveout instead of an 82 KB one. Setting the cudaFuncAttributePreferredSharedMemoryCarveout is considered a hint by the driver; the driver may choose a different configuration, if needed. Devices of compute capability 8.0 and 8.7 allow a single thread block to address up to 163 KB of shared memory, while devices of compute capabilities 8.6 and 8.9 allow up to 99 KB of shared memory. Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, and must use dynamic shared memory rather than statically sized shared memory arrays. These kernels require an explicit opt-in by using cudaFuncSetAttribute() to set the cudaFuncAttributeMaxDynamicSharedMemorySize ; see Shared Memory for the Volta architecture. Note that the maximum amount of shared memory per thread block is smaller than the maximum shared memory partition available per SM. The 1 KB of shared memory not made available to a thread block is reserved for system use. 16.8.'},\n",
       " {'id': 359,\n",
       "  'content': 'Compute Capability 9.0 \\uf0c1 16.8.1. Architecture \\uf0c1 A Streaming Multiprocessor (SM) consists of: 128 FP32 cores for single-precision arithmetic operations, 64 FP64 cores for double-precision arithmetic operations, 64 INT32 cores for integer math, 4 mixed-precision fourth-generation Tensor Cores supporting the new FP8 input type in either E4M3 or E5M2 for exponent (E) and mantissa (M), half-precision (fp16), __nv_bfloat16 , tf32 , INT8 and double precision (fp64) matrix arithmetic (see Warp Matrix Functions for details) with sparsity support, 16 special function units for single-precision floating-point transcendental functions, 4 warp schedulers. An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 256 KB for devices of compute capability 9.0 (1.33x NVIDIA Ampere GPU Architecture ’s 192 KB capacity). 16.8.2. 16.8.3. Shared Memory \\uf0c1 Similar to the NVIDIA Ampere GPU architecture , the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis. For the NVIDIA H100 Tensor Core GPU architecture , the unified data cache has a size of 256 KB for devices of compute capability 9.0. The shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132, 164, 196 or 228 KB. As with the NVIDIA Ampere GPU architecture , an application can configure its preferred shared memory capacity, i.e., the carveout . Devices of compute capability 9.0 allow a single thread block to address up to 227 KB of shared memory. 32 above 48 KB requires dynamic shared memory 33 2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7.5 34 2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7.5 16.8.4. Features Accelerating Specialized Computations \\uf0c1 The NVIDIA Hopper GPU architecture includes features to accelerate matrix multiply-accumulate (MMA) computations with: asynchronous execution of MMA instructions MMA instructions acting on large matrices spanning a warp-group dynamic reassignment of register capacity among warp-groups to support even larger matrices, and operand matrices accessed directly from shared memory This feature set is only available within the CUDA compilation toolchain through inline PTX. It is strongly recommended that applications utilize this complex feature set through CUDA-X libraries such as cuBLAS, cuDNN, or cuFFT. It is strongly recommended that device kernels utilize this complex feature set through CUTLASS , a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) and related computations at all levels and scales within CUDA. 17. Driver API \\uf0c1 This section assumes knowledge of the concepts described in CUDA Runtime . The driver API is implemented in the cuda dynamic library ( cuda.dll or cuda.so ) which is copied on the system during the installation of the device driver. All its entry points are prefixed with cu. It is a handle-based, imperative API: Most objects are referenced by opaque handles that may be specified to functions to manipulate the objects. The objects available in the driver API are summarized in Table 22 . Table 22 Objects Available in the CUDA Driver API \\uf0c1 Object Handle Description Device CUdevice CUDA-enabled device Context CUcontext Roughly equivalent to a CPU process Module CUmodule Roughly equivalent to a dynamic library Function CUfunction Kernel Heap memory CUdeviceptr Pointer to device memory CUDA array CUarray Opaque container for one-dimensional or two-dimensional data on the device, readable via texture or surface references Texture object CUtexref Object that describes how to interpret texture memory data Surface reference CUsurfref Object that describes how to read or write CUDA arrays Stream CUstream Object that describes a CUDA stream Event CUevent Object that describes a CUDA event The driver API must be initialized with cuInit() before any function from the driver API is called. A CUDA context must then be created that is attached to a specific device and made current to the calling host thread as detailed in Context . Within a CUDA context, kernels are explicitly loaded as PTX or binary objects by the host code as described in Module . Kernels written in C++ must therefore be compiled separately into PTX or binary objects. Kernels are launched using API entry points as described in Kernel Execution . Any application that wants to run on future device architectures must load PTX , not binary code. This is because binary code is architecture-specific and therefore incompatible with future architectures, whereas PTX code is compiled to binary code at load time by the device driver. Here is the host code of the sample from Kernels written using the driver API: int main () { int N = ...; size_t size = N * sizeof ( float ); // Allocate input vectors h_A and h_B in host memory float * h_A = ( float * ) malloc ( size ); float * h_B = ( float * ) malloc ( size ); // Initialize input vectors ... // Initialize cuInit ( 0 ); // Get number of devices supporting CUDA int deviceCount = 0 ; cuDeviceGetCount ( & deviceCount ); if ( deviceCount == 0 ) { printf ( \"There is no device supporting CUDA. \\\\n \" ); exit ( 0 ); } // Get handle for device 0 CUdevice cuDevice ; cuDeviceGet ( & cuDevice , 0 ); // Create context CUcontext cuContext ; cuCtxCreate ( & cuContext , 0 , cuDevice ); // Create module from binary file CUmodule cuModule ; cuModuleLoad ( & cuModule , \"VecAdd.ptx\" ); // Allocate vectors in device memory CUdeviceptr d_A ; cuMemAlloc ( & d_A , size ); CUdeviceptr d_B ; cuMemAlloc ( & d_B , size ); CUdeviceptr d_C ; cuMemAlloc ( & d_C , size ); // Copy vectors from host memory to device memory cuMemcpyHtoD ( d_A , h_A , size ); cuMemcpyHtoD ( d_B , h_B , size ); // Get function handle from module CUfunction vecAdd ; cuModuleGetFunction ( & vecAdd , cuModule , \"VecAdd\" ); // Invoke kernel int threadsPerBlock = 256 ; int blocksPerGrid = ( N + threadsPerBlock - 1 ) / threadsPerBlock ; void * args [] = { & d_A , & d_B , & d_C , & N }; cuLaunchKernel ( vecAdd , blocksPerGrid , 1 , 1 , threadsPerBlock , 1 , 1 , 0 , 0 , args , 0 ); ... } Full code can be found in the vectorAddDrv CUDA sample. 17.1. Context \\uf0c1 A CUDA context is analogous to a CPU process. All resources and actions performed within the driver API are encapsulated inside a CUDA context, and the system automatically cleans up these resources when the context is destroyed. Besides objects such as modules and texture or surface references, each context has its own distinct address space. As a result, CUdeviceptr values from different contexts reference different memory locations. A host thread may have only one device context current at a time. When a context is created with cuCtxCreate( ), it is made current to the calling host thread. CUDA functions that operate in a context (most functions that do not involve device enumeration or context management) will return CUDA_ERROR_INVALID_CONTEXT if a valid context is not current to the thread. Each host thread has a stack of current contexts. cuCtxCreate() pushes the new context onto the top of the stack. cuCtxPopCurrent() may be called to detach the context from the host thread. The context is then “floating” and may be pushed as the current context for any host thread. cuCtxPopCurrent() also restores the previous current context, if any. A usage count is also maintained for each context. cuCtxCreate() creates a context with a usage count of 1. cuCtxAttach() increments the usage count and cuCtxDetach() decrements it. A context is destroyed when the usage count goes to 0 when calling cuCtxDetach() or cuCtxDestroy() . The driver API is interoperable with the runtime and it is possible to access the primary context (see Initialization ) managed by the runtime from the driver API via cuDevicePrimaryCtxRetain() . Usage count facilitates interoperability between third party authored code operating in the same context. For example, if three libraries are loaded to use the same context, each library would call cuCtxAttach() to increment the usage count and cuCtxDetach() to decrement the usage count when the library is done using the context. For most libraries, it is expected that the application will have created a context before loading or initializing the library; that way, the application can create the context using its own heuristics, and the library simply operates on the context handed to it. Libraries that wish to create their own contexts - unbeknownst to their API clients who may or may not have created contexts of their own - would use cuCtxPushCurrent() and cuCtxPopCurrent() as illustrated in the following figure. Figure 37 Library Context Management \\uf0c1 17.2. Module \\uf0c1 Modules are dynamically loadable packages of device code and data, akin to DLLs in Windows, that are output by nvcc (see Compilation with NVCC ). The names for all symbols, including functions, global variables, and texture or surface references, are maintained at module scope so that modules written by independent third parties may interoperate in the same CUDA context. This code sample loads a module and retrieves a handle to some kernel: CUmodule cuModule ; cuModuleLoad ( & cuModule , \"myModule.ptx\" ); CUfunction myKernel ; cuModuleGetFunction ( & myKernel , cuModule , \"MyKernel\" ); This code sample compiles and loads a new module from PTX code and parses compilation errors: #define BUFFER_SIZE 8192 CUmodule cuModule ; CUjit_option options [ 3 ]; void * values [ 3 ]; char * PTXCode = \"some PTX code\" ; char error_log [ BUFFER_SIZE ]; int err ; options [ 0 ] = CU_JIT_ERROR_LOG_BUFFER ; values [ 0 ] = ( void * ) error_log ; options [ 1 ] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES ; values [ 1 ] = ( void * ) BUFFER_SIZE ; options [ 2 ] = CU_JIT_TARGET_FROM_CUCONTEXT ; values [ 2 ] = 0 ; err = cuModuleLoadDataEx ( & cuModule , PTXCode , 3 , options , values ); if ( err != CUDA_SUCCESS ) printf ( \"Link error: \\\\n %s \\\\n \" , error_log ); This code sample compiles, links, and loads a new module from multiple PTX codes and parses link and compilation errors: #define BUFFER_SIZE 8192 CUmodule cuModule ; CUjit_option options [ 6 ]; void * values [ 6 ]; float walltime ; char error_log [ BUFFER_SIZE ], info_log [ BUFFER_SIZE ]; char * PTXCode0 = \"some PTX code\" ; char * PTXCode1 = \"some other PTX code\" ; CUlinkState linkState ; int err ; void * cubin ; size_t cubinSize ; options [ 0 ] = CU_JIT_WALL_TIME ; values [ 0 ] = ( void * ) & walltime ; options [ 1 ] = CU_JIT_INFO_LOG_BUFFER ; values [ 1 ] = ( void * ) info_log ; options [ 2 ] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES ; values [ 2 ] = ( void * ) BUFFER_SIZE ; options [ 3 ] = CU_JIT_ERROR_LOG_BUFFER ; values [ 3 ] = ( void * ) error_log ; options [ 4 ] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES ; values [ 4 ] = ( void * ) BUFFER_SIZE ; options [ 5 ] = CU_JIT_LOG_VERBOSE ; values [ 5 ] = ( void * ) 1 ; cuLinkCreate ( 6 , options , values , & linkState ); err = cuLinkAddData ( linkState , CU_JIT_INPUT_PTX , ( void * ) PTXCode0 , strlen ( PTXCode0 ) + 1 , 0 , 0 , 0 , 0 ); if ( err != CUDA_SUCCESS ) printf ( \"Link error: \\\\n %s \\\\n \" , error_log ); err = cuLinkAddData ( linkState , CU_JIT_INPUT_PTX , ( void * ) PTXCode1 , strlen ( PTXCode1 ) + 1 , 0 , 0 , 0 , 0 ); if ( err != CUDA_SUCCESS ) printf ( \"Link error: \\\\n %s \\\\n \" , error_log ); cuLinkComplete ( linkState , & cubin , & cubinSize ); printf ( \"Link completed in %fms. Linker Output: \\\\n %s \\\\n \" , walltime , info_log ); cuModuleLoadData ( cuModule , cubin ); cuLinkDestroy ( linkState ); Full code can be found in the ptxjit CUDA sample.'},\n",
       " {'id': 360, 'content': '17.3.'},\n",
       " {'id': 361,\n",
       "  'content': \"Kernel Execution \\uf0c1 cuLaunchKernel() launches a kernel with a given execution configuration. Parameters are passed either as an array of pointers (next to last parameter of cuLaunchKernel() ) where the nth pointer corresponds to the nth parameter and points to a region of memory from which the parameter is copied, or as one of the extra options (last parameter of cuLaunchKernel() ). When parameters are passed as an extra option (the CU_LAUNCH_PARAM_BUFFER_POINTER option), they are passed as a pointer to a single buffer where parameters are assumed to be properly offset with respect to each other by matching the alignment requirement for each parameter type in device code. Alignment requirements in device code for the built-in vector types are listed in Table 5 . For all other basic types, the alignment requirement in device code matches the alignment requirement in host code and can therefore be obtained using __alignof() . The only exception is when the host compiler aligns double and long long (and long on a 64-bit system) on a one-word boundary instead of a two-word boundary (for example, using gcc ’s compilation flag -mno-align-double ) since in device code these types are always aligned on a two-word boundary. CUdeviceptr is an integer, but represents a pointer, so its alignment requirement is __alignof(void*) . The following code sample uses a macro ( ALIGN_UP() ) to adjust the offset of each parameter to meet its alignment requirement and another macro ( ADD_TO_PARAM_BUFFER() ) to add each parameter to the parameter buffer passed to the CU_LAUNCH_PARAM_BUFFER_POINTER option. #define ALIGN_UP(offset, alignment) \\\\ (offset) = ((offset) + (alignment) - 1) & ~((alignment) - 1) char paramBuffer [ 1024 ]; size_t paramBufferSize = 0 ; #define ADD_TO_PARAM_BUFFER(value, alignment) \\\\ do { \\\\ paramBufferSize = ALIGN_UP(paramBufferSize, alignment); \\\\ memcpy(paramBuffer + paramBufferSize, \\\\ &(value), sizeof(value)); \\\\ paramBufferSize += sizeof(value); \\\\ } while (0) int i ; ADD_TO_PARAM_BUFFER ( i , __alignof ( i )); float4 f4 ; ADD_TO_PARAM_BUFFER ( f4 , 16 ); // float4's alignment is 16 char c ; ADD_TO_PARAM_BUFFER ( c , __alignof ( c )); float f ; ADD_TO_PARAM_BUFFER ( f , __alignof ( f )); CUdeviceptr devPtr ; ADD_TO_PARAM_BUFFER ( devPtr , __alignof ( devPtr )); float2 f2 ; ADD_TO_PARAM_BUFFER ( f2 , 8 ); // float2's alignment is 8 void * extra [] = { CU_LAUNCH_PARAM_BUFFER_POINTER , paramBuffer , CU_LAUNCH_PARAM_BUFFER_SIZE , & paramBufferSize , CU_LAUNCH_PARAM_END }; cuLaunchKernel ( cuFunction , blockWidth , blockHeight , blockDepth , gridWidth , gridHeight , gridDepth , 0 , 0 , 0 , extra ); The alignment requirement of a structure is equal to the maximum of the alignment requirements of its fields. The alignment requirement of a structure that contains built-in vector types, CUdeviceptr , or non-aligned double and long long , might therefore differ between device code and host code.\"},\n",
       " {'id': 362,\n",
       "  'content': 'Such a structure might also be padded differently. The following structure, for example, is not padded at all in host code, but it is padded in device code with 12 bytes after field f since the alignment requirement for field f4 is 16. typedef struct { float f ; float4 f4 ; } myStruct ; 17.4. Interoperability between Runtime and Driver APIs \\uf0c1 An application can mix runtime API code with driver API code. If a context is created and made current via the driver API, subsequent runtime calls will pick up this context instead of creating a new one. If the runtime is initialized (implicitly as mentioned in CUDA Runtime ), cuCtxGetCurrent() can be used to retrieve the context created during initialization. This context can be used by subsequent driver API calls. The implicitly created context from the runtime is called the primary context (see Initialization ). It can be managed from the driver API with the Primary Context Management functions. Device memory can be allocated and freed using either API. CUdeviceptr can be cast to regular pointers and vice-versa: CUdeviceptr devPtr ; float * d_data ; // Allocation using driver API cuMemAlloc ( & devPtr , size ); d_data = ( float * ) devPtr ; // Allocation using runtime API cudaMalloc ( & d_data , size ); devPtr = ( CUdeviceptr ) d_data ; In particular, this means that applications written using the driver API can invoke libraries written using the runtime API (such as cuFFT, cuBLAS, …). All functions from the device and version management sections of the reference manual can be used interchangeably.'},\n",
       " {'id': 363,\n",
       "  'content': '17.5. Driver Entry Point Access \\uf0c1 17.5.1. Introduction \\uf0c1 The Driver Entry Point Access APIs provide a way to retrieve the address of a CUDA driver function. Starting from CUDA 11.3, users can call into available CUDA driver APIs using function pointers obtained from these APIs. These APIs provide functionality similar to their counterparts, dlsym on POSIX platforms and GetProcAddress on Windows. The provided APIs will let users: Retrieve the address of a driver function using the CUDA Driver API. Retrieve the address of a driver function using the CUDA Runtime API. Request per-thread default stream version of a CUDA driver function. For more details, see Retrieve per-thread default stream versions Access new CUDA features on older toolkits but with a newer driver. 17.5.2. Driver Function Typedefs \\uf0c1 To help retrieve the CUDA Driver API entry points, the CUDA Toolkit provides access to headers containing the function pointer definitions for all CUDA driver APIs. These headers are installed with the CUDA Toolkit and are made available in the toolkit’s include/ directory. The table below summarizes the header files containing the typedefs for each CUDA API header file. Table 23 Typedefs header files for CUDA driver APIs \\uf0c1 API header file API Typedef header file cuda.h cudaTypedefs.h cudaGL.h cudaGLTypedefs.h cudaProfiler.h cudaProfilerTypedefs.h cudaVDPAU.h cudaVDPAUTypedefs.h cudaEGL.h cudaEGLTypedefs.h cudaD3D9.h cudaD3D9Typedefs.h cudaD3D10.h cudaD3D10Typedefs.h cudaD3D11.h cudaD3D11Typedefs.h The above headers do not define actual function pointers themselves; they define the typedefs for function pointers. For example, cudaTypedefs.h has the below typedefs for the driver API cuMemAlloc : typedef CUresult ( CUDAAPI * PFN_cuMemAlloc_v3020 )( CUdeviceptr_v2 * dptr , size_t bytesize ); typedef CUresult ( CUDAAPI * PFN_cuMemAlloc_v2000 )( CUdeviceptr_v1 * dptr , unsigned int bytesize ); CUDA driver symbols have a version based naming scheme with a _v* extension in its name except for the first version. When the signature or the semantics of a specific CUDA driver API changes, we increment the version number of the corresponding driver symbol. In the case of the cuMemAlloc driver API, the first driver symbol name is cuMemAlloc and the next symbol name is cuMemAlloc_v2 . The typedef for the first version which was introduced in CUDA 2.0 (2000) is PFN_cuMemAlloc_v2000 . The typedef for the next version which was introduced in CUDA 3.2 (3020) is PFN_cuMemAlloc_v3020 . The typedefs can be used to more easily define a function pointer of the appropriate type in code: PFN_cuMemAlloc_v3020 pfn_cuMemAlloc_v2 ; PFN_cuMemAlloc_v2000 pfn_cuMemAlloc_v1 ; The above method is preferable if users are interested in a specific version of the API. Additionally, the headers have predefined macros for the latest version of all driver symbols that were available when the installed CUDA toolkit was released; these typedefs do not have a _v* suffix. For CUDA 11.3 toolkit, cuMemAlloc_v2 was the latest version and so we can also define its function pointer as below: PFN_cuMemAlloc pfn_cuMemAlloc ; 17.5.3. Driver Function Retrieval \\uf0c1 Using the Driver Entry Point Access APIs and the appropriate typedef, we can get the function pointer to any CUDA driver API. 17.5.3.1. Using the driver API \\uf0c1 The driver API requires CUDA version as an argument to get the ABI compatible version for the requested driver symbol. CUDA Driver APIs have a per-function ABI denoted with a _v* extension. For example, consider the versions of cuStreamBeginCapture and their corresponding typedefs from cudaTypedefs.h : // cuda.h CUresult CUDAAPI cuStreamBeginCapture ( CUstream hStream ); CUresult CUDAAPI cuStreamBeginCapture_v2 ( CUstream hStream , CUstreamCaptureMode mode ); // cudaTypedefs.h typedef CUresult ( CUDAAPI * PFN_cuStreamBeginCapture_v10000 )( CUstream hStream ); typedef CUresult ( CUDAAPI * PFN_cuStreamBeginCapture_v10010 )( CUstream hStream , CUstreamCaptureMode mode ); From the above typedefs in the code snippet, version suffixes _v10000 and _v10010 indicate that the above APIs were introduced in CUDA 10.0 and CUDA 10.1 respectively. #include // Declare the entry points for cuStreamBeginCapture PFN_cuStreamBeginCapture_v10000 pfn_cuStreamBeginCapture_v1 ; PFN_cuStreamBeginCapture_v10010 pfn_cuStreamBeginCapture_v2 ; // Get the function pointer to the cuStreamBeginCapture driver symbol cuGetProcAddress ( \"cuStreamBeginCapture\" , & pfn_cuStreamBeginCapture_v1 , 10000 , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); // Get the function pointer to the cuStreamBeginCapture_v2 driver symbol cuGetProcAddress ( \"cuStreamBeginCapture\" , & pfn_cuStreamBeginCapture_v2 , 10010 , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Referring to the code snippet above, to retrieve the address to the _v1 version of the driver API cuStreamBeginCapture , the CUDA version argument should be exactly 10.0 (10000). Similarly, the CUDA version for retrieving the address to the _v2 version of the API should be 10.1 (10010). Specifying a higher CUDA version for retrieving a specific version of a driver API might not always be portable. For example, using 11030 here would still return the _v2 symbol, but if a hypothetical _v3 version is released in CUDA 11.3, the cuGetProcAddress API would start returning the newer _v3 symbol instead when paired with a CUDA 11.3 driver. Since the ABI and function signatures of the _v2 and _v3 symbols might differ, calling the _v3 function using the _v10010 typedef intended for the _v2 symbol would exhibit undefined behavior. To retrieve the latest version of a driver API for a given CUDA Toolkit, we can also specify CUDA_VERSION as the version argument and use the unversioned typedef to define the function pointer. Since _v2 is the latest version of the driver API cuStreamBeginCapture in CUDA 11.3, the below code snippet shows a different method to retrieve it. // Assuming we are using CUDA 11.3 Toolkit #include // Declare the entry point PFN_cuStreamBeginCapture pfn_cuStreamBeginCapture_latest ; // Intialize the entry point. Specifying CUDA_VERSION will give the function pointer to the // cuStreamBeginCapture_v2 symbol since it is latest version on CUDA 11.3. cuGetProcAddress ( \"cuStreamBeginCapture\" , & pfn_cuStreamBeginCapture_latest , CUDA_VERSION , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Note that requesting a driver API with an invalid CUDA version will return an error CUDA_ERROR_NOT_FOUND . In the above code examples, passing in a version less than 10000 (CUDA 10.0) would be invalid. 17.5.3.2. Using the runtime API \\uf0c1 The runtime API cudaGetDriverEntryPoint uses the CUDA runtime version to get the ABI compatible version for the requested driver symbol. In the below code snippet, the minimum CUDA runtime version required would be CUDA 11.2 as cuMemAllocAsync was introduced then. #include // Declare the entry point PFN_cuMemAllocAsync pfn_cuMemAllocAsync ; // Intialize the entry point. Assuming CUDA runtime version >= 11.2 cudaGetDriverEntryPoint ( \"cuMemAllocAsync\" , & pfn_cuMemAllocAsync , cudaEnableDefault , & driverStatus ); // Call the entry point if ( driverStatus == cudaDriverEntryPointSuccess && pfn_cuMemAllocAsync ) { pfn_cuMemAllocAsync (...); } The runtime API cudaGetDriverEntryPointByVersion uses the user provided CUDA version to get the ABI compatible version for the requested driver symbol. This allows more specific control over the requested ABI version. 17.5.3.3. Retrieve per-thread default stream versions \\uf0c1 Some CUDA driver APIs can be configured to have default stream or per-thread default stream semantics. Driver APIs having per-thread default stream semantics are suffixed with _ptsz or _ptds in their name. For example, cuLaunchKernel has a per-thread default stream variant named cuLaunchKernel_ptsz . With the Driver Entry Point Access APIs, users can request for the per-thread default stream version of the driver API cuLaunchKernel instead of the default stream version. Configuring the CUDA driver APIs for default stream or per-thread default stream semantics affects the synchronization behavior. More details can be found here . The default stream or per-thread default stream versions of a driver API can be obtained by one of the following ways: Use the compilation flag --default-stream per-thread or define the macro CUDA_API_PER_THREAD_DEFAULT_STREAM to get per-thread default stream behavior. Force default stream or per-thread default stream behavior using the flags CU_GET_PROC_ADDRESS_LEGACY_STREAM/cudaEnableLegacyStream or CU_GET_PROC_ADDRESS_PER_THREAD_DEFAULT_STREAM/cudaEnablePerThreadDefaultStream respectively. 17.5.3.4. Access new CUDA features \\uf0c1 It is always recommended to install the latest CUDA toolkit to access new CUDA driver features, but if for some reason, a user does not want to update or does not have access to the latest toolkit, the API can be used to access new CUDA features with only an updated CUDA driver. For discussion, let us assume the user is on CUDA 11.3 and wants to use a new driver API cuFoo available in the CUDA 12.0 driver. The below code snippet illustrates this use-case: int main () { // Assuming we have CUDA 12.0 driver installed. // Manually define the prototype as cudaTypedefs.h in CUDA 11.3 does not have the cuFoo typedef typedef CUresult ( CUDAAPI * PFN_cuFoo )(...); PFN_cuFoo pfn_cuFoo = NULL ; CUdriverProcAddressQueryResult driverStatus ; // Get the address for cuFoo API using cuGetProcAddress. Specify CUDA version as // 12000 since cuFoo was introduced then or get the driver version dynamically // using cuDriverGetVersion int driverVersion ; cuDriverGetVersion ( & driverVersion ); CUresult status = cuGetProcAddress ( \"cuFoo\" , & pfn_cuFoo , driverVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( status == CUDA_SUCCESS && pfn_cuFoo ) { pfn_cuFoo (...); } else { printf ( \"Cannot retrieve the address to cuFoo - driverStatus = %d. Check if the latest driver for CUDA 12.0 is installed. \\\\n \" , driverStatus ); assert ( 0 ); } // rest of code here } 17.5.4. Potential Implications with cuGetProcAddress \\uf0c1 Below is a set of concrete and theoretical examples of potential issues with cuGetProcAddress and cudaGetDriverEntryPoint . 17.5.4.1. Implications with cuGetProcAddress vs Implicit Linking \\uf0c1 cuDeviceGetUuid was introduced in CUDA 9.2. This API has a newer revision ( cuDeviceGetUuid_v2 ) introduced in CUDA 11.4. To preserve minor version compatibility, cuDeviceGetUuid will not be version bumped to cuDeviceGetUuid_v2 in cuda.h until CUDA 12.0. This means that calling it by obtaining a function pointer to it via cuGetProcAddress might have different behavior. Example using the API directly: #include CUuuid uuid ; CUdevice dev ; CUresult status ; status = cuDeviceGet ( & dev , 0 ); // Get device 0 // handle status status = cuDeviceGetUuid ( & uuid , dev ) // Get uuid of device 0 In this example, assume the user is compiling with CUDA 11.4. Note that this will perform the behavior of cuDeviceGetUuid , not _v2 version. Now an example of using cuGetProcAddress : #include CUuuid uuid ; CUdevice dev ; CUresult status ; CUdriverProcAddressQueryResult driverStatus ; status = cuDeviceGet ( & dev , 0 ); // Get device 0 // handle status PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid ; status = cuGetProcAddress ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuid , CUDA_VERSION , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && pfn_cuDeviceGetUuid ) { // pfn_cuDeviceGetUuid points to ? ?'},\n",
       " {'id': 364,\n",
       "  'content': '? }\\nIn this example, assume the user is compiling with CUDA 11.4. This will get the function pointer of cuDeviceGetUuid_v2 . Calling the function pointer will then invoke the new _v2 function, not the same cuDeviceGetUuid as shown in the previous example. 17.5.4.2. Compile Time vs Runtime Version Usage in cuGetProcAddress \\uf0c1 Let’s take the same issue and make one small tweak. The last example used the compile time constant of CUDA_VERSION to determine which function pointer to obtain. More complications arise if the user queries the driver version dynamically using cuDriverGetVersion or cudaDriverGetVersion to pass to cuGetProcAddress . Example: #include CUuuid uuid ; CUdevice dev ; CUresult status ; int cudaVersion ; CUdriverProcAddressQueryResult driverStatus ; status = cuDeviceGet ( & dev , 0 ); // Get device 0 // handle status status = cuDriverGetVersion ( & cudaVersion ); // handle status PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid ; status = cuGetProcAddress ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuid , cudaVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && pfn_cuDeviceGetUuid ) { // pfn_cuDeviceGetUuid points to ? In this example, assume the user is compiling with CUDA 11.3. The user would debug, test, and deploy this application with the known behavior of getting cuDeviceGetUuid (not the _v2 version). Since CUDA has guaranteed ABI compatibility between minor versions, this same application is expected to run after the driver is upgraded to CUDA 11.4 (without updating the toolkit and runtime) without requiring recompilation. This will have undefined behavior though, because now the typedef for PFN_cuDeviceGetUuid will still be of the signature for the original version, but since cudaVersion would now be 11040 (CUDA 11.4), cuGetProcAddress would return the function pointer to the _v2 version, meaning calling it might have undefined behavior. Note in this case the original (not the _v2 version) typedef looks like: typedef CUresult ( CUDAAPI * PFN_cuDeviceGetUuid_v9020 )( CUuuid * uuid , CUdevice_v1 dev ); But the _v2 version typedef looks like: typedef CUresult ( CUDAAPI * PFN_cuDeviceGetUuid_v11040 )( CUuuid * uuid , CUdevice_v1 dev ); So in this case, the API/ABI is going to be the same and the runtime API call will likely not cause issues–only the potential for unknown uuid return. In Implications to API/ABI , we discuss a more problematic case of API/ABI compatibility.'},\n",
       " {'id': 365,\n",
       "  'content': '17.5.4.3. API Version Bumps with Explicit Version Checks \\uf0c1 Above, was a specific concrete example.'},\n",
       " {'id': 366,\n",
       "  'content': 'Now for instance let’s use a theoretical example that still has issues with compatibility across driver versions. Example: CUresult cuFoo ( int bar ); // Introduced in CUDA 11.4 CUresult cuFoo_v2 ( int bar ); // Introduced in CUDA 11.5 CUresult cuFoo_v3 ( int bar , void * jazz ); // Introduced in CUDA 11.6 typedef CUresult ( CUDAAPI * PFN_cuFoo_v11040 )( int bar ); typedef CUresult ( CUDAAPI * PFN_cuFoo_v11050 )( int bar ); typedef CUresult ( CUDAAPI * PFN_cuFoo_v11060 )( int bar , void * jazz ); Notice that the API has been modified twice since original creation in CUDA 11.4 and the latest in CUDA 11.6 also modified the API/ABI interface to the function. The usage in user code compiled against CUDA 11.5 is: #include #include CUresult status ; int cudaVersion ; CUdriverProcAddressQueryResult driverStatus ; status = cuDriverGetVersion ( & cudaVersion ); // handle status PFN_cuFoo_v11040 pfn_cuFoo_v11040 ; PFN_cuFoo_v11050 pfn_cuFoo_v11050 ; if ( cudaVersion = CUDA 11.5 version we can use the second version status = cuGetProcAddress ( \"cuFoo\" , & pfn_cuFoo_v11050 , cudaVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); // Handle status and validating pfn_cuFoo_v11050 } In this example, without updates for the new typedef in CUDA 11.6 and recompiling the application with those new typedefs and case handling, the application will get the cuFoo_v3 function pointer returned and any usage of that function would then cause undefined behavior. The point of this example was to illustrate that even explicit version checks for cuGetProcAddress may not safely cover the minor version bumps within a CUDA major release. 17.5.4.4. Issues with Runtime API Usage \\uf0c1 The above examples were focused on the issues with the Driver API usage for obtaining the function pointers to driver APIs. Now we will discuss the potential issues with the Runtime API usage for cudaApiGetDriverEntryPoint . We will start by using the Runtime APIs similar to the above. #include #include #include CUresult status ; cudaError_t error ; int driverVersion , runtimeVersion ; CUdriverProcAddressQueryResult driverStatus ; // Ask the runtime for the function PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidRuntime ; error = cudaGetDriverEntryPoint ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuidRuntime , cudaEnableDefault , & driverStatus ); if ( cudaSuccess == error && pfn_cuDeviceGetUuidRuntime ) { // pfn_cuDeviceGetUuid points to ? The function pointer in this example is even more complicated than the driver only examples above because there is no control over which version of the function to obtain; it will always get the API for the current CUDA Runtime version. See the following table for more information: Static Runtime Version Linkage Driver Version Installed V11.3 V11.4 V11.3 v1 v1x V11.4 v1 v2 V11.3 => 11.3 CUDA Runtime and Toolkit (includes header files cuda.h and cudaTypedefs.h) V11.4 => 11.4 CUDA Runtime and Toolkit (includes header files cuda.h and cudaTypedefs.h) v1 => cuDeviceGetUuid v2 => cuDeviceGetUuid_v2 x => Implies the typedef function pointer won\\'t match the returned function pointer. In these cases, the typedef at compile time using a CUDA 11.4 runtime, would match the _v2 version, but the returned function pointer would be the original (non _v2) function. The problem in the table comes in with a newer CUDA 11.4 Runtime and Toolkit and older driver (CUDA 11.3) combination, labeled as v1x in the above. This combination would have the driver returning the pointer to the older function (non _v2), but the typedef used in the application would be for the new function pointer. 17.5.4.5. Issues with Runtime API and Dynamic Versioning \\uf0c1 More complications arise when we consider different combinations of the CUDA version with which an application is compiled, CUDA runtime version, and CUDA driver version that an application dynamically links against. #include #include #include CUresult status ; cudaError_t error ; int driverVersion , runtimeVersion ; CUdriverProcAddressQueryResult driverStatus ; enum cudaDriverEntryPointQueryResult runtimeStatus ; PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidDriver ; status = cuGetProcAddress ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuidDriver , CUDA_VERSION , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && pfn_cuDeviceGetUuidDriver ) { // pfn_cuDeviceGetUuidDriver points to ? // Ask the runtime for the function PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidRuntime ; error = cudaGetDriverEntryPoint ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuidRuntime , cudaEnableDefault , & runtimeStatus ); if ( cudaSuccess == error && pfn_cuDeviceGetUuidRuntime ) { // pfn_cuDeviceGetUuidRuntime points to ? // Ask the driver for the function based on the driver version (obtained via runtime) error = cudaDriverGetVersion ( & driverVersion ); PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidDriverDriverVer ; status = cuGetProcAddress ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuidDriverDriverVer , driverVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && pfn_cuDeviceGetUuidDriverDriverVer ) { // pfn_cuDeviceGetUuidDriverDriverVer points to ? The following matrix of function pointers is expected: Function Pointer Application Compiled/Runtime Dynamic Linked Version/Driver Version (3 => CUDA 11.3 and 4 => CUDA 11.4) 3/3/3 3/3/4 3/4/3 3/4/4 4/3/3 4/3/4 4/4/3 4/4/4 pfn_cuDeviceGetUuidDriver t1/v1 t1/v1 t1/v1 t1/v1 N/A N/A t2/v1 t2/v2 pfn_cuDeviceGetUuidRuntime t1/v1 t1/v1 t1/v1 t1/v2 N/A N/A t2/v1 t2/v2 pfn_cuDeviceGetUuidDriverDriverVer t1/v1 t1/v2 t1/v1 t1/v2 N/A N/A t2/v1 t2/v2 tX -> Typedef version used at compile time vX -> Version returned/used at runtime If the application is compiled against CUDA Version 11.3, it would have the typedef for the original function, but if compiled against CUDA Version 11.4, it would have the typedef for the _v2 function. Because of that, notice the number of cases where the typedef does not match the actual version returned/used. 17.5.4.6. Issues with Runtime API allowing CUDA Version \\uf0c1 Unless specified otherwise, the CUDA runtime API cudaGetDriverEntryPointByVersion will have similar implications as the driver entry point cuGetProcAddress since it allows for the user to request a specific CUDA driver version. 17.5.4.7. Implications to API/ABI \\uf0c1 In the above examples using cuDeviceGetUuid , the implications of the mismatched API are minimal, and may not be entirely noticeable to many users as the _v2 was added to support Multi-Instance GPU (MIG) mode. So, on a system without MIG, the user might not even realize they are getting a different API. More problematic is an API which changes its application signature (and hence ABI) such as cuCtxCreate . The _v2 version, introduced in CUDA 3.2 is currently used as the default cuCtxCreate when using cuda.h but now has a newer version introduced in CUDA 11.4 ( cuCtxCreate_v3 ). The API signature has been modified as well, and now takes extra arguments. So, in some of the cases above, where the typedef to the function pointer doesn’t match the returned function pointer, there is a chance for non-obvious ABI incompatibility which would lead to undefined behavior. For example, assume the following code compiled against a CUDA 11.3 toolkit with a CUDA 11.4 driver installed: PFN_cuCtxCreate cuUnknown ; CUdriverProcAddressQueryResult driverStatus ; status = cuGetProcAddress ( \"cuCtxCreate\" , ( void ** ) & cuUnknown , cudaVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && cuUnknown ) { status = cuUnknown ( & ctx , 0 , dev ); } Running this code where cudaVersion is set to anything >=11040 (indicating CUDA 11.4) could have undefined behavior due to not having adequately supplied all the parameters required for the _v3 version of the cuCtxCreate_v3 API. 17.5.5. Determining cuGetProcAddress Failure Reasons \\uf0c1 There are two types of errors with cuGetProcAddress. Those are (1) API/usage errors and (2) inability to find the driver API requested. The first error type will return error codes from the API via the CUresult return value. Things like passing NULL as the pfn variable or passing invalid flags . The second error type encodes in the CUdriverProcAddressQueryResult *symbolStatus and can be used to help distinguish potential issues with the driver not being able to find the symbol requested. Take the following example: // cuDeviceGetExecAffinitySupport was introduced in release CUDA 11.4 #include CUdriverProcAddressQueryResult driverStatus ; cudaVersion = ...; status = cuGetProcAddress ( \"cuDeviceGetExecAffinitySupport\" , & pfn , cudaVersion , 0 , & driverStatus ); if ( CUDA_SUCCESS == status ) { if ( CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT == driverStatus ) { printf ( \"We can use the new feature when you upgrade cudaVersion to 11.4, but CUDA driver is good to go! \\\\n \" ); // Indicating cudaVersion was = 11.4 } else if ( CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND == driverStatus ) { printf ( \"Please update both CUDA driver and cudaVersion to at least 11.4 to use the new feature! \\\\n \" ); // Indicating driver is = 11.4, using new feature! \\\\n \" ); pfn (); } } The first case with the return code CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT indicates that the symbol was found when searching in the CUDA driver but it was added later than the cudaVersion supplied. In the example, specifying cudaVersion as anything 11030 or less and when running against a CUDA driver >= CUDA 11.4 would give this result of CU_GET_PROC_ADDRESS_VERSION_NOT_SUFFICIENT . This is because cuDeviceGetExecAffinitySupport was added in CUDA 11.4 (11040). The second case with the return code CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND indicates that the symbol was not found when searching in the CUDA driver. This can be due to a few reasons such as unsupported CUDA function due to older driver as well as just having a typo. In the latter, similar to the last example if the user had put symbol as CUDeviceGetExecAffinitySupport - notice the capital CU to start the string - cuGetProcAddress would not be able to find the API because the string doesn’t match. In the former case an example might be the user developing an application against a CUDA driver supporting the new API, and deploying the application against an older CUDA driver. Using the last example, if the developer developed against CUDA 11.4 or later but was deployed against a CUDA 11.3 driver, during their development they may have had a succesful cuGetProcAddress , but when deploying an application running against a CUDA 11.3 driver the call would no longer work with the CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND returned in driverStatus . 18. CUDA Environment Variables \\uf0c1 The following table lists the CUDA environment variables. Environment variables related to the Multi-Process Service are documented in the Multi-Process Service section of the GPU Deployment and Management guide. Table 24 CUDA Environment Variables \\uf0c1 Variable Values Description Device Enumeration and Properties CUDA_VISIBLE_DEVICES A comma-separated sequence of GPU identifiers MIG support: MIG-// GPU identifiers are given as integer indices or as UUID strings. GPU UUID strings should follow the same format as given by nvidia-smi , such as GPU-8932f937-d72c-4106-c12f-20bd9faed9f6. However, for convenience, abbreviated forms are allowed; simply specify enough digits from the beginning of the GPU UUID to uniquely identify that GPU in the target system. For example, CUDA_VISIBLE_DEVICES=GPU-8932f937 may be a valid way to refer to the above GPU UUID, assuming no other GPU in the system shares this prefix. Only the devices whose index is present in the sequence are visible to CUDA applications and they are enumerated in the order of the sequence. If one of the indices is invalid, only the devices whose index precedes the invalid index are visible to CUDA applications. For example, setting CUDA_VISIBLE_DEVICES to 2,1 causes device 0 to be invisible and device 2 to be enumerated before device 1. Setting CUDA_VISIBLE_DEVICES to 0,2,-1,1 causes devices 0 and 2 to be visible and device 1 to be invisible. MIG format starts with MIG keyword and GPU UUID should follow the same format as given by nvidia-smi . For example, MIG-GPU-8932f937-d72c-4106-c12f-20bd9faed9f6/1/2. Only single MIG instance enumeration is supported. CUDA_MANAGED_FORCE_DEVICE_ALLOC 0 or 1 (default is 0) Forces the driver to place all managed allocations in device memory. CUDA_DEVICE_ORDER FASTEST_FIRST, PCI_BUS_ID, (default is FASTEST_FIRST) FASTEST_FIRST causes CUDA to enumerate the available devices in fastest to slowest order using a simple heuristic. PCI_BUS_ID orders devices by PCI bus ID in ascending order. Compilation CUDA_CACHE_DISABLE 0 or 1 (default is 0) Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation. When disabled, no binary code is added to or retrieved from the cache. CUDA_CACHE_PATH filepath Specifies the folder where the just-in-time compiler caches binary codes; the default values are: on Windows, %APPDATA%\\\\NVIDIA\\\\ComputeCache on Linux, ~/.nv/ComputeCache CUDA_CACHE_MAXSIZE integer (default is 1073741824 (1 GiB) for desktop/server platforms and 268435456 (256 MiB) for embedded platforms and the maximum is 4294967296 (4 GiB)) Specifies the size in bytes of the cache used by the just-in-time compiler. Binary codes whose size exceeds the cache size are not cached. Older binary codes are evicted from the cache to make room for newer binary codes if needed. CUDA_FORCE_PTX_JIT 0 or 1 (default is 0) When set to 1, forces the device driver to ignore any binary code embedded in an application (see Application Compatibility ) and to just-in-time compile embedded PTX code instead. If a kernel does not have embedded PTX code, it will fail to load. This environment variable can be used to validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application forward compatibility with future architectures (see Just-in-Time Compilation ). CUDA_DISABLE_PTX_JIT 0 or 1 (default is 0) When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see Application Compatibility ). If a kernel does not have embedded binary code or the embedded binary was compiled for an incompatible architecture, then it will fail to load. This environment variable can be used to validate that an application has the compatible SASS code generated for each kernel. (see Binary Compatibility ). CUDA_FORCE_JIT 0 or 1 (default is 0) When set to 1, forces the device driver to ignore any binary code embedded in an application (see Application Compatibility ) and to just-in-time compile embedded PTX code instead. The behavior can be overridden for embedded PTX by setting CUDA_FORCE_PTX_JIT=0 . CUDA_DISABLE_JIT 0 or 1 (default is 0) When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see Application Compatibility ). The behavior can be overridden for embedded PTX by setting CUDA_DISABLE_PTX_JIT=0 . Execution CUDA_LAUNCH_BLOCKING 0 or 1 (default is 0) Disables (when set to 1) or enables (when set to 0) asynchronous kernel launches. CUDA_DEVICE_MAX_CONNECTIONS 1 to 32 (default is 8) Sets the number of compute and copy engine concurrent connections (work queues) from the host to each device of compute capability 3.5 and above. CUDA_AUTO_BOOST 0 or 1 Overrides the autoboost behavior set by the –auto-boost-default option of nvidia-smi. If an application requests via this environment variable a behavior that is different from nvidia-smi’s, its request is honored if there is no other application currently running on the same GPU that successfully requested a different behavior, otherwise it is ignored. CUDA_SCALE_LAUNCH_QUEUES “0.25x”, “0.5x”, “2x” or “4x” Scales the size of the queues available for launching work by a fixed multiplier. cuda-gdb (on Linux platform) CUDA_DEVICE_WAITS_ON_EXCEPTION 0 or 1 (default is 0) When set to 1, a CUDA application will halt when a device exception occurs, allowing a debugger to be attached for further debugging. MPS service (on Linux platform) CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT Percentage value (between 0 - 100, default is 0) Devices of compute capability 8.x allow, a portion of L2 cache to be set-aside for persisting data accesses to global memory. When using CUDA MPS service, the set-aside size can only be controlled using this environment variable, before starting CUDA MPS control daemon. I.e., the environment variable should be set before running the command nvidia-cuda-mps-control -d . Module loading CUDA_MODULE_LOADING DEFAULT, LAZY, EAGER (default is LAZY) Specifies the module loading mode for the application. When set to EAGER, all kernels and data from a cubin, fatbin or a PTX file are fully loaded upon corresponding cuModuleLoad* and cuLibraryLoad* API call. When set to LAZY, loading of specific kernels is delayed to the point a CUfunc handle is extracted with cuModuleGetFunction or cuKernelGetFunction API calls and data from the cubin is loaded at load of first kernel in the cubin or at first access of variables in the cubin. Default behavior may change in future CUDA releases. CUDA_MODULE_DATA_LOADING DEFAULT, LAZY, EAGER (default is LAZY) Specifies the data loading mode for the application. When set to EAGER, all data from a cubin, fatbin or a PTX file are fully loaded to memory upon corresponding cuLibraryLoad* . This doesn’t affect the LAZY or EAGER loading of kernels. When set to LAZY, loading of data is delayed to the point at which a handle is required. Data loading behavior is inherited from CUDA_MODULE_LOADING if this environment variable is not set. Pre-loading dependent libraries CUDA_FORCE_PRELOAD_LIBRARIES 0 or 1 (default is 0) When set to 1, forces the driver to preload the libraries required for NVVM and PTX just-in-time compilation during driver initialization. This will increase the memory footprint and the time taken for CUDA driver initialization. This environment variable needs to be set to avoid certain deadlock situations involving multiple CUDA threads. CUDA Graphs CUDA_GRAPHS_USE_NODE_PRIORITY 0 or 1 Overrides the cudaGraphInstantiateFlagUseNodePriority flag on graph instantiation. When set to 1, the flag will be set for all graphs and when set to 0, the flag will be cleared for all graphs. 19.'},\n",
       " {'id': 367,\n",
       "  'content': 'Unified Memory Programming \\uf0c1 Note This chapter applies to devices with compute capability 5.0 or higher unless stated otherwise. For devices with compute capability lower than 5.0, refer to the CUDA toolkit documentation for CUDA 11.8. This documentation on Unified Memory is divided into 3 parts: General description of unified memory Unified Memory on devices with full CUDA Unified Memory support Unified Memory on devices without full CUDA Unified Memory support 19.1. Unified Memory Introduction \\uf0c1 CUDA Unified Memory provides all processors with: a single unified memory pool, that is, a single pointer value enables all processors in the system (all CPUs, all GPUs, etc.)\\nto access this memory with all of their native memory operations (pointer dereferenes, atomics, etc.). concurrent access to the unified memory pool from all processors in the system. Unified Memory improves GPU programming in several ways: Producitivity : GPU programs may access Unified Memory from GPU and CPU threads concurrently without needing to create separate allocations ( cudaMalloc() ) and copy memory manually back and forth ( cudaMemcpy*() ). Performance : Data access speed may be maximized by migrating data towards processors that access it most frequently. Applications can trigger manual migration of data and may use hints to control migration heuristics. Total system memory usage may be reduced by avoiding duplicating memory on both CPUs and GPUs. Functionality : it enables GPU programs to work on data that exceeds the GPU memory’s capacity. With CUDA Unified Memory, data movement still takes place, and hints may improve performance. These hints are not required for correctness or functionality, that is, programmers may focus on parallelizing their applications across GPUs and CPUs first, and worry about data-movement later in the development cycle as a performance optimzation. Note that the physical location of data is invisible to a program and may be changed at any time, but accesses to the data’s virtual address will remain valid and coherent from any processor regardless of locality. There are two main ways to obtain CUDA Unified Memory: System-Allocated Memory : memory allocated on the host with system APIs: stack variables, global-/file-scope variables, malloc() / mmap() (see System Allocator for in-depth examples), thread locals, etc. CUDA APIs that explicitly allocate Unified Memory : memory allocated with, for example, cudaMallocManaged() , are available on more systems and may perform better than System-Allocated Memory. 19.1.1. System Requirements for Unified Memory \\uf0c1 The following table shows the different levels of support for CUDA Unified Memory, the device properties required to detect these levels of support and links to the documentation specific to each level of support: Table 25 Overview of levels of unified memory support \\uf0c1 Unified Memory Support Level System device properties Further documentation Full CUDA Unified Memory: all memory has full support. This includes System-Allocated and CUDA Managed Memory. Set to 1: pageableMemoryAccess Systems with hardware acceleration also have the following properties set to 1: hostNativeAtomicSupported , pageableMemoryAccessUsesHostPageTables , directManagedMemAccessFromHost Unified Memory on devices with full CUDA Unified Memory support Only CUDA Managed Memory has full support. Set to 1: concurrentManagedAccess Set to 0: pageableMemoryAccess Unified Memory on devices with only CUDA Managed Memory support CUDA Managed Memory without full support: unified addressing but no concurrent access. Set to 1: managedMemory Set to 0: concurrentManagedAccess Unified Memory on Windows or devices with compute capability 5.x CUDA for Tegra Memory Management Unified Memory on Tegra No Unified Memory support. Set to 0: managedMemory CUDA for Tegra Memory Management The behavior of an application that attempts to use Unified Memory on a system that does not support it is undefined. The following properties enable CUDA applications to check the level of system support for Unified Memory, and to be portable between systems with different levels of support: pageableMemoryAccess : This property is set to 1 on systems with CUDA Unified Memory support where all threads may access System-Allocated Memory and CUDA Managed Memory. These systems include NVIDIA Grace Hopper, IBM Power9 + Volta, and modern Linux systems with HMM enabled (see next bullet), among others. Linux HMM requires Linux kernel version 6.1.24+, 6.2.11+ or 6.3+, devices with compute capability 7.5 or higher and a CUDA driver version 535+ installed with Open Kernel Modules . concurrentManagedAccess : This property is set to 1 on systems with full CUDA Managed Memory support. When this property is set to 0, there is only partial support for Unified Memory in CUDA Managed Memory. For Tegra support of Unified Memory, see CUDA for Tegra Memory Management . A program may query the level of GPU support for CUDA Unified Memory, by querying the attributes in Table Overview of levels of unified memory support above using cudaGetDeviceProperties() . 19.1.2. Programming Model \\uf0c1 With CUDA Unified Memory, separate allocations between host and device, and explicit memory transfers between them, are no longer required. Programs may allocate Unified Memory in the following ways: System-Allocation APIs : on systems with full CUDA Unified Memory support via any system allocation of the host process (C’s malloc() , C++’s new operator, POSIX’s mmap and so on). CUDA Managed Memory Allocation APIs : via the cudaMallocManaged() API which is syntactically similar to cudaMalloc() . CUDA Managed Variables : variables declared with __managed__ , which are semantically similar to a __device__ variable. Most examples in this chapter provide at least two versions, one using CUDA Managed Memory and one using System-Allocated Memory. Tabs allow you to choose between them. The following samples illustrate how Unified Memory simplifies CUDA programs: System ( malloc() ) __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { int * d_ptr = nullptr ; // Does not require any unified memory support cudaMalloc ( & d_ptr , sizeof ( int )); write_value >> ( d_ptr , 1 ); int host ; // Copy memory back to the host and synchronize cudaMemcpy ( & host , d_ptr , sizeof ( int ), cudaMemcpyDefault ); printf ( \"value = %d \\\\n \" , host ); cudaFree ( d_ptr ); return 0 ; } __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { // Requires System-Allocated Memory support int * ptr = ( int * ) malloc ( sizeof ( int )); write_value >> ( ptr , 1 ); // Synchronize required // (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \\\\n \" , * ptr ); free ( ptr ); return 0 ; } System (Stack) __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { int * d_ptr = nullptr ; // Does not require any unified memory support cudaMalloc ( & d_ptr , sizeof ( int )); write_value >> ( d_ptr , 1 ); int host ; // Copy memory back to the host and synchronize cudaMemcpy ( & host , d_ptr , sizeof ( int ), cudaMemcpyDefault ); printf ( \"value = %d \\\\n \" , host ); cudaFree ( d_ptr ); return 0 ; } __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { // Requires System-Allocated Memory support int value ; write_value >> ( & value , 1 ); // Synchronize required // (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \\\\n \" , value ); return 0 ; } Managed ( cudaMallocManaged() ) __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { int * d_ptr = nullptr ; // Does not require any unified memory support cudaMalloc ( & d_ptr , sizeof ( int )); write_value >> ( d_ptr , 1 ); int host ; // Copy memory back to the host and synchronize cudaMemcpy ( & host , d_ptr , sizeof ( int ), cudaMemcpyDefault ); printf ( \"value = %d \\\\n \" , host ); cudaFree ( d_ptr ); return 0 ; } __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { int * ptr = nullptr ; // Requires CUDA Managed Memory support cudaMallocManaged ( & ptr , sizeof ( int )); write_value >> ( ptr , 1 ); // Synchronize required // (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \\\\n \" , * ptr ); cudaFree ( ptr ); return 0 ; } Managed ( __managed__ ) __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { int * d_ptr = nullptr ; // Does not require any unified memory support cudaMalloc ( & d_ptr , sizeof ( int )); write_value >> ( d_ptr , 1 ); int host ; // Copy memory back to the host and synchronize cudaMemcpy ( & host , d_ptr , sizeof ( int ), cudaMemcpyDefault ); printf ( \"value = %d \\\\n \" , host ); cudaFree ( d_ptr ); return 0 ; } __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } // Requires CUDA Managed Memory support __managed__ int value ; int main () { write_value >> ( & value , 1 ); // Synchronize required // (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \\\\n \" , value ); return 0 ; } These examples combine two numbers together on the GPU with a per-thread ID returning the values in an array: Without Unified Memory : both host- and device-side storage for the return values is required ( host_ret and ret in the example), as is an explicit copy between the two using cudaMemcpy() . With Unified Memory : GPU accesses data directly from the host. ret may be used without a separate host_ret allocation and no copy routine is required, greatly simplifying and reducing the size of the program. With: System Allocated : no other changes required. Managed Memory : data allocation changed to use cudaMallocManaged() , which returns a pointer valid from both host and device code. 19.1.2.1. Allocation APIs for System-Allocated Memory \\uf0c1 On systems with full CUDA Unified Memory support , all memory is unified memory. This includes memory allocated with system allocation APIs, such as malloc() , mmap() , C++ new() operator, and also automatic variables on CPU thread stacks, thread locals, global variables, and so on. System-Allocated Memory may be popullated on first touch, depending on the API and system settings used. First touch means that: - The allocation APIs allocate virtual memory and return immediately, and - physical memory is populated when a thread accesses the memory for the first time. Usually, the physical memory will be chosen “close” to the processor that thread is running on. For example, - GPU thread accesses it first: physical GPU memory of GPU that thread runs on is chosen. - CPU thread accesses it first: physical CPU memory in the memory NUMA node of the CPU core that thread runs on is chosen. CUDA Unified Memory Hint and Prefetch APIs, cudaMemAdvise and cudaMemPreftchAsync , may be used on System-Allocated Memory. These APIs are covered below in the Data Usage Hints section. __global__ void printme ( char * str ) { printf ( str ); } int main () { // Allocate 100 bytes of memory, accessible to both Host and Device code char * s = ( char * ) malloc ( 100 ); // Physical allocation placed in CPU memory because host accesses \"s\" first strncpy ( s , \"Hello Unified Memory \\\\n \" , 99 ); // Here we pass \"s\" to a kernel without explicitly copying printme >> ( s ); cudaDeviceSynchronize (); // Free as for normal CUDA allocations cudaFree ( s ); return 0 ; } 19.1.2.2. Allocation API for CUDA Managed Memory: cudaMallocManaged() \\uf0c1 On systems with CUDA Managed Memory support, unified memory may be allocated using: __host__ cudaError_t cudaMallocManaged ( void ** devPtr , size_t size ); This API is syntactically identical to cudaMalloc() : it allocates size bytes of managed memory and sets devPtr to refer to the allocation. CUDA Managed Memory is also deallocated with cudaFree() . On systems with full CUDA Managed Memory support , managed memory allocations may be accessed concurrently by all CPUs and GPUs in the system. Replacing host calls to cudaMalloc() with cudaMallocManaged() , does not impact program semantics on these systems; device code is not able to call cudaMallocManaged() . The following example shows the use of cudaMallocManaged() : __global__ void printme ( char * str ) { printf ( str ); } int main () { // Allocate 100 bytes of memory, accessible to both Host and Device code char * s ; cudaMallocManaged ( & s , 100 ); // Note direct Host-code use of \"s\" strncpy ( s , \"Hello Unified Memory \\\\n \" , 99 ); // Here we pass \"s\" to a kernel without explicitly copying printme >> ( s ); cudaDeviceSynchronize (); // Free as for normal CUDA allocations cudaFree ( s ); return 0 ; } Note For systems that support CUDA Managed Memory allocations, but do not provide full support, see Coherency and Concurrency . Implementation details (may change any time): Devices of compute capability 5.x allocate CUDA Managed Memory on the GPU. Devices of compute capability 6.x and greater populate the memory on first touch, just like System-Allocated Memory APIs. 19.1.2.3. Global-Scope Managed Variables Using __managed__ \\uf0c1 CUDA __managed__ variables behave as if they were allocated via cudaMallocManaged() (see Explicit Allocation Using cudaMallocManaged() ). They simplify programs with global variables, making it particularly easy to exchange data between host and device without manual allocations or copying. On systems with full CUDA Unified Memory support , file-scope or global-scope variables cannot be directly accessed by device code. But a pointer to these variables may be passed to the kernel as an argument, see System Allocator for examples. System Allocator __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { // Requires System-Allocated Memory support int value ; write_value >> ( & value , 1 ); // Synchronize required // (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \\\\n \" , value ); return 0 ; } Managed __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } // Requires CUDA Managed Memory support __managed__ int value ; int main () { write_value >> ( & value , 1 ); // Synchronize required // (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \\\\n \" , value ); return 0 ; } Note the absence of explicit cudaMemcpy() commands and the fact that the return array ret is visible on both CPU and GPU. CUDA __managed__ variable implies __device__ and is equivalent to __managed__ __device__ , which is also allowed.'},\n",
       " {'id': 368,\n",
       "  'content': 'Variables marked __constant__ may not be marked as __managed__ . A valid CUDA context is necessary for the correct operation of __managed__ variables. Accessing __managed__ variables can trigger CUDA context creation if a context for the current device hasn’t already been created. In the example above, accessing x before the kernel launch triggers context creation on device 0. In the absence of that access, the kernel launch would have triggered context creation. C++ objects declared as __managed__ are subject to certain specific constraints, particularly where static initializers are concerned. Please refer to C++ Language Support for a list of these constraints. Note For devices with CUDA Managed Memory without full support , visibility of __managed__ variables for asynchronous operations executing in CUDA streams is discussed in the section on Managing Data Visibility and Concurrent CPU + GPU Access with Streams . 19.1.2.4. Difference between Unified Memory and Mapped Memory \\uf0c1 The main difference between Unified Memory and CUDA Mapped Memory is that CUDA Mapped Memory does not guarantee that all kinds of memory accesses (for example atomics) are supported on all systems, while Unified Memory does. The limited set of memory operations that are guaranteed to be portably supported by CUDA Mapped Memory is available on more systems than Unified Memory. 19.1.2.5. Pointer Attributes \\uf0c1 CUDA Programs may check whether a pointer addresses a CUDA Managed Memory allocation by calling cudaPointerGetAttributes() and testing whether the pointer attribute value is cudaMemoryTypeManaged . This API returns cudaMemoryTypeHost for system-allocated memory that has been registered with cudaHostRegister() and cudaMemoryTypeUnregistered for system-allocated memory that CUDA is unaware of. Pointer attributes do not state where the memory resides, they state how the memory was allocated or registered. The following example shows how to detect the type of pointer at runtime: char const * kind ( cudaPointerAttributes a , bool pma , bool cma ) { switch ( a . type ) { case cudaMemoryTypeHost : return pma ? \"Unified: CUDA Host or Registered Memory\" : \"Not Unified: CUDA Host or Registered Memory\" ; case cudaMemoryTypeDevice : return \"Not Unified: CUDA Device Memory\" ; case cudaMemoryTypeManaged : return cma ? \"Unified: CUDA Managed Memory\" : \"Not Unified: CUDA Managed Memory\" ; case cudaMemoryTypeUnregistered : return pma ? \"Unified: System-Allocated Memory\" : \"Not Unified: System-Allocated Memory\" ; default : return \"unknown\" ; } } void check_pointer ( int i , void * ptr ) { cudaPointerAttributes attr ; cudaPointerGetAttributes ( & attr , ptr ); int pma = 0 , cma = 0 , device = 0 ; cudaGetDevice ( & device ); cudaDeviceGetAttribute ( & pma , cudaDevAttrPageableMemoryAccess , device ); cudaDeviceGetAttribute ( & cma , cudaDevAttrConcurrentManagedAccess , device ); printf ( \"Pointer %d: memory is %s \\\\n \" , i , kind ( attr , pma , cma )); } __managed__ int managed_var = 5 ; int main () { int * ptr [ 5 ]; ptr [ 0 ] = ( int * ) malloc ( sizeof ( int )); cudaMallocManaged ( & ptr [ 1 ], sizeof ( int )); cudaMallocHost ( & ptr [ 2 ], sizeof ( int )); cudaMalloc ( & ptr [ 3 ], sizeof ( int )); ptr [ 4 ] = & managed_var ; for ( int i = 0 ; i >> ( data , N ); // execute on GPU cudaMemPrefetchAsync ( data , N , cudaCpuDeviceId , s ); // prefetch to CPU cudaStreamSynchronize ( s ); use_data ( data , N ); free ( data ); } Managed void test_prefetch_managed ( cudaStream_t s ) { char * data ; cudaMallocManaged ( & data , N ); init_data ( data , N ); // execute on CPU cudaMemPrefetchAsync ( data , N , myGpuId , s ); // prefetch to GPU mykernel >> ( data , N ); // execute on GPU cudaMemPrefetchAsync ( data , N , cudaCpuDeviceId , s ); // prefetch to CPU cudaStreamSynchronize ( s ); use_data ( data , N ); cudaFree ( data ); } 19.1.2.8.2. Data Usage Hints \\uf0c1 When multiple processors simultaneously access the same data, cudaMemAdvise may be used to hint how the data at [devPtr, devPtr + count) will be accessed: cudaError_t cudaMemAdvise ( const void * devPtr , size_t count , enum cudaMemoryAdvise advice , int device ); Where advice may take the following values: cudaMemAdviseSetReadMostly : This implies that the data is mostly going to be read from and only occasionally written to. In general, it allows trading off read bandwidth for write bandwidth on this region. Example: void test_advise_managed ( cudaStream_t stream ) { char * dataPtr ; size_t dataSize = 64 * TPB ; // 16 KiB // Allocate memory using cudaMallocManaged // (malloc may be used on systems with full CUDA Unified memory support) cudaMallocManaged ( & dataPtr , dataSize ); // Set the advice on the memory region cudaMemAdvise ( dataPtr , dataSize , cudaMemAdviseSetReadMostly , myGpuId ); int outerLoopIter = 0 ; while ( outerLoopIter >> (( const char * ) dataPtr , dataSize ); innerLoopIter ++ ; } outerLoopIter ++ ; } cudaFree ( dataPtr ); } cudaMemAdviseSetPreferredLocation : In general, any memory may be migrated at any time to any location, for example, when a given processor is running out of physical memory. This hint tells the system that migrating this memory region away from its preferred location is undesired, by setting the preferred location for the data to be the physical memory belonging to device. Passing in a value of cudaCpuDeviceId for device sets the preferred location as CPU memory. Other hints, like cudaMemPrefetchAsync , may override this hint, leading the memory to be migrated away from its preferred location. cudaMemAdviseSetAccessedBy : In some systems, it may be beneficial for performance to establish a mapping into memory before accessing the data from a given processor. This hint tells the system that the data will be frequently accessed by device , enabling the system to assume that creating these mappings pays off. This hint does not imply where the data should reside, but it can be combined with cudaMemAdviseSetPreferredLocation to specify that. Each advice can be also unset by using one of the following values: cudaMemAdviseUnsetReadMostly , cudaMemAdviseUnsetPreferredLocation and cudaMemAdviseUnsetAccessedBy .'},\n",
       " {'id': 369,\n",
       "  'content': '19.1.2.8.3. Querying Data Usage Attributes on Managed Memory \\uf0c1 A program can query memory range attributes assigned through cudaMemAdvise or cudaMemPrefetchAsync on CUDA Managed Memory by using the following API: cudaMemRangeGetAttribute ( void * data , size_t dataSize , enum cudaMemRangeAttribute attribute , const void * devPtr , size_t count ); This function queries an attribute of the memory range starting at devPtr with a size of count bytes. The memory range must refer to managed memory allocated via cudaMallocManaged or declared via __managed__ variables. It is possible to query the following attributes: cudaMemRangeAttributeReadMostly : the result returned will be 1 if the entire memory range has the cudaMemAdviseSetReadMostly attribute set, or 0 otherwise. cudaMemRangeAttributePreferredLocation : the result returned will be a GPU device id or cudaCpuDeviceId if the entire memory range has the corresponding processor as preferred location, otherwise cudaInvalidDeviceId will be returned. An application can use this query API to make decision about staging data through CPU or GPU depending on the preferred location attribute of the managed pointer. Note that the actual location of the memory range at the time of the query may be different from the preferred location. cudaMemRangeAttributeAccessedBy : will return the list of devices that have that advise set for that memory range. cudaMemRangeAttributeLastPrefetchLocation : will return the last location to which the memory range was prefetched explicitly using cudaMemPrefetchAsync . Note that this simply returns the last location that the application requested to prefetch the memory range to. It gives no indication as to whether the prefetch operation to that location has completed or even begun. Additionally, multiple attributes can be queried by using corresponding cudaMemRangeGetAttributes function.'},\n",
       " {'id': 370,\n",
       "  'content': '19.2. Unified memory on devices with full CUDA Unified Memory support \\uf0c1 19.2.1. System-Allocated Memory: in-depth examples \\uf0c1 Systems with full CUDA Unified Memory support allow the device to access any memory owned by the host process interacting with the device. This section shows a few advanced use-cases, using a kernel that simply prints the first 8 characters of an input character array to the standard output stream: __global__ void kernel ( const char * type , const char * data ) { static const int n_char = 8 ; printf ( \"%s - first %d characters: \\'\" , type , n_char ); for ( int i = 0 ; i >> ( \"malloc\" , heap_data ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with \\'%s\\'\" , cudaGetErrorString ( cudaGetLastError ())); free ( heap_data ); } Managed void test_managed () { const char test_string [] = \"Hello World\" ; char * data ; cudaMallocManaged ( & data , sizeof ( test_string )); strncpy ( data , test_string , sizeof ( test_string )); kernel >> ( \"managed\" , data ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with \\'%s\\'\" , cudaGetErrorString ( cudaGetLastError ())); cudaFree ( data ); } Stack variable void test_stack () { const char test_string [] = \"Hello World\" ; kernel >> ( \"stack\" , test_string ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with \\'%s\\'\" , cudaGetErrorString ( cudaGetLastError ())); } File-scope static variable void test_static () { static const char test_string [] = \"Hello World\" ; kernel >> ( \"static\" , test_string ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with \\'%s\\'\" , cudaGetErrorString ( cudaGetLastError ())); } Global-scope variable const char global_string [] = \"Hello World\" ; void test_global () { kernel >> ( \"global\" , global_string ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with \\'%s\\'\" , cudaGetErrorString ( cudaGetLastError ())); } Global-scope extern variable // declared in separate file, see below extern char * ext_data ; void test_extern () { kernel >> ( \"extern\" , ext_data ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with \\'%s\\'\" , cudaGetErrorString ( cudaGetLastError ())); } /** This may be a non-CUDA file */ char * ext_data ; static const char global_string [] = \"Hello World\" ; void __attribute__ (( constructor )) setup ( void ) { ext_data = ( char * ) malloc ( sizeof ( global_string )); strncpy ( ext_data , global_string , sizeof ( global_string )); } void __attribute__ (( destructor )) tear_down ( void ) { free ( ext_data ); } The first three tabs above show the example as already detailed in the Programming Model section . The next three tabs show various ways a file-scope or global-scope variable can be accessed from the device.'},\n",
       " {'id': 371,\n",
       "  'content': 'Note that for the extern variable, it could be declared and its memory owned and managed by a third-party library, which does not interact with CUDA at all. Also note that stack variables as well as file-scope and global-scope variables can only be accessed through a pointer by the GPU. In this specific example, this is convenient because the character array is already declared as a pointer: const char* . However, consider the following example with a global-scope integer: // this variable is declared at global scope int global_variable ; __global__ void kernel_uncompilable () { // this causes a compilation error: global (__host__) variables must not // be accessed from __device__ / __global__ code printf ( \"%d \\\\n \" , global_variable ); } // On systems with pageableMemoryAccess set to 1, we can access the address // of a global variable. The below kernel takes that address as an argument __global__ void kernel ( int * global_variable_addr ) { printf ( \"%d \\\\n \" , * global_variable_addr ); } int main () { kernel >> ( & global_variable ); ... return 0 ; } In the example above, we need to ensure to pass a pointer to the global variable to the kernel instead of directly accessing the global variable in the kernel. This is because global variables without the __managed__ specifier are declared as __host__ -only by default, thus most compilers won’t allow using these variables directly in device code as of now. 19.2.1.1. File-backed Unified Memory \\uf0c1 Since systems with full CUDA Unified Memory support allow the device to access any memory owned by the host process, they can directly access file-backed memory. Here, we show a modified version of the initial example shown in the previous section to use file-backed memory in order to print a string from the GPU, read directly from an input file. In the following example, the memory is backed by a physical file, but the example applies to memory-backed files, too, as shown in the section on Inter-Process Communication with Unified Memory . __global__ void kernel ( const char * type , const char * data ) { static const int n_char = 8 ; printf ( \"%s - first %d characters: \\'\" , type , n_char ); for ( int i = 0 ; i = 0 , \"Invalid file handle\" ); struct stat file_stat ; int status = fstat ( fd , & file_stat ); ASSERT ( status >= 0 , \"Invalid file stats\" ); char * mapped = ( char * ) mmap ( 0 , file_stat . st_size , PROT_READ , MAP_PRIVATE , fd , 0 ); ASSERT ( mapped != MAP_FAILED , \"Cannot map file into memory\" ); kernel >> ( \"file-backed\" , mapped ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with \\'%s\\'\" , cudaGetErrorString ( cudaGetLastError ())); ASSERT ( munmap ( mapped , file_stat . st_size ) == 0 , \"Cannot unmap file\" ); ASSERT ( close ( fd ) == 0 , \"Cannot close file\" ); } Note that on systems without the hostNativeAtomicSupported property, including systems with Linux HMM enabled , atomic accesses to file-backed memory are not supported. 19.2.1.2.'},\n",
       " {'id': 372,\n",
       "  'content': 'Inter-Process Communication (IPC) with Unified Memory \\uf0c1 Note As of now, using IPC with Unified Memory can have significant performance implications. Many applications prefer to manage one GPU per process, but still need to use Unified Memory, for example for over-subscription, and access it from multiple GPUs. CUDA IPC (see Interprocess Communication ) does not support Managed Memory: handles to this type of memory may not be shared through any of the mechanisms discussed in this section. On systems with full CUDA Unified Memory support , System-Allocated Memory is Inter-Process Communication (IPC) capable. Once access to System-Allocated Memory has been shared with other processes, the same Unified Memory Programming Model applies, similar to File-backed Unified Memory . See the following references for more information on various ways of creating IPC-capable System-Allocated Memory under Linux: mmap with MAP_SHARED POSIX IPC APIs Linux memfd_create Note that it is not possible to share memory between different hosts and their devices using this technique. 19.2.2. Performance Tuning \\uf0c1 In order to achieve good performance with Unified Memory, it is important to: Understand how paging works on your system, and how to avoid unnecessary page faults. Understand the various mechanisms allowing to keep data local to the accessing processor. Consider tuning your application for the granularity of memory transfers of your system. As general advice, Unified Memory Performance Hints might provide improved performance, but using them incorrectly might degrade performance compared to the default behavior. Also note that any hint has a performance cost associated with it on the host, thus useful hints must at the very least improve performance enough to overcome this cost. 19.2.2.1. Memory Paging and Page Sizes \\uf0c1 Many of the sections for unified memory performance tuning assume prior knowledge on virtual addressing, memory pages and page sizes. This section attempts to define all necessary terms and explain why paging matters for performance. All currently supported systems for Unified Memory use a virtual address space: this means that memory addresses used by an application represent a virtual location which might be mapped to a physical location where the memory actually resides. All currently supported processors, including both CPUs and GPUs, additionally use memory paging . Because all systems use a virtual address space, there are two types of memory pages: Virtual pages: this represents a fixed-size contiguous chunk of virtual memory per process tracked by the operating system, which can be mapped into physical memory. Note that the virtual page is linked to the mapping : for example, a single virtual address might be mapped into physical memory using different page sizes. Physical pages: this represents a fixed-size contiguous chunk of memory the processor’s main Memory Management Unit (MMU) supports and into which a virtual page can be mapped. Currently, all x86_64 CPUs use 4KiB physical pages. Arm CPUs support multiple physical page sizes - 4KiB, 16KiB, 32KiB and 64KiB - depending on the exact CPU. Finally, NVIDIA GPUs support multiple physical page sizes, but prefer 2MiB physical pages or larger. Note that these sizes are subject to change in future hardware. The default page size of virtual pages usually corresponds to the physical page size, but an application may use different page sizes as long as they are supported by the operating system and the hardware. Typically, supported virtual page sizes must be powers of 2 and multiples of the physical page size. The logical entity tracking the mapping of virtual pages into physical pages will be referred to as a page table , and each mapping of a given virtual page with a given virtual size to physical pages is called a page table entry (PTE) . All supported processors provide specific caches for the page table to speed up the translation of virtual addresses to physical addresses. These caches are called translation lookaside buffers (TLBs) . There are two important aspects for performance tuning of applications: the choice of virtual page size, whether the system offers a combined page table used by both CPUs and GPUs, or separate page tables for each CPU and GPU individually. 19.2.2.1.1. Choosing the right page size \\uf0c1 In general, small page sizes lead to less (virtual) memory fragmentation but more TLB misses, whereas larger page sizes lead to more memory fragmentation but less TLB misses. Additionally, memory migration is generally more expensive with larger page sizes compared to smaller page sizes, because we typically migrate full memory pages. This can cause larger latency spikes in an application using large page sizes. See also the next section for more details on page faults. One important aspect for performance tuning is that TLB misses are generally significantly more expensive on the GPU compared to the CPU. This means that if a GPU thread frequently accesses random locations of Unified Memory mapped using a small enough page size, it might be significantly slower compared to the same accesses to Unified Memory mapped using a large enough page size. While a similar effect might occur for a CPU thread randomly accessing a large area of memory mapped using a small page size, the slowdown is less pronounced, meaning that the application might want to trade-off this slowdown with having less memory fragmentation. Note that in general, applications should not tune their performance to the physical page size of a given processor, since physical page sizes are subject to change depending on the hardware. The advice above only applies to virtual page sizes. 19.2.2.1.2. CPU and GPU page tables: hardware coherency vs. software coherency \\uf0c1 Note In the remainder of the performance tuning documentation, we will refer to systems with a combined page table for both CPUs and GPUs as hardware coherent systems. Systems with separate page tables for CPUs and GPUs are referred to as software coherent . Hardware coherent systems such as NVIDIA Grace Hopper offer a logically combined page table for both CPUs and GPUs. This is important because in order to access System-Allocated Memory from the GPU , the GPU uses whichever page table entry was created by the CPU for the requested memory. If that page table entry uses the default CPU page size of 4KiB or 64KiB, accesses to large virtual memory areas will cause significant TLB misses, thus significant slowdowns. See the section on configuring huge pages for examples on how to ensure System-Allocated Memory uses large enough page sizes to avoid this type of issue. On the other hand, on systems where the CPUs and GPUs each have their own logical page table, different performance tuning aspects should be considered: in order to guarantee coherency , these systems usually use page faults in case a processor accesses a memory address mapped into the physical memory of a different processor. Such a page fault means that: it needs to be ensured that the currently owning processor (where the physical page currently resides) cannot access this page anymore, either by deleting the page table entry or updating it. it needs to be ensured that the processor requesting access can access this page, either by creating a new page table entry or updating and existing entry, such that it becomes valid/active. the physical page backing this virtual page must be moved/migrated to the processor requesting access: this can be an expensive operation, and the amount of work is proportional to the page size. Overall, hardware coherent systems provide significant performance benefits compared to software coherent systems in cases where frequent concurrent accesses to the same memory page are made by both CPU and GPU threads: less page-faults: these systems do not need to use page-faults for emulating coherency or migrating memory, less contention: these systems are coherent at cache-line granularity instead of page-size granularity, that is, when there is contention from multiple processors within a cache line, only the cache line is exchanged which is much smaller than the smallest page-size, and when the different processors access different cache-lines within a page, then there is no contention. This impacts the performance of the following scenarios: Atomic updates to the same address concurrently from both CPUs and GPUs. Signaling a GPU thread from a CPU thread or vice-versa. 19.2.2.2. Direct Unified Memory Access from host \\uf0c1 Some devices have hardware support for coherent reads, stores and atomic accesses from the host on GPU-resident unified memory. These devices have the attribute cudaDevAttrDirectManagedMemAccessFromHost set to 1. Note that all hardware coherent systems have this attribute set for NVLink-connected devices. On these systems, the host has direct access to GPU-resident memory without page faults and data migration (see Data Usage Hints for more details on memory usage hints). Note that with CUDA Managed Memory, the cudaMemAdviseSetAccessedBy hint with cudaCpuDeviceId is necessary to enable this direct access without page faults. Consider an example code below: System Allocator __global__ void write ( int * ret , int a , int b ) { ret [ threadIdx .'},\n",
       " {'id': 373,\n",
       "  'content': 'x ] = a + b + threadIdx . x ; } __global__ void append ( int * ret , int a , int b ) { ret [ threadIdx .'},\n",
       " {'id': 374,\n",
       "  'content': 'x ] += a + b + threadIdx . x ; } void test_malloc () { int * ret = ( int * ) malloc ( 1000 * sizeof ( int )); // for shared page table systems, the following hint is not necesary cudaMemAdvise ( ret , 1000 * sizeof ( int ), cudaMemAdviseSetAccessedBy , cudaCpuDeviceId ); write >> ( ret , 10 , 100 ); // pages populated in GPU memory cudaDeviceSynchronize (); for ( int i = 0 ; i >> ( ret , 10 , 100 ); // directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations cudaDeviceSynchronize (); // directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations free ( ret ); } Managed __global__ void write ( int * ret , int a , int b ) { ret [ threadIdx . x ; } void test_managed () { int * ret ; cudaMallocManaged ( & ret , 1000 * sizeof ( int )); cudaMemAdvise ( ret , 1000 * sizeof ( int ), cudaMemAdviseSetAccessedBy , cudaCpuDeviceId ); // set direct access hint write >> ( ret , 10 , 100 ); // pages populated in GPU memory cudaDeviceSynchronize (); for ( int i = 0 ; i >> ( ret , 10 , 100 ); // directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations cudaDeviceSynchronize (); // directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations cudaFree ( ret ); } After write kernel is completed, ret will be created and initialized in GPU memory. Next, the CPU will access ret followed by append kernel using the same ret memory again. This code will show different behavior depending on the system architecture and support of hardware coherency: On systems with directManagedMemAccessFromHost=1 : CPU accesses to the managed buffer will not trigger any migrations; the data will remain resident in GPU memory and any subsequent GPU kernels can continue to access it directly without inflicting faults or migrations. On systems with directManagedMemAccessFromHost=0 : CPU accesses to the managed buffer will page fault and initiate data migration; any GPU kernel trying to access the same data first time will page fault and migrate pages back to GPU memory. 19.2.2.3. Host Native Atomics \\uf0c1 Some devices, including NVLink-connected devices in hardware coherent systems , support hardware-accelerated atomic accesses to CPU-resident memory. This implies that atomic accesses to host memory do not have to be emulated with a page fault. For these devices, the attribute cudaDevAttrHostNativeAtomicSupported is set to 1. 19.3. Unified memory on devices without full CUDA Unified Memory support \\uf0c1 19.3.1. Unified memory on devices with only CUDA Managed Memory support \\uf0c1 For devices with compute capability 6.x or higher but without pageable memory access , CUDA Managed Memory is fully supported and coherent. The programming model and performance tuning of unified memory is largely similar to the model as described in Unified memory on devices with full CUDA Unified Memory support , with the notable exception that system allocators cannot be used to allocate memory. Thus, the following list of sub-sections do not apply: System Allocator Hardware/Software Coherency 19.3.2. Unified memory on Windows or devices with compute capability 5.x \\uf0c1 Devices with compute capability lower than 6.0 or Windows platforms support CUDA Managed Memory v1.0 with limited support for data migration and coherency as well as memory oversubscription. The following sub-sections describe in more detail how to use and optimize Managed Memory on these platforms. 19.3.2.1. Data Migration and Coherency \\uf0c1 GPU architectures of compute capability lower than 6.0 do not support fine-grained movement of the managed data to GPU on-demand. Whenever a GPU kernel is launched all managed memory generally has to be transferred to GPU memory to avoid faulting on memory access. With compute capability 6.x a new GPU page faulting mechanism is introduced that provides more seamless Unified Memory functionality. Combined with the system-wide virtual address space, page faulting provides several benefits. First, page faulting means that the CUDA system software doesn’t need to synchronize all managed memory allocations to the GPU before each kernel launch. If a kernel running on the GPU accesses a page that is not resident in its memory, it faults, allowing the page to be automatically migrated to the GPU memory on-demand. Alternatively, the page may be mapped into the GPU address space for access over the PCIe or NVLink interconnects (mapping on access can sometimes be faster than migration). Note that Unified Memory is system-wide: GPUs (and CPUs) can fault on and migrate memory pages either from CPU memory or from the memory of other GPUs in the system. 19.3.2.2. GPU Memory Oversubscription \\uf0c1 Devices of compute capability lower than 6.0 cannot allocate more managed memory than the physical size of GPU memory. 19.3.2.3. Multi-GPU \\uf0c1 On systems with devices of compute capabilities lower than 6.0 managed allocations are automatically visible to all GPUs in a system via the peer-to-peer capabilities of the GPUs. Managed memory allocations behave similar to unmanaged memory allocated using cudaMalloc() : the current active device is the home for the physical allocation but other GPUs in the system will access the memory at reduced bandwidth over the PCIe bus. On Linux the managed memory is allocated in GPU memory as long as all GPUs that are actively being used by a program have the peer-to-peer support. If at any time the application starts using a GPU that doesn’t have peer-to-peer support with any of the other GPUs that have managed allocations on them, then the driver will migrate all managed allocations to system memory. In this case, all GPUs experience PCIe bandwidth restrictions. On Windows, if peer mappings are not available (for example, between GPUs of different architectures), then the system will automatically fall back to using zero-copy memory, regardless of whether both GPUs are actually used by a program. If only one GPU is actually going to be used, it is necessary to set the CUDA_VISIBLE_DEVICES environment variable before launching the program. This constrains which GPUs are visible and allows managed memory to be allocated in GPU memory. Alternatively, on Windows users can also set CUDA_MANAGED_FORCE_DEVICE_ALLOC to a non-zero value to force the driver to always use device memory for physical storage. When this environment variable is set to a non-zero value, all devices used in that process that support managed memory have to be peer-to-peer compatible with each other. The error ::cudaErrorInvalidDevice will be returned if a device that supports managed memory is used and it is not peer-to-peer compatible with any of the other managed memory supporting devices that were previously used in that process, even if ::cudaDeviceReset has been called on those devices. These environment variables are described in CUDA Environment Variables . Note that starting from CUDA 8.0 CUDA_MANAGED_FORCE_DEVICE_ALLOC has no effect on Linux operating systems. 19.3.2.4. Coherency and Concurrency \\uf0c1 Simultaneous access to managed memory on devices of compute capability lower than 6.0 is not possible, because coherence could not be guaranteed if the CPU accessed a Unified Memory allocation while a GPU kernel was active. 19.3.2.4.1. GPU Exclusive Access To Managed Memory \\uf0c1 To ensure coherency on pre-6.x GPU architectures, the Unified Memory programming model puts constraints on data accesses while both the CPU and GPU are executing concurrently. In effect, the GPU has exclusive access to all managed data while any kernel operation is executing, regardless of whether the specific kernel is actively using the data. When managed data is used with cudaMemcpy*() or cudaMemset*() , the system may choose to access the source or destination from the host or the device, which will put constraints on concurrent CPU access to that data while the cudaMemcpy*() or cudaMemset*() is executing. See Memcpy()/Memset() Behavior With Managed Memory for further details. It is not permitted for the CPU to access any managed allocations or variables while the GPU is active for devices with concurrentManagedAccess property set to 0. On these systems concurrent CPU/GPU accesses, even to different managed memory allocations, will cause a segmentation fault because the page is considered inaccessible to the CPU. __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { kernel >> (); y = 20 ; // Error on GPUs not supporting concurrent access cudaDeviceSynchronize (); return 0 ; } In example above, the GPU program kernel is still active when the CPU touches y . (Note how it occurs before cudaDeviceSynchronize() .)\\nThe code runs successfully on devices of compute capability 6.x due to the GPU page faulting capability which lifts all restrictions on simultaneous access. However, such memory access is invalid on pre-6.x architectures even though the CPU is accessing different data than the GPU. The program must explicitly synchronize with the GPU before accessing y : __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { kernel >> (); cudaDeviceSynchronize (); y = 20 ; // Success on GPUs not supporing concurrent access return 0 ; } As this example shows, on systems with pre-6.x GPU architectures, a CPU thread may not access any managed data in between performing a kernel launch and a subsequent synchronization call, regardless of whether the GPU kernel actually touches that same data (or any managed data at all). The mere potential for concurrent CPU and GPU access is sufficient for a process-level exception to be raised. Note that if memory is dynamically allocated with cudaMallocManaged() or cuMemAllocManaged() while the GPU is active, the behavior of the memory is unspecified until additional work is launched or the GPU is synchronized. Attempting to access the memory on the CPU during this time may or may not cause a segmentation fault. This does not apply to memory allocated using the flag cudaMemAttachHost or CU_MEM_ATTACH_HOST . 19.3.2.4.2. Explicit Synchronization and Logical GPU Activity \\uf0c1 Note that explicit synchronization is required even if kernel runs quickly and finishes before the CPU touches y in the above example. Unified Memory uses logical activity to determine whether the GPU is idle. This aligns with the CUDA programming model, which specifies that a kernel can run at any time following a launch and is not guaranteed to have finished until the host issues a synchronization call. Any function call that logically guarantees the GPU completes its work is valid. This includes cudaDeviceSynchronize() ; cudaStreamSynchronize() and cudaStreamQuery() (provided it returns cudaSuccess and not cudaErrorNotReady ) where the specified stream is the only stream still executing on the GPU; cudaEventSynchronize() and cudaEventQuery() in cases where the specified event is not followed by any device work; as well as uses of cudaMemcpy() and cudaMemset() that are documented as being fully synchronous with respect to the host. Dependencies created between streams will be followed to infer completion of other streams by synchronizing on a stream or event. Dependencies can be created via cudaStreamWaitEvent() or implicitly when using the default (NULL) stream. It is legal for the CPU to access managed data from within a stream callback, provided no other stream that could potentially be accessing managed data is active on the GPU. In addition, a callback that is not followed by any device work can be used for synchronization: for example, by signaling a condition variable from inside the callback; otherwise, CPU access is valid only for the duration of the callback(s). There are several important points of note: It is always permitted for the CPU to access non-managed zero-copy data while the GPU is active. The GPU is considered active when it is running any kernel, even if that kernel does not make use of managed data. If a kernel might use data, then access is forbidden, unless device property concurrentManagedAccess is 1. There are no constraints on concurrent inter-GPU access of managed memory, other than those that apply to multi-GPU access of non-managed memory. There are no constraints on concurrent GPU kernels accessing managed data. Note how the last point allows for races between GPU kernels, as is currently the case for non-managed GPU memory. As mentioned previously, managed memory functions identically to non-managed memory from the perspective of the GPU. The following code example illustrates these points: int main () { cudaStream_t stream1 , stream2 ; cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); int * non_managed , * managed , * also_managed ; cudaMallocHost ( & non_managed , 4 ); // Non-managed, CPU-accessible memory cudaMallocManaged ( & managed , 4 ); cudaMallocManaged ( & also_managed , 4 ); // Point 1: CPU can access non-managed data. kernel >> ( managed ); * non_managed = 1 ; // Point 2: CPU cannot access any managed data while GPU is busy, // unless concurrentManagedAccess = 1 // Note we have not yet synchronized, so \"kernel\" is still active. * also_managed = 2 ; // Will issue segmentation fault // Point 3: Concurrent GPU kernels can access the same data. kernel >> ( managed ); // Point 4: Multi-GPU concurrent access is also permitted. cudaSetDevice ( 1 ); kernel >> ( managed ); return 0 ; } 19.3.2.4.3. Managing Data Visibility and Concurrent CPU + GPU Access with Streams \\uf0c1 Until now it was assumed that for SM architectures before 6.x: 1) any active kernel may use any managed memory, and 2) it was invalid to use managed memory from the CPU while a kernel is active. Here we present a system for finer-grained control of managed memory designed to work on all devices supporting managed memory, including older architectures with concurrentManagedAccess equal to 0. The CUDA programming model provides streams as a mechanism for programs to indicate dependence and independence among kernel launches. Kernels launched into the same stream are guaranteed to execute consecutively, while kernels launched into different streams are permitted to execute concurrently. Streams describe independence between work items and hence allow potentially greater efficiency through concurrency. Unified Memory builds upon the stream-independence model by allowing a CUDA program to explicitly associate managed allocations with a CUDA stream. In this way, the programmer indicates the use of data by kernels based on whether they are launched into a specified stream or not. This enables opportunities for concurrency based on program-specific data access patterns. The function to control this behavior is: cudaError_t cudaStreamAttachMemAsync ( cudaStream_t stream , void * ptr , size_t length = 0 , unsigned int flags = 0 ); The cudaStreamAttachMemAsync() function associates length bytes of memory starting from ptr with the specified stream . (Currently, length must always be 0 to indicate that the entire region should be attached.)\\nBecause of this association, the Unified Memory system allows CPU access to this memory region so long as all operations in stream have completed, regardless of whether other streams are active. In effect, this constrains exclusive ownership of the managed memory region by an active GPU to per-stream activity instead of whole-GPU activity. Most importantly, if an allocation is not associated with a specific stream, it is visible to all running kernels regardless of their stream. This is the default visibility for a cudaMallocManaged() allocation or a __managed__ variable; hence, the simple-case rule that the CPU may not touch the data while any kernel is running. By associating an allocation with a specific stream, the program makes a guarantee that only kernels launched into that stream will touch that data. No error checking is performed by the Unified Memory system: it is the programmer’s responsibility to ensure that guarantee is honored. In addition to allowing greater concurrency, the use of cudaStreamAttachMemAsync() can (and typically does) enable data transfer optimizations within the Unified Memory system that may affect latencies and other overhead. 19.3.2.4.4. Stream Association Examples \\uf0c1 Associating data with a stream allows fine-grained control over CPU + GPU concurrency, but what data is visible to which streams must be kept in mind when using devices of compute capability lower than 6.0. Looking at the earlier synchronization example: __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { cudaStream_t stream1 ; cudaStreamCreate ( & stream1 ); cudaStreamAttachMemAsync ( stream1 , & y , 0 , cudaMemAttachHost ); cudaDeviceSynchronize (); // Wait for Host attachment to occur. kernel >> (); // Note: Launches into stream1. y = 20 ; // Success – a kernel is running but “y” // has been associated with no stream. return 0 ; } Here we explicitly associate y with host accessibility, thus enabling access at all times from the CPU. (As before, note the absence of cudaDeviceSynchronize() before the access.)\\nAccesses to y by the GPU running kernel will now produce undefined results. Note that associating a variable with a stream does not change the associating of any other variable. For example, associating x with stream1 does not ensure that only x is accessed by kernels launched in stream1 , thus an error is caused by this code: __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { cudaStream_t stream1 ; cudaStreamCreate ( & stream1 ); cudaStreamAttachMemAsync ( stream1 , & x ); // Associate “x” with stream1. cudaDeviceSynchronize (); // Wait for “x” attachment to occur. y = 20 ; // ERROR: “y” is still associated globally // with all streams by default return 0 ; } Note how the access to y will cause an error because, even though x has been associated with a stream, we have told the system nothing about who can see y . The system therefore conservatively assumes that kernel might access it and prevents the CPU from doing so.'},\n",
       " {'id': 375,\n",
       "  'content': '19.3.2.4.5. Stream Attach With Multithreaded Host Programs \\uf0c1 The primary use for cudaStreamAttachMemAsync() is to enable independent task parallelism using CPU threads. Typically in such a program, a CPU thread creates its own stream for all work that it generates because using CUDA’s NULL stream would cause dependencies between threads. The default global visibility of managed data to any GPU stream can make it difficult to avoid interactions between CPU threads in a multi-threaded program. Function cudaStreamAttachMemAsync() is therefore used to associate a thread’s managed allocations with that thread’s own stream, and the association is typically not changed for the life of the thread. Such a program would simply add a single call to cudaStreamAttachMemAsync() to use unified memory for its data accesses: // This function performs some task, in its own private stream. void run_task ( int * in , int * out , int length ) { // Create a stream for us to use. cudaStream_t stream ; cudaStreamCreate ( & stream ); // Allocate some managed data and associate with our stream. // Note the use of the host-attach flag to cudaMallocManaged(); // we then associate the allocation with our stream so that // our GPU kernel launches can access it. int * data ; cudaMallocManaged (( void ** ) & data , length , cudaMemAttachHost ); cudaStreamAttachMemAsync ( stream , data ); cudaStreamSynchronize ( stream ); // Iterate on the data in some way, using both Host & Device. for ( int i = 0 ; i >> ( in , data , length ); cudaStreamSynchronize ( stream ); host_process ( data , length ); // CPU uses managed data. convert >> ( out , data , length ); } cudaStreamSynchronize ( stream ); cudaStreamDestroy ( stream ); cudaFree ( data ); } In this example, the allocation-stream association is established just once, and then data is used repeatedly by both the host and device. The result is much simpler code than occurs with explicitly copying data between host and device, although the result is the same. 19.3.2.4.6. Advanced Topic: Modular Programs and Data Access Constraints \\uf0c1 In the previous example cudaMallocManaged() specifies the cudaMemAttachHost flag, which creates an allocation that is initially invisible to device-side execution. (The default allocation would be visible to all GPU kernels on all streams.)\\nThis ensures that there is no accidental interaction with another thread’s execution in the interval between the data allocation and when the data is acquired for a specific stream. Without this flag, a new allocation would be considered in-use on the GPU if a kernel launched by another thread happens to be running. This might impact the thread’s ability to access the newly allocated data from the CPU (for example, within a base-class constructor) before it is able to explicitly attach it to a private stream. To enable safe independence between threads, therefore, allocations should be made specifying this flag. Note An alternative would be to place a process-wide barrier across all threads after the allocation has been attached to the stream. This would ensure that all threads complete their data/stream associations before any kernels are launched, avoiding the hazard. A second barrier would be needed before the stream is destroyed because stream destruction causes allocations to revert to their default visibility. The cudaMemAttachHost flag exists both to simplify this process, and because it is not always possible to insert global barriers where required. 19.3.2.4.7. Memcpy()/Memset() Behavior With Stream-associated Unified Memory \\uf0c1 See Memcpy()/Memset() Behavior With Unified Memory for a general overview of cudaMemcpy* / cudaMemset* behavior on devices with concurrentManagedAccess set. On devices where concurrentManagedAccess is not set, the following rules apply: If cudaMemcpyHostTo* is specified and the source data is unified memory, then it will be accessed from the host if it is coherently accessible from the host in the copy stream (1) ; otherwise it will be accessed from the device. Similar rules apply to the destination when cudaMemcpy*ToHost is specified and the destination is unified memory. If cudaMemcpyDeviceTo* is specified and the source data is unified memory, then it will be accessed from the device. The source must be coherently accessible from the device in the copy stream (2) ; otherwise, an error is returned. Similar rules apply to the destination when cudaMemcpy*ToDevice is specified and the destination is unified memory. If cudaMemcpyDefault is specified, then unified memory will be accessed from the host either if it cannot be coherently accessed from the device in the copy stream (2) or if the preferred location for the data is cudaCpuDeviceId and it can be coherently accessed from the host in the copy stream (1) ; otherwise, it will be accessed from the device. When using cudaMemset*() with unified memory, the data must be coherently accessible from the device in the stream being used for the cudaMemset() operation (2) ; otherwise, an error is returned. When data is accessed from the device either by cudaMemcpy* or cudaMemset* , the stream of operation is considered to be active on the GPU. During this time, any CPU access of data that is associated with that stream or data that has global visibility, will result in a segmentation fault if the GPU has a zero value for the device attribute concurrentManagedAccess . The program must synchronize appropriately to ensure the operation has completed before accessing any associated data from the CPU. Coherently accessible from the host in a given stream means that the memory neither has global visibility nor is it associated with the given stream. Coherently accessible from the device in a given stream means that the memory either has global visibility or is associated with the given stream. 20.'},\n",
       " {'id': 376,\n",
       "  'content': 'Lazy Loading \\uf0c1 20.1. \\uf0c1 Lazy Loading delays loading of CUDA modules and kernels from program initalization closer to kernels execution. If a program does not use every single kernel it has included, then some kernels will be loaded unneccesarily. This is very common, especially if you include any libraries. Most of the time, programs only use a small amount of kernels from libraries they include. Thanks to Lazy Loading, programs are able to only load kernels they are actually going to use, saving time on initialization. This reduces memory overhead, both on GPU memory and host memory. Lazy Loading is enabled by setting the CUDA_MODULE_LOADING environment variable to LAZY . Firstly, CUDA Runtime will no longer load all modules during program initialization, with the exception of modules containing managed variables. Each module will be loaded on first usage of a variable or a kernel from that module. This optimization is only relevant to CUDA Runtime users, CUDA Driver users who use cuModuleLoad are unaffected. This optimization shipped in CUDA 11.8. The behavior for CUDA Driver users who use cuLibraryLoad to load module data into memory can be changed by setting the CUDA_MODULE_DATA_LOADING environment variable. Secondly, loading a module ( cuModuleLoad*() family of functions) will not be loading kernels immediately, instead it will delay loading of a kernel until cuModuleGetFunction() is called. There are certain exceptions here, some kernels have to be loaded during cuModuleLoad*() , such as kernels of which pointers are stored in global variables. This optimization is relevant to both CUDA Runtime and CUDA Driver users. CUDA Runtime will only call cuModuleGetFunction() when a kernel is used/referenced for the first time. This optimization shipped in CUDA 11.7. Both of these optimizations are designed to be invisible to the user, assuming CUDA Programming Model is followed. Lazy Loading version support \\uf0c1 Lazy Loading is a CUDA Runtime and CUDA Driver feature. Upgrades to both might be necessary to utilize the feature. 20.2.1. Driver \\uf0c1 Lazy Loading requires R515+ user-mode library, but it supports Forward Compatibility, meaning it can run on top of older kernel mode drivers. Without R515+ user-mode library, Lazy Loading is not available in any shape or form, even if toolkit version is 11.7+. 20.2.2. Toolkit \\uf0c1 Lazy Loading was introduced in CUDA 11.7, and received a significant upgrade in CUDA 11.8. If your application uses CUDA Runtime, then in order to see benefits from Lazy Loading your application must use 11.7+ CUDA Runtime. As CUDA Runtime is usually linked statically into programs and libraries, this means that you have to recompile your program with CUDA 11.7+ toolkit and use CUDA 11.7+ libraries. Otherwise you will not see the benefits of Lazy Loading, even if your driver version supports it. If only some of your libraries are 11.7+, you will only see benefits of Lazy Loading in those libraries. Other libraries will still load everything eagerly. 20.2.3. Compiler \\uf0c1 Lazy Loading does not require any compiler support. Both SASS and PTX compiled with pre-11.7 compilers can be loaded with Lazy Loading enabled, and will see full benefits of the feature. However, 11.7+ CUDA Runtime is still required, as described above. 20.3. Triggering loading of kernels in lazy mode \\uf0c1 Loading kernels and variables happens automatically, without any need for explicit loading. Simply launching a kernel or referencing a variable or a kernel will automatically load relevant modules and kernels. However, if for any reason you wish to load a kernel without executing it or modifying it in any way, we recommend the following. 20.3.1. CUDA Driver API \\uf0c1 Loading of kernels happens during cuModuleGetFunction() call. This call is necessary even without Lazy Loading, as it is the only way to obtain a kernel handle. However, you can also use this API to control with finer granularity when kernels are loaded.'},\n",
       " {'id': 377,\n",
       "  'content': '20.3.2. CUDA Runtime API \\uf0c1 CUDA Runtime API manages module management automatically, so we recommend simply using cudaFuncGetAttributes() to reference the kernel. This will ensure that the kernel is loaded without changing the state.'},\n",
       " {'id': 378,\n",
       "  'content': '20.4. Querying whether Lazy Loading is turned on \\uf0c1 In order to check whether user enabled Lazy Loading, CUresult cuModuleGetLoadingMode ( CUmoduleLoadingMode* mode ) can be used. It’s important to note that CUDA must be initialized before running this function. Sample usage can be seen in the snippet below. #include \"cuda.h\" #include \"assert.h\" #include \"iostream\" int main () { CUmoduleLoadingMode mode ; assert ( CUDA_SUCCESS == cuInit ( 0 )); assert ( CUDA_SUCCESS == cuModuleGetLoadingMode ( & mode )); std :: cout << \"CUDA Module Loading Mode is \" << (( mode == CU_MODULE_LAZY_LOADING ) ? \"lazy\" : \"eager\" ) << std :: endl ; return 0 ; } 20.5. Possible issues when adopting lazy loading \\uf0c1 Lazy Loading is designed so that it should not require any modifications to applications to use it. That said, there are some caveats, especially when applications are not fully compliant with CUDA Programming Model. 20.5.1. Concurrent execution \\uf0c1 Loading kernels might require context synchronization. Some programs incorrectly treat the possibility of concurrent execution of kernels as a guarantee. In such cases, if program assumes that two kernels will be able to execute concurrently, and one of the kernels will not return without the other kernel executing, there is a possibility of a deadlock. If kernel A will be spinning in an infinite loop until kernel B is executing. In such case launching kernel B will trigger lazy loading of kernel B. If this loading will require context synchronization, then we have a deadlock: kernel A is waiting for kernel B, but loading kernel B is stuck waiting for kernel A to finish to synchronize the context. Such program is an anti-pattern, but if for any reason you want to keep it you can do the following: preload all kernels that you hope to execute concurrently prior to launching them run application with CUDA_MODULE_DATA_LOADING=EAGER to force loading data eagerly without forcing each function to load eagerly 20.5.2. Allocators \\uf0c1 Lazy Loading delays loading code from initialization phase of the program closer to execution phase. Loading code onto the GPU requires memory allocation. If your application tries to allocate the entire VRAM on startup, e.g. to use it for its own allocator, then it might turn out that there will be no more memory left to load the kernels. This is despite the fact that overall Lazy Loading frees up more memory for the user. CUDA will need to allocate some memory to load each kernel, which usually happens at first launch time of each kernel. If your application allocator greedily allocated everything, CUDA will fail to allocate memory. Possible solutions: use cudaMallocAsync() instead of an allocator that allocates the entire VRAM on startup add some buffer to compensate for the delayed loading of kernels preload all kernels that will be used in the program before trying to initialize your allocator 20.5.3. Autotuning \\uf0c1 Some applications launch several kernels implementing the same functionality to determine which one is the fastest. While it is overall advisable to run at least one warmup iteration, it becomes especially important with Lazy Loading. After all, including time taken to load the kernel will skew your results. Possible solutions: do at least one warmup interaction prior to measurement preload the benchmarked kernel prior to launching it 21. Extended GPU Memory \\uf0c1 The Extended GPU Memory (EGM) feature, utilizing the high-bandwidth NVLink-C2C, facilitates efficient access to all system memory by GPUs, in a single-node system. EGM applies to integrated CPU-GPU NVIDIA systems by allowing physical memory allocation that can be accessed from any GPU thread within the setup. EGM ensures that all GPUs can access its resources at the speed of either GPU-GPU NVLink or NVLink-C2C. In this setup, memory accesses occur via the local high-bandwidth NVLink-C2C. For remote memory accesses, GPU NVLink and, in some cases, NVLink-C2C are used. With EGM, GPU threads gain the capability to access all available memory resources, including CPU attached memory and HBM3, over the NVSwitch fabric. 21.1. Preliminaries \\uf0c1 Before diving into API changes for EGM functionalities, we are going to cover currently supported topologies, identifier assignment, prerequisites for virtual memory management, and CUDA types for EGM. 21.1.1. EGM Platforms: System topology \\uf0c1 Currently, EGM can be enabled in three platforms: (1) Single-Node, Single-GPU : Consists of an Arm-based CPU, CPU attached memory, and a GPU. Between the CPU and the GPU there is a high bandwidth C2C (Chip-to-Chip) interconnect. (2) Single-Node, Multi-GPU : Consists of fully connected four single-node, single-GPU platforms. (3) Multi-Node, Single-GPU : Two or more single-node multi-socket systems. Note Using cgroups to limit available devices will block routing over EGM and cause performance issues. Use CUDA_VISIBLE_DEVICES instead. 21.1.2. \\uf0c1 NUMA (Non-Uniform Memory Access) is a memory architecture used in multi-processor computer systems such that the memory is divided into multiple nodes. Each node has its own processors and memory. In such a system, NUMA divides the system into nodes and assigns a unique identifier ( numaID ) to every node. EGM uses the NUMA node identifier which is assigned by the operating system. Note that, this identifier is different from the ordinal of a device and it is associated with the closest host node. In addition to the existing methods, the user can obtain the identifier of the host node ( numaID ) by calling cuDeviceGetAttribute with CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID attribute type as follows: int numaId ; cuDeviceGetAttribute ( & numaId , CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID , deviceOrdinal ); 21.1.3. Allocators and EGM support \\uf0c1 Mapping system memory as EGM does not cause any performance issues. In fact, accessing a remote socket’s system memory mapped as EGM is going to be faster. Because, with EGM traffic is guaranteed to be routed over NVLinks. Currently, cuMemCreate and cudaMemPoolCreate allocators are supported with appropriate location type and NUMA identifiers. 21.1.4. Memory management extensions to current APIs \\uf0c1 Currently, EGM memory can be mapped with Virtual Memory ( cuMemCreate ) or Stream Ordered Memory ( cudaMemPoolCreate ) allocators. The user is responsible for allocating physical memory and mapping it to a virtual memory address space on all sockets. Note Multi-node, single-GPU platforms require interprocess communication. Therefore we encourage the reader to see Chapter 3 Note We encourage readers to read CUDA Programming Guide’s Chapter 10 and Chapter 11 for a better understanding. New CUDA property types have been added to APIs for allowing those approaches to understand allocation locations using NUMA-like node identifiers: CUDA Type Used with CU_MEM_LOCATION_TYPE_HOST_NUMA CUmemAllocationProp for cuMemCreate cudaMemLocationTypeHostNuma cudaMemPoolProps for cudaMemPoolCreate Note Please see CUDA Driver API and CUDA Runtime Data Types to find more about NUMA specific CUDA types. 21.2.'},\n",
       " {'id': 379,\n",
       "  'content': 'Using the EGM Interface \\uf0c1 21.2.1. Single-Node, Single-GPU \\uf0c1 Any of the existing CUDA host allocators as well as system allocated memory can be used to benefit from high-bandwidth C2C. To the user, local access is what a host allocation is today. Note Refer to the tuning guide for more information about memory allocators and page sizes. 21.2.2. Single-Node, Multi-GPU \\uf0c1 In a multi-GPU system, the user has to provide host information for the placement. As we mentioned, a natural way to express that information would be by using NUMA node IDs and EGM follows this approach. Therefore, using the cuDeviceGetAttribute function the user should be able to learn the closest NUMA node id. (See Socket Identifiers: What are they?'},\n",
       " {'id': 380,\n",
       "  'content': '). Then the user can allocate and manage EGM memory using VMM (Virtual Memory Management) API or CUDA Memory Pool. 21.2.2.1. Using VMM APIs \\uf0c1 The first step in memory allocation using Virtual Memory Management APIs is to create a physical memory chunk that will provide a backing for the allocation. See CUDA Programming Guide’s Virtual Memory Management section for more details. In EGM allocations the user has to explicitly provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaID as the location identifier. Also in EGM, allocations must be aligned to appropriate granularity of the platform. The following code snippet shows allocating physical memory with cuMemCreate : CUmemAllocationProp prop {}; prop . type = CU_MEM_LOCATION_TYPE_HOST_NUMA ; prop . id = numaId ; size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 ); After physical memory allocation, we have to reserve an address space and map it to a pointer. These procedures do not have EGM-specific changes: CUdeviceptr dptr ; cuMemAddressReserve ( & dptr , padded_size , 0 , 0 , 0 ); cuMemMap ( dptr , padded_size , 0 , allocHandle , 0 ); Finally, the user has to explicitly protect mapped virtual address ranges. Otherwise access to the mapped space would result in a crash. Similar to the memory allocation, the user has to provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaId as the location identifier. Following code snippet create an access descriptors for the host node and the GPU to give read and write access for the mapped memory to both of them: CUmemAccessDesc accessDesc [ 2 ]{{}}; accessDesc [ 0 ]. type = CU_MEM_LOCATION_TYPE_HOST_NUMA ; accessDesc [ 0 ]. id = numaId ; accessDesc [ 0 ]. flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; accessDesc [ 1 ]. type = CU_MEM_LOCATION_TYPE_DEVICE ; accessDesc [ 1 ]. id = currentDev ; accessDesc [ 1 ]. flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; cuMemSetAccess ( dptr , size , accessDesc , 2 ); 21.2.2.2. Using CUDA Memory Pool \\uf0c1 To define EGM, the user can create a memory pool on a node and give access to peers. In this case, the user has to explicitly define cudaMemLocationTypeHostNuma as the location type and numaId as the location identifier. The following code snippet shows creating a memory pool cudaMemPoolCreate : cudaSetDevice ( homeDevice ); cudaMemPoolProps props {}; props . allocType = cudaMemAllocationTypePinned ; props . type = cudaMemLocationTypeHostNuma ; props . id = numaId ; cudaMemPoolCreate ( & memPool , & props ); Additionally, for direct connect peer access, it is also possible to use the existing peer access API, cudaMemPoolSetAccess . An example for an accessingDevice is shown in the following code snippet: cudaMemAccessDesc desc {}; desc . flags = cudaMemAccessFlagsProtReadWrite ; desc .'},\n",
       " {'id': 381,\n",
       "  'content': 'type = cudaMemLocationTypeDevice ; desc . id = accessingDevice ; cudaMemPoolSetAccess ( memPool , & desc , 1 ); When the memory pool is created, and accesses are given, the user can set created memory pool to the residentDevice and start allocating memory using cudaMallocAsync : cudaDeviceSetMemPool ( residentDevice , memPool ); cudaMallocAsync ( & ptr , size , memPool , stream ); Note EGM is mapped with 2MB pages. Therefore, users may encounter more TLB misses when accessing very large allocations. 21.2.3. Multi-Node, Single-GPU \\uf0c1 Beyond memory allocation, remote peer access does not have EGM-specific modification and it follows CUDA inter process (IPC) protocol. See CUDA Programming Guide for more details in IPC. The user should allocate memory using cuMemCreate and again the user has to explicitly provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaID as the location identifier. In addition CU_MEM_HANDLE_TYPE_FABRIC should be defined as the requested handle type. The following code snippet shows allocating physical memory on Node A: CUmemAllocationProp prop {}; prop . requestedHandleTypes = CU_MEM_HANDLE_TYPE_FABRIC ; prop . id = numaId ; size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); size_t page_size = ...; assert ( padded_size % page_size == 0 ); CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 ); After creating allocation handle using cuMemCreate the user can export that handle to the other node, Node B, calling cuMemExportToShareableHandle : cuMemExportToShareableHandle ( & fabricHandle , allocHandle , CU_MEM_HANDLE_TYPE_FABRIC , 0 ); // At this point, fabricHandle should be sent to Node B via TCP/IP. On Node B, the handle can be imported using cuMemImportFromShareableHandle and treated as any other fabric handle // At this point, fabricHandle should be received from Node A via TCP/IP. CUmemGenericAllocationHandle allocHandle ; cuMemImportFromShareableHandle ( & allocHandle , & fabricHandle , CU_MEM_HANDLE_TYPE_FABRIC ); When handle is imported at Node B, then the user can reserve an address space and map it locally in a regular fashion: size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); size_t page_size = ...; assert ( padded_size % page_size == 0 ); CUdeviceptr dptr ; cuMemAddressReserve ( & dptr , padded_size , 0 , 0 , 0 ); cuMemMap ( dptr , padded_size , 0 , allocHandle , 0 ); As the final step, the user should give appropriate accesses to each of the local GPUs at Node B. An example code snippet that gives read and write access to eight local GPUs: // Give all 8 local GPUS access to exported EGM memory located on Node A. | CUmemAccessDesc accessDesc [ 8 ]; for ( int i = 0 ; i < 8 ; i ++ ) { accessDesc [ i ].'},\n",
       " {'id': 382,\n",
       "  'content': 'type = CU_MEM_LOCATION_TYPE_DEVICE ; accessDesc [ i ]. id = i ; accessDesc [ i ]. flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; } cuMemSetAccess ( dptr , size , accessDesc , 8 ); 22. Notices \\uf0c1 22.1.'},\n",
       " {'id': 383,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 22.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 384,\n",
       "  'content': '22.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 385,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); });1. Preface 1.1.'},\n",
       " {'id': 386,\n",
       "  'content': 'What Is This Document? 1.2. Who Should Read This Guide? 1.3. Assess, Parallelize, Optimize, Deploy 1.3.1. Assess 1.3.2. Parallelize 1.3.3. Optimize 1.3.4. Deploy 1.4. Recommendations and Best Practices 1.5. Assessing Your Application 2. Heterogeneous Computing 2.1. Differences between Host and Device 2.2. What Runs on a CUDA-Enabled Device? 3. Application Profiling 3.1. Profile 3.1.1. Creating the Profile 3.1.2. Identifying Hotspots 3.1.3. Understanding Scaling 3.1.3.1. Strong Scaling and Amdahl’s Law 3.1.3.2. Weak Scaling and Gustafson’s Law 3.1.3.3. Applying Strong and Weak Scaling 4. Parallelizing Your Application 5. Getting Started 5.1. Parallel Libraries 5.2. Parallelizing Compilers 5.3. Coding to Expose Parallelism 6. Getting the Right Answer 6.1.'},\n",
       " {'id': 387,\n",
       "  'content': 'Verification 6.1.1. Reference Comparison 6.1.2. Unit Testing 6.2. Debugging 6.3. Numerical Accuracy and Precision 6.3.1. Single vs. Double Precision 6.3.2. Floating Point Math Is not Associative 6.3.3. IEEE 754 Compliance 6.3.4. x86 80-bit Computations 7. Optimizing CUDA Applications 8. Performance Metrics 8.1. Timing 8.1.1. Using CPU Timers 8.1.2. Using CUDA GPU Timers 8.2. Bandwidth 8.2.1. Theoretical Bandwidth Calculation 8.2.2. Effective Bandwidth Calculation 8.2.3. Throughput Reported by Visual Profiler 9. Memory Optimizations 9.1. Data Transfer Between Host and Device 9.1.1. Pinned Memory 9.1.2. Asynchronous and Overlapping Transfers with Computation 9.1.3. Zero Copy 9.1.4. Unified Virtual Addressing 9.2. Device Memory Spaces 9.2.1. Coalesced Access to Global Memory 9.2.1.1. A Simple Access Pattern 9.2.1.2. A Sequential but Misaligned Access Pattern 9.2.1.3. Effects of Misaligned Accesses 9.2.1.4. Strided Accesses 9.2.2. L2 Cache 9.2.2.1. L2 Cache Access Window 9.2.2.2. Tuning the Access Window Hit-Ratio 9.2.3. Shared Memory 9.2.3.1.'},\n",
       " {'id': 388,\n",
       "  'content': 'Shared Memory and Memory Banks 9.2.3.2. Shared Memory in Matrix Multiplication (C=AB) 9.2.3.3. Shared Memory in Matrix Multiplication (C=AAT) 9.2.3.4. Asynchronous Copy from Global Memory to Shared Memory 9.2.4. Local Memory 9.2.5. Texture Memory 9.2.5.1. Additional Texture Capabilities 9.2.6. Constant Memory 9.2.7. Registers 9.2.7.1. Register Pressure 9.3. Allocation 9.4. NUMA Best Practices 10. Execution Configuration Optimizations 10.1. Occupancy 10.1.1. Calculating Occupancy 10.2. Hiding Register Dependencies 10.3. Thread and Block Heuristics 10.4. Effects of Shared Memory 10.5. Concurrent Kernel Execution 10.6. Multiple contexts 11. Instruction Optimization 11.1. Arithmetic Instructions 11.1.1. Division Modulo Operations 11.1.2. Loop Counters Signed vs. Unsigned 11.1.3. Reciprocal Square Root 11.1.4. Other Arithmetic Instructions 11.1.5. Exponentiation With Small Fractional Arguments 11.1.6. Math Libraries 11.1.7. Precision-related Compiler Flags 11.2. Memory Instructions 12. Control Flow 12.1. Branching and Divergence 12.2. Branch Predication 13. Deploying CUDA Applications 14. Understanding the Programming Environment 14.1. CUDA Compute Capability 14.2. Additional Hardware Data 14.3. Which Compute Capability Target 14.4. CUDA Runtime 15. CUDA Compatibility Developer’s Guide 15.1. CUDA Toolkit Versioning 15.2. Source Compatibility 15.3. Binary Compatibility 15.3.1. CUDA Binary (cubin) Compatibility 15.4. CUDA Compatibility Across Minor Releases 15.4.1. Existing CUDA Applications within Minor Versions of CUDA 15.4.1.1. Handling New CUDA Features and Driver APIs 15.4.1.2. Using PTX 15.4.1.3. Dynamic Code Generation 15.4.1.4. Recommendations for building a minor-version compatible library 15.4.1.5. Recommendations for taking advantage of minor version compatibility in your application 16. Preparing for Deployment 16.1. Testing for CUDA Availability 16.2. Error Handling 16.3. Building for Maximum Compatibility 16.4. Distributing the CUDA Runtime and Libraries 16.4.1. CUDA Toolkit Library Redistribution 16.4.1.1. Which Files to Redistribute 16.4.1.2. Where to Install Redistributed CUDA Libraries 17. Deployment Infrastructure Tools 17.1. Nvidia-SMI 17.1.1. Queryable state 17.1.2. Modifiable state 17.2. NVML 17.3. Cluster Management Tools 17.4. Compiler JIT Cache Management Tools 17.5. CUDA_VISIBLE_DEVICES 18. Recommendations and Best Practices 18.1.'},\n",
       " {'id': 389,\n",
       "  'content': 'Overall Performance Optimization Strategies 19. nvcc Compiler Switches 19.1. nvcc 20. Notices 20.1. Notice 20.2. OpenCL 20.3. Trademarks CUDA C++ Best Practices Guide » 1. Preface v12.5 | PDF | Archive CUDA C++ Best Practices Guide The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs. Preface \\uf0c1 1.1. \\uf0c1 This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIA ® CUDA ® GPUs. It presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures. While the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored.'},\n",
       " {'id': 390,\n",
       "  'content': 'As a result, it is recommended that first-time readers proceed through the guide sequentially. This approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later. \\uf0c1 The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code. This guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the CUDA website https://docs.nvidia.com/cuda/ . The following documents are especially important resources: CUDA Installation Guide CUDA C++ Programming Guide CUDA Toolkit Reference Manual In particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit (if not, please refer to the relevant CUDA Installation Guide for your platform) and that you have a basic familiarity with the CUDA C++ programming language and environment (if not, please refer to the CUDA C++ Programming Guide). Assess, Parallelize, Optimize, Deploy \\uf0c1 This guide introduces the Assess, Parallelize, Optimize, Deploy(APOD) design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible. APOD is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production. 1.3.1.'},\n",
       " {'id': 391,\n",
       "  'content': 'Assess \\uf0c1 For an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time. Armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate GPU acceleration. By understanding the end-user’s requirements and constraints and by applying Amdahl’s and Gustafson’s laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application. 1.3.2. Parallelize \\uf0c1 Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as cuBLAS , cuFFT , or Thrust , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler. On the other hand, some applications’ designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.)\\naims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput. 1.3.3.'},\n",
       " {'id': 392,\n",
       "  'content': 'Optimize \\uf0c1 After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned. Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developer’s optimization efforts and provide references into the relevant portions of the optimization section of this guide. 1.3.4. Deploy \\uf0c1 Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots. Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application.'},\n",
       " {'id': 393,\n",
       "  'content': '1.4. Recommendations and Best Practices \\uf0c1 Throughout this guide, specific recommendations are made regarding the design and implementation of CUDA C++ code. These recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope. Actions that present substantial improvements for most CUDA applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority. Before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied. This approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization. The criteria of benefit and scope for establishing priority will vary depending on the nature of the program. In this guide, they represent a typical case. Your code might reflect different priority factors. Regardless of this possibility, it is good practice to verify that no higher-priority recommendations have been overlooked before undertaking lower-priority items. Note Code samples throughout the guide omit error checking for conciseness. Production code should, however, systematically check the error code returned by each API call and check for failures in kernel launches by calling cudaGetLastError() . 1.5.'},\n",
       " {'id': 394,\n",
       "  'content': 'Assessing Your Application \\uf0c1 From supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance. The core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network. As a result, all modern processors require parallel code in order to achieve good utilization of their computational power. While processors are evolving to expose more fine-grained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarse-grained parallel codes (for example, where the data is decomposed into regions processed in parallel, with sub-regions shared using MPI). In order to profit from any modern processor architecture, GPUs included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future. 2. Heterogeneous Computing \\uf0c1 CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU devices . While NVIDIA GPUs are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel. This capability makes them well suited to computations that can leverage parallel execution. However, the device is based on a distinctly different design from the host system, and it’s important to understand those differences and how they determine the performance of CUDA applications in order to use CUDA effectively. 2.1. Differences between Host and Device \\uf0c1 The primary differences are in threading model and in separate physical memories: Threading resources Execution pipelines on host systems can support a limited number of concurrent threads. For example, servers that have two 32 core processors can run only 64 threads concurrently (or small multiple of that if the CPUs support simultaneous multithreading). By comparison, the smallest executable unit of parallelism on a CUDA device comprises 32 threads (termed a warp of threads). Modern NVIDIA GPUs can support up to 2048 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C++ Programming Guide) On GPUs with 80 multiprocessors, this leads to more than 160,000 concurrently active threads. Threads Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches (when two threads are swapped) are therefore slow and expensive. By comparison, threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work (in warps of 32 threads each). If the GPU must wait on one warp of threads, it simply begins executing work on another. Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads. Resources stay allocated to each thread until it completes its execution. In short, CPU cores are designed to minimize latency for a small number of threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput . RAM The host system and the device each have their own distinct attached physical memories 1 . As the host and device memories are separated, items in the host memory must occasionally be communicated between device memory and host memory as described in What Runs on a CUDA-Enabled Device? . These are the primary hardware differences between CPU hosts and GPU devices with respect to parallel programming. Other differences are discussed as they arise elsewhere in this document. Applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device. 2.2. \\uf0c1 The following issues should be considered when determining what parts of an application to run on the device: The device is ideally suited for computations that can be run on numerous data elements simultaneously in parallel. This typically involves arithmetic on large data sets (such as matrices) where the same operation can be performed across thousands, if not millions, of elements at the same time. This is a requirement for good performance on CUDA: the software must use a large number (generally thousands or tens of thousands) of concurrent threads. The support for running numerous threads in parallel derives from CUDA’s use of a lightweight threading model described above. To use CUDA, data values must be transferred from the host to the device. These transfers are costly in terms of performance and should be minimized. (See Data Transfer Between Host and Device .)\\nThis cost has several ramifications: The complexity of operations should justify the cost of moving data to and from the device. Code that transfers data for brief use by a small number of threads will see little or no performance benefit. The ideal scenario is one in which many threads perform a substantial amount of work. For example, transferring two matrices to the device to perform a matrix addition and then transferring the results back to the host will not realize much performance benefit. The issue here is the number of operations performed per data element transferred. For the preceding procedure, assuming matrices of size NxN, there are N 2 operations (additions) and 3N 2 elements transferred, so the ratio of operations to elements transferred is 1:3 or O(1). Performance benefits can be more readily achieved when this ratio is higher. For example, a matrix multiplication of the same matrices requires N 3 operations (multiply-add), so the ratio of operations to elements transferred is O(N), in which case the larger the matrix the greater the performance benefit. The types of operations are an additional factor, as additions have different complexity profiles than, for example, trigonometric functions. It is important to include the overhead of transferring data to and from the device in determining whether operations should be performed on the host or on the device. Data should be kept on the device as long as possible. Because transfers should be minimized, programs that run multiple kernels on the same data should favor leaving the data on the device between kernel calls, rather than transferring intermediate results to the host and then sending them back to the device for subsequent calculations. So, in the previous example, had the two matrices to be added already been on the device as a result of some previous calculation, or if the results of the addition would be used in some subsequent calculation, the matrix addition should be performed locally on the device. This approach should be used even if one of the steps in a sequence of calculations could be performed faster on the host. Even a relatively slow kernel may be advantageous if it avoids one or more transfers between host and device memory. Data Transfer Between Host and Device provides further details, including the measurements of bandwidth between the host and the device versus within the device proper. For best performance, there should be some coherence in memory access by adjacent threads running on the device.'},\n",
       " {'id': 395,\n",
       "  'content': 'Certain memory access patterns enable the hardware to coalesce groups of reads or writes of multiple data items into one operation. Data that cannot be laid out so as to enable coalescing , or that doesn’t have enough locality to use the L1 or texture caches effectively, will tend to see lesser speedups when used in computations on GPUs. A noteworthy exception to this are completely random memory access patterns. In general, they should be avoided, because compared to peak capabilities any architecture processes these memory access patterns at a low efficiency. However, compared to cache based architectures, like CPUs, latency hiding architectures, like GPUs, tend to cope better with completely random memory access patterns. 1 On Systems on a Chip with integrated GPUs, such as NVIDIA® Tegra®, host and device memory are physically the same, but there is still a logical distinction between host and device memory. See the Application Note on CUDA for Tegra for details.'},\n",
       " {'id': 396,\n",
       "  'content': 'Application Profiling \\uf0c1 3.1. Profile \\uf0c1 Many codes accomplish a significant portion of the work with a relatively small amount of code. Using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization. 3.1.1. Creating the Profile \\uf0c1 There are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time. Note High Priority: To maximize developer productivity, profile the application to determine hotspots and bottlenecks. The most important consideration with any profiling activity is to ensure that the workload is realistic - i.e., that information gained from the test and decisions based upon that information are relevant to real data. Using unrealistic workloads can lead to sub-optimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions. There are a number of tools that can be used to generate the profile. The following example is based on gprof , which is an open-source profiler for Linux platforms from the GNU Binutils collection. $ gcc -O2 -g -pg myprog.c $ gprof ./a.out > profile.txt Each sample counts as 0.01 seconds. % cumulative self self total time seconds seconds calls ms/call ms/call name 33.34 0.02 0.02 7208 0.00 0.00 genTimeStep 16.67 0.03 0.01 240 0.04 0.12 calcStats 16.67 0.04 0.01 8 1.25 1.25 calcSummaryData 16.67 0.05 0.01 7 1.43 1.43 write 16.67 0.06 0.01 mcount 0.00 0.06 0.00 236 0.00 0.00 tzset 0.00 0.06 0.00 192 0.00 0.00 tolower 0.00 0.06 0.00 47 0.00 0.00 strlen 0.00 0.06 0.00 45 0.00 0.00 strchr 0.00 0.06 0.00 1 0.00 50.00 main 0.00 0.06 0.00 1 0.00 0.00 memcpy 0.00 0.06 0.00 1 0.00 10.11 print 0.00 0.06 0.00 1 0.00 0.00 profil 0.00 0.06 0.00 1 0.00 50.00 report 3.1.2. Identifying Hotspots \\uf0c1 In the example above, we can clearly see that the function genTimeStep() takes one-third of the total running time of the application.'},\n",
       " {'id': 397,\n",
       "  'content': 'This should be our first candidate function for parallelization. Understanding Scaling discusses the potential benefit we might expect from such parallelization. It is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such as calcStats() and calcSummaryData() . Parallelizing these functions as well should increase our speedup potential. However, since APOD is a cyclical process, we might opt to parallelize these functions in a subsequent APOD pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes. 3.1.3. Understanding Scaling \\uf0c1 The amount of performance benefit an application will realize by running on CUDA depends entirely on the extent to which it can be parallelized. Code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device. Note High Priority: To get the maximum benefit from CUDA, focus first on finding ways to parallelize sequential code. By understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy. Strong Scaling and Amdahl’s Law describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size. Weak Scaling and Gustafson’s Law describes weak scaling, where the speedup is attained by growing the problem size. In many applications, a combination of strong and weak scaling is desirable. 3.1.3.1. Strong Scaling and Amdahl’s Law \\uf0c1 Strong scaling is a measure of how, for a fixed overall problem size, the time to solution decreases as more processors are added to a system. An application that exhibits linear strong scaling has a speedup equal to the number of processors used. Strong scaling is usually equated with Amdahl’s Law, which specifies the maximum speedup that can be expected by parallelizing portions of a serial program. Essentially, it states that the maximum speedup S of a program is: \\\\(S = \\\\frac{1}{(1 - P) + \\\\frac{P}{N}}\\\\) Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs. The larger N is(that is, the greater the number of processors), the smaller the P/N fraction. It can be simpler to view N as a very large number, which essentially transforms the equation into \\\\(S = 1/(1 - P)\\\\) . Now, if 3/4 of the running time of a sequential program is parallelized, the maximum speedup over serial code is 1 / (1 - 3/4) = 4. In reality, most applications do not exhibit perfectly linear strong scaling, even if they do exhibit some degree of strong scaling. For most purposes, the key point is that the larger the parallelizable portion P is, the greater the potential speedup. Conversely, if P is a small number (meaning that the application is not substantially parallelizable), increasing the number of processors N does little to improve performance. Therefore, to get the largest speedup for a fixed problem size, it is worthwhile to spend effort on increasing P , maximizing the amount of code that can be parallelized. 3.1.3.2. Weak Scaling and Gustafson’s Law \\uf0c1 Weak scaling is a measure of how the time to solution changes as more processors are added to a system with a fixed problem size per processor ; i.e., where the overall problem size increases as the number of processors is increased. Weak scaling is often equated with Gustafson’s Law, which states that in practice, the problem size scales with the number of processors. Because of this, the maximum speedup S of a program is: \\\\(S = N + (1 - P)(1 - N)\\\\) Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs. Another way of looking at Gustafson’s Law is that it is not the problem size that remains constant as we scale up the system but rather the execution time. Note that Gustafson’s Law assumes that the ratio of serial to parallel execution remains constant, reflecting additional cost in setting up and handling the larger problem. 3.1.3.3. Applying Strong and Weak Scaling \\uf0c1 Understanding which type of scaling is most applicable to an application is an important part of estimating speedup. For some applications the problem size will remain constant and hence only strong scaling is applicable. An example would be modeling how two molecules interact with each other, where the molecule sizes are fixed. For other applications, the problem size will grow to fill the available processors. Examples include modeling fluids or structures as meshes or grids and some Monte Carlo simulations, where increasing the problem size provides increased accuracy. Having understood the application profile, the developer should understand how the problem size would change if the computational performance changes and then apply either Amdahl’s or Gustafson’s Law to determine an upper bound for the speedup. 4. Parallelizing Your Application \\uf0c1 Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. As even CPU architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.)\\n5. Getting Started \\uf0c1 There are several key strategies for parallelizing sequential code. While the details of how to apply these strategies to a particular application is a complex and problem-specific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore CPUs or for use on CUDA GPUs. 5.1. Parallel Libraries \\uf0c1 The most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf. The CUDA Toolkit includes a number of such libraries that have been fine-tuned for NVIDIA CUDA GPUs, such as cuBLAS , cuFFT , and so on. The key here is that libraries are most useful when they match well with the needs of the application. Applications already using other BLAS libraries can often quite easily switch to cuBLAS , for example, whereas applications that do little to no linear algebra will have little use for cuBLAS . The same goes for other CUDA Toolkit libraries: cuFFT has an interface similar to that of FFTW , etc. Also of note is the Thrust library, which is a parallel C++ template library similar to the C++ Standard Template Library. Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically. As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial. 5.2. Parallelizing Compilers \\uf0c1 Another common approach to parallelization of sequential codes is to make use of parallelizing compilers. Often this means the use of directives-based approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself. By exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel architecture. The OpenACC standard provides a set of compiler directives to specify loops and regions of code in standard C, C++ and Fortran that should be offloaded from a host CPU to an attached accelerator such as a CUDA GPU. The details of managing the accelerator device are handled implicitly by an OpenACC-enabled compiler and runtime. See http://www.openacc.org/ for details.'},\n",
       " {'id': 398,\n",
       "  'content': '5.3. Coding to Expose Parallelism \\uf0c1 For applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as CUDA C++ that integrate seamlessly with existing sequential code are essential. Once we have located a hotspot in our application’s profile assessment and determined that custom code is the best approach, we can use CUDA C++ to expose the parallelism in that portion of our code as a CUDA kernel. We can then launch this kernel onto the GPU and retrieve the results without requiring major rewrites to the rest of our application. This approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the code. More difficult to parallelize are applications with a very flat profile - i.e., applications where the time spent is spread out relatively evenly across a wide portion of the code base. For the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, CPU and GPU alike, so it is well worth the effort should it become necessary. 6. Getting the Right Answer \\uf0c1 Obtaining the right answer is clearly the principal goal of all computation. On parallel systems, it is possible to run into difficulties not typically found in traditional serial-oriented programming. These include threading issues, unexpected values due to the way floating-point values are computed, and challenges arising from differences in the way CPU and GPU processors operate. This chapter examines issues that can affect the correctness of returned data and points to appropriate solutions.'},\n",
       " {'id': 399,\n",
       "  'content': '6.1. Verification \\uf0c1 6.1.1. Reference Comparison \\uf0c1 A key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous known-good reference outputs from representative inputs can be compared to new results. After each change is made, ensure that the results match using whatever criteria apply to the particular algorithm. Some will expect bitwise identical results, which is not always possible, especially where floating-point arithmetic is concerned; see Numerical Accuracy and Precision regarding numerical accuracy. For other algorithms, implementations may be considered correct if they match the reference within some small epsilon. Note that the process used for validating numerical results can easily be extended to validate performance results as well. We want to ensure that each change we make is correct and that it improves performance (and by how much). Checking these things frequently as an integral part of our cyclical APOD process will help ensure that we achieve the desired results as rapidly as possible. 6.1.2. Unit Testing \\uf0c1 A useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level. For example, we can write our CUDA kernels as a collection of many short __device__ functions rather than one large monolithic __global__ function; each device function can be tested independently before hooking them all together. For example, many kernels have complex addressing logic for accessing memory in addition to their actual computation. If we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts. (Note that the CUDA compiler considers any device code that does not contribute to a write to global memory as dead code subject to elimination, so we must at least write something out to global memory as a result of our addressing logic in order to successfully apply this strategy.)\\nGoing a step further, if most functions are defined as __host__ __device__ rather than just __device__ functions, then these functions can be tested on both the CPU and the GPU, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results. If there are differences, then those differences will be seen early and can be understood in the context of a simple function. As a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both CPU and GPU execution paths in our application: if the bulk of the work of our CUDA kernels is done in __host__ __device__ functions, we can easily call those functions from both the host code and the device code without duplication. 6.2. Debugging \\uf0c1 CUDA-GDB is a port of the GNU Debugger that runs on Linux and Mac; see: https://developer.nvidia.com/cuda-gdb . The NVIDIA Nsight Visual Studio Edition for Microsoft Windows 7, Windows HPC Server 2008, Windows 8.1, and Windows 10 is available as a free plugin for Microsoft Visual Studio; see: https://developer.nvidia.com/nsight-visual-studio-edition . Several third-party debuggers support CUDA debugging as well; see: https://developer.nvidia.com/debugging-solutions for more details. 6.3.'},\n",
       " {'id': 400,\n",
       "  'content': 'Numerical Accuracy and Precision \\uf0c1 Incorrect or unexpected results arise principally from issues of floating-point accuracy due to the way floating-point values are computed and stored. The following sections explain the principal items of interest. Other peculiarities of floating-point arithmetic are presented in Features and Technical Specifications of the CUDA C++ Programming Guide as well as in a whitepaper and accompanying webinar on floating-point precision and performance available from https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus . 6.3.1. Double Precision \\uf0c1 Devices of compute capability 1.3 and higher provide native support for double-precision floating-point values (that is, values 64 bits wide). Results obtained using double-precision arithmetic will frequently differ from the same operation performed via single-precision arithmetic due to the greater precision of the former and due to rounding issues. Therefore, it is important to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact. 6.3.2. Floating Point Math Is not Associative \\uf0c1 Each floating-point arithmetic operation involves a certain amount of rounding. Consequently, the order in which arithmetic operations are performed is important. If A, B, and C are floating-point values, (A+B)+C is not guaranteed to equal A+(B+C) as it is in symbolic math. When you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results. This limitation is not specific to CUDA, but an inherent part of parallel computation on floating-point values. 6.3.3. IEEE 754 Compliance \\uf0c1 All CUDA compute devices follow the IEEE 754 standard for binary floating-point representation, with some small exceptions. These exceptions, which are detailed in Features and Technical Specifications of the CUDA C++ Programming Guide, can lead to results that differ from IEEE 754 values computed on the host system. One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution. Its result will often differ slightly from results obtained by doing the two operations separately. 6.3.4. x86 80-bit Computations \\uf0c1 x86 processors can use an 80-bit double extended precision math when performing floating-point calculations. The results of these calculations can frequently differ from pure 64-bit operations performed on the CUDA device. To get a closer match between values, set the x86 host processor to use regular double or single precision (64 bits and 32 bits, respectively). This is done with the FLDCW x86 assembly instruction or the equivalent operating system API.'},\n",
       " {'id': 401,\n",
       "  'content': '7. Optimizing CUDA Applications \\uf0c1 After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. 8. Performance Metrics \\uf0c1 When attempting to optimize CUDA code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement. This chapter discusses how to correctly measure performance using CPU timers and CUDA events. It then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses. 8.1. Timing \\uf0c1 CUDA calls and kernel executions can be timed using either CPU or GPU timers. This section examines the functionality, advantages, and pitfalls of both approaches. 8.1.1. Using CPU Timers \\uf0c1 Any CPU timer can be used to measure the elapsed time of a CUDA call or kernel execution. The details of various CPU timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide. When using CPU timers, it is critical to remember that many CUDA API functions are asynchronous; that is, they return control back to the calling CPU thread prior to completing their work. All kernel launches are asynchronous, as are memory-copy functions with the Async suffix on their names. Therefore, to accurately measure the elapsed time for a particular call or sequence of CUDA calls, it is necessary to synchronize the CPU thread with the GPU by calling cudaDeviceSynchronize() immediately before starting and stopping the CPU timer. cudaDeviceSynchronize() blocks the calling CPU thread until all CUDA calls previously issued by the thread are completed. Although it is also possible to synchronize the CPU thread with a particular stream or event on the GPU, these synchronization functions are not suitable for timing code in streams other than the default stream. cudaStreamSynchronize() blocks the CPU thread until all CUDA calls previously issued into the given stream have completed. cudaEventSynchronize() blocks until a given event in a particular stream has been recorded by the GPU. Because the driver may interleave execution of CUDA calls from other non-default streams, calls in other streams may be included in the timing. Because the default stream, stream 0, exhibits serializing behavior for work on the device (an operation in the default stream can begin only after all preceding calls in any stream have completed; and no subsequent operation in any stream can begin until it finishes), these functions can be used reliably for timing in the default stream. Be aware that CPU-to-GPU synchronization points such as those mentioned in this section imply a stall in the GPU’s processing pipeline and should thus be used sparingly to minimize their performance impact.'},\n",
       " {'id': 402,\n",
       "  'content': '8.1.2. Using CUDA GPU Timers \\uf0c1 The CUDA event API provides calls that create and destroy events, record events (including a timestamp), and convert timestamp differences into a floating-point value in milliseconds. How to time code using CUDA events illustrates their use. How to time code using CUDA events cudaEvent_t start , stop ; float time ; cudaEventCreate ( & start ); cudaEventCreate ( & stop ); cudaEventRecord ( start , 0 ); kernel >> ( d_odata , d_idata , size_x , size_y , NUM_REPS ); cudaEventRecord ( stop , 0 ); cudaEventSynchronize ( stop ); cudaEventElapsedTime ( & time , start , stop ); cudaEventDestroy ( start ); cudaEventDestroy ( stop ); Here cudaEventRecord() is used to place the start and stop events into the default stream, stream 0. The device will record a timestamp for the event when it reaches that event in the stream. The cudaEventElapsedTime() function returns the time elapsed between the recording of the start and stop events. This value is expressed in milliseconds and has a resolution of approximately half a microsecond. Like the other calls in this listing, their specific operation, parameters, and return values are described in the CUDA Toolkit Reference Manual . Note that the timings are measured on the GPU clock, so the timing resolution is operating-system-independent. 8.2. Bandwidth \\uf0c1 Bandwidth - the rate at which data can be transferred - is one of the most important gating factors for performance. Almost all changes to code should be made in the context of how they affect bandwidth. As described in Memory Optimizations of this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors. To measure performance accurately, it is useful to calculate theoretical and effective bandwidth. When the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it. Note High Priority: Use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits. 8.2.1. Theoretical Bandwidth Calculation \\uf0c1 Theoretical bandwidth can be calculated using hardware specifications available in the product literature. For example, the NVIDIA Tesla V100 uses HBM2 (double data rate) RAM with a memory clock rate of 877 MHz and a 4096-bit-wide memory interface. Using these data items, the peak theoretical memory bandwidth of the NVIDIA Tesla V100 is 898 GB/s: \\\\(\\\\left. \\\\left( 0.877 \\\\times 10^{9} \\\\right. \\\\times (4096/8) \\\\times 2 \\\\right) \\\\div 10^{9} = 898\\\\text{GB/s}\\\\) In this calculation, the memory clock rate is converted in to Hz, multiplied by the interface width (divided by 8, to convert bits to bytes) and multiplied by 2 due to the double data rate. Finally, this product is divided by 10 9 to convert the result to GB/s. Note Some calculations use 1024 3 instead of 10 9 for the final calculation. In such a case, the bandwidth would be 836.4 GiB/s. It is important to use the same divisor when calculating theoretical and effective bandwidth so that the comparison is valid. Note On GPUs with GDDR memory with ECC enabled the available DRAM is reduced by 6.25% to allow for the storage of ECC bits. Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled, though the exact impact of ECC on bandwidth can be higher and depends on the memory access pattern. HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection. 2 8.2.2.'},\n",
       " {'id': 403,\n",
       "  'content': 'Effective Bandwidth Calculation \\uf0c1 Effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program. To do so, use this equation: \\\\(\\\\text{Effective\\\\ bandwidth} = \\\\left( {\\\\left( B_{r} + B_{w} \\\\right) \\\\div 10^{9}} \\\\right) \\\\div \\\\text{time}\\\\) Here, the effective bandwidth is in units of GB/s, B r is the number of bytes read per kernel, B w is the number of bytes written per kernel, and time is given in seconds. For example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used: \\\\(\\\\text{Effective\\\\ bandwidth} = \\\\left( {\\\\left( 2048^{2} \\\\times 4 \\\\times 2 \\\\right) \\\\div 10^{9}} \\\\right) \\\\div \\\\text{time}\\\\) The number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by 2 (because of the read and write), divided by 10 9 (or 1,024 3 ) to obtain GB of memory transferred. This number is divided by the time in seconds to obtain GB/s. 8.2.3. Throughput Reported by Visual Profiler \\uf0c1 For devices with compute capability of 2.0 or greater, the Visual Profiler can be used to collect several different memory throughput measures. The following throughput metrics can be displayed in the Details or Detail Graphs view: Requested Global Load Throughput Requested Global Store Throughput Global Load Throughput Global Store Throughput DRAM Read Throughput DRAM Write Throughput The Requested Global Load Throughput and Requested Global Store Throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown under Effective Bandwidth Calculation . Because the minimum memory transaction size is larger than most word sizes, the actual memory throughput required for a kernel can include the transfer of data not used by the kernel. For global memory accesses, this actual throughput is reported by the Global Load Throughput and Global Store Throughput values. It’s important to note that both numbers are useful. The actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses (see Coalesced Access to Global Memory ). For global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the Global Memory Load Efficiency and Global Memory Store Efficiency metrics. 2 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory. 9. Memory Optimizations \\uf0c1 Memory optimizations are the most important area for performance. The goal is to maximize the use of the hardware by maximizing bandwidth. Bandwidth is best served by using as much fast memory and as little slow-access memory as possible. This chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively. 9.1. Data Transfer Between Host and Device \\uf0c1 The peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s on the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory and device memory (16 GB/s on the PCIe x16 Gen3). Hence, for best overall application performance, it is important to minimize data transfer between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU. Note High Priority: Minimize data transfer between the host and the device, even if it means running some kernels on the device that do not show performance gains when compared with running them on the host CPU. Intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory. Also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking after the transfer. Finally, higher bandwidth between the host and the device is achieved when using page-locked (or pinned ) memory, as discussed in the CUDA C++ Programming Guide and the Pinned Memory section of this document. 9.1.1. Pinned Memory \\uf0c1 Page-locked or pinned memory transfers attain the highest bandwidth between the host and the device. On PCIe x16 Gen3 cards, for example, pinned memory can attain roughly 12 GB/s transfer rates. Pinned memory is allocated using the cudaHostAlloc() functions in the Runtime API. The bandwidthTest CUDA Sample shows how to use these functions as well as how to measure memory transfer performance. For regions of system memory that have already been pre-allocated, cudaHostRegister() can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it. Pinned memory should not be overused. Excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance. Furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters. 9.1.2. Asynchronous and Overlapping Transfers with Computation \\uf0c1 Data transfers between the host and the device using cudaMemcpy() are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete. The cudaMemcpyAsync() function is a non-blocking variant of cudaMemcpy() in which control is returned immediately to the host thread. In contrast with cudaMemcpy() , the asynchronous transfer version requires pinned host memory (see Pinned Memory ), and it contains an additional argument, a stream ID. A stream is simply a sequence of operations that are performed in order on the device. Operations in different streams can be interleaved and in some cases overlapped - a property that can be used to hide data transfers between the host and the device. Asynchronous transfers enable overlap of data transfers with computation in two different ways. On all CUDA-enabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations. For example, Overlapping computation and data transfers demonstrates how host computation in the routine cpuFunction() is performed while data is transferred to the device and a kernel using the device is executed. Overlapping computation and data transfers cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , 0 ); kernel >> ( a_d ); cpuFunction (); The last argument to the cudaMemcpyAsync() function is the stream ID, which in this case uses the default stream, stream 0. The kernel also uses the default stream, and it will not begin execution until the memory copy completes; therefore, no explicit synchronization is needed. Because the memory copy and the kernel both return control to the host immediately, the host function cpuFunction() overlaps their execution. In Overlapping computation and data transfers , the memory copy and kernel execution occur sequentially. On devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device. Whether a device has this capability is indicated by the asyncEngineCount field of the cudaDeviceProp structure (or listed in the output of the deviceQuery CUDA Sample). On devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, non-default streams (streams with non-zero stream IDs). Non-default streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished. Concurrent copy and execute illustrates the basic technique. Concurrent copy and execute cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , stream1 ); kernel >> ( otherData_d ); In this code, two streams are created and used in the data transfer and kernel executions as specified in the last arguments of the cudaMemcpyAsync call and the kernel’s execution configuration. Concurrent copy and execute demonstrates how to overlap kernel execution with asynchronous data transfer. This technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives. Sequential copy and execute and Staged concurrent copy and execute demonstrate this. They produce equivalent results. The first segment shows the reference sequential implementation, which transfers and operates on an array of N floats (where N is assumed to be evenly divisible by nThreads). Sequential copy and execute cudaMemcpy ( a_d , a_h , N * sizeof ( float ), dir ); kernel >> ( a_d ); Staged concurrent copy and execute shows how the transfer and kernel execution can be broken up into nStreams stages. This approach permits some overlapping of the data transfer and execution. Staged concurrent copy and execute size = N * sizeof ( float ) / nStreams ; for ( i = 0 ; i >> ( a_d + offset ); } (In Staged concurrent copy and execute , it is assumed that N is evenly divisible by nThreads*nStreams .)\\nBecause execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete. Current GPUs can simultaneously process asynchronous data transfers and execute kernels. GPUs with a single copy engine can perform one asynchronous data transfer and execute kernels whereas GPUs with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels. The number of copy engines on a GPU is given by the asyncEngineCount field of the cudaDeviceProp structure, which is also listed in the output of the deviceQuery CUDA Sample. (It should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous CUDA calls complete. It will not allow any other CUDA call to begin until it has completed.)\\nA diagram depicting the timeline of execution for the two code segments is shown in Figure 1 , and nStreams is equal to 4 for Staged concurrent copy and execute in the bottom half of the figure. Timeline comparison for copy and kernel execution \\uf0c1 Top Sequential Bottom Concurrent For this example, it is assumed that the data transfer and kernel execution times are comparable. In such cases, and when the execution time ( tE ) exceeds the transfer time ( tT ), a rough estimate for the overall time is tE + tT/nStreams for the staged version versus tE + tT for the sequential version. If the transfer time exceeds the execution time, a rough estimate for the overall time is tT + tE/nStreams . 9.1.3. Zero Copy \\uf0c1 Zero copy is a feature that was added in version 2.2 of the CUDA Toolkit. It enables GPU threads to directly access host memory. For this purpose, it requires mapped pinned (non-pageable) memory. On integrated GPUs (i.e., GPUs with the integrated field of the CUDA device properties structure set to 1), mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated GPU and CPU memory are physically the same. On discrete GPUs, mapped pinned memory is advantageous only in certain cases. Because the data is not cached on the GPU, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should be coalesced. Zero copy can be used in place of streams because kernel-originated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams. Note Low Priority: Use zero-copy operations on integrated GPUs for CUDA Toolkit version 2.2 and later. The host code in Zero-copy host code shows how zero copy is typically set up. Zero-copy host code float * a_h , * a_map ; ... cudaGetDeviceProperties ( & prop , 0 ); if ( ! prop . canMapHostMemory ) exit ( 0 ); cudaSetDeviceFlags ( cudaDeviceMapHost ); cudaHostAlloc ( & a_h , nBytes , cudaHostAllocMapped ); cudaHostGetDevicePointer ( & a_map , a_h , 0 ); kernel >> ( a_map ); In this code, the canMapHostMemory field of the structure returned by cudaGetDeviceProperties() is used to check that the device supports mapping host memory to the device’s address space. Page-locked memory mapping is enabled by calling cudaSetDeviceFlags() with cudaDeviceMapHost . Note that cudaSetDeviceFlags() must be called prior to setting a device or making a CUDA call that requires state (that is, essentially, before a context is created). Page-locked mapped host memory is allocated using cudaHostAlloc() , and the pointer to the mapped device address space is obtained via the function cudaHostGetDevicePointer() . In the code in Zero-copy host code , kernel() can reference the mapped pinned host memory using the pointer a_map in exactly the same was as it would if a_map referred to a location in device memory. Note Mapped pinned host memory allows you to overlap CPU-GPU memory transfers with computation while avoiding the use of CUDA streams. But since any repeated access to such memory areas causes repeated CPU-GPU transfers, consider creating a second area in device memory to manually cache the previously read host memory data. 9.1.4. Unified Virtual Addressing \\uf0c1 Devices of compute capability 2.0 and later support a special addressing mode called Unified Virtual Addressing (UVA) on 64-bit Linux and Windows. With UVA, the host memory and the device memories of all installed supported devices share a single virtual address space. Prior to UVA, an application had to keep track of which pointers referred to device memory (and for which device) and which referred to host memory as a separate bit of metadata (or as hard-coded information in the program) for each pointer. Using UVA, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer using cudaPointerGetAttributes() . Under UVA, pinned host memory allocated with cudaHostAlloc() will have identical host and device pointers, so it is not necessary to call cudaHostGetDevicePointer() for such allocations. Host memory allocations pinned after-the-fact via cudaHostRegister() , however, will continue to have different device pointers than their host pointers, so cudaHostGetDevicePointer() remains necessary in that case. UVA is also a necessary precondition for enabling peer-to-peer (P2P) transfer of data directly across the PCIe bus or NVLink for supported GPUs in supported configurations, bypassing host memory. See the CUDA C++ Programming Guide for further explanations and software requirements for UVA and P2P. 9.2. Device Memory Spaces \\uf0c1 CUDA devices use several memory spaces, which have different characteristics that reflect their distinct usages in CUDA applications. These memory spaces include global, local, shared, texture, and registers, as shown in Figure 2 . Memory spaces on a CUDA device \\uf0c1 Of these different memory spaces, global memory is the most plentiful; see Features and Technical Specifications of the CUDA C++ Programming Guide for the amounts of memory available in each memory space at each compute capability level. Global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file. The various principal traits of the memory types are shown in Table 1 . Table 1. Salient Features of Device Memory \\uf0c1 Memory Location on/off chip Cached Access Scope Lifetime Register On n/a R/W 1 thread Thread Local Off Yes†† R/W 1 thread Thread Shared On n/a R/W All threads in block Block Global Off † R/W All threads + host Host allocation Constant Off Yes R All threads + host Host allocation Texture Off Yes R All threads + host Host allocation † Cached in L1 and L2 by default on devices of compute capability 6.0 and 7.x; cached only in L2 by default on devices of lower compute capabilities, though some allow opt-in to caching in L1 as well via compilation flags. †† Cached in L1 and L2 by default except on devices of compute capability 5.x; devices of compute capability 5.x cache locals only in L2. In the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array. Texture references that are bound to CUDA arrays can be written to via surface-write operations by binding a surface to the same underlying CUDA array storage). Reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are read-only and are not invalidated when the associated global memory is modified. 9.2.1. Coalesced Access to Global Memory \\uf0c1 A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions. Note High Priority: Ensure global memory accesses are coalesced whenever possible. The access requirements for coalescing depend on the compute capability of the device and are documented in the CUDA C++ Programming Guide. For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp. For certain devices of compute capability 5.2, L1-caching of accesses to global memory can be optionally enabled. If L1-caching is enabled on these devices, the number of required transactions is equal to the number of required 128-byte aligned segments. Note On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not. On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory. Coalescing concepts are illustrated in the following simple examples. These examples assume compute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted. 9.2.1.1. A Simple Access Pattern \\uf0c1 The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the k -th thread accesses the k -th word in a 32-byte aligned array. Not all threads need to participate. For example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent float values), four coalesced 32-byte transactions will service that memory access. Such a pattern is shown in Figure 3 . Coalesced access \\uf0c1 This access pattern results in four 32-byte transactions, indicated by the red rectangles. If from any of the four 32-byte segments only a subset of the words are requested (e.g. if several threads had accessed the same word or if some threads did not participate in the access), the full segment is fetched anyway. Furthermore, if accesses by the threads of the warp had been permuted within or accross the four segments, still only four 32-byte transactions would have been performed by a device with compute capability 6.0 or higher. 9.2.1.2. A Sequential but Misaligned Access Pattern \\uf0c1 If sequential threads in a warp access memory that is sequential but not aligned with a 32-byte segment, five 32-byte segments will be requested, as shown in Figure 4 . Misaligned sequential addresses that fall within five 32-byte segments \\uf0c1 Memory allocated through the CUDA Runtime API, such as via cudaMalloc() , is guaranteed to be aligned to at least 256 bytes. Therefore, choosing sensible thread block sizes, such as multiples of the warp size (i.e., 32 on current GPUs), facilitates memory accesses by warps that are properly aligned. (Consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example.)\\n9.2.1.3. Effects of Misaligned Accesses \\uf0c1 It is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one in A copy kernel that illustrates misaligned accesses . A copy kernel that illustrates misaligned accesses __global__ void offsetCopy ( float * odata , float * idata , int offset ) { int xid = blockIdx . x * blockDim . x + threadIdx . x + offset ; odata [ xid ] = idata [ xid ]; } In A copy kernel that illustrates misaligned accesses , data is copied from the input array idata to the output array, both of which exist in global memory. The kernel is executed within a loop in host code that varies the parameter offset from 0 to 32.'},\n",
       " {'id': 404,\n",
       "  'content': '(e.g. Figure 4 corresponds to this misalignments) The effective bandwidth for the copy with various offsets on an NVIDIA Tesla V100 ( compute capability 7.0) is shown in Figure 5 . Performance of offsetCopy kernel \\uf0c1 For the NVIDIA Tesla V100, global memory accesses with no offset or with offsets that are multiples of 8 words result in four 32-byte transactions. The achieved bandwidth is approximately 790 GB/s. Otherwise, five 32-byte segments are loaded per warp, and we would expect approximately 4/5 th of the memory throughput achieved with no offsets. In this particular example, the offset memory throughput achieved is, however, approximately 9/10 th , because adjacent warps reuse the cache lines their neighbors fetched. So while the impact is still evident it is not as large as we might have expected. It would have been more so if adjacent warps had not exhibited such a high degree of reuse of the over-fetched cache lines. 9.2.1.4. Strided Accesses \\uf0c1 As seen above, in the case of misaligned sequential accesses, caches help to alleviate the performance impact. It may be different with non-unit-strided accesses, however, and this is a pattern that occurs frequently when dealing with multidimensional data or matrices. For this reason, ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices. To illustrate the effect of strided access on effective bandwidth, see the kernel strideCopy() in A kernel to illustrate non-unit stride data copy , which copies data with a stride of stride elements between threads from idata to odata . A kernel to illustrate non-unit stride data copy __global__ void strideCopy ( float * odata , float * idata , int stride ) { int xid = ( blockIdx . x ) * stride ; odata [ xid ] = idata [ xid ]; } Figure 6 illustrates such a situation; in this case, threads within a warp access words in memory with a stride of 2. This action leads to a load of eight L2 cache segments per warp on the Tesla V100 (compute capability 7.0). Adjacent threads accessing memory with a stride of 2 \\uf0c1 A stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth. As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp, as indicated in Figure 7 . Performance of strideCopy kernel \\uf0c1 As illustrated in Figure 7 , non-unit-stride global memory accesses should be avoided whenever possible. One method for doing so utilizes shared memory, which is discussed in the next section.'},\n",
       " {'id': 405,\n",
       "  'content': '9.2.2. L2 Cache \\uf0c1 Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache. Because L2 cache is on-chip, it potentially provides higher bandwidth and lower latency accesses to global memory. For more details refer to the L2 Access Management section in the CUDA C++ Programming Guide . 9.2.2.1. L2 Cache Access Window \\uf0c1 When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be persisting . On the other hand, if the data is only accessed once, such data accesses can be considered to be streaming . A portion of the L2 cache can be set aside for persistent accesses to a data region in global memory. If this set-aside portion is not used by persistent accesses, then streaming or normal data accesses can use it. The L2 cache set-aside size for persisting accesses may be adjusted, within limits: cudaGetDeviceProperties ( & prop , device_id ); cudaDeviceSetLimit ( cudaLimitPersistingL2CacheSize , prop . persistingL2CacheMaxSize ); /* Set aside max possible size of L2 cache for persisting accesses */ Mapping of user data to L2 set-aside portion can be controlled using an access policy window on a CUDA stream or CUDA graph kernel node. The example below shows how to use the access policy window on a CUDA stream. cudaStreamAttrValue stream_attribute ; // Stream level attributes data structure stream_attribute . accessPolicyWindow . base_ptr = reinterpret_cast ( ptr ); // Global Memory data pointer stream_attribute . num_bytes = num_bytes ; // Number of bytes for persisting accesses. // (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize) stream_attribute . hitRatio = 1.0 ; // Hint for L2 cache hit ratio for persisting accesses in the num_bytes region stream_attribute . hitProp = cudaAccessPropertyPersisting ; // Type of access property on cache hit stream_attribute . missProp = cudaAccessPropertyStreaming ; // Type of access property on cache miss. //Set the attributes to a CUDA stream of type cudaStream_t cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); The access policy window requires a value for hitRatio and num_bytes . Depending on the value of the num_bytes parameter and the size of L2 cache, one may need to tune the value of hitRatio to avoid thrashing of L2 cache lines. 9.2.2.2. Tuning the Access Window Hit-Ratio \\uf0c1 The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property. For example, if the hitRatio value is 0.6, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property. To understand the effect of hitRatio and num_bytes , we use a sliding window micro benchmark. This microbenchmark uses a 1024 MB region in GPU global memory. First, we set aside 30 MB of the L2 cache for persisting accesses using cudaDeviceSetLimit() , as discussed above. Then, as shown in the figure below, we specify that the accesses to the first freqSize * sizeof(int) bytes of the memory region are persistent. This data will thus use the L2 set-aside portion. In our experiment, we vary the size of this persistent data region from 10 MB to 60 MB to model various scenarios where data fits in or exceeds the available L2 set-aside portion of 30 MB. Note that the NVIDIA Tesla A100 GPU has 40 MB of total L2 cache capacity. Accesses to the remaining data of the memory region (i.e., streaming data) are considered normal or streaming accesses and will thus use the remaining 10 MB of the non set-aside L2 portion (unless part of the L2 set-aside portion is unused). Mapping Persistent data accesses to set-aside L2 in sliding window experiment \\uf0c1 Consider the following kernel code and access window parameters, as the implementation of the sliding window experiment. __global__ void kernel ( int * data_persistent , int * data_streaming , int dataSize , int freqSize ) { int tid = blockIdx . x ; /*Each CUDA thread accesses one element in the persistent data section and one element in the streaming data section. Because the size of the persistent memory region (freqSize * sizeof(int) bytes) is much smaller than the size of the streaming memory region (dataSize * sizeof(int) bytes), data in the persistent region is accessed more frequently*/ data_persistent [ tid % freqSize ] = 2 * data_persistent [ tid % freqSize ]; data_streaming [ tid % dataSize ] = 2 * data_streaming [ tid % dataSize ]; } stream_attribute . base_ptr = reinterpret_cast ( data_persistent ); stream_attribute . num_bytes = freqSize * sizeof ( int ); //Number of bytes for persisting accesses in range 10-60 MB stream_attribute . hitRatio = 1.0 ; //Hint for cache hit ratio. Fixed value 1.0 The performance of the above kernel is shown in the chart below. When the persistent data region fits well into the 30 MB set-aside portion of the L2 cache, a performance increase of as much as 50% is observed. However, once the size of this persistent data region exceeds the size of the L2 set-aside cache portion, approximately 10% performance drop is observed due to thrashing of L2 cache lines. The performance of the sliding-window benchmark with fixed hit-ratio of 1.0 \\uf0c1 In order to optimize the performance, when the size of the persistent data is more than the size of the set-aside L2 cache portion, we tune the num_bytes and hitRatio parameters in the access window as below. stream_attribute .'},\n",
       " {'id': 406,\n",
       "  'content': 'num_bytes = 20 * 1024 * 1024 ; //20 MB stream_attribute . hitRatio = ( 20 * 1024 * 1024 ) / (( float ) freqSize * sizeof ( int )); //Such that up to 20MB of data is resident. We fix the num_bytes in the access window to 20 MB and tune the hitRatio such that a random 20 MB of the total persistent data is resident in the L2 set-aside cache portion. The remaining portion of this persistent data will be accessed using the streaming property. This helps in reducing cache thrashing. The results are shown in the chart below, where we see good performance regardless of whether the persistent data fits in the L2 set-aside or not. The performance of the sliding-window benchmark with tuned hit-ratio \\uf0c1 9.2.3. Shared Memory \\uf0c1 Because it is on-chip, shared memory has much higher bandwidth and lower latency than local and global memory - provided there are no bank conflicts between the threads, as detailed in the following section. 9.2.3.1. Shared Memory and Memory Banks \\uf0c1 To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules ( banks ) that can be accessed simultaneously. Therefore, any memory load or store of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is n times as high as the bandwidth of a single bank. However, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized. The hardware splits a memory request that has bank conflicts into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests. The one exception here is when multiple threads in a warp address the same shared memory location, resulting in a broadcast. In this case, multiple broadcasts from different banks are coalesced into a single multicast from the requested shared memory locations to the threads. To minimize bank conflicts, it is important to understand how memory addresses map to memory banks and how to optimally schedule memory requests. On devices of compute capability 5.x or newer, each bank has a bandwidth of 32 bits every clock cycle, and successive 32-bit words are assigned to successive banks. The warp size is 32 threads and the number of banks is also 32, so bank conflicts can occur between any threads in the warp. See Compute Capability 5.x in the CUDA C++ Programming Guide for further details. 9.2.3.2. Shared Memory in Matrix Multiplication (C=AB) \\uf0c1 Shared memory enables cooperation between threads in a block. When multiple threads in a block use the same data from global memory, shared memory can be used to access the data from global memory only once. Shared memory can also be used to avoid uncoalesced memory accesses by loading and storing data in a coalesced pattern from global memory and then reordering it in shared memory. Aside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a warp in shared memory. The use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for the case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN. To keep the kernels simple, M and N are multiples of 32, since the warp size (w) is 32 for current devices. A natural decomposition of the problem is to use a block and tile size of wxw threads. Therefore, in terms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see Figure 11 . A grid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different tile in C from a single tile of A and a single tile of B. Block-column matrix multiplied by block-row matrix. Block-column matrix (A) multiplied by block-row matrix (B) with resulting product matrix (C). \\uf0c1 To do this, the simpleMultiply kernel ( Unoptimized matrix multiplication ) calculates the output elements of a tile of matrix C. Unoptimized matrix multiplication __global__ void simpleMultiply ( float * a , float * b , float * c , int N ) { int row = blockIdx . y * blockDim .'},\n",
       " {'id': 407, 'content': 'y + threadIdx .'},\n",
       " {'id': 408,\n",
       "  'content': 'y ; int col = blockIdx . x ; float sum = 0.0f ; for ( int i = 0 ; i __global__ void pipeline_kernel_sync ( T * global , uint64_t * clock , size_t copy_count ) { extern __shared__ char s []; T * shared = reinterpret_cast ( s ); uint64_t clock_start = clock64 (); for ( size_t i = 0 ; i ( clock ), clock_end - clock_start ); } template __global__ void pipeline_kernel_async ( T * global , uint64_t * clock , size_t copy_count ) { extern __shared__ char s []; T * shared = reinterpret_cast ( s ); uint64_t clock_start = clock64 (); //pipeline pipe; for ( size_t i = 0 ; i ( clock ), clock_end - clock_start ); } The synchronous version for the kernel loads an element from global memory to an intermediate register and then stores the intermediate register value to shared memory. In the asynchronous version of the kernel, instructions to load from global memory and store directly into shared memory are issued as soon as __pipeline_memcpy_async() function is called. The __pipeline_wait_prior(0) will wait until all the instructions in the pipe object have been executed. Using asynchronous copies does not use any intermediate register. Not using intermediate registers can help reduce register pressure and can increase kernel occupancy. Data copied from global memory to shared memory using asynchronous copy instructions can be cached in the L1 cache or the L1 cache can be optionally bypassed. If individual CUDA threads are copying elements of 16 bytes, the L1 cache can be bypassed. This difference is illustrated in Figure 13 . Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memory \\uf0c1 We evaluate the performance of both kernels using elements of size 4B, 8B and 16B per thread i.e., using int , int2 and int4 for the template parameter. We adjust the copy_count in the kernels such that each thread block copies from 512 bytes up to 48 MB. The performance of the kernels is shown in Figure 14 . Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memory \\uf0c1 From the performance chart, the following observations can be made for this experiment. Best performance with synchronous copy is achieved when the copy_count parameter is a multiple of 4 for all three element sizes. The compiler can optimize groups of 4 load and store instructions. This is evident from the saw tooth curves. Asynchronous copy achieves better performance in nearly all cases. The async-copy does not require the copy_count parameter to be a multiple of 4, to maximize performance through compiler optimizations. Overall, best performance is achieved when using asynchronous copies with an element of size 8 or 16 bytes. 9.2.4. Local Memory \\uf0c1 Local memory is so named because its scope is local to the thread, not because of its physical location. In fact, local memory is off-chip. Hence, access to local memory is as expensive as access to global memory. In other words, the term local in the name does not imply faster access. Local memory is used only to hold automatic variables. This is done by the nvcc compiler when it determines that there is insufficient register space to hold the variable. Automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically. Inspection of the PTX assembly code (obtained by compiling with -ptx or -keep command-line options to nvcc ) reveals whether a variable has been placed in local memory during the first compilation phases. If it has, it will be declared using the .local mnemonic and accessed using the ld.local and st.local mnemonics. If it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture. There is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel (lmem) when run with the --ptxas-options=-v option. 9.2.5.'},\n",
       " {'id': 409,\n",
       "  'content': 'Texture Memory \\uf0c1 The read-only texture memory space is cached. Therefore, a texture fetch costs one device memory read only on a cache miss; otherwise, it just costs one read from the texture cache. The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance. Texture memory is also designed for streaming fetches with a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency. In certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory. 9.2.5.1. Additional Texture Capabilities \\uf0c1 If textures are fetched using tex1D() , tex2D() , or tex3D() rather than tex1Dfetch() , the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in Table 4 . Table 4. Useful Features for tex1D(), tex2D(), and tex3D() Fetches \\uf0c1 Feature Use Caveat Filtering Fast, low-precision interpolation between texels Valid only if the texture reference returns floating-point data Normalized texture coordinates Resolution-independent coding None Addressing modes Automatic handling of boundary cases 1 Can be used only with normalized texture coordinates 1 The automatic handling of boundary cases in the bottom row of Table 4 refers to how a texture coordinate is resolved when it falls outside the valid addressing range. There are two options: clamp and wrap . If x is the coordinate and N is the number of texels for a one-dimensional texture, then with clamp, x is replaced by 0 if x >> ( data_1 ); kernel2 >> ( data_2 ); 10.6. Multiple contexts \\uf0c1 CUDA work occurs within a process space for a particular GPU known as a context . The context encapsulates kernel launches and memory allocations for that GPU as well as supporting constructs such as the page tables. The context is explicit in the CUDA Driver API but is entirely implicit in the CUDA Runtime API, which creates and manages contexts automatically. With the CUDA Driver API, a CUDA application process can potentially create more than one context for a given GPU. If multiple CUDA application processes access the same GPU concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unless Multi-Process Service is in use. While multiple contexts (and their associated resources such as global memory allocations) can be allocated concurrently on a given GPU, only one of these contexts can execute work at any given moment on that GPU; contexts sharing the same GPU are time-sliced. Creating additional contexts incurs memory overhead for per-context data and time overhead for context switching. Furthermore, the need for context switching can reduce utilization when work from several contexts could otherwise execute concurrently (see also Concurrent Kernel Execution ). Therefore, it is best to avoid multiple contexts per GPU within the same CUDA application. To assist with this, the CUDA Driver API provides methods to access and manage a special context on each GPU called the primary context . These are the same contexts used implicitly by the CUDA Runtime when there is not already a current context for a thread. // When initializing the program/library CUcontext ctx ; cuDevicePrimaryCtxRetain ( & ctx , dev ); // When the program/library launches work cuCtxPushCurrent ( ctx ); kernel >> (...); cuCtxPopCurrent ( & ctx ); // When the program/library is finished with the context cuDevicePrimaryCtxRelease ( dev ); Note NVIDIA-SMI can be used to configure a GPU for exclusive process mode , which limits the number of contexts per GPU to one. This context can be current to as many threads as desired within the creating process, and cuDevicePrimaryCtxRetain will fail if a non-primary context that was created with the CUDA driver API already exists on the device. 11.'},\n",
       " {'id': 410,\n",
       "  'content': 'Instruction Optimization \\uf0c1 Awareness of how instructions are executed often permits low-level optimizations that can be useful, especially in code that is run frequently (the so-called hot spot in a program). Best practices suggest that this optimization be performed after all higher-level optimizations have been completed. 11.1. Arithmetic Instructions \\uf0c1 Single-precision floats provide the best performance, and their use is highly encouraged. The throughput of individual arithmetic operations is detailed in the CUDA C++ Programming Guide. 11.1.1.'},\n",
       " {'id': 411,\n",
       "  'content': 'Division Modulo Operations \\uf0c1 Note Low Priority: Use shift operations to avoid expensive division and modulo calculations. Integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible: If \\\\(n\\\\) is a power of 2, ( \\\\(i/n\\\\) ) is equivalent to ( \\\\(i \\\\gg {log2}(n)\\\\) ) and ( \\\\(i\\\\% n\\\\) ) is equivalent to ( \\\\(i\\\\&\\\\left( {n - 1} \\\\right)\\\\) ). The compiler will perform these conversions if n is literal. (For further information, refer to Performance Guidelines in the CUDA C++ Programming Guide).'},\n",
       " {'id': 412,\n",
       "  'content': '11.1.2. Loop Counters Signed vs. Unsigned \\uf0c1 Note Low Medium Priority: Use signed integers rather than unsigned integers as loop counters. In the C language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results. Therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic. This is of particular note with loop counters: since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned. For slightly better performance, however, they should instead be declared as signed. For example, consider the following code: for ( i = 0 ; i = 0, x != -0 , that is, signbit(x) == 0 .'},\n",
       " {'id': 413,\n",
       "  'content': 'Table 5. Formulae for exponentiation by small fractions \\uf0c1 Computation Formula x 1/9 r = rcbrt(rcbrt(x)) x -1/9 r = cbrt(rcbrt(x)) x 1/6 r = rcbrt(rsqrt(x)) x -1/6 r = rcbrt(sqrt(x)) x 1/4 r = rsqrt(rsqrt(x)) x -1/4 r = sqrt(rsqrt(x)) x 1/3 r = cbrt(x) x -1/3 r = rcbrt(x) x 1/2 r = sqrt(x) x -1/2 r = rsqrt(x) x 2/3 r = cbrt(x); r = r*r x -2/3 r = rcbrt(x); r = r*r x 3/4 r = sqrt(x); r = r*sqrt(r) x -3/4 r = rsqrt(x); r = r*sqrt(r) x 7/6 r = x*rcbrt(rsqrt(x)) x -7/6 r = (1/x) * rcbrt(sqrt(x)) x 5/4 r = x*rsqrt(rsqrt(x)) x -5/4 r = (1/x)*sqrt(rsqrt(x)) x 4/3 r = x*cbrt(x) x -4/3 r = (1/x)*rcbrt(x) x 3/2 r = x*sqrt(x) x -3/2 r = (1/x)*rsqrt(x) 11.1.6. Math Libraries \\uf0c1 Note Medium Priority: Use the fast math library whenever speed trumps precision.'},\n",
       " {'id': 414,\n",
       "  'content': 'Two types of runtime math operations are supported. They can be distinguished by their names: some have names with prepended underscores, whereas others do not (e.g., __functionName() versus functionName() ). Functions following the __functionName() naming convention map directly to the hardware level. They are faster but provide somewhat lower accuracy (e.g., __sinf(x) and __expf(x) ). Functions following functionName() naming convention are slower but have higher accuracy (e.g., sinf(x) and expf(x) ). The throughput of __sinf(x) , __cosf(x) , and __expf(x) is much greater than that of sinf(x) , cosf(x) , and expf(x) . The latter become even more expensive (about an order of magnitude slower) if the magnitude of the argument x needs to be reduced. Moreover, in such cases, the argument-reduction code uses local memory, which can affect performance even more because of the high latency of local memory. More details are available in the CUDA C++ Programming Guide . Note also that whenever sine and cosine of the same argument are computed, the sincos family of instructions should be used to optimize performance: __sincosf() for single-precision fast math (see next paragraph) sincosf() for regular single-precision sincos() for double precision The -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call. It also disables single-precision denormal support and lowers the precision of single-precision division in general.'},\n",
       " {'id': 415,\n",
       "  'content': 'This is an aggressive optimization that can both reduce numerical accuracy and alter special case handling. A more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated. Note this switch is effective only on single-precision floating point. Note Medium Priority: Prefer faster, more specialized math functions over slower, more general ones when possible. For small integer powers (e.g., x2 or x3 ), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as pow() . While compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage. This advantage is increased when several powers of the same base are needed (e.g., where both x2 and x5 are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization. For exponentiation using base 2 or 10, use the functions exp2() or expf2() and exp10() or expf10() rather than the functions pow() or powf() . Both pow() and powf() are heavy-weight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and the exponent. The functions exp2() , exp2f() , exp10() , and exp10f() , on the other hand, are similar to exp() and expf() in terms of performance, and can be as much as ten times faster than their pow() / powf() equivalents. For exponentiation with an exponent of 1/3, use the cbrt() or cbrtf() function rather than the generic exponentiation functions pow() or powf() , as the former are significantly faster than the latter. Likewise, for exponentation with an exponent of -1/3, use rcbrt() or rcbrtf() . Replace sin(π*) with sinpi() , cos(π*) with cospi() , and sincos(π*) with sincospi() . This is advantageous with regard to both accuracy and performance. As a particular example, to evaluate the sine function in degrees instead of radians, use sinpi(x/180.0) . Similarly, the single-precision functions sinpif() , cospif() , and sincospif() should replace calls to sinf() , cosf() , and sincosf() when the function argument is of the form π* . (The performance advantage sinpi() has over sin() is due to simplified argument reduction; the accuracy advantage is because sinpi() multiplies by π only implicitly, effectively using an infinitely precise mathematical π rather than a single- or double-precision approximation thereof.)\\n11.1.7. Precision-related Compiler Flags \\uf0c1 By default, the nvcc compiler generates IEEE-compliant code, but it also provides options to generate code that somewhat less accurate but faster: -ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) Another, more aggressive, option is -use_fast_math , which coerces every functionName() call to the equivalent __functionName() call. This makes the code run faster at the cost of diminished precision and accuracy.'},\n",
       " {'id': 416,\n",
       "  'content': 'See Math Libraries . 11.2. Memory Instructions \\uf0c1 Note High Priority: Minimize the use of global memory. Prefer shared memory access where possible. Memory instructions include any instruction that reads from or writes to shared, local, or global memory. When accessing uncached local or global memory, there are hundreds of clock cycles of memory latency. As an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a latency of hundreds of clock cycles to read data from global memory: __shared__ float shared [ 32 ]; __device__ float device [ 32 ]; shared [ threadIdx . x ] = device [ threadIdx . x ]; Much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete. However, it is best to avoid accessing global memory whenever possible. 12. Control Flow \\uf0c1 12.1. Branching and Divergence \\uf0c1 Note High Priority: Avoid different execution paths within the same warp. Flow control instructions ( if , switch , do , for , while ) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp. To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps. This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture of the CUDA C++ Programming Guide. A trivial example is when the controlling condition depends only on ( threadIdx / WSIZE ) where WSIZE is the warp size. In this case, no warp diverges because the controlling condition is perfectly aligned with the warps. For branches including just a few instructions, warp divergence generally results in marginal performance losses. For example, the compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Threads with a false predicate do not write results, and also do not evaluate addresses or read operands. Starting with the Volta architecture, Independent Thread Scheduling allows a warp to remain diverged outside of the data-dependent conditional block. An explicit __syncwarp() can be used to guarantee that the warp has reconverged for subsequent instructions. 12.2.'},\n",
       " {'id': 417,\n",
       "  'content': 'Branch Predication \\uf0c1 Note Low Priority: Make it easy for the compiler to use branch predication in lieu of loops or control statements. Sometimes, the compiler may unroll loops or optimize out if or switch statements by using branch predication instead. In these cases, no warp can ever diverge. The programmer can also control loop unrolling using #pragma unroll For more information on this pragma, refer to the CUDA C++ Programming Guide. When using branch predication, none of the instructions whose execution depends on the controlling condition is skipped. Instead, each such instruction is associated with a per-thread condition code or predicate that is set to true or false according to the controlling condition. Although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually executed. Instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands. The compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold. 13. Deploying CUDA Applications \\uf0c1 Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. 14. Understanding the Programming Environment \\uf0c1 With each generation of NVIDIA processors, new features are added to the GPU that CUDA can leverage. Consequently, it’s important to understand the characteristics of the architecture. Programmers should be aware of two version numbers. The first is the compute capability , and the second is the version number of the CUDA Runtime and CUDA Driver APIs. 14.1. CUDA Compute Capability \\uf0c1 The compute capability describes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor. Higher compute capability versions are supersets of lower (that is, earlier) versions, so they are backward compatible. The compute capability of the GPU in the device can be queried programmatically as illustrated in the deviceQuery CUDA Sample. The output for that program is shown in Figure 16 . This information is obtained by calling cudaGetDeviceProperties() and accessing the information in the structure it returns. Sample CUDA configuration data reported by deviceQuery \\uf0c1 The major and minor revision numbers of the compute capability are shown on the seventh line of Figure 16 . Device 0 of this system has compute capability 7.0. More details about the compute capabilities of various GPUs are in CUDA-Enabled GPUs and Compute Capabilities of the CUDA C++ Programming Guide. In particular, developers should note the number of multiprocessors on the device, the number of registers and the amount of memory available, and any special capabilities of the device. 14.2. Additional Hardware Data \\uf0c1 Certain hardware features are not described by the compute capability. For example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all GPUs irrespective of the compute capability. In such cases, call cudaGetDeviceProperties() to determine whether the device is capable of a certain feature. For example, the asyncEngineCount field of the device property structure indicates whether overlapping kernel execution and data transfers is possible (and, if so, how many concurrent transfers are possible); likewise, the canMapHostMemory field indicates whether zero-copy data transfers can be performed. 14.3. Which Compute Capability Target \\uf0c1 To target specific versions of NVIDIA hardware and CUDA software, use the -arch , -code , and -gencode options of nvcc . Code that uses the warp shuffle operation, for example, must be compiled with -arch=sm_30 (or higher compute capability). See Building for Maximum Compatibility for further discussion of the flags used for building code for multiple generations of CUDA-capable device simultaneously. 14.4. CUDA Runtime \\uf0c1 The host runtime component of the CUDA software environment can be used only by host functions. It provides functions to handle the following: Device management Context management Memory management Code module management Execution control Texture reference management Interoperability with OpenGL and Direct3D As compared to the lower-level CUDA Driver API, the CUDA Runtime greatly eases device management by providing implicit initialization, context management, and device code module management. The C++ host code generated by nvcc utilizes the CUDA Runtime, so applications that link to this code will depend on the CUDA Runtime; similarly, any code that uses the cuBLAS , cuFFT , and other CUDA Toolkit libraries will also depend on the CUDA Runtime, which is used internally by these libraries. The functions that make up the CUDA Runtime API are explained in the CUDA Toolkit Reference Manual. The CUDA Runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched. The implicit driver version checking, code initialization, CUDA context management, CUDA module management (cubin to function mapping), kernel configuration, and parameter passing are all performed by the CUDA Runtime. It comprises two principal parts: A C-style function interface ( cuda_runtime_api.h ). C++-style convenience wrappers ( cuda_runtime.h ) built on top of the C-style functions. For more information on the Runtime API, refer to CUDA Runtime of the CUDA C++ Programming Guide. 15. CUDA Compatibility Developer’s Guide \\uf0c1 CUDA Toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes. CUDA compatibility allows users to update the latest CUDA Toolkit software (including the compiler, libraries, and tools) without requiring update to the entire driver stack. The CUDA software environment consists of three parts: CUDA Toolkit (libraries, CUDA runtime and developer tools) - SDK for developers to build CUDA applications. CUDA driver - User-mode driver component used to run CUDA applications (e.g. libcuda.so on Linux systems). NVIDIA GPU device driver - Kernel-mode driver component for NVIDIA GPUs. On Linux systems, the CUDA driver and kernel mode components are delivered together in the NVIDIA display driver package. This is shown in Figure 1. Components of CUDA \\uf0c1 The CUDA compiler (nvcc), provides a way to handle CUDA and non-CUDA code (by splitting and steering compilation), along with the CUDA runtime, is part of the CUDA compiler toolchain. The CUDA Runtime API provides developers with high-level C++ interface for simplified management of devices, kernel executions etc., While the CUDA driver API provides ( CUDA Driver API ) a low-level programming interface for applications to target NVIDIA hardware. Built on top of these technologies are CUDA libraries, some of which are included in the CUDA Toolkit, while others such as cuDNN may be released independently of the CUDA Toolkit. 15.1. CUDA Toolkit Versioning \\uf0c1 Starting with CUDA 11, the toolkit versions are based on an industry-standard semantic versioning scheme: .X.Y.Z, where: .X stands for the major version - APIs have changed and binary compatibility is broken. .Y stands for the minor version - Introduction of new APIs, deprecation of old APIs, and source compatibility might be broken but binary compatibility is maintained. .Z stands for the release/patch version - new updates and patches will increment this. Each component in the toolkit is recommended to be semantically versioned. From CUDA 11.3 NVRTC is also semantically versioned. We will note some of them later on in the document. The versions of the components in the toolkit are available in this table . Compatibility of the CUDA platform is thus intended to address a few scenarios: NVIDIA driver upgrades to systems with GPUs running in production for enterprises or datacenters can be complex and may need advance planning. Delays in rolling out new NVIDIA drivers could mean that users of such systems may not have access to new features available in CUDA releases. Not requiring driver updates for new CUDA releases can mean that new versions of the software can be made available faster to users. Many software libraries and applications built on top of CUDA (e.g. math libraries or deep learning frameworks) do not have a direct dependency on the CUDA runtime, compiler or driver. In such cases, users or developers can still benefit from not having to upgrade the entire CUDA Toolkit or driver to use these libraries or frameworks. Upgrading dependencies is error-prone and time consuming, and in some corner cases, can even change the semantics of a program. Constantly recompiling with the latest CUDA Toolkit means forcing upgrades on the end-customers of an application product. Package managers facilitate this process but unexpected issues can still arise and if a bug is found, it necessitates a repeat of the above upgrade process. CUDA supports several compatibility choices: First introduced in CUDA 10, the CUDA Forward Compatible Upgrade is designed to allow users to get access to new CUDA features and run applications built with new CUDA releases on systems with older installations of the NVIDIA datacenter driver. First introduced in CUDA 11.1, CUDA Enhanced Compatibility provides two benefits: By leveraging semantic versioning across components in the CUDA Toolkit, an application can be built for one CUDA minor release (for example 11.1) and work across all future minor releases within the major family (i.e. 11.x). The CUDA runtime has relaxed the minimum driver version check and thus no longer requires a driver upgrade when moving to a new minor release. The CUDA driver ensures backward Binary Compatibility is maintained for compiled CUDA applications. Applications compiled with CUDA toolkit versions as old as 3.2 will run on newer drivers. 15.2. Source Compatibility \\uf0c1 We define source compatibility as a set of guarantees provided by the library, where a well-formed application built against a specific version of the library (using the SDK) will continue to build and run without errors when a newer version of the SDK is installed. Both the CUDA driver and the CUDA runtime are not source compatible across the different SDK releases. APIs can be deprecated and removed. Therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit. Developers are notified through deprecation and documentation mechanisms of any current or upcoming changes. This does not mean that application binaries compiled using an older toolkit will not be supported anymore. Application binaries rely on CUDA Driver API interface and even though the CUDA Driver API itself may also have changed across toolkit versions, CUDA guarantees Binary Compatibility of the CUDA Driver API interface. 15.3. Binary Compatibility \\uf0c1 We define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library. The CUDA Driver API has a versioned C-style ABI, which guarantees that applications that were running against an older driver (for example CUDA 3.2) will still run and function correctly against a modern driver (for example one shipped with CUDA 11.0). This means that even though an application source might need to be changed if it has to be recompiled against a newer CUDA Toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions. The CUDA Driver API thus is binary-compatible (the OS loader can pick up a newer version and the application continues to work) but not source-compatible (rebuilding your application against a newer SDK might require source changes). CUDA Toolkit and Minimum Driver Versions \\uf0c1 Before we proceed further on this topic, it’s important for developers to understand the concept of Minimum Driver Version and how that may affect them. Each version of the CUDA Toolkit (and runtime) requires a minimum version of the NVIDIA driver. Applications compiled against a CUDA Toolkit version will only run on systems with the specified minimum driver version for that toolkit version. Prior to CUDA 11.0, the minimum driver version for a toolkit was the same as the driver shipped with that version of the CUDA Toolkit. So, when an application is built with CUDA 11.0, it can only run on a system with an R450 or later driver. If such an application is run on a system with the R418 driver installed, CUDA initialization will return an error as can be seen in the example below. In this example, the deviceQuery sample is compiled with CUDA 11.1 and is run on a system with R418. In this scenario, CUDA initialization returns an error due to the minimum driver requirement. ubuntu@:~/samples/1_Utilities/deviceQuery $ make /usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc -m64 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp /usr/local/cuda-11.1/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o $ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.165.02 Driver Version: 418.165.02 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M.'},\n",
       " {'id': 418,\n",
       "  'content': '| |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 42C P0 28W / 70W | 0MiB / 15079MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ $ samples/bin/x86_64/linux/release/deviceQuery samples/bin/x86_64/linux/release/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) cudaGetDeviceCount returned 3 -> initialization error Result = FAIL Refer to the CUDA Toolkit Release Notes for details for the minimum driver version and the version of the driver shipped with the toolkit.'},\n",
       " {'id': 419,\n",
       "  'content': '15.3.1. CUDA Binary (cubin) Compatibility \\uf0c1 A slightly related but important topic is one of application binary compatibility across GPU architectures in CUDA. CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device. Kernels can be written using the CUDA instruction set architecture, called PTX, which is described in the PTX reference manual. It is however usually more effective to use a high-level programming language such as C++. In both cases, kernels must be compiled into binary code by nvcc (called cubins) to execute on the device. The cubins are architecture-specific. Binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions. In other words, a cubin object generated for compute capability X.y will only execute on devices of compute capability X.z where z≥y . To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability. For portability, that is, to be able to execute code on future GPU architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled by the NVIDIA driver for these future devices. More information on cubins, PTX and application compatibility can be found in the CUDA C++ Programming Guide . 15.4. CUDA Compatibility Across Minor Releases \\uf0c1 By leveraging the semantic versioning, starting with CUDA 11, components in the CUDA Toolkit will remain binary compatible across the minor versions of the toolkit. In order to maintain binary compatibility across minor versions, the CUDA runtime no longer bumps up the minimum driver version required for every minor release - this only happens when a major release is shipped. One of the main reasons a new toolchain requires a new minimum driver is to handle the JIT compilation of PTX code and the JIT linking of binary code. In this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the CUDA platform. 15.4.1. Existing CUDA Applications within Minor Versions of CUDA \\uf0c1 $ nvidia - smi +-----------------------------------------------------------------------------+ | NVIDIA - SMI 450.80.02 Driver Version : 450.80.02 CUDA Version : 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence - M | Bus - Id Disp . A | Volatile Uncorr . ECC | | Fan Temp Perf Pwr : Usage / Cap | Memory - Usage | GPU - Util Compute M . | | | | MIG M . | |===============================+======================+======================| | 0 Tesla T4 On | 00000000 : 00 : 1 E .0 Off | 0 | | N / A 39 C P8 9 W / 70 W | 0 MiB / 15109 MiB | 0 % Default | | | | N / A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes : | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ When our CUDA 11.1 application (i.e. cudart 11.1 is statically linked) is run on the system, we see that it runs successfully even when the driver reports a 11.0 version - that is, without requiring the driver or other toolkit components to be updated on the system.'},\n",
       " {'id': 420,\n",
       "  'content': '$ samples / bin / x86_64 / linux / release / deviceQuery samples / bin / x86_64 / linux / release / deviceQuery Starting ... CUDA Device Query ( Runtime API ) version ( CUDART static linking ) Detected 1 CUDA Capable device ( s ) Device 0 : \"Tesla T4\" CUDA Driver Version / Runtime Version 11.0 / 11.1 CUDA Capability Major / Minor version number : 7.5 ... ... deviceQuery , CUDA Driver = CUDART , CUDA Driver Version = 11.0 , CUDA Runtime Version = 11.1 , NumDevs = 1 Result = PASS By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features. The following sections discuss some caveats and considerations. 15.4.1.1. Handling New CUDA Features and Driver APIs \\uf0c1 A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies. For example, cuMemMap APIs or any of APIs introduced prior to CUDA 11.0, such as cudaDeviceSynchronize , do not require a driver upgrade. To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully. This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions. Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release. When working with a feature exposed in a minor version of the toolkit, the feature might not be available at runtime if the application is running against an older CUDA driver. Users wishing to take advantage of such a feature should query its availability with a dynamic check in the code: static bool hostRegisterFeatureSupported = false ; static bool hostRegisterIsDeviceAddress = false ; static error_t cuFooFunction ( int * ptr ) { int * dptr = null ; if ( hostRegisterFeatureSupported ) { cudaHostRegister ( ptr , size , flags ); if ( hostRegisterIsDeviceAddress ) { qptr = ptr ; } else { cudaHostGetDevicePointer ( & qptr , ptr , 0 ); } } else { // cudaMalloc(); // cudaMemcpy(); } gemm >> ( dptr ); cudaDeviceSynchronize (); } int main () { // rest of code here cudaDeviceGetAttribute ( & hostRegisterFeatureSupported , cudaDevAttrHostRegisterSupported , 0 ); cudaDeviceGetAttribute ( & hostRegisterIsDeviceAddress , cudaDevAttrCanUseHostPointerForRegisteredMem , 0 ); cuFooFunction ( /* malloced pointer */ ); } Alternatively the application’s interface might not work at all without a new CUDA driver and then its best to return an error right away: #define MIN_VERSION 11010 cudaError_t foo () { int version = 0 ; cudaGetDriverVersion ( & version ); if ( version #include #include void NVRTC_SAFE_CALL ( nvrtcResult result ) { if ( result != NVRTC_SUCCESS ) { std :: cerr >> syntax, which does not return any error code, the return code of cudaGetLastError() should be checked immediately after the kernel launch. Applications that do not check for CUDA API errors could at times run to completion without having noticed that the data calculated by the GPU is incomplete, invalid, or uninitialized.'},\n",
       " {'id': 421,\n",
       "  'content': 'Note The CUDA Toolkit Samples provide several helper functions for error checking with the various CUDA APIs; these helper functions are located in the samples/common/inc/helper_cuda.h file in the CUDA Toolkit. 16.3. Building for Maximum Compatibility \\uf0c1 Each generation of CUDA-capable device has an associated compute capability version that indicates the feature set supported by the device (see CUDA Compute Capability ). One or more compute capability versions can be specified to the nvcc compiler while building a file; compiling for the native compute capability for the target GPU(s) of the application is important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of GPU. When an application is built for multiple compute capabilities simultaneously (using several instances of the -gencode flag to nvcc), the binaries for the specified compute capabilities are combined into the executable, and the CUDA Driver selects the most appropriate binary at runtime according to the compute capability of the present device. If an appropriate native binary ( cubin ) is not available, but the intermediate PTX code (which targets an abstract virtual instruction set and is used for forward-compatibility) is available, then the kernel will be compiled Just In Time (JIT) (see Compiler JIT Cache Management Tools ) from the PTX to the native cubin for the device. If the PTX is also not available, then the kernel launch will fail. Windows nvcc.exe -ccbin \"C:\\\\vs2008\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel.o -c mykernel.cu Alternatively, the nvcc command-line option -arch=sm_XX can be used as a shorthand equivalent to the following more explicit -gencode= command-line options described above: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default (due to the code=compute_XX target it implies), it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 16.4.'},\n",
       " {'id': 422,\n",
       "  'content': 'Distributing the CUDA Runtime and Libraries \\uf0c1 CUDA applications are built against the CUDA Runtime library, which handles device, memory, and kernel management. Unlike the CUDA Driver, the CUDA Runtime guarantees neither forward nor backward binary compatibility across versions. It is therefore best to redistribute the CUDA Runtime library with the application when using dynamic linking or else to statically link against the CUDA Runtime. This will ensure that the executable will be able to run even if the user does not have the same CUDA Toolkit installed that the application was built against. Note When statically linking to the CUDA Runtime, multiple versions of the runtime can peacably coexist in the same application process simultaneously; for example, if an application uses one version of the CUDA Runtime, and a plugin to that application is statically linked to a different version, that is perfectly acceptable, as long as the installed NVIDIA Driver is sufficient for both. Statically-linked CUDA Runtime The easiest option is to statically link against the CUDA Runtime. This is the default if using nvcc to link in CUDA 5.5 and later. Static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the CUDA Runtime library. Dynamically-linked CUDA Runtime If static linking against the CUDA Runtime is impractical for some reason, then a dynamically-linked version of the CUDA Runtime library is also available. (This was the default and only option provided in CUDA versions 5.0 and earlier.)\\nTo use dynamic linking with the CUDA Runtime when using the nvcc from CUDA 5.5 or later to link the application, add the --cudart=shared flag to the link command line; otherwise the statically-linked CUDA Runtime library is used by default. After the application is dynamically linked against the CUDA Runtime, this version of the runtime library should be bundled with the application. It can be copied into the same directory as the application executable or into a subdirectory of that installation path. Other CUDA Libraries Although the CUDA Runtime provides the option of static linking, some libraries included in the CUDA Toolkit are available only in dynamically-linked form. As with the dynamically-linked version of the CUDA Runtime library , these libraries should be bundled with the application executable when distributing that application. 16.4.1. CUDA Toolkit Library Redistribution \\uf0c1 The CUDA Toolkit’s End-User License Agreement (EULA) allows for redistribution of many of the CUDA libraries under certain terms and conditions. This allows applications that depend on these libraries to redistribute the exact versions of the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a different version of the CUDA Toolkit (or perhaps none at all) installed on their machines. Please refer to the EULA for details.'},\n",
       " {'id': 423,\n",
       "  'content': 'Note This does not apply to the NVIDIA Driver; the end user must still download and install an NVIDIA Driver appropriate to their GPU(s) and operating system. 16.4.1.1. Which Files to Redistribute \\uf0c1 When redistributing the dynamically-linked versions of one or more CUDA libraries, it is important to identify the exact files that need to be redistributed. The following examples use the cuBLAS library from CUDA Toolkit 5.5 as an illustration: Linux In a shared library on Linux, there is a string field called the SONAME that indicates the binary compatibility level of the library. The SONAME of the library against which the application was built must match the filename of the library that is redistributed with the application. For example, in the standard CUDA Toolkit installation, the files libcublas.so and libcublas.so.5.5 are both symlinks pointing to a specific build of cuBLAS, which is named like libcublas.so.5.5.x , where x is the build number (e.g., libcublas.so.5.5.17 ). However, the SONAME of this library is given as “ libcublas.so.5.5 ”: $ objdump -p /usr/local/cuda/lib64/libcublas.so | grep SONAME SONAME libcublas.so.5.5 Because of this, even if -lcublas (with no version number specified) is used when linking the application, the SONAME found at link time implies that “ libcublas.so.5.5 ” is the name of the file that the dynamic loader will look for when loading the application and therefore must be the name of the file (or a symlink to the same) that is redistributed with the application. The ldd tool is useful for identifying the exact filenames of the libraries that the application expects to find at runtime as well as the path, if any, of the copy of that library that the dynamic loader would select when loading the application given the current library search path: $ ldd a.out | grep libcublas libcublas.so.5.5 => /usr/local/cuda/lib64/libcublas.so.5.5 Mac In a shared library on Mac OS X, there is a field called the install name that indicates the expected installation path and filename the library; the CUDA libraries also use this filename to indicate binary compatibility. The value of this field is propagated into an application built against the library and is used to locate the library of the correct version at runtime. For example, if the install name of the cuBLAS library is given as @rpath/libcublas.5.5.dylib , then the library is version 5.5 and the copy of this library redistributed with the application must be named libcublas.5.5.dylib , even though only -lcublas (with no version number specified) is used at link time. Furthermore, this file should be installed into the @rpath of the application; see Where to Install Redistributed CUDA Libraries . To view a library’s install name, use the otool -L command: $ otool -L a.out a.out: @rpath/libcublas.5.5.dylib (...) Windows The binary compatibility version of the CUDA libraries on Windows is indicated as part of the filename. For example, a 64-bit application linked to cuBLAS 5.5 will look for cublas64_55.dll at runtime, so this is the file that should be redistributed with that application, even though cublas.lib is the file that the application is linked against. For 32-bit applications, the file would be cublas32_55.dll . To verify the exact DLL filename that the application expects to find at runtime, use the dumpbin tool from the Visual Studio command prompt: $ dumpbin /IMPORTS a.exe Microsoft (R) COFF/PE Dumper Version 10.00.40219.01 Copyright (C) Microsoft Corporation. All rights reserved. Dump of file a.exe File Type: EXECUTABLE IMAGE Section contains the following imports: ... cublas64_55.dll ...'},\n",
       " {'id': 424,\n",
       "  'content': '16.4.1.2. Where to Install Redistributed CUDA Libraries \\uf0c1 Once the correct library files are identified for redistribution, they must be configured for installation into a location where the application will be able to find them. On Windows, if the CUDA Runtime or other dynamically-linked CUDA Toolkit library is placed in the same directory as the executable, Windows will locate it automatically. On Linux and Mac, the -rpath linker option should be used to instruct the executable to search its local path for these libraries before searching the system paths: Linux/Mac nvcc -I $(CUDA_HOME)/include -Xlinker \"-rpath \\'$ORIGIN\\'\" --cudart=shared -o myprogram myprogram.cu Windows nvcc.exe -ccbin \"C:\\\\vs2008\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" --cudart=shared -o \"Release\\\\myprogram.exe\" \"myprogram.cu\" Note It may be necessary to adjust the value of -ccbin to reflect the location of your Visual Studio installation. To specify an alternate path where the libraries will be distributed, use linker options similar to those below: Linux/Mac nvcc -I $(CUDA_HOME)/include -Xlinker \"-rpath \\'$ORIGIN/lib\\'\" --cudart=shared -o myprogram myprogram.cu Windows nvcc.exe -ccbin \"C:\\\\vs2008\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT /DELAY\" --cudart=shared -o \"Release\\\\myprogram.exe\" \"myprogram.cu\" For Linux and Mac, the -rpath option is used as before. For Windows, the /DELAY option is used; this requires that the application call SetDllDirectory() before the first call to any CUDA API function in order to specify the directory containing the CUDA DLLs. Note For Windows 8, SetDefaultDLLDirectories() and AddDllDirectory() should be used instead of SetDllDirectory() . Please see the MSDN documentation for these routines for more information.'},\n",
       " {'id': 425, 'content': '17.'},\n",
       " {'id': 426,\n",
       "  'content': 'Deployment Infrastructure Tools \\uf0c1 17.1. Nvidia-SMI \\uf0c1 The NVIDIA System Management Interface ( nvidia-smi ) is a command line utility that aids in the management and monitoring of NVIDIA GPU devices. This utility allows administrators to query GPU device state and, with the appropriate privileges, permits administrators to modify GPU device state. nvidia-smi is targeted at Tesla and certain Quadro GPUs, though limited support is also available on other NVIDIA GPUs. nvidia-smi ships with NVIDIA GPU display drivers on Linux, and with 64-bit Windows Server 2008 R2 and Windows 7. nvidia-smi can output queried information as XML or as human-readable plain text either to standard output or to a file. See the nvidia-smi documenation for details. Please note that new versions of nvidia-smi are not guaranteed to be backward-compatible with previous versions. 17.1.1. Queryable state \\uf0c1 ECC error counts Both correctable single-bit and detectable double-bit errors are reported. Error counts are provided for both the current boot cycle and the lifetime of the GPU. GPU utilization Current utilization rates are reported for both the compute resources of the GPU and the memory interface. Active compute process The list of active processes running on the GPU is reported, along with the corresponding process name/ID and allocated GPU memory. Clocks and performance state Max and current clock rates are reported for several important clock domains, as well as the current GPU performance state ( pstate ). Temperature and fan speed The current GPU core temperature is reported, along with fan speeds for products with active cooling. Power management The current board power draw and power limits are reported for products that report these measurements. Identification Various dynamic and static information is reported, including board serial numbers, PCI device IDs, VBIOS/Inforom version numbers and product names. 17.1.2.'},\n",
       " {'id': 427,\n",
       "  'content': 'Modifiable state \\uf0c1 ECC mode Enable and disable ECC reporting. ECC reset Clear single-bit and double-bit ECC error counts. Compute mode Indicate whether compute processes can run on the GPU and whether they run exclusively or concurrently with other compute processes. Persistence mode Indicate whether the NVIDIA driver stays loaded when no applications are connected to the GPU. It is best to enable this option in most circumstances. GPU reset Reinitialize the GPU hardware and software state via a secondary bus reset.'},\n",
       " {'id': 428,\n",
       "  'content': '17.2. NVML \\uf0c1 The NVIDIA Management Library (NVML) is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications. The NVML API is shipped with the CUDA Toolkit (since version 8.0) and is also available standalone on the NVIDIA developer website as part of the GPU Deployment Kit through a single header file accompanied by PDF documentation, stub libraries, and sample applications; see https://developer.nvidia.com/gpu-deployment-kit . Each new version of NVML is backward-compatible. An additional set of Perl and Python bindings are provided for the NVML API. These bindings expose the same features as the C-based interface and also provide backwards compatibility. The Perl bindings are provided via CPAN and the Python bindings via PyPI. All of these products ( nvidia-smi , NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality. See https://developer.nvidia.com/nvidia-management-library-nvml for additional information.'},\n",
       " {'id': 429,\n",
       "  'content': '17.3. Cluster Management Tools \\uf0c1 Managing your GPU cluster will help achieve maximum GPU utilization and help you and your users extract the best possible performance. Many of the industry’s most popular cluster management tools support CUDA GPUs via NVML. For a listing of some of these tools, see https://developer.nvidia.com/cluster-management . 17.4. Compiler JIT Cache Management Tools \\uf0c1 Any PTX device code loaded by an application at runtime is compiled further to binary code by the device driver. This is called just-in-time compilation ( JIT ). Just-in-time compilation increases application load time but allows applications to benefit from latest compiler improvements. It is also the only way for applications to run on devices that did not exist at the time the application was compiled. When JIT compilation of PTX device code is used, the NVIDIA driver caches the resulting binary code on disk. Some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables; see Just in Time Compilation of the CUDA C++ Programming Guide. 17.5. CUDA_VISIBLE_DEVICES \\uf0c1 It is possible to rearrange the collection of installed CUDA devices that will be visible to and enumerated by a CUDA application prior to the start of that application by way of the CUDA_VISIBLE_DEVICES environment variable. Devices to be made visible to the application should be included as a comma-separated list in terms of the system-wide list of enumerable devices. For example, to use only devices 0 and 2 from the system-wide list of devices, set CUDA_VISIBLE_DEVICES=0,2 before launching the application. The application will then enumerate these devices as device 0 and device 1, respectively. 18.'},\n",
       " {'id': 430,\n",
       "  'content': 'Recommendations and Best Practices \\uf0c1 This chapter contains a summary of the recommendations for optimization that are explained in this document. 18.1. Overall Performance Optimization Strategies \\uf0c1 Performance optimization revolves around three basic strategies: Maximizing parallel execution Optimizing memory usage to achieve maximum memory bandwidth Optimizing instruction usage to achieve maximum instruction throughput Maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible. Once the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible. This is done by carefully choosing the execution configuration of each kernel launch. The application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device. Optimizing memory usage starts with minimizing data transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers. Kernel access to global memory also should be minimized by maximizing the use of shared memory on the device. Sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed. The effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory. The next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns. This optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles. Shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts. As for optimizing instruction usage, the use of arithmetic instructions that have low throughput should be avoided. This suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision. Finally, particular attention must be paid to control flow instructions due to the SIMT (single instruction multiple thread) nature of the device.'},\n",
       " {'id': 431,\n",
       "  'content': '19. nvcc Compiler Switches \\uf0c1 19.1. nvcc \\uf0c1 The NVIDIA nvcc compiler driver converts .cu files into C++ for the host system and CUDA assembly or binary instructions for the device. It supports a number of command-line parameters, of which the following are especially useful for optimization and related best practices: -maxrregcount=N specifies the maximum number of registers kernels can use at a per-file level. See Register Pressure . (See also the __launch_bounds__ qualifier discussed in Execution Configuration of the CUDA C++ Programming Guide to control the number of registers used on a per-kernel basis.)\\n--ptxas-options=-v or -Xptxas=-v lists per-kernel register, shared, and constant memory usage. -ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call. 20.'},\n",
       " {'id': 432,\n",
       "  'content': 'Notices \\uf0c1 20.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 20.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 433,\n",
       "  'content': '20.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates.'},\n",
       " {'id': 434,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Maxwell Compatibility 1.1.'},\n",
       " {'id': 435,\n",
       "  'content': 'About this Document 1.2. Application Compatibility on Maxwell 1.3. Verifying Maxwell Compatibility for Existing Applications 1.3.1. Applications Using CUDA Toolkit 5.5 or Earlier 1.3.2. Applications Using CUDA Toolkit 6.0 or Later 1.4. Building Applications with Maxwell Support 1.4.1. Applications Using CUDA Toolkit 5.5 or Earlier 1.4.2. Applications Using CUDA Toolkit 6.0 or Later 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 436,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Maxwell Compatibility Guide » 1. Maxwell Compatibility v12.5 | PDF | Archive Maxwell Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Maxwell Architecture. Maxwell Compatibility \\uf0c1 1.1. About this Document \\uf0c1 This application note, Maxwell Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Maxwell Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Maxwell. 1.2. Application Compatibility on Maxwell \\uf0c1 The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Maxwell-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Maxwell-compatible PTX or cubins. 1.3. Verifying Maxwell Compatibility for Existing Applications \\uf0c1 The first step is to check that Maxwell-compatible device code (at least PTX) is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 5.5 or Earlier \\uf0c1 CUDA applications built using CUDA Toolkit versions 2.1 through 5.5 are compatible with Maxwell as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Maxwell compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 6.0 or Later \\uf0c1 CUDA applications built using CUDA Toolkit 6.0 or Later 1 are compatible with Maxwell as long as they are built to include kernels in either Maxwell-native cubin format (see Building Applications with Maxwell Support ) or PTX format (see Applications Using CUDA Toolkit 5.5 or Earlier ) or both. 1.4. Building Applications with Maxwell Support \\uf0c1 When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Maxwell depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 5.5 or Earlier \\uf0c1 The compilers included in CUDA Toolkit 5.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Fermi and Kepler, but they cannot generate cubin files native to the Maxwell architecture. To allow support for Maxwell and future architectures when using version 5.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Fermi or Kepler devices natively and on Maxwell devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Maxwell compatibility. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_35,code=compute_35 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_35,code=compute_35 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 6.0 or Later \\uf0c1 With version 6.0 of the CUDA Toolkit, nvcc can generate cubin files native to the first-generation Maxwell architecture (compute capability 5.0); CUDA Toolkit 6.5 and later further add native support for second-generation Maxwell devices (compute capability 5.2). When using CUDA Toolkit 6.x or Later, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.'},\n",
       " {'id': 437, 'content': '2.'},\n",
       " {'id': 438,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial public release. Version 1.1 Updated for second-generation Maxwell (compute capability 5.2). Version 1.2 Use CUDA C++ instead of CUDA C/C++. Updated CUDA Toolkit reference to 6.0 and Later. 3.'},\n",
       " {'id': 439,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 440,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 441,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Future CUDA Toolkit version might deprecate support for the Maxwell Architecture. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 442,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Pascal Compatibility 1.1.'},\n",
       " {'id': 443,\n",
       "  'content': 'About this Document 1.2. Application Compatibility on Pascal 1.3. Verifying Pascal Compatibility for Existing Applications 1.3.1. Applications Using CUDA Toolkit 7.5 or Earlier 1.3.2. Applications Using CUDA Toolkit 8.0 1.4. Building Applications with Pascal Support 1.4.1. Applications Using CUDA Toolkit 7.5 or Earlier 1.4.2. Applications Using CUDA Toolkit 8.0 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 444,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Pascal Compatibility Guide » 1. Pascal Compatibility v12.5 | PDF | Archive Pascal Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Pascal Architecture. Pascal Compatibility \\uf0c1 1.1. About this Document \\uf0c1 This application note, Pascal Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Pascal Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Pascal. 1.2. Application Compatibility on Pascal \\uf0c1 The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Pascal-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Pascal-compatible PTX or cubins. 1.3. Verifying Pascal Compatibility for Existing Applications \\uf0c1 The first step is to check that Pascal-compatible device code (at least PTX) is compiled in to the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 7.5 or Earlier \\uf0c1 CUDA applications built using CUDA Toolkit versions 2.1 through 7.5 are compatible with Pascal as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Pascal compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 8.0 \\uf0c1 CUDA applications built using CUDA Toolkit 8.0 are compatible with Pascal as long as they are built to include kernels in either Pascal-native cubin format (see Building Applications with Pascal Support ) or PTX format (see Applications Using CUDA Toolkit 7.5 or Earlier ) or both. 1.4. Building Applications with Pascal Support \\uf0c1 When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Pascal depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 7.5 or Earlier \\uf0c1 The compilers included in CUDA Toolkit 7.5 or earlier generate cubin files native to earlier NVIDIA architectures such as Kepler and Maxwell, but they cannot generate cubin files native to the Pascal architecture. To allow support for Pascal and future architectures when using version 7.5 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Kepler or Maxwell devices natively and on Pascal devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Pascal compatibility. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 8.0 \\uf0c1 With version 8.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Pascal architectures (compute capability 6.0 and 6.1). When using CUDA Toolkit 8.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.'},\n",
       " {'id': 445, 'content': '2.'},\n",
       " {'id': 446,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial public release. Version 1.1 Use CUDA C++ instead of CUDA C/C++ 3. Notices \\uf0c1 3.1.'},\n",
       " {'id': 447,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 448,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 449,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Volta Compatibility 1.1.'},\n",
       " {'id': 450,\n",
       "  'content': 'About this Document 1.2. Application Compatibility on Volta 1.3. Verifying Volta Compatibility for Existing Applications 1.3.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.3.2. Applications Using CUDA Toolkit 9.0 1.4. Building Applications with Volta Support 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.4.2. Applications Using CUDA Toolkit 9.0 1.4.3. Independent Thread Scheduling Compatibility 2. Revision History 3.'},\n",
       " {'id': 451, 'content': 'Notices 3.1.'},\n",
       " {'id': 452,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Volta Compatibility Guide » 1. Volta Compatibility v12.5 | PDF | Archive Volta Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Volta Architecture. Volta Compatibility \\uf0c1 1.1. About this Document \\uf0c1 This application note, Volta Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Volta Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Volta. 1.2. Application Compatibility on Volta \\uf0c1 The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Volta-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Volta-compatible PTX or cubins. 1.3. Verifying Volta Compatibility for Existing Applications \\uf0c1 The first step is to check that Volta-compatible device code (at least PTX) is compiled into the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.3.1. Applications Using CUDA Toolkit 8.0 or Earlier \\uf0c1 CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Volta as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from http://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Volta compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.3.2. Applications Using CUDA Toolkit 9.0 \\uf0c1 CUDA applications built using CUDA Toolkit 9.0 are compatible with Volta as long as they are built to include kernels in either Volta-native cubin format (see Building Applications with Volta Support ) or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ) or both. 1.4. Building Applications with Volta Support \\uf0c1 When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Volta depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier \\uf0c1 The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to the Volta architecture. To allow support for Volta and future architectures when using version 8.0 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Maxwell or Pascal devices natively and on Volta devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Volta compatibility. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.4.2. Applications Using CUDA Toolkit 9.0 \\uf0c1 With version 9.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7.0). When using CUDA Toolkit 9.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.'},\n",
       " {'id': 453,\n",
       "  'content': 'Also, note that CUDA 9.0 removes support for compute capability 2.x (Fermi) devices. Any compute_2x and sm_2x flags need to be removed from your compiler commands. 1.4.3. Independent Thread Scheduling Compatibility \\uf0c1 The Volta architecture introduces Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.0 in the CUDA C++ Programming Guide for details and corrective actions. To aid migration Volta developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -arch=compute_60 -code=sm_70 ... 2.'},\n",
       " {'id': 454,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial public release. Version 1.1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3.'},\n",
       " {'id': 455,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 456,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 457,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates.'},\n",
       " {'id': 458, 'content': 'All rights reserved.'},\n",
       " {'id': 459,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Turing Compatibility 1.1.'},\n",
       " {'id': 460,\n",
       "  'content': 'About this Document 1.2. Application Compatibility on Turing 1.3. Compatibility between Volta and Turing 1.4. Verifying Turing Compatibility for Existing Applications 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.4.2. Applications Using CUDA Toolkit 9.x 1.4.3. Applications Using CUDA Toolkit 10.0 1.5. Building Applications with Turing Support 1.5.1. Applications Using CUDA Toolkit 8.0 or Earlier 1.5.2. Applications Using CUDA Toolkit 9.x 1.5.3. Applications Using CUDA Toolkit 10.0 1.5.4. Independent Thread Scheduling Compatibility 2. Revision History 3.'},\n",
       " {'id': 461, 'content': 'Notices 3.1.'},\n",
       " {'id': 462,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Turing Compatibility Guide » 1. Turing Compatibility v12.5 | PDF | Archive Turing Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Turing GPUs. Turing Compatibility \\uf0c1 1.1. About this Document \\uf0c1 This application note, Turing Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Turing Architecture. This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Turing. 1.2. Application Compatibility on Turing \\uf0c1 The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel. Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number . For example, cubin files that target compute capability 3.0 are supported on all compute-capability 3.x (Kepler) devices but are not supported on compute-capability 5.x (Maxwell) or 6.x (Pascal) devices. For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels. Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes. Applications that already include PTX versions of their kernels should work as-is on Turing-based GPUs. Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Turing-compatible PTX or cubins. 1.3. Compatibility between Volta and Turing \\uf0c1 The Turing architecture is based on Volta’s Instruction Set Architecture ISA 7.0, extending it with new instructions. As a consequence, any binary that runs on Volta will be able to run on Turing (forward compatibility), but a Turing binary will not be able to run on Volta. Please note that Volta kernels using more than 64KB of shared memory (via the explicit opt-in, see CUDA C++ Programming Guide ) will not be able to launch on Turing, as they would exceed Turing’s shared memory capacity. Most applications compiled for Volta should run efficiently on Turing, except if the application uses heavily the Tensor Cores, or if recompiling would allow use of new Turing-specific instructions. Volta’s Tensor Core instructions can only reach half of the peak performance on Turing. Recompiling explicitly for Turing is thus recommended.'},\n",
       " {'id': 463,\n",
       "  'content': '1.4. Verifying Turing Compatibility for Existing Applications \\uf0c1 The first step is to check that Turing-compatible device code (at least PTX) is compiled into the application. The following sections show how to accomplish this for applications built with different CUDA Toolkit versions. 1.4.1. Applications Using CUDA Toolkit 8.0 or Earlier \\uf0c1 CUDA applications built using CUDA Toolkit versions 2.1 through 8.0 are compatible with Turing as long as they are built to include PTX versions of their kernels. To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch your application. When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code. If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Turing compatibility. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing. 1.4.2. Applications Using CUDA Toolkit 9.x \\uf0c1 CUDA applications built using CUDA Toolkit 9.x are compatible with Turing as long as they are built to include kernels in either Volta-native cubin format (see Compatibility between Volta and Turing ) or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ) or both. 1.4.3. Applications Using CUDA Toolkit 10.0 \\uf0c1 CUDA applications built using CUDA Toolkit 10.0 are compatible with Turing as long as they are built to include kernels in Volta-native or Turing-native cubin format (see Compatibility between Volta and Turing ), or PTX format (see Applications Using CUDA Toolkit 8.0 or Earlier ), or both. 1.5. Building Applications with Turing Support \\uf0c1 When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available. If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it. If neither is available, then the kernel launch will fail. The method used to build your application with either native cubin or at least PTX support for Turing depend on the version of the CUDA Toolkit used. The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code may be faster or of greater accuracy. 1.5.1. Applications Using CUDA Toolkit 8.0 or Earlier \\uf0c1 The compilers included in CUDA Toolkit 8.0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to Volta or Turing architecture. To allow support for Volta, Turing and future architectures when using version 8.0 or earlier of the CUDA Toolkit, the compiler must generate a PTX version of each kernel. Below are compiler settings that could be used to build mykernel.cu to run on Maxwell or Pascal devices natively and on Turing devices via PTX JIT. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Turing compatibility. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel.o -c mykernel.cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. 1.5.2. Applications Using CUDA Toolkit 9.x \\uf0c1 With versions 9.x of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7.0). When using CUDA Toolkit 9.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures.'},\n",
       " {'id': 464,\n",
       "  'content': 'Also, note that CUDA 9.0 removes support for compute capability 2.x (Fermi) devices. Any compute_2x and sm_2x flags need to be removed from your compiler commands. 1.5.3. Applications Using CUDA Toolkit 10.0 \\uf0c1 With version 10.0 of the CUDA Toolkit, nvcc can generate cubin files native to the Turing architecture (compute capability 7.5). When using CUDA Toolkit 10.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. 1.5.4.'},\n",
       " {'id': 465,\n",
       "  'content': 'Independent Thread Scheduling Compatibility \\uf0c1 The Volta and Turing architectures feature Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.0 in the CUDA C++ Programming Guide for details and corrective actions. To aid migration Volta and Turing developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -arch=compute_60 -code=sm_70 ... 2.'},\n",
       " {'id': 466,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial public release. Version 1.1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3.'},\n",
       " {'id': 467,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 468,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 469,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation.'},\n",
       " {'id': 470,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. NVIDIA Ampere GPU Architecture Compatibility 1.1.'},\n",
       " {'id': 471,\n",
       "  'content': 'About this Document 1.2. Application Compatibility on the NVIDIA Ampere GPU Architecture 1.3. Verifying Ampere Compatibility for Existing Applications 1.3.1. Applications Built Using CUDA Toolkit 10.2 or Earlier 1.3.2. Applications Built Using CUDA Toolkit 11.0 1.4. Building Applications with the NVIDIA Ampere GPU Architecture Support 1.4.1. Building Applications Using CUDA Toolkit 10.x or Earlier 1.4.2. Building Applications Using CUDA Toolkit 11.0 1.4.3. Independent Thread Scheduling Compatibility 2. Revision History 3.'},\n",
       " {'id': 472, 'content': 'Notices 3.1.'},\n",
       " {'id': 473,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Ampere Compatibility Guide » 1. NVIDIA Ampere GPU Architecture Compatibility v12.5 | PDF | Archive NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Ampere GPU Architecture. NVIDIA Ampere GPU Architecture Compatibility \\uf0c1 1.1. About this Document \\uf0c1 This application note, NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Ampere Architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ampere GPU architecture. 1.2. Application Compatibility on the NVIDIA Ampere GPU Architecture \\uf0c1 A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 7.0 is supported to run on a GPU with compute capability 7.5, however a cubin generated for compute capability 7.5 is not supported to run on a GPU with compute capability 7.0, and a cubin generated with compute capability 7.x is not supported to run on a GPU with compute capability 8.x. Kernel can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forward-compatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 7.x is supported to run on compute capability 7.x or any higher revision (major or minor), including compute capability 8.x. Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the Programming Guide. When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels, should work as-is on the NVIDIA Ampere architecture based GPUs. In such cases, rebuilding the application is not required. However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the NVIDIA Ampere architecture based GPUs. To know more about building compatible applications read Building Applications with the NVIDIA Ampere GPU Architecture Support . 1.3. Verifying Ampere Compatibility for Existing Applications \\uf0c1 The first step towards making a CUDA application compatible with the NVIDIA Ampere GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX). The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 10.2 or Earlier \\uf0c1 CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ampere architecture based GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch the application. With CUDA_FORCE_PTX_JIT=1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JIT-compiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not compatible with the NVIDIA Ampere GPU architecture and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ampere GPU architecture. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.0 \\uf0c1 CUDA applications built using CUDA Toolkit 11.0 are compatible with the NVIDIA Ampere GPU architecture as long as they are built to include kernels in native cubin (compute capability 8.0) or PTX form or both. 1.4. Building Applications with the NVIDIA Ampere GPU Architecture Support \\uf0c1 Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ampere GPU architecture. Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. 1.4.1. Building Applications Using CUDA Toolkit 10.x or Earlier \\uf0c1 The nvcc compiler included with versions 10.x (10.0, 10.1 and 10.2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7.x). When using CUDA Toolkit 10.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. It is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. For CUDA toolkits prior to 10.0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9.x supports architectures up to _60 and _61). The final -gencode to generate PTX would also need to be update – for further information and examples see the documentation for the specific CUDA toolkit version. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.2. Building Applications Using CUDA Toolkit 11.0 \\uf0c1 With versions 11.0 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ampere GPU architecture (compute capability 8.0). When using CUDA Toolkit 11.0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. 1.4.3.'},\n",
       " {'id': 474,\n",
       "  'content': 'Independent Thread Scheduling Compatibility \\uf0c1 NVIDIA GPUs since Volta architecture have Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.0 in the Programming Guide for details and corrective actions. To aid migration to the NVIDIA Ampere GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -gencode=arch=compute_60,code=sm_80 ... 2.'},\n",
       " {'id': 475,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial public release. 3.'},\n",
       " {'id': 476,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 477,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 478,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates.'},\n",
       " {'id': 479, 'content': 'All rights reserved.'},\n",
       " {'id': 480,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Hopper Architecture Compatibility 1.1.'},\n",
       " {'id': 481,\n",
       "  'content': 'About this Document 1.2. Application Compatibility on Hopper Architecture 1.3. Verifying Hopper Compatibility for Existing Applications 1.3.1. Applications Built Using CUDA Toolkit 11.7 or Earlier 1.3.2. Applications Built Using CUDA Toolkit 11.8 1.4. Building Applications with Hopper Architecture Support 1.4.1. Building Applications Using CUDA Toolkit 11.7 or Earlier 1.4.2. Building Applications Using CUDA Toolkit 11.8 1.4.3. Independent Thread Scheduling Compatibility 2. Revision History 3.'},\n",
       " {'id': 482, 'content': 'Notices 3.1.'},\n",
       " {'id': 483,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Hopper Compatibility Guide » 1. Hopper Architecture Compatibility v12.5 | PDF | Archive Hopper Compatibility Guide for CUDA Applications The guide to building CUDA applications for Hopper GPUs 1. Hopper Architecture Compatibility \\uf0c1 1.1. About this Document \\uf0c1 This application note, Hopper Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Hopper architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Hopper architecture. 1.2. Application Compatibility on Hopper Architecture \\uf0c1 A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 8.0 is supported to run on a GPU with compute capability 8.6, however a cubin generated for compute capability 8.6 is not supported to run on a GPU with compute capability 8.0, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0. Kernel can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forward-compatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.0. Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide . When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels, should work as-is on the Hopper GPUs. In such cases, rebuilding the application is not required. However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the Hopper GPUs. To know more about building compatible applications read Building Applications with Hopper Architecture Support Application binaries that include PTX version of kernels with architecture conditional features using sm_90a or compute_90a in order to take full advantage of Hopper GPU architecture, are not forward or backward compatible. 1.3. Verifying Hopper Compatibility for Existing Applications \\uf0c1 The first step towards making a CUDA application compatible with Hopper architecture is to check if the application binary already contains compatible GPU code (at least the PTX). The following sections explain how to accomplish this for an already built CUDA application. 1.3.1. Applications Built Using CUDA Toolkit 11.7 or Earlier \\uf0c1 CUDA applications built using CUDA Toolkit versions 2.1 through 11.7 are compatible with Hopper GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch the application. With CUDA_FORCE_PTX_JIT=1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JIT-compiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not Hopper architecture compatible and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is Hopper compatible. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.3.2. Applications Built Using CUDA Toolkit 11.8 \\uf0c1 CUDA applications built using CUDA Toolkit 11.8 are compatible with Hopper architecture as long as they are built to include kernels in native cubin (compute capability 9.0) or PTX form or both. 1.4. Building Applications with Hopper Architecture Support \\uf0c1 Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the Hopper architecture. Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application 2 . Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. PTX code compiled to target architecture conditional features using sm_90a or compute_90a only runs on devices with compute capability 9.0 and is not backward or forward compatible. 1.4.1. Building Applications Using CUDA Toolkit 11.7 or Earlier \\uf0c1 The nvcc compiler included with version 11.7 or earlier (11.0-11.7) of the CUDA Toolkit can generate cubins native to the NVIDIA Ampere GPU architectures (compute capability 8.x). When using CUDA Toolkit 11.7 or earlier, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. It is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. For CUDA toolkits prior to 11.0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10.x supports architectures up to sm_72 and sm_75). The final -gencode to generate PTX also needs to be updated. For further information and examples see the documentation for the specific CUDA toolkit version. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.4.2. Building Applications Using CUDA Toolkit 11.8 \\uf0c1 With versions 11.8 of the CUDA Toolkit, nvcc can generate cubin native to the Hopper architecture (compute capability 9.0). When using CUDA Toolkit 11.8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. 1.4.3.'},\n",
       " {'id': 484,\n",
       "  'content': 'Independent Thread Scheduling Compatibility \\uf0c1 NVIDIA GPUs since Volta architecture have Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity 3 , this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.x in the CUDA C++ Programming Guide for details and corrective actions. To aid migration to the Hopper architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -gencode=arch=compute_60,code=sm_90 ... 2.'},\n",
       " {'id': 485,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial public release. 3.'},\n",
       " {'id': 486,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 487,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Just-in-time compilation. 2 Starting with CUDA toolkit 11.8, this default behavior can be changed with environment variable CUDA_MODULE_LOADING. See Environment Variables in the CUDA C++ Programming Guide for details. 3 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates.'},\n",
       " {'id': 488, 'content': 'All rights reserved.'},\n",
       " {'id': 489,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. NVIDIA Ada GPU Architecture Compatibility 1.1.'},\n",
       " {'id': 490,\n",
       "  'content': 'About this Document 1.2. Application Compatibility on the NVIDIA Ada GPU Architecture 1.3. Compatibility between Ampere and Ada 1.4. Verifying Ada Compatibility for Existing Applications 1.4.1. Applications Built Using CUDA Toolkit 10.2 or Earlier 1.4.2. Applications Built Using CUDA Toolkit 11.0 through 11.7 1.4.3. Applications Built Using CUDA Toolkit 11.8 1.5. Building Applications with the NVIDIA Ada GPU Architecture Support 1.5.1. Building Applications Using CUDA Toolkit 10.x or Earlier 1.5.2. Building Applications Using CUDA Toolkit 11.0 through 11.7 1.5.3. Building Applications Using CUDA Toolkit 11.8 1.5.4. Independent Thread Scheduling Compatibility 2. Revision History 3.'},\n",
       " {'id': 491, 'content': 'Notices 3.1.'},\n",
       " {'id': 492,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Ada Compatibility Guide » 1. NVIDIA Ada GPU Architecture Compatibility v12.5 | PDF | Archive NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Ada GPUs. NVIDIA Ada GPU Architecture Compatibility \\uf0c1 1.1. About this Document \\uf0c1 This application note, NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications , is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Ada Architecture based GPUs. This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ada GPU architecture. 1.2. Application Compatibility on the NVIDIA Ada GPU Architecture \\uf0c1 A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel. Both cubin and PTX are generated for a certain target compute capability. A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability. For example, a cubin generated for compute capability 8.6 is supported to run on a GPU with compute capability 8.9; however, a cubin generated for compute capability 8.9 is not supported to run on a GPU with compute capability 8.6, and a cubin generated with compute capability 8.x is not supported to run on a GPU with compute capability 9.0. Kernels can also be compiled to a PTX form. At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution. Unlike cubin, PTX is forward-compatible. Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX. For example, PTX code generated for compute capability 8.x is supported to run on compute capability 8.x or any higher revision (major or minor), including compute capability 9.x. Therefore, although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility. To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide . When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel. If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution. Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution. If neither compatible cubin nor PTX is available, kernel launch results in a failure. Application binaries that include PTX version of kernels should work as-is on the NVIDIA Ada architecture based GPUs. In such cases, rebuilding the application is not required. However, application binaries that do not include PTX (only include cubins) need to be rebuilt to run on the NVIDIA Ada architecture based GPUs. To know more about building compatible applications, read Building Applications with the NVIDIA Ada GPU Architecture Support . 1.3. Compatibility between Ampere and Ada \\uf0c1 The NVIDIA Ada architecture is based on Ampere’s Instruction Set Architecture ISA 8.0, extending it with new instructions. As a consequence, any binary that runs on Ampere will be able to run on Ada (forward compatibility), but an Ada binary will not be able to run on Ampere. 1.4. Verifying Ada Compatibility for Existing Applications \\uf0c1 The first step towards making a CUDA application compatible with the NVIDIA Ada GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX). The following sections explain how to accomplish this for an already built CUDA application. 1.4.1. Applications Built Using CUDA Toolkit 10.2 or Earlier \\uf0c1 CUDA applications built using CUDA Toolkit versions 2.1 through 10.2 are compatible with NVIDIA Ada architecture based GPUs as long as they are built to include PTX versions of their kernels. This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https://www.nvidia.com/drivers . Set the environment variable CUDA_FORCE_PTX_JIT=1 . Launch the application. With CUDA_FORCE_PTX_JIT=1 , GPU binary code embedded in an application binary is ignored. Instead PTX code for each kernel is JIT-compiled to produce GPU binary code. An application fails to execute if it does not include PTX. This means the application is not compatible with the NVIDIA Ada GPU architecture and needs to be rebuilt for compatibility. On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ada GPU architecture. Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done. 1.4.2. Applications Built Using CUDA Toolkit 11.0 through 11.7 \\uf0c1 CUDA applications built using CUDA Toolkit 11.0 through 11.7 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native cubin (see Compatibility between Ampere and Ada ) or PTX format (see Applications Built Using CUDA Toolkit 10.2 or Earlier ), or both. 1.4.3. Applications Built Using CUDA Toolkit 11.8 \\uf0c1 CUDA applications built using CUDA Toolkit 11.8 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native or Ada-native cubin (see Compatibility between Ampere and Ada ), or PTX format (see Applications Built Using CUDA Toolkit 10.2 or Earlier ), or both. 1.5. Building Applications with the NVIDIA Ada GPU Architecture Support \\uf0c1 Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ada GPU architecture. Although it is sufficient to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX. All kernels that do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application. Especially when using large libraries, this JIT compilation can take a significant amount of time. The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible. PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be faster or of greater accuracy. 1.5.1. Building Applications Using CUDA Toolkit 10.x or Earlier \\uf0c1 The nvcc compiler included with versions 10.x (10.0, 10.1 and 10.2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7.x). When using CUDA Toolkit 10.x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. It is a shorthand equivalent to the following more explicit -gencode= command-line options used above. -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly. For CUDA toolkits prior to 10.0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9.x supports architectures up to _60 and _61). The final -gencode to generate PTX would also need to be updated. For further information and examples, see the documentation for the specific CUDA toolkit version. Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version. The code= clause specifies the back-end compilation target and can either be cubin or PTX, or both. Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures. 1.5.2. Building Applications Using CUDA Toolkit 11.0 through 11.7 \\uf0c1 The nvcc compiler included with versions 11.0 through 11.7 of the CUDA Toolkit can generate cubins native to the Ampere architecture (compute capability 8.0 and 8.6). When using CUDA Toolkit 11.0 through 11.7, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -O2 -o mykernel.o -c mykernel.cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used. For CUDA toolkits prior to 11.0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10.x supports architectures up to _72 and _75).'},\n",
       " {'id': 493,\n",
       "  'content': 'The final -gencode to generate PTX also needs to be updated. 1.5.3. Building Applications Using CUDA Toolkit 11.8 \\uf0c1 With version 11.8 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ada GPU architecture (compute capability 8.9). When using CUDA Toolkit 11.8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below. Windows nvcc.exe -ccbin \"C:\\\\vs2010\\\\VC\\\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 --compile -o \"Release\\\\mykernel.cu.obj\" \"mykernel.cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -O2 -o mykernel.o -c mykernel.cu Note compute_XX refers to a PTX version and sm_XX refers to a cubin version. 1.5.4.'},\n",
       " {'id': 494,\n",
       "  'content': 'Independent Thread Scheduling Compatibility \\uf0c1 NVIDIA GPUs since Volta architecture have Independent Thread Scheduling among threads in a warp. If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures. Please see Compute Capability 7.x in the CUDA C++ Programming Guide for details and corrective actions. To aid migration to the NVIDIA Ada GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options. nvcc -gencode=arch=compute_60,code=sm_89 ... 2.'},\n",
       " {'id': 495,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial public release. 1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization. 3.'},\n",
       " {'id': 496,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 497,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 498,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Maxwell Tuning Guide 1.1.'},\n",
       " {'id': 499,\n",
       "  'content': 'NVIDIA Maxwell Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Maxwell Tuning 1.4.1. SMM 1.4.1.1.'},\n",
       " {'id': 500,\n",
       "  'content': 'Occupancy 1.4.1.2. Instruction Scheduling 1.4.1.3. Instruction Latencies 1.4.1.4. Instruction Throughput 1.4.2. Memory Throughput 1.4.2.1. Unified L1/Texture Cache 1.4.3. Shared Memory 1.4.3.1. Shared Memory Capacity 1.4.3.2. Shared Memory Bandwidth 1.4.3.3. Fast Shared Memory Atomics 1.4.4. Dynamic Parallelism 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 501,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Maxwell Tuning Guide » 1. Maxwell Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Maxwell The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Maxwell Architecture. Maxwell Tuning Guide \\uf0c1 1.1. NVIDIA Maxwell Compute Architecture \\uf0c1 Maxwell is NVIDIA’s next-generation architecture for CUDA compute applications. Maxwell retains and extends the same CUDA programming model as in previous NVIDIA architectures such as Fermi and Kepler, and applications that follow the best practices for those architectures should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features. 1 Maxwell introduces an all-new design for the Streaming Multiprocessor ( SM ) that dramatically improves energy efficiency. Although the Kepler SMX design was extremely efficient for its generation, through its development, NVIDIA’s GPU architects saw an opportunity for another big leap forward in architectural efficiency; the Maxwell SM is the realization of that vision. Improvements to control logic partitioning, workload balancing, clock-gating granularity, compiler-based scheduling, number of instructions issued per clock cycle, and many other enhancements allow the Maxwell SM (also called SMM ) to far exceed Kepler SMX efficiency. The first Maxwell-based GPU is codenamed GM107 and is designed for use in power-limited environments like notebooks and small form factor (SFF) PCs. GM107 is described in a whitepaper entitled NVIDIA GeForce GTX 750 Ti: Featuring First-Generation Maxwell GPU Technology, Designed for Extreme Performance per Watt . 2 The first GPU using the second-generation Maxwell architecture is codenamed GM204 . Second-generation Maxwell GPUs retain the power efficiency of the earlier generation while delivering significantly higher performance. GM204 is described in a whitepaper entitled NVIDIA GeForce GTX 980: Featuring Maxwell, The Most Advanced GPU Ever Made . Compute programming features of GM204 are similar to those of GM107, except where explicitly noted in this guide. For details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices \\uf0c1 The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility \\uf0c1 Before addressing specific performance tuning issues covered in this guide, refer to the Maxwell Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Maxwell. 1.4.'},\n",
       " {'id': 502,\n",
       "  'content': 'Maxwell Tuning \\uf0c1 1.4.1. SMM \\uf0c1 The Maxwell Streaming Multiprocessor, SMM, is similar in many respects to the Kepler architecture’s SMX. The key enhancements of SMM over SMX are geared toward improving efficiency without requiring significant increases in available parallelism per SM from the application. 1.4.1.1. Occupancy \\uf0c1 The maximum number of concurrent warps per SMM remains the same as in SMX (i.e., 64), and factors influencing warp occupancy remain similar or improved over SMX: The register file size (64k 32-bit registers) is the same as that of SMX. The maximum registers per thread, 255, matches that of Kepler GK110. As with Kepler, experimentation should be used to determine the optimum balance of register spilling vs. occupancy, however. The maximum number of thread blocks per SM has been increased from 16 to 32. This should result in an automatic occupancy improvement for kernels with small thread blocks of 64 or fewer threads (shared memory and register file resource requirements permitting). Such kernels would have tended to under-utilize SMX, but less so SMM. Shared memory capacity is increased (see Shared Memory Capacity ). As such, developers can expect similar or improved occupancy on SMM without changes to their application. At the same time, warp occupancy requirements (i.e., available parallelism) for maximum device utilization are similar to or less than those of SMX (see Instruction Latencies ). 1.4.1.2. Instruction Scheduling \\uf0c1 The number of CUDA Cores per SM has been reduced to a power of two, however with Maxwell’s improved execution efficiency, performance per SM is usually within 10% of Kepler performance, and the improved area efficiency of SMM means CUDA Cores per GPU will be substantially higher vs. comparable Fermi or Kepler chips. SMM retains the same number of instruction issue slots per clock and reduces arithmetic latencies compared to the Kepler design. As with SMX, each SMM has four warp schedulers. Unlike SMX, however, all SMM core functional units are assigned to a particular scheduler, with no shared units. Along with the selection of a power-of-two number of CUDA Cores per SM, which simplifies scheduling and reduces stall cycles, this partitioning of SM computational resources in SMM is a major component of the streamlined efficiency of SMM. The power-of-two number of CUDA Cores per partition simplifies scheduling, as each of SMM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width. Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores. 1.4.1.3. Instruction Latencies \\uf0c1 Another major improvement of SMM is that dependent math latencies have been significantly reduced; a consequence of this is a further reduction of stall cycles, as the available warp-level parallelism (i.e., occupancy) on SMM should be equal to or greater than that of SMX (see Occupancy ), while at the same time each math operation takes less time to complete, improving utilization and throughput. 1.4.1.4. Instruction Throughput \\uf0c1 The most significant changes to peak instruction throughputs in SMM are as follows: The change in number of CUDA Cores per SM brings with it a corresponding change in peak single-precision floating point operations per clock per SM. However, since the number of SMs is typically increased, the result is an increase in aggregate peak throughput; furthermore, the scheduling and latency improvements also discussed above make this peak easier to approach. The throughput of many integer operations including multiply, logical operations and shift is improved. In addition, there are now specialized integer instructions that can accelerate pointer arithmetic. These instructions are most efficient when data structures are a power of two in size. Note As was already the recommended best practice, signed arithmetic should be preferred over unsigned arithmetic wherever possible for best throughput on SMM. The C language standard places more restrictions on overflow behavior for unsigned math, limiting compiler optimization opportunities. 1.4.2.'},\n",
       " {'id': 503,\n",
       "  'content': 'Memory Throughput \\uf0c1 1.4.2.1. Unified L1/Texture Cache \\uf0c1 Maxwell combines the functionality of the L1 and texture caches into a single unit. As with Kepler, global loads in Maxwell are cached in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler. In a manner similar to Kepler GK110B, GM204 retains this behavior by default but also allows applications to opt-in to caching of global loads in its unified L1/Texture cache. The opt-in mechanism is the same as with GK110B: pass the -Xptxas -dlcm=ca flag to nvcc at compile time. Local loads also are cached in L2 only, which could increase the cost of register spilling if L1 local load hit rates were high with Kepler. The balance of occupancy versus spilling should therefore be reevaluated to ensure best performance. Especially given the improvements to arithmetic latencies, code built for Maxwell may benefit from somewhat lower occupancy (due to increased registers per thread) in exchange for lower spilling. The unified L1/texture cache acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. This function previously was served by the separate L1 cache in Fermi and Kepler. Two new device attributes were added in CUDA Toolkit 6.0: globalL1CacheSupported and localL1CacheSupported . Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process. Note Enabling caching of globals in GM204 can affect occupancy. If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed. This situation is reported by the profiler.'},\n",
       " {'id': 504, 'content': '1.4.3.'},\n",
       " {'id': 505,\n",
       "  'content': 'Shared Memory \\uf0c1 1.4.3.1. Shared Memory Capacity \\uf0c1 With Fermi and Kepler, shared memory and the L1 cache shared the same on-chip storage. Maxwell, by contrast, provides dedicated space to the shared memory of each SMM, since the functionality of the L1 and texture caches have been merged in SMM. This increases the shared memory space available per SMM as compared to SMX: GM107 provides 64 KB shared memory per SMM, and GM204 further increases this to 96 KB shared memory per SMM. This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e.g., radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count. Applications no longer need to select a preference of the L1/shared split for optimal performance. For purposes of backward compatibility with Fermi and Kepler, applications may optionally continue to specify such a preference, but the preference will be ignored on Maxwell, with the full 64 KB per SMM always going to shared memory. Note While the per-SM shared memory capacity is increased in SMM, the per-thread-block limit remains 48 KB. For maximum flexibility on possible future GPUs, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block, which would for example allow at least two such thread blocks to fit per SMM. 1.4.3.2. Shared Memory Bandwidth \\uf0c1 Kepler SMX introduced an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM over Fermi for shared memory accesses of 8 or 16 bytes. However, applications could only benefit from this when storing these larger elements in shared memory (i.e., integers and fp32 values saw no benefit), and only when the developer explicitly opted into the 8-byte bank mode via the API. To simplify this, Maxwell returns to the Fermi style of shared memory banking, where banks are always four bytes wide. Aggregate shared memory bandwidth across the chip remains comparable to that of corresponding Kepler chips, given increased SM count. In this way, all applications using shared memory can now benefit from the higher bandwidth, even when storing only four-byte items into shared memory and without specifying any particular preference via the API. 1.4.3.3. Fast Shared Memory Atomics \\uf0c1 Kepler introduced a dramatically higher throughput for atomic operations to global memory as compared to Fermi. However, atomic operations to shared memory remained essentially unchanged: both architectures implemented shared memory atomics using a lock/update/unlock pattern that could be expensive in the case of high contention for updates to particular locations in shared memory. Maxwell improves upon this by implementing native shared memory atomic operations for 32-bit integers and native shared memory 32-bit and 64-bit compare-and-swap (CAS), which can be used to implement other atomic functions with reduced overhead compared to the Fermi and Kepler methods. Note Refer to the CUDA C++ Programming Guide for an example implementation of an fp64 atomicAdd() using atomicCAS() . 1.4.4. Dynamic Parallelism \\uf0c1 GK110 introduced a new architectural feature called Dynamic Parallelism, which allows the GPU to create additional work for itself. A programming model enhancement leveraging this feature was introduced in CUDA 5.0 to enable kernels running on GK110 to launch additional kernels onto the same GPU. SMM brings Dynamic Parallelism into the mainstream by supporting it across the product line, even in lower-power chips such as GM107. This will benefit developers, as it means that applications will no longer need special-case algorithm implementations for high-end GPUs that differ from those usable in more power-constrained environments. 2. Revision History \\uf0c1 Version 1.0 Initial Public Release Version 1.1 Updated for second-generation Maxwell (compute capability 5.2). Version 1.2 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3.'},\n",
       " {'id': 506,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 507,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 508,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Fermi refers to devices of compute capability 2.x, Kepler refers to devices of compute capability 3.x, and Maxwell refers to devices of compute capability 5.x. 2 The features of GM108 are similar to those of GM107. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 509,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Pascal Tuning Guide 1.1.'},\n",
       " {'id': 510,\n",
       "  'content': 'NVIDIA Pascal Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Pascal Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Occupancy 1.4.2. New Arithmetic Primitives 1.4.2.1. FP16 Arithmetic Support 1.4.2.2. INT8 Dot Product 1.4.3. Memory Throughput 1.4.3.1. High Bandwidth Memory 2 DRAM 1.4.3.2. Unified L1/Texture Cache 1.4.4. Atomic Memory Operations 1.4.5. Shared Memory 1.4.5.1. Shared Memory Capacity 1.4.5.2. Shared Memory Bandwidth 1.4.6. Inter-GPU Communication 1.4.6.1. NVLink Interconnect 1.4.6.2. GPUDirect RDMA Bandwidth 1.4.7. Compute Preemption 1.4.8. Unified Memory Improvements 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 511,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Pascal Tuning Guide » 1. Pascal Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Pascal The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Pascal Architecture. Pascal Tuning Guide \\uf0c1 1.1. NVIDIA Pascal Compute Architecture \\uf0c1 Pascal retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell, and applications that follow the best practices for those architectures should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Pascal architectural features. 1 Pascal architecture comprises two major variants: GP100 and GP104. 2 A detailed overview of the major improvements in GP100 and GP104 over earlier NVIDIA architectures are described in a pair of white papers entitled NVIDIA Tesla P100: The Most Advanced Datacenter Accelerator Ever Built for GP100 and NVIDIA GeForce GTX 1080: Gaming Perfected for GP104. For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . Some of the Pascal features described in this guide are specific to either GP100 or GP104, as noted; if not specified, features apply to both Pascal variants. 1.2. CUDA Best Practices \\uf0c1 The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility \\uf0c1 Before addressing specific performance tuning issues covered in this guide, refer to the Pascal Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Pascal. 1.4.'},\n",
       " {'id': 512,\n",
       "  'content': 'Pascal Tuning \\uf0c1 1.4.1. Streaming Multiprocessor \\uf0c1 The Pascal Streaming Multiprocessor (SM) is in many respects similar to that of Maxwell. Pascal further improves the already excellent power efficiency provided by the Maxwell architecture through both an improved 16-nm FinFET manufacturing process and various architectural modifications. 1.4.1.1. Instruction Scheduling \\uf0c1 Like Maxwell, Pascal employs a power-of-two number of CUDA Cores per partition. This simplifies scheduling, since each of the SM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width (32). Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores. GP100 and GP104 designs incorporate different numbers of CUDA Cores per SM. Like Maxwell, each GP104 SM provides four warp schedulers managing a total of 128 single-precision (FP32) and four double-precision (FP64) cores. A GP104 processor provides up to 20 SMs, and the similar GP102 design provides up to 30 SMs. By contrast GP100 provides smaller but more numerous SMs. Each GP100 provides up to 60 SMs. 3 Each SM contains two warp schedulers managing a total of 64 FP32 and 32 FP64 cores. The resulting 2:1 ratio of FP32 to FP64 cores aligns well with GP100’s new datapath configuration, allowing Pascal to process FP64 workloads more efficiently than Kepler GK210, the previous NVIDIA architecture to emphasize FP64 performance. 1.4.1.2. Occupancy \\uf0c1 The maximum number of concurrent warps per SM remains the same as in Maxwell (i.e., 64), and other factors influencing warp occupancy remain similar as well: The register file size (64k 32-bit registers) is the same as that of Maxwell. The maximum registers per thread, 255, matches that of Maxwell. As with previous architectures, experimentation should be used to determine the optimum balance of register spilling vs. occupancy, however. The maximum number of thread blocks per SM is 32, the same as Maxwell. Shared memory capacity per SM is 64KB for GP100 and 96KB for GP104. For comparison, Maxwell provided 96KB and up to 112KB of shared memory, respectively. But each GP100 SM contains fewer CUDA Cores, so the shared memory available per core actually increases on GP100. The maximum shared memory per block remains limited at 48KB as with prior architectures (see Shared Memory Capacity ). As such, developers can expect similar occupancy as on Maxwell without changes to their application. As a result of scheduling improvements relative to Kepler, warp occupancy requirements (i.e., available parallelism) needed for maximum device utilization are generally reduced. 1.4.2.'},\n",
       " {'id': 513,\n",
       "  'content': 'New Arithmetic Primitives \\uf0c1 1.4.2.1. FP16 Arithmetic Support \\uf0c1 Pascal provides improved FP16 support for applications, like deep learning, that are tolerant of low floating-point precision. The half type is used to represent FP16 values on the device. As with Maxwell, FP16 storage can be used to reduce the required memory footprint and bandwidth compared to FP32 or FP64 storage. Pascal also adds support for native FP16 instructions. Peak FP16 throughput is attained by using a paired operation to perform two FP16 instructions per core simultaneously. To be eligible for the paired operation the operands must be stored in a half2 vector type. GP100 and GP104 provide different FP16 throughputs. GP100, designed with training deep neural networks in mind, provides FP16 throughput up to 2x that of FP32 arithmetic. On GP104, FP16 throughput is lower, 1/64th that of FP32. However, compensating for reduced FP16 throughput, GP104 provides additional high-throughput INT8 support not available in GP100. 1.4.2.2. INT8 Dot Product \\uf0c1 GP104 provides specialized instructions for two-way and four-way integer dot products. These are well suited for accelerating Deep Learning inference workloads. The __dp4a intrinsic computes a dot product of four 8-bit integers with accumulation into a 32-bit integer. Similarly, __dp2a performs a two-element dot product between two 16-bit integers in one vector, and two 8-bit integers in another with accumulation into a 32-bit integer. Both instructions offer a throughput equal to that of FP32 arithmetic. 1.4.3.'},\n",
       " {'id': 514,\n",
       "  'content': 'Memory Throughput \\uf0c1 1.4.3.1. High Bandwidth Memory 2 DRAM \\uf0c1 GP100 uses High Bandwidth Memory 2 (HBM2) for its DRAM. HBM2 memories are stacked on a single silicon package along with the GPU die. This allows much wider interfaces at similar power compared to traditional GDDR technology. GP100 is linked to up to four stacks of HBM2 and uses two 512-bit memory controllers for each stack. The effective width of the memory bus is then 4096 bits, a significant increase over the 384 bits in GM200. This allows a tremendous boost in peak bandwidth even at reduced memory clocks. Thus, the GP100 equipped Tesla P100 has a peak bandwidth of 732 GB/s with a modest 715 MHz memory clock. DRAM access latencies remain similar to those observed on Maxwell. In order to hide DRAM latencies at full HBM2 bandwidth, more memory accesses must be kept in flight compared to GPUs equipped with traditional GDDR5. Helpfully, the large complement of SMs in GP100 will typically boost the number of concurrent threads (and thus reads-in-flight) compared to previous architectures. Resource constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread. The GP100 GPU’s register files, shared memories, L1 and L2 caches, and DRAM are all protected by Single-Error Correct Double-Error Detect (SECDED) ECC code. When enabling ECC support on a Kepler GK210, the available DRAM would be reduced by 6.25% to allow for the storage of ECC bits. Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled. HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection. 4 1.4.3.2. Unified L1/Texture Cache \\uf0c1 Like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. By default, GP100 caches global loads in the L1/Texture cache. In contrast, GP104 follows Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism. As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time. Kepler serviced loads at a granularity of 128B when L1 caching of global loads was enabled and 32B otherwise. On Pascal the data access unit is 32B regardless of whether global loads are cached in L1. So it is no longer necessary to turn off L1 caching in order to reduce wasted global memory transactions associated with uncoalesced accesses. Unlike Maxwell, Pascal caches thread-local memory in the L1 cache. This can mitigate the cost of register spills compared to Maxwell. The balance of occupancy versus spilling should therefore be re-evaluated to ensure best performance. Two new device attributes were added in CUDA Toolkit 6.0: globalL1CacheSupported and localL1CacheSupported . Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process. Note Enabling caching of globals in GP104 can affect occupancy. If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed. This situation is reported by the profiler.'},\n",
       " {'id': 515,\n",
       "  'content': '1.4.4. Atomic Memory Operations \\uf0c1 Like Maxwell, Pascal provides native shared memory atomic operations for 32-bit integer arithmetic, along with native 32 or 64-bit compare-and-swap (CAS). Developers coming from Kepler, where shared memory atomics were implemented in software using a lock/update/unlock sequence, should see a large performance improvement particularly for heavily contended shared-memory atomics. Pascal also extends atomic addition in global memory to function on FP64 data. The atomicAdd() function in CUDA has thus been generalized to support 32 and 64-bit integer and floating-point types. The rounding mode for all floating-point atomic operations is round-to-nearest-even in Pascal. As in previous generations FP32 atomicAdd() flushes denormalized values to zero. For GP100 atomic operations may target the memories of peer GPUs connected through NVLink. Peer-to-peer atomics over NVLink use the same API as atomics targeting global memory. GPUs connected via PCIE do not support this feature. Pascal GPUs provide support system-wide atomic operations targeting migratable allocations 5 If system-wide atomic visibility is desired, operations targeting migratable memory must specify a system scope by using the atomic[Op]_system() intrinsics 6 . Using the device-scope atomics (e.g. atomicAdd() ) on migratable memory remains valid, but enforces atomic visibility only within the local GPU. Note Given the potential for incorrect usage of atomic scopes, it is recommended that applications use compute-sanitizer to detect and eliminate errors. As implemented for Pascal, system-wide atomics are intended to allow developers to experiment with enhanced memory models. They are implemented in software and some care is required to achieve good performance. When an atomic targets a migratable address backed by a remote memory space, the local processor page-faults so that the kernel can migrate the appropriate memory page to local memory. Then the usual hardware instructions are used to execute the atomic. Since the page is now locally resident, subsequent atomics from the same processor will not result in additional page-faults. However, atomic updates from different processors can incur frequent page-faults. 1.4.5. Shared Memory \\uf0c1 1.4.5.1. Shared Memory Capacity \\uf0c1 For Kepler, shared memory and the L1 cache shared the same on-chip storage. Maxwell and Pascal, by contrast, provide dedicated space to the shared memory of each SM, since the functionality of the L1 and texture caches have been merged. This increases the shared memory space available per SM as compared to Kepler: GP100 offers 64 KB shared memory per SM, and GP104 provides 96 KB per SM. This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e.g., radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count. Applications no longer need to select a preference of the L1/shared split for optimal performance. Note Thread-blocks remain limited to 48 KB of shared memory. For maximum flexibility, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block. This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM. 1.4.5.2. Shared Memory Bandwidth \\uf0c1 Kepler provided an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM for shared memory accesses of 8 or 16 bytes. However, applications could only benefit from this when storing these larger elements in shared memory (i.e., integers and fp32 values saw no benefit), and only when the developer explicitly opted in to the 8-byte bank mode via the API. To simplify this, Pascal follows Maxwell in returning to fixed four-byte banks. This allows all applications using shared memory to benefit from the higher bandwidth, without specifying any particular preference via the API. 1.4.6. Inter-GPU Communication \\uf0c1 1.4.6.1. NVLink Interconnect \\uf0c1 NVLink is NVIDIA’s new high-speed data interconnect. NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory. GP100 supports up to four NVLink connections with each connection carrying up to 40 GB/s of bi-directional bandwidth. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 1.4.6.2. GPUDirect RDMA Bandwidth \\uf0c1 GPUDirect RDMA allows third party devices such as network interface cards (NICs) to directly access GPU memory. This eliminates unnecessary copy buffers, lowers CPU overhead, and significantly decreases the latency of MPI send/receive messages from/to GPU memory. Pascal doubles the delivered RDMA bandwidth when reading data from the source GPU memory and writing to the target NIC memory over PCIe. 1.4.7. Compute Preemption \\uf0c1 Compute Preemption is a new feature specific to GP100. Compute Preemption allows compute tasks running on the GPU to be interrupted at instruction-level granularity. The execution context (registers, shared memory, etc.)\\nare swapped to GPU DRAM so that another application can be swapped in and run. Compute preemption offers two key advantages for developers: Long-running kernels no longer need to be broken up into small timeslices to avoid an unresponsive graphical user interface or kernel timeouts when a GPU is used simultaneously for compute and graphics. Interactive kernel debugging on a single-GPU system is now possible. 1.4.8. Unified Memory Improvements \\uf0c1 Pascal offers new hardware capabilities to extend Unified Memory (UM) support. An extended 49-bit virtual addressing space allows Pascal GPUs to address the full 48-bit virtual address space of modern CPUs as well as the memories of all GPUs in the system through a single virtual address space, not limited by the physical memory sizes of any one processor. Pascal GPUs also support memory page faulting. Page faulting allows applications to access the same managed memory allocations from both host and device without explicit synchronization. It also removes the need for the CUDA runtime to pre-synchronize all managed memory allocations before each kernel launch. Instead, when a kernel accesses a non-resident memory page, it faults, and the page can be migrated to the GPU memory on-demand, or mapped into the GPU address space for access over PCIe/NVLink interfaces. These features boost performance on Pascal for many typical UM workloads. In cases where the UM heuristics prove suboptimal, further tuning is possible through a set of migration hints that can be added to the source code. On supporting operating system platforms, any memory allocated with the default OS allocator (for example, malloc or new) can be accessed from both GPU and CPU code using the same pointer. In fact, all system virtual memory can be accessed from the GPU. On such systems, there is no need to explicitly allocate managed memory using cudaMallocManaged() . 2.'},\n",
       " {'id': 516,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial Public Release Version 1.1 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide. 3.'},\n",
       " {'id': 517,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 518,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 519,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, and Pascal refers to device of compute capability 6.x. 2 The specific compute capabilities of GP100 and GP104 are 6.0 and 6.1, respectively. The GP102 architecture is similar to GP104. 3 The Tesla P100 has 56 SMs enabled. 4 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory. 5 Migratable, or Unified Memory (UM) , allocations are made with cudaMallocManaged() or, for systems with Heterogeneous Memory Management (HMM) support, malloc() . 6 Here [Op] would be one of Add , CAS , etc.'},\n",
       " {'id': 520,\n",
       "  'content': 'Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 521,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Volta Tuning Guide 1.1.'},\n",
       " {'id': 522,\n",
       "  'content': 'NVIDIA Volta Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Volta Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Independent Thread Scheduling 1.4.1.3. Occupancy 1.4.1.4.'},\n",
       " {'id': 523,\n",
       "  'content': 'Integer Arithmetic 1.4.2. Tensor Core Operations 1.4.3. Memory Throughput 1.4.3.1. High Bandwidth Memory 1.4.3.2. Unified Shared Memory/L1/Texture Cache 1.4.4. Cooperative Groups 1.4.5.'},\n",
       " {'id': 524,\n",
       "  'content': 'Multi-Process Service 1.4.6. NVLink Interconnect 2. Revision History 3.'},\n",
       " {'id': 525, 'content': 'Notices 3.1.'},\n",
       " {'id': 526,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Volta Tuning Guide » 1. Volta Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Volta The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Volta Architecture. Volta Tuning Guide \\uf0c1 1.1. NVIDIA Volta Compute Architecture \\uf0c1 Volta is NVIDIA’s latest architecture for CUDA compute applications. Volta retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell and Pascal, and applications that follow the best practices for those architectures should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Volta architectural features. 1 Volta architecture comprises a single variant: GV100. A detailed overview of the major improvements in GV100 over earlier NVIDIA architectures is provided in a white paper entitled NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU . For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices \\uf0c1 The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility \\uf0c1 Before addressing specific performance tuning issues covered in this guide, refer to the Volta Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Volta. 1.4.'},\n",
       " {'id': 527,\n",
       "  'content': 'Volta Tuning \\uf0c1 1.4.1. Streaming Multiprocessor \\uf0c1 The Volta Streaming Multiprocessor (SM) provides the following improvements over Pascal. 1.4.1.1. Instruction Scheduling \\uf0c1 Each Volta SM includes 4 warp-scheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations are reduced to four clock cycles, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp. Many more warps are, of course, recommended to cover the much greater latency of memory transactions and control-flow operations. Similar to GP100, the GV100 SM provides 64 FP32 cores and 32 FP64 cores. The GV100 SM additionally includes 64 INT32 cores and 8 mixed-precision Tensor Cores. GV100 provides up to 84 SMs. 1.4.1.2. Independent Thread Scheduling \\uf0c1 The Volta architecture introduces Independent Thread Scheduling among threads in a warp. This feature enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures. When porting existing codes to Volta, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide . To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid. Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier. The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy \\uf0c1 The maximum number of concurrent warps per SM remains the same as in Pascal (i.e., 64), and other factors influencing warp occupancy remain similar as well: The register file size is 64k 32-bit registers per SM. The maximum registers per thread is 255. The maximum number of thread blocks per SM is 32. Shared memory capacity per SM is 96KB, similar to GP104, and a 50% increase compared to GP100. Overall, developers can expect similar occupancy as on Pascal without changes to their application. 1.4.1.4. Integer Arithmetic \\uf0c1 Unlike Pascal GPUs, the GV100 SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can now interleave pointer arithmetic with floating-point computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations \\uf0c1 Each Tensor Core performs the following operation: D = AxB + C, where A, B, C, and D are 4x4 matrices. The matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition with the other intermediate products for a 4x4x4 matrix multiply. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller elements. The Volta tensor cores are exposed as Warp-Level Matrix Operations in the CUDA 9 C++ API. The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently use Tensor Cores from a CUDA-C++ program. At the CUDA level, the warp-level interface assumes 16x16 size matrices spanning all 32 threads of the warp. See the CUDA C++ Programming Guide for more information. 1.4.3.'},\n",
       " {'id': 528,\n",
       "  'content': 'Memory Throughput \\uf0c1 1.4.3.1. High Bandwidth Memory \\uf0c1 GV100 uses up to eight memory dies per HBM2 stack and four stacks, with a maximum of 32 GB of GPU memory. A faster and more efficient HBM2 implementation delivers up to 900 GB/s of peak memory bandwidth, compared to 732 GB/s for GP100. This combination of a new generation HBM2 memory, and a new generation memory controller, in Volta provides 1.5x delivered memory bandwidth, compared to Pascal GP100—and a greater than 95% memory bandwidth efficiency running many workloads. In order to hide the DRAM latencies at full HBM2 bandwidth more memory accesses must be kept in flight, compared to GPUs equipped with traditional GDDR5. This is accomplished by the large complement of SMs in GV100, which typically boost the number of concurrent threads, and thus the reads-in-flight, compared to previous architectures. Resource-constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread. 1.4.3.2. Unified Shared Memory/L1/Texture Cache \\uf0c1 In Volta the L1 cache, texture cache, and shared memory are backed by a combined 128 KB data cache. As in previous architectures, the portion of the cache dedicated to shared memory (known as the carveout ) can be selected at runtime using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . Volta supports shared memory capacities of 0, 8, 16, 32, 64, or 96 KB per SM. A new feature, Volta enables a single thread block to address the full 96 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Pascal, Volta combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Volta increases the maximum capacity of the L1 cache to 128 KB, more than 7x larger than the GP100 L1. Another benefit of its union with shared memory, the Volta L1 improves in terms of both latency and bandwidth compared to Pascal. The result is that for many applications Volta narrows the performance gap between explicitly managed shared memory and direct access to device memory. Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 1.4.4. Cooperative Groups \\uf0c1 The Volta architecture introduced Independent Thread Scheduling, which enables intra-warp synchronization patterns that were previously not possible. To efficiently express these new patterns, CUDA 9 introduces Cooperative Groups. This is an extension to the CUDA programming model for organizing groups of communicating threads. Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions. 1.4.5. Multi-Process Service \\uf0c1 The Volta Multi-Process Service is significantly improved compared to previous architecutres, both in terms of performance and robustness. Intermediary software schedulers, used for MPS with previous architectures, have been replaced by hardware accelerated units within the GPU. MPS clients now submit tasks directly to the GPU work queues, significantly decreasing submission latency and increasing aggregate throughput. The limit on the number of MPS clients has also been increased by 3x to 48. Volta MPS also provides each client with an isolated address space, 3 and extends Unified Memory support for MPS applications. Volta MPS also provides control for clients to restrict each client to a fraction of the GPU execution resources. Developers can use this feature to reduce or eliminate head-of-line blocking where work from one MPS client overwhelms GPU execution resources and prevents other clients from making progress, and thus improve average latency and jitter accross the system. 1.4.6. NVLink Interconnect \\uf0c1 NVLink is NVIDIA’s high-speed data interconnect. NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory. GV100 supports up to six NVLink connections with each connection carrying up to 50 GB/s of bi-directional bandwidth. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2.'},\n",
       " {'id': 529,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.0 Initial Public Release Version 1.1 Added Cooperative Groups section. Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide . 3.'},\n",
       " {'id': 530,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 531,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 532,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, and Volta refers to devices of compute capability 7.x. 2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction. 3 As with previous architectures, MPS does not provide fatal fault isolation between clients. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 533,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Turing Tuning Guide 1.1.'},\n",
       " {'id': 534,\n",
       "  'content': 'NVIDIA Turing Compute Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. Turing Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Instruction Scheduling 1.4.1.2. Independent Thread Scheduling 1.4.1.3. Occupancy 1.4.1.4.'},\n",
       " {'id': 535,\n",
       "  'content': 'Integer Arithmetic 1.4.2. Tensor Core Operations 1.4.3. Memory Throughput 1.4.3.1. Unified Shared Memory/L1/Texture Cache 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 536,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Turing Tuning Guide » 1. Turing Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Turing The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Turing Architecture. Turing Tuning Guide \\uf0c1 1.1. NVIDIA Turing Compute Architecture \\uf0c1 Turing is NVIDIA’s latest architecture for CUDA compute applications. Turing retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Pascal and Volta, and applications that follow the best practices for those architectures should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Turing architectural features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices \\uf0c1 The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility \\uf0c1 Before addressing specific performance tuning issues covered in this guide, refer to the Turing Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Turing. 1.4.'},\n",
       " {'id': 537,\n",
       "  'content': 'Turing Tuning \\uf0c1 1.4.1. Streaming Multiprocessor \\uf0c1 The Turing Streaming Multiprocessor (SM) is based on the same major architecture (7.x) as Volta, and provides similar improvements over Pascal. 1.4.1.1. Instruction Scheduling \\uf0c1 Each Turing SM includes 4 warp-scheduler units. Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units. Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle. Dependent instruction issue latency for core FMA math operations is four clock cycles, like Volta, compared to six cycles on Pascal. As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp, or by 16 warps per SM without any instuction-level parallelism. Like Volta, the Turing SM provides 64 FP32 cores, 64 INT32 cores and 8 improved mixed-precision Tensor Cores. Turing has a lower double precision throughput than Volta with only 2 FP64 cores. 1.4.1.2. Independent Thread Scheduling \\uf0c1 The Turing architecture features the same Independent Thread Scheduling introduced with Volta. This enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code. However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures. When porting existing codes to Volta or Turing, the following three code patterns need careful attention. For more details see the CUDA C++ Programming Guide . To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix. The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic. Applications that assume reads and writes are implicitly visible to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory. Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid. Applications using __syncthreads() or the PTX bar.sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier. The racecheck and synccheck tools provided by compute-sanitizer can help with locating violations. 1.4.1.3. Occupancy \\uf0c1 The maximum number of concurrent warps per SM is 32 on Turing (versus 64 on Volta). Other factors influencing warp occupancy remain otherwise similar: The register file size is 64k 32-bit registers per SM. The maximum registers per thread is 255.'},\n",
       " {'id': 538,\n",
       "  'content': 'The maximum number of thread blocks per SM is 16. Shared memory capacity per SM is 64KB. Overall, developers can expect similar occupancy as on Pascal or Volta without changes to their application. 1.4.1.4. Integer Arithmetic \\uf0c1 Similar to Volta, the Turing SM includes dedicated FP32 and INT32 cores. This enables simultaneous execution of FP32 and INT32 operations. Applications can interleave pointer arithmetic with floating-point computations. For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput. 1.4.2. Tensor Core Operations \\uf0c1 Volta introduced Tensor Cores to accelerate matrix multiply operations on mixed precision floating point data. Turing adds acceleration for integer matrix multiply operations. The tensor cores are exposed as Warp-Level Matrix Operations in the CUDA 10 C++ API. The API provides specialized matrix load, matrix multiply and accumulate, and matrix store operations, where each warp processes a small matrix fragment, allowing to efficiently use Tensor Cores from a CUDA-C++ program. In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller matrix fragments. Each Tensor Core performs the matrix multiply-accumulate: D = A x B + C. The Tensor Cores support half precision matrix multiplication, where the matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be either FP16 or FP32 matrices. When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition. CUDA 10 supports several fragment sizes, 16x16x16, 32x8x16, and 8x32x16 to use the Tensor Cores on Volta or Turing with FP16 inputs. Any binary compiled for Volta will run on Turing, but Volta binaries using Tensor Cores will only be able to reach half of Turing’s Tensor Core peak performance. Recompiling the binary specifically for Turing would allow it to reach the peak performance. See the Turing Compatibility Guide for more information. Turing’s Tensor Core supports integer matrix multiply operations, which can operate on 8-bit, 4-bit and 1-bit integer inputs, with 32-bit integer accumulation. When operating on 8-bit inputs, CUDA exposes fragment sizes of 16x16x16, 32x8x16, and 8x32x16. For sub-byte operations the fragment sizes available are 8x8x32 for 4-bit inputs, or 8x8x128 for 1-bit inputs. See the CUDA C++ Programming Guide for more information. 1.4.3.'},\n",
       " {'id': 539,\n",
       "  'content': 'Memory Throughput \\uf0c1 1.4.3.1. Unified Shared Memory/L1/Texture Cache \\uf0c1 Turing features a unified L1 / Shared Memory cache similar to the one introduced in Volta, but with a smaller size. The total size of the unified L1 / Shared Memory cache in Turing is 96 KB. The portion of the cache dedicated to shared memory or L1 (known as the carveout ) can be changed at runtime, either automatically by the driver, or manually using the cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . Turing supports two carveout configurations, either with 64 KB of shared memory and 32 KB of L1, or with 32 KB of shared memory and 64 KB of L1. Turing allows a single thread block to address the full 64 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Pascal and Volta, Turing combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. The state-of-the-art L1 cache in Volta and Turing offers lower latency, higher bandwidth, and higher capacity compared to the earlier architectures. Like Volta, Turing’s L1 can cache write operations (write-through). The result is that for many applications Volta and Turing narrow the performance gap between explicitly managed shared memory and direct access to device memory. Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance. 2. Revision History \\uf0c1 Version 1.0 Initial Public Release Version 1.1 Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide . 3.'},\n",
       " {'id': 540,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 541,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'id': 542,\n",
       "  'content': 'Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to devices of compute capability 6.x, Volta refers to devices of compute capability 7.0, and Turing refers to devices of compute capability 7.5. 2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates.'},\n",
       " {'id': 543, 'content': 'All rights reserved.'},\n",
       " {'id': 544,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. NVIDIA Ampere GPU Architecture Tuning Guide 1.1.'},\n",
       " {'id': 545,\n",
       "  'content': 'NVIDIA Ampere GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Ampere GPU Architecture Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1.'},\n",
       " {'id': 546,\n",
       "  'content': 'Occupancy 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory 1.4.1.3. Hardware Acceleration for Split Arrive/Wait Barrier 1.4.1.4. Warp level support for Reduction Operations 1.4.1.5. Improved Tensor Core Operations 1.4.1.6. Improved FP32 throughput 1.4.2. Memory System 1.4.2.1. Increased Memory Capacity and High Bandwidth Memory 1.4.2.2. Increased L2 capacity and L2 Residency Controls 1.4.2.3. Unified Shared Memory/L1/Texture Cache 1.4.3. Third Generation NVLink 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 547,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Ampere Tuning Guide » 1. NVIDIA Ampere GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ampere GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ampere GPU Architecture. NVIDIA Ampere GPU Architecture Tuning Guide \\uf0c1 1.1. NVIDIA Ampere GPU Architecture \\uf0c1 The NVIDIA Ampere GPU architecture is NVIDIA’s latest architecture for CUDA compute applications. The NVIDIA Ampere GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as Turing and Volta, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA A100 GPU without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ampere GPU architecture’s features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices \\uf0c1 The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility \\uf0c1 Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ampere GPU Architecture. 1.4. NVIDIA Ampere GPU Architecture Tuning \\uf0c1 1.4.1. Streaming Multiprocessor \\uf0c1 The NVIDIA Ampere GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Volta and Turing. 1.4.1.1. Occupancy \\uf0c1 The maximum number of concurrent warps per SM remains the same as in Volta (i.e., 64) for compute capability 8.0, while for compute capability 8.6 it is 48. Other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 8.0 (i.e., A100 GPUs) and 16 for GPUs with compute capability 8.6. For devices of compute capability 8.0 (i.e., A100 GPUs) shared memory capacity per SM is 164 KB, a 71% increase compared to V100’s capacity of 96 KB. For GPUs with compute capability 8.6, shared memory capacity per SM is 100 KB. For devices of compute capability 8.0 (i.e., A100 GPUs) the maximum shared memory per thread block is 163 KB. For GPUs with compute capability 8.6 maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on Volta without changes to their application. 1.4.1.2. Asynchronous Data Copy from Global Memory to Shared Memory \\uf0c1 The NVIDIA Ampere GPU architecture adds hardware acceleration for copying data from global memory to shared memory. These copy instructions are asynchronous, with respect to computation and allow users to explicitly control overlap of compute with data movement from global memory into the SM. These instructions also avoid using extra registers for memory copies and can also bypass the L1 cache. This new feature is exposed via the pipeline API in CUDA. For more information please refer to the section on Async Copy in the CUDA C++ Programming Guide . 1.4.1.3. Hardware Acceleration for Split Arrive/Wait Barrier \\uf0c1 The NVIDIA Ampere GPU architecture adds hardware acceleration for a split arrive/wait barrier in shared memory. These barriers can be used to implement fine grained thread controls, producer-consumer computation pipeline and divergence code patterns in CUDA. These barriers can also be used alongside the asynchronous copy. For more information on the Arrive/Wait Barriers refer to the Arrive/Wait Barrier section in the CUDA C++ Programming Guide . 1.4.1.4. Warp level support for Reduction Operations \\uf0c1 The NVIDIA Ampere GPU architecture adds native support for warp wide reduction operations for 32-bit signed and unsigned integer operands. The warp wide reduction operations support arithmetic add , min , and max operations on 32-bit signed and unsigned integers and bitwise and , or and xor operations on 32-bit unsigned integers. For more details on the new warp wide reduction operations refer to Warp Reduce Functions in the CUDA C++ Programming Guide . 1.4.1.5. Improved Tensor Core Operations \\uf0c1 The NVIDIA Ampere GPU architecture includes new Third Generation Tensor Cores that are more powerful than the Tensor Cores used in Volta and Turing SMs. The new Tensor Cores use a larger base matrix size and add powerful new math modes including: Support for FP64 Tensor Core, using new DMMA instructions. Support for Bfloat16 Tensor Core, through HMMA instructions. BFloat16 format is especially effective for DL training scenarios. Bfloat16 provides 8-bit exponent i.e., same range as FP32, 7-bit mantissa and 1 sign-bit. Support for TF32 Tensor Core, through HMMA instructions. TF32 is a new 19-bit Tensor Core format that can be easily integrated into programs for more accurate DL training than 16-bit HMMA formats. TF32 provides 8-bit exponent, 10-bit mantissa and 1 sign-bit. Support for bitwise AND along with bitwise XOR which was introduced in Turing, through BMMA instructions. The following table presents the evolution of matrix instruction sizes and supported data types for Tensor Cores across different GPU architecture generations. Instruction GPU Architecture Input Matrix format Output Accumulator format Matrix Instruction Size (MxNxK) HMMA (16-bit precision) NVIDIA Volta Architecture FP16 FP16 / FP32 8x8x4 NVIDIA Turing Architecture FP16 FP16 / FP32 8x8x4 / 16x8x8 / 16x8x16 NVIDIA Ampere Architecture FP16 / BFloat16 FP16 / FP32 (BFloat16 only supports FP32 as accumulator) 16x8x8 / 16x8x16 HMMA (19-bit precision) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture N/A N/A N/A NVIDIA Ampere Architecture TF32 (19-bits) FP32 16x8x4 IMMA (Integer MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture unsigned char/signed char (8-bit precision) int32 8x8x16 NVIDIA Ampere Architecture unsigned char/signed char (8-bit precision) int32 8x8x16 / 16x8x16 / 16x8x32 IMMA (Integer sub-byte MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture unsigned u4/signed u4 (4-bit precision) int32 8x8x32 NVIDIA Ampere Architecture unsigned u4/signed u4 (4-bit precision) int32 8x8x32 / 16x8x32 / 16x8x64 BMMA (Binary MMA) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture single bit int32 8x8x128 NVIDIA Ampere Architecture single bit int32 8x8x128 / 16x8x128 / 16x8x256 DMMA (64-bit precision) NVIDIA Volta Architecture N/A N/A N/A NVIDIA Turing Architecture N/A N/A N/A NVIDIA Ampere Architecture FP64 FP64 8x8x4 For more details on the new Tensor Core operations refer to the Warp Matrix Multiply section in the CUDA C++ Programming Guide . 1.4.1.6.'},\n",
       " {'id': 548,\n",
       "  'content': 'Improved FP32 throughput \\uf0c1 Devices of compute capability 8.6 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. While a binary compiled for 8.0 will run as is on 8.6, it is recommended to compile explicitly for 8.6 to benefit from the increased FP32 throughput. 1.4.2.'},\n",
       " {'id': 549,\n",
       "  'content': 'Memory System \\uf0c1 1.4.2.1. Increased Memory Capacity and High Bandwidth Memory \\uf0c1 The NVIDIA A100 GPU increases the HBM2 memory capacity from 32 GB in V100 GPU to 40 GB in A100 GPU. Along with the increased memory capacity, the bandwidth is increased by 72%, from 900 GB/s on Volta V100 to 1550 GB/s on A100. 1.4.2.2. Increased L2 capacity and L2 Residency Controls \\uf0c1 The NVIDIA Ampere GPU architecture increases the capacity of the L2 cache to 40 MB in Tesla A100, which is 7x larger than Tesla V100. Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased. The NVIDIA Ampere GPU architecture allows CUDA users to control the persistence of data in L2 cache. For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Unified Shared Memory/L1/Texture Cache \\uf0c1 The NVIDIA A100 GPU based on compute capability 8.0 increases the maximum capacity of the combined L1 cache, texture cache and shared memory to 192 KB, 50% larger than the L1 cache in NVIDIA V100 GPU. The combined L1 cache capacity for GPUs with compute capability 8.6 is 128 KB. In the NVIDIA Ampere GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures such as Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA A100 GPU supports shared memory capacity of 0, 8, 16, 32, 64, 100, 132 or 164 KB per SM. GPUs with compute capability 8.6 support shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, the A100 GPU enables a single thread block to address up to 163 KB of shared memory and GPUs with compute capability 8.6 can address up to 99 KB of shared memory in a single thread block. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like Volta, the NVIDIA Ampere GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Another benefit of its union with shared memory, similar to Volta L1 is improvement in terms of both latency and bandwidth. 1.4.3. Third Generation NVLink \\uf0c1 The third generation of NVIDIA’s high-speed NVLink interconnect is implemented in A100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features. The third generation NVLink has the same bi-directional data rate of 50 GB/s per link, but uses half the number of signal pairs to achieve this bandwidth. Therefore, the total number of links available is increased to twelve in A100, versus six in V100, yielding 600 GB/s bidirectional bandwidth versus 300 GB/s for V100. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. In the NVIDIA Ampere GPU architecture remote NVLINK accesses go through a Link TLB on the remote GPU. This Link TLB has a reach of 64 GB to the remote GPU’s memory. Applications with remote random accesses may want to constrain the remotely accessed region to 64 GB for each peer GPU. 2.'},\n",
       " {'id': 550,\n",
       "  'content': 'Revision History \\uf0c1 Version 1.1 Initial Public Release Added support for compute capability 8.6 3. Notices \\uf0c1 3.1.'},\n",
       " {'id': 551,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 552,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. 1 Throughout this guide, Kepler refers to devices of compute capability 3.x, Maxwell refers to devices of compute capability 5.x, Pascal refers to device of compute capability 6.x, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, and NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 553,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. NVIDIA Hopper Tuning Guide 1.1.'},\n",
       " {'id': 554,\n",
       "  'content': 'NVIDIA Hopper GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Hopper Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Tensor Memory Accelerator 1.4.1.3. Thread Block Clusters 1.4.1.4. Improved FP32 Throughput 1.4.1.5. Dynamic Programming Instructions 1.4.2. Memory System 1.4.2.1. High-Bandwidth Memory HBM3 Subsystem 1.4.2.2. Increased L2 Capacity 1.4.2.3. Inline Compression 1.4.2.4. Unified Shared Memory/L1/Texture Cache 1.4.3. Fourth-Generation NVLink 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 555,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Hopper Tuning Guide » 1. NVIDIA Hopper Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for Hopper GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the Hopper GPU Architecture. NVIDIA Hopper Tuning Guide \\uf0c1 1.1. NVIDIA Hopper GPU Architecture \\uf0c1 The NVIDIA® Hopper GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications. The NVIDIA Hopper GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere GPU architecture and NVIDIA Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA H100 GPU without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Hopper GPU architecture’s features. 1 For further details on the programming features discussed in this guide, refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices \\uf0c1 The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure that global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility \\uf0c1 Before addressing specific performance tuning issues covered in this guide, refer to the Hopper Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with NVIDIA Hopper. 1.4. NVIDIA Hopper Tuning \\uf0c1 1.4.1. Streaming Multiprocessor \\uf0c1 The NVIDIA Hopper Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy \\uf0c1 The maximum number of concurrent warps per SM remains the same as in NVIDIA Ampere GPU architecture (that is, 64), and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 32 for devices of compute capability 9.0 (that is, H100 GPUs). For devices of compute capability 9.0 (H100 GPUs), shared memory capacity per SM is 228 KB, a 39% increase compared to A100’s capacity of 164 KB. For devices of compute capability 9.0 (H100 GPUs), the maximum shared memory per thread block is 227 KB. For applications using Thread Block Clusters, it is always recommended to compute the occupancy using cudaOccupancyMaxActiveClusters and launch cluster-based kernels accordingly. Overall, developers can expect similar occupancy as on NVIDIA Ampere GPU architecture GPUs without changes to their application. 1.4.1.2. Tensor Memory Accelerator \\uf0c1 The Hopper architecture builds on top of the asynchronous copies introduced by NVIDIA Ampere GPU architecture and provides a more sophisticated asynchronous copy engine: the Tensor Memory Accelerator (TMA). TMA allows applications to transfer 1D and up to 5D tensors between global memory and shared memory, in both directions, as well as between the shared memory regions of different SMs in the same cluster (refer to Thread Block Clusters ). Additionally, for writes from shared memory to global memory, it allows specifying element wise reduction operations such as add/min/max as well as bitwise and/or for most common data types. This has several advantages: Avoids using registers for moving data between the different memory spaces. Avoids using SM instructions for moving data: a single thread can issue large data movement instructions to the TMA unit. The whole block can then continue working on other instructions while the data is in flight and only wait for the data to be consumed when actually necessary. Enables users to write warp specialized codes, where specific warps specialize on data movement between the different memory spaces while other warps only work on local data within the SM. This feature will be exposed through cuda::memcpy_async along with the cuda::barrier and cuda::pipeline for synchronizing data movement. 1.4.1.3. Thread Block Clusters \\uf0c1 NVIDIA Hopper Architecture adds a new optional level of hierarchy, Thread Block Clusters, that allows for further possibilities when parallelizing applications. A thread block can read from, write to, and perform atomics in shared memory of other thread blocks within its cluster. This is known as Distributed Shared Memory. As demonstrated in the CUDA C++ Programming Guide , there are applications that cannot fit required data within shared memory and must use global memory instead. Distributed shared memory can act as an intermediate step between these two options. Distributed Shared Memory can be used by an SM simultaneously with L2 cache accesses. This can benefit applications that need to communicate data between SMs by utilizing the combined bandwidth of both distributed shared memory and L2. In order to achieve best performance for accesses to Distributed Shared Memory, access patterns to those described in the CUDA C++ Best Practices Guide for Global Memory should be used. Specifically, accesses to Distributed Shared Memory should be coalesced and aligned to 32-byte segments, if possible. Access patterns with non-unit stride should be avoided if possible, which can be achieved by using local shared memory, similar to what is shown in the CUDA C++ Best Practices Guide for Shared Memory . The maximum portable cluster size supported is 8; however, NVIDIA Hopper H100 GPU allows for a nonportable cluster size of 16 by opting in. Launching a kernel with a nonportable cluster size requires setting the cudaFuncAttributeNonPortableClusterSizeAllowed function attribute. Using larger cluster sizes may reduce the maximum number of active blocks across the GPU (refer to Occupancy ). 1.4.1.4. Improved FP32 Throughput \\uf0c1 Devices of compute capability 9.0 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. 1.4.1.5. Dynamic Programming Instructions \\uf0c1 The NVIDIA Hopper architecture adds support for new instructions to accelerate dynamic programming algorithms, such as the Smith-Waterman algorithm for sequence alignment in bioinformatics, and algorithms in graph theory, game theory, ML, and finance problems. The new instructions permit computation of max and min values among three operands, max and min operations yielding predicates, combined add operation with max or min, operating on signed and unsigned 32-bit int and 16-bit short2 types, and half2. All DPX instructions with 16-bit short types DPX instructions enable 128 operations per cycle per SM. 1.4.2.'},\n",
       " {'id': 556,\n",
       "  'content': 'Memory System \\uf0c1 1.4.2.1. High-Bandwidth Memory HBM3 Subsystem \\uf0c1 The NVIDIA H100 GPU has support for HBM3 and HBM2e memory, with capacity up to 80 GB. GPUs HBM3 memory system supports up to 3 TB/s memory bandwidth, a 93% increase over the 1.55 TB/s on A100-40GB. 1.4.2.2. Increased L2 Capacity \\uf0c1 The NVIDIA Hopper architecture increases the L2 cache capacity from 40 MB in the A100 GPU to 50 MB in the H100 GPU. Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased. The NVIDIA Hopper architecture allows CUDA users to control the persistence of data in L2 cache similar to the NVIDIA Ampere GPU Architecture. For more information on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide . 1.4.2.3. Inline Compression \\uf0c1 The NVIDIA Hopper architecture allows CUDA compute kernels to benefit from the new inline compression (ILC). This feature can be applied to individual memory allocation, and the compressor automatically chooses between several possible compression algorithms, or none if there is no suitable pattern. In case compression can be used, this feature allows accessing global memory at significantly higher bandwidth than global memory bandwidth, since only compressed data needs to be transferred between global memory and SMs. However, the feature does not allow for reducing memory footprint: since compression is automatic, even if compression is active, the memory region will use the same footprint as if there was no compression. This is because underlying data may be changed by the user application and may not be compressible during the entire duration of the application. The feature is available through the CUDA driver API. See the CUDA C++ Programming Guide section on compressible memory : CUmemGenericAllocationHandle allocationHandle ; CUmemAllocationProp prop = {}; memset ( prop , 0 , sizeof ( CUmemAllocationProp )); prop -> type = CU_MEM_ALLOCATION_TYPE_PINNED ; prop -> location . type = CU_MEM_LOCATION_TYPE_DEVICE ; prop -> location . id = currentDevice ; prop -> allocFlags . compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; cuMemCreate ( & allocationHandle , size , & prop , 0 ); One can check whether compressible memory is available on the given device with: cuDeviceGetAttribute ( & compressionAvailable , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , currentDevice ) Note that this example code does not handle errors and compiling this code requires linking against the CUDA library ( libcuda.so ). 1.4.2.4. Unified Shared Memory/L1/Texture Cache \\uf0c1 The NVIDIA H100 GPU based on compute capability 9.0 increases the maximum capacity of the combined L1 cache, texture cache, and shared memory to 256 KB, from 192 KB in NVIDIA Ampere Architecture, an increase of 33%. In the NVIDIA Hopper GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout) can be selected at runtime as in previous architectures such as NVIDIA Ampere Architecture and NVIDIA Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA H100 GPU supports shared memory capacities of 0, 8, 16, 32, 64, 100, 132, 164, 196 and 228 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, the H100 GPU enables a single thread block to address up to 227 KB of shared memory. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like the NVIDIA Ampere Architecture and NVIDIA Volta GPU architectures, the NVIDIA Hopper GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp before delivery of that data to the warp. Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 1.4.3. Fourth-Generation NVLink \\uf0c1 The fourth generation of NVIDIA’s high-speed NVLink interconnect is implemented in H100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features. The fourth-generation NVLink has the same bidirectional data rate of 50 GB/s per link. The total number of links available is increased to 18 in H100, compared to 12 in A100, yielding 900 GB/s bidirectional bandwidth compared to 600 GB/s for A100. NVLink operates transparently within the existing CUDA model. Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe. The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs. The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs. 2. Revision History \\uf0c1 Version 1.0 Initial Public Release Added support for compute capability 9.0 1 Throughout this guide, NVIDIA Volta refers to devices of compute capability 7.0, NVIDIA Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.x, and NVIDIA Hopper refers to devices of compute capability 9.0. 3.'},\n",
       " {'id': 557,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 558,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 559,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. NVIDIA Ada GPU Architecture Tuning Guide 1.1.'},\n",
       " {'id': 560,\n",
       "  'content': 'NVIDIA Ada GPU Architecture 1.2. CUDA Best Practices 1.3. Application Compatibility 1.4. NVIDIA Ada GPU Architecture Tuning 1.4.1. Streaming Multiprocessor 1.4.1.1. Occupancy 1.4.1.2. Improved Tensor Core Operations 1.4.1.3. Improved FP32 throughput 1.4.2. Memory System 1.4.2.1. Increased L2 capacity 1.4.2.2. Unified Shared Memory/L1/Texture Cache 2. Revision History 3. Notices 3.1.'},\n",
       " {'id': 561,\n",
       "  'content': 'Notice 3.2. OpenCL 3.3. Trademarks Ada Tuning Guide » 1. NVIDIA Ada GPU Architecture Tuning Guide v12.5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ada GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ada GPU Architecture. NVIDIA Ada GPU Architecture Tuning Guide \\uf0c1 1.1. NVIDIA Ada GPU Architecture \\uf0c1 The NVIDIA ® Ada GPU architecture is NVIDIA’s latest architecture for CUDA ® compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features. 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide . 1.2. CUDA Best Practices \\uf0c1 The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures. Programmers must primarily focus on following those recommendations to achieve the best performance. The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code. Minimize data transfers between the host and the device. Adjust kernel launch configuration to maximize device utilization. Ensure global memory accesses are coalesced. Minimize redundant accesses to global memory whenever possible. Avoid long sequences of diverged execution by threads within the same warp. 1.3. Application Compatibility \\uf0c1 Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ada GPU Architecture. 1.4. NVIDIA Ada GPU Architecture Tuning \\uf0c1 1.4.1. Streaming Multiprocessor \\uf0c1 The NVIDIA Ada GPU architecture’s Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures. 1.4.1.1. Occupancy \\uf0c1 The maximum number of concurrent warps per SM is 48, remaining the same compared to compute capability 8.6 GPUs, and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM. The maximum number of registers per thread is 255. The maximum number of thread blocks per SM is 24. The shared memory capacity per SM is 100 KB. The maximum shared memory per thread block is 99 KB. Overall, developers can expect similar occupancy as on compute capability 8.6 GPUs without changes to their application.'},\n",
       " {'id': 562,\n",
       "  'content': '1.4.1.2. Improved Tensor Core Operations \\uf0c1 The NVIDIA Ada GPU architecture includes new Ada Fourth Generation Tensor Cores featuring the Hopper FP8 Transformer Engine. 1.4.1.3. Improved FP32 throughput \\uf0c1 Devices of compute capability 8.9 have 2x more FP32 operations per cycle per SM than devices of compute capability 8.0. While a binary compiled for 8.0 will run as-is on 8.9, it is recommended to compile explicitly for 8.9 to benefit from the increased FP32 throughput. 1.4.2.'},\n",
       " {'id': 563,\n",
       "  'content': 'Memory System \\uf0c1 1.4.2.1. Increased L2 capacity \\uf0c1 The NVIDIA Ada GPU architecture increases the capacity of the L2 cache to 98304 KB in AD102, 16x larger than GA102. The NVIDIA Ada GPU architecture allows CUDA users to control the persistence of data in the L2 cache. For more information on the persistence of data in the L2 cache, refer to the section on managing the L2 cache in the CUDA C++ Programming Guide . 1.4.2.2. Unified Shared Memory/L1/Texture Cache \\uf0c1 NVIDIA Ada architecture features a unified L1 cache, texture cache, and shared memory similar to that of the NVIDIA Ampere architecture. The combined L1 cache capacity is 128 KB. In the NVIDIA Ada GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout . The NVIDIA Ada GPU architecture supports shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM. CUDA reserves 1 KB of shared memory per thread block. Hence, GPUs with compute capability 8.9 can address up to 99 KB of shared memory in a single thread block. To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit. See the CUDA C++ Programming Guide for details. Like the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp. Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth. 2. Revision History \\uf0c1 Version 1.0 Initial Public Release Added support for compute capability 8.9 1 Throughout this guide, Volta refers to devices of compute capability 7.0, Turing refers to devices of compute capability 7.5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8.0 and 8.6, NVIDIA Ada refers to devices of compute capability 8.9. 3.'},\n",
       " {'id': 564,\n",
       "  'content': 'Notices \\uf0c1 3.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 3.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 565,\n",
       "  'content': '3.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 566,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Introduction 1.1.'},\n",
       " {'id': 567,\n",
       "  'content': 'Scalable Data-Parallel Computing using GPUs 1.2. Goals of PTX 1.3.'},\n",
       " {'id': 568, 'content': 'PTX ISA Version 8.5 1.4. Document Structure 2.'},\n",
       " {'id': 569,\n",
       "  'content': 'Programming Model 2.1. A Highly Multithreaded Coprocessor 2.2. Thread Hierarchy 2.2.1. Cooperative Thread Arrays 2.2.2. Cluster of Cooperative Thread Arrays 2.2.3. Grid of Clusters 2.3. Memory Hierarchy 3. PTX Machine Model 3.1. A Set of SIMT Multiprocessors 3.2. Independent Thread Scheduling 3.3. On-chip Shared Memory 4. Syntax 4.1.'},\n",
       " {'id': 570,\n",
       "  'content': 'Source Format 4.2. Comments 4.3. Statements 4.3.1. Directive Statements 4.3.2. Instruction Statements 4.4. Identifiers 4.5. Constants 4.5.1. Integer Constants 4.5.2. Floating-Point Constants 4.5.3. Predicate Constants 4.5.4. Constant Expressions 4.5.5. Integer Constant Expression Evaluation 4.5.6. Summary of Constant Expression Evaluation Rules 5. State Spaces, Types, and Variables 5.1. State Spaces 5.1.1. Register State Space 5.1.2. Special Register State Space 5.1.3. Constant State Space 5.1.3.1. Banked Constant State Space (deprecated) 5.1.4. Global State Space 5.1.5. Local State Space 5.1.6. Parameter State Space 5.1.6.1. Kernel Function Parameters 5.1.6.2. Kernel Function Parameter Attributes 5.1.6.3. Kernel Parameter Attribute: .ptr 5.1.6.4. Device Function Parameters 5.1.7. Shared State Space 5.1.8. Texture State Space (deprecated) 5.2. Types 5.2.1.'},\n",
       " {'id': 571,\n",
       "  'content': 'Fundamental Types 5.2.2. Restricted Use of Sub-Word Sizes 5.2.3. Alternate Floating-Point Data Formats 5.2.4. Packed Data Types 5.2.4.1. Packed Floating Point Data Types 5.2.4.2. Packed Integer Data Types 5.3. Texture Sampler and Surface Types 5.3.1. Texture and Surface Properties 5.3.2. Sampler Properties 5.3.3. Channel Data Type and Channel Order Fields 5.4. Variables 5.4.1.'},\n",
       " {'id': 572,\n",
       "  'content': 'Variable Declarations 5.4.2. Vectors 5.4.3. Array Declarations 5.4.4. Initializers 5.4.5. Alignment 5.4.6. Parameterized Variable Names 5.4.7. Variable Attributes 5.4.8. Variable and Function Attribute Directive: .attribute 5.5. Tensors 5.5.1. Tensor Dimension, size and format 5.5.2. Tensor Access Modes 5.5.3. Tiled Mode 5.5.3.1. Bounding Box 5.5.3.2. Traversal-Stride 5.5.3.3. Out of Boundary Access 5.5.4. Im2col mode 5.5.4.1. Bounding Box 5.5.4.2. Traversal Stride 5.5.4.3. Out of Boundary Access 5.5.5. Interleave layout 5.5.6. Swizzling Modes 5.5.7. Tensor-map 6. Instruction Operands 6.1. Operand Type Information 6.2. Source Operands 6.3. Destination Operands 6.4. Using Addresses, Arrays, and Vectors 6.4.1. Addresses as Operands 6.4.1.1. Generic Addressing 6.4.2. Arrays as Operands 6.4.3. Vectors as Operands 6.4.4. Labels and Function Names as Operands 6.5. Type Conversion 6.5.1. Scalar Conversions 6.5.2. Rounding Modifiers 6.6. Operand Costs 7. Abstracting the ABI 7.1. Function Declarations and Definitions 7.1.1. Changes from PTX ISA Version 1.x 7.2. Variadic Functions 7.3. Alloca 8. Memory Consistency Model 8.1. Scope and applicability of the model 8.1.1. Limitations on atomicity at system scope 8.2. Memory operations 8.2.1. Overlap 8.2.2. Aliases 8.2.3. Multimem Addresses 8.2.4. Memory Operations on Vector Data Types 8.2.5. Memory Operations on Packed Data Types 8.2.6. Initialization 8.3. State spaces 8.4. Operation types 8.4.1. mmio Operation 8.5. Scope 8.6. Proxies 8.7. Morally strong operations 8.7.1. Conflict and Data-races 8.7.2. Limitations on Mixed-size Data-races 8.8. Release and Acquire Patterns 8.9. Ordering of memory operations 8.9.1. Program Order 8.9.1.1. Asynchronous Operations 8.9.2. Observation Order 8.9.3. Fence-SC Order 8.9.4. Memory synchronization 8.9.5. Causality Order 8.9.6. Coherence Order 8.9.7. Communication Order 8.10. Axioms 8.10.1. Coherence 8.10.2. Fence-SC 8.10.3. Atomicity 8.10.4. No Thin Air 8.10.5. Sequential Consistency Per Location 8.10.6. Causality 9. Instruction Set 9.1. Format and Semantics of Instruction Descriptions 9.2. PTX Instructions 9.3. Predicated Execution 9.3.1. Comparisons 9.3.1.1. Integer and Bit-Size Comparisons 9.3.1.2. Floating Point Comparisons 9.3.2. Manipulating Predicates 9.4. Type Information for Instructions and Operands 9.4.1. Operand Size Exceeding Instruction-Type Size 9.5. Divergence of Threads in Control Constructs 9.6. Semantics 9.6.1. Machine-Specific Semantics of 16-bit Code 9.7. Instructions 9.7.1. Integer Arithmetic Instructions 9.7.1.1. Integer Arithmetic Instructions: add 9.7.1.2. Integer Arithmetic Instructions: sub 9.7.1.3. Integer Arithmetic Instructions: mul 9.7.1.4. Integer Arithmetic Instructions: mad 9.7.1.5. Integer Arithmetic Instructions: mul24 9.7.1.6. Integer Arithmetic Instructions: mad24 9.7.1.7. Integer Arithmetic Instructions: sad 9.7.1.8. Integer Arithmetic Instructions: div 9.7.1.9. Integer Arithmetic Instructions: rem 9.7.1.10. Integer Arithmetic Instructions: abs 9.7.1.11. Integer Arithmetic Instructions: neg 9.7.1.12. Integer Arithmetic Instructions: min 9.7.1.13. Integer Arithmetic Instructions: max 9.7.1.14. Integer Arithmetic Instructions: popc 9.7.1.15. Integer Arithmetic Instructions: clz 9.7.1.16. Integer Arithmetic Instructions: bfind 9.7.1.17. Integer Arithmetic Instructions: fns 9.7.1.18. Integer Arithmetic Instructions: brev 9.7.1.19. Integer Arithmetic Instructions: bfe 9.7.1.20. Integer Arithmetic Instructions: bfi 9.7.1.21. Integer Arithmetic Instructions: szext 9.7.1.22. Integer Arithmetic Instructions: bmsk 9.7.1.23. Integer Arithmetic Instructions: dp4a 9.7.1.24. Integer Arithmetic Instructions: dp2a 9.7.2. Extended-Precision Integer Arithmetic Instructions 9.7.2.1. Extended-Precision Arithmetic Instructions: add.cc 9.7.2.2. Extended-Precision Arithmetic Instructions: addc 9.7.2.3. Extended-Precision Arithmetic Instructions: sub.cc 9.7.2.4. Extended-Precision Arithmetic Instructions: subc 9.7.2.5. Extended-Precision Arithmetic Instructions: mad.cc 9.7.2.6. Extended-Precision Arithmetic Instructions: madc 9.7.3. Floating-Point Instructions 9.7.3.1. Floating Point Instructions: testp 9.7.3.2. Floating Point Instructions: copysign 9.7.3.3. Floating Point Instructions: add 9.7.3.4. Floating Point Instructions: sub 9.7.3.5. Floating Point Instructions: mul 9.7.3.6. Floating Point Instructions: fma 9.7.3.7. Floating Point Instructions: mad 9.7.3.8. Floating Point Instructions: div 9.7.3.9. Floating Point Instructions: abs 9.7.3.10. Floating Point Instructions: neg 9.7.3.11. Floating Point Instructions: min 9.7.3.12. Floating Point Instructions: max 9.7.3.13. Floating Point Instructions: rcp 9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64 9.7.3.15. Floating Point Instructions: sqrt 9.7.3.16. Floating Point Instructions: rsqrt 9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64 9.7.3.18. Floating Point Instructions: sin 9.7.3.19. Floating Point Instructions: cos 9.7.3.20. Floating Point Instructions: lg2 9.7.3.21. Floating Point Instructions: ex2 9.7.3.22. Floating Point Instructions: tanh 9.7.4. Half Precision Floating-Point Instructions 9.7.4.1. Half Precision Floating Point Instructions: add 9.7.4.2. Half Precision Floating Point Instructions: sub 9.7.4.3. Half Precision Floating Point Instructions: mul 9.7.4.4. Half Precision Floating Point Instructions: fma 9.7.4.5. Half Precision Floating Point Instructions: neg 9.7.4.6. Half Precision Floating Point Instructions: abs 9.7.4.7. Half Precision Floating Point Instructions: min 9.7.4.8. Half Precision Floating Point Instructions: max 9.7.4.9. Half Precision Floating Point Instructions: tanh 9.7.4.10. Half Precision Floating Point Instructions: ex2 9.7.5. Comparison and Selection Instructions 9.7.5.1. Comparison and Selection Instructions: set 9.7.5.2. Comparison and Selection Instructions: setp 9.7.5.3. Comparison and Selection Instructions: selp 9.7.5.4. Comparison and Selection Instructions: slct 9.7.6. Half Precision Comparison Instructions 9.7.6.1. Half Precision Comparison Instructions: set 9.7.6.2. Half Precision Comparison Instructions: setp 9.7.7. Logic and Shift Instructions 9.7.7.1. Logic and Shift Instructions: and 9.7.7.2. Logic and Shift Instructions: or 9.7.7.3. Logic and Shift Instructions: xor 9.7.7.4. Logic and Shift Instructions: not 9.7.7.5. Logic and Shift Instructions: cnot 9.7.7.6. Logic and Shift Instructions: lop3 9.7.7.7. Logic and Shift Instructions: shf 9.7.7.8. Logic and Shift Instructions: shl 9.7.7.9. Logic and Shift Instructions: shr 9.7.8. Data Movement and Conversion Instructions 9.7.8.1. Cache Operators 9.7.8.2. Cache Eviction Priority Hints 9.7.8.3. Data Movement and Conversion Instructions: mov 9.7.8.4. Data Movement and Conversion Instructions: mov 9.7.8.5. Data Movement and Conversion Instructions: shfl (deprecated) 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync 9.7.8.7. Data Movement and Conversion Instructions: prmt 9.7.8.8. Data Movement and Conversion Instructions: ld 9.7.8.9. Data Movement and Conversion Instructions: ld.global.nc 9.7.8.10. Data Movement and Conversion Instructions: ldu 9.7.8.11. Data Movement and Conversion Instructions: st 9.7.8.12. Data Movement and Conversion Instructions: st.async 9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red 9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchu 9.7.8.15. Data Movement and Conversion Instructions: applypriority 9.7.8.16. Data Movement and Conversion Instructions: discard 9.7.8.17. Data Movement and Conversion Instructions: createpolicy 9.7.8.18. Data Movement and Conversion Instructions: isspacep 9.7.8.19. Data Movement and Conversion Instructions: cvta 9.7.8.20. Data Movement and Conversion Instructions: cvt 9.7.8.21. Data Movement and Conversion Instructions: cvt.pack 9.7.8.22. Data Movement and Conversion Instructions: mapa 9.7.8.23. Data Movement and Conversion Instructions: getctarank 9.7.8.24. Data Movement and Conversion Instructions: Asynchronous copy 9.7.8.24.1. Completion Mechanisms for Asynchronous Copy Operations 9.7.8.24.2. Async Proxy 9.7.8.24.3. Data Movement and Conversion Instructions: cp.async 9.7.8.24.4. Data Movement and Conversion Instructions: cp.async.commit_group 9.7.8.24.5. Data Movement and Conversion Instructions: cp.async.wait_group / cp.async.wait_all 9.7.8.24.6. Data Movement and Conversion Instructions: cp.async.bulk 9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulk 9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetch 9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensor 9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor 9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor 9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_group 9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_group 9.7.8.25. Data Movement and Conversion Instructions: tensormap.replace 9.7.9. Texture Instructions 9.7.9.1. Texturing Modes 9.7.9.2. Mipmaps 9.7.9.3. Texture Instructions: tex 9.7.9.4. Texture Instructions: tld4 9.7.9.5. Texture Instructions: txq 9.7.9.6. Texture Instructions: istypep 9.7.10. Surface Instructions 9.7.10.1. Surface Instructions: suld 9.7.10.2. Surface Instructions: sust 9.7.10.3. Surface Instructions: sured 9.7.10.4. Surface Instructions: suq 9.7.11. Control Flow Instructions 9.7.11.1. Control Flow Instructions: {} 9.7.11.2. Control Flow Instructions: @ 9.7.11.3. Control Flow Instructions: bra 9.7.11.4. Control Flow Instructions: brx.idx 9.7.11.5. Control Flow Instructions: call 9.7.11.6. Control Flow Instructions: ret 9.7.11.7. Control Flow Instructions: exit 9.7.12. Parallel Synchronization and Communication Instructions 9.7.12.1. Parallel Synchronization and Communication Instructions: bar, barrier 9.7.12.2. Parallel Synchronization and Communication Instructions: bar.warp.sync 9.7.12.3. Parallel Synchronization and Communication Instructions: barrier.cluster 9.7.12.4. Parallel Synchronization and Communication Instructions: membar/fence 9.7.12.5. Parallel Synchronization and Communication Instructions: atom 9.7.12.6. Parallel Synchronization and Communication Instructions: red 9.7.12.7. Parallel Synchronization and Communication Instructions: red.async 9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated) 9.7.12.9. Parallel Synchronization and Communication Instructions: vote.sync 9.7.12.10. Parallel Synchronization and Communication Instructions: match.sync 9.7.12.11. Parallel Synchronization and Communication Instructions: activemask 9.7.12.12. Parallel Synchronization and Communication Instructions: redux.sync 9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrol 9.7.12.14. Parallel Synchronization and Communication Instructions: elect.sync 9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrier 9.7.12.15.1. Size and alignment of mbarrier object 9.7.12.15.2. Contents of the mbarrier object 9.7.12.15.3. Lifecycle of the mbarrier object 9.7.12.15.4. Phase of the mbarrier object 9.7.12.15.5. Tracking asynchronous operations by the mbarrier object 9.7.12.15.5.1. expect-tx operation 9.7.12.15.5.2. complete-tx operation 9.7.12.15.6. Phase Completion of the mbarrier object 9.7.12.15.7. Arrive-on operation on mbarrier object 9.7.12.15.8. mbarrier support with shared memory 9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.init 9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.inval 9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_tx 9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_tx 9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arrive 9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop 9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive 9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait 9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_count 9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy 9.7.13. Warp Level Matrix Multiply-Accumulate Instructions 9.7.13.1. Matrix Shape 9.7.13.2. Matrix Data-types 9.7.13.3. Matrix multiply-accumulate operation using wmma instructions 9.7.13.3.1. Matrix Fragments for WMMA 9.7.13.3.2. Matrix Storage for WMMA 9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.load 9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.store 9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mma 9.7.13.4. Matrix multiply-accumulate operation using mma instruction 9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type 9.7.13.4.2. Matrix Fragments for mma.m8n8k4 with .f64 floating point type 9.7.13.4.3. Matrix Fragments for mma.m8n8k16 9.7.13.4.4. Matrix Fragments for mma.m8n8k32 9.7.13.4.5. Matrix Fragments for mma.m8n8k128 9.7.13.4.6. Matrix Fragments for mma.m16n8k4 9.7.13.4.7. Matrix Fragments for mma.m16n8k8 9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point type 9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer type 9.7.13.4.10. Matrix Fragments for mma.m16n8k32 9.7.13.4.11. Matrix Fragments for mma.m16n8k64 9.7.13.4.12. Matrix Fragments for mma.m16n8k128 9.7.13.4.13. Matrix Fragments for mma.m16n8k256 9.7.13.4.14. Multiply-and-Accumulate Instruction: mma 9.7.13.4.15. Warp-level matrix load instruction: ldmatrix 9.7.13.4.16. Warp-level matrix store instruction: stmatrix 9.7.13.4.17. Warp-level matrix transpose instruction: movmatrix 9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A 9.7.13.5.1. Sparse matrix storage 9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A 9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types 9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types 9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type 9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type 9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type 9.7.13.5.2.6. Matrix Fragments for sparse mma.m16n8k64 with .u8/.s8/.e4m3/.e5m2 type 9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type 9.7.13.5.2.8. Matrix Fragments for sparse mma.m16n8k128 with .u4/.s4 integer type 9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadata 9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions 9.7.14.1. Warpgroup 9.7.14.2. Matrix Shape 9.7.14.3. Matrix Data-types 9.7.14.4. Async Proxy 9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction 9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts 9.7.14.5.1.1. Register Fragments 9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16 9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8 9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32 9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256 9.7.14.5.1.2. Shared Memory Matrix Layout 9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16 9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8 9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32 9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256 9.7.14.5.1.2.5. Strides 9.7.14.5.1.2.6.'},\n",
       " {'id': 573,\n",
       "  'content': 'Swizzling Modes 9.7.14.5.1.2.7. Matrix Descriptor Format 9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async 9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction 9.7.14.6.1. Sparse matrix storage 9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A 9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32 9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16 9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64 9.7.14.6.3. Shared Memory Matrix Layout 9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32 9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16 9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64 9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.sp 9.7.14.7. Asynchronous wgmma Proxy Operations 9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence 9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group 9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group 9.7.15. Stack Manipulation Instructions 9.7.15.1. Stack Manipulation Instructions: stacksave 9.7.15.2. Stack Manipulation Instructions: stackrestore 9.7.15.3. Stack Manipulation Instructions: alloca 9.7.16. Video Instructions 9.7.16.1.'},\n",
       " {'id': 574,\n",
       "  'content': 'Scalar Video Instructions 9.7.16.1.1. Scalar Video Instructions: vadd, vsub, vabsdiff, vmin, vmax 9.7.16.1.2. Scalar Video Instructions: vshl, vshr 9.7.16.1.3. Scalar Video Instructions: vmad 9.7.16.1.4. Scalar Video Instructions: vset 9.7.16.2. SIMD Video Instructions 9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 9.7.16.2.2. SIMD Video Instructions: vset2 9.7.16.2.3. SIMD Video Instructions: vadd4, vsub4, vavrg4, vabsdiff4, vmin4, vmax4 9.7.16.2.4. SIMD Video Instructions: vset4 9.7.17. Miscellaneous Instructions 9.7.17.1.'},\n",
       " {'id': 575,\n",
       "  'content': 'Miscellaneous Instructions: brkpt 9.7.17.2. Miscellaneous Instructions: nanosleep 9.7.17.3. Miscellaneous Instructions: pmevent 9.7.17.4. Miscellaneous Instructions: trap 9.7.17.5. Miscellaneous Instructions: setmaxnreg 10. Special Registers 10.1. Special Registers: %tid 10.2. Special Registers: %ntid 10.3. Special Registers: %laneid 10.4. Special Registers: %warpid 10.5. Special Registers: %nwarpid 10.6. Special Registers: %ctaid 10.7. Special Registers: %nctaid 10.8. Special Registers: %smid 10.9. Special Registers: %nsmid 10.10. Special Registers: %gridid 10.11. Special Registers: %is_explicit_cluster 10.12. Special Registers: %clusterid 10.13. Special Registers: %nclusterid 10.14. Special Registers: %cluster_ctaid 10.15. Special Registers: %cluster_nctaid 10.16. Special Registers: %cluster_ctarank 10.17. Special Registers: %cluster_nctarank 10.18. Special Registers: %lanemask_eq 10.19. Special Registers: %lanemask_le 10.20. Special Registers: %lanemask_lt 10.21. Special Registers: %lanemask_ge 10.22. Special Registers: %lanemask_gt 10.23. Special Registers: %clock, %clock_hi 10.24. Special Registers: %clock64 10.25. Special Registers: %pm0..%pm7 10.26. Special Registers: %pm0_64..%pm7_64 10.27. Special Registers: %envreg 10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi 10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_ 10.30. Special Registers: %total_smem_size 10.31. Special Registers: %aggr_smem_size 10.32. Special Registers: %dynamic_smem_size 10.33. Special Registers: %current_graph_exec 11. Directives 11.1. PTX Module Directives 11.1.1. PTX Module Directives: .version 11.1.2. PTX Module Directives: .target 11.1.3. PTX Module Directives: .address_size 11.2. Specifying Kernel Entry Points and Functions 11.2.1. Kernel and Function Directives: .entry 11.2.2. Kernel and Function Directives: .func 11.2.3. Kernel and Function Directives: .alias 11.3. Control Flow Directives 11.3.1. Control Flow Directives: .branchtargets 11.3.2. Control Flow Directives: .calltargets 11.3.3. Control Flow Directives: .callprototype 11.4. Performance-Tuning Directives 11.4.1. Performance-Tuning Directives: .maxnreg 11.4.2. Performance-Tuning Directives: .maxntid 11.4.3. Performance-Tuning Directives: .reqntid 11.4.4. Performance-Tuning Directives: .minnctapersm 11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated) 11.4.6. Performance-Tuning Directives: .noreturn 11.4.7. Performance-Tuning Directives: .pragma 11.5. Debugging Directives 11.5.1. Debugging Directives: @@dwarf 11.5.2. Debugging Directives: .section 11.5.3. Debugging Directives: .file 11.5.4. Debugging Directives: .loc 11.6. Linking Directives 11.6.1. Linking Directives: .extern 11.6.2. Linking Directives: .visible 11.6.3. Linking Directives: .weak 11.6.4. Linking Directives: .common 11.7. Cluster Dimension Directives 11.7.1. Cluster Dimension Directives: .reqnctapercluster 11.7.2. Cluster Dimension Directives: .explicitcluster 11.7.3. Cluster Dimension Directives: .maxclusterrank 12. Release Notes 12.1.'},\n",
       " {'id': 576,\n",
       "  'content': 'Changes in PTX ISA Version 8.5 12.2. Changes in PTX ISA Version 8.4 12.3. Changes in PTX ISA Version 8.3 12.4. Changes in PTX ISA Version 8.2 12.5. Changes in PTX ISA Version 8.1 12.6. Changes in PTX ISA Version 8.0 12.7. Changes in PTX ISA Version 7.8 12.8. Changes in PTX ISA Version 7.7 12.9. Changes in PTX ISA Version 7.6 12.10. Changes in PTX ISA Version 7.5 12.11. Changes in PTX ISA Version 7.4 12.12. Changes in PTX ISA Version 7.3 12.13. Changes in PTX ISA Version 7.2 12.14. Changes in PTX ISA Version 7.1 12.15. Changes in PTX ISA Version 7.0 12.16. Changes in PTX ISA Version 6.5 12.17. Changes in PTX ISA Version 6.4 12.18. Changes in PTX ISA Version 6.3 12.19. Changes in PTX ISA Version 6.2 12.20. Changes in PTX ISA Version 6.1 12.21. Changes in PTX ISA Version 6.0 12.22. Changes in PTX ISA Version 5.0 12.23. Changes in PTX ISA Version 4.3 12.24. Changes in PTX ISA Version 4.2 12.25. Changes in PTX ISA Version 4.1 12.26. Changes in PTX ISA Version 4.0 12.27. Changes in PTX ISA Version 3.2 12.28. Changes in PTX ISA Version 3.1 12.29. Changes in PTX ISA Version 3.0 12.30. Changes in PTX ISA Version 2.3 12.31. Changes in PTX ISA Version 2.2 12.32. Changes in PTX ISA Version 2.1 12.33. Changes in PTX ISA Version 2.0 14. Descriptions of .pragma Strings 14.1. Pragma Strings: “nounroll” 14.2. Pragma Strings: “used_bytes_mask” 15. Notices 15.1.'},\n",
       " {'id': 577,\n",
       "  'content': 'Notice 15.2. OpenCL 15.3. Trademarks PTX ISA » 1. Introduction v8.5 | PDF | Archive Parallel Thread Execution ISA Version 8.5 The programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture). Introduction \\uf0c1 This document describes PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device . 1.1. Scalable Data-Parallel Computing using GPUs \\uf0c1 Driven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable GPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous computational horsepower and very high memory bandwidth. The GPU is especially well-suited to address problems that can be expressed as data-parallel computations - the same program is executed on many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic operations to memory operations. Because the same program is executed for each data element, there is a lower requirement for sophisticated flow control; and because it is executed on many data elements and has high arithmetic intensity, the memory access latency can be hidden with calculations instead of big data caches. Data-parallel processing maps data elements to parallel processing threads. Many applications that process large data sets can use a data-parallel programming model to speed up the computations. In 3D rendering large sets of pixels and vertices are mapped to parallel threads. Similarly, image and media processing applications such as post-processing of rendered images, video encoding and decoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to parallel processing threads. In fact, many algorithms outside the field of image rendering and processing are accelerated by data-parallel processing, from general signal processing or physics simulation to computational finance or computational biology. PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs are translated at install time to the target hardware instruction set. The PTX-to-GPU translator and driver enable NVIDIA GPUs to be used as programmable parallel computers. 1.2. Goals of PTX \\uf0c1 PTX provides a stable programming model and instruction set for general purpose parallel programming. It is designed to be efficient on NVIDIA GPUs supporting the computation features defined by the NVIDIA Tesla architecture. High level language compilers for languages such as CUDA and C/C++ generate PTX instructions, which are optimized for and translated to native target-architecture instructions. The goals for PTX include the following: Provide a stable ISA that spans multiple GPU generations. Achieve performance in compiled applications comparable to native GPU performance. Provide a machine-independent ISA for C/C++ and other compilers to target. Provide a code distribution ISA for application and middleware developers. Provide a common source-level ISA for optimizing code generators and translators, which map PTX to specific target machines. Facilitate hand-coding of libraries, performance kernels, and architecture tests. Provide a scalable programming model that spans GPU sizes from a single unit to many parallel units. 1.3. PTX ISA Version 8.5 \\uf0c1 PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction. 1.4. Document Structure \\uf0c1 The information in this document is organized into the following Chapters: Programming Model outlines the programming model. PTX Machine Model gives an overview of the PTX virtual machine model. Syntax describes the basic syntax of the PTX language. State Spaces, Types, and Variables describes state spaces, types, and variable declarations. Instruction Operands describes instruction operands. Abstracting the ABI describes the function and call syntax, calling convention, and PTX support for abstracting the Application Binary Interface (ABI) . Instruction Set describes the instruction set. Special Registers lists special registers. Directives lists the assembly directives supported in PTX. Release Notes provides release notes for PTX ISA versions 2.x and beyond. References 754-2008 IEEE Standard for Floating-Point Arithmetic. ISBN 978-0-7381-5752-8, 2008. http://ieeexplore.ieee.org/servlet/opac?punumber=4610933 The OpenCL Specification, Version: 1.1, Document Revision: 44, June 1, 2011. http://www.khronos.org/registry/cl/specs/opencl-1.1.pdf CUDA Programming Guide. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html CUDA Dynamic Parallelism Programming Guide. https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism CUDA Atomicity Requirements. https://nvidia.github.io/cccl/libcudacxx/extended_api/memory_model.html#atomicity PTX Writers Guide to Interoperability. https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html 2. Programming Model \\uf0c1 2.1. A Highly Multithreaded Coprocessor \\uf0c1 The GPU is a compute device capable of executing a very large number of threads in parallel. It operates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive portions of applications running on the host are off-loaded onto the device. More precisely, a portion of an application that is executed many times, but independently on different data, can be isolated into a kernel function that is executed on the GPU as many different threads. To that effect, such a function is compiled to the PTX instruction set and the resulting kernel is translated at install time to the target GPU instruction set. 2.2. Thread Hierarchy \\uf0c1 The batch of threads that executes a kernel is organized as a grid. A grid consists of either cooperative thread arrays or clusters of cooperative thread arrays as described in this section and illustrated in Figure 1 and Figure 2 . Cooperative thread arrays (CTAs) implement CUDA thread blocks and clusters implement CUDA thread block clusters. 2.2.1. Cooperative Thread Arrays \\uf0c1 The Parallel Thread Execution (PTX) programming model is explicitly parallel: a PTX program specifies the execution of a given thread of a parallel thread array. A cooperative thread array , or CTA, is an array of threads that execute a kernel concurrently or in parallel. Threads within a CTA can communicate with each other. To coordinate the communication of the threads within the CTA, one can specify synchronization points where threads wait until all threads in the CTA have arrived. Each thread has a unique thread identifier within the CTA. Programs use a data parallel decomposition to partition inputs, work, and results across the threads of the CTA. Each CTA thread uses its thread identifier to determine its assigned role, assign specific input and output positions, compute addresses, and select work to perform. The thread identifier is a three-element vector tid , (with elements tid.x , tid.y , and tid.z ) that specifies the thread’s position within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the number of thread ids in that CTA dimension. Each CTA has a 1D, 2D, or 3D shape specified by a three-element vector ntid (with elements ntid.x , ntid.y , and ntid.z ). The vector ntid specifies the number of threads in each CTA dimension. Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called warps . A warp is a maximal subset of threads from a single CTA, such that the threads execute the same instructions at the same time. Threads within a warp are sequentially numbered. The warp size is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be able to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate constant, WARP_SZ , which may be used in any instruction where an immediate operand is allowed. 2.2.2. Cluster of Cooperative Thread Arrays \\uf0c1 Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate with each other via shared memory. The executing CTA has to make sure that the shared memory of the peer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before completing the shared memory operation. Threads within the different CTAs in a cluster can synchronize and communicate with each other via shared memory. Cluster-wide barriers can be used to synchronize all the threads within the cluster. Each CTA in a cluster has a unique CTA identifier within its cluster ( cluster_ctaid ). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter cluster_nctaid . Each CTA in the cluster also has a unique CTA identifier ( cluster_ctarank ) across all dimensions. The total number of CTAs across all the dimensions in the cluster is specified by cluster_nctarank . Threads may read and use these values through predefined, read-only special registers %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank . Cluster level is applicable only on target architecture sm_90 or higher. Specifying cluster level during launch time is optional. If the user specifies the cluster dimensions at launch time then it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster launch with default dimension 1x1x1. PTX provides read-only special register %is_explicit_cluster to differentiate between explicit and implicit cluster launch. 2.2.3. Grid of Clusters \\uf0c1 There is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a cluster can contain. However, clusters with CTAs that execute the same kernel can be batched together into a grid of clusters, so that the total number of threads that can be launched in a single kernel invocation is very large. This comes at the expense of reduced thread communication and synchronization, because threads in different clusters cannot communicate and synchronize with each other. Each cluster has a unique cluster identifier ( clusterid ) within a grid of clusters. Each grid of clusters has a 1D, 2D , or 3D shape specified by the parameter nclusterid . Each grid also has a unique temporal grid identifier ( gridid ). Threads may read and use these values through predefined, read-only special registers %tid , %ntid , %clusterid , %nclusterid , and %gridid . Each CTA has a unique identifier ( ctaid ) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape specified by the parameter nctaid . Thread may use and read these values through predefined, read-only special registers %ctaid and %nctaid . Each kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs where cluster is optional level and is applicable only for target architectures sm_90 and higher. Figure 1 shows a grid consisting of CTAs and Figure 2 shows a grid consisting of clusters. Grids may be launched with dependencies between one another - a grid may be a dependent grid and/or a prerequisite grid. To understand how grid dependencies may be defined, refer to the section on CUDA Graphs in the Cuda Programming Guide . Figure 1 Grid with CTAs \\uf0c1 Figure 2 Grid with clusters \\uf0c1 A cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads that execute the same kernel program. A grid is a set of clusters consisting of CTAs that execute independently. 2.3. Memory Hierarchy \\uf0c1 PTX threads may access data from multiple state spaces during their execution as illustrated by Figure 3 where cluster level is introduced from target architecture sm_90 onwards. Each thread has a private local memory. Each thread block (CTA) has a shared memory visible to all threads of the block and to all active blocks in the cluster and with the same lifetime as the block. Finally, all threads have access to the same global memory. There are additional state spaces accessible by all threads: the constant, param, texture, and surface state spaces. Constant and texture memory are read-only; surface memory is readable and writable. The global, constant, param, texture, and surface state spaces are optimized for different memory usages. For example, texture memory offers different addressing modes as well as data filtering for specific data formats. Note that texture and surface memory is cached, and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global or a surface write in the same kernel call returns undefined data. In other words, a thread can safely read some texture or surface memory location only if this memory location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call. The global, constant, and texture state spaces are persistent across kernel launches by the same application. Both the host and the device maintain their own local memory, referred to as host memory and device memory , respectively. The device memory may be mapped and read or written by the host, or, for more efficient transfer, copied from the host memory through optimized API calls that utilize the device’s high-performance Direct Memory Access (DMA) engine. Figure 3 Memory Hierarchy \\uf0c1 3. PTX Machine Model \\uf0c1 3.1. A Set of SIMT Multiprocessors \\uf0c1 The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) . When a host program invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors. A multiprocessor consists of multiple Scalar Processor (SP) cores, a multithreaded instruction unit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent threads in hardware with zero scheduling overhead. It implements a single-instruction barrier synchronization. Fast barrier synchronization together with lightweight thread creation and zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for example, a low granularity decomposition of problems by assigning one thread to each data element (such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation). To manage hundreds of threads running several different programs, the multiprocessor employs an architecture we call SIMT (single-instruction, multiple-thread) . The multiprocessor maps each thread to one scalar processor core, and each scalar thread executes independently with its own instruction address and register state. The multiprocessor SIMT unit creates, manages, schedules, and executes threads in groups of parallel threads called warps . (This term originates from weaving, the first parallel thread technology.)\\nIndividual threads composing a SIMT warp start together at the same program address but are otherwise free to branch and execute independently. When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that get scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0. At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues the next instruction to the active threads of the warp. A warp executes one common instruction at a time, so full efficiency is realized when all threads of a warp agree on their execution path. If threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each branch path taken, disabling threads that are not on that path, and when all paths complete, the threads converge back to the same execution path. Branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjointed code paths. SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction controls multiple processing elements. A key difference is that SIMD vector organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables programmers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads. For the purposes of correctness, the programmer can essentially ignore the SIMT behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. In practice, this is analogous to the role of cache lines in traditional code: Cache line size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance. Vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually. How many blocks a multiprocessor can process at once depends on how many registers per thread and how much shared memory per block are required for a given kernel since the multiprocessor’s registers and shared memory are split among all the threads of the batch of blocks. If there are not enough registers or shared memory available per multiprocessor to process at least one block, the kernel will fail to launch. Figure 4 Hardware Model \\uf0c1 A set of SIMT multiprocessors with on-chip shared memory.'},\n",
       " {'id': 578,\n",
       "  'content': '3.2. Independent Thread Scheduling \\uf0c1 On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp. As a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from. Starting with the Volta architecture, Independent Thread Scheduling allows full concurrency between threads, regardless of warp. With Independent Thread Scheduling , the GPU maintains execution state per thread, including a program counter and call stack, and can yield execution at a per-thread granularity, either to make better use of execution resources or to allow one thread to wait for data to be produced by another. A schedule optimizer determines how to group active threads from the same warp together into SIMT units. This retains the high throughput of SIMT execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity. Independent Thread Scheduling can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures. In particular, any warp-synchronous code (such as synchronization-free, intra-warp reductions) should be revisited to ensure compatibility with Volta and beyond. See the section on Compute Capability 7.x in the Cuda Programming Guide for further details.'},\n",
       " {'id': 579,\n",
       "  'content': '3.3. On-chip Shared Memory \\uf0c1 As illustrated by Figure 4 , each multiprocessor has on-chip memory of the four following types: One set of local 32-bit registers per processor, A parallel data cache or shared memory that is shared by all scalar processor cores and is where the shared memory space resides, A read-only constant cache that is shared by all scalar processor cores and speeds up reads from the constant memory space, which is a read-only region of device memory, A read-only texture cache that is shared by all scalar processor cores and speeds up reads from the texture memory space, which is a read-only region of device memory; each multiprocessor accesses the texture cache via a texture unit that implements the various addressing modes and data filtering. The local and global memory spaces are read-write regions of device memory.'},\n",
       " {'id': 580,\n",
       "  'content': '4. Syntax \\uf0c1 PTX programs are a collection of text source modules (files). PTX source modules have an assembly-language style syntax with instruction operation codes and operands. Pseudo-operations specify symbol and addressing management. The ptxas optimizing backend compiler optimizes and assembles PTX source modules to produce corresponding binary object files. 4.1.'},\n",
       " {'id': 581,\n",
       "  'content': 'Source Format \\uf0c1 Source modules are ASCII text. Lines are separated by the newline character ( \\\\n ). All whitespace characters are equivalent; whitespace is ignored except for its use in separating tokens in the language. The C preprocessor cpp may be used to process PTX source modules. Lines beginning with # are preprocessor directives. The following are common preprocessor directives: #include , #define , #if , #ifdef , #else , #endif , #line , #file C: A Reference Manual by Harbison and Steele provides a good description of the C preprocessor. PTX is case sensitive and uses lowercase for keywords. Each PTX module must begin with a .version directive specifying the PTX language version, followed by a .target directive specifying the target architecture assumed. See PTX Module Directives for a more information on these directives.'},\n",
       " {'id': 582,\n",
       "  'content': '4.2. Comments \\uf0c1 Comments in PTX follow C/C++ syntax, using non-nested /* and */ for comments that may span multiple lines, and using // to begin a comment that extends up to the next newline character, which terminates the current line. Comments cannot occur within character constants, string literals, or within other comments. Comments in PTX are treated as whitespace. 4.3. Statements \\uf0c1 A PTX statement is either a directive or an instruction. Statements begin with an optional label and end with a semicolon. Examples .reg .b32 r1, r2; .global .f32 array[N]; start: mov.b32 r1, %tid.x; shl.b32 r1, r1, 2; // shift thread id by 2 bits ld.global.b32 r2, array[r1]; // thread[tid] gets array[tid] add.f32 r2, r2, 0.5; // add 1/2 4.3.1. Directive Statements \\uf0c1 Directive keywords begin with a dot, so no conflict is possible with user-defined identifiers. The directives in PTX are listed in Table 1 and described in State Spaces, Types, and Variables and Directives . Table 1 PTX Directives \\uf0c1 .address_size .explicitcluster .maxnreg .section .alias .extern .maxntid .shared .align .file .minnctapersm .sreg .branchtargets .func .noreturn .target .callprototype .global .param .tex .calltargets .loc .pragma .version .common .local .reg .visible .const .maxclusterrank .reqnctapercluster .weak .entry .maxnctapersm .reqntid 4.3.2. Instruction Statements \\uf0c1 Instructions are formed from an instruction opcode followed by a comma-separated list of zero or more operands, and terminated with a semicolon. Operands may be register variables, constant expressions, address expressions, or label names. Instructions have an optional guard predicate which controls conditional execution. The guard predicate follows the optional label and precedes the opcode, and is written as @p , where p is a predicate register. The guard predicate may be optionally negated, written as @!p . The destination operand is first, followed by source operands. Instruction keywords are listed in Table 2 . All instruction keywords are reserved tokens in PTX. Table 2 Reserved Instruction Keywords \\uf0c1 abs discard min shf vadd activemask div mma shfl vadd2 add dp2a mov shl vadd4 addc dp4a movmatrix shr vavrg2 alloca elect mul sin vavrg4 and ex2 mul24 slct vmad applypriority exit multimem sqrt vmax atom fence nanosleep st vmax2 bar fma neg stackrestore vmax4 barrier fns not stacksave vmin bfe getctarank or stmatrix vmin2 bfi griddepcontrol pmevent sub vmin4 bfind isspacep popc subc vote bmsk istypep prefetch suld vset bra ld prefetchu suq vset2 brev ldmatrix prmt sured vset4 brkpt ldu rcp sust vshl brx lg2 red szext vshr call lop3 redux tanh vsub clz mad rem testp vsub2 cnot mad24 ret tex vsub4 copysign madc rsqrt tld4 wgmma cos mapa sad trap wmma cp match selp txq xor createpolicy max set vabsdiff cvt mbarrier setmaxnreg vabsdiff2 cvta membar setp vabsdiff4 4.4. Identifiers \\uf0c1 User-defined identifiers follow extended C++ rules: they either start with a letter followed by zero or more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar, or percentage character followed by one or more letters, digits, underscore, or dollar characters: followsym: [a-zA-Z0-9_$] identifier: [a-zA-Z]{followsym}* | {[_$%]{followsym}+ PTX does not specify a maximum length for identifiers and suggests that all implementations support a minimum length of at least 1024 characters.'},\n",
       " {'id': 583,\n",
       "  'content': 'Many high-level languages such as C and C++ follow similar rules for identifier names, except that the percentage sign is not allowed. PTX allows the percentage sign as the first character of an identifier. The percentage sign can be used to avoid name conflicts, e.g., between user-defined variable names and compiler-generated names. PTX predefines one constant and a small number of special registers that begin with the percentage sign, listed in Table 3 . Table 3 Predefined Identifiers \\uf0c1 %clock %laneid %lanemask_gt %pm0, ..., %pm7 %clock64 %lanemask_eq %nctaid %smid %ctaid %lanemask_le %ntid %tid %envreg %lanemask_lt %nsmid %warpid %gridid %lanemask_ge %nwarpid WARP_SZ 4.5. Constants \\uf0c1 PTX supports integer and floating-point constants and constant expressions. These constants may be used in data initialization and as operands to instructions. Type checking rules remain the same for integer, floating-point, and bit-size types. For predicate-type data and instructions, integer constants are allowed and are interpreted as in C, i.e., zero values are False and non-zero values are True . 4.5.1. Integer Constants \\uf0c1 Integer constants are 64-bits in size and are either signed or unsigned, i.e., every integer constant has type .s64 or .u64 . The signed/unsigned nature of an integer constant is needed to correctly evaluate constant expressions containing operations such as division and ordered comparisons, where the behavior of the operation depends on the operand types. When used in an instruction or data initialization, each integer constant is converted to the appropriate size based on the data or instruction type at its use. Integer literals may be written in decimal, hexadecimal, octal, or binary notation. The syntax follows that of C. Integer literals may be followed immediately by the letter U to indicate that the literal is unsigned. hexadecimal literal: 0[xX]{hexdigit}+U? octal literal: 0{octal digit}+U? binary literal: 0[bB]{bit}+U? decimal literal {nonzero-digit}{digit}*U? Integer literals are non-negative and have a type determined by their magnitude and optional type suffix as follows: literals are signed ( .s64 ) unless the value cannot be fully represented in .s64 or the unsigned suffix is specified, in which case the literal is unsigned ( .u64 ). The predefined integer constant WARP_SZ specifies the number of threads per warp for the target platform; to date, all target architectures have a WARP_SZ value of 32. 4.5.2. Floating-Point Constants \\uf0c1 Floating-point constants are represented as 64-bit double-precision values, and all floating-point constant expressions are evaluated using 64-bit double precision arithmetic. The only exception is the 32-bit hex notation for expressing an exact single-precision floating-point value; such values retain their exact 32-bit single-precision value and may not be used in constant expressions. Each 64-bit floating-point constant is converted to the appropriate floating-point size based on the data or instruction type at its use. Floating-point literals may be written with an optional decimal point and an optional signed exponent. Unlike C and C++, there is no suffix letter to specify size; literals are always represented in 64-bit double-precision format. PTX includes a second representation of floating-point constants for specifying the exact machine representation using a hexadecimal constant. To specify IEEE 754 double-precision floating point values, the constant begins with 0d or 0D followed by 16 hex digits. To specify IEEE 754 single-precision floating point values, the constant begins with 0f or 0F followed by 8 hex digits. 0[fF]{hexdigit}{8} // single-precision floating point 0[dD]{hexdigit}{16} // double-precision floating point Example mov.f32 $f3, 0F3f800000; // 1.0 4.5.3. Predicate Constants \\uf0c1 In PTX, integer constants may be used as predicates. For predicate-type data initializers and instruction operands, integer constants are interpreted as in C, i.e., zero values are False and non-zero values are True . 4.5.4. Constant Expressions \\uf0c1 In PTX, constant expressions are formed using operators as in C and are evaluated using rules similar to those in C, but simplified by restricting types and sizes, removing most casts, and defining full semantics to eliminate cases where expression evaluation in C is implementation dependent. Constant expressions are formed from constant literals, unary plus and minus, basic arithmetic operators (addition, subtraction, multiplication, division), comparison operators, the conditional ternary operator ( ? : ), and parentheses. Integer constant expressions also allow unary logical negation ( ! ), bitwise complement ( ~ ), remainder ( % ), shift operators ( > ), bit-type operators ( & , | , and ^ ), and logical operators ( && , || ). Constant expressions in PTX do not support casts between integer and floating-point. Constant expressions are evaluated using the same operator precedence as in C. Table 4 gives operator precedence and associativity. Operator precedence is highest for unary operators and decreases with each line in the chart. Operators on the same line have the same precedence and are evaluated right-to-left for unary operators and left-to-right for binary operators. Table 4 Operator Precedence \\uf0c1 Kind Operator Symbols Operator Names Associates Primary () parenthesis n/a Unary +- ! ~ plus, minus, negation, complement right (.s64) (.u64) casts right Binary */ % multiplication, division, remainder left +- addition, subtraction >> = ordered comparisons == != equal, not equal & bitwise AND ^ bitwise XOR | bitwise OR && logical AND || logical OR Ternary ? : conditional right 4.5.5. Integer Constant Expression Evaluation \\uf0c1 Integer constant expressions are evaluated at compile time according to a set of rules that determine the type (signed .s64 versus unsigned .u64 ) of each sub-expression. These rules are based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and behavior is fully defined in all cases (specifically, for remainder and shift operators). Literals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a U suffix. For example: 42 , 0x1234 , 0123 are signed. 0xfabc123400000000 , 42U , 0x1234U are unsigned. Unary plus and minus preserve the type of the input operand. For example: +123 , -1 , -(-42) are signed. -1U , -0xfabc123400000000 are unsigned. Unary logical negation ( ! )\\nproduces a signed result with value 0 or 1 . Unary bitwise complement ( ~ ) interprets the source operand as unsigned and produces an unsigned result. Some binary operators require normalization of source operands. This normalization is known as the usual arithmetic conversions and simply converts both operands to unsigned type if either operand is unsigned. Addition, subtraction, multiplication, and division perform the usual arithmetic conversions and produce a result with the same type as the converted operands. That is, the operands and result are unsigned if either source operand is unsigned, and is otherwise signed. Remainder ( % ) interprets the operands as unsigned. Note that this differs from C, which allows a negative divisor but defines the behavior to be implementation dependent. Left and right shift interpret the second operand as unsigned and produce a result with the same type as the first operand. Note that the behavior of right-shift is determined by the type of the first operand: right shift of a signed value is arithmetic and preserves the sign, and right shift of an unsigned value is logical and shifts in a zero bit. AND ( & ), OR ( | ), and XOR ( ^ ) perform the usual arithmetic conversions and produce a result with the same type as the converted operands. AND_OP ( && ), OR_OP ( || ), Equal ( == ), and Not_Equal ( != ) produce a signed result. The result value is 0 or 1. Ordered comparisons ( , >= ) perform the usual arithmetic conversions on source operands and produce a signed result. The result value is 0 or 1 . Casting of expressions to signed or unsigned is supported using ( .s64 ) and ( .u64 ) casts. For the conditional operator ( ? : ) , the first operand must be an integer, and the second and third operands are either both integers or both floating-point. The usual arithmetic conversions are performed on the second and third operands, and the result type is the same as the converted type. 4.5.6. Summary of Constant Expression Evaluation Rules \\uf0c1 Table 5 contains a summary of the constant expression evaluation rules. Table 5 Constant Expression Evaluation Rules \\uf0c1 Kind Operator Operand Types Operand Interpretation Result Type Primary () any type same as source same as source constant literal n/a n/a .u64 , .s64 , or .f64 Unary +- any type same as source same as source ! integer zero or non-zero .s64 ~ integer .u64 .u64 Cast (.u64) integer .u64 .u64 (.s64) integer .s64 .s64 Binary +- * / .f64 .f64 .f64 integer use usual conversions converted type = .f64 .f64 .s64 integer use usual conversions .s64 == != .f64 .f64 .s64 integer use usual conversions .s64 % integer .u64 .s64 >> =10 bits). The internal layout of tf32 format is implementation defined. PTX facilitates conversion from single precision .f32 type to tf32 format. A register variable containing tf32 data must be declared with .b32 type. Alternate data formats cannot be used as fundamental types. They are supported as source or destination formats by certain instructions. 5.2.4. Packed Data Types \\uf0c1 Certain PTX instructions operate on two sets of inputs in parallel, and produce two outputs. Such instructions can use the data stored in a packed format. PTX supports packing two values of the same scalar data type into a single, larger value. The packed value is considered as a value of a packed data type . In this section we describe the packed data types supported in PTX. 5.2.4.1. Packed Floating Point Data Types \\uf0c1 PTX supports the following four variants of packed floating point data types: .f16x2 packed type containing two .f16 floating point values. .bf16x2 packed type containing two .bf16 alternate floating point values. .e4m3x2 packed type containing two .e4m3 alternate floating point values. .e5m2x2 packed type containing two .e5m2 alternate floating point values. .f16x2 is supported as a fundamental type. .bf16x2 , .e4m3x2 and .e5m2x2 cannot be used as fundamental types - they are supported as instruction types on certain instructions. A register variable containing .bf16x2 data must be declared with .b32 type. A register variable containing .e4m3x2 or .e5m2x2 data must be declared with .b16 type. 5.2.4.2. Packed Integer Data Types \\uf0c1 PTX supports two variants of packed integer data types: .u16x2 and .s16x2 . The packed data type consists of two .u16 or .s16 values. A register variable containing .u16x2 or .s16x2 data must be declared with .b32 type. Packed integer data types cannot be used as fundamental types. They are supported as instruction types on certain instructions.'},\n",
       " {'id': 584,\n",
       "  'content': '5.3. Texture Sampler and Surface Types \\uf0c1 PTX includes built-in opaque types for defining texture, sampler, and surface descriptor variables. These types have named fields similar to structures, but all information about layout, field ordering, base address, and overall size is hidden to a PTX program, hence the term opaque . The use of these opaque types is limited to: Variable definition within global (module) scope and in kernel entry parameter lists. Static initialization of module-scope variables using comma-delimited static assignment expressions for the named members of the type. Referencing textures, samplers, or surfaces via texture and surface load/store instructions ( tex , suld , sust , sured ). Retrieving the value of a named member via query instructions ( txq , suq ). Creating pointers to opaque variables using mov , e.g., mov.u64 reg, opaque_var; . The resulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and de-referenced by texture and surface load, store, and query instructions, but the pointer cannot otherwise be treated as an address, i.e., accessing the pointer with ld and st instructions, or performing pointer arithmetic will result in undefined results. Opaque variables may not appear in initializers, e.g., to initialize a pointer to an opaque variable. Note Indirect access to textures and surfaces using pointers to opaque variables is supported beginning with PTX ISA version 3.1 and requires target sm_20 or later. Indirect access to textures is supported only in unified texture mode (see below). The three built-in types are .texref , .samplerref , and .surfref . For working with textures and samplers, PTX has two modes of operation. In the unified mode, texture and sampler information is accessed through a single .texref handle. In the independent mode , texture and sampler information each have their own handle, allowing them to be defined separately and combined at the site of usage in the program. In independent mode, the fields of the .texref type that describe sampler properties are ignored, since these properties are defined by .samplerref variables. Table 9 and Table 10 list the named members of each type for unified and independent texture modes. These members and their values have precise mappings to methods and values defined in the texture HW class as well as exposed values via the API. Table 9 Opaque Type Fields in Unified Texture Mode \\uf0c1 Member .texref values .surfref values width in elements height in elements depth in elements channel_data_type enum type corresponding to source language API channel_order enum type corresponding to source language API normalized_coords 0 , 1 N/A filter_mode nearest , linear N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A array_size as number of textures in a texture array as number of surfaces in a surface array num_mipmap_levels as number of levels in a mipmapped texture N/A num_samples as number of samples in a multi-sample texture N/A memory_layout N/A 1 for linear memory layout; 0 otherwise 5.3.1. Texture and Surface Properties \\uf0c1 Fields width , height , and depth specify the size of the texture or surface in number of elements in each dimension. The channel_data_type and channel_order fields specify these properties of the texture or surface using enumeration types corresponding to the source language API. For example, see Channel Data Type and Channel Order Fields for the OpenCL enumeration types currently supported in PTX. 5.3.2. Sampler Properties \\uf0c1 The normalized_coords field indicates whether the texture or surface uses normalized coordinates in the range [0.0, 1.0) instead of unnormalized coordinates in the range [0, N). If no value is specified, the default is set by the runtime system based on the source language. The filter_mode field specifies how the values returned by texture reads are computed based on the input texture coordinates. The addr_mode_{0,1,2} fields define the addressing mode in each dimension, which determine how out-of-range coordinates are handled. See the CUDA C++ Programming Guide for more details of these properties. Table 10 Opaque Type Fields in Independent Texture Mode \\uf0c1 Member .samplerref values .texref values .surfref values width N/A in elements height N/A in elements depth N/A in elements channel_data_type N/A enum type corresponding to source language API channel_order N/A enum type corresponding to source language AP normalized_coords N/A 0 , 1 N/A force_unnormalized_coords 0 , 1 N/A N/A filter_mode nearest , linear ignored N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A N/A array_size N/A as number of textures in a texture array as number of surfaces in a surface array num_mipmap_levels N/A as number of levels in a mipmapped texture N/A num_samples N/A as number of samples in a multi-sample texture N/A memory_layout N/A N/A 1 for linear memory layout; 0 otherwise In independent texture mode, the sampler properties are carried in an independent .samplerref variable, and these fields are disabled in the .texref variables. One additional sampler property, force_unnormalized_coords , is available in independent texture mode.'},\n",
       " {'id': 585,\n",
       "  'content': 'The force_unnormalized_coords field is a property of .samplerref variables that allows the sampler to override the texture header normalized_coords property. This field is defined only in independent texture mode. When True , the texture header setting is overridden and unnormalized coordinates are used; when False , the texture header setting is used. The force_unnormalized_coords property is used in compiling OpenCL; in OpenCL, the property of normalized coordinates is carried in sampler headers. To compile OpenCL to PTX, texture headers are always initialized with normalized_coords set to True, and the OpenCL sampler-based normalized_coords flag maps (negated) to the PTX-level force_unnormalized_coords flag. Variables using these types may be declared at module scope or within kernel entry parameter lists.'},\n",
       " {'id': 586,\n",
       "  'content': 'At module scope, these variables must be in the .global state space. As kernel parameters, these variables are declared in the .param state space. Example .global .texref my_texture_name; .global .samplerref my_sampler_name; .global .surfref my_surface_name; When declared at module scope, the types may be initialized using a list of static expressions assigning values to the named members. Example .global .texref tex1; .global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border, filter_mode = nearest }; 5.3.3. Channel Data Type and Channel Order Fields \\uf0c1 The channel_data_type and channel_order fields have enumeration types corresponding to the source language API. Currently, OpenCL is the only source language that defines these fields. Table 12 and Table 11 show the enumeration values defined in OpenCL version 1.0 for channel data type and channel order. Table 11 OpenCL 1.0 Channel Data Type Definition \\uf0c1 CL_SNORM_INT8 0x10D0 CL_SNORM_INT16 0x10D1 CL_UNORM_INT8 0x10D2 CL_UNORM_INT16 0x10D3 CL_UNORM_SHORT_565 0x10D4 CL_UNORM_SHORT_555 0x10D5 CL_UNORM_INT_101010 0x10D6 CL_SIGNED_INT8 0x10D7 CL_SIGNED_INT16 0x10D8 CL_SIGNED_INT32 0x10D9 CL_UNSIGNED_INT8 0x10DA CL_UNSIGNED_INT16 0x10DB CL_UNSIGNED_INT32 0x10DC CL_HALF_FLOAT 0x10DD CL_FLOAT 0x10DE Table 12 OpenCL 1.0 Channel Order Definition \\uf0c1 CL_R 0x10B0 CL_A 0x10B1 CL_RG 0x10B2 CL_RA 0x10B3 CL_RGB 0x10B4 CL_RGBA 0x10B5 CL_BGRA 0x10B6 CL_ARGB 0x10B7 CL_INTENSITY 0x10B8 CL_LUMINANCE 0x10B9 5.4. Variables \\uf0c1 In PTX, a variable declaration describes both the variable’s type and its state space.'},\n",
       " {'id': 587,\n",
       "  'content': 'In addition to fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays. 5.4.1.'},\n",
       " {'id': 588,\n",
       "  'content': 'Variable Declarations \\uf0c1 All storage for data is specified with variable declarations. Every variable must reside in one of the state spaces enumerated in the previous section. A variable declaration names the space in which the variable resides, its type and size, its name, an optional array size, an optional initializer, and an optional fixed address for the variable. Predicate variables may only be declared in the register state space. Examples .global .u32 loc; .reg .s32 i; .const .f32 bias[] = {-1.0, 1.0}; .global .u8 bg[4] = {0, 0, 0, 0}; .reg .v4 .f32 accel; .reg .pred p, q, r; 5.4.2. Vectors \\uf0c1 Limited-length vector types are supported. Vectors of length 2 and 4 of any non-predicate fundamental type can be declared by prefixing the type with .v2 or .v4 . Vectors must be based on a fundamental type, and they may reside in the register space. Vectors cannot exceed 128-bits in length; for example, .v4 .f64 is not allowed. Three-element vectors may be handled by using a .v4 vector, where the fourth element provides padding. This is a common case for three-dimensional grids, textures, etc. Examples .global .v4 .f32 V; // a length-4 vector of floats .shared .v2 .u16 uv; // a length-2 vector of unsigned ints .global .v4 .b8 v; // a length-4 vector of bytes By default, vector variables are aligned to a multiple of their overall size (vector length times base-type size), to enable vector load and store instructions which require addresses aligned to a multiple of the access size. 5.4.3. Array Declarations \\uf0c1 Array declarations are provided to allow the programmer to reserve space. To declare an array, the variable name is followed with dimensional declarations similar to fixed-size array declarations in C. The size of each dimension is a constant expression. Examples .local .u16 kernel[19][19]; .shared .u8 mailbox[128]; The size of the array specifies how many elements should be reserved. For the declaration of array kernel above, 19*19 = 361 halfwords are reserved, for a total of 722 bytes. When declared with an initializer, the first dimension of the array may be omitted. The size of the first array dimension is determined by the number of elements in the array initializer. Examples .global .u32 index[] = { 0, 1, 2, 3, 4, 5, 6, 7 }; .global .s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} }; Array index has eight elements, and array offset is a 4x2 array. 5.4.4. Initializers \\uf0c1 Declared variables may specify an initial value using a syntax similar to C/C++, where the variable name is followed by an equals sign and the initial value or values for the variable. A scalar takes a single value, while vectors and arrays take nested lists of values inside of curly braces (the nesting matches the dimensionality of the declaration). As in C, array initializers may be incomplete, i.e., the number of initializer elements may be less than the extent of the corresponding array dimension, with remaining array locations initialized to the default value for the specified array type. Examples .const .f32 vals[8] = { 0.33, 0.25, 0.125 }; .global .s32 x[3][2] = { {1,2}, {3} }; is equivalent to .const .f32 vals[8] = { 0.33, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0 }; .global .s32 x[3][2] = { {1,2}, {3,0}, {0,0} }; Currently, variable initialization is supported only for constant and global state spaces. Variables in constant and global state spaces with no explicit initializer are initialized to zero by default. Initializers are not allowed in external variable declarations. Variable names appearing in initializers represent the address of the variable; this can be used to statically initialize a pointer to a variable. Initializers may also contain var+offset expressions, where offset is a byte offset added to the address of var . Only variables in .global or .const state spaces may be used in initializers. By default, the resulting address is the offset in the variable’s state space (as is the case when taking the address of a variable with a mov instruction). An operator, generic() , is provided to create a generic address for variables used in initializers. Starting PTX ISA version 7.1, an operator mask() is provided, where mask is an integer immediate. The only allowed expressions in the mask() operator are integer constant expression and symbol expression representing address of variable. The mask() operator extracts n consecutive bits from the expression used in initializers and inserts these bits at the lowest position of the initialized variable. The number n and the starting position of the bits to be extracted is specified by the integer immediate mask . PTX ISA version 7.1 only supports extracting a single byte starting at byte boundary from the address of the variable. PTX ISA version 7.3 supports Integer constant expression as an operand in the mask() operator. Supported values for mask are: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000, 0xFF000000000000, 0xFF00000000000000. Examples .const .u32 foo = 42; .global .u32 bar[] = { 2, 3, 5 }; .global .u32 p1 = foo; // offset of foo in .const space .global .u32 p2 = generic(foo); // generic address of foo // array of generic-address pointers to elements of bar .global .u32 parr[] = { generic(bar), generic(bar)+4, generic(bar)+8 }; // examples using mask() operator are pruned for brevity .global .u8 addr[] = {0xff(foo), 0xff00(foo), 0xff0000(foo), ...}; .global .u8 addr2[] = {0xff(foo+4), 0xff00(foo+4), 0xff0000(foo+4),...} .global .u8 addr3[] = {0xff(generic(foo)), 0xff00(generic(foo)),...} .global .u8 addr4[] = {0xff(generic(foo)+4), 0xff00(generic(foo)+4),...} // mask() operator with integer const expression .global .u8 addr5[] = { 0xFF(1000 + 546), 0xFF00(131187), ...}; Note PTX 3.1 redefines the default addressing for global variables in initializers, from generic addresses to offsets in the global state space. Legacy PTX code is treated as having an implicit generic() operator for each global variable used in an initializer.'},\n",
       " {'id': 589,\n",
       "  'content': 'PTX 3.1 code should either include explicit generic() operators in initializers, use cvta.global to form generic addresses at runtime, or load from the non-generic address using ld.global . Device function names appearing in initializers represent the address of the first instruction in the function; this can be used to initialize a table of function pointers to be used with indirect calls. Beginning in PTX ISA version 3.1, kernel function names can be used as initializers e.g. to initialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch kernels from GPU. See the CUDA Dynamic Parallelism Programming Guide for details. Labels cannot be used in initializers. Variables that hold addresses of variables or functions should be of type .u8 or .u32 or .u64 . Type .u8 is allowed only if the mask() operator is used. Initializers are allowed for all types except .f16 , .f16x2 and .pred . Examples .global .s32 n = 10; .global .f32 blur_kernel[][3] = {{.05,.1,.05},{.1,.4,.1},{.05,.1,.05}}; .global .u32 foo[] = { 2, 3, 5, 7, 9, 11 }; .global .u64 ptr = generic(foo); // generic address of foo[0] .global .u64 ptr = generic(foo)+8; // generic address of foo[2] 5.4.5. Alignment \\uf0c1 Byte alignment of storage for all addressable variables can be specified in the variable declaration. Alignment is specified using an optional .align byte-count specifier immediately following the state-space specifier. The variable will be aligned to an address which is an integer multiple of byte-count. The alignment value byte-count must be a power of two. For arrays, alignment specifies the address alignment for the starting address of the entire array, not for individual elements. The default alignment for scalar and array variables is to a multiple of the base-type size. The default alignment for vector variables is to a multiple of the overall vector size. Examples // allocate array at 4-byte aligned address. Elements are bytes. .const .align 4 .b8 bar[8] = {0,0,0,0,2,0,0,0}; Note that all PTX instructions that access memory require that the address be aligned to a multiple of the access size. The access size of a memory instruction is the total number of bytes accessed in memory. For example, the access size of ld.v4.b32 is 16 bytes, while the access size of atom.f16x2 is 4 bytes. 5.4.6. Parameterized Variable Names \\uf0c1 Since PTX supports virtual registers, it is quite common for a compiler frontend to generate a large number of register names. Rather than require explicit declaration of every name, PTX supports a syntax for creating a set of variables having a common prefix string appended with integer suffixes. For example, suppose a program uses a large number, say one hundred, of .b32 variables, named %r0 , %r1 , …, %r99 . These 100 register variables can be declared as follows: .reg .b32 %r; // declare %r0, %r1, ..., %r99 This shorthand syntax may be used with any of the fundamental types and with any state space, and may be preceded by an alignment specifier. Array variables cannot be declared this way, nor are initializers permitted. 5.4.7. Variable Attributes \\uf0c1 Variables may be declared with an optional .attribute directive which allows specifying special attributes of variables. Keyword .attribute is followed by attribute specification inside parenthesis. Multiple attributes are separated by comma. Variable and Function Attribute Directive: .attribute describes the .attribute directive. 5.4.8. Variable and Function Attribute Directive: .attribute \\uf0c1 .attribute Variable and function attributes Description Used to specify special attributes of a variable or a function. The following attributes are supported. .managed .managed attribute specifies that variable will be allocated at a location in unified virtual memory environment where host and other devices in the system can reference the variable directly. This attribute can only be used with variables in .global state space. See the CUDA UVM-Lite Programming Guide for details. .unified .unified attribute specifies that function has the same memory address on the host and on other devices in the system. Integer constants uuid1 and uuid2 respectively specify upper and lower 64 bits of the unique identifier associated with the function or the variable. This attribute can only be used on device functions or on variables in the .global state space. Variables with .unified attribute are read-only and must be loaded by specifying .unified qualifier on the address operand of ld instruction, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 4.0. Support for function attributes introduced in PTX ISA version 8.0. Target ISA Notes .managed attribute requires sm_30 or higher. .unified attribute requires sm_90 or higher. Examples .global .attribute(.managed) .s32 g; .global .attribute(.managed) .u64 x; .global .attribute(.unified(19,95)) .f32 f; .func .attribute(.unified(0xAB, 0xCD)) bar() { ...'},\n",
       " {'id': 590,\n",
       "  'content': '} 5.5. Tensors \\uf0c1 A tensor is a multi-dimensional matrix structure in the memory. Tensor is defined by the following properties: Dimensionality Dimension sizes across each dimension Individual element types Tensor stride across each dimension PTX supports instructions which can operate on the tensor data. PTX Tensor instructions include: Copying data between global and shared memories Reducing the destination tensor data with the source. The Tensor data can be operated on by various wmma.mma , mma and wgmma.mma_async instructions. PTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure and treat the data in the shared memory as a linear data. 5.5.1. Tensor Dimension, size and format \\uf0c1 Tensors can have dimensions: 1D, 2D, 3D, 4D or 5D. Each dimension has a size which represents the number of elements along the dimension. The elements can have one the following types: Bit-sized type: .b32 , .b64 Integer: .u8 , .u16 , .u32 , .s32 , .u64 , .s64 Floating point and alternate floating point: .f16 , .bf16 , .tf32 , .f32 , .f64 (rounded to nearest even). Tensor can have padding at the end in each of the dimensions to provide alignment for the data in the subsequent dimensions. Tensor stride can be used to specify the amount of padding in each dimension. 5.5.2. Tensor Access Modes \\uf0c1 Tensor data can be accessed in two modes: Tiled mode: In tiled mode, the source multi-dimensional tensor layout is preserved at the destination. Im2col mode: In im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns at the destination. Refer here for more details.'},\n",
       " {'id': 591,\n",
       "  'content': '5.5.3. Tiled Mode \\uf0c1 This section talks about how Tensor and Tensor access work in tiled mode. 5.5.3.1. Bounding Box \\uf0c1 A tensor can be accessed in chunks known as Bounding Box . The Bounding Box has the same dimensionality as the tensor they are accessing into. Size of each bounding Box must be a multiple of 16 bytes. The address of the bounding Box must also be aligned to 16 bytes. Bounding Box has the following access properties: Bounding Box dimension sizes Out of boundary access mode Traversal strides The tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the bounding box. Starting offset of the bounding box along with the rest of the bounding box information together are used to determine the elements which are to be accessed. 5.5.3.2. Traversal-Stride \\uf0c1 While the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies the exact number of elements to be skipped. If no jump over is required, default value of 1 must be specified. The traversal stride in dimension 0 can be used for the Interleave layout . For non-interleaved layout, the traversal stride in dimension 0 must always be 1. Figure 5 illustrates tensor, tensor size, tensor stride, Bounding Box size and traversal stride. Figure 5 Tiled mode bounding box, tensor size and traversal stride \\uf0c1 5.5.3.3. Out of Boundary Access \\uf0c1 PTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor boundary in any dimension. There are 2 modes: Zero fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to 0. OOB-NaN fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN called OOB-NaN . Figure 6 shows an example of the out of boundary access. Figure 6 Out of boundary access \\uf0c1 5.5.4. Im2col mode \\uf0c1 Im2col mode supports the following tensor dimensions : 3D, 4D and 5D. In this mode, the tensor data is treated as a batch of images with the following properties: N : number of images in the batch D, H, W : size of a 3D image (depth, height and width) C: channels per image element The above properties are associated with 3D, 4D and 5D tensors as follows: Dimension N/D/H/W/C applicability 3D NWC 4D NHWC 5D NDHWC 5.5.4.1. Bounding Box \\uf0c1 In im2col mode, the Bounding Box is defined in DHW space. Boundaries along other dimensions are specified by Pixels-per-Column and Channels-per-Pixel parameters as described below. The dimensionality of the Bounding Box is two less than the tensor dimensionality. The following properties describe how to access of the elements in im2col mode: Bounding-Box Lower-Corner Bounding-Box Upper-Corner Pixels-per-Column Channels-per-Pixel Bounding-box Lower-Corner and Bounding-box Upper-Corner specify the two opposite corners of the Bounding Box in the DHW space. Bounding-box Lower-Corner specifies the corner with the smallest coordinate and Bounding-box Upper-Corner specifies the corner with the largest coordinate. Bounding-box Upper- and Lower-Corners are 16-bit signed values whose limits varies across the dimensions and are as shown below: 3D 4D 5D Upper- / Lower- Corner sizes [-2 15 , 2 15 -1] [-2 7 , 2 7 -1] [-2 4 , 2 4 -1] Figure 7 and Figure 8 show the Upper-Corners and Lower-Corners. Figure 7 im2col mode bounding box example 1 \\uf0c1 Figure 8 im2col mode bounding box example 2 \\uf0c1 The Bounding-box Upper- and Lower- Corners specify only the boundaries and not the number of elements to be accessed. Pixels-per-Column specifies the number of elements to be accessed in the NDHW space.'},\n",
       " {'id': 592,\n",
       "  'content': 'Channels-per-Pixel specifies the number of elements to access across the C dimension. The tensor coordinates, specified in the PTX tensor instructions, behaves differently in different dimensions: Across N and C dimensions: specify the starting offsets along the dimension, similar to the tiled mode. Across DHW dimensions: specify the location of the convolution filter base in the tensor space. The filter corner location must be within the bounding box. The im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter base coordinates to determine the starting location in the tensor space from where the elements are accessed. The size of the im2col offsets varies across the dimensions and their valid ranges are as shown below: 3D 4D 5D im2col offsets range [0, 2 16 -1] [0, 2 8 -1] [0, 2 5 -1] Following are some examples of the im2col mode accesses: Example 1 ( Figure 9 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1. tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 0 , 0 ) Figure 9 im2col mode example 1 \\uf0c1 Example 2 ( Figure 10 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower - Corner W = 0 Bounding - Box Lower - Corner H = 0 Bounding - Box Upper - Corner W = -2 Bounding - Box Upper - Corner H = -2 tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 2 , 2 ) Figure 10 im2col mode example 2 \\uf0c1 5.5.4.2. Traversal Stride \\uf0c1 The traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being accessed unlike the tiled mode. Pixels-per-Column determines the total number of elements being accessed, in im2col mode. The number of elements traversed along the D, H and W dimensions is strided by the traversal stride for that dimension. The following example with Figure 11 illustrates accesse with traversal-strides: Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 8 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Traversal Stride = 2 Pixels - per - Column = 32 channels - per - pixel = 16 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1. Tensor coordinates in the instruction = ( 7 , 7 , 5 , 0 ) Im2col offsets in the instruction : ( 1 , 1 ) Figure 11 im2col mode traversal stride example \\uf0c1 5.5.4.3. Out of Boundary Access \\uf0c1 In im2col mode, when the number of requested pixels in NDHW space specified by Pixels-per-Column exceeds the number of available pixels in the image batch then out-of-bounds access is performed. Similar to tiled mode, zero fill or OOB-NaN fill can be performed based on the Fill-Mode specified. 5.5.5. Interleave layout \\uf0c1 Tensor can be interleaved and the following interleave layouts are supported: No interleave (NDHWC) 8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel. 16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel. The C information is organized in slices where sequential C elements are grouped in 16 byte or 32 byte quantities. If the total number of channels is not a multiple of the number of channels per slice, then the last slice must be padded with zeros to make it complete 16B or 32B slice. Interleaved layouts are supported only for the dimensionalities : 3D, 4D and 5D.'},\n",
       " {'id': 593,\n",
       "  'content': '5.5.6. Swizzling Modes \\uf0c1 The layout of the data in the shared memory can be different to that of global memory, for access performance reasons. The following describes various swizzling modes: No swizzle mode: There is no swizzling in this mode and the destination data layout is exactly similar to the source data layout. 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 … Pattern repeats … 32 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is 256 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 … Pattern repeats … An example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension, with the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in Figure 12 . Figure 12 32-byte swizzle mode example \\uf0c1 Figure 13 shows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1. Figure 13 32-byte swizzle mode fragments \\uf0c1 Figure 14 shows the destination data layout with 32 byte swizzling. Figure 14 32-byte swizzle mode destination data layout \\uf0c1 64 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is 512 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 … Pattern repeats … An example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes / channel and 32 channels, is shown in Figure 15 . Figure 15 64-byte swizzle mode example \\uf0c1 Each colored cell represents 8 channels. Figure 16 shows the source data layout. Figure 16 64-byte swizzle mode source data layout \\uf0c1 Figure 17 shows the destination data layout with 64 byte swizzling. Figure 17 64-byte swizzle mode destination data layout \\uf0c1 128 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is 1024 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 4 5 6 7 0 1 2 3 5 4 7 6 1 0 3 2 6 7 4 5 2 3 0 1 … Pattern repeats … An example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes / channel and 64 channels, is shown in Figure 18 . Figure 18 128-byte swizzle mode example \\uf0c1 Each colored cell represents 8 channels. Figure 19 shows the source data layout. Figure 19 128-byte swizzle mode source data layout \\uf0c1 Figure 20 shows the destination data layout with 128 byte swizzling. Figure 20 128-byte swizzle mode destination data layout \\uf0c1 5.5.7. Tensor-map \\uf0c1 The tensor-map is a 128-byte opaque object either in .const space or .param (kernel function parameter) space or .global space which describes the tensor properties and the access properties of the tensor data described in previous sections. Tensor-Map can be created using CUDA APIs. Refer to CUDA programming guide for more details.'},\n",
       " {'id': 594, 'content': '6.'},\n",
       " {'id': 595,\n",
       "  'content': 'Instruction Operands \\uf0c1 6.1. Operand Type Information \\uf0c1 All operands in instructions have a known type from their declarations. Each operand type must be compatible with the type determined by the instruction template and instruction type. There is no automatic conversion between types. The bit-size type is compatible with every type having the same size. Integer types of a common size are compatible with each other. Operands having type different from but compatible with the instruction type are silently cast to the instruction type. 6.2. Source Operands \\uf0c1 The source operands are denoted in the instruction descriptions by the names a , b , and c . PTX describes a load-store machine, so operands for ALU instructions must all be in variables declared in the .reg register state space. For most operations, the sizes of the operands must be consistent. The cvt (convert) instruction takes a variety of operand types and sizes, as its job is to convert from nearly any data type to any other data type (and size). The ld , st , mov , and cvt instructions copy data from one location to another. Instructions ld and st move data from/to addressable state spaces to/from registers. The mov instruction copies data between registers. Most instructions have an optional predicate guard that controls conditional execution, and a few instructions have additional predicate source operands. Predicate operands are denoted by the names p , q , r , s . 6.3. Destination Operands \\uf0c1 PTX instructions that produce a single result store the result in the field denoted by d (for destination) in the instruction descriptions. The result operand is a scalar or vector variable in the register state space. 6.4.'},\n",
       " {'id': 596,\n",
       "  'content': 'Using Addresses, Arrays, and Vectors \\uf0c1 Using scalar variables as operands is straightforward. The interesting capabilities begin with addresses, arrays, and vectors. 6.4.1. Addresses as Operands \\uf0c1 All the memory instructions take an address operand that specifies the memory location being accessed. This addressable operand is one of: [var] the name of an addressable variable var . [reg] an integer or bit-size type register reg containing a byte address. [reg+immOff] a sum of register reg containing a byte address plus a constant integer byte offset (signed, 32-bit). [var+immOff] a sum of address of addressable variable var containing a byte address plus a constant integer byte offset (signed, 32-bit). [immAddr] an immediate absolute byte address (unsigned, 32-bit). var[immOff] an array element as described in Arrays as Operands . The register containing an address may be declared as a bit-size type or integer type.'},\n",
       " {'id': 597,\n",
       "  'content': 'The address must be naturally aligned to a multiple of the access size. If an address is not properly aligned, the resulting behavior is undefined. For example, among other things, the access may proceed by silently masking off low-order address bits to achieve proper rounding, or the instruction may fault. The address size may be either 32-bit or 64-bit. 128-bit adresses are not supported. Addresses are zero-extended to the specified width as needed, and truncated if the register width exceeds the state space address width for the target architecture. Address arithmetic is performed using integer arithmetic and logical instructions. Examples include pointer arithmetic and pointer comparisons. All addresses and address computations are byte-based; there is no support for C-style pointer arithmetic. The mov instruction can be used to move the address of a variable into a pointer. The address is an offset in the state space in which the variable is declared. Load and store operations move data between registers and locations in addressable state spaces. The syntax is similar to that used in many assembly languages, where scalar variables are simply named and addresses are de-referenced by enclosing the address expression in square brackets. Address expressions include variable names, address registers, address register plus byte offset, and immediate address expressions which evaluate at compile-time to a constant address. Here are a few examples: .shared .u16 x; .reg .u16 r0; .global .v4 .f32 V; .reg .v4 .f32 W; .const .s32 tbl[256]; .reg .b32 p; .reg .s32 q; ld.shared.u16 r0,[x]; ld.global.v4.f32 W, [V]; ld.const.s32 q, [tbl+12]; mov.u32 p, tbl; 6.4.1.1. Generic Addressing \\uf0c1 If a memory instruction does not specify a state space, the operation is performed using generic addressing. The state spaces .const , Kernel Function Parameters ( .param ), .local and .shared are modeled as windows within the generic address space. Each window is defined by a window base and a window size that is equal to the size of the corresponding state space. A generic address maps to global memory unless it falls within the window for const , local , or shared memory. The Kernel Function Parameters ( .param ) window is contained within the .global window. Within each window, a generic address maps to an address in the underlying state space by subtracting the window base from the generic address. 6.4.2. Arrays as Operands \\uf0c1 Arrays of all types can be declared, and the identifier becomes an address constant in the space where the array is declared. The size of the array is a constant in the program. Array elements can be accessed using an explicitly calculated byte address, or by indexing into the array using square-bracket notation. The expression within square brackets is either a constant integer, a register variable, or a simple register with constant offset expression, where the offset is a constant expression that is either added or subtracted from a register variable. If more complicated indexing is desired, it must be written as an address calculation prior to use. Examples are: ld.global.u32 s, a[0]; ld.global.u32 s, a[N-1]; mov.u32 s, a[1]; // move address of a[1] into s 6.4.3. Vectors as Operands \\uf0c1 Vector operands are supported by a limited subset of instructions, which include mov , ld , st , atom , red and tex . Vectors may also be passed as arguments to called functions. Vector elements can be extracted from the vector with the suffixes .x , .y , .z and .w , as well as the typical color fields .r , .g , .b and .a . A brace-enclosed list is used for pattern matching to pull apart vectors. .reg .v4 .f32 V; .reg .f32 a, b, c, d; mov.v4.f32 {a,b,c,d}, V; Vector loads and stores can be used to implement wide loads and stores, which may improve memory performance. The registers in the load/store operations can be a vector, or a brace-enclosed list of similarly typed scalars. Here are examples: ld.global.v4.f32 {a,b,c,d}, [addr+16]; ld.global.v2.u32 V2, [addr+8]; Elements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows: Ra = V.x = V.r Rb = V.y = V.g Rc = V.z = V.b Rd = V.w = V.a 6.4.4. Labels and Function Names as Operands \\uf0c1 Labels and function names can be used only in bra / brx.idx and call instructions respectively. Function names can be used in mov instruction to get the address of the function into a register, for use in an indirect call. Beginning in PTX ISA version 3.1, the mov instruction may be used to take the address of kernel functions, to be passed to a system call that initiates a kernel launch from the GPU. This feature is part of the support for CUDA Dynamic Parallelism.'},\n",
       " {'id': 598,\n",
       "  'content': '6.5. Type Conversion \\uf0c1 All operands to all arithmetic, logic, and data movement instruction must be of the same type and size, except for operations where changing the size and/or type is part of the definition of the instruction. Operands of different sizes or types must be converted prior to the operation. 6.5.1. Scalar Conversions \\uf0c1 Table 13 shows what precision and format the cvt instruction uses given operands of differing types. For example, if a cvt.s32.u16 instruction is given a u16 source operand and s32 as a destination operand, the u16 is zero-extended to s32 . Conversions to floating-point that are beyond the range of floating-point numbers are represented with the maximum floating-point value (IEEE 754 Inf for f32 and f64 , and ~131,000 for f16 ). Table 13 Convert Instruction Precision and Format \\uf0c1 Destination Format s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Source Format s8 – sext sext sext – sext sext sext s2f s2f s2f s16 chop 1 – sext sext chop 1 – sext sext s2f s2f s2f s32 chop 1 chop 1 – sext chop 1 chop 1 – sext s2f s2f s2f s64 chop 1 chop 1 chop – chop 1 chop 1 chop – s2f s2f s2f u8 – zext zext zext – zext zext zext u2f u2f u2f u16 chop 1 – zext zext chop 1 – zext zext u2f u2f u2f u32 chop 1 chop 1 – zext chop 1 chop 1 – zext u2f u2f u2f u64 chop 1 chop 1 chop – chop 1 chop 1 chop – u2f u2f u2f f16 f2s f2s f2s f2s f2u f2u f2u f2u – f2f f2f f32 f2s f2s f2s f2s f2u f2u f2u f2u f2f – f2f f64 f2s f2s f2s f2s f2u f2u f2u f2u f2f f2f – Notes sext = sign-extend; zext = zero-extend; chop = keep only low bits that fit; s2f = signed-to-float; f2s = float-to-signed; u2f = unsigned-to-float; f2u = float-to-unsigned; f2f = float-to-float. 1 If the destination register is wider than the destination format, the result is extended to the destination register width after chopping.'},\n",
       " {'id': 599,\n",
       "  'content': 'The type of extension (sign or zero) is based on the destination format. For example, cvt.s16.u32 targeting a 32-bit register first chops to 16-bit, then sign-extends to 32-bit. 6.5.2.'},\n",
       " {'id': 600,\n",
       "  'content': 'Rounding Modifiers \\uf0c1 Conversion instructions may specify a rounding modifier. In PTX, there are four integer rounding modifiers and four floating-point rounding modifiers. Table 14 and Table 15 summarize the rounding modifiers. Table 14 Floating-Point Rounding Modifiers \\uf0c1 Modifier Description .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Table 15 Integer Rounding Modifiers \\uf0c1 Modifier Description .rni round to nearest integer, choosing even integer if source is equidistant between two integers. .rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity 6.6. Operand Costs \\uf0c1 Operands from different state spaces affect the speed of an operation.'},\n",
       " {'id': 601,\n",
       "  'content': 'Registers are fastest, while global memory is slowest. Much of the delay to memory can be hidden in a number of ways. The first is to have multiple threads of execution so that the hardware can issue a memory operation and then switch to other execution. Another way to hide latency is to issue the load instructions as early as possible, as execution is not blocked until the desired result is used in a subsequent (in time) instruction. The register in a store operation is available much more quickly. Table 16 gives estimates of the costs of using different kinds of memory. Table 16 Cost Estimates for Accessing State-Spaces \\uf0c1 Space Time Notes Register 0 Shared 0 Constant 0 Amortized cost is low, first access is high Local > 100 clocks Parameter 0 Immediate 0 Global > 100 clocks Texture > 100 clocks Surface > 100 clocks 7. Abstracting the ABI \\uf0c1 Rather than expose details of a particular calling convention, stack layout, and Application Binary Interface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI implementations. In this section, we describe the features of PTX needed to achieve this hiding of the ABI. These include syntax for function definitions, function calls, parameter passing, support for variadic functions ( varargs ), and memory allocated on the stack ( alloca ). Refer to PTX Writers Guide to Interoperability for details on generating PTX compliant with Application Binary Interface (ABI) for the CUDA ® architecture. 7.1. Function Declarations and Definitions \\uf0c1 In PTX, functions are declared and defined using the .func directive. A function declaration specifies an optional list of return parameters, the function name, and an optional list of input parameters; together these specify the function’s interface, or prototype. A function definition specifies both the interface and the body of the function. A function must be declared or defined prior to being called. The simplest function has no parameters or return values, and is represented in PTX as follows: .func foo { ... ret; } ... call foo; ... Here, execution of the call instruction transfers control to foo , implicitly saving the return address. Execution of the ret instruction within foo transfers control to the instruction following the call. Scalar and vector base-type input and return parameters may be represented simply as register variables. At the call, arguments may be register variables or constants, and return values may be placed directly into register variables. The arguments and return variables at the call must have type and size that match the callee’s corresponding formal parameters. Example .func (.reg .u32 %res) inc_ptr ( .reg .u32 %ptr, .reg .u32 %inc ) { add.u32 %res, %ptr, %inc; ret; } ... call (%r1), inc_ptr, (%r1,4); ... When using the ABI, .reg state space parameters must be at least 32-bits in size. Subword scalar objects in the source language should be promoted to 32-bit registers in PTX, or use .param state space byte arrays described next. Objects such as C structures and unions are flattened into registers or byte arrays in PTX and are represented using .param space memory. For example, consider the following C structure, passed by value to a function: struct { double dbl; char c[4]; }; In PTX, this structure will be flattened into a byte array. Since memory accesses are required to be aligned to a multiple of the access size, the structure in this example will be a 12 byte array with 8 byte alignment so that accesses to the .f64 field are aligned. The .param state space is used to pass the structure by value: Example .func (.reg .s32 out) bar (.reg .s32 x, .param .align 8 .b8 y[12]) { .reg .f64 f1; .reg .b32 c1, c2, c3, c4; ... ld.param.f64 f1, [y+0]; ld.param.b8 c1, [y+8]; ld.param.b8 c2, [y+9]; ld.param.b8 c3, [y+10]; ld.param.b8 c4, [y+11]; ... ... // computation using x,f1,c1,c2,c3,c4; } { .param .b8 .align 8 py[12]; ... st.param.b64 [py+ 0], %rd; st.param.b8 [py+ 8], %rc1; st.param.b8 [py+ 9], %rc2; st.param.b8 [py+10], %rc1; st.param.b8 [py+11], %rc2; // scalar args in .reg space, byte array in .param space call (%out), bar, (%x, py); ... In this example, note that .param space variables are used in two ways. First, a .param variable y is used in function definition bar to represent a formal parameter. Second, a .param variable py is declared in the body of the calling function and used to set up the structure being passed to bar. The following is a conceptual way to think about the .param state space use in device functions. For a caller, The .param state space is used to set values that will be passed to a called function and/or to receive return values from a called function. Typically, a .param byte array is used to collect together fields of a structure being passed by value. For a callee, The .param state space is used to receive parameter values and/or pass return values back to the caller. The following restrictions apply to parameter passing. For a caller, Arguments may be .param variables, .reg variables, or constants. In the case of .param space formal parameters that are byte arrays, the argument must also be a .param space byte array with matching type, size, and alignment. A .param argument must be declared within the local scope of the caller. In the case of .param space formal parameters that are base-type scalar or vector variables, the corresponding argument may be either a .param or .reg space variable with matching type and size, or a constant that can be represented in the type of the formal parameter. In the case of .reg space formal parameters, the corresponding argument may be either a .param or .reg space variable of matching type and size, or a constant that can be represented in the type of the formal parameter. In the case of .reg space formal parameters, the register must be at least 32-bits in size. All st.param instructions used for passing arguments to function call must immediately precede the corresponding call instruction and ld.param instruction used for collecting return value must immediately follow the call instruction without any control flow alteration. st.param and ld.param instructions used for argument passing cannot be predicated. This enables compiler optimization and ensures that the .param variable does not consume extra space in the caller’s frame beyond that needed by the ABI. The .param variable simply allows a mapping to be made at the call site between data that may be in multiple locations (e.g., structure being manipulated by caller is located in registers and memory) to something that can be passed as a parameter or return value to the callee. For a callee, Input and return parameters may be .param variables or .reg variables. Parameters in .param memory must be aligned to a multiple of 1, 2, 4, 8, or 16 bytes. Parameters in the .reg state space must be at least 32-bits in size. The .reg state space can be used to receive and return base-type scalar and vector values, including sub-word size objects when compiling in non-ABI mode. Supporting the .reg state space provides legacy support. Note that the choice of .reg or .param state space for parameter passing has no impact on whether the parameter is ultimately passed in physical registers or on the stack. The mapping of parameters to physical registers and stack locations depends on the ABI definition and the order, size, and alignment of parameters. 7.1.1. Changes from PTX ISA Version 1.x \\uf0c1 In PTX ISA version 1.x, formal parameters were restricted to .reg state space, and there was no support for array parameters. Objects such as C structures were flattened and passed or returned using multiple registers. PTX ISA version 1.x supports multiple return values for this purpose. Beginning with PTX ISA version 2.0, formal parameters may be in either .reg or .param state space, and .param space parameters support arrays. For targets sm_20 or higher, PTX restricts functions to a single return value, and a .param byte array should be used to return objects that do not fit into a register. PTX continues to support multiple return registers for sm_1x targets. Note PTX implements a stack-based ABI only for targets sm_20 or higher. PTX ISA versions prior to 3.0 permitted variables in .reg and .local state spaces to be defined at module scope. When compiling to use the ABI, PTX ISA version 3.0 and later disallows module-scoped .reg and .local variables and restricts their use to within function scope. When compiling without use of the ABI, module-scoped .reg and .local variables are supported as before. When compiling legacy PTX code (ISA versions prior to 3.0) containing module-scoped .reg or .local variables, the compiler silently disables use of the ABI. 7.2. Variadic Functions \\uf0c1 Note Support for variadic functions which was unimplemented has been removed from the spec. PTX version 6.0 supports passing unsized array parameter to a function which can be used to implement variadic functions. Refer to Kernel and Function Directives: .func for details 7.3. Alloca \\uf0c1 PTX provides alloca instruction for allocating storage at runtime on the per-thread local memory stack. The allocated stack memory can be accessed with ld.local and st.local instructions using the pointer returned by alloca . In order to facilitate deallocation of memory allocated with alloca , PTX provides two additional instructions: stacksave which allows reading the value of stack pointer in a local variable, and stackrestore which can restore the stack pointer with the saved value. alloca , stacksave , and stackrestore instructions are described in Stack Manipulation Instructions . Preview Feature: Stack manipulation instructions alloca , stacksave and stackrestore are preview features in PTX ISA version 7.3. All details are subject to change with no guarantees of backward compatibility on future PTX ISA versions or SM architectures. 8. Memory Consistency Model \\uf0c1 In multi-threaded executions, the side-effects of memory operations performed by each thread become visible to other threads in a partial and non-identical order. This means that any two operations may appear to happen in no order, or in different orders, to different threads. The axioms introduced by the memory consistency model specify exactly which contradictions are forbidden between the orders observed by different threads. In the absence of any constraint, each read operation returns the value committed by some write operation to the same memory location, including the initial write to that memory location. The memory consistency model effectively constrains the set of such candidate writes from which a read operation can return a value. 8.1. Scope and applicability of the model \\uf0c1 The constraints specified under this model apply to PTX programs with any PTX ISA version number, running on sm_70 or later architectures. The memory consistency model does not apply to texture (including ld.global.nc ) and surface accesses. 8.1.1. Limitations on atomicity at system scope \\uf0c1 When communicating with the host CPU, certain strong operations with system scope may not be performed atomically on some systems. For more details on atomicity guarantees to host memory, see the CUDA Atomicity Requirements . 8.2. Memory operations \\uf0c1 The fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits. Each state space available to a PTX program is a sequence of contiguous bytes in memory. Every byte in a PTX state space has a unique address relative to all threads that have access to the same state space. Each PTX memory instruction specifies an address operand and a data type. The address operand contains a virtual address that gets converted to a physical address during memory access. The physical address and the size of the data type together define a physical memory location, which is the range of bytes starting from the physical address and extending up to the size of the data type in bytes. The memory consistency model specification uses the terms “address” or “memory address” to indicate a virtual address, and the term “memory location” to indicate a physical memory location. Each PTX memory instruction also specifies the operation — either a read, a write or an atomic read-modify-write — to be performed on all the bytes in the corresponding memory location. 8.2.1. Overlap \\uf0c1 Two memory locations are said to overlap when the starting address of one location is within the range of bytes constituting the other location. Two memory operations are said to overlap when they specify the same virtual address and the corresponding memory locations overlap. The overlap is said to be complete when both memory locations are identical, and it is said to be partial otherwise. 8.2.2. Aliases \\uf0c1 Two distinct virtual addresses are said to be aliases if they map to the same memory location. 8.2.3. Multimem Addresses \\uf0c1 A multimem address is a virtual address which points to multiple distinct memory locations across devices. Only multimem. * operations are valid on multimem addresses. That is, the behavior of accessing a multimem address in any other memory operation is undefined. 8.2.4. Memory Operations on Vector Data Types \\uf0c1 The memory consistency model relates operations executed on memory locations with scalar data types, which have a maximum size and alignment of 64 bits. Memory operations with a vector data type are modelled as a set of equivalent memory operations with a scalar data type, executed in an unspecified order on the elements in the vector. 8.2.5. Memory Operations on Packed Data Types \\uf0c1 A packed data type consists of two values of the same scalar data type, as described in Packed Data Types . These values are accessed in adjacent memory locations. A memory operation on a packed data type is modelled as a pair of equivalent memory operations on the scalar data type, executed in an unspecified order on each element of the packed data. 8.2.6. Initialization \\uf0c1 Each byte in memory is initialized by a hypothetical write W0 executed before starting any thread in the program. If the byte is included in a program variable, and that variable has an initial value, then W0 writes the corresponding initial value for that byte; else W0 is assumed to have written an unknown but constant value to the byte. 8.3. State spaces \\uf0c1 The relations defined in the memory consistency model are independent of state spaces. In particular, causality order closes over all memory operations across all the state spaces. But the side-effect of a memory operation in one state space can be observed directly only by operations that also have access to the same state space. This further constrains the synchronizing effect of a memory operation in addition to scope. For example, the synchronizing effect of the PTX instruction ld.relaxed.shared.sys is identical to that of ld.relaxed.shared.cluster , since no thread outside the same cluster can execute an operation that accesses the same memory location. 8.4.'},\n",
       " {'id': 602,\n",
       "  'content': 'Operation types \\uf0c1 For simplicity, the rest of the document refers to the following operation types, instead of mentioning specific instructions that give rise to them. Table 17 Operation Types \\uf0c1 Operation Type Instruction/Operation atomic operation atom or red instruction. read operation All variants of ld instruction and atom instruction (but not red instruction). write operation All variants of st instruction, and atomic operations if they result in a write. memory operation A read or write operation. volatile operation An instruction with .volatile qualifier. acquire operation A memory operation with .acquire or .acq_rel qualifier. release operation A memory operation with .release or .acq_rel qualifier. mmio operation An ld or st instruction with .mmio qualifier. memory fence operation A membar , fence.sc or fence.acq_rel instruction. proxy fence operation A fence.proxy or a membar.proxy instruction. strong operation A memory fence operation, or a memory operation with a .relaxed , .acquire , .release , .acq_rel , .volatile , or .mmio qualifier. weak operation An ld or st instruction with a .weak qualifier. synchronizing operation A barrier instruction, fence operation, release operation or acquire operation.'},\n",
       " {'id': 603,\n",
       "  'content': '8.4.1. mmio Operation \\uf0c1 An mmio operation is a memory operation with .mmio qualifier specified. It is usually performed on a memory location which is mapped to the control registers of peer I/O devices. It can also be used for communication between threads but has poor performance relative to non- mmio operations. The semantic meaning of mmio operations cannot be defined precisely as it is defined by the underlying I/O device. For formal specification of semantics of mmio operation from Memory Consistency Model perspective, it is equivalent to the semantics of a strong operation. But it follows a few implementation-specific properties, if it meets the CUDA atomicity requirements at the specified scope: Writes are always performed and are never combined within the scope specified. Reads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any cache within the scope specified. As an exception, in some implementations, the surrounding locations may also be loaded. In such cases the amount of data loaded is implementation specific and varies between 32 and 128 bytes in size. 8.5. Scope \\uf0c1 Each strong operation must specify a scope , which is the set of threads that may interact directly with that operation and establish any of the relations described in the memory consistency model. There are four scopes: Table 18 Scopes \\uf0c1 Scope Description .cta The set of all threads executing in the same CTA as the current thread. .cluster The set of all threads executing in the same cluster as the current thread. .gpu The set of all threads in the current program executing on the same compute device as the current thread. This also includes other kernel grids invoked by the host program on the same compute device. .sys The set of all threads in the current program, including all kernel grids invoked by the host program on all compute devices, and all threads constituting the host program itself. Note that the warp is not a scope ; the CTA is the smallest collection of threads that qualifies as a scope in the memory consistency model. 8.6. Proxies \\uf0c1 A memory proxy , or a proxy is an abstract label applied to a method of memory access. When two memory operations use distinct methods of memory access, they are said to be different proxies . Memory operations as defined in Operation types use generic method of memory access, i.e. a generic proxy . Other operations such as textures and surfaces all use distinct methods of memory access, also distinct from the generic method. A proxy fence is required to synchronize memory operations across different proxies . Although virtual aliases use the generic method of memory access, since using distinct virtual addresses behaves as if using different proxies , they require a proxy fence to establish memory ordering. 8.7. Morally strong operations \\uf0c1 Two operations are said to be morally strong relative to each other if they satisfy all of the following conditions: The operations are related in program order (i.e, they are both executed by the same thread), or each operation is strong and specifies a scope that includes the thread executing the other operation. Both operations are performed via the same proxy . If both are memory operations, then they overlap completely. Most (but not all) of the axioms in the memory consistency model depend on relations between morally strong operations. 8.7.1. Conflict and Data-races \\uf0c1 Two overlapping memory operations are said to conflict when at least one of them is a write . Two conflicting memory operations are said to be in a data-race if they are not related in causality order and they are not morally strong . 8.7.2. Limitations on Mixed-size Data-races \\uf0c1 A data-race between operations that overlap completely is called a uniform-size data-race , while a data-race between operations that overlap partially is called a mixed-size data-race . The axioms in the memory consistency model do not apply if a PTX program contains one or more mixed-size data-races . But these axioms are sufficient to describe the behavior of a PTX program with only uniform-size data-races . Atomicity of mixed-size RMW operations In any program with or without mixed-size data-races , the following property holds for every pair of overlapping atomic operations A1 and A2 such that each specifies a scope that includes the other: Either the read-modify-write operation specified by A1 is performed completely before A2 is initiated, or vice versa. This property holds irrespective of whether the two operations A1 and A2 overlap partially or completely. 8.8. Release and Acquire Patterns \\uf0c1 Some sequences of instructions give rise to patterns that participate in memory synchronization as described later. The release pattern makes prior operations from the current thread 1 visible to some operations from other threads. The acquire pattern makes some operations from other threads visible to later operations from the current thread. A release pattern on a location M consists of one of the following: A release operation on M E.g. : st.release [M]; or atom.acq_rel [M]; or mbarrier.arrive.release [M]; Or a release operation on M followed by a strong write on M in program order E.g. : st.release [M] ; st.relaxed [M]; Or a memory fence followed by a strong write on M in program order E.g. : fence; st.relaxed [M]; Any memory synchronization established by a release pattern only affects operations occurring in program order before the first instruction in that pattern. An acquire pattern on a location M consists of one of the following: An acquire operation on M E.g. : ld.acquire [M]; or atom.acq_rel [M]; or mbarrier.test_wait.acquire [M]; Or a strong read on M followed by an acquire operation on M in program order E.g. : ld.relaxed [M]; ld.acquire [M]; Or a strong read on M followed by a memory fence in program order E.g. : ld.relaxed [M]; fence; Any memory synchronization established by an acquire pattern only affects operations occurring in program order after the last instruction in that pattern. 1 For both release and acquire patterns, this effect is further extended to operations in other threads through the transitive nature of causality order . 8.9. Ordering of memory operations \\uf0c1 The sequence of operations performed by each thread is captured as program order while memory synchronization across threads is captured as causality order . The visibility of the side-effects of memory operations to other memory operations is captured as communication order . The memory consistency model defines contradictions that are disallowed between communication order on the one hand, and causality order and program order on the other. 8.9.1. Program Order \\uf0c1 The program order relates all operations performed by a thread to the order in which a sequential processor will execute instructions in the corresponding PTX source. It is a transitive relation that forms a total order over the operations performed by the thread, but does not relate operations from different threads. 8.9.1.1. Asynchronous Operations \\uf0c1 Some PTX instructions (all variants of cp.async , cp.async.bulk , cp.reduce.async.bulk , wgmma.mma_async ) perform operations that are asynchronous to the thread that executed the instruction. These asynchronous operations are ordered after prior instructions in the same thread (except in the case of wgmma.mma_async ), but they are not part of the program order for that thread. Instead, they provide weaker ordering guarantees as documented in the instruction description. For example, the loads and stores performed as part of a cp.async are ordered with respect to each other, but not to those of any other cp.async instructions initiated by the same thread, nor any other instruction subsequently issued by the thread with the exception of cp.async.commit_group or cp.async.mbarrier.arrive . The asynchronous mbarrier arrive-on operation performed by a cp.async.mbarrier.arrive instruction is ordered with respect to the memory operations performed by all prior cp.async operations initiated by the same thread, but not to those of any other instruction issued by the thread. The implicit mbarrier complete-tx operation that is part of all variants of cp.async.bulk and cp.reduce.async.bulk instructions is ordered only with respect to the memory operations performed by the same asynchronous instruction, and in particular it does not transitively establish ordering with respect to prior instructions from the issuing thread. 8.9.2. Observation Order \\uf0c1 Observation order relates a write W to a read R through an optional sequence of atomic read-modify-write operations. A write W precedes a read R in observation order if: R and W are morally strong and R reads the value written by W, or For some atomic operation Z, W precedes Z and Z precedes R in observation order . 8.9.3. Fence-SC Order \\uf0c1 The Fence-SC order is an acyclic partial order, determined at runtime, that relates every pair of morally strong fence.sc operations. 8.9.4.'},\n",
       " {'id': 604,\n",
       "  'content': 'Memory synchronization \\uf0c1 Synchronizing operations performed by different threads synchronize with each other at runtime as described here. The effect of such synchronization is to establish causality order across threads. A fence.sc operation X synchronizes with a fence.sc operation Y if X precedes Y in the Fence-SC order. A bar{.cta}.sync or bar{.cta}.red or bar{.cta}.arrive operation synchronizes with a bar{.cta}.sync or bar{.cta}.red operation executed on the same barrier. A barrier.cluster.arrive operation synchronizes with a barrier.cluster.wait operation. A release pattern X synchronizes with an acquire pattern Y, if a write operation in X precedes a read operation in Y in observation order , and the first operation in X and the last operation in Y are morally strong . API synchronization A synchronizes relation can also be established by certain CUDA APIs. Completion of a task enqueued in a CUDA stream synchronizes with the start of the following task in the same stream, if any. For purposes of the above, recording or waiting on a CUDA event in a stream, or causing a cross-stream barrier to be inserted due to cudaStreamLegacy , enqueues tasks in the associated streams even if there are no direct side effects. An event record task synchronizes with matching event wait tasks, and a barrier arrival task synchronizes with matching barrier wait tasks. Start of a CUDA kernel synchronizes with start of all threads in the kernel. End of all threads in a kernel synchronize with end of the kernel. Start of a CUDA graph synchronizes with start of all source nodes in the graph. Completion of all sink nodes in a CUDA graph synchronizes with completion of the graph. Completion of a graph node synchronizes with start of all nodes with a direct dependency. Start of a CUDA API call to enqueue a task synchronizes with start of the task. Completion of the last task queued to a stream, if any, synchronizes with return from cudaStreamSynchronize . Completion of the most recently queued matching event record task, if any, synchronizes with return from cudaEventSynchronize . Synchronizing a CUDA device or context behaves as if synchronizing all streams in the context, including ones that have been destroyed. Returning cudaSuccess from an API to query a CUDA handle, such as a stream or event, behaves the same as return from the matching synchronization API. In addition to establishing a synchronizes relation, the CUDA API synchronization mechanisms above also participate in proxy-preserved base causality order . 8.9.5. Causality Order \\uf0c1 Causality order captures how memory operations become visible across threads through synchronizing operations. The axiom “Causality” uses this order to constrain the set of write operations from which a read operation may read a value. Relations in the causality order primarily consist of relations in Base causality order 1 , which is a transitive order, determined at runtime. Base causality order An operation X precedes an operation Y in base causality order if: X precedes Y in program order , or X synchronizes with Y, or For some operation Z, X precedes Z in program order and Z precedes Y in base causality order , or X precedes Z in base causality order and Z precedes Y in program order , or X precedes Z in base causality order and Z precedes Y in base causality order . Proxy-preserved base causality order A memory operation X precedes a memory operation Y in proxy-preserved base causality order if X precedes Y in base causality order , and: X and Y are performed to the same address, using the generic proxy , or X and Y are performed to the same address, using the same proxy , and by the same thread block, or X and Y are aliases and there is an alias proxy fence along the base causality path from X to Y. Causality order Causality order combines base causality order with some non-transitive relations as follows: An operation X precedes an operation Y in causality order if: X precedes Y in proxy-preserved base causality order , or For some operation Z, X precedes Z in observation order, and Z precedes Y in proxy-preserved base causality order . 1 The transitivity of base causality order accounts for the “cumulativity” of synchronizing operations. 8.9.6. Coherence Order \\uf0c1 There exists a partial transitive order that relates overlapping write operations, determined at runtime, called the coherence order 1 . Two overlapping write operations are related in coherence order if they are morally strong or if they are related in causality order . Two overlapping writes are unrelated in coherence order if they are in a data-race , which gives rise to the partial nature of coherence order . 1 Coherence order cannot be observed directly since it consists entirely of write operations. It may be observed indirectly by its use in constraining the set of candidate writes that a read operation may read from. 8.9.7. Communication Order \\uf0c1 The communication order is a non-transitive order, determined at runtime, that relates write operations to other overlapping memory operations. A write W precedes an overlapping read R in communication order if R returns the value of any byte that was written by W. A write W precedes a write W’ in communication order if W precedes W’ in coherence order . A read R precedes an overlapping write W in communication order if, for any byte accessed by both R and W, R returns the value written by a write W’ that precedes W in coherence order . Communication order captures the visibility of memory operations — when a memory operation X1 precedes a memory operation X2 in communication order , X1 is said to be visible to X2. 8.10.'},\n",
       " {'id': 605,\n",
       "  'content': 'Axioms \\uf0c1 8.10.1. Coherence \\uf0c1 If a write W precedes an overlapping write W’ in causality order , then W must precede W’ in coherence order . 8.10.2. Fence-SC \\uf0c1 Fence-SC order cannot contradict causality order . For a pair of morally strong fence.sc operations F1 and F2, if F1 precedes F2 in causality order , then F1 must precede F2 in Fence-SC order. 8.10.3. Atomicity \\uf0c1 Single-Copy Atomicity Conflicting morally strong operations are performed with single-copy atomicity . When a read R and a write W are morally strong , then the following two communications cannot both exist in the same execution, for the set of bytes accessed by both R and W: R reads any byte from W. R reads any byte from any write W’ which precedes W in coherence order . Atomicity of read-modify-write (RMW) operations When an atomic operation A and a write W overlap and are morally strong , then the following two communications cannot both exist in the same execution, for the set of bytes accessed by both A and W: A reads any byte from a write W’ that precedes W in coherence order . A follows W in coherence order .'},\n",
       " {'id': 606, 'content': 'Litmus Test 1: .'},\n",
       " {'id': 607,\n",
       "  'content': 'global . u32 x = 0 ; T1 T2 A1 : atom . sys . inc . u32 % r0 , [ x ]; A2 : atom . u32 % r0 , [ x ]; FINAL STATE : x == 2 Atomicity is guaranteed when the operations are morally strong . Litmus Test 2: . u32 x = 0 ; T1 T2 (In a different CTA) A1 : atom . cta . gpu . u32 % r0 , [ x ]; FINAL STATE : x == 1 OR x == 2 Atomicity is not guaranteed if the operations are not morally strong . 8.10.4. No Thin Air \\uf0c1 Values may not appear “out of thin air”: an execution cannot speculatively produce a value in such a way that the speculation becomes self-satisfying through chains of instruction dependencies and inter-thread communication. This matches both programmer intuition and hardware reality, but is necessary to state explicitly when performing formal analysis. Litmus Test: Load Buffering . u32 x = 0 ; . u32 y = 0 ; T1 T2 A1 : ld . u32 % r0 , [ x ]; B1 : st . u32 [ y ], % r0 ; A2 : ld . u32 % r1 , [ y ]; B2 : st . u32 [ x ], % r1 ; FINAL STATE : x == 0 AND y == 0 The litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of thin air. Two threads T1 and T2 each read from a first variable and copy the observed result into a second variable, with the first and second variable exchanged between the threads. If each variable is initially zero, the final result shall also be zero. If A1 reads from B2 and A2 reads from B1, then values passing through the memory operations in this example form a cycle: A1->B1->A2->B2->A1. Only the values x == 0 and y == 0 are allowed to satisfy this cycle. If any of the memory operations in this example were to speculatively associate a different value with the corresponding memory location, then such a speculation would become self-fulfilling, and hence forbidden. 8.10.5. Sequential Consistency Per Location \\uf0c1 Within any set of overlapping memory operations that are pairwise morally strong , communication order cannot contradict program order , i.e., a concatenation of program order between overlapping operations and morally strong relations in communication order cannot result in a cycle. This ensures that each program slice of overlapping pairwise morally strong operations is strictly sequentially-consistent . Litmus Test: CoRR .'},\n",
       " {'id': 608,\n",
       "  'content': 'u32 x = 0 ; T1 T2 W1 : st . relaxed . u32 [ x ], 1 ; R1 : ld . u32 % r0 , [ x ]; R2 : ld . u32 % r1 , [ x ]; IF % r0 == 1 THEN % r1 == 1 The litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee. A thread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence of) reads R1 and R2 on the same location x. No other writes are executed on x, except the one modelling the initial value. The operations W1, R1 and R2 are pairwise morally strong . If R1 reads from W1, then the subsequent read R2 must also observe the same value. If R2 observed the initial value of x instead, then this would form a sequence of morally-strong relations R2->W1->R1 in communication order that contradicts the program order R1->R2 in thread T2. Hence R2 cannot read the initial value of x in such an execution. 8.10.6. Causality \\uf0c1 Relations in communication order cannot contradict causality order . This constrains the set of candidate write operations that a read operation may read from: If a read R precedes an overlapping write W in causality order , then R cannot read from W. If a write W precedes an overlapping read R in causality order , then for any byte accessed by both R and W, R cannot read from any write W’ that precedes W in coherence order . Litmus Test: Message Passing .'},\n",
       " {'id': 609,\n",
       "  'content': 'u32 data = 0 ; . u32 flag = 0 ; T1 T2 W1 : st . u32 [ data ], 1 ; F1 : fence . sys ; W2 : st . u32 [ flag ], 1 ; R1 : ld . u32 % r0 , [ flag ]; F2 : fence . sys ; R2 : ld . u32 % r1 , [ data ]; IF % r0 == 1 THEN % r1 == 1 The litmus test known as “MP” (Message Passing) represents the essence of typical synchronization algorithms. A vast majority of useful programs can be reduced to sequenced applications of this pattern. Thread T1 first writes to a data variable and then to a flag variable while a second thread T2 first reads from the flag variable and then from the data variable. The operations on the flag are morally strong and the memory operations in each thread are separated by a fence , and these fences are morally strong . If R1 observes W2, then the release pattern “F1; W2” synchronizes with the acquire pattern “R1; F2”. This establishes the causality order W1 -> F1 -> W2 -> R1 -> F2 -> R2. Then axiom causality guarantees that R2 cannot read from any write that precedes W1 in coherence order . In the absence of any other writes in this example, R2 must read from W1. Litmus Test: CoWR // These addresses are aliases . u32 data_alias_1 ; . u32 data_alias_2 ; T1 W1 : st . u32 [ data_alias_1 ], 1 ; F1 : fence . proxy . alias ; R1 : ld . u32 % r1 , [ data_alias_2 ]; % r1 == 1 Virtual aliases require an alias proxy fence along the synchronization path. Litmus Test: Store Buffering The litmus test known as “SB” (Store Buffering) demonstrates the sequential consistency enforced by the fence.sc . A thread T1 writes to a first variable, and then reads the value of a second variable, while a second thread T2 writes to the second variable and then reads the value of the first variable. The memory operations in each thread are separated by fence. sc instructions, and these fences are morally strong .'},\n",
       " {'id': 610,\n",
       "  'content': '. u32 y = 0 ; T1 T2 W1 : st . u32 [ x ], 1 ; F1 : fence . sc . sys ; R1 : ld . u32 % r0 , [ y ]; W2 : st . u32 [ y ], 1 ; F2 : fence . u32 % r1 , [ x ]; % r0 == 1 OR % r1 == 1 In any execution, either F1 precedes F2 in Fence-SC order, or vice versa. If F1 precedes F2 in Fence-SC order, then F1 synchronizes with F2. This establishes the causality order in W1 -> F1 -> F2 -> R2. Axiom causality ensures that R2 cannot read from any write that precedes W1 in coherence order . In the absence of any other write to that variable, R2 must read from W1. Similarly, in the case where F2 precedes F1 in Fence-SC order, R1 must read from W2. If each fence.sc in this example were replaced by a fence.acq_rel instruction, then this outcome is not guaranteed. There may be an execution where the write from each thread remains unobserved from the other thread, i.e., an execution is possible, where both R1 and R2 return the initial value “0” for variables y and x respectively. 9.'},\n",
       " {'id': 611,\n",
       "  'content': \"Instruction Set \\uf0c1 9.1. Format and Semantics of Instruction Descriptions \\uf0c1 This section describes each PTX instruction. In addition to the name and the format of the instruction, the semantics are described, followed by some examples that attempt to show several possible instantiations of the instruction. 9.2. PTX Instructions \\uf0c1 PTX instructions generally have from zero to four operands, plus an optional guard predicate appearing after an @ symbol to the left of the opcode : @p opcode; @p opcode a; @p opcode d, a; @p opcode d, a, b; @p opcode d, a, b, c; For instructions that create a result value, the d operand is the destination operand, while a , b , and c are source operands. The setp instruction writes two destination registers. We use a | symbol to separate multiple destination registers. setp.lt.s32 p|q, a, b; // p = (a b gt hi n/a a >= b ge hs n/a 9.3.1.2. Floating Point Comparisons \\uf0c1 The ordered floating-point comparisons are eq , ne , lt , le , gt , and ge . If either operand is NaN , the result is False . Table 20 lists the floating-point comparison operators. Table 20 Floating-Point Comparison Operators \\uf0c1 Meaning Floating-Point Operator a == b && !isNaN(a) && !isNaN(b) eq a != b && !isNaN(a) && !isNaN(b) ne a b && !isNaN(a) && !isNaN(b) gt a >= b && !isNaN(a) && !isNaN(b) ge To aid comparison operations in the presence of NaN values, unordered floating-point comparisons are provided: equ , neu , ltu , leu , gtu , and geu . If both operands are numeric values (not NaN ), then the comparison has the same result as its ordered counterpart. If either operand is NaN , then the result of the comparison is True . Table 21 lists the floating-point comparison operators accepting NaN values. Table 21 Floating-Point Comparison Operators Accepting NaN \\uf0c1 Meaning Floating-Point Operator a == b || isNaN(a) || isNaN(b) equ a != b || isNaN(a) || isNaN(b) neu a b || isNaN(a) || isNaN(b) gtu a >= b || isNaN(a) || isNaN(b) geu To test for NaN values, two operators num ( numeric ) and nan ( isNaN ) are provided. num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN . Table 22 lists the floating-point comparison operators testing for NaN values. Table 22 Floating-Point Comparison Operators Testing for NaN \\uf0c1 Meaning Floating-Point Operator !isNaN(a) && !isNaN(b) num isNaN(a) || isNaN(b) nan 9.3.2. Manipulating Predicates \\uf0c1 Predicate values may be computed and manipulated using the following instructions: and , or , xor , not , and mov . There is no direct conversion between predicates and integer values, and no direct way to load or store predicate register values. However, setp can be used to generate a predicate from an integer, and the predicate-based select ( selp ) instruction can be used to generate an integer value based on the value of a predicate; for example: selp.u32 %r1,1,0,%p; // convert predicate to 32-bit value 9.4. Type Information for Instructions and Operands \\uf0c1 Typed instructions must have a type-size modifier. For example, the add instruction requires type and size information to properly perform the addition operation (signed, unsigned, float, different sizes), and this information must be specified as a suffix to the opcode. Example .reg .u16 d, a, b; add.u16 d, a, b; // perform a 16-bit unsigned add Some instructions require multiple type-size modifiers, most notably the data conversion instruction cvt . It requires separate type-size modifiers for the result and source, and these are placed in the same order as the operands. For example: .reg .u16 a; .reg .f32 d; cvt.f32.u16 d, a; // convert 16-bit unsigned to 32-bit float In general, an operand’s type must agree with the corresponding instruction-type modifier. The rules for operand and instruction type conformance are as follows: Bit-size types agree with any type of the same size. Signed and unsigned integer types agree provided they have the same size, and integer operands are silently cast to the instruction type if needed. For example, an unsigned integer operand used in a signed integer instruction will be treated as a signed integer by the instruction. Floating-point types agree only if they have the same size; i.e., they must match exactly. Table 23 summarizes these type checking rules. Table 23 Type Checking Rules \\uf0c1 Operand Type .bX .sX .uX .fX Instruction Type .bX okay okay okay okay .sX okay okay okay invalid .uX okay okay okay invalid .fX okay invalid invalid okay Note Some operands have their type and size defined independently from the instruction type-size. For example, the shift amount operand for left and right shift instructions always has type .u32 , while the remaining operands have their type and size determined by the instruction type. Example // 64-bit arithmetic right shift; shift amount 'b' is .u32 shr.s64 d,a,b; 9.4.1. Operand Size Exceeding Instruction-Type Size \\uf0c1 For convenience, ld , st , and cvt instructions permit source and destination data operands to be wider than the instruction-type size, so that narrow values may be loaded, stored, and converted using regular-width registers. For example, 8-bit or 16-bit values may be held directly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and sizes. The operand type checking rules are relaxed for bit-size and integer (signed and unsigned) instruction types; floating-point instruction types still require that the operand type-size matches exactly, unless the operand is of bit-size type. When a source operand has a size that exceeds the instruction-type size, the source data is truncated (chopped) to the appropriate number of bits specified by the instruction type-size. Table 24 summarizes the relaxed type-checking rules for source operands. Note that some combinations may still be invalid for a particular instruction; for example, the cvt instruction does not support .bX instruction types, so those rows are invalid for cvt . Table 24 Relaxed Type-checking Rules for Source Operands \\uf0c1 Source Operand Type b8 b16 b32 b64 b128 s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Instruction Type b8 – chop chop chop chop – chop chop chop – chop chop chop chop chop chop b16 inv – chop chop chop inv – chop chop inv – chop chop – chop chop b32 inv inv – chop chop inv inv – chop inv inv – chop inv – chop b64 inv inv inv – chop inv inv inv – inv inv inv – inv inv – b128 inv inv inv inv – inv inv inv inv inv inv inv inv inv inv inv s8 – chop chop chop chop – chop chop chop – chop chop chop inv inv inv s16 inv – chop chop chop inv – chop chop inv – chop chop inv inv inv s32 inv inv – chop chop inv inv – chop inv inv – chop inv inv inv s64 inv inv inv – chop inv inv inv – inv inv inv – inv inv inv u8 – chop chop chop chop – chop chop chop – chop chop chop inv inv inv u16 inv – chop chop chop inv – chop chop inv – chop chop inv inv inv u32 inv inv – chop chop inv inv – chop inv inv – chop inv inv inv u64 inv inv inv – chop inv inv inv – inv inv inv – inv inv inv f16 inv – chop chop chop inv inv inv inv inv inv inv inv – inv inv f32 inv inv – chop chop inv inv inv inv inv inv inv inv inv – inv f64 inv inv inv – chop inv inv inv inv inv inv inv inv inv inv – Notes chop = keep only low bits that fit; “–” = allowed, but no conversion needed; inv = invalid, parse error. Source register size must be of equal or greater size than the instruction-type size.\"},\n",
       " {'id': 612,\n",
       "  'content': 'Bit-size source registers may be used with any appropriately-sized instruction type. The data are truncated (“chopped”) to the instruction-type size and interpreted according to the instruction type. Integer source registers may be used with any appropriately-sized bit-size or integer instruction type. The data are truncated to the instruction-type size and interpreted according to the instruction type. Floating-point source registers can only be used with bit-size or floating-point instruction types. When used with a narrower bit-size instruction type, the data are truncated. When used with a floating-point instruction type, the size must match exactly. When a destination operand has a size that exceeds the instruction-type size, the destination data is zero- or sign-extended to the size of the destination register. If the corresponding instruction type is signed integer, the data is sign-extended; otherwise, the data is zero-extended. Table 25 summarizes the relaxed type-checking rules for destination operands. Table 25 Relaxed Type-checking Rules for Destination Operands \\uf0c1 Destination Operand Type b8 b16 b32 b64 b128 s8 s16 s32 s64 u8 u16 u32 u64 f16 f32 f64 Instruction Type b8 – zext zext zext zext – zext zext zext – zext zext zext zext zext zext b16 inv – zext zext zext inv – zext zext inv – zext zext – zext zext b32 inv inv – zext zext inv inv – zext inv inv – zext inv – zext b64 inv inv inv – zext inv inv inv – inv inv inv – inv inv – b128 inv inv inv inv – inv inv inv inv inv inv inv inv inv inv inv s8 – sext sext sext sext – sext sext sext – sext sext sext inv inv inv s16 inv – sext sext sext inv – sext sext inv – sext sext inv inv inv s32 inv inv – sext sext inv inv – sext inv inv – sext inv inv inv s64 inv inv inv – sext inv inv inv – inv inv inv – inv inv inv u8 – zext zext zext zext – zext zext zext – zext zext zext inv inv inv u16 inv – zext zext zext inv – zext zext inv – zext zext inv inv inv u32 inv inv – zext zext inv inv – zext inv inv – zext inv inv inv u64 inv inv inv – zext inv inv inv – inv inv inv – inv inv inv f16 inv – zext zext zext inv inv inv inv inv inv inv inv – inv inv f32 inv inv – zext zext inv inv inv inv inv inv inv inv inv – inv f64 inv inv inv – zext inv inv inv inv inv inv inv inv inv inv – Notes sext = sign-extend; zext = zero-extend; “–” = allowed, but no conversion needed; inv = invalid, parse error. Destination register size must be of equal or greater size than the instruction-type size.'},\n",
       " {'id': 613,\n",
       "  'content': 'Bit-size destination registers may be used with any appropriately-sized instruction type. The data are sign-extended to the destination register width for signed integer instruction types, and are zero-extended to the destination register width otherwise. Integer destination registers may be used with any appropriately-sized bit-size or integer instruction type. The data are sign-extended to the destination register width for signed integer instruction types, and are zero-extended to the destination register width for bit-size an d unsigned integer instruction types. Floating-point destination registers can only be used with bit-size or floating-point instruction types. When used with a narrower bit-size instruction type, the data are zero-extended.'},\n",
       " {'id': 614,\n",
       "  'content': '9.5. Divergence of Threads in Control Constructs \\uf0c1 Threads in a CTA execute together, at least in appearance, until they come to a conditional control construct such as a conditional branch, conditional function call, or conditional return. If threads execute down different control flow paths, the threads are called divergent . If all of the threads act in unison and follow a single control flow path, the threads are called uniform . Both situations occur often in programs. A CTA with divergent threads may have lower performance than a CTA with uniformly executing threads, so it is important to have divergent threads re-converge as soon as possible. All control constructs are assumed to be divergent points unless the control-flow instruction is marked as uniform, using the .uni suffix. For divergent control flow, the optimizing code generator automatically determines points of re-convergence. Therefore, a compiler or code author targeting PTX can ignore the issue of divergent threads, but has the opportunity to improve performance by marking branch points as uniform when the compiler or author can guarantee that the branch point is non-divergent. 9.6. Semantics \\uf0c1 The goal of the semantic description of an instruction is to describe the results in all cases in as simple language as possible. The semantics are described using C, until C is not expressive enough.'},\n",
       " {'id': 615,\n",
       "  'content': '9.6.1. Machine-Specific Semantics of 16-bit Code \\uf0c1 A PTX program may execute on a GPU with either a 16-bit or a 32-bit data path. When executing on a 32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit computations are promoted to 32-bit computations. This can lead to computational differences between code run on a 16-bit machine versus the same code run on a 32-bit machine, since the promoted computation may have bits in the high-order half-word of registers that are not present in 16-bit physical registers. These extra precision bits can become visible at the application level, for example, by a right-shift instruction. At the PTX language level, one solution would be to define semantics for 16-bit code that is consistent with execution on a 16-bit data path. This approach introduces a performance penalty for 16-bit code executing on a 32-bit data path, since the translated code would require many additional masking instructions to suppress extra precision bits in the high-order half-word of 32-bit registers. Rather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of 16-bit instructions in PTX is machine-specific. A compiler or programmer may chose to enforce portable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at appropriate points in the program to guarantee portability of the code. However, for many performance-critical applications, this is not desirable, and for many applications the difference in execution is preferable to limiting performance.'},\n",
       " {'id': 616,\n",
       "  'content': '9.7. Instructions \\uf0c1 All PTX instructions may be predicated. In the following descriptions, the optional guard predicate is omitted from the syntax. 9.7.1. Integer Arithmetic Instructions \\uf0c1 Integer arithmetic instructions operate on the integer types in register and constant immediate forms. The integer arithmetic instructions are: add sub mul mad mul24 mad24 sad div rem abs neg min max popc clz bfind fns brev bfe bfi bmsk szext dp4a dp2a 9.7.1.1. Integer Arithmetic Instructions: add \\uf0c1 add Add two values. Syntax add.type d, a, b; add{.sat}.s32 d, a, b; // .sat applies only to .s32 .type = { .u16, .u32, .u64, .s16, .s32, .s64, .u16x2, .s16x2 }; Description Performs addition and writes the resulting value into a destination register. For .u16x2 , .s16x2 instruction types, forms input vectors by half word values from source operands. Half-word operands are then added in parallel to produce .u16x2 , .s16x2 result in destination. Operands d , a and b have type .type . For instruction types .u16x2 , .s16x2 , operands d , a and b have type .b32 . Semantics if (type == u16x2 || type == s16x2) { iA[0] = a[0:15]; iA[1] = a[16:31]; iB[0] = b[0:15]; iB[1] = b[16:31]; for (i = 0; i ; // for .hi variant d = t; // for .lo variant Notes The type of the operation represents the types of the a and b operands. If .hi or .lo is specified, then d is the same size as a and b , and either the upper or lower half of the result is written to the destination register. If .wide is specified, then d is twice as wide as a and b to receive the full result of the multiplication. The .wide suffix is supported only for 16- and 32-bit integer types. PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes Supported on all target architectures. Examples mul.wide.s16 fa,fxs,fys; // 16*16 bits yields 32 bits mul.lo.s16 fa,fxs,fys; // 16*16 bits, save only the low 16 bits mul.wide.s32 z,x,y; // 32*32 bits, creates 64 bit result 9.7.1.4. Integer Arithmetic Instructions: mad \\uf0c1 mad Multiply two values, optionally extract the high or low half of the intermediate result, and add a third value. Syntax mad.mode.type d, a, b, c; mad.hi.sat.s32 d, a, b, c; .mode = { .hi, .lo, .wide }; .type = { .u16, .u32, .u64, .s16, .s32, .s64 }; Description Multiplies two values, optionally extracts the high or low half of the intermediate result, and adds a third value. Writes the result into a destination register. Semantics t = a * b; n = bitwidth of type; d = t + c; // for .wide d = t + c; // for .hi variant d = t + c; // for .lo variant Notes The type of the operation represents the types of the a and b operands. If .hi or .lo is specified, then d and c are the same size as a and b , and either the upper or lower half of the result is written to the destination register. If .wide is specified, then d and c are twice as wide as a and b to receive the result of the multiplication. The .wide suffix is supported only for 16-bit and 32-bit integer types. Saturation modifier: .sat limits result to MININT..MAXINT (no overflow) for the size of the operation. Applies only to .s32 type in .hi mode.'},\n",
       " {'id': 617,\n",
       "  'content': 'Examples @p mad.lo.s32 d,a,b,c; mad.lo.s32 r,p,q,r; 9.7.1.5. Integer Arithmetic Instructions: mul24 \\uf0c1 mul24 Multiply two 24-bit integer values. Syntax mul24.mode.type d, a, b; .mode = { .hi, .lo }; .type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and return either the high or low 32-bits of the 48-bit result. Semantics t = a * b; d = t; // for .hi variant d = t; // for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits. mul24.hi performs a 24x24-bit multiply and returns the high 32 bits of the 48-bit result. mul24.lo performs a 24x24-bit multiply and returns the low 32 bits of the 48-bit result. All operands are of the same type and size. mul24.hi may be less efficient on machines without hardware support for 24-bit multiply. Examples mul24.lo.s32 d,a,b; // low 32-bits of 24x24-bit signed multiply. 9.7.1.6. Integer Arithmetic Instructions: mad24 \\uf0c1 mad24 Multiply two 24-bit integer values and add a third value. Syntax mad24.mode.type d, a, b, c; mad24.hi.sat.s32 d, a, b, c; .mode = { .hi, .lo }; .type = { .u32, .s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and add a third, 32-bit value to either the high or low 32-bits of the 48-bit result. Return either the high or low 32-bits of the 48-bit result. Semantics t = a * b; d = t + c; // for .hi variant d = t + c; // for .lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i.e., 48-bits. mad24.hi performs a 24x24-bit multiply and adds the high 32 bits of the 48-bit result to a third value. mad24.lo performs a 24x24-bit multiply and adds the low 32 bits of the 48-bit result to a third value. Saturation modifier: .sat limits result of 32-bit signed addition to MININT..MAXINT (no overflow). mad24.hi may be less efficient on machines without hardware support for 24-bit multiply. Examples mad24.lo.s32 d,a,b,c; // low 32-bits of 24x24-bit signed multiply. 9.7.1.7. Integer Arithmetic Instructions: sad \\uf0c1 sad Sum of absolute differences. Syntax sad.type d, a, b, c; .type = { .u16, .u32, .u64, .s16, .s32, .s64 }; Description Adds the absolute value of a-b to c and writes the resulting value into d . Semantics d = c + ((a iB[i]) ? iA[i] : iB[i]; } } else { d = (a > b) ? a : b; // Integer (signed and unsigned) } Notes Signed and unsigned differ. Saturation modifier: max.relu. {s16x2, s32} clamps the result to 0 if negative. PTX ISA Notes Introduced in PTX ISA version 1.0. max.u16x2 , max{.relu}.s16x2 and max.relu.s32 introduced in PTX ISA version 8.0. max.u16x2 , max{.relu}.s16x2 and max.relu.s32 require sm_90 or higher. Examples max.u32 d,a,b; max.s32 q,q,0; max.relu.s16x2 t,t,u; 9.7.1.14. Integer Arithmetic Instructions: popc \\uf0c1 popc Population count. Syntax popc.type d, a; .type = { .b32, .b64 }; Description Count the number of one bits in a and place the resulting population count in 32-bit destination register d . Operand a has the instruction type and destination d has type .u32 . Semantics .u32 d = 0; while (a != 0) { if (a & 0x1) d++; a = a >> 1; } PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes popc requires sm_20 or higher. Examples popc.b32 d, a; popc.b64 cnt, X; // cnt is .u32 9.7.1.15. Integer Arithmetic Instructions: clz \\uf0c1 clz Count leading zeros. Syntax clz.type d, a; .type = { .b32, .b64 }; Description Count the number of leading zeros in a starting with the most-significant bit and place the result in 32-bit destination register d . Operand a has the instruction type, and destination d has type .u32 . For .b32 type, the number of leading zeros is between 0 and 32, inclusively. For .b64 type, the number of leading zeros is between 0 and 64, inclusively. Semantics .u32 d = 0; if (.type == .b32) { max = 32; mask = 0x80000000; } else { max = 64; mask = 0x8000000000000000; } while (d =0; i--) { if (a & (1 0) ? 1 : -1; while ((pos >= 0) && (pos = 32 && .mode == .clamp) ? true : false; mask = too_large ? 0 : (~0) > sign_pos) & 1; } d = (a & ~mask) | (sign_bit ? mask | 0); PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes szext requires sm_70 or higher. Examples szext.clamp.s32 rd, ra, rb; szext.wrap.u32 rd, 0xffffffff, 0; // Result is 0. 9.7.1.22. Integer Arithmetic Instructions: bmsk \\uf0c1 bmsk Bit Field Mask. Syntax bmsk.mode.b32 d, a, b; .mode = { .clamp, .wrap }; Description Generates a 32-bit mask starting from the bit position specified in operand a , and of the width specified in operand b . The generated bitmask is stored in the destination operand d . The resulting bitmask is 0 in the following cases: When the value of a is 32 or higher and .mode is .clamp . When either the specified value of b or the wrapped value of b (when .mode is specified as .wrap ) is 0. Semantics a1 = a & 0x1f; mask0 = (~0) = 32 ? true : false; bit-position-overflow = false; bit-width-overflow = false; if (.mode == .clamp) { if (a >= 32) { bit-position-overflow = true; mask0 = 0; } if (b >= 32) { bit-width-overflow = true; } } if (sum-overflow || bit-position-overflow || bit-width-overflow) { mask1 = 0; } else if (b1 == 0) { mask1 = ~0; } d = mask0 & ~mask1; Notes The bitmask width specified by operand b is limited to range 0..32 in .clamp mode and to range 0..31 in .wrap mode. PTX ISA Notes Introduced in PTX ISA version 7.6. Target ISA Notes bmsk requires sm_70 or higher. Examples bmsk.clamp.b32 rd, ra, rb; bmsk.wrap.b32 rd, 1, 2; // Creates a bitmask of 0x00000006. 9.7.1.23. Integer Arithmetic Instructions: dp4a \\uf0c1 dp4a Four-way byte dot product-accumulate. Syntax dp4a.atype.btype d, a, b, c; .atype = .btype = { .u32, .s32 }; Description Four-way byte dot product which is accumulated in 32-bit result. Operand a and b are 32-bit inputs which hold 4 byte inputs in packed form for dot product. Operand c has type .u32 if both .atype and .btype are .u32 else operand c has type .s32 . Semantics d = c; // Extract 4 bytes from a 32bit input and sign or zero extend // based on input type. Va = extractAndSignOrZeroExt_4(a, .atype); Vb = extractAndSignOrZeroExt_4(b, .btype); for (i = 0; i + c; // for .hi variant d = t + c; // for .lo variant carry-out from addition is written to CC.CF Notes Generally used in combination with madc and addc to implement extended-precision multi-word multiplication. See madc for an example.'},\n",
       " {'id': 618,\n",
       "  'content': 'PTX ISA Notes 32-bit mad.cc introduced in PTX ISA version 3.0. 64-bit mad.cc introduced in PTX ISA version 4.3. Target ISA Notes Requires target sm_20 or higher. Examples @p mad.lo.cc.u32 d,a,b,c; mad.lo.cc.u32 r,p,q,r; 9.7.2.6. Extended-Precision Arithmetic Instructions: madc \\uf0c1 madc Multiply two values, extract high or low half of result, and add a third value with carry-in and optional carry-out. Syntax madc{.hi,.lo}{.cc}.type d, a, b, c; .type = { .u32, .s32, .u64, .s64 }; Description Multiplies two values, extracts either the high or low part of the result, and adds a third value along with carry-in. Writes the result to the destination register and optionally writes the carry-out from the addition into the condition code register. Semantics t = a * b; d = t + c + CC.CF; // for .hi variant d = t + c + CC.CF; // for .lo variant if .cc specified, carry-out from addition is written to CC.CF Notes Generally used in combination with mad.cc and addc to implement extended-precision multi-word multiplication. See example below.'},\n",
       " {'id': 619,\n",
       "  'content': 'PTX ISA Notes 32-bit madc introduced in PTX ISA version 3.0. 64-bit madc introduced in PTX ISA version 4.3. Examples // extended-precision multiply: [r3,r2,r1,r0] = [r5,r4] * [r7,r6] mul.lo.u32 r0,r4,r6; // r0=(r4*r6). [31:0], no carry-out mul.hi.u32 r1,r4,r6; // r1=(r4*r6). [63:32], no carry-out mad.lo.cc.u32 r1,r5,r6,r1; // r1+=(r5*r6). [31:0], may carry-out madc.hi.u32 r2,r5,r6,0; // r2 =(r5*r6). [63:32]+carry-in, // no carry-out mad.lo.cc.u32 r1,r4,r7,r1; // r1+=(r4*r7). [31:0], may carry-out madc.hi.cc.u32 r2,r4,r7,r2; // r2+=(r4*r7). [63:32]+carry-in, // may carry-out addc.u32 r3,0,0; // r3 = carry-in, no carry-out mad.lo.cc.u32 r2,r5,r7,r2; // r2+=(r5*r7). [31:0], may carry-out madc.hi.u32 r3,r5,r7,r3; // r3+=(r5*r7). [63:32]+carry-in 9.7.3. Floating-Point Instructions \\uf0c1 Floating-point instructions operate on .f32 and .f64 register operands and constant immediate values. The floating-point instructions are: testp copysign add sub mul fma mad div abs neg min max rcp sqrt rsqrt sin cos lg2 ex2 tanh Instructions that support rounding modifiers are IEEE-754 compliant. Double-precision instructions support subnormal inputs and results. Single-precision instructions support subnormal inputs and results by default for sm_20 and subsequent targets, and flush subnormal inputs and results to sign-preserving zero for sm_1x targets. The optional .ftz modifier on single-precision instructions provides backward compatibility with sm_1x targets by flushing subnormal inputs and results to sign-preserving zero regardless of the target architecture. Single-precision add , sub , mul , and mad support saturation of results to the range [0.0, 1.0], with NaN s being flushed to positive zero. NaN payloads are supported for double-precision instructions (except for rcp.approx.ftz.f64 and rsqrt.approx.ftz.f64 , which maps input NaN s to a canonical NaN ). Single-precision instructions return an unspecified NaN . Note that future implementations may support NaN payloads for single-precision instructions, so PTX programs should not rely on the specific single-precision NaN s being generated. Table 26 summarizes floating-point instructions in PTX. Table 26 Summary of Floating-Point Instructions \\uf0c1 Instruction .rn .rz .rm .rp .ftz .sat Notes {add,sub,mul}.rnd.f32 x x x x x x If no rounding modifier is specified, default is .rn and instructions may be folded into a multiply-add. {add,sub,mul}.rnd.f64 x x x x n/a n/a If no rounding modifier is specified, default is .rn and instructions may be folded into a multiply-add. mad.f32 n/a n/a n/a n/a x x .target sm_1x No rounding modifier. {mad,fma}.rnd.f32 x x x x x x .target sm_20 or higher mad.f32 and fma.f32 are the same. {mad,fma}.rnd.f64 x x x x n/a n/a mad.f64 and fma.f64 are the same. div.full.f32 n/a n/a n/a n/a x n/a No rounding modifier. {div,rcp,sqrt}.approx.f32 n/a n/a n/a n/a x n/a n/a rcp.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f32 x x x x x n/a .target sm_20 or higher {div,rcp,sqrt}.rnd.f64 x x x x n/a n/a .target sm_20 or higher {abs,neg,min,max}.f32 n/a n/a n/a n/a x n/a {abs,neg,min,max}.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.f32 n/a n/a n/a n/a x n/a rsqrt.approx.f64 n/a n/a n/a n/a n/a n/a rsqrt.approx.ftz.f64 n/a n/a n/a n/a x n/a .target sm_20 or higher {sin,cos,lg2,ex2}.approx.f32 n/a n/a n/a n/a x n/a tanh.approx.f32 n/a n/a n/a n/a n/a n/a .target sm_75 or higher 9.7.3.1. Floating Point Instructions: testp \\uf0c1 testp Test floating-point property.'},\n",
       " {'id': 620,\n",
       "  'content': 'Syntax testp.op.type p, a; // result is .pred .op = { .finite, .infinite, .number, .notanumber, .normal, .subnormal }; .type = { .f32, .f64 }; Description testp tests common properties of floating-point numbers and returns a predicate value of 1 if True and 0 if False . testp.finite True if the input is not infinite or NaN testp.infinite True if the input is positive or negative infinity testp.number True if the input is not NaN testp.notanumber True if the input is NaN testp.normal True if the input is a normal number (not NaN , not infinity) testp.subnormal True if the input is a subnormal number (not NaN , not infinity) As a special case, positive and negative zero are considered normal numbers. PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes Requires sm_20 or higher. Examples testp.notanumber.f32 isnan, f0; testp.infinite.f64 p, X; 9.7.3.2. Floating Point Instructions: copysign \\uf0c1 copysign Copy sign of one input to another. Syntax copysign.type d, a, b; .type = { .f32, .f64 }; Description Copy sign bit of a into value of b , and return the result as d . Examples copysign.f32 x, y, z; copysign.f64 A, B, C; 9.7.3.3. Floating Point Instructions: add \\uf0c1 add Add two values. Syntax add{.rnd}{.ftz}{.sat}.f32 d, a, b; add{.rnd}.f64 d, a, b; .rnd = { .rn, .rz, .rm, .rp }; Description Performs addition and writes the resulting value into a destination register. Semantics d = a + b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that an add instruction with an explicit rounding modifier is treated conservatively by the code optimizer. An add instruction with no rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code optimizer. In particular, mul / add sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device. Subnormal numbers: sm_20+ By default, subnormal numbers are supported. add.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x add.f64 supports subnormal numbers. add.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: add.sat.f32 clamps the result to [0.0, 1.0]. NaN results are flushed to +0.0f . Target ISA Notes add.f32 supported on all target architectures. add.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for add.f64 , requires sm_13 or higher. for add.f32 , requires sm_20 or higher. Examples @p add.rz.ftz.f32 f1,f2,f3; 9.7.3.4. Floating Point Instructions: sub \\uf0c1 sub Subtract one value from another. Syntax sub{.rnd}{.ftz}{.sat}.f32 d, a, b; sub{.rnd}.f64 d, a, b; .rnd = { .rn, .rz, .rm, .rp }; Description Performs subtraction and writes the resulting value into a destination register. Semantics d = a - b; Notes Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that a sub instruction with an explicit rounding modifier is treated conservatively by the code optimizer. A sub instruction with no rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code optimizer. In particular, mul / sub sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device. sub.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x sub.f64 supports subnormal numbers. sub.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: sub.sat.f32 clamps the result to [0.0, 1.0]. Target ISA Notes sub.f32 supported on all target architectures. sub.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for sub.f64 , requires sm_13 or higher. for sub.f32 , requires sm_20 or higher. Examples sub.f32 c,a,b; sub.rn.ftz.f32 f1,f2,f3; 9.7.3.5. Floating Point Instructions: mul \\uf0c1 mul Multiply two values. Syntax mul{.rnd}{.ftz}{.sat}.f32 d, a, b; mul{.rnd}.f64 d, a, b; .rnd = { .rn, .rz, .rm, .rp }; Description Compute the product of two values. Semantics d = a * b; Notes For floating-point multiplication, all operands must be the same size. Rounding modifiers: .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is .rn . Note that a mul instruction with an explicit rounding modifier is treated conservatively by the code optimizer. A mul instruction with no rounding modifier defaults to round-to-nearest-even and may be optimized aggressively by the code optimizer. In particular, mul/add and mul/sub sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device. mul.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x mul.f64 supports subnormal numbers. mul.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mul.sat.f32 clamps the result to [0.0, 1.0]. Target ISA Notes mul.f32 supported on all target architectures. mul.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz available for all targets .rm , .rp for mul.f64 , requires sm_13 or higher. for mul.f32 , requires sm_20 or higher. Examples mul.ftz.f32 circumf,radius,pi // a single-precision multiply 9.7.3.6. Floating Point Instructions: fma \\uf0c1 fma Fused multiply-add. Syntax fma.rnd{.ftz}{.sat}.f32 d, a, b, c; fma.rnd.f64 d, a, b, c; .rnd = { .rn, .rz, .rm, .rp }; Description Performs a fused multiply-add with no loss of precision in the intermediate product and addition. Semantics d = a*b + c; Notes fma.f32 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision. The resulting value is then rounded to single precision using the rounding mode specified by .rnd . fma.f64 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision. The resulting value is then rounded to double precision using the rounding mode specified by .rnd . fma.f64 is the same as mad.f64 . Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. fma.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x fma.f64 supports subnormal numbers. fma.f32 is unimplemented for sm_1x targets. Saturation: fma.sat.f32 clamps the result to [0.0, 1.0]. PTX ISA Notes fma.f64 introduced in PTX ISA version 1.4. fma.f32 introduced in PTX ISA version 2.0. Target ISA Notes fma.f32 requires sm_20 or higher. fma.f64 requires sm_13 or higher. Examples fma.rn.ftz.f32 w,x,y,z; @p fma.rn.f64 d,a,b,c; 9.7.3.7. Floating Point Instructions: mad \\uf0c1 mad Multiply two values and add a third value. Syntax mad{.ftz}{.sat}.f32 d, a, b, c; // .target sm_1x mad.rnd{.ftz}{.sat}.f32 d, a, b, c; // .target sm_20 mad.rnd.f64 d, a, b, c; // .target sm_13 and higher .rnd = { .rn, .rz, .rm, .rp }; Description Multiplies two values and adds a third, and then writes the resulting value into a destination register. Semantics d = a*b + c; Notes For .target sm_20 and higher: mad.f32 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision. mad.f64 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision. mad. {f32,f64} is the same as fma. {f32,f64} . For .target sm_1x : mad.f32 computes the product of a and b at double precision, and then the mantissa is truncated to 23 bits, but the exponent is preserved. Note that this is different from computing the product with mul , where the mantissa can be rounded and the exponent will be clamped. The exception for mad.f32 is when c = +/-0.0 , mad.f32 is identical to the result computed using separate mul and add instructions. When JIT-compiled for SM 2.0 devices, mad.f32 is implemented as a fused multiply-add (i.e., fma.rn.ftz.f32 ). In this case, mad.f32 can produce slightly different numeric results and backward compatibility is not guaranteed in this case. Unlike mad.f32 , the treatment of subnormal inputs and output follows IEEE 754 standard. mad.f64 is the same as fma.f64 . mad.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x mad.f64 supports subnormal numbers. mad.f32 flushes subnormal inputs and results to sign-preserving zero. Saturation modifier: mad.sat.f32 clamps the result to [0.0, 1.0]. In PTX ISA versions 1.4 and later, a rounding modifier is required for mad.f64 . Legacy mad.f64 instructions having no rounding modifier will map to mad.rn.f64 . In PTX ISA versions 2.0 and later, a rounding modifier is required for mad.f32 for sm_20 and higher targets. Errata mad.f32 requires a rounding modifier for sm_20 and higher targets. However for PTX ISA version 3.0 and earlier, ptxas does not enforce this requirement and mad.f32 silently defaults to mad.rn.f32 . For PTX ISA version 3.1, ptxas generates a warning and defaults to mad.rn.f32 , and in subsequent releases ptxas will enforce the requirement for PTX ISA version 3.2 and later. Target ISA Notes mad.f32 supported on all target architectures. mad.f64 requires sm_13 or higher. Rounding modifiers have the following target requirements: .rn , .rz , .rm , .rp for mad.f64 , requires sm_13 or higher. .rn , .rz , .rm , .rp for mad.f32 , requires sm_20 or higher. Examples @p mad.f32 d,a,b,c; 9.7.3.8. Floating Point Instructions: div \\uf0c1 div Divide one value by another. Syntax div.approx{.ftz}.f32 d, a, b; // fast, approximate divide div.full{.ftz}.f32 d, a, b; // full-range approximate divide div.rnd{.ftz}.f32 d, a, b; // IEEE 754 compliant rounding div.rnd.f64 d, a, b; // IEEE 754 compliant rounding .rnd = { .rn, .rz, .rm, .rp }; Description Divides a by b , stores result in d . Semantics d = a / b; Notes Fast, approximate single-precision divides: div.approx.f32 implements a fast approximation to divide, computed as d = a * (1/b) . For |b| in [2 -126 , 2 126 ], the maximum ulp error is 2.'},\n",
       " {'id': 621,\n",
       "  'content': 'For 2 126 -0.0. PTX ISA Notes Introduced in PTX ISA version 1.0. min.NaN introduced in PTX ISA version 7.0. min.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes min.f32 supported on all target architectures. min.f64 requires sm_13 or higher. min.NaN requires sm_80 or higher. min.xorsign.abs requires sm_86 or higher. Examples @p min.ftz.f32 z,z,x; min.f64 a,b,c; // fp32 min with .NaN min.NaN.f32 f0,f1,f2; // fp32 min with .xorsign.abs min.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.12. Floating Point Instructions: max \\uf0c1 max Find the maximum of two values. Syntax max{.ftz}{.NaN}{.xorsign.abs}.f32 d, a, b; max.f64 d, a, b; Description Store the maximum of a and b in d . If .NaN modifier is specified, the result is canonical NaN if either of the inputs is NaN . If .abs modifier is specified, the magnitude of destination operand d is the maximum of absolute values of both the input arguments. If .xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the sign bits of both the inputs. Modifiers .abs and .xorsign must be specified together and .xorsign considers the sign bit of both inputs before applying .abs operation. If the result of max is NaN then the .xorsign and .abs modifiers will be ignored. Semantics if (.xorsign) { xorsign = getSignBit(a) ^ getSignBit(b); if (.abs) { a = |a|; b = |b|; } } if (isNaN(a) && isNaN(b)) d = NaN; else if (.NaN && (isNaN(a) || isNaN(b))) d = NaN; else if (isNaN(a)) d = b; else if (isNaN(b)) d = a; else d = (a > b) ? a : b; if (.xorsign && !isNaN(d)) { setSignBit(d, xorsign); } Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported. max.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x max.f64 supports subnormal numbers. max.f32 flushes subnormal inputs and results to sign-preserving zero. If values of both inputs are 0.0, then +0.0 > -0.0. PTX ISA Notes Introduced in PTX ISA version 1.0. max.NaN introduced in PTX ISA version 7.0. max.xorsign.abs introduced in PTX ISA version 7.2. Target ISA Notes max.f32 supported on all target architectures. max.f64 requires sm_13 or higher. max.NaN requires sm_80 or higher. max.xorsign.abs requires sm_86 or higher. Examples max.ftz.f32 f0,f1,f2; max.f64 a,b,c; // fp32 max with .NaN max.NaN.f32 f0,f1,f2; // fp32 max with .xorsign.abs max.xorsign.abs.f32 Rd, Ra, Rb; 9.7.3.13. Floating Point Instructions: rcp \\uf0c1 rcp Take the reciprocal of a value. Syntax rcp.approx{.ftz}.f32 d, a; // fast, approximate reciprocal rcp.rnd{.ftz}.f32 d, a; // IEEE 754 compliant rounding rcp.rnd.f64 d, a; // IEEE 754 compliant rounding .rnd = { .rn, .rz, .rm, .rp }; Description Compute 1/a , store result in d . Semantics d = 1 / a; Notes Fast, approximate single-precision reciprocal: rcp.approx.f32 implements a fast approximation to reciprocal. The maximum absolute error is 2 -23.0 over the range 1.0-2.0. Input Result -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Reciprocal with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. rcp.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x rcp.f64 supports subnormal numbers. rcp.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes rcp.f32 and rcp.f64 introduced in PTX ISA version 1.0. rcp.rn.f64 and explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4. General rounding modifiers were added in PTX ISA version 2.0. For PTX ISA version 1.4 and later, one of .approx or .rnd is required. For PTX ISA versions 1.0 through 1.3, rcp.f32 defaults to rcp.approx.ftz.f32 , and rcp.f64 defaults to rcp.rn.f64 . Target ISA Notes rcp.approx.f32 supported on all target architectures. rcp.rnd.f32 requires sm_20 or higher. rcp.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32. rcp. {rz,rm,rp}.f64 requires sm_20 or higher. Examples rcp.approx.ftz.f32 ri,r; rcp.rn.ftz.f32 xi,x; rcp.rn.f64 xi,x; 9.7.3.14. Floating Point Instructions: rcp.approx.ftz.f64 \\uf0c1 rcp.approx.ftz.f64 Compute a fast, gross approximation to the reciprocal of a value. Syntax rcp.approx.ftz.f64 d, a; Description Compute a fast, gross approximation to the reciprocal as follows: extract the most-significant 32 bits of .f64 operand a in 1.11.20 IEEE floating-point format (i.e., ignore the least-significant 32 bits of a ), compute an approximate .f64 reciprocal of this value using the most-significant 20 bits of the mantissa of operand a , place the resulting 32-bits in 1.11.20 IEEE floating-point format in the most-significant 32-bits of destination d ,and zero the least significant 32 mantissa bits of .f64 destination d . Semantics tmp = a[63:32]; // upper word of a, 1.11.20 format d[63:32] = 1.0 / tmp; d[31:0] = 0x00000000; Notes rcp.approx.ftz.f64 implements a fast, gross approximation to reciprocal. Input a[63:32] Result d[63:32] -Inf -0.0 -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 . Subnormal inputs and results are flushed to sign-preserving zero. PTX ISA Notes rcp.approx.ftz.f64 introduced in PTX ISA version 2.1. Target ISA Notes rcp.approx.ftz.f64 requires sm_20 or higher. Examples rcp.ftz.f64 xi,x; 9.7.3.15. Floating Point Instructions: sqrt \\uf0c1 sqrt Take the square root of a value. Syntax sqrt.approx{.ftz}.f32 d, a; // fast, approximate square root sqrt.rnd{.ftz}.f32 d, a; // IEEE 754 compliant rounding sqrt.rnd.f64 d, a; // IEEE 754 compliant rounding .rnd = { .rn, .rz, .rm, .rp }; Description Compute sqrt( a ) and store the result in d . Semantics d = sqrt(a); Notes sqrt.approx.f32 implements a fast approximation to square root. Input Result -Inf NaN -normal NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf +Inf NaN NaN Square root with IEEE 754 compliant rounding: Rounding modifiers (no default): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported. sqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x sqrt.f64 supports subnormal numbers. sqrt.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes sqrt.f32 and sqrt.f64 introduced in PTX ISA version 1.0. sqrt.rn.f64 and explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4. For PTX ISA versions 1.0 through 1.3, sqrt.f32 defaults to sqrt.approx.ftz.f32 , and sqrt.f64 defaults to sqrt.rn.f64 . Target ISA Notes sqrt.approx.f32 supported on all target architectures. sqrt.rnd.f32 requires sm_20 or higher. sqrt.rn.f64 requires sm_13 or higher, or .target map_f64_to_f32 . sqrt. Examples sqrt.approx.ftz.f32 r,x; sqrt.rn.ftz.f32 r,x; sqrt.rn.f64 r,x; 9.7.3.16. Floating Point Instructions: rsqrt \\uf0c1 rsqrt Take the reciprocal of the square root of a value. Syntax rsqrt.approx{.ftz}.f32 d, a; rsqrt.approx.f64 d, a; Description Compute 1/sqrt(a) and store the result in d . Semantics d = 1/sqrt(a); Notes rsqrt.approx implements an approximation to the reciprocal square root. Input Result -Inf NaN -normal NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN The maximum absolute error for rsqrt.f32 is 2 -22.4 over the range 1.0-4.0. rsqrt.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x rsqrt.f64 supports subnormal numbers. rsqrt.f32 flushes subnormal inputs and results to sign-preserving zero. Note that rsqrt.approx.f64 is emulated in software and are relatively slow. PTX ISA Notes rsqrt.f32 and rsqrt.f64 were introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz were introduced in PTX ISA version 1.4. For PTX ISA version 1.4 and later, the .approx modifier is required. For PTX ISA versions 1.0 through 1.3, rsqrt.f32 defaults to rsqrt.approx.ftz.f32 , and rsqrt.f64 defaults to rsqrt.approx.f64 . Target ISA Notes rsqrt.f32 supported on all target architectures. rsqrt.f64 requires sm_13 or higher. Examples rsqrt.approx.ftz.f32 isr, x; rsqrt.approx.f64 ISR, X; 9.7.3.17. Floating Point Instructions: rsqrt.approx.ftz.f64 \\uf0c1 rsqrt.approx.ftz.f64 Compute an approximation of the square root reciprocal of a value. Syntax rsqrt.approx.ftz.f64 d, a; Description Compute a double-precision ( .f64 ) approximation of the square root reciprocal of a value. The least significant 32 bits of the double-precision ( .f64 ) destination d are all zeros. Semantics tmp = a[63:32]; // upper word of a, 1.11.20 format d[63:32] = 1.0 / sqrt(tmp); d[31:0] = 0x00000000; Notes rsqrt.approx.ftz.f64 implements a fast approximation of the square root reciprocal of a value. Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 +Inf +subnormal +Inf +Inf +0.0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 . PTX ISA Notes rsqrt.approx.ftz.f64 introduced in PTX ISA version 4.0.'},\n",
       " {'id': 622,\n",
       "  'content': 'Target ISA Notes rsqrt.approx.ftz.f64 requires sm_20 or higher. Examples rsqrt.approx.ftz.f64 xi,x; 9.7.3.18. Floating Point Instructions: sin \\uf0c1 sin Find the sine of a value. Syntax sin.approx{.ftz}.f32 d, a; Description Find the sine of the angle a (in radians). Semantics d = sin(a); Notes sin.approx.f32 implements a fast approximation to sine. Input Result -Inf NaN -subnormal -0.0 -0.0 -0.0 +0.0 +0.0 +subnormal +0.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00. sin.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. sm_1x Subnormal inputs and results to sign-preserving zero. PTX ISA Notes sin.f32 introduced in PTX ISA version 1.0. Explicit modifiers .approx and .ftz introduced in PTX ISA version 1.4. For PTX ISA versions 1.0 through 1.3, sin.f32 defaults to sin.approx.ftz.f32 . Examples sin.approx.ftz.f32 sa, a; 9.7.3.19. Floating Point Instructions: cos \\uf0c1 cos Find the cosine of a value. Syntax cos.approx{.ftz}.f32 d, a; Description Find the cosine of the angle a (in radians). Semantics d = cos(a); Notes cos.approx.f32 implements a fast approximation to cosine. Input Result -Inf NaN -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf NaN NaN NaN The maximum absolute error is 2 -20.9 in quadrant 00. cos.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes cos.f32 introduced in PTX ISA version 1.0. For PTX ISA versions 1.0 through 1.3, cos.f32 defaults to cos.approx.ftz.f32 . Examples cos.approx.ftz.f32 ca, a; 9.7.3.20. Floating Point Instructions: lg2 \\uf0c1 lg2 Find the base-2 logarithm of a value. Syntax lg2.approx{.ftz}.f32 d, a; Description Determine the log 2 of a . Semantics d = log(a) / log(2); Notes lg2.approx.f32 implements a fast approximation to log 2 (a). Input Result -Inf NaN -subnormal -Inf -0.0 -Inf +0.0 -Inf +subnormal -Inf +Inf +Inf NaN NaN The maximum absolute error is 2 -22.6 for mantissa. lg2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes lg2.f32 introduced in PTX ISA version 1.0. For PTX ISA versions 1.0 through 1.3, lg2.f32 defaults to lg2.approx.ftz.f32 . Examples lg2.approx.ftz.f32 la, a; 9.7.3.21. Floating Point Instructions: ex2 \\uf0c1 ex2 Find the base-2 exponential of a value. Syntax ex2.approx{.ftz}.f32 d, a; Description Raise 2 to the power a . Semantics d = 2 ^ a; Notes ex2.approx.f32 implements a fast approximation to 2 a . Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN The maximum absolute error is 2 -22.5 for fraction in the primary range. ex2.ftz.f32 flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes ex2.f32 introduced in PTX ISA version 1.0. For PTX ISA versions 1.0 through 1.3, ex2.f32 defaults to ex2.approx.ftz.f32 . Examples ex2.approx.ftz.f32 xa, a; 9.7.3.22. Floating Point Instructions: tanh \\uf0c1 tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.f32 d, a; Description Take hyperbolic tangent value of a . The operands d and a are of type .f32 . Semantics d = tanh(a); Notes tanh.approx.f32 implements a fast approximation to FP32 hyperbolic-tangent. Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -subnormal Same as input -0.0 -0.0 +0.0 +0.0 +subnormal Same as input +Inf 1.0 NaN NaN The subnormal numbers are supported. Note The subnormal inputs gets passed through to the output since the value of tanh(x) for small values of x is approximately the same as x . PTX ISA Notes Introduced in PTX ISA version 7.0. Target ISA Notes Requires sm_75 or higher. Examples tanh.approx.f32 sa, a; 9.7.4. Half Precision Floating-Point Instructions \\uf0c1 Half precision floating-point instructions operate on .f16 and .f16x2 register operands. The half precision floating-point instructions are: add sub mul fma neg abs min max tanh ex2 Half-precision add , sub , mul , and fma support saturation of results to the range [0.0, 1.0], with NaN s being flushed to positive zero. Half-precision instructions return an unspecified NaN . 9.7.4.1. Half Precision Floating Point Instructions: add \\uf0c1 add Add two values. Syntax add{.rnd}{.ftz}{.sat}.f16 d, a, b; add{.rnd}{.ftz}{.sat}.f16x2 d, a, b; add{.rnd}.bf16 d, a, b; add{.rnd}.bf16x2 d, a, b; .rnd = { .rn }; Description Performs addition and writes the resulting value into a destination register. For .f16x2 and .bf16x2 instruction type, forms input vectors by half word values from source operands. Half-word operands are then added in parallel to produce .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d , a and b have .f16 or .b16 type. For .f16x2 instruction type, operands d , a and b have .b32 type. For .bf16 instruction type, operands d , a , b have .b16 type. For .bf16x2 instruction type, operands d , a , b have .b32 type. Semantics if (type == f16 || type == bf16) { d = a + b; } else if (type == f16x2 || type == bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; for (i = 0; i -0.0. PTX ISA Notes Introduced in PTX ISA version 7.0. min.xorsign introduced in PTX ISA version 7.2. Target ISA Notes Requires sm_80 or higher. min.xorsign.abs support requires sm_86 or higher. Examples min.ftz.f16 h0,h1,h2; min.f16x2 b0,b1,b2; // SIMD fp16 min with .NaN min.NaN.f16x2 b0,b1,b2; min.bf16 h0, h1, h2; // SIMD bf16 min with NaN min.NaN.bf16x2 b0, b1, b2; // scalar bf16 min with xorsign.abs min.xorsign.abs.bf16 Rd, Ra, Rb 9.7.4.8. Half Precision Floating Point Instructions: max \\uf0c1 max Find the maximum of two values. Syntax max{.ftz}{.NaN}{.xorsign.abs}.f16 d, a, b; max{.ftz}{.NaN}{.xorsign.abs}.f16x2 d, a, b; max{.NaN}{.xorsign.abs}.bf16 d, a, b; max{.NaN}{.xorsign.abs}.bf16x2 d, a, b; Description Store the maximum of a and b in d . For .f16x2 and .bf16x2 instruction types, input vectors are formed with half-word values from source operands. Half-word operands are then processed in parallel to store .f16x2 or .bf16x2 result in destination. For .f16 instruction type, operands d and a have .f16 or .b16 type. For .f16x2 instruction type, operands d and a have .f16x2 or .b32 type. For .bf16 instruction type, operands d and a have .b16 type. For .bf16x2 instruction type, operands d and a have .b32 type. Semantics if (type == f16 || type == bf16) { if (.xorsign) { xorsign = getSignBit(a) ^ getSignBit(b); if (.abs) { a = |a|; b = |b|; } } if (isNaN(a) && isNaN(b)) d = NaN; if (.NaN && (isNaN(a) || isNaN(b))) d = NaN; else if (isNaN(a)) d = b; else if (isNaN(b)) d = a; else d = (a > b) ? a : b; if (.xorsign && !isNaN(d)) { setSignBit(d, xorsign); } } else if (type == f16x2 || type == bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; for (i = 0; i fB[i]) ? fA[i] : fB[i]; if (.xorsign && !isNaN(fA[i])) { setSignBit(d[i], xorsign); } } } Notes Subnormal numbers: By default, subnormal numbers are supported. max.ftz.'},\n",
       " {'id': 623,\n",
       "  'content': '{f16, f16x2} flushes subnormal inputs and results to sign-preserving zero. PTX ISA Notes Introduced in PTX ISA version 7.0. max.xorsign.abs introduced in PTX ISA version 7.2. max.xorsign.abs support requires sm_86 or higher. Examples max.ftz.f16 h0,h1,h2; max.f16x2 b0,b1,b2; // SIMD fp16 max with NaN max.NaN.f16x2 b0,b1,b2; // scalar f16 max with xorsign.abs max.xorsign.abs.f16 Rd, Ra, Rb; max.bf16 h0, h1, h2; // scalar bf16 max and NaN max.NaN.bf16x2 b0, b1, b2; // SIMD bf16 max with xorsign.abs max.xorsign.abs.bf16x2 Rd, Ra, Rb; 9.7.4.9. Half Precision Floating Point Instructions: tanh \\uf0c1 tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh.approx.type d, a; .type = {.f16, .f16x2, .bf16, .bf16x2} Description Take hyperbolic tangent value of a . The type of operands d and a are as specified by .type . For .f16x2 or .bf16x2 instruction type, each of the half-word operands are operated in parallel and the results are packed appropriately into a .f16x2 or .bf16x2 . Semantics if (.type == .f16 || .type == .bf16) { d = tanh(a) } else if (.type == .f16x2 || .type == .bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; d[0] = tanh(fA[0]) d[1] = tanh(fA[1]) } Notes tanh.approx. {f16, f16x2, bf16, bf16x2} implements an approximate hyperbolic tangent in the target format. Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1.0 -0.0 -0.0 +0.0 +0.0 +Inf 1.0 NaN NaN The maximum absolute error for .f16 type is 2-10.987. The maximum absolute error for .bf16 type is 2-8. The subnormal numbers are supported. tanh.approx. {bf16/bf16x2} introduced in PTX ISA version 7.8. {bf16/bf16x2} requires sm_90 or higher. Examples tanh.approx.f16 h1, h0; tanh.approx.f16x2 hd1, hd0; tanh.approx.bf16 b1, b0; tanh.approx.bf16x2 hb1, hb0; 9.7.4.10. Half Precision Floating Point Instructions: ex2 \\uf0c1 ex2 Find the base-2 exponent of input. Syntax ex2.approx.atype d, a; ex2.approx.ftz.btype d, a; .atype = { .f16, .f16x2} .btype = { .bf16, .bf16x2} Description Raise 2 to the power a . Semantics if (.type == .f16 || .type == .bf16) { d = 2 ^ a } else if (.type == .f16x2 || .type == .bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; d[0] = 2 ^ fA[0] d[1] = 2 ^ fA[1] } Notes ex2.approx. {f16, f16x2, bf16, bf16x2} implement a fast approximation to 2 a . For the .f16 type, subnormal inputs are supported. ex2.approx.ftz.bf16 flushes subnormal inputs and results to sign-preserving zero. Results of ex2.approx.ftz.bf16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -subnormal +1.0 -0.0 +1.0 +0.0 +1.0 +subnormal +1.0 +Inf +Inf NaN NaN Results of ex2.approx.f16 for various corner-case inputs are as follows: Input Result -Inf +0.0 -0.0 +1.0 +0.0 +1.0 +Inf +Inf NaN NaN The maximum relative error for .f16 type is 2-9.9. The maximum relative error for .bf16 type is 2-7. ex2.approx.ftz. Examples ex2.approx.f16 h1, h0; ex2.approx.f16x2 hd1, hd0; ex2.approx.ftz.bf16 b1, b2; ex2.approx.ftz.bf16x2 hb1, hb2; 9.7.5. Comparison and Selection Instructions \\uf0c1 The comparison select instructions are: set setp selp slct As with single-precision floating-point instructions, the set , setp , and slct instructions support subnormal numbers for sm_20 and higher targets and flush single-precision subnormal inputs to sign-preserving zero for sm_1x targets. The optional .ftz modifier provides backward compatibility with sm_1x targets by flushing subnormal inputs and results to sign-preserving zero regardless of the target architecture. 9.7.5.1.'},\n",
       " {'id': 624,\n",
       "  'content': 'Comparison and Selection Instructions: set \\uf0c1 set Compare two numeric values with a relational operator, and optionally combine this result with a predicate value by applying a Boolean operator. Syntax set.CmpOp{.ftz}.dtype.stype d, a, b; set.CmpOp.BoolOp{.ftz}.dtype.stype d, a, b, {! }c; .CmpOp = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs, equ, neu, ltu, leu, gtu, geu, num, nan }; .BoolOp = { and, or, xor }; .dtype = { .u32, .s32, .f32 }; .stype = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Compares two numeric values and optionally combines the result with another predicate value by applying a Boolean operator. If this result is True , 1.0f is written for floating-point destination types, and 0xffffffff is written for integer destination types. Otherwise, 0x00000000 is written. Operand d has type .dtype ; operands a and b have type .stype ; operand c has type .pred . Semantics t = (a CmpOp b) ? 1 : 0; if (isFloat(dtype)) d = BoolOp(t, c) ? 1.0f : 0x00000000; else d = BoolOp(t, c) ? 0xffffffff : 0x00000000; Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge . For unsigned values, the comparison operators lo , ls , hi , and hs for lower, lower-or-same, higher, and higher-or-same may be used instead of lt , le , gt , ge , respectively. The untyped, bit-size comparisons are eq and ne . Floating Point Notes The ordered comparisons are eq , ne , lt , le , gt , ge . To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu . If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts. If either operand is NaN , then the result of these comparisons is True . set.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero. sm_1x set.dtype.f64 supports subnormal numbers. set.dtype.f32 flushes subnormal inputs to sign-preserving zero. Modifier .ftz applies only to .f32 comparisons. Target ISA Notes set with .f64 source type requires sm_13 or higher. Examples @p set.lt.and.f32.s32 d,a,b,r; set.eq.u32.u32 d,i,n; 9.7.5.2. Comparison and Selection Instructions: setp \\uf0c1 setp Compare two numeric values with a relational operator, and (optionally) combine this result with a predicate value by applying a Boolean operator. Syntax setp.CmpOp{.ftz}.type p[|q], a, b; setp.CmpOp.BoolOp{.ftz}.type p[|q], a, b, {! }c; .CmpOp = { eq, ne, lt, le, gt, ge, lo, ls, hi, hs, equ, neu, ltu, leu, gtu, geu, num, nan }; .BoolOp = { and, or, xor }; .type = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Compares two values and combines the result with another predicate value by applying a Boolean operator. This result is written to the first destination operand.'},\n",
       " {'id': 625,\n",
       "  'content': 'A related value computed using the complement of the compare result is written to the second destination operand. Applies to all numeric types. Operands a and b have type .type ; operands p , q , and c have type .pred . The sink symbol ‘_’ may be used in place of any one of the destination operands. 1 : 0; p = BoolOp(t, c); q = BoolOp(!t, c); Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge . setp.ftz.dtype.f32 flushes subnormal inputs to sign-preserving zero. sm_1x setp.dtype.f64 supports subnormal numbers. setp.dtype.f32 flushes subnormal inputs to sign-preserving zero. Target ISA Notes setp with .f64 source type requires sm_13 or higher. Examples setp.lt.and.s32 p|q,a,b,r; @q setp.eq.u32 p,i,n; 9.7.5.3. Comparison and Selection Instructions: selp \\uf0c1 selp Select between source operands, based on the value of the predicate source operand. Syntax selp.type d, a, b, c; .type = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Conditional selection. If c is True , a is stored in d , b otherwise.'},\n",
       " {'id': 626,\n",
       "  'content': 'Operands d , a , and b must be of the same type. Operand c is a predicate. Semantics d = (c == 1) ? a : b; PTX ISA Notes Introduced in PTX ISA version 1.0. Target ISA Notes selp.f64 requires sm_13 or higher. Examples selp.s32 r0,r,g,p; @q selp.f32 f0,t,x,xp; 9.7.5.4. Comparison and Selection Instructions: slct \\uf0c1 slct Select one source operand, based on the sign of the third operand. Syntax slct.dtype.s32 d, a, b, c; slct{.ftz}.dtype.f32 d, a, b, c; .dtype = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Conditional selection. If c ≥ 0, a is stored in d , otherwise b is stored in d . Operands d , a , and b are treated as a bitsize type of the same width as the first instruction type; operand c must match the second instruction type ( .s32 or .f32 ). The selected input is copied to the output without modification. Semantics d = (c >= 0) ? a : b; Floating Point Notes For .f32 comparisons, negative zero equals zero. slct.ftz.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and operand a is selected. sm_1x slct.dtype.f32 flushes subnormal values of operand c to sign-preserving zero, and operand a is selected. If operand c is NaN , the comparison is unordered and operand b is selected. Target ISA Notes slct.f64 requires sm_13 or higher. Examples slct.u32.s32 x, y, z, val; slct.ftz.u64.f32 A, B, C, fval; 9.7.6. Half Precision Comparison Instructions \\uf0c1 The comparison instructions are: set setp 9.7.6.1. Half Precision Comparison Instructions: set \\uf0c1 set Compare two numeric values with a relational operator, and optionally combine this result with a predicate value by applying a Boolean operator. Syntax set.CmpOp{.ftz}.f16.stype d, a, b; set.CmpOp.BoolOp{.ftz}.f16.stype d, a, b, {! }c; set.CmpOp.bf16.stype d, a, b; set.CmpOp.BoolOp.bf16.stype d, a, b, {! }c; set.CmpOp{.ftz}.dtype.f16 d, a, b; set.CmpOp.BoolOp{.ftz}.dtype.f16 d, a, b, {! }c; .dtype = { .u16, .s16, .u32, .s32} set.CmpOp.dtype.bf16 d, a, b; set.CmpOp.BoolOp.dtype.bf16 d, a, b, {! }c; .dtype = { .u16, .s16, .u32, .s32} set.CmpOp{.ftz}.dtype.f16x2 d, a, b; set.CmpOp.BoolOp{.ftz}.dtype.f16x2 d, a, b, {! }c; .dtype = { .f16x2, .u32, .s32} set.CmpOp.dtype.bf16x2 d, a, b; set.CmpOp.BoolOp.dtype.bf16x2 d, a, b, {! }c; .dtype = { .bf16x2, .u32, .s32} .CmpOp = { eq, ne, lt, le, gt, ge, equ, neu, ltu, leu, gtu, geu, num, nan }; .BoolOp = { and, or, xor }; .stype = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f16, .f32, .f64}; Description Compares two numeric values and optionally combines the result with another predicate value by applying a Boolean operator. Result of this computation is written in destination register in the following way: If result is True , 0xffffffff is written for destination types .u32 / .s32 . 0xffff is written for destination types .u16 / .s16 . 1.0 in target precision floating point format is written for destination type .f16 , .bf16 . If result is False , 0x0 is written for all integer destination types. 0.0 in target precision floating point format is written for destination type .f16 , .bf16 . If the source type is .f16x2 or .bf16x2 then result of individual operations are packed in the 32-bit destination operand. Operand c has type .pred . Semantics if (stype == .f16x2 || stype == .bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; t[0] = (fA[0] CmpOp fB[0]) ? 1 : 0; t[1] = (fA[1] CmpOp fB[1]) ? 1 : 0; if (dtype == .f16x2 || stype == .bf16x2) { for (i = 0; i > (32-n)); case shf.r: // extract 32 lsbs u32 d = (b > n); } Notes Use funnel shift for multi-word shift operations and for rotate operations. The shift amount is limited to the range 0..32 in clamp mode and 0..31 in wrap mode, so shifting multi-word values by distances greater than 32 requires first moving 32-bit words, then using shf to shift the remaining 0..31 distance. To shift data sizes greater than 64 bits to the right, use repeated shf.r instructions applied to adjacent words, operating from least-significant word towards most-significant word. At each step, a single word of the shifted result is computed. The most-significant word of the result is computed using a shr. {u32,s32} instruction, which zero or sign fills based on the instruction type. To shift data sizes greater than 64 bits to the left, use repeated shf.l instructions applied to adjacent words, operating from most-significant word towards least-significant word. The least-significant word of the result is computed using a shl instruction. Use funnel shift to perform 32-bit left or right rotate by supplying the same value for source arguments a and b . PTX ISA Notes Introduced in PTX ISA version 3.1. Target ISA Notes Requires sm_32 or higher. Example shf.l.clamp.b32 r3,r1,r0,16; // 128-bit left shift; n > n shf.r.clamp.b32 r4,r0,r1,n; shf.r.clamp.b32 r5,r1,r2,n; shf.r.clamp.b32 r6,r2,r3,n; shr.s32 r7,r3,n; // result is sign-extended shf.r.clamp.b32 r1,r0,r0,n; // rotate right by n; n > b; Notes Shift amounts greater than the register width N are clamped to N . The sizes of the destination and first source operand must match, but not necessarily the type. The b operand must be a 32-bit value, regardless of the instruction type. Bit-size types are included for symmetry with shl . Example shr.u16 c,a,2; shr.s32 i,i,1; shr.b16 k,i,j; 9.7.8. Data Movement and Conversion Instructions \\uf0c1 These instructions copy data from place to place, and from state space to state space, possibly converting it from one format to another. mov , ld , ldu , and st operate on both scalar and vector types. The isspacep instruction is provided to query whether a generic address falls within a particular state space window. The cvta instruction converts addresses between generic and const , global , local , or shared state spaces. Instructions ld , st , suld , and sust support optional cache operations. The Data Movement and Conversion Instructions are: mov shfl.sync prmt ld ldu st st.async multimem.ld_reduce , multimem.st , multimem.red prefetch , prefetchu isspacep cvta cvt cvt.pack cp.async cp.async.commit_group cp.async.wait_group , cp.async.wait_all cp.async.bulk cp.reduce.async.bulk cp.async.bulk.prefetch cp.async.bulk.tensor cp.reduce.async.bulk.tensor cp.async.bulk.prefetch.tensor cp.async.bulk.commit_group cp.async.bulk.wait_group tensormap.replace 9.7.8.1. Cache Operators \\uf0c1 PTX ISA version 2.0 introduced optional cache operators on load and store instructions.'},\n",
       " {'id': 627,\n",
       "  'content': \"The cache operators require a target architecture of sm_20 or higher. Cache operators on load or store instructions are treated as performance hints only. The use of a cache operator on an ld or st instruction does not change the memory consistency behavior of the program. For sm_20 and higher, the cache operators have the following definitions and behavior. Table 27 Cache Operators for Memory Load Instructions \\uf0c1 Operator Meaning .ca Cache at all levels, likely to be accessed again. The default load instruction cache operation is ld.ca, which allocates cache lines in all levels (L1 and L2) with normal eviction policy. Global data is coherent at the L2 level, but multiple L1 caches are not coherent for global data. If one thread stores to global memory via one L1 cache, and a second thread loads that address via a second L1 cache with ld.ca , the second thread may get stale L1 cache data, rather than the data stored by the first thread. The driver must invalidate global L1 cache lines between dependent grids of parallel threads. Stores by the first grid program are then correctly fetched by the second grid program issuing default ld.ca loads cached in L1. .cg Cache at global level (cache in L2 and below, not L1). Use ld.cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2 cache. .cs Cache streaming, likely to be accessed once. The ld.cs load cached streaming operation allocates global lines with evict-first policy in L1 and L2 to limit cache pollution by temporary streaming data that may be accessed once or twice. When ld.cs is applied to a Local window address, it performs the ld.lu operation. .lu Last use. The compiler/programmer may use ld.lu when restoring spilled registers and popping function stack frames to avoid needless write-backs of lines that will not be used again. The ld.lu instruction performs a load cached streaming operation ( ld.cs ) on global addresses. .cv Don’t cache and fetch again (consider cached system memory lines stale, fetch again). The ld.cv load operation applied to a global System Memory address invalidates (discards) a matching L2 line and re-fetches the line on each new load. Table 28 Cache Operators for Memory Store Instructions \\uf0c1 Operator Meaning .wb Cache write-back all coherent levels. The default store instruction cache operation is st.wb , which writes back cache lines of coherent cache levels with normal eviction policy. If one thread stores to global memory, bypassing its L1 cache, and a second thread in a different SM later loads from that address via a different L1 cache with ld.ca , the second thread may get a hit on stale L1 cache data, rather than get the data from L2 or memory stored by the first thread. The driver must invalidate global L1 cache lines between dependent grids of thread arrays. Stores by the first grid program are then correctly missed in L1 and fetched by the second grid program issuing default ld.ca loads. Use st.cg to cache global store data only globally, bypassing the L1 cache, and cache only in the L2 cache. The st.cs store cached-streaming operation allocates cache lines with evict-first policy to limit cache pollution by streaming output data. .wt Cache write-through (to system memory). The st.wt store write-through operation applied to a global System Memory address writes through the L2 cache. 9.7.8.2. Cache Eviction Priority Hints \\uf0c1 PTX ISA version 7.4 adds optional cache eviction priority hints on load and store instructions. Cache eviction priority requires target architecture sm_70 or higher. Cache eviction priority on load or store instructions is treated as a performance hint. It is supported for .global state space and generic addresses where the address points to .global state space. Table 29 Cache Eviction Priority Hints for Memory Load and Store Instructions \\uf0c1 Cache Eviction Priority Meaning evict_normal Cache data with normal eviction priority. This is the default eviction priority. evict_first Data cached with this priority will be first in the eviction priority order and will likely be evicted when cache eviction is required. This priority is suitable for streaming data. evict_last Data cached with this priority will be last in the eviction priority order and will likely be evicted only after other data with evict_normal or evict_first eviction priotity is already evicted. This priority is suitable for data that should remain persistent in cache. evict_unchanged Do not change eviction priority order as part of this operation. no_allocate Do not allocate data to cache. 9.7.8.3. Data Movement and Conversion Instructions: mov \\uf0c1 mov Set a register variable with the value of a register variable or an immediate value. Take the non-generic address of a variable in global, local, or shared state space. Syntax mov.type d, a; mov.type d, sreg; mov.type d, avar; // get address of variable mov.type d, avar+imm; // get address of variable with offset mov.u32 d, fname; // get address of device function mov.u64 d, fname; // get address of device function mov.u32 d, kernel; // get address of entry function mov.u64 d, kernel; // get address of entry function .type = { .pred, .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 }; Description Write register d with the value of a . Operand a may be a register, special register, variable with optional offset in an addressable memory space, or function name. For variables declared in .const , .global , .local , and .shared state spaces, mov places the non-generic address of the variable (i.e., the address of the variable in its state space) into the destination register. The generic address of a variable in const , global , local , or shared state space may be generated by first taking the address within the state space with mov and then converting it to a generic address using the cvta instruction; alternately, the generic address of a variable declared in const , global , local , or shared state space may be taken directly using the cvta instruction. Note that if the address of a device function parameter is moved to a register, the parameter will be copied onto the stack and the address will be in the local state space. Semantics d = a; d = sreg; d = &avar; // address is non-generic; i.e., within the variable's declared state space d = &avar+imm; Notes Although only predicate and bit-size types are required, we include the arithmetic types for the programmer’s convenience: their use enhances program readability and allows additional type checking. When moving address of a kernel or a device function, only .u32 or .u64 instruction types are allowed. However, if a signed type is used, it is not treated as a compilation error. The compiler issues a warning in this case. Taking the address of kernel entry functions requires PTX ISA version 3.1 or later. Kernel function addresses should only be used in the context of CUDA Dynamic Parallelism system calls. Target ISA Notes mov.f64 requires sm_13 or higher. Taking the address of kernel entry functions requires sm_35 or higher. Examples mov.f32 d,a; mov.u16 u,v; mov.f32 k,0.1; mov.u32 ptr, A; // move address of A into ptr mov.u32 ptr, A[5]; // move address of A[5] into ptr mov.u32 ptr, A+20; // move address with offset into ptr mov.u32 addr, myFunc; // get address of device function 'myFunc' mov.u64 kptr, main; // get address of entry function 'main' 9.7.8.4. Data Movement and Conversion Instructions: mov \\uf0c1 mov Move vector-to-scalar (pack) or scalar-to-vector (unpack). Syntax mov.type d, a; .type = { .b16, .b32, .b64, .b128 }; Description Write scalar register d with the packed value of vector register a , or write vector register d with the unpacked values from scalar register a . When destination operand d is a vector register, the sink symbol '_' may be used for one or more elements provided that at least one element is a scalar register. For bit-size types, mov may be used to pack vector elements into a scalar register or unpack sub-fields of a scalar register into a vector. Both the overall size of the vector and the size of the scalar must match the size of the instruction type. Semantics // pack two 8-bit elements into .b16 d = a.x | (a.y = maxLane); break; case .down: j = lane + bval; pval = (j = maxLane); break; case .down: j = lane + bval; pval = (j > 0) & 0xf; ctl[1] = (c >> 4) & 0xf; ctl[2] = (c >> 8) & 0xf; ctl[3] = (c >> 12) & 0xf; } else { ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >> 0) & 0x3; } tmp[07:00] = ReadByte( mode, ctl[0], tmp64 ); tmp[15:08] = ReadByte( mode, ctl[1], tmp64 ); tmp[23:16] = ReadByte( mode, ctl[2], tmp64 ); tmp[31:24] = ReadByte( mode, ctl[3], tmp64 ); PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes prmt requires sm_20 or higher.\"},\n",
       " {'id': 628,\n",
       "  'content': 'Examples prmt.b32 r1, r2, r3, r4; prmt.b32.f4e r1, r2, r3, r4; 9.7.8.8. Data Movement and Conversion Instructions: ld \\uf0c1 ld Load a register variable from an addressable state space variable. Syntax ld{.weak}{.ss}{.cop}{.level::cache_hint}{.level::prefetch_size}{.vec}.type d, [a]{.unified}{, cache-policy}; ld{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type d, [a]{.unified}{, cache-policy}; ld.volatile{.ss}{.level::prefetch_size}{.vec}.type d, [a]; ld.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type d, [a]{, cache-policy}; ld.acquire.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}{.vec}.type d, [a]{, cache-policy}; ld.mmio.relaxed.sys{.global}.type d, [a]; .ss = { .const, .global, .local, .param{::entry, ::func}, .shared{::cta, ::cluster} }; .cop = { .ca, .cg, .cs, .lu, .cv }; .level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged, .L1::evict_first, .L1::evict_last, .L1::no_allocate }; .level::cache_hint = { .L2::cache_hint }; .level::prefetch_size = { .L2::64B, .L2::128B, .L2::256B } .scope = { .cta, .cluster, .gpu, .sys }; .vec = { .v2, .v4 }; .type = { .b8, .b16, .b32, .b64, .b128, .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .f32, .f64 }; Description Load register variable d from the location specified by the source address operand a in specified state space. If no state space is given, perform the load using Generic Addressing .'},\n",
       " {'id': 629,\n",
       "  'content': 'If no sub-qualifier is specified with .shared state space, then ::cta is assumed by default. Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands If no sub-qualifier is specified with .param state space, then : ::func is assumed when access is inside a device function. ::entry is assumed when accessing kernel function parameters from entry function. Otherwise, when accessing device function parameters or any other .param variables from entry function ::func is assumed by default. For ld.param::entry instruction, operand a must be a kernel parameter address, otherwise behavior is undefined. For ld.param::func instruction, operand a must be a device function parameter address, otherwise behavior is undefined. Instruction ld.param{::func} used for reading value returned from device function call cannot be predicated. See Parameter State Space and Function Declarations and Definitions for descriptions of the proper use of ld.param . The .relaxed and .acquire qualifiers indicate memory synchronization as described in the Memory Consistency Model . The .scope qualifier indicates the set of threads with which an ld.relaxed or ld.acquire instruction can directly synchronize 1 . The .weak qualifier indicates a memory instruction with no synchronization. The effects of this instruction become visible to other threads only when synchronization is established by other means. The semantic details of .mmio qualifier are described in the Memory Consistency Model . Only .sys thread scope is valid for ld.mmio operation. The qualifiers .mmio and .relaxed must be specified together. The .weak , .volatile , .relaxed and .acquire qualifiers are mutually exclusive. When none of these is specified, the .weak qualifier is assumed by default. An ld.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location. volatile and non-volatile load operations to the same memory location may be reordered. ld.volatile has the same memory synchronization semantics as ld.relaxed.sys . The qualifiers .volatile , .relaxed and .acquire may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. Cache operations are not permitted with these qualifiers. The qualifier .mmio may be used only with .global space and with generic addressing, where the address points to .global space. The optional qualifier .unified must be specified on operand a if a is the address of a variable declared with .unified attribute as described in Variable and Function Attribute Directive: .attribute . The qualifier .level::eviction_priority specifies the eviction policy that will be used during memory access. The .level::prefetch_size qualifier is a hint to fetch additional data of the specified size into the respective cache level.The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes respectively. The qualifier .level::prefetch_size may only be used with .global state space and with generic addressing where the address points to .global state space. If the generic address does not fall within the address window of the global memory, then the prefetching behavior is undefined. The .level::prefetch_size qualifier is treated as a performance hint only. When the optional argument cache-policy is specified, the qualifier .level::cache_hint is required. The 64-bit operand cache-policy specifies the cache eviction policy that may be used during the memory access. The qualifiers .unified and .level::cache_hint are only supported for .global state space and for generic addressing where the address points to the .global state space. cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as a performance hint only, and does not change the memory consistency behavior of the program. 1 This synchronization is further extended to other threads through the transitive nature of causality order , as described in the memory consistency model. Semantics d = a; // named variable a d = *(&a+immOff) // variable-plus-offset d = *a; // register d = *(a+immOff); // register-plus-offset d = *(immAddr); // immediate address Notes Destination d must be in the .reg state space. A destination register wider than the specified type may be used. The value loaded is sign-extended to the destination register width for signed integers, and is zero-extended to the destination register width for unsigned and bit-size types. See Table 25 for a description of these relaxed type-checking rules. .f16 data may be loaded using ld.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions. .f16x2 data may be loaded using ld.b32 and then used in half precision floating point instructions. PTX ISA Notes ld introduced in PTX ISA version 1.0. ld.volatile introduced in PTX ISA version 1.1. Generic addressing and cache operations introduced in PTX ISA version 2.0. Support for scope qualifier, .relaxed , .acquire , .weak qualifiers introduced in PTX ISA version 6.0. Support for generic addressing of .const space added in PTX ISA version 3.1. Support for .level::eviction_priority , .level::prefetch_size and .level::cache_hint qualifiers introduced in PTX ISA version 7.4. Support for .cluster scope qualifier introduced in PTX ISA version 7.8. Support for ::cta and ::cluster sub-qualifiers introduced in PTX ISA version 7.8. Support for .unified qualifier introduced in PTX ISA version 8.0. Support for .mmio qualifier introduced in PTX ISA version 8.2. Support for ::entry and ::func sub-qualifiers on .param space introduced in PTX ISA version 8.3. Support for .b128 type introduced in PTX ISA version 8.3. Support for .sys scope with .b128 type introduced in PTX ISA version 8.4. Target ISA Notes ld.f64 requires sm_13 or higher. Support for scope qualifier, .relaxed , .acquire , .weak qualifiers require sm_70 or higher. Generic addressing requires sm_20 or higher. Cache operations require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_70 or higher. Support for .level::prefetch_size qualifier requires sm_75 or higher. Support for .L2::256B and .L2::cache_hint qualifiers requires sm_80 or higher. Support for .cluster scope qualifier requires sm_90 or higher. Sub-qualifier ::cta requires sm_30 or higher. Sub-qualifier ::cluster requires sm_90 or higher. Support for .unified qualifier requires sm_90 or higher. Support for .mmio qualifier requires sm_70 or higher. Support for .b128 type requires sm_70 or higher. Examples ld.global.f32 d,[a]; ld.shared.v4.b32 Q,[p]; ld.const.s32 d,[p+4]; ld.local.b32 x,[p+-8]; // negative offset ld.local.b64 x,[240]; // immediate address ld.global.b16 %r,[fs]; // load .f16 data into 32-bit reg cvt.f32.f16 %r,%r; // up-convert f16 data to f32 ld.global.b32 %r0, [fs]; // load .f16x2 data in 32-bit reg ld.global.b32 %r1, [fs + 4]; // load .f16x2 data in 32-bit reg add.rn.f16x2 %d0, %r0, %r1; // addition of f16x2 data ld.global.relaxed.gpu.u32 %r0, [gbl]; ld.shared.acquire.gpu.u32 %r1, [sh]; ld.global.relaxed.cluster.u32 %r2, [gbl]; ld.shared::cta.acquire.gpu.u32 %r2, [sh + 4]; ld.shared::cluster.u32 %r3, [sh + 8]; ld.global.mmio.relaxed.sys.u32 %r3, [gbl]; ld.global.f32 d,[ugbl].unified; ld.b32 %r0, [%r1].unified; ld.global.L1::evict_last.u32 d, [p]; ld.global.L2::64B.b32 %r0, [gbl]; // Prefetch 64B to L2 ld.L2::128B.f64 %r1, [gbl]; // Prefetch 128B to L2 ld.global.L2::256B.f64 %r2, [gbl]; // Prefetch 256B to L2 createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 cache-policy, 1; ld.global.L2::cache_hint.b64 x, [p], cache-policy; ld.param::entry.b32 %rp1, [kparam1]; ld.global.b128 %r0, [gbl]; // 128-bit load 9.7.8.9. Data Movement and Conversion Instructions: ld.global.nc \\uf0c1 ld.global.nc Load a register variable from global state space via non-coherent cache.'},\n",
       " {'id': 630,\n",
       "  'content': 'Syntax ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.type d, [a]{, cache-policy}; ld.global{.cop}.nc{.level::cache_hint}{.level::prefetch_size}.vec.type d, [a]{, cache-policy}; ld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.type d, [a]{, cache-policy}; ld.global.nc{.level::eviction_priority}{.level::cache_hint}{.level::prefetch_size}.vec.type d, [a]{, cache-policy}; .cop = { .ca, .cg, .cs }; // cache operation .level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged, .L1::evict_first, .L1::evict_last, .L1::no_allocate}; .level::cache_hint = { .L2::cache_hint }; .level::prefetch_size = { .L2::64B, .L2::128B, .L2::256B } .vec = { .v2, .v4 }; .type = { .b8, .b16, .b32, .b64, .b128, .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .f32, .f64 }; Description Load register variable d from the location specified by the source address operand a in the global state space, and optionally cache in non-coherent read-only cache. Note On some architectures, the texture cache is larger, has higher bandwidth, and longer latency than the global memory cache.'},\n",
       " {'id': 631,\n",
       "  'content': 'For applications with sufficient parallelism to cover the longer latency, ld.global.nc should offer better performance than ld.global on such architectures. The address operand a may contain a generic address pointing to the .global state space. Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The qualifier .level::eviction_priority specifies the eviction policy that will be used during memory access. .f16 data may be loaded using ld.b16 , and then converted to .f32 or .f64 using cvt . Support for .level::cache_hint qualifier requires sm_80 or higher. Examples ld.global.nc.f32 d, [a]; ld.gloal.nc.L1::evict_last.u32 d, [a]; createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.5; ld.global.nc.L2::cache_hint.f32 d, [a], cache-policy; ld.global.nc.L2::64B.b32 d, [a]; // Prefetch 64B to L2 ld.global.nc.L2::256B.f64 d, [a]; // Prefetch 256B to L2 ld.global.nc.b128 d, [a]; 9.7.8.10. Data Movement and Conversion Instructions: ldu \\uf0c1 ldu Load read-only data from an address that is common across threads in the warp. Syntax ldu{.ss}.type d, [a]; // load from address ldu{.ss}.vec.type d, [a]; // vec load from address .ss = { .global }; // state space .vec = { .v2, .v4 }; .type = { .b8, .b16, .b32, .b64, .b128, .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .f32, .f64 }; Description Load read-only data into register variable d from the location specified by the source address operand a in the global state space, where the address is guaranteed to be the same across all threads in the warp. Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands Semantics d = a; // named variable a d = *(&a+immOff) // variable-plus-offset d = *a; // register d = *(a+immOff); // register-plus-offset d = *(immAddr); // immediate address Notes Destination d must be in the .reg state space. .f16 data may be loaded using ldu.b16 , and then converted to .f32 or .f64 using cvt or can be used in half precision floating point instructions. .f16x2 data may be loaded using ldu.b32 and then used in half precision floating point instructions. Target ISA Notes ldu.f64 requires sm_13 or higher. Examples ldu.global.f32 d,[a]; ldu.global.b32 d,[p+4]; ldu.global.v4.f32 Q,[p]; ldu.global.b128 d,[a]; 9.7.8.11. Data Movement and Conversion Instructions: st \\uf0c1 st Store data to an addressable state space variable. Syntax st{.weak}{.ss}{.cop}{.level::cache_hint}{.vec}.type [a], b{, cache-policy}; st{.weak}{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type [a], b{, cache-policy}; st.volatile{.ss}{.vec}.type [a], b; st.relaxed.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type [a], b{, cache-policy}; st.release.scope{.ss}{.level::eviction_priority}{.level::cache_hint}{.vec}.type [a], b{, cache-policy}; st.mmio.relaxed.sys{.global}.type [a], b; .ss = { .global, .local, .param{::func}, .shared{::cta, ::cluster} }; .level::eviction_priority = { .L1::evict_normal, .L1::evict_unchanged, .L1::evict_first, .L1::evict_last, .L1::no_allocate }; .level::cache_hint = { .L2::cache_hint }; .cop = { .wb, .cg, .cs, .wt }; .sem = { .relaxed, .release }; .scope = { .cta, .cluster, .gpu, .sys }; .vec = { .v2, .v4 }; .type = { .b8, .b16, .b32, .b64, .b128, .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .f32, .f64 }; Description Store the value of operand b in the location specified by the destination address operand a in specified state space. If no state space is given, perform the store using Generic Addressing .'},\n",
       " {'id': 632,\n",
       "  'content': 'Stores to const memory are illegal. Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands If .param is specified without any sub-qualifiers then it defaults to .param::func . Instruction st.param{::func} used for passing arguments to device function cannot be predicated. See Parameter State Space and Function Declarations and Definitions for descriptions of the proper use of st.param . The qualifiers .relaxed and .release indicate memory synchronization as described in the Memory Consistency Model . The .scope qualifier indicates the set of threads with which an st.relaxed or st.release instruction can directly synchronize 1 . Only .sys thread scope is valid for st.mmio operation. The .weak , .volatile , .relaxed and .release qualifiers are mutually exclusive. An st.volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location. st.volatile has the same memory synchronization semantics as st.relaxed.sys . The qualifiers .volatile , .relaxed and .release may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. The qualifier .level::cache_hint is only supported for .global state space and for generic addressing where the address points to the .global state space. Semantics d = a; // named variable d *(&a+immOffset) = b; // variable-plus-offset *a = b; // register *(a+immOffset) = b; // register-plus-offset *(immAddr) = b; // immediate address Notes Operand b must be in the .reg state space. A source register wider than the specified type may be used. The lower n bits corresponding to the instruction-type width are stored to memory. See Table 24 for a description of these relaxed type-checking rules. .f16 data resulting from a cvt instruction may be stored using st.b16 . .f16x2 data may be stored using st.b32 . PTX ISA Notes st introduced in PTX ISA version 1.0. st.volatile introduced in PTX ISA version 1.1. Support for scope qualifier, .relaxed , .release , .weak qualifiers introduced in PTX ISA version 6.0. Support for .level::eviction_priority and .level::cache_hint qualifiers introduced in PTX ISA version 7.4. Support for ::func sub-qualifier on .param space introduced in PTX ISA version 8.3. Target ISA Notes st.f64 requires sm_13 or higher. Support for scope qualifier, .relaxed , .release , .weak qualifiers require sm_70 or higher. Examples st.global.f32 [a],b; st.local.b32 [q+4],a; st.global.v4.s32 [p],Q; st.local.b32 [q+-8],a; // negative offset st.local.s32 [100],r7; // immediate address cvt.f16.f32 %r,%r; // %r is 32-bit register st.b16 [fs],%r; // store lower st.global.relaxed.sys.u32 [gbl], %r0; st.shared.release.cta.u32 [sh], %r1; st.global.relaxed.cluster.u32 [gbl], %r2; st.shared::cta.release.cta.u32 [sh + 4], %r1; st.shared::cluster.u32 [sh + 8], %r1; st.global.mmio.relaxed.sys.u32 [gbl], %r1; st.global.L1::no_allocate.f32 [p], a; createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25; st.global.L2::cache_hint.b32 [a], b, cache-policy; st.param::func.b64 [param1], %rp1; st.global.b128 [a], b; // 128-bit store 9.7.8.12. Data Movement and Conversion Instructions: st.async \\uf0c1 st.async Asynchronous store operation on shared memory.'},\n",
       " {'id': 633,\n",
       "  'content': 'Syntax st.async{.weak}{.ss}{.completion_mechanism}{.vec}.type [a], b, [mbar]; .ss = { .shared::cluster }; .type = { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 }; .vec = { .v2, .v4 }; .completion_mechanism = { .mbarrier::complete_tx::bytes }; Description st.async is a non-blocking instruction which initiates an asynchronous store operation that stores the value specified by source operand b to the destination memory location specified by operand a . The modifier .completion_mechanism specifies that upon completion of the asynchronous operation, complete-tx operation, with completeCount argument equal to amount of data stored in bytes, will be performed on the mbarrier object specified by the operand mbar . Operand a represents destination address and must be a register or of the form register + immOff as described in Addresses as Operands . The shared memory addresses of destination operand a and the mbarrier object mbar , must meet all of the following conditions: They belong to the same CTA. They are different to the CTA of the executing thread but must be within the same cluster. Otherwise, the behavior is undefined. The state space of the address {.ss} , if specified, is applicable to both operands a and mbar . If not specified, then Generic Addressing is used for both a and mbar . If the generic addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined. The store operation in st.async is treated as a weak memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.1. Target ISA Notes Requires sm_90 or higher. Examples st.async.shared::cluster.mbarrier::complete_tx::bytes.u32 [addr], b, [mbar_addr] 9.7.8.13. Data Movement and Conversion Instructions: multimem.ld_reduce, multimem.st, multimem.red \\uf0c1 The multimem. * operations operate on multimem addresses and accesses all of the multiple memory locations which the multimem address points to. Multimem addresses can only be accessed only by multimem. * operations. Accessing a multimem address with ld , st or any other memory operations results in undefined behavior. Refer to CUDA programming guide for creation and management of the multimem addresses. multimem.ld_reduce, multimem.st, multimem.red Perform memory operations on the multimem address. Syntax // Integer type: multimem.ld_reduce{.ldsem}{.scope}{.ss}.op.type d, [a]; multimem.st{.stsem}{.scope}{.ss}.type [a], b; multimem.red{.redsem}{.scope}{.ss}.op.type [a], b; .ss = { .global } .ldsem = { .weak, .relaxed, .acquire } .stsem = { .weak, .relaxed, .release } .redsem = { .relaxed, .release } .scope = { .cta, .cluster, .gpu, .sys } .op = { .min, .max, .add, .and, .or, .xor } .type = { .b32, .b64, .u32, .u64, .s32, .s64 } // Floating point type: multimem.ld_reduce{.ldsem}{.scope}{.ss}.op{.acc_prec}{.vec}.type d, [a]; multimem.st{.stsem}{.scope}{.ss}{.vec}.type [a], b; multimem.red{.redsem}{.scope}{.ss}.redop{.vec}.type [a], b; .ss = { .global } .ldsem = { .weak, .relaxed, .acquire } .stsem = { .weak, .relaxed, .release } .redsem = { .relaxed, .release } .scope = { .cta, .cluster, .gpu, .sys } .op = { .min, .max, .add } .redop = { .add } .acc_prec = { .acc::f32 } .vec = { .v2, .v4, .v8 } .type= { .f16, .f16x2, .bf16, .bf16x2, .f32, .f64 } Description Instruction multimem.ld_reduce performs the following operations: load operation on the multimem address a , which involves loading of data from all of the multiple memory locations pointed to by the multimem address a , reduction operation specified by .op on the multiple data loaded from the multimem address a . The result of the reduction operation in returned in register d .'},\n",
       " {'id': 634,\n",
       "  'content': 'Instruction multimem.st performs a store operation of the input operand b to all the memory locations pointed to by the multimem address a . Instruction multimem.red performs a reduction operation on all the memory locations pointed to by the multimem address a , with operand b . Instruction multimem.ld_reduce performs reduction on the values loaded from all the memory locations that the multimem address points to. In contrast, the multimem.red perform reduction on all the memory locations that the multimem address points to. Address operand a must be a multimem address. Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands . If no state space is specified then Generic Addressing is used. If the address specified by a does not fall within the address window of .global state space then the behavior is undefined. For floating-point type multi- operations, the size of the specified type along with .vec must equal either 32-bits or 64-bits or 128-bits. No other combinations of .vec and type are allowed. Type .f64 cannot be used with .vec qualifier. The following table describes the valid combinations of .op and base type: op Base type .add .u32 , .u64 , .s32 .f16 , .f16x2 , .bf16 , .bf16x2 .f32 , .f64 .and , .or , .xor .b32 , .b64 .min , .max .u32 , .s32 , .u64 , .s644 .f16 , .f16x2 , .bf16 , .bf16x2 For multimem.ld_reduce , the default precision of the intermediate accumulation is same as the specified type. Optionally for .f16 , .f16x2 , .bf16 and .bf16x2 types, .acc::f32 can be specified to change the precision of the intermediate accumulation to .f32 . Optional qualifiers .ldsem , .stsem and .redsem specify the memory synchronizing effect of the multimem.ld_reduce , multimem.st and multimem.red respectively, as described in Memory Consistency Model . If explicit semantics qualifiers are not specified, then multimem.ld_reduce and multimem.st default to .weak and multimem.red defaults to .relaxed . The optional .scope qualifier specifies the set of threads that can directly observe the memory synchronizing effect of this operation, as described in Memory Consistency Model . If the .scope qualifier is not specified for multimem.red then .sys scope is assumed by default. Support for .acc::f32 qualifier introduced in PTX ISA version 8.2. Examples multimem.ld_reduce.and.b32 val1_b32, [addr1]; multimem.ld_reduce.acquire.gpu.global.add.u32 val2_u32, [addr2]; multimem.st.relaxed.gpu.b32 [addr3], val3_b32; multimem.st.release.cta.global.u32 [addr4], val4_u32; multimem.red.relaxed.gpu.max.f64 [addr5], val5_f64; multimem.red.release.cta.global.add.v4.f32 [addr6], {val6, val7, val8, val9}; multimem.ld_reduce.add.acc::f32.v2.f16x2 {val_10, val_11}, [addr7]; 9.7.8.14. Data Movement and Conversion Instructions: prefetch, prefetchu \\uf0c1 prefetch, prefetchu Prefetch line containing a generic address at a specified level of memory hierarchy, in specified state space.'},\n",
       " {'id': 635,\n",
       "  'content': 'Syntax prefetch{.space}.level [a]; // prefetch to data cache prefetch.global.level::eviction_priority [a]; // prefetch to data cache prefetchu.L1 [a]; // prefetch to uniform cache prefetch{.tensormap_space}.tensormap [a]; // prefetch the tensormap .space = { .global, .local }; .level = { .L1, .L2 }; .level::eviction_priority = { .L2::evict_last, .L2::evict_normal }; .tensormap_space = { .const, .param }; Description The prefetch instruction brings the cache line containing the specified address in global or local memory state space into the specified cache level. If the .tensormap qualifier is specified then the prefetch instruction brings the cache line containing the specified address in the .const or .param memory state space for subsequent use by the cp.async.bulk.tensor instruction. If no state space is given, the prefetch uses Generic Addressing . Optionally, the eviction priority to be applied on the prefetched cache line can be specified by the modifier .level::eviction_priority . Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The prefetchu instruction brings the cache line containing the specified generic address into the specified uniform cache level. A prefetch to a shared memory location performs no operation. A prefetch into the uniform cache requires a generic address, and no operation occurs if the address maps to a const , local , or shared memory location. Support for .level::eviction_priority qualifier introduced in PTX ISA version 7.4. Support for the .tensormap qualifier is introduced in PTX ISA version 8.0. Target ISA Notes prefetch and prefetchu require sm_20 or higher. Support for .level::eviction_priority qualifier requires sm_80 or higher. Support for the .tensormap qualifier requires sm_90 or higher. Examples prefetch.global.L1 [ptr]; prefetch.global.L2::evict_last [ptr]; prefetchu.L1 [addr]; prefetch.global.tensormap [ptr]; 9.7.8.15. Data Movement and Conversion Instructions: applypriority \\uf0c1 applypriority Apply the cache eviction priority to the specified address in the specified cache level. Syntax applypriority{.global}.level::eviction_priority [a], size; .level::eviction_priority = { .L2::evict_normal }; Description The applypriority instruction applies the cache eviction priority specified by the .level::eviction_priority qualifier to the address range [a..a+size) in the specified cache level. If the specified address does not fall within the address window of .global state space then the behavior is undefined. The operand size is an integer constant that specifies the amount of data, in bytes, in the specified cache level on which the priority is to be applied. The only supported value for the size operand is 128. Supported addressing modes for operand a are described in Addresses as Operands . a must be aligned to 128 bytes. If the data pointed to by address a is not already present in the specified cache level, then the data will be prefetched before applying the specified priority. PTX ISA Notes Introduced in PTX ISA version 7.4. Examples applypriority.global.L2::evict_normal [ptr], 128; 9.7.8.16. Data Movement and Conversion Instructions: discard \\uf0c1 discard Invalidate the data in cache at the specified address and cache level. Syntax discard{.global}.level [a], size; .level = { .L2 }; Description The discard instruction invalidates the data at the address range [a .. a + (size - 1)] in the cache level specified by the .level qualifier without writing back the data in the cache to the memory. Therefore after the discard operation, the data at the address range [a .. a+ (size - 1)] has undetermined value. The operand size is an integer constant that specifies the amount of data, in bytes, in the cache level specified by the .level qualifier to be discarded. Supported addressing modes for address operand a are described in Addresses as Operands . Examples discard.global.L2 [ptr], 128; 9.7.8.17.'},\n",
       " {'id': 636,\n",
       "  'content': 'Data Movement and Conversion Instructions: createpolicy \\uf0c1 createpolicy Create a cache eviction policy for the specified cache level. Syntax // Range-based policy createpolicy.range{.global}.level::primary_priority{.level::secondary_priority}.b64 cache-policy, [a], primary-size, total-size; // Fraction-based policy createpolicy.fractional.level::primary_priority{.level::secondary_priority}.b64 cache-policy{, fraction}; // Converting the access property from CUDA APIs createpolicy.cvt.L2.b64 cache-policy, access-property; .level::primary_priority = { .L2::evict_last, .L2::evict_normal, .L2::evict_first, .L2::evict_unchanged }; .level::secondary_priority = { .L2::evict_first, .L2::evict_unchanged }; Description The createpolicy instruction creates a cache eviction policy for the specified cache level in an opaque 64-bit register specified by the destination operand cache-policy . The cache eviction policy specifies how cache eviction priorities are applied to global memory addresses used in memory operations with .level::cache_hint qualifier. There are two types of cache eviction policies: Range-based policy The cache eviction policy created using createpolicy.range specifies the cache eviction behaviors for the following three address ranges: [a .. a + (primary-size - 1)] referred to as primary range.'},\n",
       " {'id': 637,\n",
       "  'content': '[a + primary-size .. a + (total-size - 1)] referred to as trailing secondary range. [a - (total-size - primary-size) ..'},\n",
       " {'id': 638,\n",
       "  'content': '(a - 1)] referred to as preceding secondary range. When a range-based cache eviction policy is used in a memory operation with .level::cache_hint qualifier, the eviction priorities are applied as follows: If the memory address falls in the primary range, the eviction priority specified by .L2::primary_priority is applied. If the memory address falls in any of the secondary ranges, the eviction priority specified by .L2::secondary_priority is applied. If the memory address does not fall in either of the above ranges, then the applied eviction priority is unspecified. The 32-bit operand primary-size specifies the size, in bytes, of the primary range. The 32-bit operand total-size specifies the combined size, in bytes, of the address range including primary and secondary ranges. The value of primary-size must be less than or equal to the value of total-size .'},\n",
       " {'id': 639,\n",
       "  'content': 'Maximum allowed value of total-size is 4GB. If .L2::secondary_priority is not specified, then it defaults to .L2::evict_unchanged . Fraction-based policy A memory operation with .level::cache_hint qualifier can use the fraction-based cache eviction policy to request the cache eviction priority specified by .L2:primary_priority to be applied to a fraction of cache accesses specified by the 32-bit floating point operand fraction . The remainder of the cache accesses get the eviction priority specified by .L2::secondary_priority . This implies that in a memory operation that uses a fraction-based cache policy, the memory access has a probability specified by the operand fraction of getting the cache eviction priority specified by .L2::primary_priority . The valid range of values for the operand fraction is (0.0,.., 1.0] . If the operand fraction is not specified, it defaults to 1.0. The access property created using the CUDA APIs can be converted into cache eviction policy by the instruction createpolicy.cvt . The source operand access-property is a 64-bit opaque register. Examples createpolicy.fractional.L2::evict_last.b64 policy, 1.0; createpolicy.fractional.L2::evict_last.L2::evict_unchanged.b64 policy, 0.5; createpolicy.range.L2::evict_last.L2::evict_first.b64 policy, [ptr], 0x100000, 0x200000; // access-prop is created by CUDA APIs. createpolicy.cvt.L2.b64 policy, access-prop; 9.7.8.18. Data Movement and Conversion Instructions: isspacep \\uf0c1 isspacep Query whether a generic address falls within a specified state space window. Syntax isspacep.space p, a; // result is .pred .space = { const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} }; Description Write predicate register p with 1 if generic address a falls within the specified state space window and with 0 otherwise. Destination p has type .pred ; the source address operand must be of type .u32 or .u64 . isspacep.param{::entry} returns 1 if the generic address falls within the window of Kernel Function Parameters , otherwise returns 0 . If .param is specified without any sub-qualifiers then it defaults to .param::entry . isspacep.global returns 1 for Kernel Function Parameters as .param window is contained within the .global window. Note ispacep.shared::cluster will return 1 for every shared memory address that is accessible to the threads in the cluster, whereas ispacep.shared::cta will return 1 only if the address is of a variable declared in the executing CTA. PTX ISA Notes Introduced in PTX ISA version 2.0. isspacep.const introduced in PTX ISA version 3.1. isspacep.param introduced in PTX ISA version 7.7. Support for sub-qualifier ::entry on .param space introduced in PTX ISA version 8.3. Target ISA Notes isspacep requires sm_20 or higher. isspacep.param{::entry} requires sm_70 or higher. Examples isspacep.const iscnst, cptr; isspacep.global isglbl, gptr; isspacep.local islcl, lptr; isspacep.shared isshrd, sptr; isspacep.param::entry isparam, pptr; isspacep.shared::cta isshrdcta, sptr; isspacep.shared::cluster ishrdany sptr; 9.7.8.19. Data Movement and Conversion Instructions: cvta \\uf0c1 cvta Convert address from .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space to generic, or vice-versa. Take the generic address of a variable declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space. Syntax // convert const, global, local, or shared address to generic address cvta.space.size p, a; // source address in register a cvta.space.size p, var; // get generic address of var cvta.space.size p, var+imm; // generic address of var+offset // convert generic address to const, global, local, or shared address cvta.to.space.size p, a; .space = { .const, .global, .local, .shared{::cta, ::cluster}, .param{::entry} }; .size = { .u32, .u64 }; Description Convert a const , Kernel Function Parameters ( .param ), global , local , or shared address to a generic address, or vice-versa. The source and destination addresses must be the same size. Use cvt.u32.u64 or cvt.u64.u32 to truncate or zero-extend addresses. For variables declared in .const , Kernel Function Parameters ( .param ), .global , .local , or .shared state space, the generic address of the variable may be taken using cvta . The source is either a register or a variable defined in const , Kernel Function Parameters ( .param ), global , local , or shared memory with an optional offset. When converting a generic address into a const , Kernel Function Parameters ( .param ), global , local , or shared address, the resulting address is undefined in cases where the generic address does not fall within the address window of the specified state space. A program may use isspacep to guard against such incorrect behavior. For cvta with .shared state space, the address must belong to the space specified by ::cta or ::cluster sub-qualifier, otherwise the behavior is undefined. PTX ISA Notes Introduced in PTX ISA version 2.0. cvta.const and cvta.to.const introduced in PTX ISA version 3.1. cvta.param and cvta.to.param introduced in PTX ISA version 7.7. Note: The current implementation does not allow generic pointers to const space variables in programs that contain pointers to constant buffers passed as kernel parameters. Target ISA Notes cvta requires sm_20 or higher. cvta.param{::entry} and cvta.to.param{::entry} requires sm_70 or higher. Examples cvta.const.u32 ptr,cvar; cvta.local.u32 ptr,lptr; cvta.shared::cta.u32 p,As+4; cvta.shared::cluster.u32 ptr, As; cvta.to.global.u32 p,gptr; cvta.param.u64 ptr,pvar; cvta.to.param::entry.u64 epptr, ptr; 9.7.8.20. Data Movement and Conversion Instructions: cvt \\uf0c1 cvt Convert a value from one type to another. Syntax cvt{.irnd}{.ftz}{.sat}.dtype.atype d, a; // integer rounding cvt{.frnd}{.ftz}{.sat}.dtype.atype d, a; // fp rounding cvt.frnd2{.relu}{.satfinite}.f16.f32 d, a; cvt.frnd2{.relu}{.satfinite}.f16x2.f32 d, a, b; cvt.frnd2{.relu}{.satfinite}.bf16.f32 d, a; cvt.frnd2{.relu}{.satfinite}.bf16x2.f32 d, a, b; cvt.rna{.satfinite}.tf32.f32 d, a; cvt.frnd2{.relu}.tf32.f32 d, a; cvt.rn.satfinite{.relu}.f8x2type.f32 d, a, b; cvt.rn.satfinite{.relu}.f8x2type.f16x2 d, a; cvt.rn. {.relu}.f16x2.f8x2type d, a; .irnd = { .rni, .rzi, .rmi, .rpi }; .frnd = { .rn, .rz, .rm, .rp }; .frnd2 = { .rn, .rz }; .dtype = .atype = { .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .bf16, .f16, .f32, .f64 }; .f8x2type = { .e4m3x2, .e5m2x2 }; Description Convert between different types and sizes. For .f16x2 and .bf16x2 instruction type, two inputs a and b of .f32 type are converted into .f16 or .bf16 type and the converted values are packed in the destination register d , such that the value converted from input a is stored in the upper half of d and the value converted from input b is stored in the lower half of d For .f16x2 instruction type, destination operand d has .f16x2 or .b32 type. For .bf16 instruction type, operand d has .b16 type. For .bf16x2 instruction type, operand d has .b32 type. For .tf32 instruction type, operand d has .b32 type. When converting to .e4m3x2 / .e5m2x2 data formats, the destination operand d has .b16 type. When converting two .f32 inputs to .e4m3x2 / .e5m2x2 , each input is converted to the specified format, and the converted values are packed in the destination operand d such that the value converted from input a is stored in the upper 8 bits of d and the value converted from input b is stored in the lower 8 bits of d . When converting an .f16x2 input to .e4m3x2 / .e5m2x2 , each .f16 input from operand a is converted to the specified format. The converted values are packed in the destination operand d such that the value converted from the upper 16 bits of input a is stored in the upper 8 bits of d and the value converted from the lower 16 bits of input a is stored in the lower 8 bits of d . When converting from .e4m3x2 / .e5m2x2 to .f16x2 , source operand a has .b16 type. Each 8-bit input value in operand a is converted to .f16 type. The converted values are packed in the destination operand d such that the value converted from the upper 8 bits of a is stored in the upper 16 bits of d and the value converted from the lower 8 bits of a is stored in the lower 16 bits of d . Rounding modifier is mandatory in all of the following cases: float-to-float conversions, when destination type is smaller than source type All float-to-int conversions All int-to-float conversions All conversions involving .f16x2 , .e4m3x2, .e5m2x2, .bf16x2 and .tf32 instruction types. .satfinite modifier is only supported for conversions involving the following types: .e4m3x2 and .e5m2x2 destination types. .satfinite modifier is mandatory for such conversions. .f16 , .bf16 , .f16x2 , .bf16x2 as destination types. .tf32 as destination type with rounding mode specified as round to nearest, ties away from zero. Semantics if (/* inst type is .f16x2 or .bf16x2 */) { d[31:16] = convert(a); d[15:0] = convert(b); } else { d = convert(a); } Integer Notes Integer rounding is required for float-to-integer conversions, and for same-size float-to-float conversions where the value is rounded to an integer. Integer rounding is illegal in all other instances. Integer rounding modifiers: .rni round to nearest integer, choosing even integer if source is equidistant between two integers .rzi round to nearest integer in the direction of zero .rmi round to nearest integer in direction of negative infinity .rpi round to nearest integer in direction of positive infinity In float-to-integer conversion, NaN inputs are converted to 0. For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. Modifier .ftz can only be specified when either .dtype or .atype is .f32 and applies only to single precision ( .f32 ) inputs and results. sm_1x For cvt.ftz.dtype.f32 float-to-integer conversions and cvt.ftz.f32.f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero. The optional .ftz modifier may be specified in these cases for clarity. Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush single-precision subnormal inputs or results to zero if the destination type size was 64-bits. The compiler will preserve this behavior for legacy PTX code. Saturation modifier: .sat For integer destination types, .sat limits the result to MININT..MAXINT for the size of the operation. Note that saturation applies to both signed and unsigned integer types. The saturation modifier is allowed only in cases where the destination type’s value range is not a superset of the source type’s value range; i.e., the .sat modifier is illegal in cases where saturation is not possible based on the source and destination types. For float-to-integer conversions, the result is clamped to the destination range by default; i.e, .sat is redundant. Floating Point Notes Floating-point rounding is required for float-to-float conversions that result in loss of precision, and for integer-to-float conversions. Floating-point rounding is illegal in all other instances. Floating-point rounding modifiers: .rn mantissa LSB rounds to nearest even .rna mantissa LSB rounds to nearest, ties away from zero .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity A floating-point value may be rounded to an integral value using the integer rounding modifiers (see Integer Notes). The operands must be of the same size.'},\n",
       " {'id': 640,\n",
       "  'content': 'The result is an integral value, stored in floating-point format. Modifier .ftz may be specified to flush single-precision subnormal inputs and results to sign-preserving zero. sm_1x Single-precision subnormal inputs and results are flushed to sign-preserving zero. Note: In PTX ISA versions 1.4 and earlier, the cvt instruction did not flush single-precision subnormal inputs or results to zero if either source or destination type was .f64 . Specifically, if the PTX ISA version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to sign-preserving zero only for cvt.f32.f16 , cvt.f16.f32 , and cvt.f32.f32 instructions. Saturation modifier: .sat : For floating-point destination types, .sat limits the result to the range [0.0, 1.0]. NaN results are flushed to positive zero. Applies to .f16 , .f32 , and .f64 types. .relu : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination types, .relu clamps the result to 0 if negative. NaN results are converted to canonical NaN . .satfinite : For .f16 , .f16x2 , .bf16 , .bf16x2 , .e4m3x2 , .e5m2x2 and .tf32 destination formats, if the input value is NaN , then the result is NaN in the specified destination format. If the absolute value of input (ignoring sign) is greater than MAX_NORM of the specified destination format, then the result is sign-preserved MAX_NORM of the destination format. Notes A source register wider than the specified type may be used, except when the source operand has .bf16 or .bf16x2 format. The lower n bits corresponding to the instruction-type width are used in the conversion. See Operand Size Exceeding Instruction-Type Size for a description of these relaxed type-checking rules. A destination register wider than the specified type may be used, except when the destination operand has .bf16 , .bf16x2 or .tf32 format. The result of conversion is sign-extended to the destination register width for signed integers, and is zero-extended to the destination register width for unsigned, bit-size, and floating-point types. For cvt.f32.bf16 , NaN input yields unspecified NaN . .relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats introduced in PTX ISA version 7.0. cvt.bf16. {u8/s8/u16/s16/u32/s32/u64/s64/f16/f64/bf16} , cvt. {u8/s8/u16/s16/u32/s32/u64/s64/f16/f64}.bf16 , and cvt.tf32.f32.{relu}. {rn/rz} introduced in PTX ISA 7.8. cvt with .e4m3x2 / .e5m2x2 for sm_90 or higher introduced in PTX ISA version 7.8. cvt.satfinite. {e4m3x2, e5m2x2}. {f32, f16x2} for sm_90 or higher introduced in PTX ISA version 7.8. cvt with .e4m3x2 / .e5m2x2 for sm_89 introduced in PTX ISA version 8.1. {f32, f16x2} for sm_89 introduced in PTX ISA version 8.1. {f16, bf16, f16x2, bf16x2, tf32}.f32 introduced in PTX ISA version 8.1. Target ISA Notes cvt to or from .f64 requires sm_13 or higher. .relu modifier and { .f16x2 , .bf16 , .bf16x2 , .tf32 } destination formats require sm_80 or higher. {rn/rz} require sm_90 or higher. cvt with .e4m3x2 / .e5m2x2 requires sm89 or higher. {f32, f16x2} requires sm_89 or higher. Examples cvt.f32.s32 f,i; cvt.s32.f64 j,r; // float-to-int saturates by default cvt.rni.f32.f32 x,y; // round to nearest int, result is fp cvt.f32.f32 x,y; // note .ftz behavior for sm_1x targets cvt.rn.relu.f16.f32 b, f; // result is saturated with .relu saturation mode cvt.rz.f16x2.f32 b1, f, f1; // convert two fp32 values to packed fp16 outputs cvt.rn.relu.satfinite.f16x2.f32 b1, f, f1; // convert two fp32 values to packed fp16 outputs with .relu saturation on each output cvt.rn.bf16.f32 b, f; // convert fp32 to bf16 cvt.rz.relu.satfinite.bf16.f3 2 b, f; // convert fp32 to bf16 with .relu and .satfinite saturation cvt.rz.satfinite.bf16x2.f32 b1, f, f1; // convert two fp32 values to packed bf16 outputs cvt.rn.relu.bf16x2.f32 b1, f, f1; // convert two fp32 values to packed bf16 outputs with .relu saturation on each output cvt.rna.satfinite.tf32.f32 b1, f; // convert fp32 to tf32 format cvt.rn.relu.tf32.f32 d, a; // convert fp32 to tf32 format cvt.f64.bf16.rp f, b; // convert bf16 to f64 format cvt.bf16.f16.rz b, f // convert f16 to bf16 format cvt.bf16.u64.rz b, u // convert u64 to bf16 format cvt.s8.bf16.rpi s, b // convert bf16 to s8 format cvt.bf16.bf16.rpi b1, b2 // convert bf16 to corresponding int represented in bf16 format cvt.rn.satfinite.e4m3x2.f32 d, a, b; // convert a, b to .e4m3 and pack as .e4m3x2 output cvt.rn.relu.satfinite.e5m2x2.f16x2 d, a; // unpack a and convert the values to .e5m2 outputs with .relu // saturation on each output and pack as .e5m2x2 cvt.rn.f16x2.e4m3x2 d, a; // unpack a, convert two .e4m3 values to packed f16x2 output 9.7.8.21. Data Movement and Conversion Instructions: cvt.pack \\uf0c1 cvt.pack Convert two integer values from one integer type to another and pack the results.'},\n",
       " {'id': 641,\n",
       "  'content': 'Syntax cvt.pack.sat.convertType.abType d, a, b; .convertType = { .u16, .s16 } .abType = { .s32 } cvt.pack.sat.convertType.abType.cType d, a, b, c; .convertType = { .u2, .s2, .u4, .s4, .u8, .s8 } .abType = { .s32 } .cType = { .b32 } Description Convert two 32-bit integers a and b into specified type and pack the results into d . Destination d is an unsigned 32-bit integer. Source operands a and b are integers of type .abType and the source operand c is an integer of type .cType . The inputs a and b are converted to values of type specified by .convertType with saturation and the results after conversion are packed into lower bits of d . If operand c is specified then remaining bits of d are copied from lower bits of c . Semantics ta = a MAX(convertType) ?'},\n",
       " {'id': 642,\n",
       "  'content': 'MAX(convertType) : a; tb = b MAX(convertType) ? MAX(convertType) : b; size = sizeInBits(convertType); td = tb ; for (i = size; i .shared::cluster: cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar]; cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster [dstMem], [srcMem], size, [mbar], ctaMask; cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint [dstMem], [srcMem], size, [mbar], cache-policy; // .shared::cta -> .shared::cluster (strictly remote): cp.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes [dstMem], [srcMem], size, [mbar]; // .shared::cta -> .global: cp.async.bulk.global.shared::cta.bulk_group [dstMem], [srcMem], size; cp.async.bulk.global.shared::cta.bulk_group.L2::cache_hint} [dstMem], [srcMem], size, cache-policy; 9.7.8.24.7. Data Movement and Conversion Instructions: cp.reduce.async.bulk \\uf0c1 cp.reduce.async.bulk Initiates an asynchronous reduction operation. Syntax cp.reduce.async.bulk.dst.src.completion_mechanism.redOp.type [dstMem], [srcMem], size, [mbar] .dst = { .shared::cluster } .src = { .shared::cta } .completion_mechanism = { .mbarrier::complete_tx::bytes } .redOp= { .and, .or, .xor, .add, .inc, .dec, .min, .max } .type = { .b32, .u32, .s32, .b64, .u64 } cp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.redOp.type [dstMem], [srcMem], size{, cache-policy} .dst = { .global } .src = { .shared::cta } .completion_mechanism = { .bulk_group } .level::cache_hint = { .L2::cache_hint } .redOp= { .and, .or, .xor, .add, .inc, .dec, .min, .max } .type = { .f16, .bf16, .b32, .u32, .s32, .b64, .u64, .s64, .f32, .f64 } cp.reduce.async.bulk.dst.src.completion_mechanism{.level::cache_hint}.add.noftz.type [dstMem], [srcMem], size{, cache-policy} .dst = { .global } .src = { .shared::cta } .completion_mechanism = { .bulk_group } .type = { .f16, .bf16 } Description cp.reduce.async.bulk is a non-blocking instruction which initiates an asynchronous reduction operation on an array of memory locations specified by the destination address operand dstMem with the source array whose location is specified by the source address operand srcMem . The size of the source and the destination array must be the same and is specified by the operand size .'},\n",
       " {'id': 643,\n",
       "  'content': 'Each data element in the destination array is reduced inline with the corresponding data element in the source array with the reduction operation specified by the modifier .redOp . The type of each data element in the source and the destination array is specified by the modifier .type . The source address operand srcMem is located in the state space specified by .src and the destination address operand dstMem is located in the state specified by the .dst . The 32-bit operand size specifies the amount of memory to be copied from the source location and used in the reduction operation, in terms of number of bytes. size must be a multiple of 16. If the value is not a multiple of 16, then the behavior is undefined. The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory space and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory space. The addresses dstMem and srcMem must be aligned to 16 bytes. The operations supported by .redOp are classified as follows: The bit-size operations are .and , .or , and .xor . The integer operations are .add , .inc , .dec , .min , and .max . The .inc and .dec operations return a result in the range [0..x] where x is the value at the source state space. The floating point operation .add rounds to the nearest even. The current implementation of cp.reduce.async.bulk.add.f32 flushes subnormal inputs and results to sign-preserving zero. The cp.reduce.async.bulk.add.f16 and cp.reduce.async.bulk.add.bf16 operations require .noftz qualifier. It preserves input and result subnormals, and does not flush them to zero. The following table describes the valid combinations of .redOp and element type: .dst .redOp Element type .shared::cluster .add .u32 , .s32 , .u64 .min , .max .u32 , .s32 .inc , .dec .u32 .and , .or , .xor .b32 .global .add .u32 , .s32 , .u64 , .f32 , .f64 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the instruction variant. The completion mechanisms that are supported for different variants are summarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .shared::cluster .shared::cta .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.reduce.async.bulk variant uses mbarrier based completion mechanism. The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be performed on the mbarrier object specified by the operand mbar . The modifier .bulk_group specifies that the cp.reduce.async.bulk variant uses bulk async-group based completion mechanism. The qualifier .level::cache_hint is only supported when at least one of the .src or .dst statespaces is .global state space. Each reduction operation performed by the cp.reduce.async.bulk has individually .relaxed.gpu memory ordering semantics. The load operations in cp.reduce.async.bulk are treated as weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.0. Examples cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.add.u64 [dstMem], [srcMem], size, [mbar]; cp.reduce.async.bulk.shared::cluster.shared::cta.mbarrier::complete_tx::bytes.min.s32 [dstMem], [srcMem], size, [mbar]; cp.reduce.async.bulk.global.shared::cta.bulk_group.min.f16 [dstMem], [srcMem], size; cp.reduce.async.bulk.global.shared::cta.bulk_group.L2::cache_hint.xor.s32 [dstMem], [srcMem], size, policy; cp.reduce.async.bulk.global.shared::cta.bulk_group.add.noftz.f16 [dstMem], [srcMem], size; 9.7.8.24.8. Data Movement and Conversion Instructions: cp.async.bulk.prefetch \\uf0c1 cp.async.bulk.prefetch Provides a hint to the system to initiate the asynchronous prefetch of data to the cache. Syntax cp.async.bulk.prefetch.L2.src{.level::cache_hint} [srcMem], size {, cache-policy} .src = { .global } .level::cache_hint = { .L2::cache_hint } Description cp.async.bulk.prefetch is a non-blocking instruction which may initiate an asynchronous prefetch of data from the location specified by source address operand srcMem , in .src statespace, to the L2 cache. The 32-bit operand size specifies the amount of memory to be prefetched in terms of number of bytes. The address srcMem must be aligned to 16 bytes. Examples cp.async.bulk.prefetch.L2.global [srcMem], size; cp.async.bulk.prefetch.L2.global.L2::cache_hint [srcMem], size, policy; 9.7.8.24.9. Data Movement and Conversion Instructions: cp.async.bulk.tensor \\uf0c1 cp.async.bulk.tensor Initiates an asynchronous copy operation on the tensor data from one state space to another. Syntax // global -> shared::cluster: cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.multicast}{.level::cache_hint} [dstMem], [tensorMap, tensorCoords], [mbar]{, im2colOffsets} {, ctaMask} {, cache-policy} .dst = { .shared::cluster } .src = { .global } .dim = { .1d, .2d, .3d, .4d, .5d } .completion_mechanism = { .mbarrier::complete_tx::bytes } .load_mode = { .tile, .im2col } .level::cache_hint = { .L2::cache_hint } .multicast = { .multicast::cluster } // shared::cta -> global: cp.async.bulk.tensor.dim.dst.src{.load_mode}.completion_mechanism{.level::cache_hint} [tensorMap, tensorCoords], [srcMem] {, cache-policy} .dst = { .global } .src = { .shared::cta } .dim = { .1d, .2d, .3d, .4d, .5d } .completion_mechanism = { .bulk_group } .load_mode = { .tile, .im2col_no_offs } .level::cache_hint = { .L2::cache_hint } Description cp.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous copy operation of tensor data from the location in .src state space to the location in the .dst state space. The operand dstMem specifies the location in the .dst state space into which the tensor data has to be copied and srcMem specifies the location in the .src state space from which the tensor data has to be copied.'},\n",
       " {'id': 644,\n",
       "  'content': 'The operand tensorMap is the generic address of the opaque tensor-map object which resides either in .param space or .const space or .global space. The operand tensorMap specifies the properties of the tensor copy operation, as described in Tensor-map . The tensorMap is accessed in tensormap proxy. Refer to the CUDA programming guide for creating the tensor-map objects on the host side. The dimension of the tensor data is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates in the tensor data in the global memory from or to which the copy operation has to be performed. The number of tensor coordinates in the vector argument tensorCoords should be equal to the dimension specified by the modifier .dim . The individual tensor coordinates in tensorCoords are of type .s32 . The modifier .completion_mechanism specifies the completion mechanism that is supported on the instruction variant. The completion mechanisms that are supported for different variants are summarized in the following table: Completion mechanism .dst .src Description .mbarrier::... .shared::cluster .global mbarrier based completion mechanism .bulk_group .global .shared::cta Bulk async-group based completion mechanism The modifier .mbarrier::complete_tx::bytes specifies that the cp.async.bulk.tensor variant uses mbarrier based completion mechanism. The modifier .bulk_group specifies that the cp.async.bulk.tensor variant uses bulk async-group based completion mechanism. The qualifier .load_mode specifies how the data in the source location is copied into the destination location. If .load_mode is not specified, it defaults to .tile . In .tile mode, the multi-dimensional layout of the source tensor is preserved at the destination. In .im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column at the destination. Details of the im2col mode are described in Im2col mode . In .im2col mode, the tensor has to be at least 3-dimensional. The vector operand im2colOffsets can be specified only when .load_mode is .im2col. The length of the vector operand im2colOffsets is two less than the number of dimension .dim of the tensor operation. The modifier .im2col_no_offs is the same as .im2col mode except there is no im2colOffsets vector involved. The optional modifier .multicast::cluster allows copying of data from global memory to shared memory of multiple CTAs in the cluster. Operand ctaMask specifies the destination CTAs in the cluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid of the destination CTA. The source data is multicast to the same offset as dstMem in the shared memory of each destination CTA. The mbarrier signal is also multicast to the same offset as mbar in the shared memory of the destination CTA. The copy operation in cp.async.bulk.tensor is treated as a weak memory operation and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . Notes .multicast::cluster qualifier is optimized for target architecture sm_90a and may have substantially reduced performance on other targets and hence .multicast::cluster is advised to be used with .target sm_90a . .multicast::cluster qualifier advised to be used with .target sm_90a . Examples .reg .b16 ctaMask; .reg .u16 i2cOffW, i2cOffH, i2cOffD; .reg .b64 l2CachePolicy; cp.async.bulk.tensor.1d.shared::cluster.global.tile [sMem0], [tensorMap0, {tc0}], [mbar0]; @p cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster [sMem1], [tensorMap1, {tc0, tc1}], [mbar2], ctaMask; @p cp.async.bulk.tensor.5d.shared::cluster.global.im2col.mbarrier::complete_tx::bytes [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD}; @p cp.async.bulk.tensor.3d.im2col.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint [sMem3], [tensorMap3, {tc0, tc1, tc2}], [mbar3], {i2cOffW}, policy; @p cp.async.bulk.tensor.1d.global.shared::cta.bulk_group [tensorMap3, {tc0}], [sMem3]; 9.7.8.24.10. Data Movement and Conversion Instructions: cp.reduce.async.bulk.tensor \\uf0c1 cp.reduce.async.bulk.tensor Initiates an asynchronous reduction operation on the tensor data. Syntax // shared::cta -> global: cp.reduce.async.bulk.tensor.dim.dst.src.redOp{.load_mode}.completion_mechanism{.level::cache_hint} [tensorMap, tensorCoords], [srcMem] {,cache-policy} .dst = { .global } .src = { .shared::cta } .dim = { .1d, .2d, .3d, .4d, .5d } .completion_mechanism = { .bulk_group } .load_mode = { .tile, .im2col_no_offs } .redOp = { .add, .min, .max, .inc, .dec, .and, .or, .xor} Description cp.reduce.async.bulk.tensor is a non-blocking instruction which initiates an asynchronous reduction operation of tensor data in the .dst state space with tensor data in the .src state space. The operand srcMem specifies the location of the tensor data in the .src state space using which the reduction operation has to be performed. Each element of the tensor data in the .dst state space is reduced inline with the corresponding element from the tensor data in the .src state space. The modifier .redOp specifies the reduction operation used for the inline reduction. The type of each tensor data element in the source and the destination tensor is specified in Tensor-map . The dimension of the tensor is specified by the .dim modifier. The vector operand tensorCoords specifies the starting coordinates of the tensor data in the global memory on which the reduce operation is to be performed. The individual tensor coordinates are of the type .s32 . The following table describes the valid combinations of .redOp and element type: .redOp Element type .add .u32 , .s32 , .u64 , .f32 , .f16 , .bf16 .min , .max .u32 , .s32 , .u64 , .s64 , .f16 , .bf16 .inc , .dec .u32 .and , .or , .xor .b32 , .b64 The modifier .completion_mechanism specifies the completion mechanism that is supported on the instruction variant. Value .bulk_group of the modifier .completion_mechanism specifies that cp.reduce.async.bulk.tensor instruction uses bulk async-group based completion mechanism. In .im2col_no_offs mode, some dimensions of the source tensors are unrolled in a single dimensional column at the destination. Each reduction operation performed by cp.reduce.async.bulk.tensor has individually .relaxed.gpu memory ordering semantics. The load operations in cp.reduce.async.bulk.tensor are treated as weak memory operations and the complete-tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . Examples cp.reduce.async.bulk.tensor.1d.global.shared::cta.add.tile.bulk_group [tensorMap0, {tc0}], [sMem0]; cp.reduce.async.bulk.tensor.2d.global.shared::cta.and.bulk_group.L2::cache_hint [tensorMap1, {tc0, tc1}], [sMem1] , policy; cp.reduce.async.bulk.tensor.3d.global.shared::cta.xor.im2col.bulk_group [tensorMap2, {tc0, tc1, tc2}], [sMem2] 9.7.8.24.11. Data Movement and Conversion Instructions: cp.async.bulk.prefetch.tensor \\uf0c1 cp.async.bulk.prefetch.tensor Provides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache. Syntax // global -> shared::cluster: cp.async.bulk.prefetch.tensor.dim.L2.src{.load_mode}{.level::cache_hint} [tensorMap, tensorCoords] {, im2colOffsets } {, cache-policy} .src = { .global } .dim = { .1d, .2d, .3d, .4d, .5d } .load_mode = { .tile, .im2col } .level::cache_hint = { .L2::cache_hint } Description cp.async.bulk.prefetch.tensor is a non-blocking instruction which may initiate an asynchronous prefetch of tensor data from the location in .src statespace to the L2 cache. cp.async.bulk.prefetch.tensor is treated as a weak memory operation in the Memory Consistency Model . Examples .reg .b16 ctaMask; .reg .u16 i2cOffW, i2cOffH, i2cOffD; .reg .b64 l2CachePolicy; cp.async.bulk.prefetch.tensor.1d.L2.global.tile [tensorMap0, {tc0}]; @p cp.async.bulk.prefetch.tensor.2d.L2.global [tensorMap1, {tc0, tc1}]; @p cp.async.bulk.prefetch.tensor.5d.L2.global.im2col [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], {i2cOffW, i2cOffH, i2cOffD}; @p cp.async.bulk.prefetch.tensor.3d.L2.global.im2col.L2::cache_hint [tensorMap3, {tc0, tc1, tc2}], {i2cOffW}, policy; 9.7.8.24.12. Data Movement and Conversion Instructions: cp.async.bulk.commit_group \\uf0c1 cp.async.bulk.commit_group Commits all prior initiated but uncommitted cp.async.bulk instructions into a cp.async.bulk-group .'},\n",
       " {'id': 645,\n",
       "  'content': \"Syntax cp.async.bulk.commit_group; Description cp.async.bulk.commit_group instruction creates a new per-thread bulk async-group and batches all prior cp{.reduce}.async.bulk. {.prefetch}{.tensor} instructions satisfying the following conditions into the new bulk async-group : The prior cp{.reduce}.async.bulk. {.prefetch}{.tensor} instructions use bulk_group based completion mechanism, and They are initiated by the executing thread but not committed to any bulk async-group . If there are no uncommitted cp{.reduce}.async.bulk. {.prefetch}{.tensor} instructions then cp.async.bulk.commit_group results in an empty bulk async-group . An executing thread can wait for the completion of all cp{.reduce}.async.bulk. {.prefetch}{.tensor} operations in a bulk async-group using cp.async.wait_group . There is no memory ordering guarantee provided between any two cp{.reduce}.async.bulk. {.prefetch}{.tensor} operations within the same bulk async-group . Examples cp.async.bulk.commit_group; 9.7.8.24.13. Data Movement and Conversion Instructions: cp.async.bulk.wait_group \\uf0c1 cp.async.bulk.wait_group Wait for completion of bulk async-groups . Syntax cp.async.bulk.wait_group{.read} N; Description cp.async.bulk.wait_group instruction will cause the executing thread to wait until only N or fewer of the most recent bulk async-groups are pending and all the prior bulk async-groups committed by the executing threads are complete. For example, when N is 0, the executing thread waits on all the prior bulk async-groups to complete. Operand N is an integer constant. By default, cp.async.bulk.wait_group instruction will cause the executing thread to wait till all the bulk async operations in the specified bulk async-group have completed all of the following: Reading from the source locations. Writing to their respective destination locations. Writes being made visible to the executing thread. The optional .read modifier indicates that the waiting has to be done until all the bulk async operations in the specified bulk async-group have completed reading from their source locations. Examples cp.async.bulk.wait_group.read 0; cp.async.bulk.wait_group 2; 9.7.8.25. Data Movement and Conversion Instructions: tensormap.replace \\uf0c1 tensormap.replace Modifies the field of a tensor-map object. Syntax tensormap.replace.mode.field1{.ss}.b1024.type [addr], new_val; tensormap.replace.mode.field2{.ss}.b1024.type [addr], ord, new_val; tensormap.replace.mode.field3{.ss}.b1024.type [addr], new_val; .mode = { .tile } .field1 = { .global_address, .rank } .field2 = { .box_dim, .global_dim, .global_stride, .element_stride } .field3 = { .elemtype, .interleave_layout, .swizzle_mode, .fill_mode } .ss = { .global, .shared::cta } .type = { .b32, .b64 } Description The tensormap.replace instruction replaces the field, specified by .field qualifier, of the tensor-map object at the location specified by the address operand addr with a new value. The new value is specified by the argument new_val . Qualifier .mode specifies the mode of the tensor-map object located at the address operand addr . Instruction type .b1024 indicates the size of the tensor-map object, which is 1024 bits. Operand new_val has the type .type . When .field is specified as .global_address or .global_stride , .type must be .b64 . Otherwise, .type must be .b32 . The immediate integer operand ord specifies the ordinal of the field across the rank of the tensor which needs to be replaced in the tensor-map object. For field .rank , the operand new_val must be ones less than the desired tensor rank as this field uses zero-based numbering. When .field3 is specified, the operand new_val must be an immediate and the Table 30 shows the mapping of the operand new_val across various fields. Table 30 Tensormap new_val validity \\uf0c1 new_val .field3 .elemtype .interleave_layout .swizzle_mode .fill_mode 0 .u8 No interleave No swizzling Zero fill 1 .u16 16B interleave 32B swizzling OOB-NaN fill 2 .u32 32B interleave 64B swizzling x 3 .s32 x 128B swizzling x 4 .u64 x x x 5 .s64 x x x 6 .f16 x x x 7 .f32 x x x 8 .f32.ftz x x x 9 .f64 x x x 10 .bf16 x x x 11 .tf32 x x x 12 .tf32.ftz x x x If no state space is specified then Generic Addressing is used. If the address specified by addr does not fall within the address window of .global or .shared::cta state space then the behavior is undefined. tensormap.replace is treated as a weak memory operation, on the entire 1024-bit opaque tensor-map object, in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 8.3. Target ISA Notes Requires sm_90a . Examples tensormap.replace.tile.global_address.shared::cta.b1024.b64 [sMem], new_val; 9.7.9. Texture Instructions \\uf0c1 This section describes PTX instructions for accessing textures and samplers. PTX supports the following operations on texture and sampler descriptors: Static initialization of texture and sampler descriptors. Module-scope and per-entry scope definitions of texture and sampler descriptors. Ability to query fields within texture and sampler descriptors. 9.7.9.1. Texturing Modes \\uf0c1 For working with textures and samplers, PTX has two modes of operation. The advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior to sm_3x ), with the restriction that they correspond 1-to-1 with the 256 possible textures per kernel (128 for architectures prior to sm_3x ). The advantage of independent mode is that textures and samplers can be mixed and matched, but the number of samplers is greatly restricted to 32 per kernel (16 for architectures prior to sm_3x ). Table 31 summarizes the number of textures, samplers and surfaces available in different texturing modes. Table 31 Texture, sampler and surface limits \\uf0c1 Texturing mode Resource sm_1x , sm_2x sm_3x+ Unified mode Textures 128 256 Samplers 128 256 Surfaces 8 16 Independent mode Textures 128 256 Samplers 16 32 Surfaces 8 16 The texturing mode is selected using .target options texmode_unified and texmode_independent . A PTX module may declare only one texturing mode. If no texturing mode is declared, the module is assumed to use unified mode. Example : calculate an element’s power contribution as element’s power/total number of elements. .target texmode_independent .global .samplerref tsamp1 = { addr_mode_0 = clamp_to_border, filter_mode = nearest }; ... .entry compute_power ( .param .texref tex1 ) { txq.width.b32 r6, [tex1]; // get tex1's width txq.height.b32 r5, [tex1]; // get tex1's height tex.2d.v4.f32.f32 {r1,r2,r3,r4}, [tex1, tsamp1, {f1,f2}]; mul.u32 r5, r5, r6; add.f32 r1, r1, r2; add.f32 r3, r3, r4; add.f32 r1, r1, r3; cvt.f32.u32 r5, r5; div.f32 r1, r1, r5; } 9.7.9.2. Mipmaps \\uf0c1 A mipmap is a sequence of textures, each of which is a progressively lower resolution representation of the same image. The height and width of each image, or level of detail (LOD), in the mipmap is a power of two smaller than the previous level. Mipmaps are used in graphics applications to improve rendering speed and reduce aliasing artifacts. For example, a high-resolution mipmap image is used for objects that are close to the user; lower-resolution images are used as the object appears farther away. Mipmap filtering modes are provided when switching between two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity. Example: If the texture has a basic size of 256 by 256 pixels, then the associated mipmap set may contain a series of eight images, each one-fourth the total area of the previous one: 128×128 pixels, 64×64, 32×32, 16×16, 8×8, 4×4, 2×2, 1×1 (a single pixel). If, for example, a scene is rendering this texture in a space of 40×40 pixels, then either a scaled up version of the 32×32 (without trilinear interpolation) or an interpolation of the 64×64 and the 32×32 mipmaps (with trilinear interpolation) would be used. The total number of LODs in a complete mipmap pyramid is calculated through the following equation: numLODs = 1 + floor(log2(max(w, h, d))) The finest LOD is called the base level and is the 0th level. The next (coarser) level is the 1st level, and so on. The coarsest level is the level of size (1 x 1 x 1). Each successively smaller mipmap level has half the {width, height, depth} of the previous level, but if this half value is a fractional value, it’s rounded down to the next largest integer. Essentially, the size of a mipmap level can be specified as: max(1, floor(w_b / 2^i)) x max(1, floor(h_b / 2^i)) x max(1, floor(d_b / 2^i)) where i is the ith level beyond the 0th level (the base level). And w_b , h_b and d_b are the width, height and depth of the base level respectively. PTX support for mipmaps The PTX tex instruction supports three modes for specifying the LOD: base , level , and grad ient. In base mode, the instruction always picks level 0. In level mode, an additional argument is provided to specify the LOD to fetch from. In gradmode, two floating-point vector arguments provide partials (e.g., {ds/dx, dt/dx} and {ds/dy, dt/dy} for a 2d texture), which the tex instruction uses to compute the LOD. These instructions provide access to texture memory. tex tld4 txq 9.7.9.3. Texture Instructions: tex \\uf0c1 tex Perform a texture memory lookup. Syntax tex.geom.v4.dtype.ctype d, [a, c] {, e} {, f}; tex.geom.v4.dtype.ctype d[|p], [a, b, c] {, e} {, f}; // explicit sampler tex.geom.v2.f16x2.ctype d[|p], [a, c] {, e} {, f}; tex.geom.v2.f16x2.ctype d[|p], [a, b, c] {, e} {, f}; // explicit sampler // mipmaps tex.base.geom.v4.dtype.ctype d[|p], [a, {b,} c] {, e} {, f}; tex.level.geom.v4.dtype.ctype d[|p], [a, {b,} c], lod {, e} {, f}; tex.grad.geom.v4.dtype.ctype d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f}; tex.base.geom.v2.f16x2.ctype d[|p], [a, {b,} c] {, e} {, f}; tex.level.geom.v2.f16x2.ctype d[|p], [a, {b,} c], lod {, e} {, f}; tex.grad.geom.v2.f16x2.ctype d[|p], [a, {b,} c], dPdx, dPdy {, e} {, f}; .geom = { .1d, .2d, .3d, .a1d, .a2d, .cube, .acube, .2dms, .a2dms }; .dtype = { .u32, .s32, .f16, .f32 }; .ctype = { .s32, .f32 }; // .cube, .acube require .f32 // .2dms, .a2dms require .s32 Description tex. {1d,2d,3d} Texture lookup using a texture coordinate vector.\"},\n",
       " {'id': 646,\n",
       "  'content': 'The instruction loads data from the texture named by operand a at coordinates given by operand c into destination d . Operand c is a scalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a four-element vector for 3d textures, where the fourth element is ignored. An optional texture sampler b may be specified. If no sampler is specified, the sampler behavior is a property of the named texture. The optional destination predicate p is set to True if data from texture at specified coordinates is resident in memory, False otherwise. When optional destination predicate p is set to False , data loaded will be all zeros. Memory residency of Texture Data at specified coordinates is dependent on execution environment setup using Driver API calls, prior to kernel launch. Refer to Driver API documentation for more details including any system/implementation specific behavior.'},\n",
       " {'id': 647,\n",
       "  'content': 'An optional operand e may be specified. Operand e is a vector of .s32 values that specifies coordinate offset. Offset is applied to coordinates before doing texture lookup. Offset value is in the range of -8 to +7. Operand e is a singleton tuple for 1d textures; is a two element vector 2d textures; and is four-element vector for 3d textures, where the fourth element is ignored. An optional operand f may be specified for depth textures . Depth textures are special type of textures which hold data from the depth buffer. Depth buffer contains depth information of each pixel. Operand f is .f32 scalar value that specifies depth compare value for depth textures. Each element fetched from texture is compared against value given in f operand. If comparison passes, result is 1.0; otherwise result is 0.0. These per-element comparison results are used for the filtering. When using depth compare operand, the elements in texture coordinate vector c have .f32 type. Depth compare operand is not supported for 3d textures. The instruction returns a two-element vector for destination type .f16x2 .'},\n",
       " {'id': 648,\n",
       "  'content': 'For all other destination types, the instruction returns a four-element vector. Coordinates may be given in either signed 32-bit integer or 32-bit floating point form. A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the coordinate vector must be naturally aligned to a multiple of the access size. If an address is not properly aligned, the resulting behavior is undefined; i.e., the access may proceed by silently masking off low-order address bits to achieve proper rounding, or the instruction may fault. tex. {a1d,a2d} Texture array selection, followed by texture lookup. The instruction first selects a texture from the texture array named by operand a using the index given by the first element of the array coordinate vector c . The instruction then loads data from the selected texture at coordinates given by the remaining elements of operand c into destination d . Operand c is a bit-size type vector or tuple containing an index into the array of textures followed by coordinates within the selected texture, as follows: For 1d texture arrays, operand c has type .v2.b32 . The first element is interpreted as an unsigned integer index ( .u32 ) into the texture array, and the second element is interpreted as a 1d texture coordinate of type .ctype . For 2d texture arrays, operand c has type .v4.b32 . The first element is interpreted as an unsigned integer index ( .u32 ) into the texture array, and the next two elements are interpreted as 2d texture coordinates of type .ctype . The fourth element is ignored. Operand e is a singleton tuple for 1d texture arrays; and is a two element vector 2d texture arrays. An optional operand f may be specified for depth textures arrays. When using depth compare operand, the coordinates in texture coordinate vector c have .f32 type. The texture array index is a 32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point values. tex.cube Cubemap texture lookup. The instruction loads data from the cubemap texture named by operand a at coordinates given by operand c into destination d . Cubemap textures are special two-dimensional layered textures consisting of six layers that represent the faces of a cube. All layers in a cubemap are of the same size and are square (i.e., width equals height). When accessing a cubemap, the texture coordinate vector c has type .v4.f32 , and comprises three floating-point coordinates ( s , t , r ) and a fourth padding argument which is ignored. Coordinates ( s , t , r ) are projected onto one of the six cube faces. The ( s , t , r ) coordinates can be thought of as a direction vector emanating from the center of the cube. Of the three coordinates ( s , t , r ), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by the absolute value of the major axis to produce a new ( s , t ) coordinate pair to lookup into the selected cube face. Offset vector operand e is not supported for cubemap textures. an optional operand f may be specified for cubemap depth textures. operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures. tex.acube Cubemap array selection, followed by cubemap lookup. The instruction first selects a cubemap texture from the cubemap array named by operand a using the index given by the first element of the array coordinate vector c . The instruction then loads data from the selected cubemap texture at coordinates given by the remaining elements of operand c into destination d . Cubemap array textures consist of an array of cubemaps, i.e., the total number of layers is a multiple of six. When accessing a cubemap array texture, the coordinate vector c has type .v4.b32 . The first element is interpreted as an unsigned integer index ( .u32 ) into the cubemap array, and the remaining three elements are interpreted as floating-point cubemap coordinates ( s , t , r ), used to lookup in the selected cubemap as described above. Offset vector operand e is not supported for cubemap texture arrays. An optional operand f may be specified for cubemap depth texture arrays. Operand f is .f32 scalar value that specifies depth compare value for cubemap depth textures. tex.2dms Multi-sample texture lookup using a texture coordinate vector. Multi-sample textures consist of multiple samples per data element. The instruction loads data from the texture named by operand a from sample number given by first element of the operand c , at coordinates given by remaining elements of operand c into destination d . When accessing a multi-sample texture, texture coordinate vector c has type .v4.b32 . The first element in operand c is interpreted as unsigned integer sample number ( .u32 ), and the next two elements are interpreted as signed integer ( .s32 ) 2d texture coordinates. Operand e is a vector of type .v2.s32 that specifies coordinate offset. Depth compare operand f is not supported for multi-sample textures. tex.a2dms Multi-sample texture array selection, followed by multi-sample texture lookup. The instruction first selects a multi-sample texture from the multi-sample texture array named by operand a using the index given by the first element of the array coordinate vector c . The instruction then loads data from the selected multi-sample texture from sample number given by second element of the operand c , at coordinates given by remaining elements of operand c into destination d . When accessing a multi-sample texture array, texture coordinate vector c has type .v4.b32 . The first element in operand c is interpreted as unsigned integer sampler number, the second element is interpreted as unsigned integer index ( .u32 ) into the multi-sample texture array and the next two elements are interpreted as signed integer ( .s32 ) 2d texture coordinates. Operand e is a vector of type .v2.s32 values that specifies coordinate offset. Depth compare operand f is not supported for multi-sample texture arrays. Mipmaps .base (lod zero) Pick level 0 (base level).'},\n",
       " {'id': 649,\n",
       "  'content': 'This is the default if no mipmap mode is specified. No additional arguments. .level (lod explicit) Requires an additional 32-bit scalar argument, lod , which contains the LOD to fetch from. The type of lod follows .ctype (either .s32 or .f32 ). Geometries .2dms and .a2dms are not supported in this mode. .grad (lod gradient) Requires two .f32 vectors, dPdx and dPdy , that specify the partials. The vectors are singletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are four-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d and cube geometries. For mipmap texture lookup, an optional operand e may be specified. Operand e is a vector of .s32 that specifies coordinate offset. Offset vector operand is not supported for cube and cubemap geometries. An optional operand f may be specified for mipmap textures. Indirect texture access Beginning with PTX ISA version 3.1, indirect texture access is supported in unified mode for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of a .texref variable. Notes For compatibility with prior versions of PTX, the square brackets are not required and .v4 coordinate vectors are allowed for any geometry, with the extra elements being ignored. PTX ISA Notes Unified mode texturing introduced in PTX ISA version 1.0. Extension using opaque .texref and .samplerref types and independent mode texturing introduced in PTX ISA version 1.5. Texture arrays tex. {a1d,a2d} introduced in PTX ISA version 2.3. Cubemaps and cubemap arrays introduced in PTX ISA version 3.0. Support for mipmaps introduced in PTX ISA version 3.1. Indirect texture access introduced in PTX ISA version 3.1. Multi-sample textures and multi-sample texture arrays introduced in PTX ISA version 3.2. Support for textures returning .f16 and .f16x2 data introduced in PTX ISA version 4.2. Support for tex.grad. {cube, acube} introduced in PTX ISA version 4.3. Offset vector operand introduced in PTX ISA version 4.3. Depth compare operand introduced in PTX ISA version 4.3. Support for optional destination predicate introduced in PTX ISA version 7.1. The cubemap array geometry ( .acube ) requires sm_20 or higher. Mipmaps require sm_20 or higher. Indirect texture access requires sm_20 or higher. Multi-sample textures and multi-sample texture arrays require sm_30 or higher. Texture fetch returning .f16 and .f16x2 data require sm_53 or higher. tex.grad. {cube, acube} requires sm_20 or higher. Offset vector operand requires sm_30 or higher. Depth compare operand requires sm_30 or higher. Support for optional destination predicate requires sm_60 or higher. Examples // Example of unified mode texturing // - f4 is required to pad four-element tuple and is ignored tex.3d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,{f1,f2,f3,f4}]; // Example of independent mode texturing tex.1d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,smpl_x,{f1}]; // Example of 1D texture array, independent texturing mode tex.a1d.v4.s32.s32 {r1,r2,r3,r4}, [tex_a,smpl_x,{idx,s1}]; // Example of 2D texture array, unified texturing mode // - f3 is required to pad four-element tuple and is ignored tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2,f3}]; // Example of cubemap array, unified textureing mode tex.acube.v4.f32.f32 {r0,r1,r2,r3}, [tex_cuarray,{idx,f1,f2,f3}]; // Example of multi-sample texture, unified texturing mode tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms,{sample,r6,r7,r8}]; // Example of multi-sample texture, independent texturing mode tex.2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ms, smpl_x,{sample,r6,r7,r8}]; // Example of multi-sample texture array, unified texturing mode tex.a2dms.v4.s32.s32 {r0,r1,r2,r3}, [tex_ams,{idx,sample,r6,r7}]; // Example of texture returning .f16 data tex.1d.v4.f16.f32 {h1,h2,h3,h4}, [tex_a,smpl_x,{f1}]; // Example of texture returning .f16x2 data tex.1d.v2.f16x2.f32 {h1,h2}, [tex_a,smpl_x,{f1}]; // Example of 3d texture array access with tex.grad,unified texturing mode tex.grad.3d.v4.f32.f32 {%f4,%f5,%f6,%f7},[tex_3d,{%f0,%f0,%f0,%f0}], {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3}; // Example of cube texture array access with tex.grad,unified texturing mode tex.grad.cube.v4.f32.f32{%f4,%f5,%f6,%f7},[tex_cube,{%f0,%f0,%f0,%f0}], {fl0,fl1,fl2,fl3},{fl0,fl1,fl2,fl3}; // Example of 1d texture lookup with offset, unified texturing mode tex.1d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a, {f1}], {r5}; // Example of 2d texture array lookup with offset, unified texturing mode tex.a2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{idx,f1,f2}], {f5,f6}; // Example of 2d mipmap texture lookup with offset, unified texturing mode tex.level.2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{f1,f2}], flvl, {r7, r8}; // Example of 2d depth texture lookup with compare, unified texturing mode tex.1d.v4.f32.f32 {f1,f2,f3,f4}, [tex_a, {f1}], f0; // Example of depth 2d texture array lookup with offset, compare tex.a2d.v4.s32.f32 {f0,f1,f2,f3}, [tex_a,{idx,f4,f5}], {r5,r6}, f6; // Example of destination predicate use tex.3d.v4.s32.s32 {r1,r2,r3,r4}|p, [tex_a,{f1,f2,f3,f4}]; 9.7.9.4. Texture Instructions: tld4 \\uf0c1 tld4 Perform a texture fetch of the 4-texel bilerp footprint.'},\n",
       " {'id': 650,\n",
       "  'content': 'Syntax tld4.comp.2d.v4.dtype.f32 d[|p], [a, c] {, e} {, f}; tld4.comp.geom.v4.dtype.f32 d[|p], [a, b, c] {, e} {, f}; // explicit sampler .comp = { .r, .g, .b, .a }; .geom = { .2d, .a2d, .cube, .acube }; .dtype = { .u32, .s32, .f32 }; Description Texture fetch of the 4-texel bilerp footprint using a texture coordinate vector. The instruction loads the bilerp footprint from the texture named by operand a at coordinates given by operand c into vector destination d . The texture component fetched for each texel sample is specified by .comp . The four texel samples are placed into destination vector d in counter-clockwise order starting at lower left. tld4.2d For 2D textures, operand c specifies coordinates as a two-element, 32-bit floating-point vector. Offset is applied to coordinates before doing texture fetch. tld4.a2d Texture array selection, followed by tld4 texture fetch of 2d texture. For 2d texture arrays operand c is a four element, 32-bit vector. The first element in operand c is interpreted as an unsigned integer index ( .u32 ) into the texture array, and the next two elements are interpreted as 32-bit floating point coordinates of 2d texture. tld4.cube For cubemap textures, operand c specifies four-element vector which comprises three floating-point coordinates (s, t, r) and a fourth padding argument which is ignored. Coordinates (s, t, r) are projected onto one of the six cube faces. The (s, t, r) coordinates can be thought of as a direction vector emanating from the center of the cube. Of the three coordinates (s, t, r), the coordinate of the largest magnitude (the major axis) selects the cube face. Then, the other two coordinates (the minor axes) are divided by the absolute value of the major axis to produce a new (s, t) coordinate pair to lookup into the selected cube face. tld4.acube Cubemap array selection, followed by tld4 texture fetch of cubemap texture. The first element in operand c is interpreted as an unsigned integer index ( .u32 ) into the cubemap texture array, and the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r), used to lookup in the selected cubemap. PTX ISA Notes Introduced in PTX ISA version 2.2. tld4. {a2d,cube,acube} introduced in PTX ISA version 4.3. Target ISA Notes tld4 requires sm_20 or higher. {a2d,cube,acube} requires sm_30 or higher. Examples //Example of unified mode texturing tld4.r.2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{f1,f2}]; // Example of independent mode texturing tld4.r.2d.v4.u32.f32 {u1,u2,u3,u4}, [tex_a,smpl_x,{f1,f2}]; // Example of unified mode texturing using offset tld4.r.2d.v4.s32.f32 {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6}; // Example of unified mode texturing using compare tld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}, [tex_a,{f5,f6}], f7; // Example of optional destination predicate tld4.r.2d.v4.f32.f32 {f1,f2,f3,f4}|p, [tex_a,{f5,f6}], f7; 9.7.9.5. Texture Instructions: txq \\uf0c1 txq Query texture and sampler attributes.'},\n",
       " {'id': 651,\n",
       "  'content': 'Syntax txq.tquery.b32 d, [a]; // texture attributes txq.level.tlquery.b32 d, [a], lod; // texture attributes txq.squery.b32 d, [a]; // sampler attributes .tquery = { .width, .height, .depth, .channel_data_type, .channel_order, .normalized_coords, .array_size, .num_mipmap_levels, .num_samples}; .tlquery = { .width, .height, .depth }; .squery = { .force_unnormalized_coords, .filter_mode, .addr_mode_0, addr_mode_1, addr_mode_2 }; Description Query an attribute of a texture or sampler. Operand a is either a .texref or .samplerref variable, or a .u64 register. Query Returns .width .height .depth value in elements .channel_data_type Unsigned integer corresponding to source language’s channel data type enumeration. If the source language combines channel data type and channel order into a single enumeration type, that value is returned for both channel_data_type and channel_order queries. .channel_order Unsigned integer corresponding to source language’s channel order enumeration. .normalized_coords 1 ( True ) or 0 ( False ).'},\n",
       " {'id': 652,\n",
       "  'content': '.force_unnormalized_coords 1 ( True) or 0 ( False). Defined only for .samplerref variables in independent texture mode. Overrides the normalized_coords field of a .texref variable used with a .samplerref in a tex instruction. .filter_mode Integer from enum { nearest, linear } .addr_mode_0 .addr_mode_1 .addr_mode_2 Integer from enum { wrap, mirror, clamp_ogl, clamp_to_edge, clamp_to_border } .array_size For a texture array, number of textures in array, 0 otherwise. .num_mipmap_levels For a mipmapped texture, number of levels of details (LOD), 0 otherwise.'},\n",
       " {'id': 653,\n",
       "  'content': '.num_samples For a multi-sample texture, number of samples, 0 otherwise. Texture attributes are queried by supplying a .texref argument to txq . In unified mode, sampler attributes are also accessed via a .texref argument, and in independent mode sampler attributes are accessed via a separate .samplerref argument. txq.level txq.level requires an additional 32bit integer argument, lod , which specifies LOD and queries requested attribute for the specified LOD. PTX ISA Notes Introduced in PTX ISA version 1.5. Channel data type and channel order queries were added in PTX ISA version 2.1. The .force_unnormalized_coords query was added in PTX ISA version 2.2. .array_size , .num_mipmap_levels , .num_samples samples queries were added in PTX ISA version 4.1. txq.level introduced in PTX ISA version 4.3. Querying the number of mipmap levels requires sm_20 or higher. Querying the number of samples requires sm_30 or higher. txq.level requires sm_30 or higher. Examples txq.width.b32 %r1, [tex_A]; txq.filter_mode.b32 %r1, [tex_A]; // unified mode txq.addr_mode_0.b32 %r1, [smpl_B]; // independent mode txq.level.width.b32 %r1, [tex_A], %r_lod; 9.7.9.6. Texture Instructions: istypep \\uf0c1 istypep Query whether a register points to an opaque variable of a specified type. Syntax istypep.type p, a; // result is .pred .type = { .texref, .samplerref, .surfref }; Description Write predicate register p with 1 if register a points to an opaque variable of the specified type, and with 0 otherwise. Destination p has type .pred ; the source address operand must be of type .u64 . Target ISA Notes istypep requires sm_30 or higher. Examples istypep.texref istex, tptr; istypep.samplerref issampler, sptr; istypep.surfref issurface, surfptr; 9.7.10. Surface Instructions \\uf0c1 This section describes PTX instructions for accessing surfaces. PTX supports the following operations on surface descriptors: Static initialization of surface descriptors. Module-scope and per-entry scope definitions of surface descriptors. Ability to query fields within surface descriptors. These instructions provide access to surface memory. suld sust sured suq 9.7.10.1. Surface Instructions: suld \\uf0c1 suld Load from surface memory. Syntax suld.b.geom{.cop}.vec.dtype.clamp d, [a, b]; // unformatted .geom = { .1d, .2d, .3d, .a1d, .a2d }; .cop = { .ca, .cg, .cs, .cv }; // cache operation .vec = { none, .v2, .v4 }; .dtype = { .b8 , .b16, .b32, .b64 }; .clamp = { .trap, .clamp, .zero }; Description suld.b. {1d,2d,3d} Load from surface memory using a surface coordinate vector. The instruction loads data from the surface named by operand a at coordinates given by operand b into destination d . Operand a is a .surfref variable or .u64 register. Operand b is a scalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces, where the fourth element is ignored. Coordinate elements are of type .s32 . suld.b performs an unformatted load of binary data. The lowest dimension coordinate represents a byte offset into the surface and is not scaled, and the size of the data transfer matches the size of destination operand d . suld.b. {a1d,a2d} Surface layer selection, followed by a load from the selected surface. The instruction first selects a surface layer from the surface array named by operand a using the index given by the first element of the array coordinate vector b . The instruction then loads data from the selected surface at coordinates given by the remaining elements of operand b into destination d . Operand b is a bit-size type vector or tuple containing an index into the array of surfaces followed by coordinates within the selected surface, as follows: For 1d surface arrays, operand b has type .v2.b32 . The first element is interpreted as an unsigned integer index ( .u32 ) into the surface array, and the second element is interpreted as a 1d surface coordinate of type .s32 . For 2d surface arrays, operand b has type .v4.b32 . The first element is interpreted as an unsigned integer index ( .u32 ) into the surface array, and the next two elements are interpreted as 2d surface coordinates of type .s32 . A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the coordinate vector must be naturally aligned to a multiple of the access size. The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp loads data at the nearest surface location (sized appropriately) .zero loads zero for out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. In indirect access, operand a is a .u64 register holding the address of a .surfref variable. PTX ISA Notes suld.b.trap introduced in PTX ISA version 1.5. Additional clamp modifiers and cache operations introduced in PTX ISA version 2.0. suld.b.3d and suld.b. {a1d,a2d} introduced in PTX ISA version 3.0. Indirect surface access introduced in PTX ISA version 3.1. Target ISA Notes suld.b supported on all target architectures. sm_1x targets support only the .trap clamping modifier. suld.3d and suld.'},\n",
       " {'id': 654,\n",
       "  'content': '{a1d,a2d} require sm_20 or higher. Indirect surface access requires sm_20 or higher. Examples suld.b.1d.v4.b32.trap {s1,s2,s3,s4}, [surf_B, {x}]; suld.b.3d.v2.b64.trap {r1,r2}, [surf_A, {x,y,z,w}]; suld.b.a1d.v2.b32 {r0,r1}, [surf_C, {idx,x}]; suld.b.a2d.b32 r0, [surf_D, {idx,x,y,z}]; // z ignored 9.7.10.2. Surface Instructions: sust \\uf0c1 sust Store to surface memory. Syntax sust.b. {1d,2d,3d}{.cop}.vec.ctype.clamp [a, b], c; // unformatted sust.p. {1d,2d,3d}.vec.b32.clamp [a, b], c; // formatted sust.b. {a1d,a2d}{.cop}.vec.ctype.clamp [a, b], c; // unformatted .cop = { .wb, .cg, .cs, .wt }; // cache operation .vec = { none, .v2, .v4 }; .ctype = { .b8 , .b16, .b32, .b64 }; .clamp = { .trap, .clamp, .zero }; Description sust. {1d,2d,3d} Store to surface memory using a surface coordinate vector. The instruction stores data from operand c to the surface named by operand a at coordinates given by operand b . sust.b performs an unformatted store of binary data. The lowest dimension coordinate represents a byte offset into the surface and is not scaled. The size of the data transfer matches the size of source operand c . sust.p performs a formatted store of a vector of 32-bit data values to a surface sample. The source vector elements are interpreted left-to-right as R , G , B , and A surface components. These elements are written to the corresponding surface sample components. Source elements that do not occur in the surface sample are ignored. Surface sample components that do not occur in the source vector will be written with an unpredictable value. The lowest dimension coordinate represents a sample offset rather than a byte offset. The source data interpretation is based on the surface sample format as follows: If the surface format contains UNORM , SNORM , or FLOAT data, then .f32 is assumed; if the surface format contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed. The source data is then converted from this type to the surface sample format. sust.b. {a1d,a2d} Surface layer selection, followed by an unformatted store to the selected surface. The instruction then stores the data in operand c to the selected surface at coordinates given by the remaining elements of operand b . The .clamp field specifies how to handle out-of-bounds addresses: .trap causes an execution trap on out-of-bounds addresses .clamp stores data at the nearest surface location (sized appropriately) .zero drops stores to out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. PTX ISA Notes sust.b.trap introduced in PTX ISA version 1.5. sust.p , additional clamp modifiers, and cache operations introduced in PTX ISA version 2.0. sust.b.3d and sust.b. Target ISA Notes sust.b supported on all target architectures. sust.3d and sust.'},\n",
       " {'id': 655,\n",
       "  'content': 'sust.p requires sm_20 or higher. Examples sust.p.1d.v4.b32.trap [surf_B, {x}], {f1,f2,f3,f4}; sust.b.3d.v2.b64.trap [surf_A, {x,y,z,w}], {r1,r2}; sust.b.a1d.v2.b64 [surf_C, {idx,x}], {r1,r2}; sust.b.a2d.b32 [surf_D, {idx,x,y,z}], r0; // z ignored 9.7.10.3. Surface Instructions: sured \\uf0c1 sured Reduce surface memory. Syntax sured.b.op.geom.ctype.clamp [a,b],c; // byte addressing sured.p.op.geom.ctype.clamp [a,b],c; // sample addressing .op = { .add, .min, .max, .and, .or }; .geom = { .1d, .2d, .3d }; .ctype = { .u32, .u64, .s32, .b32, .s64 }; // for sured.b .ctype = { .b32, .b64 }; // for sured.p .clamp = { .trap, .clamp, .zero }; Description Reduction to surface memory using a surface coordinate vector. The instruction performs a reduction operation with data from operand c to the surface named by operand a at coordinates given by operand b . sured.b performs an unformatted reduction on .u32 , .s32 , .b32 , .u64 , or .s64 data. Operation add applies to .u32 , .u64 , and .s32 types; min and max apply to .u32 , .s32 , .u64 and .s64 types; operations and and or apply to .b32 type. sured.p performs a reduction on sample-addressed data. The instruction type .b64 is restricted to min and max operations. For type .b32 , the data is interpreted as .u32 or .s32 based on the surface sample format as follows: if the surface format contains UINT data, then .u32 is assumed; if the surface format contains SINT data, then .s32 is assumed. For type .b64 , if the surface format contains UINT data, then .u64 is assumed; if the surface format contains SINT data, then .s64 is assumed. .u64 / .s64 / .b64 types with .min / .max operations introduced in PTX ISA version 8.1. Target ISA Notes sured requires sm_20 or higher. .u64 / .s64 / .b64 types with .min / .max operations requires sm_50 or higher. Examples sured.b.add.2d.u32.trap [surf_A, {x,y}], r1; sured.p.min.1d.u32.trap [surf_B, {x}], r1; sured.b.max.1d.u64.trap [surf_C, {x}], r1; sured.p.min.1d.b64.trap [surf_D, {x}], r1; 9.7.10.4. Surface Instructions: suq \\uf0c1 suq Query a surface attribute. Syntax suq.query.b32 d, [a]; .query = { .width, .height, .depth, .channel_data_type, .channel_order, .array_size, .memory_layout }; Description Query an attribute of a surface. Operand a is a .surfref variable or a .u64 register. .array_size For a surface array, number of surfaces in array, 0 otherwise. .memory_layout 1 for surface with linear memory layout; 0 otherwise Indirect surface access Beginning with PTX ISA version 3.1, indirect surface access is supported for target architecture sm_20 or higher. Channel data type and channel order queries added in PTX ISA version 2.1. The .array_size query was added in PTX ISA version 4.1. The .memory_layout query was added in PTX ISA version 4.2. Examples suq.width.b32 %r1, [surf_A]; 9.7.11. Control Flow Instructions \\uf0c1 The following PTX instructions and syntax are for controlling execution in a PTX program: {} @ bra call ret exit 9.7.11.1. Control Flow Instructions: {} \\uf0c1 {} Instruction grouping. Syntax { instructionList } Description The curly braces create a group of instructions, used primarily for defining a function body. The curly braces also provide a mechanism for determining the scope of a variable: any variable declared within a scope is not available outside the scope. Examples { add.s32 a,b,c; mov.s32 d,a; } 9.7.11.2. Control Flow Instructions: @ \\uf0c1 @ Predicated execution. Syntax @{! }p instruction; Description Execute an instruction or instruction block for threads that have the guard predicate True . Threads with a False guard predicate do nothing. Semantics If {! }p then instruction PTX ISA Notes Introduced in PTX ISA version 1.0. Examples setp.eq.f32 p,y,0; // is y zero? @!p div.f32 ratio,x,y // avoid division by zero @q bra L23; // conditional branch 9.7.11.3. Control Flow Instructions: bra \\uf0c1 bra Branch to a target and continue execution there. Syntax @p bra{.uni} tgt; // tgt is a label bra{.uni} tgt; // unconditional branch Description Continue execution at the target. Conditional branches are specified by using a guard predicate. The branch target must be a label. bra.uni is guaranteed to be non-divergent, i.e. all active threads in a warp that are currently executing this instruction have identical values for the guard predicate and branch target. Semantics if (p) { pc = tgt; } PTX ISA Notes Introduced in PTX ISA version 1.0. Unimplemented indirect branch introduced in PTX ISA version 2.1 has been removed from the spec. Examples bra.uni L_exit; // uniform unconditional jump @q bra L23; // conditional branch 9.7.11.4. Control Flow Instructions: brx.idx \\uf0c1 brx.idx Branch to a label indexed from a list of potential branch targets. Syntax @p brx.idx{.uni} index, tlist; brx.idx{.uni} index, tlist; Description Index into a list of possible destination labels, and continue execution from the chosen label. brx.idx.uni guarantees that the branch is non-divergent, i.e. all active threads in a warp that are currently executing this instruction have identical values for the guard predicate and the index argument. The index operand is a .u32 register. The tlist operand must be the label of a .branchtargets directive. It is accessed as a zero-based sequence using index . Behaviour is undefined if the value of index is greater than or equal to the length of tlist . The .branchtargets directive must be defined in the local function scope before it is used. It must refer to labels within the current function.'},\n",
       " {'id': 656,\n",
       "  'content': 'Semantics if (p) { if (index = s) ? 0 : r+1; dec(r, s) = (r==0 || r > s) ? s : r-1; exch(r, s) = s; cas(r,s,t) = (r == s) ? t : r; Notes Simple reductions may be specified by using the bit bucket destination operand _ . PTX ISA Notes 32-bit atom.global introduced in PTX ISA version 1.1. atom.shared and 64-bit atom.global. {add,cas,exch} introduced in PTX ISA 1.2. atom.add.f32 and 64-bit atom.shared. {add,cas,exch} introduced in PTX ISA 2.0. 64-bit atom. {and,or,xor,min,max} introduced in PTX ISA 3.1. atom.add.f64 introduced in PTX ISA 5.0. .scope qualifier introduced in PTX ISA 5.0. .sem qualifier introduced in PTX ISA version 6.0. atom.add.noftz.f16x2 introduced in PTX ISA 6.2. atom.add.noftz.f16 and atom.cas.b16 introduced in PTX ISA 6.3. Per-element atomicity of atom.f16x2 clarified in PTX ISA version 6.3, with retrospective effect from PTX ISA version 6.2. Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4. atom.add.noftz.bf16 and atom.add.noftz.bf16x2 introduced in PTX ISA 7.8. Support for vector types introduced in PTX ISA version 8.1. Target ISA Notes atom.global requires sm_11 or higher. atom.shared requires sm_12 or higher. 64-bit atom.global. {add,cas,exch} require sm_12 or higher. 64-bit atom.shared. {add,cas,exch} require sm_20 or higher. {and,or,xor,min,max} require sm_32 or higher. atom.add.f32 requires sm_20 or higher. atom.add.f64 requires sm_60 or higher. .scope qualifier requires sm_60 or higher. .sem qualifier requires sm_70 or higher. Use of generic addressing requires sm_20 or higher. atom.add.noftz.f16x2 requires sm_60 or higher. atom.add.noftz.f16 and atom.cas.b16 requires sm_70 or higher. atom.add.noftz.bf16 and atom.add.noftz.bf16x2 require sm_90 or higher. Support for vector types requires sm_90 or higher. Support for .b128 type requires sm_90 or higher. Examples atom.global.add.s32 d,[a],1; atom.shared::cta.max.u32 d,[x+4],0; @p atom.global.cas.b32 d,[p],my_val,my_new_val; atom.global.sys.add.u32 d, [a], 1; atom.global.acquire.sys.inc.u32 ans, [gbl], %r0; atom.add.noftz.f16x2 d, [a], b; atom.add.noftz.f16 hd, [ha], hb; atom.global.cas.b16 hd, [ha], hb, hc; atom.add.noftz.bf16 hd, [a], hb; atom.add.noftz.bf16x2 bd, [b], bb; atom.add.shared::cluster.noftz.f16 hd, [ha], hb; atom.shared.b128.cas d, a, b, c; // 128-bit atom atom.global.b128.exch d, a, b; // 128-bit atom atom.global.cluster.relaxed.add.u32 d, [a], 1; createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25; atom.global.add.L2::cache_hint.s32 d, [a], 1, cache-policy; atom.global.v8.f16.max.noftz {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7}; atom.global.v8.bf16.add.noftz {%hd0, %hd1, %hd2, %hd3, %hd4, %hd5, %hd6, %hd7}, [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7}; atom.global.v2.f16.add.noftz {%hd0, %hd1}, [gbl], {%h0, %h1}; atom.global.v2.bf16.add.noftz {%hd0, %hd1}, [gbl], {%h0, %h1}; atom.global.v4.b16x2.min.noftz {%hd0, %hd1, %hd2, %hd3}, [gbl], {%h0, %h1, %h2, %h3}; atom.global.v4.f32.add {%f0, %f1, %f2, %f3}, [gbl], {%f0, %f1, %f2, %f3}; atom.global.v2.f16x2.min.noftz {%bd0, %bd1}, [g], {%b0, %b1}; atom.global.v2.bf16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1}; atom.global.v2.f32.add {%f0, %f1}, [g], {%f0, %f1}; 9.7.12.6. Parallel Synchronization and Communication Instructions: red \\uf0c1 red Reduction operations on global and shared memory.'},\n",
       " {'id': 657,\n",
       "  'content': 'Syntax Reduction operation with scalar type: red{.sem}{.scope}{.space}.op{.level::cache_hint}.type [a], b{, cache-policy}; red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16 [a], b{, cache-policy}; red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.f16x2 [a], b{, cache-policy}; red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16 [a], b {, cache-policy}; red{.sem}{.scope}{.space}.add.noftz{.level::cache_hint}.bf16x2 [a], b {, cache-policy}; .space = { .global, .shared{::cta, ::cluster} }; .sem = {.relaxed, .release}; .scope = {.cta, .cluster, .gpu, .sys}; .op = { .and, .or, .xor, .add, .inc, .dec, .min, .max }; .level::cache_hint = { .L2::cache_hint }; .type = { .b32, .b64, .u32, .u64, .s32, .s64, .f32, .f64 }; Reduction operation with vector type: red{.sem}{.scope}{.global}.add{.level::cache_hint}.vec_32_bit.f32 [a], b{, cache-policy}; red{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}. vec_16_bit.half_word_type [a], b{, cache-policy}; red{.sem}{.scope}{.global}.op.noftz{.level::cache_hint}.vec_32_bit.packed_type [a], b {, cache-policy}; .sem = { .relaxed, .release }; .scope = { .cta, .cluster, .gpu, .sys }; .op = { .add, .min, .max }; .half_word_type = { .f16, .bf16 }; .packed_type = { .f16x2,.bf16x2 }; .vec_16_bit = { .v2, .v4, .v8 } .vec_32_bit = { .v2, .v4 }; .level::cache_hint = { .L2::cache_hint } Description Performs a reduction operation with operand b and the value in location a , and stores the result of the specified operation at location a , overwriting the original value. Operand a specifies a location in the specified state space.'},\n",
       " {'id': 658,\n",
       "  'content': 'If no state space is given, perform the memory accesses using Generic Addressing . red with scalar type may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. red with vector type may be used only with .global space and with generic addressing where the address points to .global space. For red with vector type, operand b is brace-enclosed vector expressions, size of which is equal to the size of vector qualifier. The optional .sem qualifier specifies a memory synchronizing effect as described in the Memory Consistency Model . If the .sem qualifier is absent, .relaxed is assumed by default. The optional .scope qualifier specifies the set of threads that can directly observe the memory synchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is absent, .gpu scope is assumed by default. For red with vector type, the supported combinations of vector qualifier, types and reduction operations supported on these combinations are depicted in following table: Vector qualifier Types .f16 / bf16 .f16x2 / bf16x2 .f32 .v2 .add , .min , .max .add , .min , .max .add .v4 .add , .min , .max .add , .min , .max .add .v8 .add , .min , .max Not supported Not Supported Two atomic operations { atom or red } are performed atomically with respect to each other only if each operation specifies a scope that includes the other. When this condition is not met, each operation observes the other operation being performed as if it were split into a read followed by a dependent write. red instruction on packed type or vector type, accesses adjacent scalar elements in memory. In such case, the atomicity is guaranteed separately for each of the individual scalar elements; the entire red is not guaranteed to be atomic as a single access. For sm_6x and earlier architectures, red operations on .shared state space do not guarantee atomicity with respect to normal store instructions to the same address. It is the programmer’s responsibility to guarantee correctness of programs that use shared memory reduction instructions, e.g., by inserting barriers between normal stores and reduction operations to a common address, or by using atom.exch to store to locations accessed by other reduction operations. Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The bit-size operations are .and , .or , and .xor . The integer operations are .add , .inc , .dec , .min , .max . The .inc and .dec operations return a result in the range [0..b] . The floating-point operation .add operation rounds to nearest even. Current implementation of red.add.f32 on global memory flushes subnormal inputs and results to sign-preserving zero; whereas red.add.f32 on shared memory supports subnormal inputs and results and doesn’t flush them to zero. red.add.f16 , red.add.f16x2 , red.add.bf16 and red.add.bf16x2 operation requires the .noftz qualifier; it preserves subnormal inputs and results, and does not flush them to zero. Semantics *a = operation(*a, b); where inc(r, s) = (r >= s) ? s : r-1; PTX ISA Notes Introduced in PTX ISA version 1.2. red.add.f32 and red.shared.add.u64 introduced in PTX ISA 2.0. 64-bit red. {and,or,xor,min,max} introduced in PTX ISA 3.1. red.add.f64 introduced in PTX ISA 5.0. .sem qualifier introduced in PTX ISA version 6.0. red.add.noftz.f16x2 introduced in PTX ISA 6.2. red.add.noftz.f16 introduced in PTX ISA 6.3. Per-element atomicity of red.f16x2 clarified in PTX ISA version 6.3, with retrospective effect from PTX ISA version 6.2 Support for .level::cache_hint qualifier introduced in PTX ISA version 7.4. red.add.noftz.bf16 and red.add.noftz.bf16x2 introduced in PTX ISA 7.8. Target ISA Notes red.global requires sm_11 or higher red.shared requires sm_12 or higher. red.global.add.u64 requires sm_12 or higher. red.shared.add.u64 requires sm_20 or higher. red.add.f32 requires sm_20 or higher. red.add.f64 requires sm_60 or higher. red.add.noftz.f16x2 requires sm_60 or higher. red.add.noftz.f16 requires sm_70 or higher. red.add.noftz.bf16 and red.add.noftz.bf16x2 require sm_90 or higher. Examples red.global.add.s32 [a],1; red.shared::cluster.max.u32 [x+4],0; @p red.global.and.b32 [p],my_val; red.global.sys.add.u32 [a], 1; red.global.acquire.sys.add.u32 [gbl], 1; red.add.noftz.f16x2 [a], b; red.add.noftz.bf16 [a], hb; red.add.noftz.bf16x2 [b], bb; red.global.cluster.relaxed.add.u32 [a], 1; red.shared::cta.min.u32 [x+4],0; createpolicy.fractional.L2::evict_last.b64 cache-policy, 0.25; red.global.and.L2::cache_hint.b32 [a], 1, cache-policy; red.global.v8.f16.add.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7}; red.global.v8.bf16.min.noftz [gbl], {%h0, %h1, %h2, %h3, %h4, %h5, %h6, %h7}; red.global.v2.f16.add.noftz [gbl], {%h0, %h1}; red.global.v2.bf16.add.noftz [gbl], {%h0, %h1}; red.global.v4.f16x2.max.noftz [gbl], {%h0, %h1, %h2, %h3}; red.global.v4.f32.add [gbl], {%f0, %f1, %f2, %f3}; red.global.v2.f16x2.max.noftz {%bd0, %bd1}, [g], {%b0, %b1}; red.global.v2.bf16x2.add.noftz {%bd0, %bd1}, [g], {%b0, %b1}; red.global.v2.f32.add {%f0, %f1}, [g], {%f0, %f1}; 9.7.12.7. Parallel Synchronization and Communication Instructions: red.async \\uf0c1 red.async Asynchronous reduction operation on shared memory. Syntax // Increment and Decrement reductions red.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar]; .ss = { .shared::cluster }; .op = { .inc, .dec }; .type = { .u32 }; .completion_mechanism = { .mbarrier::complete_tx::bytes }; // MIN and MAX reductions red.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar]; .ss = { .shared::cluster }; .op = { .min, .max }; .type = { .u32, .s32 }; .completion_mechanism = { .mbarrier::complete_tx::bytes }; // Bitwise AND, OR and XOR reductions red.async.relaxed.cluster{.ss}.completion_mechanism.op.type [a], b, [mbar]; .ss = { .shared::cluster }; .op = { .and, .or, .xor }; .type = { .b32 }; .completion_mechanism = { .mbarrier::complete_tx::bytes }; // ADD reductions red.async.relaxed.cluster{.ss}.completion_mechanism.add.type [a], b, [mbar]; .ss = { .shared::cluster }; .type = { .u32, .s32, .u64 }; .completion_mechanism = { .mbarrier::complete_tx::bytes }; Description red.async is a non-blocking instruction which initiates an asynchronous reduction operation specified by .op , with the operand b and the value at destination shared memory location specified by operand a . The shared memory addresses of destination operand a and the mbarrier object mbar , must meet all of the following conditions: They Belong to the same CTA.'},\n",
       " {'id': 659,\n",
       "  'content': \"With .shared::cluster , if the addresses specified do not fall within the address window of .shared::cluster state space, then the behaviour is undefined. The reduce operation in red.async is treated as a relaxed memory operation and the complete_tx operation on the mbarrier has .release semantics at the .cluster scope as described in the Memory Consistency Model . Examples red.async.relaxed.cluster.shared::cluster.mbarrier::complete_tx::bytes.min.u32 [addr], b, [mbar_addr]; 9.7.12.8. Parallel Synchronization and Communication Instructions: vote (deprecated) \\uf0c1 vote (deprecated) Vote across thread group. Syntax vote.mode.pred d, {! }a; vote.ballot.b32 d, {! }a; // 'ballot' form, returns bitmask .mode = { .all, .any, .uni }; Deprecation Note The vote instruction without a .sync qualifier is deprecated in PTX ISA version 6.0. Support for this instruction with .target lower than sm_70 may be removed in a future PTX ISA version. Removal Note Support for vote instruction without a .sync qualifier is removed in PTX ISA version 6.4 for .target sm_70 or higher. Description Performs a reduction of the source predicate across all active threads in a warp. The destination predicate value is the same across all threads in the warp. The reduction modes are: .all True if source predicate is True for all active threads in warp. Negate the source predicate to compute .none . .any True if source predicate is True for some active thread in warp. Negate the source predicate to compute .not_all . .uni True if source predicate has the same value in all active threads in warp. Negating the source predicate also computes .uni . In the ballot form, vote.ballot.b32 simply copies the predicate from each thread in a warp into the corresponding bit position of destination register d , where the bit position corresponds to the thread’s lane id. An inactive thread in warp will contribute a 0 for its entry when participating in vote.ballot.b32 . PTX ISA Notes Introduced in PTX ISA version 1.2. Deprecated in PTX ISA version 6.0 in favor of vote.sync . Not supported in PTX ISA version 6.4 for .target sm_70 or higher. Target ISA Notes vote requires sm_12 or higher. vote.ballot.b32 requires sm_20 or higher. vote is not supported on sm_70 or higher starting PTX ISA version 6.4. Release Notes Note that vote applies to threads in a single warp, not across an entire CTA. Examples vote.all.pred p,q; vote.uni.pred p,q; vote.ballot.b32 r1,p; // get 'ballot' across warp 9.7.12.9. Parallel Synchronization and Communication Instructions: vote.sync \\uf0c1 vote.sync Vote across thread group. Syntax vote.sync.mode.pred d, {! }a, membermask; vote.sync.ballot.b32 d, {! }a, membermask; // 'ballot' form, returns bitmask .mode = { .all, .any, .uni }; Description vote.sync will cause executing thread to wait until all non-exited threads corresponding to membermask have executed vote.sync with the same qualifiers and same membermask value before resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating in this instruction where the bit position corresponds to thread’s laneid . Operand a is a predicate register. In the mode form, vote.sync performs a reduction of the source predicate across all non-exited threads in membermask . The destination operand d is a predicate register and its value is the same across all threads in membermask . The reduction modes are: .all True if source predicate is True for all non-exited threads in membermask . .any True if source predicate is True for some thread in membermask .\"},\n",
       " {'id': 660,\n",
       "  'content': \".uni True if source predicate has the same value in all non-exited threads in membermask . In the ballot form, the destination operand d is a .b32 register. In this form, vote.sync.ballot.b32 simply copies the predicate from each thread in membermask into the corresponding bit position of destination register d , where the bit position corresponds to the thread’s lane id. A thread not specified in membermask will contribute a 0 for its entry in vote.sync.ballot.b32 . The behavior of vote.sync is undefined if the executing thread is not in the membermask . Note For .target sm_6x or below, all threads in membermask must execute the same vote.sync instruction in convergence, and only threads belonging to some membermask can be active when the vote.sync instruction is executed. PTX ISA Notes Introduced in PTX ISA version 6.0. Target ISA Notes Requires sm_30 or higher. Examples vote.sync.all.pred p,q,0xffffffff; vote.sync.ballot.b32 r1,p,0xffffffff; // get 'ballot' across warp 9.7.12.10. Parallel Synchronization and Communication Instructions: match.sync \\uf0c1 match.sync Broadcast and compare a value across threads in warp. Syntax match.any.sync.type d, a, membermask; match.all.sync.type d[|p], a, membermask; .type = { .b32, .b64 }; Description match.sync will cause executing thread to wait until all non-exited threads from membermask have executed match.sync with the same qualifiers and same membermask value before resuming execution. Operand membermask specifies a 32-bit integer which is a mask indicating threads participating in this instruction where the bit position corresponds to thread’s laneid. match.sync performs broadcast and compare of operand a across all non-exited threads in membermask and sets destination d and optional predicate p based on mode. Operand a has instruction type and d has .b32 type. Destination d is a 32-bit mask where bit position in mask corresponds to thread’s laneid. The matching operation modes are: .all d is set to mask corresponding to non-exited threads in membermask if all non-exited threads in membermask have same value of operand a ; otherwise d is set to 0. Optionally predicate p is set to true if all non-exited threads in membermask have same value of operand a ; otherwise p is set to false. .any d is set to mask of non-exited threads in membermask that have same value of operand a . The behavior of match.sync is undefined if the executing thread is not in the membermask . Target ISA Notes Requires sm_70 or higher. Release Notes Note that match.sync applies to threads in a single warp, not across an entire CTA. Examples match.any.sync.b32 d, a, 0xffffffff; match.all.sync.b64 d|p, a, mask; 9.7.12.11. Parallel Synchronization and Communication Instructions: activemask \\uf0c1 activemask Queries the active threads within a warp. Syntax activemask.b32 d; Description activemask queries predicated-on active threads from the executing warp and sets the destination d with 32-bit integer mask where bit position in the mask corresponds to the thread’s laneid . Destination d is a 32-bit destination register. An active thread will contribute 1 for its entry in the result and exited or inactive or predicated-off thread will contribute 0 for its entry in the result. PTX ISA Notes Introduced in PTX ISA version 6.2. Examples activemask.b32 %r1; 9.7.12.12. Parallel Synchronization and Communication Instructions: redux.sync \\uf0c1 redux.sync Perform reduction operation on the data from each predicated active thread in the thread group. Syntax redux.sync.op.type dst, src, membermask; .op = {.add, .min, .max} .type = {.u32, .s32} redux.sync.op.b32 dst, src, membermask; .op = {.and, .or, .xor} Description redux.sync will cause the executing thread to wait until all non-exited threads corresponding to membermask have executed redux.sync with the same qualifiers and same membermask value before resuming execution. redux.sync performs a reduction operation .op of the 32 bit source register src across all non-exited threads in the membermask . The result of the reduction operation is written to the 32 bit destination register dst .\"},\n",
       " {'id': 661,\n",
       "  'content': 'Reduction operation can be one of the bitwise operation in .and , .or , .xor or arithmetic operation in .add , .min , .max .'},\n",
       " {'id': 662,\n",
       "  'content': \"For the .add operation result is truncated to 32 bits. The behavior of redux.sync is undefined if the executing thread is not in the membermask . Release Notes Note that redux.sync applies to threads in a single warp, not across an entire CTA. Examples .reg .b32 dst, src, init, mask; redux.sync.add.s32 dst, src, 0xff; redux.sync.xor.b32 dst, src, mask; 9.7.12.13. Parallel Synchronization and Communication Instructions: griddepcontrol \\uf0c1 griddepcontrol Control execution of dependent grids. Syntax griddepcontrol.action; .action = { .launch_dependents, .wait } Description The griddepcontrol instruction allows the dependent grids and prerequisite grids as defined by the runtime, to control execution in the following way: .launch_dependents modifier signals that specific dependents the runtime system designated to react to this instruction can be scheduled as soon as all other CTAs in the grid issue the same instruction or have completed. The dependent may launch before the completion of the current grid. There is no guarantee that the dependent will launch before the completion of the current grid. Repeated invocations of this instruction by threads in the current CTA will have no additional side effects past that of the first invocation. .wait modifier causes the executing thread to wait until all prerequisite grids in flight have completed and all the memory operations from the prerequisite grids are performed and made visible to the current grid. Note If the prerequisite grid is using griddepcontrol.launch_dependents , then the dependent grid must use griddepcontrol.wait to ensure correct functional execution. PTX ISA Notes Introduced in PTX ISA version 7.8. Examples griddepcontrol.launch_dependents; griddepcontrol.wait; 9.7.12.14. Parallel Synchronization and Communication Instructions: elect.sync \\uf0c1 elect.sync Elect a leader thread from a set of threads. Syntax elect.sync d|p, membermask; Description elect.sync elects one predicated active leader thread from among a set of threads specified by membermask . laneid of the elected thread is returned in the 32-bit destination operand d . The sink symbol ‘_’ can be used for destination operand d . The predicate destination p is set to True for the leader thread, and False for all other threads. Operand membermask specifies a 32-bit integer indicating the set of threads from which a leader is to be elected. The behavior is undefined if the executing thread is not in membermask . Election of a leader thread happens deterministically, i.e. the same leader thread is elected for the same membermask every time. The mandatory .sync qualifier indicates that elect causes the executing thread to wait until all threads in the membermask execute the elect instruction before resuming execution. Examples elect.sync %r0|%p0, 0xffffffff; 9.7.12.15. Parallel Synchronization and Communication Instructions: mbarrier \\uf0c1 mbarrier is a barrier created in shared memory that supports : Synchronizing any subset of threads within a CTA One-way synchronization of threads across CTAs of a cluster. As noted in mbarrier support with shared memory , threads can perform only arrive operations but not *_wait on an mbarrier located in shared::cluster space. Waiting for completion of asynchronous memory operations initiated by a thread and making them visible to other threads. An mbarrier object is an opaque object in memory which can be initialized and invalidated using : mbarrier.init mbarrier.inval Operations supported on mbarrier object s are : mbarrier.expect_tx mbarrier.complete_tx mbarrier.arrive mbarrier.arrive_drop mbarrier.test_wait mbarrier.try_wait mbarrier.pending_count cp.async.mbarrier.arrive Performing any mbarrier operation except mbarrier.init on an uninitialized mbarrier object results in undefined behavior. Unlike bar{.cta} / barrier{.cta} instructions which can access a limited number of barriers per CTA, mbarrier objects are used defined and are only limited by the total shared memory size available. mbarrier operations enable threads to perform useful work after the arrival at the mbarrier and before waiting for the mbarrier to complete. 9.7.12.15.1. Size and alignment of mbarrier object \\uf0c1 An mbarrier object is an opaque object with the following type and alignment requirements : Type Alignment (bytes) Memory space .b64 8 .shared 9.7.12.15.2. Contents of the mbarrier object \\uf0c1 An opaque mbarrier object keeps track of the following information : Current phase of the mbarrier object Count of pending arrivals for the current phase of the mbarrier object Count of expected arrivals for the next phase of the mbarrier object Count of pending asynchronous memory operations (or transactions) tracked by the current phase of the mbarrier object . This is also referred to as tx-count . An mbarrier object progresses through a sequence of phases where each phase is defined by threads performing an expected number of arrive-on operations. The valid range of each of the counts is as shown below: Count name Minimum value Maximum value Expected arrival count 1 2 20 - 1 Pending arrival count 0 2 20 - 1 tx-count -(2 20 - 1) 2 20 - 1 9.7.12.15.3. Lifecycle of the mbarrier object \\uf0c1 The mbarrier object must be initialized prior to use. An mbarrier object is used to synchronize threads and asynchronous memory operations. An mbarrier object may be used to perform a sequence of such synchronizations. An mbarrier object must be invalidated to repurpose its memory. 9.7.12.15.4. Phase of the mbarrier object \\uf0c1 The phase of an mbarrier object is the number of times the mbarrier object has been used to synchronize threads and cp.async operations. In each phase {0, 1, 2, …}, threads perform in program order : arrive-on operations to complete the current phase and test_wait / try_wait operations to check for the completion of the current phase. An mbarrier object is automatically reinitialized upon completion of the current phase for immediate use in the next phase. The current phase is incomplete and all prior phases are complete. For each phase of the mbarrier object, at least one test_wait or try_wait operation must be performed which returns True for waitComplete before an arrive-on operation in the subsequent phase. 9.7.12.15.5. Tracking asynchronous operations by the mbarrier object \\uf0c1 Starting with the Hopper architecture ( sm_9x ), mbarrier object supports a new count, called tx-count , which is used for tracking the completion of asynchronous memory operations or transactions. tx-count tracks the number of asynchronous transactions, in units specified by the asynchronous memory operation, that are outstanding and yet to be complete. The tx-count of an mbarrier object must be set to the total amount of asynchronous memory operations, in units as specified by the asynchronous operations, to be tracked by the current phase. Upon completion of each of the asynchronous operations, the complete-tx operation will be performed on the mbarrier object and thus progress the mbarrier towards the completion of the current phase. 9.7.12.15.5.1. expect-tx operation \\uf0c1 The expect-tx operation, with an expectCount argument, increases the tx-count of an mbarrier object by the value specified by expectCount . This makes the current phase of the mbarrier object to expect and track the completion of additional asynchronous transactions. 9.7.12.15.5.2. complete-tx operation \\uf0c1 The complete-tx operation, with an completeCount argument, on an mbarrier object consists of the following: mbarrier signaling Signals the completion of asynchronous transactions that were tracked by the current phase. As a result of this, tx-count is decremented by completeCount . mbarrier potentially completing the current phase If the current phase has been completed then the mbarrier transitions to the next phase. Refer to Phase Completion of the mbarrier object for details on phase completion requirements and phase transition process. 9.7.12.15.6. Phase Completion of the mbarrier object \\uf0c1 The requirements for completion of the current phase are described below. Upon completion of the current phase, the phase transitions to the subsequent phase as described below. Current phase completion requirements An mbarrier object completes the current phase when all of the following conditions are met: The count of the pending arrivals has reached zero. The tx-count has reached zero. Phase transition When an mbarrier object completes the current phase, the following actions are performed atomically: The mbarrier object transitions to the next phase. The pending arrival count is reinitialized to the expected arrival count. 9.7.12.15.7. Arrive-on operation on mbarrier object \\uf0c1 An arrive-on operation, with an optional count argument, on an mbarrier object consists of the following 2 steps : mbarrier signalling: Signals the arrival of the executing thread OR completion of the cp.async instruction which signals the arrive-on operation initiated by the executing thread on the mbarrier object . As a result of this, the pending arrival count is decremented by count . If the count argument is not specified, then it defaults to 1. mbarrier potentially completing the current phase: If the current phase has been completed then the mbarrier transitions to the next phase. 9.7.12.15.8. mbarrier support with shared memory \\uf0c1 The following table summarizes the support of various mbarrier operations on mbarrier objects located at different shared memory locations: mbarrier operations .shared::cta .shared::cluster mbarrier.arrive Supported Supported, cannot return result mbarrier.expect_tx Supported Supported mbarrier.complete_tx Supported Supported Other mbarrier operations Supported Not supported 9.7.12.15.9. Parallel Synchronization and Communication Instructions: mbarrier.init \\uf0c1 mbarrier.init Initialize the mbarrier object . Syntax mbarrier.init{.shared{::cta}}.b64 [addr], count; Description mbarrier.init initializes the mbarrier object at the location specified by the address operand addr with the unsigned 32-bit integer count . The value of operand count must be in the range as specified in Contents of the mbarrier object . Initialization of the mbarrier object involves : Initializing the current phase to 0. Initializing the expected arrival count to count . Initializing the pending arrival count to count . Initializing the tx-count to 0. If the address specified by addr does not fall within the address window of .shared::cta state space then the behavior is undefined. Supported addressing modes for operand addr is as described in Addresses as Operands . Alignment for operand addr is as described in the Size and alignment of mbarrier object . Support for sub-qualifier ::cta on .shared introduced in PTX ISA version 7.8. Examples .shared .b64 shMem, shMem2; .reg .b64 addr; .reg .b32 %r1; cvta.shared.u64 addr, shMem2; mbarrier.init.b64 [addr], %r1; bar.cta.sync 0; // ... other mbarrier operations on addr mbarrier.init.shared::cta.b64 [shMem], 12; bar.sync 0; // ... other mbarrier operations on shMem 9.7.12.15.10. Parallel Synchronization and Communication Instructions: mbarrier.inval \\uf0c1 mbarrier.inval Invalidates the mbarrier object . Syntax mbarrier.inval{.shared{::cta}}.b64 [addr]; Description mbarrier.inval invalidates the mbarrier object at the location specified by the address operand addr . An mbarrier object must be invalidated before using its memory location for any other purpose. Performing any mbarrier operation except mbarrier.init on an invalidated mbarrier object results in undefined behaviour. Examples .shared .b64 shmem; .reg .b64 addr; .reg .b32 %r1; .reg .pred t0; // Example 1 : bar.sync 0; @t0 mbarrier.init.b64 [addr], %r1; // ... other mbarrier operations on addr bar.sync 0; @t0 mbarrier.inval.b64 [addr]; // Example 2 : bar.cta.sync 0; mbarrier.init.shared.b64 [shmem], 12; // ... other mbarrier operations on shmem bar.cta.sync 0; @t0 mbarrier.inval.shared.b64 [shmem]; // shmem can be reused here for unrelated use : bar.cta.sync 0; st.shared.b64 [shmem], ...; // shmem can be re-initialized as mbarrier object : bar.cta.sync 0; @t0 mbarrier.init.shared.b64 [shmem], 24; // ... other mbarrier operations on shmem bar.cta.sync 0; @t0 mbarrier.inval.shared::cta.b64 [shmem]; 9.7.12.15.11. Parallel Synchronization and Communication Instructions: mbarrier.expect_tx \\uf0c1 mbarrier.expect_tx Perfoms expect-tx operation on the mbarrier object . Syntax mbarrier.expect_tx{.sem}{.scope}{.space}.b64 [addr], txCount; .sem = { .relaxed } .scope = { .cta, .cluster } .space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.expect_tx performs an expect-tx operation on the mbarrier object at the location specified by the address operand addr . The 32-bit unsigned integer operand txCount specifies the expectCount argument to the expect-tx operation. If the address specified by addr does not fall within the address window of .shared::cta or .shared::cluster state space then the behavior is undefined. Supported addressing modes for operand addr are as described in Addresses as Operands . This operation does not provide any memory ordering semantics and thus is a relaxed operation. Examples mbarrier.expect_tx.b64 [addr], 32; mbarrier.expect_tx.relaxed.cta.shared.b64 [mbarObj1], 512; mbarrier.expect_tx.relaxed.cta.shared.b64 [mbarObj2], 512; 9.7.12.15.12. Parallel Synchronization and Communication Instructions: mbarrier.complete_tx \\uf0c1 mbarrier.complete_tx Perfoms complete-tx operation on the mbarrier object . Syntax mbarrier.complete_tx{.sem}{.scope}{.space}.b64 [addr], txCount; .sem = { .relaxed } .scope = { .cta, .cluster } .space = { .shared{::cta}, .shared::cluster } Description A thread executing mbarrier.complete_tx performs a complete-tx operation on the mbarrier object at the location specified by the address operand addr . The 32-bit unsigned integer operand txCount specifies the completeCount argument to the complete-tx operation. mbarrier.complete_tx does not involve any asynchronous memory operations and only simulates the completion of an asynchronous memory operation and its side effect of signaling to the mbarrier object . Examples mbarrier.complete_tx.b64 [addr], 32; mbarrier.complete_tx.shared.b64 [mbarObj1], 512; mbarrier.complete_tx.relaxed.cta.b64 [addr2], 32; 9.7.12.15.13. Parallel Synchronization and Communication Instructions: mbarrier.arrive \\uf0c1 mbarrier.arrive Performs arrive-on operation on the mbarrier object . Syntax mbarrier.arrive{.sem}{.scope}{.shared{::cta}}.b64 state, [addr]{, count}; mbarrier.arrive{.sem}{.scope}{.shared::cluster}.b64 _, [addr] {,count} mbarrier.arrive.expect_tx{.sem}{.scope}{.shared{::cta}}.b64 state, [addr], txCount; mbarrier.arrive.expect_tx{.sem}{.scope}{.shared::cluster}.b64 _, [addr], txCount; mbarrier.arrive.noComplete{.sem}{.cta}{.shared{::cta}}.b64 state, [addr], count; .sem = { .release } .scope = { .cta, .cluster } Description A thread executing mbarrier.arrive performs an arrive-on operation on the mbarrier object at the location specified by the address operand addr . The 32-bit unsigned integer operand count specifies the count argument to the arrive-on operation. The optional qualifier .expect_tx specifies that an expect-tx operation is performed prior to the arrive-on operation. When both qualifiers .arrive and .expect_tx are specified, then the count argument of the arrive-on operation is assumed to be 1. A mbarrier.arrive operation with .noComplete qualifier must not cause the mbarrier to complete its current phase, otherwise the behavior is undefined. The value of the operand count must be in the range as specified in Contents of the mbarrier object . Note: for sm_8x , when the argument count is specified, the modifier .noComplete is required. mbarrier.arrive operation on an mbarrier object located in .shared::cta returns an opaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the destination operand state. Contents of the state operand are implementation specific. Optionally, sink symbol '_' can be used for the state argument. mbarrier.arrive operation on an mbarrier object located in .shared::cluster but not in .shared::cta cannot return a value. Sink symbol ‘_’ is mandatory for the destination operand for such cases.\"},\n",
       " {'id': 663,\n",
       "  'content': 'If the .sem qualifier is absent, .release is assumed by default. The optional .scope qualifier indicates the set of threads that directly observe the memory synchronizing effect of this operation, as described in the Memory Consistency Model . If the .scope qualifier is not specified then it defaults to .cta . In contrast, the .shared:: indicates the state space where the mbarrier resides. Support for sink symbol ‘_’ as the destination operand is introduced in PTX ISA version 7.1. Support for count argument without the modifier .noComplete introduced in PTX ISA version 7.8. Support for sub-qualifier ::cluster introduced in PTX ISA version 8.0. Support for qualifier .expect_tx is introduced in PTX ISA version 8.0. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes Requires sm_80 or higher. Support for count argument without the modifier .noComplete requires sm_90 or higher. Qualifier .expect_tx requires sm_90 or higher. Support for .cluster scope requires sm_90 or higher. Examples .reg .b32 cnt, remoteAddr32, remoteCTAId, addr32; .reg .b64 %r, addr, remoteAddr64; .shared .b64 shMem, shMem2; cvta.shared.u64 addr, shMem2; mov.b32 addr32, shMem2; mapa.shared::cluster.u32 remoteAddr32, addr32, remoteCTAId; mapa.u64 remoteAddr64, addr, remoteCTAId; cvta.shared.u64 addr, shMem2; mbarrier.arrive.shared.b64 %r0, [shMem]; mbarrier.arrive.shared::cta.b64 %r0, [shMem2]; mbarrier.arrive.release.cta.shared::cluster.b64 _, [remoteAddr32]; mbarrier.arrive.release.cluster.b64 _, [remoteAddr64], cnt; mbarrier.arrive.expect_tx.release.cluster.b64 _, [remoteAddr64], tx_count; mbarrier.arrive.noComplete.b64 %r1, [addr], 2; mbarrier.arrive.b64 %r2, [addr], cnt; 9.7.12.15.14. Parallel Synchronization and Communication Instructions: mbarrier.arrive_drop \\uf0c1 mbarrier.arrive_drop Decrements the expected count of the mbarrier object and performs arrive-on operation .'},\n",
       " {'id': 664,\n",
       "  'content': 'Syntax mbarrier.arrive_drop{.sem}{.scope}{.shared{::cta}}.b64 state, [addr]{, count}; mbarrier.arrive_drop{.sem}{.scope}{.shared::cluster}.b64 _, [addr] {,count}; mbarrier.arrive_drop.expect_tx{.shared{::cta}}{.sem}{.scope}.b64 state, [addr], tx_count; mbarrier.arrive_drop.expect_tx{.shared::cluster}{.sem}{.scope}.b64 _, [addr], tx_count; mbarrier.arrive_drop.noComplete{.sem}{.cta}{.shared{::cta}}.b64 state, [addr], count; .sem = { .release } .scope = { .cta, .cluster } Description A thread executing mbarrier.arrive_drop on the mbarrier object at the location specified by the address operand addr performs the following steps: Decrements the expected arrival count of the mbarrier object by the value specified by the 32-bit integer operand count . If count operand is not specified, it defaults to 1.'},\n",
       " {'id': 665,\n",
       "  'content': 'Performs an arrive-on operation on the mbarrier object . The operand count specifies the count argument to the arrive-on operation . The decrement done in the expected arrivals count of the mbarrier object will be for all the subsequent phases of the mbarrier object . When both qualifiers .arrive and .expect_tx are specified, then the count argument of the arrive-on operation is assumed to be 1. mbarrier.arrive_drop operation forms the release pattern as described in the Memory Consistency Model and synchronizes with the acquire patterns. The optional .scope qualifier indicates the set of threads that an mbarrier.arrive_drop instruction can directly synchronize. A mbarrier.arrive_drop with .noComplete qualifier must not complete the mbarrier, otherwise the behavior is undefined. A thread that wants to either exit or opt out of participating in the arrive-on operation can use mbarrier.arrive_drop to drop itself from the mbarrier . mbarrier.arrive_drop operation on an mbarrier object located in .shared::cta returns an opaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the destination operand state . Contents of the returned state are implementation specific. mbarrier.arrive_drop operation on an mbarrier object located in .shared::cluster but not in .shared::cta cannot return a value. Examples .reg .b32 cnt; .reg .b64 %r1; .shared .b64 shMem; // Example 1 @p mbarrier.arrive_drop.shared.b64 _, [shMem]; @p exit; @p2 mbarrier.arrive_drop.noComplete.shared.b64 _, [shMem], %a; @p2 exit; .. @!p mbarrier.arrive.shared.b64 %r1, [shMem]; @!p mbarrier.test_wait.shared.b64 q, [shMem], %r1; // Example 2 mbarrier.arrive_drop.shared::cluster.b64 _, [addr]; mbarrier.arrive_drop.shared::cta.release.cluster.b64 _, [addr], cnt; // Example 3 mbarrier.arrive_drop.expect_tx.shared::cta.release.cta.b64 state, [addr], tx_count; 9.7.12.15.15. Parallel Synchronization and Communication Instructions: cp.async.mbarrier.arrive \\uf0c1 cp.async.mbarrier.arrive Makes the mbarrier object track all prior cp.async operations initiated by the executing thread. Syntax cp.async.mbarrier.arrive{.noinc}{.shared{::cta}}.b64 [addr]; Description Causes an arrive-on operation to be triggered by the system on the mbarrier object upon the completion of all prior cp.async operations initiated by the executing thread. The mbarrier object is at the location specified by the operand addr . The arrive-on operation is asynchronous to execution of cp.async.mbarrier.arrive . When .noinc modifier is not specified, the pending count of the mbarrier object is incremented by 1 prior to the asynchronous arrive-on operation . This results in a zero-net change for the pending count from the asynchronous arrive-on operation during the current phase. The pending count of the mbarrier object after the increment should not exceed the limit as mentioned in Contents of the mbarrier object . When the .noinc modifier is specified, the increment to the pending count of the mbarrier object is not performed. Hence the decrement of the pending count done by the asynchronous arrive-on operation must be accounted for in the initialization of the mbarrier object . Examples // Example 1: no .noinc mbarrier.init.shared.b64 [shMem], threadCount; .... cp.async.ca.shared.global [shard1], [gbl1], 4; cp.async.cg.shared.global [shard2], [gbl2], 16; .... // Absence of .noinc accounts for arrive-on from completion of prior cp.async operations. // So mbarrier.init must only account for arrive-on from mbarrier.arrive. cp.async.mbarrier.arrive.shared.b64 [shMem]; .... mbarrier.arrive.shared.b64 state, [shMem]; waitLoop: mbarrier.test_wait.shared.b64 p, [shMem], state; @!p bra waitLoop; // Example 2: with .noinc // Tracks arrive-on from mbarrier.arrive and cp.async.mbarrier.arrive. // All threads participating in the mbarrier perform cp.async mov.b32 copyOperationCnt, threadCount; // 3 arrive-on operations will be triggered per-thread mul.lo.u32 copyArrivalCnt, copyOperationCnt, 3; add.u32 totalCount, threadCount, copyArrivalCnt; mbarrier.init.shared.b64 [shMem], totalCount; .... cp.async.ca.shared.global [shard1], [gbl1], 4; cp.async.cg.shared.global [shard2], [gbl2], 16; ... // Presence of .noinc requires mbarrier initalization to have accounted for arrive-on from cp.async cp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 1st instance .... cp.async.ca.shared.global [shard3], [gbl3], 4; cp.async.ca.shared.global [shard4], [gbl4], 16; cp.async.mbarrier.arrive.noinc.shared::cta.b64 [shMem]; // 2nd instance .... cp.async.ca.shared.global [shard5], [gbl5], 4; cp.async.cg.shared.global [shard6], [gbl6], 16; cp.async.mbarrier.arrive.noinc.shared.b64 [shMem]; // 3rd and last instance .... mbarrier.arrive.shared.b64 state, [shMem]; waitLoop: mbarrier.test_wait.shared.b64 p, [shMem], state; @!p bra waitLoop; 9.7.12.15.16. Parallel Synchronization and Communication Instructions: mbarrier.test_wait/mbarrier.try_wait \\uf0c1 mbarrier.test_wait/mbarrier.try_wait Checks whether the mbarrier object has completed the phase. Syntax mbarrier.test_wait{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], state; mbarrier.test_wait.parity{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity; mbarrier.try_wait{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], state {, suspendTimeHint}; mbarrier.try_wait.parity{.sem}{.scope}{.shared{::cta}}.b64 waitComplete, [addr], phaseParity {, suspendTimeHint}; .sem = { .acquire } .scope = { .cta, .cluster } Description The test_wait and try_wait operations test for the completion of the current or the immediately preceding phase of an mbarrier object at the location specified by the operand addr . mbarrier.test_wait is a non-blocking instruction which tests for the completion of the phase. mbarrier.try_wait is a potentially blocking instruction which tests for the completion of the phase. If the phase is not complete, the executing thread may be suspended. Suspended thread resumes execution when the specified phase completes OR before the phase completes following a system-dependent time limit. The optional 32-bit unsigned integer operand suspendTimeHint specifies the time limit, in nanoseconds, that may be used for the time limit instead of the system-dependent limit. mbarrier.test_wait and mbarrier.try_wait test for completion of the phase : Specified by the operand state , which was returned by an mbarrier.arrive instruction on the same mbarrier object during the current or the immediately preceding phase. Or Indicated by the operand phaseParity , which is the integer parity of either the current phase or the immediately preceding phase of the mbarrier object . The .parity variant of the instructions test for the completion of the phase indicated by the operand phaseParity , which is the integer parity of either the current phase or the immediately preceding phase of the mbarrier object . An even phase has integer parity 0 and an odd phase has integer parity of 1. So the valid values of phaseParity operand are 0 and 1. Note: the use of the .parity variants of the instructions requires tracking the phase of an mbarrier object throughout its lifetime. The test_wait and try_wait operations are valid only for : the current incomplete phase, for which waitComplete returns False . the immediately preceding phase, for which waitComplete returns True . When mbarrier.test_wait and mbarrier.try_wait operations return True , they form the acquire pattern as described in the Memory Consistency Model . The optional .scope qualifier indicates the set of threads that the mbarrier.test_wait and mbarrier.try_wait instructions can directly synchronize. The following ordering of memory operations hold for the executing thread when mbarrier.test_wait or mbarrier.try_wait returns True : All memory accesses (except async operations ) requested prior, in program order, to mbarrier.arrive during the completed phase by the participating threads of the CTA are performed and are visible to the executing thread. All cp.async operations requested prior, in program order, to cp.async.mbarrier.arrive during the completed phase by the participating threads of the CTA are performed and made visible to the executing thread. All cp.async.bulk asynchronous operations using the same mbarrier object requested prior, in program order, to mbarrier.arrive during the completed phase by the participating threads of the CTA are performed and made visible to the executing thread. All memory accesses requested after the mbarrier.test_wait or mbarrier.try_wait , in program order, are not performed and not visible to memory accesses performed prior to mbarrier.arrive , in program order, by other threads participating in the mbarrier . There is no ordering and visibility guarantee for memory accesses requested by the thread after mbarrier.arrive and prior to mbarrier.test_wait , in program order. PTX ISA Notes mbarrier.test_wait introduced in PTX ISA version 7.0. Modifier .parity is introduced in PTX ISA version 7.1. mbarrier.try_wait introduced in PTX ISA version 7.8. Support for .scope and .sem qualifiers introduced in PTX ISA version 8.0 Target ISA Notes mbarrier.test_wait requires sm_80 or higher. mbarrier.try_wait requires sm_90 or higher. Examples // Example 1a, thread synchronization with test_wait: .reg .b64 %r1; .shared .b64 shMem; mbarrier.init.shared.b64 [shMem], N; // N threads participating in the mbarrier. ... mbarrier.arrive.shared.b64 %r1, [shMem]; // N threads executing mbarrier.arrive // computation not requiring mbarrier synchronization... waitLoop: mbarrier.test_wait.shared.b64 complete, [shMem], %r1; @!complete nanosleep.u32 20; @!complete bra waitLoop; // Example 1b, thread synchronization with try_wait : .reg .b64 %r1; .shared .b64 shMem; mbarrier.init.shared.b64 [shMem], N; // N threads participating in the mbarrier. ... mbarrier.arrive.shared.b64 %r1, [shMem]; // N threads executing mbarrier.arrive // computation not requiring mbarrier synchronization... waitLoop: mbarrier.try_wait.shared.b64 complete, [shMem], %r1; @!complete bra waitLoop; // Example 2, thread synchronization using phase parity : .reg .b32 i, parArg; .reg .b64 %r1; .shared .b64 shMem; mov.b32 i, 0; mbarrier.init.shared.b64 [shMem], N; // N threads participating in the mbarrier. ... loopStart : // One phase per loop iteration ... mbarrier.arrive.shared.b64 %r1, [shMem]; // N threads ... and.b32 parArg, i, 1; waitLoop: mbarrier.test_wait.parity.shared.b64 complete, [shMem], parArg; @!complete nanosleep.u32 20; @!complete bra waitLoop; ... add.u32 i, i, 1; setp.lt.u32 p, i, IterMax; @p bra loopStart; // Example 3, Asynchronous copy completion waiting : .reg .b64 state; .shared .b64 shMem2; .shared .b64 shard1, shard2; .global .b64 gbl1, gbl2; mbarrier.init.shared.b64 [shMem2], threadCount; ... cp.async.ca.shared.global [shard1], [gbl1], 4; cp.async.cg.shared.global [shard2], [gbl2], 16; // Absence of .noinc accounts for arrive-on from prior cp.async operation cp.async.mbarrier.arrive.shared.b64 [shMem2]; ... mbarrier.arrive.shared.b64 state, [shMem2]; waitLoop: mbarrier.test_wait.shared::cta.b64 p, [shMem2], state; @!p bra waitLoop; // Example 4, Synchronizing the CTA0 threads with cluster threads .reg .b64 %r1, addr, remAddr; .shared .b64 shMem; cvta.shared.u64 addr, shMem; mapa.u64 remAddr, addr, 0; // CTA0’s shMem instance // One thread from CTA0 executing the below initialization operation @p0 mbarrier.init.shared::cta.b64 [shMem], N; // N = no of cluster threads barrier.cluster.arrive; barrier.cluster.wait; // Entire cluster executing the below arrive operation mbarrier.arrive.release.cluster.b64 _, [remAddr]; // computation not requiring mbarrier synchronization ... // Only CTA0 threads executing the below wait operation waitLoop: mbarrier.try_wait.parity.acquire.cluser.shared::cta.b64 complete, [shMem], 0; @!complete bra waitLoop; 9.7.12.15.17. Parallel Synchronization and Communication Instructions: mbarrier.pending_count \\uf0c1 mbarrier.pending_count Query the pending arrival count from the opaque mbarrier state. Syntax mbarrier.pending_count.b64 count, state; Description The pending count can be queried from the opaque mbarrier state using mbarrier.pending_count . The state operand is a 64-bit register that must be the result of a prior mbarrier.arrive.noComplete or mbarrier.arrive_drop.noComplete instruction. The destination register count is a 32-bit unsigned integer representing the pending count of the mbarrier object prior to the arrive-on operation from which the state register was obtained. Examples .reg .b32 %r1; .reg .b64 state; .shared .b64 shMem; mbarrier.arrive.noComplete.b64 state, [shMem], 1; mbarrier.pending_count.b64 %r1, state; 9.7.12.15.18. Parallel Synchronization and Communication Instructions: tensormap.cp_fenceproxy \\uf0c1 tensormap.cp_fenceproxy A fused copy and fence operation. Syntax tensormap.cp_fenceproxy.cp_qualifiers.fence_qualifiers.sync.aligned [dst], [src], size; .cp_qualifiers = { .global.shared::cta } .fence_qualifiers = { .to_proxy::from_proxy.release.scope } .to_proxy::from_proxy = { .tensormap::generic } .scope = { .cta, .cluster, .gpu , .sys } Description The tensormap.cp_fence instructions perform the following operations in order : Copies data of size specified by the size argument, in bytes, from the location specified by the address operand src in shared memory to the location specified by the address operand dst in the global memory, in the generic proxy. Establishes a uni-directional proxy release pattern on the ordering from the copy operation to the subsequent access performed in the tensormap proxy on the address dst . The valid value of size operand is 128. The operands src and dst specify non-generic addresses in shared::cta and global state space respectively. The optional .scope qualifier specifies the set of threads that can directly observe the proxy synchronizing effect of this operation, as described in Memory Consistency Model . The mandatory .sync qualifier indicates that tensormap.cp_fenceproxy causes the executing thread to wait until all threads in the warp execute the same tensormap.cp_fenceproxy instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same tensormap.cp_fenceproxy instruction. In conditionally executed code, an aligned tensormap.cp_fenceproxy instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined. Examples // Example: manipulate a tensor-map object and then consume it in cp.async.bulk.tensor .reg .b64 new_addr; .global .align 128 .b8 gbl[128]; .shared .align 128 .b8 sMem[128]; cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::bytes [sMem], [gMem], 128, [mbar]; ... try_wait_loop: mbarrier.try_wait.shared.b64 p, [mbar], state; @!p bra try_wait loop; tensormap.replace.tile.global_address.shared.b1024.b64 [sMem], new_addr; tensormap.cp_fenceproxy.global.shared::cta.proxy.tensormap::generic.release.gpu .sync.aligned [gbl], [sMem], 128; fence.proxy.tensormap::generic.acquire.gpu [gbl], 128; cp.async.bulk.tensor.1d.shared::cluster.global.tile [addr0], [gbl, {tc0}], [mbar0]; 9.7.13. Warp Level Matrix Multiply-Accumulate Instructions \\uf0c1 The matrix multiply and accumulate operation has the following form: D = A * B + C where D and C are called accumulators and may refer to the same matrix. PTX provides two ways to perform matrix multiply-and-accumulate computation: Using wmma instructions: This warp-level computation is performed collectively by all threads in the warp as follows: Load matrices A, B and C from memory into registers using the wmma.load operation. When the operation completes, the destination registers in each thread hold a fragment of the loaded matrix. Perform the matrix multiply and accumulate operation using the wmma.mma operation on the loaded matrices. When the operation completes, the destination registers in each thread hold a fragment of the result matrix returned by the wmma.mma operation. Store result Matrix D back to memory using the wmma.store operation. Alternately, result matrix D can also be used as argument C for a subsequent wmma.mma operation. The wmma.load and wmma.store instructions implicitly handle the organization of matrix elements when loading the input matrices from memory for the wmma.mma operation and when storing the result back to memory. Using mma instruction: Similar to wmma , mma also requires computation to be performed collectively by all threads in the warp however distribution of matrix elements across different threads in warp needs to be done explicitly before invoking the mma operation. The mma instruction supports both dense as well as sparse matrix A. The sparse variant can be used when A is a structured sparse matrix as described in Sparse matrix storage . 9.7.13.1. Matrix Shape \\uf0c1 The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices A, B and C. The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while C and D are MxN matrices. The following matrix shapes are supported for the specified types: Instruction Sparsity Multiplicand Data-type Shape PTX ISA version wmma Dense Floating-point - .f16 .m16n16k16 , .m8n32k16 , and .m32n8k16 PTX ISA version 6.0 wmma Dense Alternate floating-point format - .bf16 .m16n16k16 , .m8n32k16 , and .m32n8k16 PTX ISA version 7.0 wmma Dense Alternate floating-point format - .tf32 .m16n16k8 PTX ISA version 7.0 wmma Dense Integer - .u8 / .s8 .m16n16k16 , .m8n32k16 , and .m32n8k16 PTX ISA version 6.3 wmma Dense Sub-byte integer - .u4 / .s4 .m8n8k32 PTX ISA version 6.3 (preview feature) wmma Dense Single-bit - .b1 .m8n8k128 PTX ISA version 6.3 (preview feature) mma Dense Floating-point - .f64 .m8n8k4 PTX ISA version 7.0 .m16n8k4 , .m16n8k8 , and .m16n8k16 PTX ISA version 7.8 mma Dense Floating-point - .f16 .m8n8k4 PTX ISA version 6.4 .m16n8k8 PTX ISA version 6.5 .m16n8k16 PTX ISA version 7.0 mma Dense Alternate floating-point format - .bf16 .m16n8k8 and .m16n8k16 PTX ISA version 7.0 mma Dense Alternate floating-point format - .tf32 .m16n8k4 and .m16n8k8 PTX ISA version 7.0 mma Dense Integer - .u8 / .s8 .m8n8k16 PTX ISA version 6.5 .m16n8k16 and .m16n8k32 PTX ISA version 7.0 mma Dense Sub-byte integer - .u4 / .s4 .m8n8k32 PTX ISA version 6.5 .m16n8k32 and .m16n8k64 PTX ISA version 7.0 mma Dense Single-bit - .b1 .m8n8k128 , .m16n8k128 , and .m16n8k256 PTX ISA version 7.0 mma Dense Alternate floating-point format - .e4m3 / .e5m2 .m16n8k32 PTX ISA version 8.4 mma Sparse Floating-point - .f16 .m16n8k16 and .m16n8k32 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .bf16 .m16n8k16 and .m16n8k32 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .tf32 .m16n8k8 and .m16n8k16 PTX ISA version 7.1 mma Sparse Integer - .u8 / .s8 .m16n8k32 and .m16n8k64 PTX ISA version 7.1 mma Sparse Sub-byte integer - .u4 / .s4 .m16n8k64 and .m16n8k128 PTX ISA version 7.1 mma Sparse Alternate floating-point format - .e4m3 / .e5m2 .m16n8k64 PTX ISA version 8.4 mma Sparse with ordered metadata Floating-point - .f16 .m16n8k16 and .m16n8k32 PTX ISA version 8.5 mma Sparse with ordered metadata Alternate floating-point format - .bf16 .m16n8k16 and .m16n8k32 PTX ISA version 8.5 mma Sparse with ordered metadata Alternate floating-point format - .tf32 .m16n8k8 and .m16n8k16 PTX ISA version 8.5 mma Sparse with ordered metadata Integer - .u8 / .s8 .m16n8k32 and .m16n8k64 PTX ISA version 8.5 mma Sparse with ordered metadata Sub-byte integer - .u4 / .s4 .m16n8k64 and .m16n8k128 PTX ISA version 8.5 mma Sparse with ordered metadata Alternate floating-point format - .e4m3 / .e5m2 .m16n8k64 PTX ISA version 8.5 9.7.13.2. Matrix Data-types \\uf0c1 The matrix multiply and accumulate operation is supported separately on integer, floating-point, sub-byte integer and single bit data-types. All operands must contain the same basic type kind, i.e., integer or floating-point. For floating-point matrix multiply and accumulate operation, different matrix operands may have different precision, as described later. Data-type Multiplicands (A or B) Accumulators (C or D) Integer .u8 , .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 or .e5m2 .f32 Floating Point .f64 .f64 Sub-byte integer both .u4 or both .s4 .s32 Single-bit integer .b1 .s32 9.7.13.3. Matrix multiply-accumulate operation using wmma instructions \\uf0c1 This section describes warp level wmma.load, wmma.mma and wmma.store instructions and the organization of various matrices invovled in these instruction. 9.7.13.3.1. Matrix Fragments for WMMA \\uf0c1 Each thread in the warp holds a fragment of the matrix. The distribution of fragments loaded by the threads in a warp is unspecified and is target architecture dependent, and hence the identity of the fragment within the matrix is also unspecified and is target architecture dependent. The fragment returned by a wmma operation can be used as an operand for another wmma operation if the shape, layout and element type of the underlying matrix matches. Since fragment layout is architecture dependent, using the fragment returned by a wmma operation in one function as an operand for a wmma operation in a different function may not work as expected if the two functions are linked together but were compiled for different link-compatible SM architectures. Note passing wmma fragment to a function having .weak linkage is unsafe since at link time references to such function may get resolved to a function in different compilation module. Each fragment is a vector expression whose contents are determined as follows.'},\n",
       " {'id': 666,\n",
       "  'content': 'The identity of individual matrix elements in the fragment is unspecified. Integer fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .u8 or .s8 .m16n16k16 A A vector expression of two .b32 registers, with each register containing four elements from the matrix. B A vector expression of two .b32 registers, with each register containing four elements from the matrix. .m8n32k16 A A vector expression containing a single .b32 register containing four elements from the matrix. B A vector expression of four .b32 registers, with each register containing four elements from the matrix. .m32n8k16 A A vector expression of four .b32 registers, with each register containing four elements from the matrix. B A vector expression containing single .b32 register, with each containing four elements from the matrix. Accumulators (C or D): Data-type Shape Fragment .s32 .m16n16k16 A vector expression of eight .s32 registers. .m8n32k16 .m32n8k16 Floating point fragments Data-type Matrix Fragment .f16 A or B A vector expression of eight .f16x2 registers. .f16 C or D A vector expression of four .f16x2 registers. .f32 A vector expression of eight .f32 registers. Floating point fragments for .bf16 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .bf16 .m16n16k16 A A vector expression of four .b32 registers, with each register containing two elements from the matrix. B .m8n32k16 A A vector expression containing a two .b32 registers, with containing two elements from the matrix. B A vector expression of eight .b32 registers, with each register containing two elements from the matrix. .m32n8k16 A A vector expression of eight .b32 registers, with each register containing two elements from the matrix. B A vector expression containing two .b32 registers, with each containing two elements from the matrix. Accumulators (C or D): Data-type Matrix Fragment .f32 C or D A vector expression containing eight .f32 registers. Floating point fragments for .tf32 data format Multiplicands (A or B): Data-type Shape Matrix Fragment .tf32 .m16n16k8 A A vector expression of four .b32 registers. B A vector expression of four .b32 registers. Accumulators (C or D): Data-type Shape Matrix Fragment .f32 .m16n16k8 C or D A vector expression containing eight .f32 registers. Double precision floating point fragments Multiplicands (A or B): Data-type Shape Matrix Fragment .f64 .m8n8k4 A or B A vector expression of single .f64 register. Accumulators (C or D): Data-type Shape Matrix Fragment .f64 .m8n8k4 C or D A vector expression containing single .f64 register. Sub-byte integer and single-bit fragments Multiplicands (A or B): Data-type Shape Fragment .u4 or .s4 .m8n8k32 A vector expression containing a single .b32 register, containing eight elements from the matrix. .b1 .m8n8k128 A vector expression containing a single .b32 register, containing 32 elements from the matrix. Accumulators (C or D): Data-type Shape Fragment .s32 .m8n8k32 A vector expression of two .s32 registers. .m8n8k128 A vector expression of two .s32 registers. Manipulating fragment contents The contents of a matrix fragment can be manipulated by reading and writing to individual registers in the fragment, provided the following conditions are satisfied: All matrix element in the fragment are operated on uniformly across threads, using the same parameters. The order of the matrix elements is not changed. For example, if each register corresponding to a given matrix is multiplied by a uniform constant value, then the resulting matrix is simply the scaled version of the original matrix. Note that type conversion between .f16 and .f32 accumulator fragments is not supported in either direction. The result is undefined even if the order of elements in the fragment remains unchanged.'},\n",
       " {'id': 667,\n",
       "  'content': '9.7.13.3.2. Matrix Storage for WMMA \\uf0c1 Each matrix can be stored in memory with a row-major or column-major layout. In a row-major format, consecutive elements of each row are stored in contiguous memory locations, and the row is called the leading dimension of the matrix. In a column-major format, consecutive elements of each column are stored in contiguous memory locations and the column is called the leading dimension of the matrix. Consecutive instances of the leading dimension (rows or columns) need not be stored contiguously in memory. The wmma.load and wmma.store operations accept an optional argument stride that specifies the offset from the beginning of each row (or column) to the next, in terms of matrix elements (and not bytes). For example, the matrix being accessed by a wmma operation may be a submatrix from a larger matrix stored in memory. This allows the programmer to compose a multiply-and-accumulate operation on matrices that are larger than the shapes supported by the wmma operation. Address Alignment: The starting address of each instance of the leading dimension (row or column) must be aligned with the size of the corresponding fragment in bytes. Note that the starting address is determined by the base pointer and the optional stride . Consider the following instruction as an example: wmma.load.a.sync.aligned.row.m16n16k16.f16 {x0,...,x7}, [p], s; Fragment size in bytes = 32 (eight elements of type .f16x2 ) Actual stride in bytes = 2 * s (since stride is specified in terms of .f16 elements, not bytes) For each row of this matrix to be aligned at fragment size the following must be true: p is a multiple of 32. 2*s is a multiple of 32.'},\n",
       " {'id': 668,\n",
       "  'content': 'Default value for stride: The default value of the stride is the size of the leading dimension of the matrix. For example, for an MxK matrix, the stride is K for a row-major layout and M for a column-major layout. In particular, the default strides for the supported matrix shapes are as follows: Shape A (row) A (column) B (row) B (column) Accumulator (row) Accumulator (column) 16x16x16 16 16 16 16 16 16 8x32x16 16 8 32 16 32 8 32x8x16 16 32 8 16 8 32 8x8x32 32 8 8 32 8 8 8x8x128 128 8 8 128 8 8 16x16x8 8 16 16 8 16 16 8x8x4 4 8 8 4 8 8 9.7.13.3.3. Warp-level Matrix Load Instruction: wmma.load \\uf0c1 wmma.load Collectively load a matrix from memory for WMMA Syntax Floating point format .f16 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride}; wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride}; wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride}; .layout = {.row, .col}; .shape = {.m16n16k16, .m8n32k16, .m32n8k16}; .ss = {.global, .shared{::cta}}; .atype = {.f16, .s8, .u8}; .btype = {.f16, .s8, .u8}; .ctype = {.f16, .f32, .s32}; Alternate floating point format .bf16 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride} wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride} wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride} .layout = {.row, .col}; .shape = {.m16n16k16, .m8n32k16, .m32n8k16}; .ss = {.global, .shared{::cta}}; .atype = {.bf16 }; .btype = {.bf16 }; .ctype = {.f32 }; Alternate floating point format .tf32 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride} wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride} wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride} .layout = {.row, .col}; .shape = {.m16n16k8 }; .ss = {.global, .shared{::cta}}; .atype = {.tf32 }; .btype = {.tf32 }; .ctype = {.f32 }; Double precision Floating point .f64 loads: wmma.load.a.sync.aligned.layout.shape{.ss}.atype r, [p] {, stride} wmma.load.b.sync.aligned.layout.shape{.ss}.btype r, [p] {, stride} wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride} .layout = {.row, .col}; .shape = {.m8n8k4 }; .ss = {.global, .shared{::cta}}; .atype = {.f64 }; .btype = {.f64 }; .ctype = {.f64 }; Sub-byte loads: wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride} wmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride} wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride} .layout = {.row, .col}; .shape = {.m8n8k32}; .ss = {.global, .shared{::cta}}; .atype = {.s4, .u4}; .btype = {.s4, .u4}; .ctype = {.s32}; Single-bit loads: wmma.load.a.sync.aligned.row.shape{.ss}.atype r, [p] {, stride} wmma.load.b.sync.aligned.col.shape{.ss}.btype r, [p] {, stride} wmma.load.c.sync.aligned.layout.shape{.ss}.ctype r, [p] {, stride} .layout = {.row, .col}; .shape = {.m8n8k128}; .ss = {.global, .shared{::cta}}; .atype = {.b1}; .btype = {.b1}; .ctype = {.s32}; Description Collectively load a matrix across all threads in a warp from the location indicated by address operand p in the specified state space into destination register r . wmma.load operation may be used only with .global and .shared spaces and with generic addressing, where the address points to .global or .shared space. The mutually exclusive qualifiers .a , .b and .c indicate whether matrix A, B or C is being loaded respectively for the wmma computation. The destination operand r is a brace-enclosed vector expression that can hold the fragment returned by the load operation, as described in Matrix Fragments for WMMA . The .shape qualifier indicates the dimensions of all the matrix arguments involved in the intended wmma computation. The .layout qualifier indicates whether the matrix to be loaded is stored in row-major or column-major format. stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements between the start of consecutive instances of the leading dimension (rows or columns). The default value of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than the default. For example, if the matrix is a sub-matrix of a larger matrix, then the value of stride is the leading dimension of the larger matrix. Specifying a value lower than the default value results in undefined behavior. The required alignment for address p and stride is described in the Matrix Storage for WMMA . The mandatory .sync qualifier indicates that wmma.load causes the executing thread to wait until all threads in the warp execute the same wmma.load instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.load instruction. In conditionally executed code, a wmma.load instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined. The behavior of wmma.load is undefined if all threads do not use the same qualifiers and the same values of p and stride , or if any thread in the warp has exited. wmma.load is treated as a weak memory operation in the Memory Consistency Model . .m8n32k16 and .m32n8k16 introduced in PTX ISA version 6.1. Integer, sub-byte integer and single-bit wmma introduced in PTX ISA version 6.3. .m8n8k4 and .m16n16k8 on wmma introduced in PTX ISA version 7.0. Double precision and alternate floating point precision wmma introduced in PTX ISA version 7.0. Modifier .aligned is required from PTX ISA version 6.3 onwards, and considered implicit in PTX ISA versions less than 6.3. Support for ::cta sub-qualifier introduced in PTX ISA version 7.8. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA version 6.3. Target ISA Notes Floating point wmma requires sm_70 or higher. Integer wmma requires sm_72 or higher. Sub-byte and single-bit wmma requires sm_75 or higher.'},\n",
       " {'id': 669,\n",
       "  'content': 'Double precision and alternate floating point precision wmma requires sm_80 or higher. Examples // Load elements from f16 row-major matrix B .reg .b32 x; wmma.load.b.sync.aligned.m16n16k16.row.f16 {x0,x1,x2,x3,x4,x5,x,x7}, [ptr]; // Now use {x0, ..., x7} for the actual wmma.mma // Load elements from f32 column-major matrix C and scale the values: .reg .b32 x; wmma.load.c.sync.aligned.m16n16k16.col.f32 {x0,x1,x2,x3,x4,x5,x6,x7}, [ptr]; mul.f32 x0, x0, 0.1; // repeat for all registers x; ... mul.f32 x7, x7, 0.1; // Now use {x0, ..., x7} for the actual wmma.mma // Load elements from integer matrix A: .reg .b32 x // destination registers x contain four packed .u8 values each wmma.load.a.sync.aligned.m32n8k16.row.u8 {x0,x1,x2,x3}, [ptr]; // Load elements from sub-byte integer matrix A: .reg .b32 x0; // destination register x0 contains eight packed .s4 values wmma.load.a.sync.aligned.m8n8k32.row.s4 {x0}, [ptr]; // Load elements from .bf16 matrix A: .reg .b32 x; wmma.load.a.sync.aligned.m16n16k16.row.bf16 {x0,x1,x2,x3}, [ptr]; // Load elements from .tf32 matrix A: .reg .b32 x; wmma.load.a.sync.aligned.m16n16k8.row.tf32 {x0,x1,x2,x3}, [ptr]; // Load elements from .f64 matrix A: .reg .b32 x; wmma.load.a.sync.aligned.m8n8k4.row.f64 {x0}, [ptr]; 9.7.13.3.4. Warp-level Matrix Store Instruction: wmma.store \\uf0c1 wmma.store Collectively store a matrix into memory for WMMA Syntax wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride}; .layout = {.row, .col}; .shape = {.m16n16k16, .m8n32k16, .m32n8k16}; .ss = {.global, .shared{::cta}}; .type = {.f16, .f32, .s32}; wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride} .layout = {.row, .col}; .shape = {.m8n8k32, .m8n8k128}; .ss = {.global, .shared{::cta}}; .type = {.s32}; wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride} .layout = {.row, .col}; .shape = {.m16n16k8}; .ss = {.global, .shared{::cta}}; .type = {.f32}; wmma.store.d.sync.aligned.layout.shape{.ss}.type [p], r {, stride} .layout = {.row, .col}; .shape = {.m8n8k4 }; .ss = {.global, .shared{::cta}}; .type = {.f64}; Description Collectively store a matrix across all threads in a warp at the location indicated by address operand p in the specified state space from source register r . The source operand r is a brace-enclosed vector expression that matches the shape of the fragment expected by the store operation, as described in Matrix Fragments for WMMA . It must match the .shape qualifier specified on the wmma.mma instruction that produced the D matrix being stored. The mandatory .sync qualifier indicates that wmma.store causes the executing thread to wait until all threads in the warp execute the same wmma.store instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.store instruction. In conditionally executed code, a wmma.store instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined. The behavior of wmma.store is undefined if all threads do not use the same qualifiers and the same values of p and stride , or if any thread in the warp has exited. wmma.store is treated as a weak memory operation in the Memory Consistency Model . .m16n16k8 introduced in PTX ISA version 7.0. Double precision wmma introduced in PTX ISA version 7.0. Double precision wmma and shape .m16n16k8 requires sm_80 or higher. Examples // Storing f32 elements computed by a wmma.mma .reg .b32 x; wmma.mma.sync.m16n16k16.row.col.f32.f32 {d0, d1, d2, d3, d4, d5, d6, d7}, ...; wmma.store.d.sync.m16n16k16.row.f32 [ptr], {d0, d1, d2, d3, d4, d5, d6, d7}; // Store s32 accumulator for m16n16k16 shape: .reg .b32 d; wmma.store.d.sync.aligned.m16n16k16.row.s32 [ptr], {d0, d1, d2, d3, d4, d5, d6, d7}; // Store s32 accumulator for m8n8k128 shape: .reg .b32 d wmma.store.d.sync.aligned.m8n8k128.row.s32 [ptr], {d0, d1}; // Store f64 accumulator for m8n8k4 shape: .reg .f64 d; wmma.store.d.sync.aligned.m8n8k4.row.f64 [ptr], {d0, d1}; 9.7.13.3.5. Warp-level Matrix Multiply-and-Accumulate Instruction: wmma.mma \\uf0c1 wmma.mma Perform a single matrix multiply-and-accumulate operation across a warp Syntax // Floating point (.f16 multiplicands) wmma.mma wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype d, a, b, c; // Integer (.u8/.s8 multiplicands) wmma.mma wmma.mma.sync.aligned.alayout.blayout.shape.s32.atype.btype.s32{.satfinite} d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .shape = {.m16n16k16, .m8n32k16, .m32n8k16}; .dtype = {.f16, .f32}; .atype = {.s8, .u8}; .btype = {.s8, .u8}; .ctype = {.f16, .f32}; Floating point format .bf16 wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .shape = {.m16n16k16, .m8n32k16, .m32n8k16}; .atype = {.bf16 }; .btype = {.bf16}; Floating point format .tf32 wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape.f32.atype.btype.f32 d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .shape = {.m16n16k8 }; .atype = {.tf32 }; .btype = {.tf32}; Floating point Double precision wmma.mma : wmma.mma.sync.aligned.alayout.blayout.shape{.rnd}.f64.f64.f64.f64 d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .shape = {.m8n8k4 }; .rnd = { .rn, .rz, .rm, .rp }; Sub-byte ( .u4 / .s4 multiplicands) wmma.mma : wmma.mma.sync.aligned.row.col.shape.s32.atype.btype.s32{.satfinite} d, a, b, c; .shape = {.m8n8k32}; .atype = {.s4, .u4}; .btype = {.s4, .u4}; Single-bit ( .b1 multiplicands) wmma.mma : wmma.mma.op.popc.sync.aligned.row.col.shape.s32.atype.btype.s32 d, a, b, c; .shape = {.m8n8k128}; .atype = {.b1}; .btype = {.b1}; .op = {.xor, .and} Description Perform a warp-level matrix multiply-and-accumulate computation D = A * B + C using matrices A, B and C loaded in registers a , b and c respectively, and store the result matrix in register d . The register arguments a , b , c and d hold unspecified fragments of the corresponding matrices as described in Matrix Fragments for WMMA The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the elements in the matrices D, A, B and C respectively.'},\n",
       " {'id': 670,\n",
       "  'content': 'For wmma.mma without explicit .atype and .btype : .atype and .btype are implicitly set to .f16 . For integer wmma , .ctype and .dtype must be specified as .s32 . Also, the values for .atype and .btype must be the same, i.e., either both are .s8 or both are .u8 . For sub-byte single-bit wmma , .ctype and .dtype must be specified as .s32 . Also, the values for .atype and .btype must be the same; i.e., either both are .s4 , both are .u4 , or both are .b1 . For single-bit wmma , multiplication is replaced by a sequence of logical operations; specifically, wmma.xor.popc and wmma.and.popc computes the XOR, AND respectively of a 128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result ( popc ). This result is added to the corresponding element of C and written into D. The qualifiers .alayout and .blayout must match the layout specified on the wmma.load instructions that produce the contents of operands a and b respectively. Similarly, the qualifiers .atype , .btype and .ctype must match the corresponding qualifiers on the wmma.load instructions that produce the contents of operands a , b and c respectively. The .shape qualifier must match the .shape qualifier used on the wmma.load instructions that produce the contents of all three input operands a , b and c respectively. The destination operand d is a brace-enclosed vector expression that matches the .shape of the fragment computed by the wmma.mma instruction. Saturation at the output: The optional qualifier .satfinite indicates that the final values in the destination register are saturated as follows: The output is clamped to the minimum or maximum 32-bit signed integer value. Otherwise, if the accumulation would overflow, the value wraps. Precision and rounding for .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .ctype or .dtype is .f32 , accumulation of the intermediate values is performed with at least single precision. When both .ctype and .dtype are specified as .f16 , the accumulation is performed with at least half precision. The accumulation order, rounding and handling of subnormal inputs is unspecified. Precision and rounding for .bf16 , .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision. Accumulation of the intermediate values is performed with at least single precision. Rounding modifiers on double precision wmma.mma (default is .rn ): .rn mantissa LSB rounds to nearest even .rz mantissa LSB rounds towards zero .rm mantissa LSB rounds towards negative infinity .rp mantissa LSB rounds towards positive infinity The mandatory .sync qualifier indicates that wmma.mma causes the executing thread to wait until all threads in the warp execute the same wmma.mma instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same wmma.mma instruction. In conditionally executed code, a wmma.mma instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined. The behavior of wmma.mma is undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited. Support for .and operation in single-bit wmma introduced in PTX ISA version 7.1. Support for .satfinite on floating point wmma.mma is deprecated in PTX ISA version 6.4 and is removed from PTX ISA version 6.5. Preview Feature: Sub-byte wmma and single-bit wmma are preview features in PTX ISA. Double precision, alternate floating point precision wmma require sm_80 or higher.'},\n",
       " {'id': 671,\n",
       "  'content': '.and operation in single-bit wmma requires sm_80 or higher. Examples .global .align 32 .f16 A[256], B[256]; .global .align 32 .f32 C[256], D[256]; .reg .b32 a b c d; wmma.load.a.sync.aligned.m16n16k16.global.row.f16 {a0, a1, a2, a3, a4, a5, a6, a7}, [A]; wmma.load.b.sync.aligned.m16n16k16.global.col.f16 {b0, b1, b2, b3, b4, b5, b6, b7}, [B]; wmma.load.c.sync.aligned.m16n16k16.global.row.f32 {c0, c1, c2, c3, c4, c5, c6, c7}, [C]; wmma.mma.sync.aligned.m16n16k16.row.col.f32.f32 {d0, d1, d2, d3, d4, d5, d6, d7}, {a0, a1, a2, a3, a4, a5, a6, a7}, {b0, b1, b2, b3, b4, b5, b6, b7}, {c0, c1, c2, c3, c4, c5, c6, c7}; wmma.store.d.sync.aligned.m16n16k16.global.col.f32 [D], {d0, d1, d2, d3, d4, d5, d6, d7}; // Compute an integer WMMA: .reg .b32 a, b; .reg .b32 c, d; wmma.mma.sync.aligned.m8n32k16.row.col.s32.s8.s8.s32 {d0, d1, d2, d3, d4, d5, d6, d7}, {a}, {b0, b1, b2, b3}, {c0, c1, c2, c3, c4, c5, c6, c7}; // Compute sub-byte WMMA: .reg .b32 a, b, c d wmma.mma.sync.aligned.m8n8k32.row.col.s32.s4.s4.s32 {d0, d1}, {a}, {b}, {c0, c1}; // Compute single-bit type WMMA: .reg .b32 a, b, c d wmma.mma.xor.popc.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32 {d0, d1}, {a}, {b}, {c0, c1}; // Compute double precision wmma .reg .f64 a, b, c, d; wmma.mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64 {d0, d1}, {a}, {b}, {c0, c1}; // Compute alternate floating point precision wmma .reg .b32 a, b, c, d; wmma.mma.sync.aligned.m16n16k8.row.col.f32.tf32.tf32.f32 {d0, d1, d2, d3, d4, d5, d6, d7}, {a0, a1, a2, a3}, {b0, b1, b2, b3}, {c0, c1, c2, c3, c4, c5, c6, c7}; 9.7.13.4. Matrix multiply-accumulate operation using mma instruction \\uf0c1 This section describes warp-level mma , ldmatrix , stmatrix , and movmatrix instructions and the organization of various matrices involved in these instructions.'},\n",
       " {'id': 672,\n",
       "  'content': '9.7.13.4.1. Matrix Fragments for mma.m8n8k4 with .f16 floating point type \\uf0c1 A warp executing mma.m8n8k4 with .f16 floating point type will compute 4 MMA operations of shape .m8n8k4 . Elements of 4 matrices need to be distributed across the threads in a warp. The following table shows distribution of matrices for MMA operations. MMA Computation Threads participating in MMA computation MMA computation 1 Threads with %laneid 0-3 (low group) and 16-19 (high group) MMA computation 2 Threads with %laneid 4-7 (low group) and 20-23 (high group) MMA computation 3 Threads with %laneid 8-11 (low group) and 24-27 (high group) MMA computation 4 Threads with %laneid 12-15 (low group) and 28-31 (high group) For each of the individual MMA computation shown above, each of the required thread holds a fragment of the matrix for performing mma operation as follows: Multiplicand A: .atype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each register containing two .f16 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown below: Fragment layout for Row Major matrix A is shown in Figure 21 . Figure 21 MMA .m8n8k4 fragment layout for row-major matrix A with .f16 type \\uf0c1 The row and column of a matrix fragment can be computed as: row = % laneid % 4 if % laneid > 2 col = % laneid % 4 Multiplicand B: .btype Fragment Elements (low to high) .f64 A vector expression containing a single .f64 register, containing a single .f64 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 29 . Figure 29 MMA .m8n8k4 fragment layout for matrix B with .f64 type \\uf0c1 The row and column of a matrix fragment can be computed as: row = % laneid % 4 col = % laneid >> 2 Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing of two .f64 registers containing two .f64 elements from the matrix C. c0, c1 The layout of the fragments held by different threads is shown in Figure 30 . Figure 30 MMA .m8n8k4 fragment layout for accumulator matrix C/D with .f64 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 } 9.7.13.4.3. Matrix Fragments for mma.m8n8k16 \\uf0c1 A warp executing mma.m8n8k16 will compute an MMA operation of shape .m8n8k16 . Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds a fragment of the matrix. Multiplicand A: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing four .s8 or .u8 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 31 . Figure 31 MMA .m8n8k16 fragment layout for matrix A with .u8 / .s8 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 4 ) + i for ai where i = { 0 ,.., 3 } Multiplicand B: .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing a single .b32 register, containing four .s8 or .u8 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 32 . Figure 32 MMA .m8n8k16 fragment layout for matrix B with .u8 / .s8 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing of two .s32 registers. c0, c1 The layout of the fragments held by different threads is shown in Figure 33 . Figure 33 MMA .m8n8k16 fragment layout for accumulator matrix C/D with .s32 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.4. Matrix Fragments for mma.m8n8k32 \\uf0c1 A warp executing mma.m8n8k32 will compute an MMA operation of shape .m8n8k32 . Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing eight .s4 or .u4 elements from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 34 . Figure 34 MMA .m8n8k32 fragment layout for matrix A with .u4 / .s4 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 8 ) + i for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing eight .s4 or .u4 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 35 . Figure 35 MMA .m8n8k32 fragment layout for matrix B with .u4 / .s4 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + i for bi where i = { 0 ,.., 7 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression of two .s32 registers. c0, c1 The layout of the fragments held by different threads is shown in Figure 36 : Figure 36 MMA .m8n8k32 fragment layout for accumulator matrix C/D with .s32 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.5. Matrix Fragments for mma.m8n8k128 \\uf0c1 A warp executing mma.m8n8k128 will compute an MMA operation of shape .m8n8k128 . Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty two .b1 elements from the matrix A. a0, a1, … a30, a31 The layout of the fragments held by different threads is shown in Figure 37 . Figure 37 MMA .m8n8k128 fragment layout for matrix A with .b1 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 32 ) + i for ai where i = { 0 ,.., 31 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register, containing thirty two .b1 elements from the matrix B. b0, b1, …, b30, b31 The layout of the fragments held by different threads is shown in Figure 38 . Figure 38 MMA .m8n8k128 fragment layout for matrix B with .b1 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,.., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing two .s32 registers, containing two .s32 elements from the matrix C (or D). c0, c1 The layout of the fragments held by different threads is shown in Figure 39 . Figure 39 MMA .m8n8k128 fragment layout for accumulator matrix C/D with .s32 type \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9.7.13.4.6. Matrix Fragments for mma.m16n8k4 \\uf0c1 A warp executing mma.m16n8k4 will compute an MMA operation of shape .m16n8k4 . Multiplicand A: .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix A. a0, a1 The layout of the fragments held by different threads is shown in Figure 40 . Figure 40 MMA .m16n8k4 fragment layout for matrix A with .tf32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix A. a0, a1 The layout of the fragments held by different threads is shown in Figure 41 . Figure 41 MMA .m16n8k4 fragment layout for matrix A with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group Multiplicand B: .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression of a single .b32 register, containing a single .tf32 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 42 . Figure 42 MMA .m16n8k4 fragment layout for matrix B with .tf32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression of a single .f64 register, containing a single .f64 element from the matrix B. b0 The layout of the fragments held by different threads is shown in Figure 43 . Figure 43 MMA .m16n8k4 fragment layout for matrix B with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID Accumulators (C or D): .tf32 : .ctype / .dtype Fragment Elements (low to high) .f32 A vector expression containing four .f32 registers, containing four .f32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 44 . Figure 44 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 45 . Figure 45 MMA .m16n8k4 fragment layout for accumulator matrix C/D with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.7. Matrix Fragments for mma.m16n8k8 \\uf0c1 A warp executing mma.m16n8k8 will compute an MMA operation of shape .m16n8k8 . Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with each register containing two .f16 / .bf16 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 46 . Figure 46 MMA .m16n8k8 fragment layout for matrix A with .f16 / .bf16 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = threadID_in_group * 2 + ( i & 0x1 ) for ai where i = { 0 ,.., 3 } .tf32 : .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, containing four .tf32 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 47 . Figure 47 MMA .m16n8k8 fragment layout for matrix A with .tf32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, containing four .f64 elements from the matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 48 . Figure 48 MMA .m16n8k8 fragment layout for matrix A with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing a single .f16x2 register, containing two .f16 / .bf16 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 49 . Figure 49 MMA .m16n8k8 fragment layout for matrix B with .f16 / .bf16 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + i for bi where i = { 0 , 1 } col = groupID .tf32 : .btype Fragment Elements (low to high) .tf32 A vector expression containing two .b32 registers, containing two .tf32 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 50 . Figure 50 MMA .m16n8k8 fragment layout for matrix B with .tf32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID .f64 : .btype Fragment Elements (low to high) .f64 A vector expression containing two .f64 registers, containing two .f64 elements from the matrix B. b0, b1 The layout of the fragments held by different threads is shown in Figure 51 . Figure 51 MMA .m16n8k8 fragment layout for matrix B with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID Accumulators (C or D): .f16 , .bf16 and .tf32 : .ctype / .dtype Fragment Elements (low to high) .f16 A vector expression containing two .f16x2 registers, with each register containing two .f16 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression of four .f32 registers. The layout of the fragments held by different threads is shown in Figure 52 . Figure 52 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f16x2 / .f32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } .f64 : .ctype / .dtype Fragment Elements (low to high) .f64 A vector expression of four .f64 registers containing four .f64 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 53 . Figure 53 MMA .m16n8k8 fragment layout for accumulator matrix C/D with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.8. Matrix Fragments for mma.m16n8k16 with floating point type \\uf0c1 A warp executing mma.m16n8k16 floating point types will compute an MMA operation of shape .m16n8k16 . Multiplicand A: .f16 and .bf16 : .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with each register containing two .f16 / .bf16 elements from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 54 . Figure 54 MMA .m16n8k16 fragment layout for matrix A with .f16 / .bf16 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 4 .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing eight .f64 registers, with each register containing one .f64 element from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 55 . Figure 55 MMA .m16n8k16 fragment layout for matrix A with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i % 2 = 0 groupID + 8 Otherwise col = ( i * 2 ) + threadID_in_group for ai where i % 2 = 0 ( i * 2 ) - 2 + ( threadID_in_group Otherwise Multiplicand B: .f16 and .bf16 : .btype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing two .f16x2 registers, with each register containing two .f16 / .bf16 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 56 . Figure 56 MMA .m16n8k16 fragment layout for matrix B with .f16 / .bf16 type. \\uf0c1 where the row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + ( i & 0x1 ) for bi where i = 2 col = groupID .f64 : .atype Fragment Elements (low to high) .f64 A vector expression containing four .f64 registers, with each register containing one .f64 element from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 57 . Figure 57 MMA .m16n8k16 fragment layout for matrix B with .f64 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group + ( i * 4 ) for bi where i > 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.9. Matrix Fragments for mma.m16n8k16 with integer type \\uf0c1 A warp executing mma.m16n8k16 with .u8 or .s8 integer type will compute an MMA operation of shape .m16n8k16 . Multiplicand A: .atype Fragment Elements (low to high) .u8 / .s8 A vector expression containing two .b32 registers, with each register containing four .u8 / .s8 elements from the matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 59 . Figure 59 MMA .m16n8k16 fragment layout for matrix A with .u8 / .s8 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 4 col = ( threadID_in_group * 4 ) + ( i & 0x3 ) for ai where i = { 0 ,.., 7 } Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing a single .b32 register, containing four .u8 / .s8 elements from the matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 60 . Figure 60 MMA .m16n8k16 fragment layout for matrix B with .u8 / .s8 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,.., 3 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 61 . Figure 61 MMA .m16n8k16 fragment layout for accumulator matrix C/D with .s32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.10. Matrix Fragments for mma.m16n8k32 \\uf0c1 A warp executing mma.m16n8k32 will compute an MMA operation of shape .m16n8k32 . Multiplicand A: .s4 or .u4 : .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each register containing eight .u4 / .s4 elements from the matrix A. a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 62 . Figure 62 MMA .m16n8k32 fragment layout for matrix A with .u4 / .s4 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 8 col = ( threadID_in_group * 8 ) + ( i & 0x7 ) for ai where i = { 0 ,.., 15 } .s8 or .u8 or .e4m3 or .e5m2 : .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four .s8 / .u8 elements from the matrix A. a0, a1, .., a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four .e4m3 / .e5m2 elements from the matrix A. a0, a1, …, a14, a15 The layout of the fragments held by different threads is shown in Figure 63 . Figure 63 MMA .m16n8k32 fragment layout for matrix A with .u8 / .s8 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 8 Multiplicand B: .s4 or .u4 : .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing a single .b32 register, containing eight .s4 / .u4 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 64 . Figure 64 MMA .m16n8k32 fragment layout for matrix B with .u4 / .s4 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = { 0 ,.., 7 } col = groupID .s8 or .u8 or .e4m3 or .e5m2 : .btype Fragment Elements (low to high) .s8 / .u8 A vector expression containing two .b32 registers, with each register containing four .s8 / .u8 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 .e4m3 / .e5m2 A vector expression containing two .b32 registers, with each register containing four .e4m3 / .e5m2 elements from the matrix B. b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 65 and Figure 66 . Figure 65 MMA .m16n8k32 fragment layout for rows 0–15 of matrix B with .u8 / .s8 type. \\uf0c1 Figure 66 MMA .m16n8k32 fragment layout for rows 16–31 of matrix B with .u8 / .s8 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + ( i & 0x3 ) for bi where i = 4 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D). c0, c1, c2, c3 .f32 A vector expression containing four .f32 registers, containing four .f32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 67 . Figure 67 MMA .m16n8k32 fragment layout for accumulator matrix C/D with .s32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.11. Matrix Fragments for mma.m16n8k64 \\uf0c1 A warp executing mma.m16n8k64 will compute an MMA operation of shape .m16n8k64 . Multiplicand A: .atype Fragment Elements (low to high) .s4 / .u4 A vector expression containing four .b32 registers, with each register containing eight .s4 / .u4 elements from the matrix A. a0, a1, …, a30, a31 The layout of the fragments held by different threads is shown in Figure 68 . Figure 68 MMA .m16n8k64 fragment layout for matrix A with .u4 / .s4 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 16 Multiplicand B: .btype Fragment Elements (low to high) .s4 / .u4 A vector expression containing two .b32 registers, with each register containing eight .s4 / .u4 elements from the matrix B. b0, b1, …, b14, b15 The layout of the fragments held by different threads is shown in Figure 69 and Figure 70 . Figure 69 MMA .m16n8k64 fragment layout for rows 0–31 of matrix B with .u4 / .s4 type. \\uf0c1 Figure 70 MMA .m16n8k64 fragment layout for rows 32–63 of matrix B with .u4 / .s4 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = 8 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 71 . Figure 71 MMA .m16n8k64 fragment layout for accumulator matrix C/D with .s32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,.., 3 } 9.7.13.4.12. Matrix Fragments for mma.m16n8k128 \\uf0c1 A warp executing mma.m16n8k128 will compute an MMA operation of shape .m16n8k128 . Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register containing thirty two .b1 elements from the matrix A. a0, a1, …, a62, a63 The layout of the fragments held by different threads is shown in Figure 72 . Figure 72 MMA .m16n8k128 fragment layout for matrix A with .b1 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 32 col = ( threadID_in_group * 32 ) + ( i & 0x1F ) for ai where i = { 0 , ..., 63 } Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing a single .b32 register containing thirty two .b1 elements from the matrix B. b0, b1, … , b30, b31 The layout of the fragments held by different threads is shown in Figure 73 . Figure 73 MMA .m16n8k128 fragment layout for matrix B with .b1 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,..., 31 } col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 74 . Figure 74 MMA .m16n8k128 fragment layout for accumulator matrix C/D with .s32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.13. Matrix Fragments for mma.m16n8k256 \\uf0c1 A warp executing mma.m16n8k256 will compute an MMA operation of shape .m16n8k256 . Multiplicand A: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each register containing thirty two .b1 elements from the matrix A. a0, a1, …, a126, a127 The layout of the fragments held by different threads is shown in Figure 75 . Figure 75 MMA .m16n8k256 fragment layout for matrix A with .b1 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 64 Multiplicand B: .btype Fragment Elements (low to high) .b1 A vector expression containing two .b32 registers, with each register containing thirty two .b1 elements from the matrix B. b0, b1, …, b62, b63 The layout of the fragments held by different threads is shown in Figure 76 and Figure 77 . Figure 76 MMA .m16n8k256 fragment layout for rows 0–127 of matrix B with .b1 type. \\uf0c1 Figure 77 MMA .m16n8k256 fragment layout for rows 128–255 of matrix B with .b1 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + ( i & 0x1F ) for bi where i = 32 col = groupID Accumulators (C or D): .ctype / .dtype Fragment Elements (low to high) .s32 A vector expression containing four .s32 registers, containing four .s32 elements from the matrix C (or D). c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 78 . Figure 78 MMA .m16n8k256 fragment layout for accumulator matrix C/D with .s32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9.7.13.4.14. Multiply-and-Accumulate Instruction: mma \\uf0c1 mma Perform matrix multiply-and-accumulate operation Syntax Half precision floating point type: mma.sync.aligned.m8n8k4.alayout.blayout.dtype.f16.f16.ctype d, a, b, c; mma.sync.aligned.m16n8k8.row.col.dtype.f16.f16.ctype d, a, b, c; mma.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c; .alayout = {.row, .col}; .blayout = {.row, .col}; .ctype = {.f16, .f32}; .dtype = {.f16, .f32}; Alternate floating point type : mma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32 d, a, b, c; mma.sync.aligned.m16n8k8.row.col.f32.atype.btype.f32 d, a, b, c; mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 d, a, b, c; mma.sync.aligned.m16n8k32.row.col.f32.f8type.f8type.f32 d, a, b, c; .atype = {.bf16, .tf32}; .btype = {.bf16, .tf32}; .f8type = {.e4m3, .e5m2}; Double precision floating point type: mma.sync.aligned.shape.row.col.f64.f64.f64.f64 d, a, b, c; .shape = {.m8n84, .m16n8k4, .m16n8k8, .m16n8k16}; Integer type: mma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c; .shape = {.m8n8k16, .m16n8k16, .m16n8k32} .atype = {.u8, .s8}; .btype = {.u8, .s8}; mma.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c; .shape = {.m8n8k32, .m16n8k32, .m16n8k64} .atype = {.u4, .s4}; .btype = {.u4, .s4}; Single bit: mma.sync.aligned.shape.row.col.s32.b1.b1.s32.bitOp.popc d, a, b, c; .bitOp = {.xor, .and} .shape = {.m8n8k128, .m16n8k128, .m16n8k256} Description Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C , where the A matrix is MxK , the B matrix is KxN , and the C and D matrices are MxN . A warp executing mma.sync.m8n8k4 instruction computes 4 matrix multiply and accumulate operations. Rest of the mma.sync operations compute a single matrix mutliply and accumulate operation per warp. For single-bit mma.sync , multiplication is replaced by a sequence of logical operations; specifically, mma.xor.popc and mma.and.popc computes the XOR, AND respectively of a k-bit row of A with a k-bit column of B, then counts the number of set bits in the result ( popc ). This result is added to the corresponding element of C and written into D. Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp. The registers in each thread hold a fragment of matrix as described in Matrix multiply-accumulate operation using mma instruction . The qualifiers .dtype , .atype , .btype and .ctype indicate the data-type of the elements in the matrices D, A, B and C respectively. Specific shapes have type restrictions : .m8n8k4 : When .ctype is .f32 , .dtype must also be .f32 . .m16n8k8 : .dtype must be the same as .ctype . .atype must be the same as .btype . The qualifiers .alayout and .blayout indicate the row-major or column-major layouts of matrices A and B respectively. Precision and rounding : .f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. The accumulation order, rounding and handling of subnormal inputs are unspecified. .e4m3 and .e5m2 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision. The accumulation order, rounding, and handling of subnormal inputs are unspecified. .bf16 and .tf32 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision. .f64 floating point operations : Precision of the element-wise multiplication and addition operation is identical to that of .f64 precision fused multiply-add. Supported rounding modifiers are : .rn : mantissa LSB rounds to nearest even. This is the default. .rz : mantissa LSB rounds towards zero. .rm : mantissa LSB rounds towards negative infinity. .rp : mantissa LSB rounds towards positive infinity. Integer operations : The integer mma operation is performed with .s32 accumulators. The .satfinite qualifier indicates that on overflow, the accumulated value is limited to the range MIN_INT32 .. MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit integer and the maximum positive signed 32-bit integer respectively). If .satfinite is not specified, the accumulated value is wrapped instead. The mandatory .sync qualifier indicates that mma instruction causes the executing thread to wait until all threads in the warp execute the same mma instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma instruction. In conditionally executed code, a mma instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined. The behavior of mma instruction is undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited. Notes Programs using double precision floating point mma instruction with shapes .m16n8k4 , .m16n8k8 , and .m16n8k16 require at least 64 registers for compilation. PTX ISA Notes Introduced in PTX ISA version 6.4. .f16 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version 6.4. .f16 floating point type mma operation with .m16n8k8 shape introduced in PTX ISA version 6.5. .u8/.s8 integer type mma operation with .m8n8k16 shape introduced in PTX ISA version 6.5. .u4/.s4 integer type mma operation with .m8n8k32 shape introduced in PTX ISA version 6.5. .f64 floating point type mma operation with .m8n8k4 shape introduced in PTX ISA version 7.0. .f16 floating point type mma operation with .m16n8k16 shape introduced in PTX ISA version 7.0. .bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes introduced in PTX ISA version 7.0. .tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes introduced in PTX ISA version 7.0. .u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes introduced in PTX ISA version 7.0. .u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes introduced in PTX ISA version 7.0. .b1 single-bit integer type mma operation with .m8n8k128 , .m16n8k128 and .m16n8k256 shapes introduced in PTX ISA version 7.0. Support for .and operation in single-bit mma introduced in PTX ISA version 7.1. .f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes introduced in PTX ISA version 7.8. Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in PTX ISA version 8.4. .f16 floating point type mma operation with .m8n8k4 shape requires sm_70 or higher. Note mma.sync.m8n8k4 is optimized for target architecture sm_70 and may have substantially reduced performance on other target architectures. .f16 floating point type mma operation with .m16n8k8 shape requires sm_75 or higher. .u8/.s8 integer type mma operation with .m8n8k16 shape requires sm_75 or higher. .u4/.s4 integer type mma operation with .m8n8k32 shape sm_75 or higher. .b1 single-bit integer type mma operation with .m8n8k128 shape sm_75 or higher. .f64 floating point type mma operation with .m8n8k4 shape requires sm_80 or higher. .f16 floating point type mma operation with .m16n8k16 shape requires sm_80 or higher. .bf16 alternate floating point type mma operation with .m16n8k8 and .m16n8k16 shapes requires sm_80 or higher. .tf32 alternate floating point type mma operation with .m16n8k4 and .m16n8k8 shapes requires sm_80 or higher. .u8/.s8 integer type mma operation with .m16n8k16 and .m16n8k32 shapes requires sm_80 or higher. .u4/.s4 integer type mma operation with .m16n8k32 and .m16n8k64 shapes requires sm_80 or higher. .b1 single-bit integer type mma operation with .m16n8k128 and .m16n8k256 shapes requires sm_80 or higher. .and operation in single-bit mma requires sm_80 or higher. .f64 floating point type mma operation with .m16n8k4 , .m16n8k8 , and .m16n8k16 shapes require sm_90 or higher. .e4m3 and .e5m2 alternate floating point type mma operation requires sm_89 or higher. Examples of half precision floating point type // f16 elements in C and D matrix .reg .f16x2 %Ra %Rb %Rc %Rd mma.sync.aligned.m8n8k4.row.col.f16.f16.f16.f16 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; // f16 elements in C and f32 elements in D .reg .f16x2 %Ra %Rb %Rc .reg .f32 %Rd mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f16 {%Rd0, %Rd1, %Rd2, %Rd3, %Rd4, %Rd5, %Rd6, %Rd7}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; // f32 elements in C and D .reg .f16x2 %Ra, %Rb; .reg .f32 %Rc, %Rd; mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .f16x2 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%Rd0, %Rd1}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1}; .reg .f16 %Ra, %Rb; .reg .f32 %Rc, %Rd; mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of alternate floating point type .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; mma.sync.aligned.m16n8k4.row.col.f32.tf32.tf32.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .f16x2 %Ra, %Rb; .reg .f32 %Rc, %Rd; mma.sync.aligned.m16n8k8.row.col.f32.bf16.bf16.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Rb2, %Rb3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .f16x2 %Ra, %Rb; .reg .f32 %Rc, %Rd; mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; mma.sync.aligned.m16n8k32.row.col.f32.e4m3.e5m2.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of integer type .reg .b32 %Ra, %Rb, %Rc, %Rd; // s8 elements in A and u8 elements in B mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.u8.s32 {%Rd0, %Rd1}, {%Ra}, {%Rb}, {%Rc0, %Rc1}; // u4 elements in A and B matrix mma.sync.aligned.m8n8k32.row.col.satfinite.s32.u4.u4.s32 {%Rd0, %Rd1}, {%Ra}, {%Rb}, {%Rc0, %Rc1}; // s8 elements in A and u8 elements in B .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k16.row.col.satfinite.s32.s8.u8.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb}, {%Rc0, %Rc1, %Rc2, %Rc3}; // u4 elements in A and s4 elements in B .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k32.row.col.satfinite.s32.u4.s4.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb}, {%Rc0, %Rc1, %Rc2, %Rc3}; // s8 elements in A and s8 elements in B .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; // u8 elements in A and u8 elements in B .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k64.row.col.satfinite.s32.u4.u4.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1 }, {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of single bit type // b1 elements in A and B .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.and.popc {%Rd0, %Rd1}, {%Ra}, {%Rb}, {%Rc0, %Rc1}; // b1 elements in A and B .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m8n8k128.row.col.s32.b1.b1.s32.xor.popc {%Rd0, %Rd1}, {%Ra}, {%Rb}, {%Rc0, %Rc1}; .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.xor.popc {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k128.row.col.s32.b1.b1.s32.and.popc {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; .reg .b32 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.and.popc {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; Examples of .f64 floating point type .reg .f64 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m8n8k4.row.col.f64.f64.f64.f64 {%Rd0, %Rd1}, {%Ra}, {%Rb}, {%Rc0, %Rc1}; .reg .f64 %Ra, %Rb, %Rc, %Rd; mma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64.rn {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0}, {%Rc0, %Rc1, %Rc2, %Rc3}; mma.sync.aligned.m16n8k8.row.col.f64.f64.f64.f64.rn {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}; mma.sync.aligned.m16n8k16.row.col.f64.f64.f64.f64.rn {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3, %Ra4, %Ra5, %Ra6, %Ra7}, {%Rb0, %Rb1, %Rb2, %Rb3}, {%Rc0, %Rc1, %Rc2, %Rc3}; 9.7.13.4.15. Warp-level matrix load instruction: ldmatrix \\uf0c1 ldmatrix Collectively load one or more matrices from shared memory for mma instruction Syntax ldmatrix.sync.aligned.shape.num{.trans}{.ss}.type r, [p]; .shape = {.m8n8}; .num = {.x1, .x2, .x4}; .ss = {.shared{::cta}}; .type = {.b16}; Description Collectively load one or more matrices across all threads in a warp from the location indicated by the address operand p , from .shared state space into destination register r .'},\n",
       " {'id': 673,\n",
       "  'content': 'If no state space is provided, generic addressing is used, such that the address in p points into .shared space. If the generic address doesn’t fall in .shared state space, then the behavior is undefined. The .shape qualifier indicates the dimensions of the matrices being loaded. Each matrix element holds 16-bit data as indicated by the .type qualifier. The values .x1 , .x2 and .x4 for .num indicate one, two or four matrices respectively. The mandatory .sync qualifier indicates that ldmatrix causes the executing thread to wait until all threads in the warp execute the same ldmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same ldmatrix instruction. In conditionally executed code, an ldmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined. The behavior of ldmatrix is undefined if all threads do not use the same qualifiers, or if any thread in the warp has exited. The destination operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit registers as per the value of .num .'},\n",
       " {'id': 674,\n",
       "  'content': 'Each component of the vector expression holds a fragment from the corresponding matrix. Supported addressing modes for p are described in Addresses as Operands . Consecutive instances of row need not be stored contiguously in memory. The eight addresses required for each matrix are provided by eight threads, depending upon the value of .num as shown in the following table. Each address corresponds to the start of a matrix row. Addresses addr0–addr7 correspond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the second matrix, and so on. .num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 Note For .target sm_75 or below, all threads must contain valid addresses. For .num = .x1 and .num = .x2 , addresses contained in lower threads can be copied to higher threads to achieve the expected behavior. When reading 8x8 matrices, a group of four consecutive threads loads 16 bytes. The matrix addresses must be naturally aligned accordingly. Each thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its register r , and so on. A group of four threads loads an entire row of the matrix as shown in Figure 79 . Figure 79 ldmatrix fragment layout \\uf0c1 When .num = .x2 , the elements of the second matrix are loaded in the next destination register in each thread as per the layout in above table. Similarly, when .num = .x4 , elements of the third and fourth matrices are loaded in the subsequent destination registers in each thread. Optional qualifier .trans indicates that the matrix is loaded in column-major format. The ldmatrix instruction is treated as a weak memory operation in the Memory Consistency Model . PTX ISA Notes Introduced in PTX ISA version 6.5. Examples // Load a single 8x8 matrix using 64-bit addressing .reg .b64 addr; .reg .b32 d; ldmatrix.sync.aligned.m8n8.x1.shared::cta.b16 {d}, [addr]; // Load two 8x8 matrices in column-major format .reg .b64 addr; .reg .b32 d; ldmatrix.sync.aligned.m8n8.x2.trans.shared.b16 {d0, d1}, [addr]; // Load four 8x8 matrices .reg .b64 addr; .reg .b32 d; ldmatrix.sync.aligned.m8n8.x4.b16 {d0, d1, d2, d3}, [addr]; 9.7.13.4.16. Warp-level matrix store instruction: stmatrix \\uf0c1 stmatrix Collectively store one or more matrices to shared memory. Syntax stmatrix.sync.aligned.shape.num{.trans}{.ss}.type [p], r; .shape = {.m8n8}; .num = {.x1, .x2, .x4}; .ss = {.shared{::cta}}; .type = {.b16}; Description Collectively store one or more matrices across all threads in a warp to the location indicated by the address operand p , in .shared state space. The mandatory .sync qualifier indicates that stmatrix causes the executing thread to wait until all threads in the warp execute the same stmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same stmatrix instruction. In conditionally executed code, an stmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined. The behavior of stmatrix is undefined if all threads do not use the same qualifiers, or if any thread in the warp has exited. The source operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit registers as per the value of .num . .num Threads 0–7 Threads 8–15 Threads 16–23 Threads 24–31 .x1 addr0–addr7 – – – .x2 addr0–addr7 addr8–addr15 – – .x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 When storing 8x8 matrices, a group of four consecutive threads stores 16 bytes. Each thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its register r , and so on. A group of four threads stores an entire row of the matrix as shown in Figure 80 . Figure 80 stmatrix fragment layout \\uf0c1 When .num = .x2 , the elements of the second matrix are storedd from the next source register in each thread as per the layout in above table. Similarly, when .num = .x4 , elements of the third and fourth matrices are stored from the subsequent source registers in each thread. Optional qualifier .trans indicates that the matrix is stored in column-major format. The stmatrix instruction is treated as a weak memory operation in the Memory Consistency Model . Examples // Store a single 8x8 matrix using 64-bit addressing .reg .b64 addr; .reg .b32 r; stmatrix.sync.aligned.m8n8.x1.shared.b16 [addr], {r}; // Store two 8x8 matrices in column-major format .reg .b64 addr; .reg .b32 r; stmatrix.sync.aligned.m8n8.x2.trans.shared::cta.b16 [addr], {r0, r1}; // Store four 8x8 matrices .reg .b64 addr; .reg .b32 r; stmatrix.sync.aligned.m8n8.x4.b16 [addr], {r0, r1, r2, r3}; 9.7.13.4.17. Warp-level matrix transpose instruction: movmatrix \\uf0c1 movmatrix Transpose a matrix in registers across the warp. Syntax movmatrix.sync.aligned.shape.trans.type d, a; .shape = {.m8n8}; .type = {.b16}; Description Move a row-major matrix across all threads in a warp, reading elements from source a , and writing the transposed elements to destination d . The .shape qualifier indicates the dimensions of the matrix being transposed. The mandatory .sync qualifier indicates that movmatrix causes the executing thread to wait until all threads in the warp execute the same movmatrix instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same movmatrix instruction. In conditionally executed code, a movmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined. Operands a and d are 32-bit registers containing fragments of the input matrix and the resulting matrix respectively. The mandatory qualifier .trans indicates that the resulting matrix in d is a transpose of the input matrix specified by a . Each thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first fragment in register a , and so on. A group of four threads holds an entire row of the input matrix as shown in Figure 81 . Figure 81 movmatrix source matrix fragment layout \\uf0c1 Each thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the first fragment in register d , and so on. A group of four threads holds an entire column of the result matrix as shown in Figure 82 . Figure 82 movmatrix result matrix fragment layout \\uf0c1 PTX ISA Notes Introduced in PTX ISA version 7.8. Examples .reg .b32 d, a; movmatrix.sync.aligned.m8n8.trans.b16 d, a; 9.7.13.5. Matrix multiply-accumulate operation using mma.sp instruction with sparse matrix A \\uf0c1 This section describes warp-level mma.sp{::ordered_metadata} instruction with sparse matrix A. This variant of the mma operation can be used when A is a structured sparse matrix with 50% zeros in each row distributed in a shape-specific granularity. For an MxNxK sparse mma.sp{::ordered_metadata} operation, the MxK matrix A is packed into MxK/2 elements. For each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements are packed in the operand representing matrix A. The mapping of these K/2 elements to the corresponding K-wide row is provided explicitly as metadata. 9.7.13.5.1. Sparse matrix storage \\uf0c1 Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a sub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the sub-chunk is shape-specific. For example, in a 16x16 matrix A, sparsity is expected to be at 2:4 granularity, i.e. each 4-element vector (i.e.'},\n",
       " {'id': 675,\n",
       "  'content': 'a sub-chunk of 4 consecutive elements) of a matrix row contains 2 zeros. Index of each non-zero element in a sub-chunk is stored in the metadata operand. Values 0b0000 , 0b0101 , 0b1010 , 0b1111 are invalid values for metadata and will result in undefined behavior. In a group of four consecutive threads, one or more threads store the metadata for the whole group depending upon the matrix shape. These threads are specified using an additional sparsity selector operand. Figure 83 shows an example of a 16x16 matrix A represented in sparse format and sparsity selector indicating which thread in a group of four consecutive threads stores the metadata. Figure 83 Sparse MMA storage example \\uf0c1 Granularities for different matrix shapes and data types are described below. Sparse mma.sp{::ordered_metadata} with half-precision and .bf16 type For the .m16n8k16 and .m16n8k32 mma.sp{::ordered_metadata} operations, matrix A is structured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A has two zeros and two non-zero elements. Only the two non-zero elements are stored in the operand representing matrix A and their positions in the four-wide chunk in matrix A are indicated by two 2-bit indices in the metadata operand. For mma.sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior. Figure 84 Sparse MMA metadata example for .f16 / .bf16 type. \\uf0c1 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k16 : One thread within a group of four consecutive threads contributes the metadata for the entire group. This thread is indicated by a value in {0, 1, 2, 3}. m16n8k32 : A thread-pair within a group of four consecutive threads contributes the sparsity metadata. Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other value results in an undefined behavior. Sparse mma.sp{::ordered_metadata} with .tf32 type When matrix A has .tf32 elements, matrix A is structured sparse at a granularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero element. Only the non-zero elements are stored in the operand for matrix A and their positions in a two-wide chunk in matrix A are indicated by the 4-bit index in the metadata. 0b1110 and 0b0100 are the only meaningful index values; any other values result in an undefined behavior. Figure 85 Sparse MMA metadata example for .tf32 type. \\uf0c1 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k8 : One thread within a group of four consecutive threads contributes the metadata for the entire group. m16n8k16 : A thread-pair within a group of four consecutive threads contributes the sparsity metadata. Sparse mma.sp{::ordered_metadata} with integer type When matrices A and B have .u8 / .s8 elements, matrix A is structured sparse at a granularity of 2:4. In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes and two non-zero elements. Only the two non-zero elements are stored in sparse matrix and their positions in the four-wide chunk are indicated by two 2-bit indices in the metadata. Figure 86 Sparse MMA metadata example for .u8 / .s8 type. \\uf0c1 when matrices A and B have .u4 / .s4 elements, matrix A is pair-wise structured sparse at a granularity of 4:8. In other words, each chunk of eight adjacent elements in a row of matrix A has four zeroes and four non-zero values. Further, the zero and non-zero values are clustered in sub-chunks of two elements each within the eight-wide chunk. i.e., each two-wide sub-chunk within the eight-wide chunk must be all zeroes or all non-zeros. Only the four non-zero values are stored in sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the eight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata. Figure 87 Sparse MMA metadata example for .u4 / .s4 type. \\uf0c1 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k32 with .u8 / .s8 type and m16n8k64 with .u4 / .s4 type: A thread-pair within a group of four consecutive threads contributes the sparsity metadata. m16n8k64 with .u8 / .s8 type and m16n8k128 with .u4 / .s4 type: All threads within a group of four consecutive threads contribute the sparsity metadata. Hence, the sparsity selector in this case must be 0. Any other value of sparsity selector results in an undefined behavior. Sparse mma.sp{::ordered_metadata} with .e4m3 / .e5m2 type When matrices A and B have .e4m3 / .e5m2 elements, matrix A is structured sparse at a granularity of 2:4. 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior. Figure 88 Sparse MMA metadata example for .e4m3 / .e5m2 type. \\uf0c1 The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k64 : All threads within a group of four consecutive threads contribute the sparsity metadata. 9.7.13.5.2. Matrix fragments for multiply-accumulate operation with sparse matrix A \\uf0c1 In this section we describe how the contents of thread registers are associated with fragments of various matrices and the sparsity metadata. The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and their association with the matrix data. For matrix B, when the combination of matrix dimension and the supported data type is not already covered in Matrix multiply-accumulate operation using mma instruction , a pictorial representation of matrix fragments is provided. For matrices C and D, since the matrix dimension - data type combination is the same for all supported shapes, and is already covered in Matrix multiply-accumulate operation using mma instruction , the pictorial representations of matrix fragments are not included in this section. For the metadata operand, pictorial representations of the association between indices of the elements of matrix A and the contents of the metadata operand are included. Tk: [m..n] present in cell [x][y..z] indicates that bits m through n (with m being higher) in the metadata operand of thread with %laneid=k contains the indices of the non-zero elements from the chunk [x][y]..[x][z] of matrix A. 9.7.13.5.2.1. Matrix Fragments for sparse mma.m16n8k16 with .f16 and .bf16 types \\uf0c1 A warp executing sparse mma.m16n8k16 with .f16 / .bf16 floating point type will compute an MMA operation of shape .m16n8k16 . Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing two .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A. Mapping of the non-zero elements is as described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 89 . Figure 89 Sparse MMA .m16n8k16 fragment layout for matrix A with .f16 / .bf16 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 4 lastcol = firstcol + 3 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with floating point type for .f16 / .b16 formats. Metadata: A .b32 register containing 16 2-bit vectors each storing the index of a non-zero element of a 4-wide chunk of matrix A as shown in Figure 90 . Figure 90 Sparse MMA .m16n8k16 metadata layout for .f16 / .bf16 type. \\uf0c1 9.7.13.5.2.2. Matrix Fragments for sparse mma.m16n8k32 with .f16 and .bf16 types \\uf0c1 A warp executing sparse mma.m16n8k32 with .f16 / .bf16 floating point type will compute an MMA operation of shape .m16n8k32 . Multiplicand A: .atype Fragment Elements .f16 / .bf16 A vector expression containing four .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A. Mapping of the non-zero elements is as described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 91 . Figure 91 Sparse MMA .m16n8k32 fragment layout for matrix A with .f16 / .bf16 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 4 lastcol = firstcol + 3 Multiplicand B: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .b32 registers, each containing two .f16 / .bf16 elements from matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 92 . Figure 92 Sparse MMA .m16n8k32 fragment layout for matrix B with .f16 / .bf16 type. \\uf0c1 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with floating point type for .f16 / .b16 formats. Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of two non-zero element from a 4-wide chunk of matrix A as shown in Figure 93 . Figure 93 Sparse MMA .m16n8k32 metadata layout for .f16 / .bf16 type. \\uf0c1 9.7.13.5.2.3. Matrix Fragments for sparse mma.m16n8k16 with .tf32 floating point type \\uf0c1 A warp executing sparse mma.m16n8k16 with .tf32 floating point type will compute an MMA operation of shape .m16n8k16 . Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing four .b32 registers, with each register containing one non-zero .tf32 element out of 2 consecutive elements from matrix A. Mapping of the non-zero elements is as described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 94 . Figure 94 Sparse MMA .m16n8k16 fragment layout for matrix A with .tf32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 for a0 and a1 ( threadID_in_group * 2 ) + 8 for a2 and a3 lastcol = firstcol + 1 Multiplicand B: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers, each containing four .tf32 elements from matrix B. b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 95 . Figure 95 Sparse MMA .m16n8k16 fragment layout for matrix B with .tf32 type. \\uf0c1 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with floating point type . Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A as shown in Figure 96 . Figure 96 Sparse MMA .m16n8k16 metadata layout for .tf32 type. \\uf0c1 9.7.13.5.2.4. Matrix Fragments for sparse mma.m16n8k8 with .tf32 floating point type \\uf0c1 A warp executing sparse mma.m16n8k8 with .tf32 floating point type will compute an MMA operation of shape .m16n8k8 . Multiplicand A: .atype Fragment Elements .tf32 A vector expression containing two .b32 registers, each containing one non-zero .tf32 element out of 2 consecutive elements from matrix A. Mapping of the non-zero elements is as described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 97 . Figure 97 Sparse MMA .m16n8k8 fragment layout for matrix A with .tf32 type. \\uf0c1 The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = [ firstcol ... lastcol ] // As per the mapping of non-zero elements // as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 lastcol = firstcol + 1 Matrix fragments for multiplicand B and accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k8 for .tf32 format. Metadata: A .b32 register containing 8 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A as shown in Figure 98 . Figure 98 Sparse MMA .m16n8k8 metadata layout for .tf32 type. \\uf0c1 9.7.13.5.2.5. Matrix Fragments for sparse mma.m16n8k32 with .u8/.s8 integer type \\uf0c1 A warp executing sparse mma.m16n8k32 with .u8 / .s8 integer type will compute an MMA operation of shape .m16n8k32 . Multiplicand A: .atype Fragment Elements .u8 / .s8 A vector expression containing two .b32 registers, with each register containing four non-zero .u8 / .s8 elements out of 8 consecutive elements from matrix A. Mapping of the non-zero elements is as described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 99 . Figure 99 Sparse MMA .m16n8k32 fragment layout for matrix A with .u8 / .s8 type. \\uf0c1 groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 > 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 8 lastcol = firstcol + 7 Multiplicand B: .btype Fragment Elements (low to high) .u8 / .s8 A vector expression containing four .b32 registers, each containing four .u8 / .s8 elements from matrix B. b0, b1, b2, b3, …, b15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, each containing four .e4m3 / .e5m2 elements from matrix B. The layout of the fragments held by different threads is shown in Figure 103 , Figure 104 , Figure 105 and Figure 106 . Figure 103 Sparse MMA .m16n8k64 fragment layout for rows 0–15 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type. \\uf0c1 Figure 104 Sparse MMA .m16n8k64 fragment layout for rows 16–31 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type. \\uf0c1 Figure 105 Sparse MMA .m16n8k64 fragment layout for rows 32–47 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type. \\uf0c1 Figure 106 Sparse MMA .m16n8k64 fragment layout for rows 48–63 of matrix B with .u8 / .s8 / .e4m3 / .e5m2 type. \\uf0c1 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k16 with integer type . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of two non-zero elements from a 4-wide chunk of matrix A as shown in Figure 107 and Figure 108 . Figure 107 Sparse MMA .m16n8k64 metadata layout for columns 0–31 for .u8 / .s8 / .e4m3 / .e5m2 type. \\uf0c1 Figure 108 Sparse MMA .m16n8k64 metadata layout for columns 32–63 for .u8 / .s8 / .e4m3 / .e5m2 type. \\uf0c1 9.7.13.5.2.7. Matrix Fragments for sparse mma.m16n8k64 with .u4/.s4 integer type \\uf0c1 A warp executing sparse mma.m16n8k64 with .u4 / .s4 integer type will compute an MMA operation of shape .m16n8k64 . Multiplicand A: .atype Fragment Elements .u4 / .s4 A vector expression containing two .b32 registers, with each register containing eight non-zero .u4 / .s4 elements out of 16 consecutive elements from matrix A. Mapping of the non-zero elements is as described in Sparse matrix storage . The layout of the fragments held by different threads is shown in Figure 109 . Figure 109 Sparse MMA .m16n8k64 fragment layout for matrix A with .u4 / .s4 type. \\uf0c1 groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 > 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 16 lastcol = firstcol + 15 Multiplicand B: .atype Fragment Elements (low to high) .u4 / .s4 A vector expression containing four .b32 registers, each containing eight .u4 / .s4 elements from matrix B. b0, b1, b2, b3, …, b31 The layout of the fragments held by different threads is shown in Figure 113 , Figure 114 , Figure 115 , Figure 116 . Figure 113 Sparse MMA .m16n8k128 fragment layout for rows 0–31 of matrix B with .u4 / .s4 type. \\uf0c1 Figure 114 Sparse MMA .m16n8k128 fragment layout for rows 31–63 of matrix B with .u4 / .s4 type. \\uf0c1 Figure 115 Sparse MMA .m16n8k128 fragment layout for rows 64–95 of matrix B with .u4 / .s4 type. \\uf0c1 Figure 116 Sparse MMA .m16n8k128 fragment layout for rows 96–127 of matrix B with .u4 / .s4 type. \\uf0c1 Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma.m16n8k64 . Metadata: A .b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of four non-zero elements from a 8-wide chunk of matrix A as shown in Figure 117 and Figure 118 . Figure 117 Sparse MMA .m16n8k128 metadata layout for columns 0–63 for .u4 / .s4 type. \\uf0c1 Figure 118 Sparse MMA .m16n8k128 metadata layout for columns 64–127 for .u4 / .s4 type. \\uf0c1 9.7.13.5.3. Multiply-and-Accumulate Instruction: mma.sp/mma.sp::ordered_metadata \\uf0c1 mma.sp/mma.sp::ordered_metadata Perform matrix multiply-and-accumulate operation with sparse matrix A Syntax Half precision floating point type: mma.spvariant.sync.aligned.m16n8k16.row.col.dtype.f16.f16.ctype d, a, b, c, e, f; mma.spvariant.sync.aligned.m16n8k32.row.col.dtype.f16.f16.ctype d, a, b, c, e, f; .ctype = {.f16, .f32}; .dtype = {.f16, .f32}; .spvariant = {.sp, .sp::ordered_metadata}; Alternate floating point type : mma.spvariant.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 d, a, b, c, e, f; mma.spvariant.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32 d, a, b, c, e, f; mma.spvariant.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 d, a, b, c, e, f; mma.spvariant.sync.aligned.m16n8k16.row.col.f32.tf32.tf32.f32 d, a, b, c, e, f; mma.spvariant.sync.aligned.m16n8k64.row.col.f32.f8type.f8type.f32 d, a, b, c, e, f; .f8type = {.e4m3, .e5m2}; .spvariant = {.sp, .sp::ordered_metadata}; Integer type: mma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f; .shape = {.m16n8k32, .m16n8k64} .atype = {.u8, .s8}; .btype = {.u8, .s8}; .spvariant = {.sp, .sp::ordered_metadata}; mma.spvariant.sync.aligned.shape.row.col{.satfinite}.s32.atype.btype.s32 d, a, b, c, e, f; .shape = {.m16n8k64, .m16n8k128} .atype = {.u4, .s4}; .btype = {.u4, .s4}; .spvariant = {.sp, .sp::ordered_metadata}; Description Perform a MxNxK matrix multiply and accumulate operation, D = A*B+C , where the A matrix is MxK , the B matrix is KxN , and the C and D matrices are MxN . A warp executing mma.sp.sync/mma.sp::ordered_metadata.sync instruction compute a single matrix mutliply and accumulate operation. Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp. Matrix A is structured sparse as described in Sparse matrix storage Operands e and f represent sparsity metadata and sparsity selector respectively. Operand e is a 32-bit integer and operand f is a 32-bit integer constant with values in the range 0..3 Instruction mma.sp::ordered_metadata requires the indices in the sparsity metadata to be sorted in an increasing order starting from LSB, otherwise behavior is undefined. The registers in each thread hold a fragment of matrix as described in Matrix fragments for multiply-accumulate operation with sparse matrix A . In case of shapes .m16n8k16 and .m16n8k32 , .dtype must be the same as .ctype Precision and rounding : .f16 floating point operations : Element-wise multiplication of matrix A and B is performed with at least single precision. Integer operations : The integer mma.sp/mma.sp::ordered_metadata operation is performed with .s32 accumulators. The mandatory .sync qualifier indicates that mma.sp/mma.sp::ordered_metadata instruction causes the executing thread to wait until all threads in the warp execute the same mma.sp/mma.sp::ordered_metadata instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warp must execute the same mma.sp/mma.sp::ordered_metadata instruction. In conditionally executed code, a mma.sp/mma.sp::ordered_metadata instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined. The behavior of mma.sp/mma.sp::ordered_metadata instruction is undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited. Notes mma.sp instruction may have substantially reduced performance on some target architectures. Hence, it is advised to use mma.sp::ordered_metadata instruction. PTX ISA Notes Introduced in PTX ISA version 7.1. Support for .e4m3 and .e5m2 alternate floating point type mma operation introduced in PTX ISA version 8.4. mma.sp::ordered_metadata introduced in PTX ISA version 8.5. mma.sp::ordered_metadata requires sm_80 or higher. Examples of half precision floating point type // f16 elements in C and D matrix .reg .f16x2 %Ra %Rb %Rc %Rd .reg .b32 %Re; mma.sp.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%Rd0, %Rd1}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1}, %Re, 0x1; .reg .f16x2 %Ra %Rb %Rc %Rd .reg .b32 %Re; mma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%Rd0, %Rd1}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1}, %Re, 0x1; Examples of alternate floating point type .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; .reg .b32 %Re; mma.sp.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; .reg .b32 %Re; mma.sp.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; .reg .b32 %Re; mma.sp.sync.aligned.m16n8k32.row.col.f32.bf16.bf16.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1, %Rb2, %Rb3}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; .reg .b32 %Re; mma.sp.sync.aligned.m16n8k64.row.col.f32.e5m2.e4m3.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1, %Rb2, %Rb3}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0; .reg .b32 %Ra, %Rb; .reg .f32 %Rc, %Rd; .reg .b32 %Re; mma.sp::ordered_metadata.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; Examples of integer type .reg .b32 %Ra, %Rb, %Rc, %Rd; .reg .u32 %Re; // u8 elements in A and B matrix mma.sp.sync.aligned.m16n8k32.row.col.satfinite.s32.u8.u8.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; // s8 elements in A and B matrix mma.sp.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1, %Rb2, %Rb3}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0; // s8 elements in A and B matrix with ordered metadata mma.sp::ordered_metadata.sync.aligned.m16n8k64.row.col.satfinite.s32.s8.s8.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1, %Rb2, %Rb3}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0; // u4 elements in A and B matrix mma.sp.sync.aligned.m16n8k64.row.col.s32.s4.s4.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1}, {%Rb0, %Rb1}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x1; // u4 elements in A and B matrix mma.sp.sync.aligned.m16n8k128.row.col.satfinite.s32.u4.u4.s32 {%Rd0, %Rd1, %Rd2, %Rd3}, {%Ra0, %Ra1, %Ra2, %Ra3}, {%Rb0, %Rb1, %Rb2, %Rb3}, {%Rc0, %Rc1, %Rc2, %Rc3}, %Re, 0x0; 9.7.14. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions \\uf0c1 The warpgroup level matrix multiply and accumulate operation has either of the following forms, where matrix D is called accumulator: D = A * B + D D = A * B , where the input from accumulator D is disabled.'},\n",
       " {'id': 676,\n",
       "  'content': 'The wgmma instructions perform warpgroup level matrix multiply-and-accumulate operation by having all threads in a warpgroup collectively perform the following actions: Load matrices A, B and D into registers or into shared memory. Perform the following fence operations: wgmma.fence operations to indicate that the register/shared-memory across the warpgroup have been written into. fence.proxy.async operation to make the generic proxy operations visible to the async proxy. Issue the asynchronous matrix multiply and accumulate operations using the wgmma.mma_async operation on the input matrices. The wgmma.mma_async operation is performed in the async proxy. Create a wgmma-group and commit all the prior outstanding wgmma.mma_async operations into the group, by using wgmma.commit_group operation. Wait for the completion of the required wgmma-group. Once the wgmma-group completes, all the wgmma.mma_async operations have been performed and completed. 9.7.14.1. Warpgroup \\uf0c1 A warpgroup is a set of four contiguous warps such that the warp-rank of the first warp is a multiple of 4. warp-rank of a warp is defined as: (%tid.x + %tid.y * %ntid.x + %tid.z * %ntid.x * %ntid.y) / 32 9.7.14.2. Matrix Shape \\uf0c1 The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices A, B and D. The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while D is a MxN matrix. The following matrix shapes are supported for the specified types for the wgmma.mma_async operation: Multiplicand Data type Sparsity Shape Floating-point - .f16 Dense .m64n8k16 , .m64n16k16 , .m64n24k16 , .m64n32k16 , .m64n40k16 , .m64n48k16 , .m64n56k16 , .m64n64k16 , .m64n72k16 , .m64n80k16 , .m64n88k16 , .m64n96k16 , .m64n104k16 , .m64n112k16 , .m64n120k16 , .m64n128k16 , .m64n136k16 , .m64n144k16 , .m64n152k16 , .m64n160k16 , .m64n168k16 , .m64n176k16 , .m64n184k16 , .m64n192k16 , .m64n200k16 , .m64n208k16 , .m64n216k16 , .m64n224k16 , .m64n232k16 , .m64n240k16 , .m64n248k16 , .m64n256k16 Alternate floating-point format - .bf16 Alternate floating-point format - .tf32 Sparse Alternate floating-point format - .tf32 Dense .m64n8k8 , .m64n16k8 , .m64n24k8 , .m64n32k8 , .m64n40k8 , .m64n48k8 , .m64n56k8 , .m64n64k8 , .m64n72k8 , .m64n80k8 , .m64n88k8 , .m64n96k8 , .m64n104k8 , .m64n112k8 , .m64n120k8 , .m64n128k8 , .m64n136k8 , .m64n144k8 , .m64n152k8 , .m64n160k8 , .m64n168k8 , .m64n176k8 , .m64n184k8 , .m64n192k8 , .m64n200k8 , .m64n208k8 , .m64n216k8 , .m64n224k8 , .m64n232k8 , .m64n240k8 , .m64n248k8 , .m64n256k8 Alternate floating-point format - .e4m3 / .e5m2 Dense .m64n8k32 , .m64n16k32 , .m64n24k32 , .m64n32k32 , .m64n40k32 , .m64n48k32 , .m64n56k32 , .m64n64k32 , .m64n72k32 , .m64n80k32 , .m64n88k32 , .m64n96k32 , .m64n104k32 , .m64n112k32 , .m64n120k32 , .m64n128k32 , .m64n136k32 , .m64n144k32 , .m64n152k32 , .m64n160k32 , .m64n168k32 , .m64n176k32 , .m64n184k32 , .m64n192k32 , .m64n200k32 , .m64n208k32 , .m64n216k32 , .m64n224k32 , .m64n232k32 , .m64n240k32 , .m64n248k32 , .m64n256k32 Floating point - .f16 Sparse Altername floating-point format - .bf16 Integer - .u8 / .s8 Dense .m64n8k32 , .m64n16k32 , .m64n24k32 , .m64n32k32 , .m64n48k32 , .m64n64k32 , .m64n80k32 , .m64n96k32 , .m64n112k32 , .m64n128k32 , .m64n144k32 , .m64n160k32 , .m64n176k32 , .m64n192k32 , .m64n208k32 , .m64n224k32 , .m64n240k32 , .m64n256k32 Alternate floating-point format - .e4m3 / .e5m2 Sparse .m64n8k64 , .m64n16k64 , .m64n24k64 , .m64n32k64 , .m64n40k64 , .m64n48k64 , .m64n56k64 , .m64n64k64 , .m64n72k64 , .m64n80k64 , .m64n88k64 , .m64n96k64 , .m64n104k64 , .m64n112k64 , .m64n120k64 , .m64n128k64 , .m64n136k64 , .m64n144k64 , .m64n152k64 , .m64n160k64 , .m64n168k64 , .m64n176k64 , .m64n184k64 , .m64n192k64 , .m64n200k64 , .m64n208k64 , .m64n216k64 , .m64n224k64 , .m64n232k64 , .m64n240k64 , .m64n248k64 , .m64n256k64 Integer - .u8 / .s8 Sparse .m64n8k64 , .m64n16k64 , .m64n24k64 , .m64n32k64 , .m64n48k64 , .m64n64k64 , .m64n80k64 , .m64n96k64 , .m64n112k64 , .m64n128k64 , .m64n144k64 , .m64n160k64 , .m64n176k64 , .m64n192k64 , .m64n208k64 , .m64n224k64 , .m64n240k64 , .m64n256k64 Single-bit - .b1 Dense .m64n8k256 , .m64n16k256 , .m64n24k256 , .m64n32k256 , .m64n48k256 , .m64n64k256 , .m64n80k256 , .m64n96k256 , .m64n112k256 , .m64n128k256 , .m64n144k256 , .m64n160k256 , .m64n176k256 , .m64n192k256 , .m64n208k256 , .m64n224k256 , .m64n240k256 , .m64n256k256 9.7.14.3. For integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have elements of the same data-type, e.g.'},\n",
       " {'id': 677, 'content': 'both signed integer or both unsigned integer.'},\n",
       " {'id': 678,\n",
       "  'content': 'Data-type Multiplicands (A or B) Accumulator (D) Integer both .u8 or both .s8 .s32 Floating Point .f16 .f16 , .f32 Alternate floating Point .bf16 .f32 Alternate floating Point .tf32 .f32 Alternate floating Point .e4m3 , .e5m2 .f16 , .f32 Single-bit integer .b1 .s32 9.7.14.4. Async Proxy \\uf0c1 The wgmma.mma_async operations are performed in the asynchronous proxy (or async proxy). Accessing the same memory location across multiple proxies needs a cross-proxy fence. For the async proxy, fence.proxy.async should be used to synchronize memory between generic proxy and the async proxy. The completion of a wgmma.mma_async operation is followed by an implicit generic-async proxy fence. So the result of the asynchronous operation is made visible to the generic proxy as soon as its completion is observed. wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion of the wgmma.mma_async instructions. 9.7.14.5. Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma.mma_async instruction \\uf0c1 This section describes warpgroup level wgmma.mma_async instruction and the organization of various matrices involved in this instruction. 9.7.14.5.1. Register Fragments and Shared Memory Matrix Layouts \\uf0c1 The input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared memory. The input matrix B of the warpgroup wide MMA operations must be in the shared memory. This section describes the layouts of register fragments and shared memory expected by the warpgroup MMA instructions. When the matrices are in shared memory, their starting addresses must be aligned to 16 bytes. 9.7.14.5.1.1. Register Fragments \\uf0c1 This section describes the organization of various matrices located in register operands of the wgmma.mma_async instruction. 9.7.14.5.1.1.1. Matrix Fragments for wgmma.mma_async.m64nNk16 \\uf0c1 A warpgroup executing wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix Shape . Elements of the matrix are distributed across the threads in a warpgroup so each thread of the warpgroup holds a fragment of the matrix. Multiplicand A in registers: .atype Fragment Elements (low to high) .f16 / .bf16 A vector expression containing four .f16x2 registers, with each register containing two .f16 / .bf16 elements from matrix A. a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 119 . Figure 119 WGMMA .m64nNk16 register fragment layout for matrix A. \\uf0c1 Accumulator D: .dtype Fragment Elements (low to high) .f16 A vector expression containing N/4 number of .f16x2 registers, with each register containing two .f16 elements from matrix D. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ... , 32} .f32 A vector expression containing N/2 number of .f32 registers. The layout of the fragments held by different threads is shown in Figure 120 . Figure 120 WGMMA .m64nNk16 register fragment layout for accumulator matrix D. \\uf0c1 9.7.14.5.1.1.2. Matrix Fragments for wgmma.mma_async.m64nNk8 \\uf0c1 A warpgroup executing wgmma.mma_async.m64nNk8 will compute an MMA operation of shape .m64nNk8 where N is a valid n dimension as listed in Matrix Shape . Multiplicand A in registers: .atype Fragment Elements (low to high) .tf32 A vector expression containing four .b32 registers containing four .tf32 elements from matrix A. a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 121 . Figure 121 WGMMA .m64nNk8 register fragment layout for matrix A. \\uf0c1 Accumulator D: .dtype Fragment Elements (low to high) .f32 A vector expression containing N/2 number of .f32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, ... , 32} The layout of the fragments held by different threads is shown in Figure 122 . Figure 122 WGMMA .m64nNk8 register fragment layout for accumulator matrix D. \\uf0c1 9.7.14.5.1.1.3. Matrix Fragments for wgmma.mma_async.m64nNk32 \\uf0c1 A warpgroup executing wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix Shape . Multiplicand A in registers: .atype Fragment Elements (low to high) .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four .u8 / .s8 elements from matrix A. a0, a1, a2, a3, … , a14, a15 .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four .e4m3 / .e5m2 elements from matrix A. The layout of the fragments held by different threads is shown in Figure 123 . Figure 123 WGMMA .m64nNk32 register fragment layout for matrix A. \\uf0c1 Accumulator D: .dtype Fragment Elements (low to high) Miscellaneous Information .s32 A vector expression containing N/2 number of .s32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N depends on .dtype, as described in the next column. N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} .f32 A vector expression containing N/2 number of .f32 registers. N = 8*i where i = {1, 2, ... , 32} .f16 A vector expression containing N/4 number of .f16x2 registers, with each register containing two .f16 elements from matrix D. The layout of the fragments held by different threads is shown in Figure 124 . Figure 124 WGMMA .m64nNk32 register fragment layout for accumulator matrix D. \\uf0c1 9.7.14.5.1.1.4. Matrix Fragments for wgmma.mma_async.m64nNk256 \\uf0c1 A warpgroup executing wgmma.mma_async.m64nNk256 will compute an MMA operation of shape .m64nNk256 where N is a valid n dimension as listed in Matrix Shape . Multiplicand A in registers: .atype Fragment Elements (low to high) .b1 A vector expression containing four .b32 registers, with each register containing thirty two .b1 element from matrix A. a0, a1, a2, …, a127 The layout of the fragments held by different threads is shown in Figure 125 . Figure 125 WGMMA .m64nNk256 register fragment layout for matrix A. \\uf0c1 Accumulator D: .dtype Fragment Elements (low to high) .s32 A vector expression containing N/2 number of .s32 registers. d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4, ..., 15, 16} The layout of the fragments held by different threads is shown in Figure 126 . Figure 126 WGMMA .m64nNk256 register fragment layout for accumulator matrix D. \\uf0c1 9.7.14.5.1.2. Shared Memory Matrix Layout \\uf0c1 Matrices in shared memory are organized into a number of smaller matrices called core matrices. Each core matrix has 8 rows or columns and the size of each row is 16 bytes. The core matrices occupy contiguous space in shared memory. Matrix A is made up of 8x2 core matrices and Matrix B is made up of 2x(N/8) core matrices. This section describes the layout of the core matrices for each shape. 9.7.14.5.1.2.1. Shared Memory Layout for wgmma.mma_async.m64nNk16 \\uf0c1 Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of eight .f16 / .bf16 elements. 8x8 B Each column is made up of eight .f16 / .bf16 elements. 8x8 Matrices A and B consist of core matrices as shown in Figure 127 . Each colored cell represents a core matrix. Figure 127 WGMMA .m64nNk16 core matrices for A and B \\uf0c1 Layout of core matrices of A is shown in Figure 128 . Each numbered cell represents an individual element of the core matrix. Figure 128 WGMMA .m64nNk16 core matrix layout for A \\uf0c1 Layout of core matrices of B is shown in Figure 129 . Figure 129 WGMMA .m64nNk16 core matrix layout for B \\uf0c1 9.7.14.5.1.2.2. Shared Memory Layout for wgmma.mma_async.m64nNk8 \\uf0c1 Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of four .tf32 elements. 8x4 B Each row is made up of four .tf32 elements. 4x8 Matrices A and B consist of core matrices as shown in Figure 130 . Figure 130 WGMMA .m64nNk8 core matrices for A and B \\uf0c1 Layout of core matrices of A is shown in Figure 131 . Figure 131 WGMMA .m64nNk8 core matrix layout for A \\uf0c1 Layout of core matrices of B is shown in Figure 132 . Figure 132 WGMMA .m64nNk8 core matrix layout for B \\uf0c1 9.7.14.5.1.2.3. Shared Memory Layout for wgmma.mma_async.m64nNk32 \\uf0c1 Core matrices of A and B are as follows: .atype/ .btype Core matrix Matrix description Matrix size .s8 / .u8 A Each row is made up of sixteen .s8 / .u8 elements. 8x4 .e4m3 / .e5m2 Each row is made up of sixteen .e4m3 / .e5m2 elements.'},\n",
       " {'id': 679,\n",
       "  'content': '.s8 / .u8 B Each column is made up of sixteen .s8 / .u8 elements. 4x8 .e4m3 / .e5m2 Each column is made up of sixteen .e4m3 / .e5m2 elements. Matrices A and B consist of core matrices as shown in Figure 133 . Figure 133 WGMMA .m64nNk32 core matrices for A and B \\uf0c1 Layout of core matrices of A is shown in Figure 134 . Figure 134 WGMMA .m64nNk32 core matrix layout for A \\uf0c1 Layout of core matrices of B is shown in Figure 135 . Figure 135 WGMMA .m64nNk32 core matrix layout for B \\uf0c1 9.7.14.5.1.2.4. Shared Memory Layout for wgmma.mma_async.m64nNk256 \\uf0c1 Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of 256 .b1 elements. 8x128 B Each column is made up of 256 .b1 elements. 128x8 Matrices A and B consist of core matrices as shown in Figure 136 . Figure 136 WGMMA .m64nNk256 core matrices for A and B \\uf0c1 Layout of core matrices of A is shown in Figure 137 . Figure 137 WGMMA .m64nNk256 core matrix layout for A \\uf0c1 Layout of core matrices of B is shown in Figure 138 . Figure 138 WGMMA .m64nNk256 core matrix layout for B \\uf0c1 9.7.14.5.1.2.5. Strides \\uf0c1 Leading dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core matrices in the K dimension. Stride dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core matrices in the M or N dimension. Figure 139 and Figure 140 show the leading dimension byte offset and the stride dimension byte offsets for A and B matrices. Matrix A: Figure 139 WGMMA stride and leading dimension byte offset for matrix A \\uf0c1 Matrix B: Figure 140 WGMMA stride and leading dimension byte offset for matrix B \\uf0c1 Leading dimension byte offset and stride dimension byte offset must be specified in the matrix descriptor as described in Matrix Descriptor Format . 9.7.14.5.1.2.6. Swizzling Modes \\uf0c1 The core matrices can be swizzled in the shared memory by specifying one of the following swizzling modes: No swizzling: All the elements of the entire core matrix are adjacent to each other and there is no swizzling. Figure 141 illustrates this: Figure 141 WGMMA core matrices with no swizzling \\uf0c1 32-Byte swizzling: A group of two adjacent core matrices are swizzled as shown in Figure 142 . The swizzling pattern repeats for the remaining core matrices. Figure 142 WGMMA core matrices with 32-byte swizzling \\uf0c1 64-Byte swizzling: A group of four adjacent core matrices are swizzled as shown in Figure 143 . Figure 143 WGMMA core matrices with 64-byte swizzling \\uf0c1 128-Byte swizzling: A group of eight adjacent core matrices are swizzled as shown in Figure 144 . Figure 144 WGMMA core matrices with 128-byte swizzling \\uf0c1 9.7.14.5.1.2.7. Matrix Descriptor Format \\uf0c1 Matrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in the matrix multiply and accumulate operation. It is a 64-bit value contained in a register with the following layout: Bit-field Size in bits Description 13–0 14 matrix-descriptor-encode(Matrix start address) 29–16 14 matrix-descriptor-encode(Leading dimension byte offset) 45–32 14 matrix-descriptor-encode(Stride dimension byte offset) 51–49 3 Matrix base offset. This is valid for all swizzling modes except the no-swizzle mode. 63–62 2 Specifies the swizzling mode to be used: 0: No swizzle 1: 128-Byte swizzle 2: 64-Byte swizzle 3: 32-Byte swizzle where matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 0x4 The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as per the below table: Swizzling mode Starting address of the repeating pattern 128-Byte swizzle 1024-Byte boundary 64-Byte swizzle 512-Byte boundary 32-Byte swizzle 256-Byte boundary Otherwise, the base offset must be a non-zero value, computed using the following formula: base offset = (pattern start addr >> 0x7) & 0x7 9.7.14.5.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async \\uf0c1 wgmma.mma_async Perform matrix multiply-and-accumulate operation across warpgroup Syntax Half precision floating point type: wgmma.mma_async.sync.aligned.shape.dtype.f16.f16 d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b; wgmma.mma_async.sync.aligned.shape.dtype.f16.f16 d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b; .shape = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16, .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16, .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16, .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16, .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16, .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16, .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16, .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16}; .dtype = {.f16, .f32}; Alternate floating point type : .bf16 floating point type: wgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16 d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-a, imm-trans-b; wgmma.mma_async.sync.aligned.shape.dtype.bf16.bf16 d, a, b-desc, scale-d, imm-scale-a, imme-scale-b, imm-trans-b; .shape = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16, .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16, .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16, .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16, .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16, .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16, .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16, .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16}; .dtype = {.f32}; .tf32 floating point type: wgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32 d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b; wgmma.mma_async.sync.aligned.shape.dtype.tf32.tf32 d, a, b-desc, scale-d, imm-scale-a, imme-scale-b; .shape = {.m64n8k8, .m64n16k8, .m64n24k8, .m64n32k8, .m64n40k8, .m64n48k8, .m64n56k8, .m64n64k8, .m64n72k8, .m64n80k8, .m64n88k8, .m64n96k8, .m64n104k8, .m64n112k8, .m64n120k8, .m64n128k8, .m64n136k8, .m64n144k8, .m64n152k8, .m64n160k8, .m64n168k8, .m648176k8, .m64n184k8, .m64n192k8, .m64n200k8, .m64n208k8, .m64n216k8, .m64n224k8, .m64n232k8, .m64n240k8, .m64n248k8, .m64n256k8}; .dtype = {.f32}; FP8 floating point type wgmma.mma_async.sync.aligned.shape.dtype.atype.btype d, a-desc, b-desc, scale-d, imm-scale-a, imme-scale-b; wgmma.mma_async.sync.aligned.shape.dtype.atype.btype d, a, b-desc, scale-d, imm-scale-a, imme-scale-b; .shape = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32, .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32, .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32, .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32, .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32, .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32, .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32, .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32}; .atype = {.e4m3, .e5m2}; .btype = {.e4m3, .e5m2}; .dtype = {.f16, .f32}; Integer type: wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype d, a-desc, b-desc, scale-d; wgmma.mma_async.sync.aligned.shape{.satfinite}.s32.atype.btype d, a, b-desc, scale-d; .shape = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32, .m64n48k32, .m64n64k32, .m64n80k32, .m64n96k32, .m64n112k32, .m64n128k32, .m64n144k32, .m64n160k32, .m648176k32, .m64n192k32, .m64n208k32, .m64n224k32}; .atype = {.s8, .u8}; .btype = {.s8, .u8}; Single bit: wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc d, a-desc, b-desc, scale-d; wgmma.mma_async.sync.aligned.shape.s32.b1.b1.op.popc d, a, b-desc, scale-d; .shape = {.m64n8k256, .m64n16k256, .m64n24k256, .m64n32k256, .m64n48k256, .m64n64k256, .m64n80k256, .m64n96k256, .m64n112k256, .m64n128k256, .m64n144k256, .m64n160k256, .m64n176k256, .m64n192k256, .m64n208k256, .m64n224k256, .m64n240k256, .m64n256k256}; .op = {.and}; Description Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D = A*B+D , where the A matrix is MxK , the B matrix is KxN , and the D matrix is MxN . The operation of the form D = A*B is issued when the input predicate argument scale-d is false. wgmma.fence instruction must be used to fence the register accesses of wgmma.mma_async instruction from their prior accesses. wgmma.commit_group and wgmma.wait_group operations must be used to wait for the completion of the asynchronous matrix multiply and accumulate operations before the results are accessed. Register operand d represents the accumulator matrix as well as the destination matrix, distributed across the participating threads. Register operand a represents the multiplicand matrix A in register distributed across the participating threads. The 64-bit register operands a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and B in shared memory respectively. The contents of a matrix descriptor must be same across all the warps in the warpgroup. The format of the matrix descriptor is described in Matrix Descriptor Format . Matrices A and B are stored in row-major and column-major format respectively. For certain floating point variants, the input matrices A and B can be transposed by specifying the value 1 for the immediate integer arguments imm-trans-a and imm-trans-b respectively. A value of 0 can be used to avoid the transpose operation. The valid values of imm-trans-a and imm-trans-b are 0 and 1. The transpose operation is only supported for the wgmma.mma_async variants with .f16 / .bf16 types on matrices accessed from shared memory using matrix descriptors. For the floating point variants of the wgmma.mma_async operation, each element of the input matrices A and B can be negated by specifying the value -1 for operands imm-scale-a and imm-scale-b respectively. A value of 1 can be used to avoid the negate operation. The valid values of imm-scale-a and imm-scale-b are -1 and 1. The qualifiers .dtype , .atype and .btype indicate the data type of the elements in matrices D, A and B respectively. .atype and .btype must be the same for all floating point wgmma.mma_async variants except for the FP8 floating point variants. The sizes of individual data elements of matrices A and B in alternate floating point variants of the wgmma.mma_async operation are as follows: Matrices A and B have 8-bit data elements when .atype / .btype is .e4m3 / .e5m2 . Matrices A and B have 16-bit data elements when .atype / .btype is .bf16 . Matrices A and B have 32-bit data elements when .atype / .btype is .tf32 . Precision and rounding: Floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision. When .dtype is .f32 , accumulation of the intermediate values is performed with at least single precision. When .dtype is .f16 , the accumulation is performed with at least half precision. .bf16 and .tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision. wgmma.mma_async operation involving type .tf32 will truncate lower 13 bits of the 32-bit input data before multiplication is issued. Integer operations: The integer wgmma.mma_async operation is performed with .s32 accumulators. The mandatory .sync qualifier indicates that wgmma.mma_async instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.mma_async instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.mma_async instruction. In conditionally executed code, a wgmma.mma_async instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise behavior is undefined. Support for .u8.s8 and .s8.u8 as .atype.btype introduced in PTX ISA version 8.4. Examples of half precision floating point type .reg .f16x2 f16a, f16d; .reg .f32 f32d; .reg .b64 descA, descB; .reg .pred scaleD; wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3}, {f16a0, f16a1, f16a2, f16a3}, descB, 1, -1, -1, 1; wgmma.mma_async.sync.aligned.m64n72k16.f16.f16.f16 {f16d0, f16d1, f16d2, f16d3, f16d4, f16d5, f16d6, f16d7, f16d8, f16d9, f16d10, f16d11, f16d12, f16d13, f16d14, f16d15, f16d16, f16d17}, descA, descB, scaleD, -1, 1, 1, 0; Examples of alternate floating point type .reg .f32 f32d; .reg .b32 bf16a .reg .b64 descA, descB; wgmma.mma_async.sync.aligned.m64n120k16.f32.bf16.bf16 {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7, f32d8, f32d9, f32d10, f32d11, f32d12, f32d13, f32d14, f32d15, f32d16, f32d17, f32d18, f32d19, f32d20, f32d21, f32d22, f32d23, f32d24, f32d25, f32d26, f32d27, f32d28, f32d29, f32d30, f32d31, f32d32, f32d33, f32d34, f32d35, f32d36, f32d37, f32d38, f32d39, f32d40, f32d41, f32d42, f32d43, f32d44, f32d45, f32d46, f32d47, f32d48, f32d49, f32d50, f32d51, f32d52, f32d53, f32d54, f32d55, f32d56, f32d57, f32d58, f32d59}, {bf16a0, bf16a1, bf16a2, bf16a3}, descB, scaleD, -1, -1, 0; .reg .f32 f32d; .reg .b64 descA, descB; wgmma.mma_async.sync.aligned.m64n16k8.f32.tf32.tf32 {f32d0, f32d1, f32d2, f32d3, f32d4, f32d5, f32d6, f32d7}, descA, descB, 0, -1, -1; .reg .b32 f16d, f16a; .reg .f32 f32d; .reg .b64 descA, descB; wgmma.mma_async.sync.aligned.m64n8k32.f16.e4m3.e5m2 {f16d0, f16d1}, descA, descB, scaleD, -1, 1; wgmma.mma_async.sync.aligned.m64n8k32.f32.e5m2.e4m3 {f32d0, f32d1, f32d2, f32d3}, {f16a0, f16a1, f16a2, f16a3}, descB, 1, -1, -1; Examples of integer type .reg .s32 s32d, s32a; .reg .u32 u32a; .reg .pred scaleD; .reg .b64 descA, descB; wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.s8.satfinite {s32d0, s32d1, s32d2, s32d3}, {s32a0, s32a1, s32a2, s32a3}, descB, 1; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; wgmma.mma_async.sync.aligned.m64n8k32.s32.s8.u8.satfinite {s32d0, s32d1, s32d2, s32d3}, {s32a0, s32a1, s32a2, s32a3}, descB, scaleD; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.s8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; Examples of single bit type .reg .s32 s32d; .reg .b32 b32a; .reg .pred scaleD; .reg .b64 descA, descB; wgmma.mma_async.sync.aligned.m64n8k256.s32.b1.b1.and.popc {s32d0, s32d1, s32d2, s32d3}, {b32a0, b32a1, b32a2, b32a3}, descB, scaleD; 9.7.14.6. Asynchronous Warpgroup Level Multiply-and-Accumulate Operation using wgmma.mma_async.sp instruction \\uf0c1 This section describes warp-level wgmma.mma_async.sp instruction with sparse matrix A.'},\n",
       " {'id': 680,\n",
       "  'content': 'This variant of the wgmma.mma_async operation can be used when A is a structured sparse matrix with 50% zeros in each row distributed in a shape-specific granularity. For an MxNxK sparse wgmma.mma_async.sp operation, the MxK matrix A is packed into MxK/2 elements. 9.7.14.6.1. For example, in a 64x32 matrix A used in floating point wgmma.mma_async operations, sparsity is expected to be at 2:4 granularity, i.e. Matrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in Figure 83 , with an appropriate matrix size. Granularities for different matrix shapes and data types are described below. Sparse wgmma.mma_async.sp with half-precision and .bf16 type For .f16 and .bf16 types, for all supported 64xNx32 shapes, matrix A is structured sparse at a granularity of 2:4. Only the two non-zero elements are stored in matrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices in the metadata operand. Figure 145 Sparse WGMMA metadata example for .f16 / .bf16 type. \\uf0c1 The sparsity selector indicates a thread-pair within a group of four consecutive threads which contributes the sparsity metadata. Sparse wgmma.mma_async.sp with .tf32 type For .tf32 type, for all supported 64xNx16 shapes, matrix A is structured sparse at a granularity of 1:2. In other words, each chunk of two adjacent elements in a row of matrix A have one zero and one non-zero element. Only the non-zero element is stored in operand for matrix A and the 4-bit index in the metadata indicates the position of the non-zero element in the two-wide chunk. 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in an undefined behavior. Figure 146 Sparse WGMMA metadata example for .tf32 type. Sparse wgmma.mma_async.sp with .e4m3 and .e5m2 floating point type For .e4m3 and .e5m2 types, for all supported 64xNx64 shapes, matrix A is structured sparse at a granularity of 2:4. Figure 147 Sparse WGMMA metadata example for .e4m3 / .e5m2 type. \\uf0c1 All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value results in an undefined behavior. Sparse wgmma.mma_async.sp with integer type For the integer type, for all supported 64xNx64 shapes, matrix A is structured sparse at a granularity of 2:4. Only the two non-zero elements are stored in matrix A and two 2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide chunk. Figure 148 Sparse WGMMA metadata example for .u8 / .s8 type. 9.7.14.6.2. Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A \\uf0c1 In this section we describe how the contents of thread registers are associated with fragments of A matrix and the sparsity metadata. Each warp in the warpgroup provides sparsity information for 16 rows of matrix A. The following table shows the assignment of warps to rows of matrix A: Warp Sparsity information for rows of matrix A %warpid % 4 = 3 48-63 %warpid % 4 = 2 32-47 %warpid % 4 = 1 16-31 %warpid % 4 = 0 0-15 The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and their association with the matrix data. For matrix D, since the matrix dimension - data type combination is the same for all supported shapes, and is already covered in Matrix multiply-accumulate operation using wgmma instruction , the pictorial representations of matrix fragments are not included in this section. 9.7.14.6.2.1. Matrix Fragments for sparse wgmma.mma_async.m64nNk32 \\uf0c1 A warpgroup executing sparse wgmma.mma_async.m64nNk32 will compute an MMA operation of shape .m64nNk32 where N is a valid n dimension as listed in Matrix shape . Multiplicand A, from shared memory is documented in Shared Memory Layout for wgmma.mma_async.m64nNk32 . Multiplicand A, from registers: .atype Fragments Elements .f16 / .bf16 A vector expression containing four .b32 registers, with each register containing two non-zero .f16 / .bf16 elements out of 4 consecutive elements from matrix A. Non-zero elements: a0, a1, a2, a3, a4, a5, a6, a7 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 149 . Figure 149 Sparse WGMMA .m64nNk32 fragment layout for matrix A with .f16 / .bf16 type. \\uf0c1 Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk32 with floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for wgmma.mma_async.m64nNk32 . Metadata operand is a .b32 register containing 16 2-bit vectors each storing the index of a non-zero element of a 4-wide chunk of matrix A. Figure 150 shows the mapping of the metadata bits to the elements of matrix A for a warp. In this figure, variable i represents the value of the sparsity selector operand. Figure 150 Sparse WGMMA .m64nNk32 metadata layout for .f16 / .bf16 type. \\uf0c1 9.7.14.6.2.2. Matrix Fragments for sparse wgmma.mma_async.m64nNk16 \\uf0c1 A warpgroup executing sparse wgmma.mma_async.m64nNk16 will compute an MMA operation of shape .m64nNk16 where N is a valid n dimension as listed in Matrix shape . Multiplicand A, from shared memory is documented in Shared Memory Layout for wgmma.mma_async.m64nNk16 . Multiplicand A, from registers: .atype Fragments Elements .tf32 A vector expression containing four .b32 registers, containing four non-zero .tf32 elements out of eight consecutive elements from matrix A. Non-zero elements: a0, a1, a2, a3 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 151 . Figure 151 Sparse WGMMA .m64nNk16 fragment layout for matrix A with .tf32 type. \\uf0c1 Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma.m64nNk8 with floating point type for the same .dtype format. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for wgmma.mma_async.m64nNk16 . Metadata operand is a .b32 register containing eight 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A. Figure 152 shows the mapping of the metadata bits to the elements of matrix A for a warp. Figure 152 Sparse WGMMA .m64nNk16 metadata layout for .tf32 type. \\uf0c1 9.7.14.6.2.3. Matrix Fragments for sparse wgmma.mma_async.m64nNk64 \\uf0c1 A warpgroup executing sparse wgmma.mma_async.m64nNk64 will compute an MMA operation of shape .m64nNk64 where N is a valid n dimension as listed in Matrix shape . Multiplicand A, from shared memory is documented in Shared Memory Layout for wgmma.mma_async.m64nNk64 . Multiplicand A, from registers: .atype Fragments Elements .e4m3 / .e5m2 A vector expression containing four .b32 registers, with each register containing four non-zero .e4m3 / .e5m2 elements out of eight consecutive elements from matrix A. Non-zero elements: a0, a1, a2, … , a15 Mapping of the non-zero elements is as described in Sparse matrix storage .s8 / .u8 A vector expression containing four .b32 registers, with each register containing four non-zero .s8 / .u8 elements out of eight consecutive elements from matrix A. The layout of the fragments held by different threads is shown in Figure 153 . Figure 153 Sparse WGMMA .m64nNk64 fragment layout for matrix A with .e4m3 / .e5m2 / .s8 / .u8 type. Multiplicand B: Shared memory layout for Matrix B is documented in Shared Memory Layout for wgmma.mma_async.m64nNk64 . Metadata operand is a .b32 register containing 16 4-bit vectors each storing the indices of two non-zero elements of a 4-wide chunk of matrix A. Figure 154 shows the mapping of the metadata bits to the elements of columns 0–31 of matrix A. Figure 154 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 0–31 \\uf0c1 Figure 155 shows the mapping of the metadata bits to the elements of columns 32–63 of matrix A. Figure 155 Sparse WGMMA .m64nNk64 metadata layout for .e4m3 / .e5m2 / .s8 / .u8 type for columns 32–63 \\uf0c1 9.7.14.6.3. Matrix A is made up of 8x2 packed core matrices and Matrix B is made up of 4x (N/8) core matrices. 9.7.14.6.3.1. Shared Memory Layout for wgmma.mma_async.sp.m64nNk32 \\uf0c1 Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of sixteen .f16 / .bf16 elements, with two non-zero elements out of four consecutive elements. 8x16 B Each column is made up of eight .f16 / .bf16 elements. 8x8 Matrices A and B consist of core matrices as shown in Figure 156 . Figure 156 Sparse WGMMA .m64nNk32 core matrices for A and B \\uf0c1 Layout of core matrices of A is shown in Figure 157 . Figure 157 Sparse WGMMA .m64nNk32 core matrix layout for A \\uf0c1 Layout of core matrices of B is shown in Figure 158 . Figure 158 Sparse WGMMA .m64nNk32 core matrix layout for B \\uf0c1 9.7.14.6.3.2. Shared Memory Layout for wgmma.mma_async.sp.m64nNk16 \\uf0c1 Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of eight .tf32 elements with a non-zero element out of two consecutive elements. 8x8 B Each column is made up of four .tf32 elements. 4x8 Matrices A and B consist of core matrices as shown in Figure 159 . Figure 159 Sparse WGMMA .m64nNk16 core matrices for A and B \\uf0c1 Layout of core matrices of A is shown in Figure 160 . Figure 160 Sparse WGMMA .m64nNk16 core matrix layout for A \\uf0c1 Layout of core matrices of B is shown in Figure 161 . Figure 161 Sparse WGMMA .m64nNk16 core matrix layout for B \\uf0c1 9.7.14.6.3.3. Shared Memory Layout for wgmma.mma_async.sp.m64nNk64 \\uf0c1 Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of thirty-two .e4m3 / .e5m2 elements, with two non-zero elements out of four consecutive elements. 8x32 B Each column is made up of eight .f16 / .bf16 elements. 16x8 Matrices A and B consist of core matrices as shown in Figure 162 . Figure 162 Sparse WGMMA .m64nNk64 core matrices for A and B \\uf0c1 Layout of core matrices of A is shown in Figure 163 . Figure 163 Sparse WGMMA .m64nNk64 core matrix layout for A \\uf0c1 Layout of core matrices of B is shown in Figure 164 . Figure 164 Sparse WGMMA .m64nNk64 core matrix layout for B \\uf0c1 9.7.14.6.4. Asynchronous Multiply-and-Accumulate Instruction: wgmma.mma_async.sp \\uf0c1 wgmma.mma_async.sp Perform matrix multiply-and-accumulate operation with sparse matrix A across warpgroup Syntax Half precision floating point type: wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16 d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b; wgmma.mma_async.sp.sync.aligned.shape.dtype.f16.f16 d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-b; .shape = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32, .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32, .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32, .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32, .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32, .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32, .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32, .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32}; .dtype = {.f16, .f32}; Alternate floating point type : .bf16 floating point type: wgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16 d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b, imm-trans-a, imm-trans-b; wgmma.mma_async.sp.sync.aligned.shape.dtype.bf16.bf16 d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imme-scale-b, imm-trans-b; .shape = {.m64n8k32, .m64n16k32, .m64n24k32, .m64n32k32, .m64n40k32, .m64n48k32, .m64n56k32, .m64n64k32, .m64n72k32, .m64n80k32, .m64n88k32, .m64n96k32, .m64n104k32, .m64n112k32, .m64n120k32, .m64n128k32, .m64n136k32, .m64n144k32, .m64n152k32, .m64n160k32, .m64n168k32, .m648176k32, .m64n184k32, .m64n192k32, .m64n200k32, .m64n208k32, .m64n216k32, .m64n224k32, .m64n232k32, .m64n240k32, .m64n248k32, .m64n256k32}; .dtype = {.f32}; .tf32 floating point type: wgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32 d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b; wgmma.mma_async.sp.sync.aligned.shape.dtype.tf32.tf32 d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b; .shape = {.m64n8k16, .m64n16k16, .m64n24k16, .m64n32k16, .m64n40k16, .m64n48k16, .m64n56k16, .m64n64k16, .m64n72k16, .m64n80k16, .m64n88k16, .m64n96k16, .m64n104k16, .m64n112k16, .m64n120k16, .m64n128k16, .m64n136k16, .m64n144k16, .m64n152k16, .m64n160k16, .m64n168k16, .m648176k16, .m64n184k16, .m64n192k16, .m64n200k16, .m64n208k16, .m64n216k16, .m64n224k16, .m64n232k16, .m64n240k16, .m64n248k16, .m64n256k16}; .dtype = {.f32}; FP8 floating point type wgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype d, a-desc, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b; wgmma.mma_async.sp.sync.aligned.shape.dtype.atype.btype d, a, b-desc, sp-meta, sp-sel, scale-d, imm-scale-a, imm-scale-b; .shape = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64, .m64n40k64, .m64n48k64, .m64n56k64, .m64n64k64, .m64n72k64, .m64n80k64, .m64n88k64, .m64n96k64, .m64n104k64, .m64n112k64, .m64n120k64, .m64n128k64, .m64n136k64, .m64n144k64, .m64n152k64, .m64n160k64, .m64n168k64, .m648176k64, .m64n184k64, .m64n192k64, .m64n200k64, .m64n208k64, .m64n216k64, .m64n224k64, .m64n232k64, .m64n240k64, .m64n248k64, .m64n256k64}; .atype = {.e4m3, .e5m2}; .btype = {.e4m3, .e5m2}; .dtype = {.f16, .f32}; Integer type: wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype d, a-desc, b-desc, sp-meta, sp-sel, scale-d; wgmma.mma_async.sp.sync.aligned.shape{.satfinite}.s32.atype.btype d, a, b-desc, sp-meta, sp-sel, scale-d; .shape = {.m64n8k64, .m64n16k64, .m64n24k64, .m64n32k64, .m64n48k64, .m64n64k64, .m64n80k64, .m64n96k64, .m64n112k64, .m64n128k64, .m64n144k64, .m64n160k64, .m648176k64, .m64n192k64, .m64n208k64, .m64n224k64, .m64n240k64, .m64n256k64}; .atype = {.s8, .u8}; .btype = {.s8, .u8}; Description Instruction wgmma.mma_async issues a MxNxK matrix multiply and accumulate operation, D = A*B+D , where the A matrix is MxK , the B matrix is KxN , and the D matrix is MxN . The matrix A is stored in the packed format Mx(K/2) as described in Matrix multiply-accumulate operation using wgmma.mma_async.sp instruction with sparse matrix A . Matrix A is structured sparse as described in Sparse matrix storage . Operands sp-meta and sp-sel represent sparsity metadata and sparsity selector respectively. Operand sp-meta is a 32-bit integer and operand sp-sel is a 32-bit integer constant with values in the range 0..3. The valid values of sp-meta and sp-sel for each shape is specified in Matrix multiply-accumulate operation using wgmma.mma_async.sp instruction with sparse matrix A and are summarized here : Matrix shape .atype Valid values of sp-meta Valid values of sp-sel .m64nNk16 .tf32 0b1110 , 0b0100 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk32 .f16 / .bf16 0b00, 0b01, 0b10, 0b11 0 (threads T0, T1) or 1 (threads T2, T3) .m64nNk64 .e4m3 / .e5m2 / .s8 / .u8 0b00, 0b01, 0b10, 0b11 0 (all threads contribute) Matrices A and B are stored in row-major and column-major format respectively. PTX ISA Notes Introduced in PTX ISA version 8.2. Examples of integer type wgmma.fence.sync.aligned; wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, spMeta, 0, scaleD; wgmma.mma_async.sp.sync.aligned.m64n8k64.s32.s8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, spMeta, 0, scaleD; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0; 9.7.14.7. Asynchronous wgmma Proxy Operations \\uf0c1 This section describes warpgroup level wgmma.fence , wgmma.commit_group and wgmma.wait_group instructions. 9.7.14.7.1. Asynchronous Multiply-and-Accumulate Instruction: wgmma.fence \\uf0c1 wgmma.fence Enforce an ordering of register accesses between wgmma.mma_async and other operations. Syntax wgmma.fence.sync.aligned; Description wgmma.fence instruction establishes an ordering between prior accesses to any warpgroup registers and subsequent accesses to the same registers by a wgmma.mma_async instruction. Only the accumulator register and the input registers containing the fragments of matrix A require this ordering. The wgmma.fence instruction must be issued by all warps of the warpgroup at the following locations: Before the first wgmma.mma_async operation in a warpgroup. Between a register access by a thread in the warpgroup and any wgmma.mma_async instruction that accesses the same registers, either as accumulator or input register containing fragments of matrix A, except when these are accumulator register accesses across multiple wgmma.mma_async instructions of the same shape. In the latter case, an ordering guarantee is provided by default. An async proxy fence must be used to establish an ordering between prior writes to shared memory matrices and subsequent reads of the same matrices in a wgmma.mma_async instruction. The mandatory .sync qualifier indicates that wgmma.fence instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.fence instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.fence instruction. In conditionally executed code, an wgmma.fence instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined. Examples // Example 1, first use example: wgmma.fence.sync.aligned; // Establishes an ordering w.r.t. prior accesses to the registers s32d wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0; // Example 2, use-case with the input value updated in between: wgmma.fence.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; ... mov.b32 s32d0, new_val; wgmma.fence.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d4, s32d5, s32d6, s32d7}, {s32d0, s32d1, s32d2, s32d3}, descB, scaleD; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0; 9.7.14.7.2. Asynchronous Multiply-and-Accumulate Instruction: wgmma.commit_group \\uf0c1 wgmma.commit_group Commits all prior uncommitted wgmma.mma_async operations into a wgmma-group . Syntax wgmma.commit_group.sync.aligned; Description wgmma.commit_group instruction creates a new wgmma-group per warpgroup and batches all prior wgmma.mma_async instructions initiated by the executing warp but not committed to any wgmma-group into the new wgmma-group. If there are no uncommitted wgmma.mma_async instructions then wgmma.commit_group results in an empty wgmma-group. An executing thread can wait for the completion of all wgmma.mma_async operations in a wgmma-group by using wgmma.wait_group . The mandatory .sync qualifier indicates that wgmma.commit_group instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.commit_group instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.commit_group instruction. In conditionally executed code, an wgmma.commit_group instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined. Examples wgmma.commit_group.sync.aligned; 9.7.14.7.3. Asynchronous Multiply-and-Accumulate Instruction: wgmma.wait_group \\uf0c1 wgmma.wait_group Signal the completion of a preceding warpgroup operation. Syntax wgmma.wait_group.sync.aligned N; Description wgmma.wait_group instruction will cause the executing thread to wait until only N or fewer of the most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing threads are complete. For example, when N is 0, the executing thread waits on all the prior wgmma-groups to complete. Accessing the accumulator register or the input register containing the fragments of matrix A of a wgmma.mma_async instruction without first performing a wgmma.wait_group instruction that waits on a wgmma-group including that wgmma.mma_async instruction is undefined behavior. The mandatory .sync qualifier indicates that wgmma.wait_group instruction causes the executing thread to wait until all threads in the warp execute the same wgmma.wait_group instruction before resuming execution. The mandatory .aligned qualifier indicates that all threads in the warpgroup must execute the same wgmma.wait_group instruction. In conditionally executed code, an wgmma.wait_group instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined. Examples wgmma.fence.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k32.s32.u8.u8 {s32d0, s32d1, s32d2, s32d3}, descA, descB, scaleD; wgmma.commit_group.sync.aligned; wgmma.mma_async.sync.aligned.m64n8k16.f32.f16.f16 {f32d0, f32d1, f32d2, f32d3}, {f16a0, f16a1, f16a2, f16a3}, descB, 1, -1, -1, 1; wgmma.commit_group.sync.aligned; wgmma.wait_group.sync.aligned 0; 9.7.15. Stack Manipulation Instructions \\uf0c1 The stack manipulation instructions can be used to dynamically allocate and deallocate memory on the stack frame of the current function. The stack manipulation instrucitons are: stacksave stackrestore alloca 9.7.15.1. Stack Manipulation Instructions: stacksave \\uf0c1 stacksave Save the value of stack pointer into a register. Syntax stacksave.type d; .type = { .u32, .u64 }; Description Copies the current value of stack pointer into the destination register d . Pointer returned by stacksave can be used in a subsequent stackrestore instruction to restore the stack pointer. If d is modified prior to use in stackrestore instruction, it may corrupt data in the stack. Destination operand d has the same type as the instruction type. Semantics d = stackptr; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: stacksave is a preview feature in PTX ISA version 7.3. Target ISA Notes stacksave requires sm_52 or higher. Examples .reg .u32 rd; stacksave.u32 rd; .reg .u64 rd1; stacksave.u64 rd1; 9.7.15.2. Stack Manipulation Instructions: stackrestore \\uf0c1 stackrestore Update the stack pointer with a new value. Syntax stackrestore.type a; .type = { .u32, .u64 }; Description Sets the current stack pointer to source register a . When stackrestore is used with operand a written by a prior stacksave instruction, it will effectively restore the state of stack as it was before stacksave was executed. Note that if stackrestore is used with an arbitrary value of a , it may cause corruption of stack pointer. This implies that the correct use of this feature requires that stackrestore.type a is used after stacksave.type a without redefining the value of a between them. Operand a has the same type as the instruction type. Semantics stackptr = a; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: stackrestore is a preview feature in PTX ISA version 7.3. Target ISA Notes stackrestore requires sm_52 or higher. Examples .reg .u32 ra; stacksave.u32 ra; // Code that may modify stack pointer ... stackrestore.u32 ra; 9.7.15.3. Stack Manipulation Instructions: alloca \\uf0c1 alloca Dynamically allocate memory on stack. Syntax alloca.type ptr, size{, immAlign}; .type = { .u32, .u64 }; Description The alloca instruction dynamically allocates memory on the stack frame of the current function and updates the stack pointer accordingly. The returned pointer ptr points to local memory and can be used in the address operand of ld.local and st.local instructions. If sufficient memory is unavailable for allocation on the stack, then execution of alloca may result in stack overflow. In such cases, attempting to access the allocated memory with ptr will result in undefined program behavior. The memory allocated by alloca is deallocated in the following ways: It is automatically deallocated when the function exits. It can be explicitly deallocated using stacksave and stackrestore instructions: stacksave can be used to save the value of stack pointer before executing alloca , and stackrestore can be used after alloca to restore stack pointer to the original value which was previously saved with stacksave . Note that accessing deallocated memory after executing stackrestore results in undefined behavior. size is an unsigned value which specifies the amount of memory in number of bytes to be allocated on stack. size = 0 may not lead to a valid memory allocation. Both ptr and size have the same type as the instruction type. immAlign is a 32-bit value which specifies the alignment requirement in number of bytes for the memory allocated by alloca . It is an integer constant, must be a power of 2 and must not exceed 2^23. immAlign is an optional argument with default value being 8 which is the minimum guaranteed alignment. Semantics alloca.type ptr, size, immAlign: a = max(immAlign, frame_align); // frame_align is the minimum guaranteed alignment // Allocate size bytes of stack memory with alignment a and update the stack pointer. // Since the stack grows down, the updated stack pointer contains a lower address. stackptr = alloc_stack_mem(size, a); // Return the new value of stack pointer as ptr. Since ptr is the lowest address of the memory // allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory). stacksave ptr; PTX ISA Notes Introduced in PTX ISA version 7.3. Preview Feature: alloca is a preview feature in PTX ISA version 7.3. Target ISA Notes alloca requires sm_52 or higher. Examples .reg .u32 ra, stackptr, ptr, size; stacksave.u32 stackptr; // Save the current stack pointer alloca ptr, size, 8; // Allocate stack memory st.local.u32 [ptr], ra; // Use the allocated stack memory stackrestore.u32 stackptr; // Deallocate memory by restoring the stack pointer 9.7.16. Video Instructions \\uf0c1 All video instructions operate on 32-bit register operands. However, the video instructions may be classified as either scalar or SIMD based on whether their core operation applies to one or multiple values. The video instructions are: vadd , vadd2 , vadd4 vsub , vsub2 , vsub4 vmad vavrg2 , vavrg4 vabsdiff , vabsdiff2 , vabsdiff4 vmin , vmin2 , vmin4 vmax , vmax2 , vmax4 vshl vshr vset , vset2 , vset4 9.7.16.1. Scalar Video Instructions \\uf0c1 All scalar video instructions operate on 32-bit register operands. The scalar video instructions are: vadd vsub vabsdiff vmin vmax vshl vshr vmad vset The scalar video instructions execute the following stages: Extract and sign- or zero-extend byte, half-word, or word values from its source operands, to produce signed 33-bit input values. Perform a scalar arithmetic operation to produce a signed 34-bit result. Optionally clamp the result to the range of the destination type. Optionally perform one of the following: apply a second operation to the intermediate result and a third operand, or truncate the intermediate result to a byte or half-word value and merge into a specified position in the third operand to produce the final result. The general format of scalar video instructions is as follows: // 32-bit scalar operation, with optional secondary operation vop.dtype.atype.btype{.sat} d, a{.asel}, b{.bsel}; vop.dtype.atype.btype{.sat}.secop d, a{.asel}, b{.bsel}, c; // 32-bit scalar operation, with optional data merge vop.dtype.atype.btype{.sat} d.dsel, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .dsel = .asel = .bsel = { .b0, .b1, .b2, .b3, .h0, .h1 }; .secop = { .add, .min, .max }; The source and destination operands are all 32-bit registers. The type of each operand ( .u32 or .s32 ) is specified in the instruction type; all combinations of dtype , atype , and btype are valid. Using the atype/btype and asel/bsel specifiers, the input values are extracted and sign- or zero-extended internally to .s33 values. The primary operation is then performed to produce an .s34 intermediate result. The sign of the intermediate result depends on dtype. The intermediate result is optionally clamped to the range of the destination type (signed or unsigned), taking into account the subword destination size in the case of optional data merging. .s33 optSaturate( .s34 tmp, Bool sat, Bool sign, Modifier dsel ) { if ( !sat ) return tmp; switch ( dsel ) { case .b0, .b1, .b2, .b3: if ( sign ) return CLAMP( tmp, S8_MAX, S8_MIN ); else return CLAMP( tmp, U8_MAX, U8_MIN ); case .h0, .h1: if ( sign ) return CLAMP( tmp, S16_MAX, S16_MIN ); else return CLAMP( tmp, U16_MAX, U16_MIN ); default: if ( sign ) return CLAMP( tmp, S32_MAX, S32_MIN ); else return CLAMP( tmp, U32_MAX, U32_MIN ); } } This intermediate result is then optionally combined with the third source operand using a secondary arithmetic operation or subword data merge, as shown in the following pseudocode. The sign of the third operand is based on dtype . .s33 optSecOp(Modifier secop, .s33 tmp, .s33 c) { switch ( secop ) { .add: return tmp + c; .min: return MIN(tmp, c); .max return MAX(tmp, c); default: return tmp; } } .s33 optMerge( Modifier dsel, .s33 tmp, .s33 c ) { switch ( dsel ) { case .h0: return ((tmp & 0xffff) | (0xffff0000 & c); case .h1: return ((tmp & 0xffff) 32 ) tb = 32; if ( mode == .wrap ) tb = tb & 0x1f; switch ( vop ){ case vshl: tmp = ta > tb; } // saturate, taking into account destination type and merge operations tmp = optSaturate( tmp, sat, isSigned(dtype), dsel ); d = optSecondaryOp( op2, tmp, c ); // optional secondary operation d = optMerge( dsel, tmp, c ); // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vshl , vshr require sm_20 or higher.'},\n",
       " {'id': 681,\n",
       "  'content': 'Examples vshl.s32.u32.u32.clamp r1, r2, r3; vshr.u32.u32.u32.wrap r1, r2, r3.h1; 9.7.16.1.3. Scalar Video Instructions: vmad \\uf0c1 vmad Integer byte/half-word/word multiply-accumulate. Syntax // 32-bit scalar operation vmad.dtype.atype.btype{.sat}{.scale} d, {-}a{.asel}, {-}b{.bsel}, {-}c; vmad.dtype.atype.btype.po{.sat}{.scale} d, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .asel = .bsel = { .b0, .b1, .b2, .b3, .h0, .h1 }; .scale = { .shr7, .shr15 }; Description Calculate (a*b) + c , with optional operand negates, plus one mode, and scaling. The source operands support optional negation with some restrictions. Although PTX syntax allows separate negation of the a and b operands, internally this is represented as negation of the product (a*b) . That is, (a*b) is negated if and only if exactly one of a or b is negated. PTX allows negation of either (a*b) or c . The plus one mode ( .po ) computes (a*b) + c + 1 , which is used in computing averages. Source operands may not be negated in .po mode. The intermediate result of (a*b) is unsigned if atype and btype are unsigned and the product (a*b) is not negated; otherwise, the intermediate result is signed. Input c has the same sign as the intermediate result. The final result is unsigned if the intermediate result is unsigned and c is not negated. Depending on the sign of the a and b operands, and the operand negates, the following combinations of operands are supported for VMAD: (u32 * u32) + u32 // intermediate unsigned; final unsigned -(u32 * u32) + s32 // intermediate signed; final signed (u32 * u32) - u32 // intermediate unsigned; final signed (u32 * s32) + s32 // intermediate signed; final signed -(u32 * s32) + s32 // intermediate signed; final signed (u32 * s32) - s32 // intermediate signed; final signed (s32 * u32) + s32 // intermediate signed; final signed -(s32 * u32) + s32 // intermediate signed; final signed (s32 * u32) - s32 // intermediate signed; final signed (s32 * s32) + s32 // intermediate signed; final signed -(s32 * s32) + s32 // intermediate signed; final signed (s32 * s32) - s32 // intermediate signed; final signed The intermediate result is optionally scaled via right-shift; this result is sign-extended if the final result is signed, and zero-extended otherwise. The final result is optionally saturated to the appropriate 32-bit range based on the type (signed or unsigned) of the final result. Semantics // extract byte/half-word/word and sign- or zero-extend // based on source operand type ta = partSelectSignExtend( a, atype, asel ); tb = partSelectSignExtend( b, btype, bsel ); signedFinal = isSigned(atype) || isSigned(btype) || (a.negate ^ b.negate) || c.negate; tmp[127:0] = ta * tb; lsb = 0; if ( .po ) { lsb = 1; } else if ( a.negate ^ b.negate ) { tmp = ~tmp; lsb = 1; } else if ( c.negate ) { c = ~c; lsb = 1; } c128[127:0] = (signedFinal) sext32( c ) : zext ( c ); tmp = tmp + c128 + lsb; switch( scale ) { case .shr7: result = (tmp >> 7) & 0xffffffffffffffff; case .shr15: result = (tmp >> 15) & 0xffffffffffffffff; } if ( .sat ) { if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN); else result = CLAMP(result, U32_MAX, U32_MIN); } PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vmad requires sm_20 or higher.'},\n",
       " {'id': 682,\n",
       "  'content': 'Examples vmad.s32.s32.u32.sat r0, r1, r2, -r3; vmad.u32.u32.u32.shr15 r0, r1.h0, r2.h0, r3; 9.7.16.1.4. Scalar Video Instructions: vset \\uf0c1 vset Integer byte/half-word/word comparison. Syntax // 32-bit scalar operation, with optional secondary operation vset.atype.btype.cmp d, a{.asel}, b{.bsel}; vset.atype.btype.cmp.op2 d, a{.asel}, b{.bsel}, c; // 32-bit scalar operation, with optional data merge vset.atype.btype.cmp d.dsel, a{.asel}, b{.bsel}, c; .atype = .btype = { .u32, .s32 }; .cmp = { .eq, .ne, .lt, .le, .gt, .ge }; .dsel = .asel = .bsel = { .b0, .b1, .b2, .b3, .h0, .h1 }; .op2 = { .add, .min, .max }; Description Compare input values using specified comparison, with optional secondary arithmetic operation or subword data merge. The intermediate result of the comparison is always unsigned, and therefore destination d and operand c are also unsigned. Semantics // extract byte/half-word/word and sign- or zero-extend // based on source operand type ta = partSelectSignExtend( a, atype, asel ); tb = partSelectSignExtend( b, btype, bsel ); tmp = compare( ta, tb, cmp ) ? 1 : 0; d = optSecondaryOp( op2, tmp, c ); // optional secondary operation d = optMerge( dsel, tmp, c ); // optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2.0. Target ISA Notes vset requires sm_20 or higher. Examples vset.s32.u32.lt r1, r2, r3; vset.u32.u32.ne r1, r2, r3.h1; 9.7.16.2. SIMD Video Instructions \\uf0c1 The SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values. The SIMD video instructions are: vadd2 , vadd4 vsub2 , vsub4 vavrg2 , vavrg4 vabsdiff2 , vabsdiff4 vmin2 , vmin4 vmax2 , vmax4 vset2 , vset4 PTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit values. The SIMD video instructions execute the following stages: Form input vectors by extracting and sign- or zero-extending byte or half-word values from the source operands, to form pairs of signed 17-bit values. Perform a SIMD arithmetic operation on the input pairs. Optionally clamp the result to the appropriate signed or unsigned range, as determinted by the destination type. Optionally perform one of the following: perform a second SIMD merge operation, or apply a scalar accumulate operation to reduce the intermediate SIMD results to a single scalar. The general format of dual half-word SIMD video instructions is as follows: // 2-way SIMD operation, with second SIMD merge or accumulate vop2.dtype.atype.btype{.sat}{.add} d{.mask}, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .mask = { .h0, .h1, .h10 }; .asel = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } }; The general format of quad byte SIMD video instructions is as follows: // 4-way SIMD operation, with second SIMD merge or accumulate vop4.dtype.atype.btype{.sat}{.add} d{.mask}, a{.asel}, b{.bsel}, c; .dtype = .atype = .btype = { .u32, .s32 }; .mask = { .b0, .b1, .b10 .b2, .b20, .b21, .b210, .b3, .b30, .b31, .b310, .b32, .b320, .b321, .b3210 }; .asel = .bsel = .bxyzw, where x,y,z,w are from { 0, ..., 7 }; The source and destination operands are all 32-bit registers. The sign of the intermediate result depends on dtype . 9.7.16.2.1. SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 \\uf0c1 vadd2, vsub2 Integer dual half-word SIMD addition/subtraction. vavrg2 Integer dual half-word SIMD average. vabsdiff2 Integer dual half-word SIMD absolute value of difference. vmin2, vmax2 Integer dual half-word SIMD minimum/maximum. Syntax // SIMD instruction with secondary SIMD merge operation vop2.dtype.atype.btype{.sat} d{.mask}, a{.asel}, b{.bsel}, c; // SIMD instruction with secondary accumulate operation vop2.dtype.atype.btype.add d{.mask}, a{.asel}, b{.bsel}, c; vop2 = { vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2 }; .dtype = .atype = .btype = { .u32, .s32 }; .mask = { .h0, .h1, .h10 }; // defaults to .h10 .asel = .bsel = { .hxy, where x,y are from { 0, 1, 2, 3 } }; .asel defaults to .h10 .bsel defaults to .h32 Description Two-way SIMD parallel arithmetic operation with secondary operation. Elements of each dual half-word source to the operation are selected from any of the four half-words in the two source operands a and b using the asel and bsel modifiers. The selected half-words are then operated on in parallel. The results are optionally clamped to the appropriate range determined by the destination type (signed or unsigned). Saturation cannot be used with the secondary accumulate operation. For instructions with a secondary SIMD merge operation: For half-word positions indicated in mask, the selected half-word results are copied into destination d . For all other positions, the corresponding half-word from source operand c is copied to d . For instructions with a secondary accumulate operation: For half-word positions indicated in mask, the selected half-word results are added to operand c , producing a result in d . Semantics // extract pairs of half-words and sign- or zero-extend // based on operand type Va = extractAndSignExt_2( a, b, .asel, .atype ); Vb = extractAndSignExt_2( a, b, .bsel, .btype ); Vc = extractAndSignExt_2( c ); for (i=0; i= 0 ) { t[i] = ( Va[i] + Vb[i] + 1 ) >> 1; } else { t[i] = ( Va[i] + Vb[i] ) >> 1; } case vabsdiff2: t[i] = | Va[i] - Vb[i] |; case vmin2: t[i] = MIN( Va[i], Vb[i] ); case vmax2: t[i] = MAX( Va[i], Vb[i] ); } if (.sat) { if ( .dtype == .s32 ) t[i] = CLAMP( t[i], S16_MAX, S16_MIN ); else t[i] = CLAMP( t[i], U16_MAX, U16_MIN ); } } // secondary accumulate or SIMD merge mask = extractMaskBits( .mask ); if (.add) { d = c; for (i=0; i= 0 ) { t[i] = ( Va[i] + Vb[i] + 1 ) >> 1; } else { t[i] = ( Va[i] + Vb[i] ) >> 1; } case vabsdiff4: t[i] = | Va[i] - Vb[i] |; case vmin4: t[i] = MIN( Va[i], Vb[i] ); case vmax4: t[i] = MAX( Va[i], Vb[i] ); } if (.sat) { if ( .dtype == .s32 ) t[i] = CLAMP( t[i], S8_MAX, S8_MIN ); else t[i] = CLAMP( t[i], U8_MAX, U8_MIN ); } } // secondary accumulate or SIMD merge mask = extractMaskBits( .mask ); if (.add) { d = c; for (i=0; i %total_smem_size %aggr_smem_size %dynamic_smem_size %current_graph_exec 10.1. Special Registers: %tid \\uf0c1 %tid Thread identifier within a CTA.'},\n",
       " {'id': 683,\n",
       "  'content': 'Syntax (predefined) .sreg .v4 .u32 %tid; // thread id vector .sreg .u32 %tid.x, %tid.y, %tid.z; // thread id components Description A predefined, read-only, per-thread special register initialized with the thread identifier within the CTA. The %tid special register contains a 1D, 2D, or 3D vector to match the CTA shape; the %tid value in unused dimensions is 0 . The fourth element is unused and always returns zero. The number of threads in each dimension are specified by the predefined special register %ntid . Every thread in the CTA has a unique %tid . %tid component values range from 0 through %ntid-1 in each CTA dimension. %tid.y == %tid.z == 0 in 1D CTAs. %tid.z == 0 in 2D CTAs. It is guaranteed that: 0 ; .reg .v4 .b32 %rx; mov.u32 %r0, %clusterid.x; mov.u32 %r1, %clusterid.z; mov.v4.u32 %rx, %clusterid; 10.13. Special Registers: %nclusterid \\uf0c1 %nclusterid Number of cluster identifiers per grid. Syntax (predefined) .sreg .v4 .u32 %nclusterid; .sreg .u32 %nclusterid.x, %nclusterid.y, %nclusterid.z; Description A predefined, read-only special register initialized with the number of clusters in each grid dimension. The %nclusterid special register contains a 3D grid shape vector that holds the grid dimensions in terms of clusters. Refer to the Cuda Programming Guide for details on the maximum values of %nclusterid. {x,y,z} .'},\n",
       " {'id': 684,\n",
       "  'content': 'Examples .reg .b32 %r; .reg .v4 .b32 %rx; mov.u32 %r0, %nclusterid.x; mov.u32 %r1, %nclusterid.z; mov.v4.u32 %rx, %nclusterid; 10.14. Special Registers: %cluster_ctaid \\uf0c1 %cluster_ctaid CTA identifier within a cluster. Syntax (predefined) .sreg .v4 .u32 %cluster_ctaid; .sreg .u32 %cluster_ctaid.x, %cluster_ctaid.y, %cluster_ctaid.z; Description A predefined, read-only special register initialized with the CTA identifier in a cluster in each dimension. Each CTA in a cluster has a unique CTA identifier. The %cluster_ctaid special register contains a 1D, 2D, or 3D vector, depending upon the shape of the cluster. It is guaranteed that: 0 ; .reg .v4 .b32 %rx; mov.u32 %r0, %cluster_ctaid.x; mov.u32 %r1, %cluster_ctaid.z; mov.v4.u32 %rx, %cluster_ctaid; 10.15. Special Registers: %cluster_nctaid \\uf0c1 %cluster_nctaid Number of CTA identifiers per cluster. Syntax (predefined) .sreg .v4 .u32 %cluster_nctaid; .sreg .u32 %cluster_nctaid.x, %cluster_nctaid.y, %cluster_nctaid.z; Description A predefined, read-only special register initialized with the number of CTAs in a cluster in each dimension. The %cluster_nctaid special register contains a 3D grid shape vector that holds the cluster dimensions in terms of CTAs. Refer to the Cuda Programming Guide for details on the maximum values of %cluster_nctaid. Examples .reg .b32 %r; .reg .v4 .b32 %rx; mov.u32 %r0, %cluster_nctaid.x; mov.u32 %r1, %cluster_nctaid.z; mov.v4.u32 %rx, %cluster_nctaid; 10.16. Special Registers: %cluster_ctarank \\uf0c1 %cluster_ctarank CTA identifier in a cluster across all dimensions. Syntax (predefined) .sreg .u32 %cluster_ctarank; Description A predefined, read-only special register initialized with the CTA rank within a cluster across all dimensions. It is guaranteed that: 0 ; Description Special registers %pm0..%pm7 are unsigned 32-bit read-only performance monitor counters. Their behavior is currently undefined. PTX ISA Notes %pm0..%pm3 introduced in PTX ISA version 1.3. %pm4..%pm7 introduced in PTX ISA version 3.0. Target ISA Notes %pm0..%pm3 supported on all target architectures. %pm4..%pm7 require sm_20 or higher. Examples mov.u32 r1,%pm0; mov.u32 r1,%pm7; 10.26. Special Registers: %pm0_64..%pm7_64 \\uf0c1 %pm0_64..%pm7_64 64 bit Performance monitoring counters. Syntax (predefined) .sreg .u64 %pm0_64; .sreg .u64 %pm1_64; .sreg .u64 %pm2_64; .sreg .u64 %pm3_64; .sreg .u64 %pm4_64; .sreg .u64 %pm5_64; .sreg .u64 %pm6_64; .sreg .u64 %pm7_64; Description Special registers %pm0_64..%pm7_64 are unsigned 64-bit read-only performance monitor counters. Notes The lower 32bits of %pm0_64..%pm7_64 are identical to %pm0..%pm7 . PTX ISA Notes %pm0_64..%pm7_64 introduced in PTX ISA version 4.0. Target ISA Notes %pm0_64..%pm7_64 require sm_50 or higher. Examples mov.u32 r1,%pm0_64; mov.u32 r1,%pm7_64; 10.27. Special Registers: %envreg \\uf0c1 %envreg Driver-defined read-only registers. Syntax (predefined) .sreg .b32 %envreg; Description A set of 32 pre-defined read-only registers used to capture execution environment of PTX program outside of PTX virtual machine. These registers are initialized by the driver prior to kernel launch and can contain cta-wide or grid-wide values. Precise semantics of these registers is defined in the driver documentation. PTX ISA Notes Introduced in PTX ISA version 2.1. Examples mov.b32 %r1,%envreg0; // move envreg0 to %r1 10.28. Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi \\uf0c1 %globaltimer, %globaltimer_lo, %globaltimer_hi %globaltimer A predefined, 64-bit global nanosecond timer. %globaltimer_lo The lower 32-bits of %globaltimer. %globaltimer_hi The upper 32-bits of %globaltimer. Syntax (predefined) .sreg .u64 %globaltimer; .sreg .u32 %globaltimer_lo, %globaltimer_hi; Description Special registers intended for use by NVIDIA tools. The behavior is target-specific and may change or be removed in future GPUs. When JIT-compiled to other targets, the value of these registers is unspecified. Target ISA Notes Requires target sm_30 or higher. Examples mov.u64 r1,%globaltimer; 10.29. Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_ \\uf0c1 %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_ %reserved_smem_offset_begin Start of the reserved shared memory region. %reserved_smem_offset_end End of the reserved shared memory region. %reserved_smem_offset_cap Total size of the reserved shared memory region. %reserved_smem_offset_ Offsets in the reserved shared memory region. Syntax (predefined) .sreg .b32 %reserved_smem_offset_begin; .sreg .b32 %reserved_smem_offset_end; .sreg .b32 %reserved_smem_offset_cap; .sreg .b32 %reserved_smem_offset_; Description These are predefined, read-only special registers containing information about the shared memory region which is reserved for the NVIDIA system software use. This region of shared memory is not available to users, and accessing this region from user code results in undefined behavior. Refer to CUDA Programming Guide for details. Target ISA Notes Require sm_80 or higher. Examples .reg .b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1; mov.b32 %reg_begin, %reserved_smem_offset_begin; mov.b32 %reg_end, %reserved_smem_offset_end; mov.b32 %reg_cap, %reserved_smem_offset_cap; mov.b32 %reg_offset0, %reserved_smem_offset_0; mov.b32 %reg_offset1, %reserved_smem_offset_1; 10.30. Special Registers: %total_smem_size \\uf0c1 %total_smem_size Total size of shared memory used by a CTA of a kernel. Syntax (predefined) .sreg .u32 %total_smem_size; Description A predefined, read-only special register initialized with total size of shared memory allocated (statically and dynamically, excluding the shared memory reserved for the NVIDIA system software use) for the CTA of a kernel at launch time. Size is returned in multiples of shared memory allocation unit size supported by target architecture. Allocation unit values are as follows: Target architecture Shared memory allocation unit size sm_2x 128 bytes sm_3x , sm_5x , sm_6x , sm_7x 256 bytes sm_8x , sm_9x 128 bytes PTX ISA Notes Introduced in PTX ISA version 4.1. Examples mov.u32 %r, %total_smem_size; 10.31. Special Registers: %aggr_smem_size \\uf0c1 %aggr_smem_size Total size of shared memory used by a CTA of a kernel. Syntax (predefined) .sreg .u32 %aggr_smem_size; Description A predefined, read-only special register initialized with total aggregated size of shared memory consisting of the size of user shared memory allocated (statically and dynamically) at launch time and the size of shared memory region which is reserved for the NVIDIA system software use. Examples mov.u32 %r, %aggr_smem_size; 10.32. Special Registers: %dynamic_smem_size \\uf0c1 %dynamic_smem_size Size of shared memory allocated dynamically at kernel launch. Syntax (predefined) .sreg .u32 %dynamic_smem_size; Description Size of shared memory allocated dynamically at kernel launch. A predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time. PTX ISA Notes Introduced in PTX ISA version 4.1. Examples mov.u32 %r, %dynamic_smem_size; 10.33. Special Registers: %current_graph_exec \\uf0c1 %current_graph_exec An Identifier for currently executing CUDA device graph. Syntax (predefined) .sreg .u64 %current_graph_exec; Description A predefined, read-only special register initialized with the identifier referring to the CUDA device graph being currently executed. This register is 0 if the executing kernel is not part of a CUDA device graph. Refer to the CUDA Programming Guide for more details on CUDA device graphs. Target ISA Notes Requires sm_50 or higher. Examples mov.u64 r1, %current_graph_exec; 11. Directives \\uf0c1 11.1. PTX Module Directives \\uf0c1 The following directives declare the PTX ISA version of the code in the module, the target architecture for which the code was generated, and the size of addresses within the PTX module. .version .target .address_size 11.1.1. PTX Module Directives: .version \\uf0c1 .version PTX ISA version number. Syntax .version major.minor // major, minor are integers Description Specifies the PTX language version number. The major number is incremented when there are incompatible changes to the PTX language, such as changes to the syntax or semantics. The version major number is used by the PTX compiler to ensure correct execution of legacy PTX code. The minor number is incremented when new features are added to PTX. Semantics Indicates that this module must be compiled with tools that support an equal or greater version number. Each PTX module must begin with a .version directive, and no other .version directive is allowed anywhere else within the module. Examples .version 3.1 .version 3.0 .version 2.3 11.1.2.'},\n",
       " {'id': 685,\n",
       "  'content': 'PTX Module Directives: .target \\uf0c1 .target Architecture and Platform target. Syntax .target stringlist // comma separated list of target specifiers string = { sm_90a, sm_90, // sm_9x target architectures sm_80, sm_86, sm_87, sm_89, // sm_8x target architectures sm_70, sm_72, sm_75, // sm_7x target architectures sm_60, sm_61, sm_62, // sm_6x target architectures sm_50, sm_52, sm_53, // sm_5x target architectures sm_30, sm_32, sm_35, sm_37, // sm_3x target architectures sm_20, // sm_2x target architectures sm_10, sm_11, sm_12, sm_13, // sm_1x target architectures texmode_unified, texmode_independent, // texturing mode debug, // platform option map_f64_to_f32 }; // platform option Description Specifies the set of features in the target architecture for which the current PTX code was generated. In general, generations of SM architectures follow an onion layer model, where each generation adds new features and retains all features of previous generations. The onion layer model allows the PTX code generated for a given target to be run on later generation devices. Target architectures with suffix “ a ”, such as sm_90a , include architecture-accelerated features that are supported on the specified architecture only, hence such targets do not follow the onion layer model. Therefore, PTX code generated for such targets cannot be run on later generation devices. Architecture-accelerated features can only be used with targets that support these features. Semantics Each PTX module must begin with a .version directive, immediately followed by a .target directive containing a target architecture and optional platform options. A .target directive specifies a single target architecture, but subsequent .target directives can be used to change the set of target features allowed during parsing. A program with multiple .target directives will compile and run only on devices that support all features of the highest-numbered architecture listed in the program. PTX features are checked against the specified target architecture, and an error is generated if an unsupported feature is used. The following table summarizes the features in PTX that vary according to target architecture. Target Description sm_90 Baseline feature set for sm_90 architecture. sm_90a Adds support for sm_90a accelerated wgmma and setmaxnreg instructions. Target Description sm_80 Baseline feature set for sm_80 architecture. sm_86 Adds support for .xorsign modifier on min and max instructions. sm_87 Baseline feature set for sm_86 architecture. sm_89 Baseline feature set for sm_86 architecture. Target Description sm_70 Baseline feature set for sm_70 architecture. sm_72 Adds support for integer multiplicand and accumulator matrices in wmma instructions. Adds support for cvt.pack instruction. sm_75 Adds support for sub-byte integer and single-bit multiplicant matrices in wmma instructions. Adds support for ldmatrix instruction. Adds support for movmatrix instruction. Adds support for tanh instruction. Target Description sm_60 Baseline feature set for sm_60 architecture. sm_61 Adds support for dp2a and dp4a instructions. sm_62 Baseline feature set for sm_61 architecture. Target Description sm_50 Baseline feature set for sm_50 architecture. sm_52 Baseline feature set for sm_50 architecture. sm_53 Adds support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types. Target Description sm_30 Baseline feature set for sm_30 architecture. sm_32 Adds 64-bit {atom,red}. {and,or,xor,min,max} instructions. Adds shf instruction. Adds ld.global.nc instruction. sm_35 Adds support for CUDA Dynamic Parallelism. sm_37 Baseline feature set for sm_35 architecture. Target Description sm_20 Baseline feature set for sm_20 architecture. Target Description sm_10 Baseline feature set for sm_10 architecture. Requires map_f64_to_f32 if any .f64 instructions used. sm_11 Adds 64-bit {atom,red}. sm_12 Adds {atom,red}.shared , 64-bit {atom,red}.global , vote instructions. sm_13 Adds double-precision support, including expanded rounding modifiers. Disallows use of map_f64_to_f32 . The texturing mode is specified for an entire module and cannot be changed within the module. The .target debug option declares that the PTX file contains DWARF debug information, and subsequent compilation of PTX will retain information needed for source-level debugging. If the debug option is declared, an error message is generated if no DWARF information is found in the file. The debug option requires PTX ISA version 3.0 or later. map_f64_to_f32 indicates that all double-precision instructions map to single-precision regardless of the target architecture. This enables high-level language compilers to compile programs containing type double to target device that do not support double-precision operations. Note that .f64 storage remains as 64-bits, with only half being used by instructions converted from .f64 to .f32 . Notes Targets of the form compute_xx are also accepted as synonyms for sm_xx targets. Target strings sm_10 and sm_11 introduced in PTX ISA version 1.0. Target strings sm_12 and sm_13 introduced in PTX ISA version 1.2. Texturing mode introduced in PTX ISA version 1.5. Target string sm_20 introduced in PTX ISA version 2.0. Target string sm_30 introduced in PTX ISA version 3.0. Platform option debug introduced in PTX ISA version 3.0. Target string sm_35 introduced in PTX ISA version 3.1. Target strings sm_32 and sm_50 introduced in PTX ISA version 4.0. Target strings sm_37 and sm_52 introduced in PTX ISA version 4.1. Target string sm_53 introduced in PTX ISA version 4.2. Target string sm_60 , sm_61 , sm_62 introduced in PTX ISA version 5.0. Target string sm_70 introduced in PTX ISA version 6.0. Target string sm_72 introduced in PTX ISA version 6.1. Target string sm_75 introduced in PTX ISA version 6.3. Target string sm_80 introduced in PTX ISA version 7.0. Target string sm_86 introduced in PTX ISA version 7.1. Target string sm_87 introduced in PTX ISA version 7.4. Target string sm_89 introduced in PTX ISA version 7.8. Target string sm_90 introduced in PTX ISA version 7.8. Target string sm_90a introduced in PTX ISA version 8.0. Target ISA Notes The .target directive is supported on all target architectures. Examples .target sm_10 // baseline target architecture .target sm_13 // supports double-precision .target sm_20, texmode_independent .target sm_90 // baseline target architecture .target sm_90a // PTX using arch accelerated features 11.1.3. PTX Module Directives: .address_size \\uf0c1 .address_size Address size used throughout PTX module. Syntax .address_size address-size address-size = { 32, 64 }; Description Specifies the address size assumed throughout the module by the PTX code and the binary DWARF information in PTX. Redefinition of this directive within a module is not allowed. In the presence of separate compilation all modules must specify (or default to) the same address size. The .address_size directive is optional, but it must immediately follow the .target directive if present within a module. Semantics If the .address_size directive is omitted, the address size defaults to 32. PTX ISA Notes Introduced in PTX ISA version 2.3. Examples // example directives .address_size 32 // addresses are 32 bit .address_size 64 // addresses are 64 bit // example of directive placement within a module .version 2.3 .target sm_20 .address_size 64 ... .entry foo () { ...'},\n",
       " {'id': 686,\n",
       "  'content': '} 11.2. Specifying Kernel Entry Points and Functions \\uf0c1 The following directives specify kernel entry points and functions. .entry .func 11.2.1. Kernel and Function Directives: .entry \\uf0c1 .entry Kernel entry point and body, with optional parameters. Syntax .entry kernel-name ( param-list ) kernel-body .entry kernel-name kernel-body Description Defines a kernel entry point name, parameters, and body for the kernel function. Parameters are passed via .param space memory and are listed within an optional parenthesized parameter list. Parameters may be referenced by name within the kernel body and loaded into registers using ld.param{::entry} instructions. In addition to normal parameters, opaque .texref , .samplerref , and .surfref variables may be passed as parameters. These parameters can only be referenced by name within texture and surface load, store, and query instructions and cannot be accessed via ld.param instructions. The shape and size of the CTA executing the kernel are available in special registers. Semantics Specify the entry point for a kernel program. At kernel launch, the kernel dimensions and properties are established and made available via special registers, e.g., %ntid , %nctaid , etc. PTX ISA Notes For PTX ISA version 1.4 and later, parameter variables are declared in the kernel parameter list. For PTX ISA versions 1.0 through 1.3, parameter variables are declared in the kernel body. The maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764 bytes. Depending upon the PTX ISA version, the parameter size limit varies. The following table shows the allowed parameter size for a PTX ISA version: PTX ISA Version Maximum parameter size (In bytes) PTX ISA version 8.1 and above 32764 PTX ISA version 1.5 and above 4352 PTX ISA version 1.4 and above 256 The CUDA and OpenCL drivers support the following limits for parameter memory: Driver Parameter memory size CUDA 256 bytes for sm_1x , 4096 bytes for sm_2x and higher , 32764 bytes fo sm_70 and higher OpenCL 32764 bytes for sm_70 and higher, 4352 bytes on sm_6x and lower Target ISA Notes Supported on all target architectures. Examples .entry cta_fft .entry filter ( .param .b32 x, .param .b32 y, .param .b32 z ) { .reg .b32 %r; ld.param.b32 %r1, [x]; ld.param.b32 %r2, [y]; ld.param.b32 %r3, [z]; ...'},\n",
       " {'id': 687,\n",
       "  'content': '} .entry prefix_sum ( .param .align 4 .s32 pitch[8000] ) { .reg .s32 %t; ld.param::entry.s32 %t, [pitch]; ... } 11.2.2.'},\n",
       " {'id': 688,\n",
       "  'content': 'Kernel and Function Directives: .func \\uf0c1 .func Function definition. Syntax .func {.attribute(attr-list)} fname {.noreturn} function-body .func {.attribute(attr-list)} fname (param-list) {.noreturn} function-body .func {.attribute(attr-list)} (ret-param) fname (param-list) function-body Description Defines a function, including input and return parameters and optional function body. An optional .noreturn directive indicates that the function does not return to the caller function. .noreturn directive cannot be specified on functions which have return parameters. See the description of .noreturn directive in Performance-Tuning Directives: .noreturn . An optional .attribute directive specifies additional information associated with the function.'},\n",
       " {'id': 689,\n",
       "  'content': 'See the description of Variable and Function Attribute Directive: .attribute for allowed attributes. A .func definition with no body provides a function prototype. The parameter lists define locally-scoped variables in the function body. Parameters must be base types in either the register or parameter state space. Parameters in register state space may be referenced directly within instructions in the function body. Parameters in .param space are accessed using ld.param{::func} and st.param{::func} instructions in the body. Parameter passing is call-by-value. The last parameter in the parameter list may be a .param array of type .b8 with no size specified. It is used to pass an arbitrary number of parameters to the function packed into a single array object. When calling a function with such an unsized last argument, the last argument may be omitted from the call instruction if no parameter is passed through it. Accesses to this array parameter must be within the bounds of the array. The result of an access is undefined if no array was passed, or if the access was outside the bounds of the actual array being passed. Semantics The PTX syntax hides all details of the underlying calling convention and ABI. The implementation of parameter passing is left to the optimizing translator, which may use a combination of registers and stack locations to pass parameters. Release Notes For PTX ISA version 1.x code, parameters must be in the register state space, there is no stack, and recursion is illegal. PTX ISA versions 2.0 and later with target sm_20 or higher allow parameters in the .param state space, implements an ABI with stack, and supports recursion. PTX ISA versions 2.0 and later with target sm_20 or higher support at most one return value. Support for unsized array parameter introduced in PTX ISA version 6.0. Support for .noreturn directive introduced in PTX ISA version 6.4. Support for .attribute directive introduced in PTX ISA version 8.0. Target ISA Notes Functions without unsized array parameter supported on all target architectures. Unsized array parameter requires sm_30 or higher. .noreturn directive requires sm_30 or higher. .attribute directive requires sm_90 or higher. Examples .func (.reg .b32 rval) foo (.reg .b32 N, .reg .f64 dbl) { .reg .b32 localVar; ... use N, dbl; other code; mov.b32 rval,result; ret; } ... call (fooval), foo, (val0, val1); // return value in fooval ... .func foo (.reg .b32 N, .reg .f64 dbl) .noreturn { .reg .b32 localVar; ... use N, dbl; other code; mov.b32 rval, result; ret; } ... call foo, (val0, val1); ... .func (.param .u32 rval) bar(.param .u32 N, .param .align 4 .b8 numbers[]) { .reg .b32 input0, input1; ld.param.b32 input0, [numbers + 0]; ld.param.b32 input1, [numbers + 4]; ... other code; ret; } ... .param .u32 N; .param .align 4 .b8 numbers[8]; st.param.u32 [N], 2; st.param.b32 [numbers + 0], 5; st.param.b32 [numbers + 4], 10; call (rval), bar, (N, numbers); ... 11.2.3. Kernel and Function Directives: .alias \\uf0c1 .alias Define an alias to existing function symbol. Syntax .alias fAlias, fAliasee; Description .alias is a module scope directive that defines identifier fAlias to be an alias to function specified by fAliasee . Both fAlias and fAliasee are non-entry function symbols. Identifier fAlias is a function declaration without body. Identifier fAliasee is a function symbol which must be defined in the same module as .alias declaration. Function fAliasee cannot have .weak linkage. Prototype of fAlias and fAliasee must match. Program can use either fAlias or fAlisee identifiers to reference function defined with fAliasee . PTX ISA Notes .alias directive introduced in PTX ISA 6.3. Target ISA Notes .alias directive requires sm_30 or higher. Examples .visible .func foo(.param .u32 p) { ... } .visible .func bar(.param .u32 p); .alias bar, foo; .entry test() { .param .u32 p; ... call foo, (p); // call foo directly ... .param .u32 p; call bar, (p); // call foo through alias } .entry filter ( .param .b32 x, .param .b32 y, .param .b32 z ) { .reg .b32 %r1, %r2, %r3; ld.param.b32 %r1, [x]; ld.param.b32 %r2, [y]; ld.param.b32 %r3, [z]; ... } 11.3. Control Flow Directives \\uf0c1 PTX provides directives for specifying potential targets for brx.idx and call instructions. See the descriptions of brx.idx and call for more information. .branchtargets .calltargets .callprototype 11.3.1. Control Flow Directives: .branchtargets \\uf0c1 .branchtargets Declare a list of potential branch targets. Syntax Label: .branchtargets list-of-labels ; Description Declares a list of potential branch targets for a subsequent brx.idx , and associates the list with the label at the start of the line. All control flow labels in the list must occur within the same function as the declaration. The list of labels may use the compact, shorthand syntax for enumerating a range of labels having a common prefix, similar to the syntax described in Parameterized Variable Names . Examples .function foo () { .reg .u32 %r0; ... L1: ...'},\n",
       " {'id': 690,\n",
       "  'content': 'L2: ... L3: ... ts: .branchtargets L1, L2, L3; @p brx.idx %r0, ts; ... .function bar() { .reg .u32 %r0; ... N0: ...'},\n",
       " {'id': 691,\n",
       "  'content': \"N1: ... N2: ... N3: ... N4: ... ts: .branchtargets N; @p brx.idx %r0, ts; ... 11.3.2. Control Flow Directives: .calltargets \\uf0c1 .calltargets Declare a list of potential call targets. Syntax Label: .calltargets list-of-functions ; Description Declares a list of potential call targets for a subsequent indirect call, and associates the list with the label at the start of the line. All functions named in the list must be declared prior to the .calltargets directive, and all functions must have the same type signature. Examples calltgt: .calltargets fastsin, fastcos; ... @p call (%f1), %r0, (%x), calltgt; ... 11.3.3. Control Flow Directives: .callprototype \\uf0c1 .callprototype Declare a prototype for use in an indirect call. Syntax // no input or return parameters label: .callprototype _ .noreturn; // input params, no return params label: .callprototype _ (param-list) .noreturn; // no input params, // return params label: .callprototype (ret-param) _ ; // input, return parameters label: .callprototype (ret-param) _ (param-list); Description Defines a prototype with no specific function name, and associates the prototype with a label. The prototype may then be used in indirect call instructions where there is incomplete knowledge of the possible call targets. Parameters may have either base types in the register or parameter state spaces, or array types in parameter state space. The sink symbol '_' may be used to avoid dummy parameter names. Examples Fproto1: .callprototype _ ; Fproto2: .callprototype _ (.param .f32 _); Fproto3: .callprototype (.param .u32 _) _ ; Fproto4: .callprototype (.param .u32 _) _ (.param .f32 _); ... @p call (%val), %r0, (%f1), Fproto4; ... // example of array parameter Fproto5: .callprototype _ (.param .b8 _[12]); Fproto6: .callprototype _ (.param .f32 _) .noreturn; ... @p call %r0, (%f1), Fproto6; ...\"},\n",
       " {'id': 692,\n",
       "  'content': '11.4. Performance-Tuning Directives \\uf0c1 To provide a mechanism for low-level performance tuning, PTX supports the following directives, which pass information to the backend optimizing compiler. .maxnreg .maxntid .reqntid .minnctapersm .maxnctapersm (deprecated) .pragma The .maxnreg directive specifies the maximum number of registers to be allocated to a single thread; the .maxntid directive specifies the maximum number of threads in a thread block (CTA); the .reqntid directive specifies the required number of threads in a thread block (CTA); and the .minnctapersm directive specifies a minimum number of thread blocks to be scheduled on a single multiprocessor (SM). These can be used, for example, to throttle the resource requirements (e.g., registers) to increase total thread count and provide a greater opportunity to hide memory latency. The .minnctapersm directive can be used together with either the .maxntid or .reqntid directive to trade-off registers-per-thread against multiprocessor utilization without needed to directly specify a maximum number of registers. This may achieve better performance when compiling PTX for multiple devices having different numbers of registers per SM. Currently, the .maxnreg , .maxntid , .reqntid , and .minnctapersm directives may be applied per-entry and must appear between an .entry directive and its body. The directives take precedence over any module-level constraints passed to the optimizing backend. A warning message is generated if the directives’ constraints are inconsistent or cannot be met for the specified target device. A general .pragma directive is supported for passing information to the PTX backend. The directive passes a list of strings to the backend, and the strings have no semantics within the PTX virtual machine model. The interpretation of .pragma values is determined by the backend implementation and is beyond the scope of the PTX ISA. Note that .pragma directives may appear at module (file) scope, at entry-scope, or as statements within a kernel or device function body. 11.4.1. Performance-Tuning Directives: .maxnreg \\uf0c1 .maxnreg Maximum number of registers that can be allocated per thread. Syntax .maxnreg n Description Declare the maximum number of registers per thread in a CTA. Semantics The compiler guarantees that this limit will not be exceeded. The actual number of registers used may be less; for example, the backend may be able to compile to fewer registers, or the maximum number of registers may be further constrained by .maxntid and .maxctapersm . PTX ISA Notes Introduced in PTX ISA version 1.3. Examples .entry foo .maxnreg 16 { ... } // max regs per thread = 16 11.4.2. Performance-Tuning Directives: .maxntid \\uf0c1 .maxntid Maximum number of threads in the thread block (CTA). Syntax .maxntid nx .maxntid nx, ny .maxntid nx, ny, nz Description Declare the maximum number of threads in the thread block (CTA). This maximum is specified by giving the maximum extent of each dimension of the 1D, 2D, or 3D CTA. The maximum number of threads is the product of the maximum extent in each dimension. Semantics The maximum number of threads in the thread block, computed as the product of the maximum extent specified for each dimension, is guaranteed not to be exceeded in any invocation of the kernel in which this directive appears. Exceeding the maximum number of threads results in a runtime error or kernel launch failure. Note that this directive guarantees that the total number of threads does not exceed the maximum, but does not guarantee that the limit in any particular dimension is not exceeded. Examples .entry foo .maxntid 256 { ... } // max threads = 256 .entry bar .maxntid 16,16,4 { ... } // max threads = 1024 11.4.3. Performance-Tuning Directives: .reqntid \\uf0c1 .reqntid Number of threads in the thread block (CTA). Syntax .reqntid nx .reqntid nx, ny .reqntid nx, ny, nz Description Declare the number of threads in the thread block (CTA) by specifying the extent of each dimension of the 1D, 2D, or 3D CTA. The total number of threads is the product of the number of threads in each dimension. Semantics The size of each CTA dimension specified in any invocation of the kernel is required to be equal to that specified in this directive. Specifying a different CTA dimension at launch will result in a runtime error or kernel launch failure. Notes The .reqntid directive cannot be used in conjunction with the .maxntid directive. Examples .entry foo .reqntid 256 { ... } // num threads = 256 .entry bar .reqntid 16,16,4 { ... } // num threads = 1024 11.4.4. Performance-Tuning Directives: .minnctapersm \\uf0c1 .minnctapersm Minimum number of CTAs per SM. Syntax .minnctapersm ncta Description Declare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor (SM). Notes Optimizations based on .minnctapersm need either .maxntid or .reqntid to be specified as well. If the total number of threads on a single SM resulting from .minnctapersm and .maxntid / .reqntid exceed maximum number of threads supported by an SM then directive .minnctapersm will be ignored. In PTX ISA version 2.1 or higher, a warning is generated if .minnctapersm is specified without specifying either .maxntid or .reqntid . PTX ISA Notes Introduced in PTX ISA version 2.0 as a replacement for .maxnctapersm . Examples .entry foo .maxntid 256 .minnctapersm 4 { ... } 11.4.5. Performance-Tuning Directives: .maxnctapersm (deprecated) \\uf0c1 .maxnctapersm Maximum number of CTAs per SM. Syntax .maxnctapersm ncta Description Declare the maximum number of CTAs from the kernel’s grid that may be mapped to a single multiprocessor (SM). Notes Optimizations based on .maxnctapersm generally need .maxntid to be specified as well. The optimizing backend compiler uses .maxntid and .maxnctapersm to compute an upper-bound on per-thread register usage so that the specified number of CTAs can be mapped to a single multiprocessor. However, if the number of registers used by the backend is sufficiently lower than this bound, additional CTAs may be mapped to a single multiprocessor. For this reason, .maxnctapersm has been renamed to .minnctapersm in PTX ISA version 2.0. Deprecated in PTX ISA version 2.0. Examples .entry foo .maxntid 256 .maxnctapersm 4 { ...'},\n",
       " {'id': 693,\n",
       "  'content': '} 11.4.6. Performance-Tuning Directives: .noreturn \\uf0c1 .noreturn Indicate that the function does not return to its caller function. Syntax .noreturn Description Indicate that the function does not return to its caller function. Semantics An optional .noreturn directive indicates that the function does not return to caller function. .noreturn directive can only be specified on device functions and must appear between a .func directive and its body. The directive cannot be specified on functions which have return parameters. If a function with .noreturn directive returns to the caller function at runtime, then the behavior is undefined. Examples .func foo .noreturn { ...'},\n",
       " {'id': 694,\n",
       "  'content': '} 11.4.7. Performance-Tuning Directives: .pragma \\uf0c1 .pragma Pass directives to PTX backend compiler. Syntax .pragma list-of-strings ; Description Pass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler. The .pragma directive may occur at module-scope, at entry-scope, or at statement-level. Semantics The interpretation of .pragma directive strings is implementation-specific and has no impact on PTX semantics. See Descriptions of .pragma Strings for descriptions of the pragma strings defined in ptxas . Examples .pragma \"nounroll\"; // disable unrolling in backend // disable unrolling for current kernel .entry foo .pragma \"nounroll\"; { ... } 11.5. Debugging Directives \\uf0c1 DWARF-format debug information is passed through PTX modules using the following directives: @@DWARF .section .file .loc The .section directive was introduced in PTX ISA version 2.0 and replaces the @@DWARF syntax. The @@DWARF syntax was deprecated in PTX ISA version 2.0 but is supported for legacy PTX ISA version 1.x code. Beginning with PTX ISA version 3.0, PTX files containing DWARF debug information should include the .target debug platform option. This forward declaration directs PTX compilation to retain mappings for source-level debugging. 11.5.1. Debugging Directives: @@dwarf \\uf0c1 @@dwarf DWARF-format information. Syntax @@DWARF dwarf-string dwarf-string may have one of the .byte byte-list // comma-separated hexadecimal byte values .4byte int32-list // comma-separated hexadecimal integers in range [0..2^32-1] .quad int64-list // comma-separated hexadecimal integers in range [0..2^64-1] .4byte label .quad label PTX ISA Notes Introduced in PTX ISA version 1.2. Deprecated as of PTX ISA version 2.0, replaced by .section directive. Examples @@DWARF .section .debug_pubnames, \"\", @progbits @@DWARF .byte 0x2b, 0x00, 0x00, 0x00, 0x02, 0x00 @@DWARF .4byte .debug_info @@DWARF .4byte 0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63 @@DWARF .4byte 0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172 @@DWARF .byte 0x00, 0x00, 0x00, 0x00, 0x00 11.5.2. Debugging Directives: .section \\uf0c1 .section PTX section definition. Syntax .section section_name { dwarf-lines } dwarf-lines have the following formats: .b8 byte-list // comma-separated list of integers // in range [-128..255] .b16 int16-list // comma-separated list of integers // in range [-2^15..2^16-1] .b32 int32-list // comma-separated list of integers // in range [-2^31..2^32-1] label: // Define label inside the debug section .b64 int64-list // comma-separated list of integers // in range [-2^63..2^64-1] .b32 label .b64 label .b32 label+imm // a sum of label address plus a constant integer byte // offset(signed, 32bit) .b64 label+imm // a sum of label address plus a constant integer byte // offset(signed, 64bit) .b32 label1-label2 // a difference in label addresses between labels in // the same dwarf section (32bit) .b64 label3-label4 // a difference in label addresses between labels in // the same dwarf section (64bit) PTX ISA Notes Introduced in PTX ISA version 2.0, replaces @@DWARF syntax. label+imm expression introduced in PTX ISA version 3.2.'},\n",
       " {'id': 695,\n",
       "  'content': 'Support for .b16 integers in dwarf-lines introduced in PTX ISA version 6.0. Support for defining label inside the DWARF section is introduced in PTX ISA version 7.2. label1-label2 expression introduced in PTX ISA version 7.5. Negative numbers in dwarf lines introduced in PTX ISA version 7.5. Examples .section .debug_pubnames { .b32 LpubNames_end0-LpubNames_begin0 LpubNames_begin0: .b8 0x2b, 0x00, 0x00, 0x00, 0x02, 0x00 .b32 .debug_info info_label1: .b32 0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63 .b32 0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172 .b8 0x00, 0x00, 0x00, 0x00, 0x00 LpubNames_end0: } .section .debug_info { .b32 11430 .b8 2, 0 .b32 .debug_abbrev .b8 8, 1, 108, 103, 101, 110, 102, 101, 58, 32, 69, 68, 71, 32, 52, 46, 49 .b8 0 .b32 3, 37, 176, -99 .b32 info_label1 .b32 .debug_loc+0x4 .b8 -11, 11, 112, 97 .b32 info_label1+12 .b64 -1 .b16 -5, -65535 } 11.5.3. Debugging Directives: .file \\uf0c1 .file Source file name.'},\n",
       " {'id': 696,\n",
       "  'content': 'Syntax .file file_index \"filename\" {, timestamp, file_size} Description Associates a source filename with an integer index. .loc directives reference source files by index. .file directive allows optionally specifying an unsigned number representing time of last modification and an unsigned integer representing size in bytes of source file. timestamp and file_size value can be 0 to indicate this information is not available. timestamp value is in format of C and C++ data type time_t . file_size is an unsigned 64-bit integer. The .file directive is allowed only in the outermost scope, i.e., at the same level as kernel and device function declarations. Semantics If timestamp and file size are not specified, they default to 0. Timestamp and file size introduced in PTX ISA version 3.2. Examples .file 1 \"example.cu\" .file 2 \"kernel.cu\" .file 1 “kernel.cu”, 1339013327, 64118 11.5.4. Debugging Directives: .loc \\uf0c1 .loc Source file location. Syntax .loc file_index line_number column_position .loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2 Description Declares the source file location (source file, line number, and column position) to be associated with lexically subsequent PTX instructions. .loc refers to file_index which is defined by a .file directive. To indicate PTX instructions that are generated from a function that got inlined, additional attribute .inlined_at can be specified as part of the .loc directive. .inlined_at attribute specifies source location at which the specified function is inlined. file_index2 , line_number2 , and column_position2 specify the location at which function is inlined. Source location specified as part of .inlined_at directive must lexically precede as source location in .loc directive. The function_name attribute specifies an offset in the DWARF section named .debug_str . Offset is specified as label expression or label + immediate expression where label is defined in .debug_str section. DWARF section .debug_str contains ASCII null-terminated strings that specify the name of the function that is inlined. Note that a PTX instruction may have a single associated source location, determined by the nearest lexically preceding .loc directive, or no associated source location if there is no preceding .loc directive. Labels in PTX inherit the location of the closest lexically following instruction. A label with no following PTX instruction has no associated source location. PTX ISA Notes Introduced in PTX ISA version 1.0. function_name and inlined_at attributes are introduced in PTX ISA version 7.2. Examples .loc 2 4237 0 L1: // line 4237, col 0 of file #2, // inherited from mov mov.u32 %r1,%r2; // line 4237, col 0 of file #2 add.u32 %r2,%r1,%r3; // line 4237, col 0 of file #2 ... L2: // line 4239, col 5 of file #2, // inherited from sub .loc 2 4239 5 sub.u32 %r2,%r1,%r3; // line 4239, col 5 of file #2 .loc 1 21 3 .loc 1 9 3, function_name info_string0, inlined_at 1 21 3 ld.global.u32 %r1, [gg]; // Function at line 9 setp.lt.s32 %p1, %r1, 8; // inlined at line 21 .loc 1 27 3 .loc 1 10 5, function_name info_string1, inlined_at 1 27 3 .loc 1 15 3, function_name .debug_str+16, inlined_at 1 10 5 setp.ne.s32 %p2, %r1, 18; @%p2 bra BB2_3; .section .debug_str { info_string0: .b8 95 // _ .b8 90 // z .b8 51 // 3 .b8 102 // f .b8 111 // o .b8 111 // o .b8 118 // v .b8 0 info_string1: .b8 95 // _ .b8 90 // z .b8 51 // 3 .b8 98 // b .b8 97 // a .b8 114 // r .b8 118 // v .b8 0 .b8 95 // _ .b8 90 // z .b8 51 // 3 .b8 99 // c .b8 97 // a .b8 114 // r .b8 118 // v .b8 0 } 11.6. Linking Directives \\uf0c1 .extern .visible .weak 11.6.1.'},\n",
       " {'id': 697,\n",
       "  'content': 'Linking Directives: .extern \\uf0c1 .extern External symbol declaration. Syntax .extern identifier Description Declares identifier to be defined external to the current module. The module defining such identifier must define it as .weak or .visible only once in a single object file. Extern declaration of symbol may appear multiple times and references to that get resolved against the single definition of that symbol. Examples .extern .global .b32 foo; // foo is defined in another module 11.6.2. Linking Directives: .visible \\uf0c1 .visible Visible (externally) symbol declaration. Syntax .visible identifier Description Declares identifier to be globally visible. Unlike C, where identifiers are globally visible unless declared static, PTX identifiers are visible only within the current module unless declared .visible outside the current. Examples .visible .global .b32 foo; // foo will be externally visible 11.6.3. Linking Directives: .weak \\uf0c1 .weak Visible (externally) symbol declaration. Syntax .weak identifier Description Declares identifier to be globally visible but weak . Weak symbols are similar to globally visible symbols, except during linking, weak symbols are only chosen after globally visible symbols during symbol resolution. Unlike globally visible symbols, multiple object files may declare the same weak symbol, and references to a symbol get resolved against a weak symbol only if no global symbols have the same name. Examples .weak .func (.reg .b32 val) foo; // foo will be externally visible 11.6.4. Linking Directives: .common \\uf0c1 .common Visible (externally) symbol declaration. Syntax .common identifier Description Declares identifier to be globally visible but “common”. Common symbols are similar to globally visible symbols. However multiple object files may declare the same common symbol and they may have different types and sizes and references to a symbol get resolved against a common symbol with the largest size. Only one object file can initialize a common symbol and that must have the largest size among all other definitions of that common symbol from different object files. .common linking directive can be used only on variables with .global storage. It cannot be used on function symbols or on symbols with opaque type. PTX ISA Notes Introduced in PTX ISA version 5.0. Target ISA Notes .common directive requires sm_20 or higher.'},\n",
       " {'id': 698,\n",
       "  'content': 'Examples .common .global .u32 gbl; 11.7. Cluster Dimension Directives \\uf0c1 The following directives specify information about clusters: .reqnctapercluster .explicitcluster .maxclusterrank The .reqnctapercluster directive specifies the number of CTAs in the cluster. The .explicitcluster directive specifies that the kernel should be launched with explicit cluster details. The .maxclusterrank directive specifies the maximum number of CTAs in the cluster. The cluster dimension directives can be applied only on kernel functions. 11.7.1. Cluster Dimension Directives: .reqnctapercluster \\uf0c1 .reqnctapercluster Declare the number of CTAs in the cluster. Syntax .reqnctapercluster nx .reqnctapercluster nx, ny .reqnctapercluster nx, ny, nz Description Set the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of the 1D, 2D, or 3D cluster. The total number of CTAs is the product of the number of CTAs in each dimension. For kernels with .reqnctapercluster directive specified, runtime will use the specified values for configuring the launch if the same are not specified at launch time. Semantics If cluster dimension is explicitly specified at launch time, it should be equal to the values specified in this directive. Specifying a different cluster dimension at launch will result in a runtime error or kernel launch failure. Examples .entry foo .reqnctapercluster 2 { .'},\n",
       " {'id': 699,\n",
       "  'content': '. }\\n.entry bar .reqnctapercluster 2, 2, 1 { . .entry ker .reqnctapercluster 3, 2 { .'},\n",
       " {'id': 700,\n",
       "  'content': '11.7.2. Cluster Dimension Directives: .explicitcluster \\uf0c1 .explicitcluster Declare that Kernel must be launched with cluster dimensions explicitly specified. Syntax .explicitcluster Description Declares that this Kernel should be launched with cluster dimension explicitly specified. Semantics Kernels with .explicitcluster directive must be launched with cluster dimension explicitly specified (either at launch time or via .reqnctapercluster ), otherwise program will fail with runtime error or kernel launch failure. Examples .entry foo .explicitcluster { . 11.7.3. Cluster Dimension Directives: .maxclusterrank \\uf0c1 .maxclusterrank Declare the maximum number of CTAs that can be part of the cluster. Syntax .maxclusterrank n Description Declare the maximum number of thread blocks (CTAs) allowed to be part of the cluster. Semantics Product of the number of CTAs in each cluster dimension specified in any invocation of the kernel is required to be less or equal to that specified in this directive. Otherwise invocation will result in a runtime error or kernel launch failure. The .maxclusterrank directive cannot be used in conjunction with the .reqnctapercluster directive. Examples .entry foo ..maxclusterrank 8 { .'},\n",
       " {'id': 701,\n",
       "  'content': '12. Release Notes \\uf0c1 This section describes the history of change in the PTX ISA and implementation. The first section describes ISA and implementation changes in the current release of PTX ISA version 8.5, and the remaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX ISA version 2.0. Table 32 shows the PTX release history. Table 32 PTX Release History \\uf0c1 PTX ISA Version CUDA Release Supported Targets PTX ISA 1.0 CUDA 1.0 sm_{10,11} PTX ISA 1.1 CUDA 1.1 sm_{10,11} PTX ISA 1.2 CUDA 2.0 sm_{10,11,12,13} PTX ISA 1.3 CUDA 2.1 sm_{10,11,12,13} PTX ISA 1.4 CUDA 2.2 sm_{10,11,12,13} PTX ISA 1.5 driver r190 sm_{10,11,12,13} PTX ISA 2.0 CUDA 3.0, driver r195 sm_{10,11,12,13} , sm_20 PTX ISA 2.1 CUDA 3.1, driver r256 sm_{10,11,12,13} , sm_20 PTX ISA 2.2 CUDA 3.2, driver r260 sm_{10,11,12,13} , sm_20 PTX ISA 2.3 CUDA 4.0, driver r270 sm_{10,11,12,13} , sm_20 PTX ISA 3.0 CUDA 4.1, driver r285 sm_{10,11,12,13} , sm_20 CUDA 4.2, driver r295 sm_{10,11,12,13} , sm_20 , sm_30 PTX ISA 3.1 CUDA 5.0, driver r302 sm_{10,11,12,13} , sm_20 , sm_{30,35} PTX ISA 3.2 CUDA 5.5, driver r319 sm_{10,11,12,13} , sm_20 , sm_{30,35} PTX ISA 4.0 CUDA 6.0, driver r331 sm_{10,11,12,13} , sm_20 , sm_{30,32,35} , sm_50 PTX ISA 4.1 CUDA 6.5, driver r340 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52} PTX ISA 4.2 CUDA 7.0, driver r346 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} PTX ISA 4.3 CUDA 7.5, driver r352 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} PTX ISA 5.0 CUDA 8.0, driver r361 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} PTX ISA 6.0 CUDA 9.0, driver r384 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 PTX ISA 6.1 CUDA 9.1, driver r387 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 PTX ISA 6.2 CUDA 9.2, driver r396 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 PTX ISA 6.3 CUDA 10.0, driver r400 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 6.4 CUDA 10.1, driver r418 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 6.5 CUDA 10.2, driver r440 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_70 , sm_72 , sm_75 PTX ISA 7.0 CUDA 11.0, driver r445 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_80 PTX ISA 7.1 CUDA 11.1, driver r455 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.2 CUDA 11.2, driver r460 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.3 CUDA 11.3, driver r465 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86} PTX ISA 7.4 CUDA 11.4, driver r470 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.5 CUDA 11.5, driver r495 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.6 CUDA 11.6, driver r510 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.7 CUDA 11.7, driver r515 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87} PTX ISA 7.8 CUDA 11.8, driver r520 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_90 PTX ISA 8.0 CUDA 12.0, driver r525 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.1 CUDA 12.1, driver r530 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.2 CUDA 12.2, driver r535 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.3 CUDA 12.3, driver r545 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.4 CUDA 12.4, driver r550 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} PTX ISA 8.5 CUDA 12.5, driver r555 sm_{10,11,12,13} , sm_20 , sm_{30,32,35,37} , sm_{50,52,53} , sm_{60,61,62} , sm_{70,72,75} , sm_{80,86,87,89} , sm_{90,90a} 12.1. Changes in PTX ISA Version 8.5 \\uf0c1 New Features PTX ISA version 8.5 introduces the following new features: Adds support for mma.sp::ordered_metadata instruction.'},\n",
       " {'id': 702,\n",
       "  'content': 'Semantic Changes and Clarifications Values 0b0000 , 0b0101 , 0b1010 , 0b1111 for sparsity metadata (operand e ) of instruction mma.sp are invalid and their usage results in undefined behavior. 12.2. Changes in PTX ISA Version 8.4 \\uf0c1 New Features PTX ISA version 8.4 introduces the following new features: Extends ld , st and atom instructions with .b128 type to support .sys scope. Extends integer wgmma.mma_async instruction to support .u8.s8 and .s8.u8 as .atype and .btype respectively. Extends mma , mma.sp instructions to support FP8 types .e4m3 and .e5m2 . Semantic Changes and Clarifications None.'},\n",
       " {'id': 703,\n",
       "  'content': '12.3. Changes in PTX ISA Version 8.3 \\uf0c1 New Features PTX ISA version 8.3 introduces the following new features: Adds support for pragma used_bytes_mask that is used to specify mask for used bytes for a load operation. Extends isspacep , cvta.to , ld and st instructions to accept ::entry and ::func sub-qualifiers with .param state space qualifier.'},\n",
       " {'id': 704,\n",
       "  'content': 'Adds support for .b128 type on instructions ld , ld.global.nc , ldu , st , mov and atom . Add support for instructions tensormap.replace , tensormap.cp_fenceproxy and support for qualifier .to_proxykind::from_proxykind on instruction fence.proxy to support modifying tensor-map . 12.4. Changes in PTX ISA Version 8.2 \\uf0c1 New Features PTX ISA version 8.2 introduces the following new features: Adds support for .mmio qualifier on ld and st instructions. Extends lop3 instruction to allow predicate destination.'},\n",
       " {'id': 705,\n",
       "  'content': 'Extends multimem.ld_reduce instruction to support .acc::f32 qualifer to allow .f32 precision of the intermediate accumulation. Extends the asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma.mma_async to support .sp modifier that allows matrix multiply-accumulate operation when input matrix A is sparse. Semantic Changes and Clarifications The .multicast::cluster qualifier on cp.async.bulk and cp.async.bulk.tensor instructions is optimized for target architecture sm_90a and may have substantially reduced performance on other targets and hence .multicast::cluster is advised to be used with sm_90a . 12.5. Changes in PTX ISA Version 8.1 \\uf0c1 New Features PTX ISA version 8.1 introduces the following new features: Adds support for st.async and red.async instructions for asynchronous store and asynchronous reduction operations respectively on shared memory. Adds support for .oob modifier on half-precision fma instruction. Adds support for .satfinite saturation modifer on cvt instruction for .f16 , .bf16 and .tf32 formats. Extends support for cvt with .e4m3 / .e5m2 to sm_89 . Extends atom and red instructions to support vector types. Adds support for special register %aggr_smem_size . Extends sured instruction with 64-bit min / max operations. Adds support for increased kernel parameter size of 32764 bytes. Adds support for multimem addresses in memory consistency model. Adds support for multimem.ld_reduce , multimem.st and multimem.red instructions to perform memory operations on multimem addresses. 12.6. Changes in PTX ISA Version 8.0 \\uf0c1 New Features PTX ISA version 8.0 introduces the following new features: Adds support for target sm_90a that supports specialized accelerated features. Adds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma . Extends the asynchronous copy operations with bulk operations that operate on large data, including tensor data. Introduces packed integer types .u16x2 and .s16x2 . Extends integer arithmetic instruction add to allow packed integer types .u16x2 and .s16x2 . Extends integer arithmetic instructions min and max to allow packed integer types .u16x2 and .s16x2 , as well as saturation modifier .relu on .s16x2 and .s32 types. Adds support for special register %current_graph_exec that identifies the currently executing CUDA device graph. Adds support for elect.sync instruction.'},\n",
       " {'id': 706,\n",
       "  'content': 'Adds support for .unified attribute on functions and variables. Adds support for setmaxnreg instruction. Adds support for .sem qualifier on barrier.cluster instruction. Extends the fence instruction to allow opcode-specific synchronizaion using op_restrict qualifier. Adds support for .cluster scope on mbarrier.arrive , mbarrier.arrive_drop , mbarrier.test_wait and mbarrier.try_wait operations. Adds support for transaction count operations on mbarrier objects, specified with .expect_tx and .complete_tx qualifiers. 12.7. Changes in PTX ISA Version 7.8 \\uf0c1 New Features PTX ISA version 7.8 introduces the following new features: Adds support for sm_89 target architecture. Adds support for sm_90 target architecture. Extends bar and barrier instructions to accept optional scope qualifier .cta . Extends .shared state space qualifier with optional sub-qualifier ::cta . Adds support for movmatrix instruction which transposes a matrix in registers across a warp. Adds support for stmatrix instruction which stores one or more matrices to shared memory. Extends the .f64 floating point type mma operation with shapes .m16n8k4 , .m16n8k8 , and .m16n8k16 . Extends add , sub , mul , set , setp , cvt , tanh , ex2 , atom and red instructions with bf16 alternate floating point data format. Adds support for new alternate floating-point data formats .e4m3 and .e5m2 . Extends cvt instruction to convert .e4m3 and .e5m2 alternate floating point data formats. Adds support for griddepcontrol instruction as a communication mechanism to control the execution of dependent grids. Extends mbarrier instruction to allow a new phase completion check operation try_wait . Adds support for new thread scope .cluster which is a set of Cooperative Thread Arrays (CTAs). Extends fence / membar , ld , st , atom , and red instructions to accept .cluster scope. Adds support for extended visibility of shared state space to all threads within a cluster. Extends .shared state space qualifier with ::cluster sub-qualifier for cluster-level visibility of shared memory. Extends isspacep , cvta , ld , st , atom , and red instructions to accept ::cluster sub-qualifier with .shared state space qualifier. Adds support for mapa instruction to map a shared memory address to the corresponding address in a different CTA within the cluster. Adds support for getctarank instruction to query the rank of the CTA that contains a given address. Adds support for new barrier synchronization instruction barrier.cluster . Extends the memory consistency model to include the new cluster scope. Adds support for special registers related to cluster information: %is_explicit_cluster , %clusterid , %nclusterid , %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank . Adds support for cluster dimension directives .reqnctapercluster , .explicitcluster , and .maxclusterrank . 12.8. Changes in PTX ISA Version 7.7 \\uf0c1 New Features PTX ISA version 7.7 introduces the following new features: Extends isspacep and cvta instructions to include the .param state space for kernel function parameters. 12.9. Changes in PTX ISA Version 7.6 \\uf0c1 New Features PTX ISA version 7.6 introduces the following new features: Support for szext instruction which performs sign-extension or zero-extension on a specified value. Support for bmsk instruction which creates a bitmask of the specified width starting at the specified bit position. Support for special registers %reserved_smem_offset_begin , %reserved_smem_offset_end , %reserved_smem_offset_cap , %reserved_smem_offset .'},\n",
       " {'id': 707,\n",
       "  'content': '12.10. Changes in PTX ISA Version 7.5 \\uf0c1 New Features PTX ISA version 7.5 introduces the following new features: Debug information enhancements to support label difference and negative values in the .section debugging directive. Support for ignore-src operand on cp.async instruction. Extensions to the memory consistency model to introduce the following new concepts: A memory proxy as an abstract label for different methods of memory access. Virtual aliases as distinct memory addresses accessing the same physical memory location. Support for new fence.proxy and membar.proxy instructions to allow synchronization of memory accesses performed via virtual aliases. 12.11. Changes in PTX ISA Version 7.4 \\uf0c1 New Features PTX ISA version 7.4 introduces the following new features: Support for sm_87 target architecture. Support for .level::eviction_priority qualifier which allows specifying cache eviction priority hints on ld , ld.global.nc , st , and prefetch instructions. Support for .level::prefetch_size qualifier which allows specifying data prefetch hints on ld and cp.async instructions. Support for createpolicy instruction which allows construction of different types of cache eviction policies. Support for .level::cache_hint qualifier which allows the use of cache eviction policies with ld , ld.global.nc , st , atom , red and cp.async instructions. Support for applypriority and discard operations on cached data. 12.12. Changes in PTX ISA Version 7.3 \\uf0c1 New Features PTX ISA version 7.3 introduces the following new features: Extends mask() operator used in initializers to also support integer constant expression. Adds support for stack manpulation instructions that allow manipulating stack using stacksave and stackrestore instructions and allocation of per-thread stack using alloca instruction. Semantic Changes and Clarifications The unimplemented version of alloca from the older PTX ISA specification has been replaced with new stack manipulation instructions in PTX ISA version 7.3. 12.13. Changes in PTX ISA Version 7.2 \\uf0c1 New Features PTX ISA version 7.2 introduces the following new features: Enhances .loc directive to represent inline function information. Adds support to define labels inside the debug sections. Extends min and max instructions to support .xorsign and .abs modifiers.'},\n",
       " {'id': 708,\n",
       "  'content': '12.14. Changes in PTX ISA Version 7.1 \\uf0c1 New Features PTX ISA version 7.1 introduces the following new features: Support for sm_86 target architecture. Adds a new operator, mask() , to extract a specific byte from variable’s address used in initializers. Extends tex and tld4 instructions to return an optional predicate that indicates if data at specified coordinates is resident in memory. Extends single-bit wmma and mma instructions to support .and operation. Extends mma instruction to support .sp modifier that allows matrix multiply-accumulate operation when input matrix A is sparse. Extends mbarrier.test_wait instruction to test the completion of specific phase parity. 12.15. Changes in PTX ISA Version 7.0 \\uf0c1 New Features PTX ISA version 7.0 introduces the following new features: Support for sm_80 target architecture. Adds support for asynchronous copy instructions that allow copying of data asynchronously from one state space to another. Adds support for mbarrier instructions that allow creation of mbarrier objects in memory and use of these objects to synchronize threads and asynchronous copy operations initiated by threads. Adds support for redux.sync instruction which allows reduction operation across threads in a warp. Adds support for new alternate floating-point data formats .bf16 and .tf32 . Extends wmma instruction to support .f64 type with shape .m8n8k4 . Extends wmma instruction to support .bf16 data format. Extends wmma instruction to support .tf32 data format with shape .m16n16k8 . Extends mma instruction to support .f64 type with shape .m8n8k4 . Extends mma instruction to support .bf16 and .tf32 data formats with shape .m16n8k8 . Extends mma instruction to support new shapes .m8n8k128 , .m16n8k4 , .m16n8k16 , .m16n8k32 , .m16n8k64 , .m16n8k128 and .m16n8k256 . Extends abs and neg instructions to support .bf16 and .bf16x2 data formats. Extends min and max instructions to support .NaN modifier and .f16 , .f16x2 , .bf16 and .bf16x2 data formats. Extends fma instruction to support .relu saturation mode and .bf16 and .bf16x2 data formats. Extends cvt instruction to support .relu saturation mode and .f16 , .f16x2 , .bf16 , .bf16x2 and .tf32 destination formats. Adds support for tanh instruction that computes hyperbolic-tangent. Extends ex2 instruction to support .f16 and .f16x2 types.'},\n",
       " {'id': 709,\n",
       "  'content': '12.16. Changes in PTX ISA Version 6.5 \\uf0c1 New Features PTX ISA version 6.5 introduces the following new features: Adds support for integer destination types for half precision comparison instruction set . Extends abs instruction to support .f16 and .f16x2 types. Adds support for cvt.pack instruction which allows converting two integer values and packing the results together. Adds new shapes .m16n8k8 , .m8n8k16 and .m8n8k32 on the mma instruction. Adds support for ldmatrix instruction which loads one or more matrices from shared memory for mma instruction. Removed Features PTX ISA version 6.5 removes the following features: Support for .satfinite qualifier on floating point wmma.mma instruction has been removed. This support was deprecated since PTX ISA version 6.4. 12.17. Changes in PTX ISA Version 6.4 \\uf0c1 New Features PTX ISA version 6.4 introduces the following new features: Adds support for .noreturn directive which can be used to indicate a function does not return to it’s caller function. Adds support for mma instruction which allows performing matrix multiply-and-accumulate operation. Deprecated Features PTX ISA version 6.4 deprecates the following features: Support for .satfinite qualifier on floating point wmma.mma instruction. Removed Features PTX ISA version 6.4 removes the following features: Support for shfl and vote instructions without the .sync qualifier has been removed for .target sm_70 and higher. This support was deprecated since PTX ISA version 6.0 as documented in PTX ISA version 6.2. Semantic Changes and Clarifications Clarified that resolving references of a .weak symbol considers only .weak or .visible symbols with the same name and does not consider local symbols with the same name. Clarified that in cvt instruction, modifier .ftz can only be specified when either .atype or .dtype is .f32 . 12.18. Changes in PTX ISA Version 6.3 \\uf0c1 New Features PTX ISA version 6.3 introduces the following new features: Support for sm_75 target architecture. Adds support for a new instruction nanosleep that suspends a thread for a specified duration. Adds support for .alias directive which allows definining alias to function symbol. Extends atom instruction to perform .f16 addition operation and .cas.b16 operation. Extends red instruction to perform .f16 addition operation. The wmma instructions are extended to support multiplicand matrices of type .s8 , .u8 , .s4 , .u4 , .b1 and accumulator matrices of type .s32 . Semantic Changes and Clarifications Introduced the mandatory .aligned qualifier for all wmma instructions. Specified the alignment required for the base address and stride parameters passed to wmma.load and wmma.store . Clarified that layout of fragment returned by wmma operation is architecture dependent and passing wmma fragments around functions compiled for different link compatible SM architectures may not work as expected. Clarified that atomicity for {atom/red}.f16x2} operations is guranteed separately for each of the two .f16 elements but not guranteed to be atomic as single 32-bit access. 12.19. Changes in PTX ISA Version 6.2 \\uf0c1 New Features PTX ISA version 6.2 introduces the following new features: A new instruction activemask for querying active threads in a warp. Extends atomic and reduction instructions to perform .f16x2 addition operation with mandatory .noftz qualifier. Deprecated Features PTX ISA version 6.2 deprecates the following features: The use of shfl and vote instructions without the .sync is deprecated retrospectively from PTX ISA version 6.0, which introduced the sm_70 architecture that implements Independent Thread Scheduling . Semantic Changes and Clarifications Clarified that wmma instructions can be used in conditionally executed code only if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined. In the memory consistency model, the definition of morally strong operations was updated to exclude fences from the requirement of complete overlap since fences do not access memory. 12.20. Changes in PTX ISA Version 6.1 \\uf0c1 New Features PTX ISA version 6.1 introduces the following new features: Support for sm_72 target architecture. Support for new matrix shapes 32x8x16 and 8x32x16 in wmma instruction. 12.21. Changes in PTX ISA Version 6.0 \\uf0c1 New Features PTX ISA version 6.0 introduces the following new features: Support for sm_70 target architecture. Specifies the memory consistency model for programs running on sm_70 and later architectures. Various extensions to memory instructions to specify memory synchronization semantics and scopes at which such synchronization can be observed. New instruction wmma for matrix operations which allows loading matrices from memory, performing multiply-and-accumulate on them and storing result in memory. Support for new barrier instruction. Extends neg instruction to support .f16 and .f16x2 types. A new instruction fns which allows finding n-th set bit in integer. A new instruction bar.warp.sync which allows synchronizing threads in warp. Extends vote and shfl instructions with .sync modifier which waits for specified threads before executing the vote and shfl operation respectively. A new instruction match.sync which allows broadcasting and comparing a value across threads in warp. A new instruction brx.idx which allows branching to a label indexed from list of potential targets. Support for unsized array parameter for .func which can be used to implement variadic functions. Support for .b16 integer type in dwarf-lines. Support for taking address of device function return parameters using mov instruction. Semantic Changes and Clarifications Semantics of bar instruction were updated to indicate that executing thread waits for other non-exited threads from it’s warp. Support for indirect branch introduced in PTX 2.1 which was unimplemented has been removed from the spec. Support for taking address of labels, using labels in initializers which was unimplemented has been removed from the spec. Support for variadic functions which was unimplemented has been removed from the spec. 12.22. Changes in PTX ISA Version 5.0 \\uf0c1 New Features PTX ISA version 5.0 introduces the following new features: Support for sm_60 , sm_61 , sm_62 target architecture. Extends atomic and reduction instructions to perform double-precision add operation. Extends atomic and reduction instructions to specify scope modifier. A new .common directive to permit linking multiple object files containing declarations of the same symbol with different size. A new dp4a instruction which allows 4-way dot product with accumulate operation. A new dp2a instruction which allows 2-way dot product with accumulate operation. Support for special register %clock_hi . Semantic Changes and Clarifications Semantics of cache modifiers on ld and st instructions were clarified to reflect cache operations are treated as performance hint only and do not change memory consistency behavior of the program. Semantics of volatile operations on ld and st instructions were clarified to reflect how volatile operations are handled by optimizing compiler. 12.23. Changes in PTX ISA Version 4.3 \\uf0c1 New Features PTX ISA version 4.3 introduces the following new features: A new lop3 instruction which allows arbitrary logical operation on 3 inputs. Adds support for 64-bit computations in extended precision arithmetic instructions. Extends tex.grad instruction to support cube and acube geometries. Extends tld4 instruction to support a2d , cube and acube geometries. Extends tex and tld4 instructions to support optional operands for offset vector and depth compare. Extends txq instruction to support querying texture fields from specific LOD. 12.24. Changes in PTX ISA Version 4.2 \\uf0c1 New Features PTX ISA version 4.2 introduces the following new features: Support for sm_53 target architecture. Support for arithmetic, comparsion and texture instructions for .f16 and .f16x2 types. Support for memory_layout field for surfaces and suq instruction support for querying this field. Semantic Changes and Clarifications Semantics for parameter passing under ABI were updated to indicate ld.param and st.param instructions used for argument passing cannot be predicated. Semantics of {atom/red}.add.f32 were updated to indicate subnormal inputs and results are flushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on shared memory preserve subnormal inputs and results and don’t flush them to zero. 12.25. Changes in PTX ISA Version 4.1 \\uf0c1 New Features PTX ISA version 4.1 introduces the following new features: Support for sm_37 and sm_52 target architectures. Support for new fields array_size , num_mipmap_levels and num_samples for Textures, and the txq instruction support for querying these fields. Support for new field array_size for Surfaces, and the suq instruction support for querying this field. Support for special registers %total_smem_size and %dynamic_smem_size .'},\n",
       " {'id': 710,\n",
       "  'content': '12.26. Changes in PTX ISA Version 4.0 \\uf0c1 New Features PTX ISA version 4.0 introduces the following new features: Support for sm_32 and sm_50 target architectures. Support for 64bit performance counter special registers %pm0_64,..,%pm7_64 .'},\n",
       " {'id': 711,\n",
       "  'content': 'A new istypep instruction. A new instruction, rsqrt.approx.ftz.f64 has been added to compute a fast approximation of the square root reciprocal of a value. Support for a new directive .attribute for specifying special attributes of a variable.'},\n",
       " {'id': 712,\n",
       "  'content': 'Support for .managed variable attribute. Semantic Changes and Clarifications The vote instruction semantics were updated to clearly indicate that an inactive thread in a warp contributes a 0 for its entry when participating in vote.ballot.b32 . 12.27. Changes in PTX ISA Version 3.2 \\uf0c1 New Features PTX ISA version 3.2 introduces the following new features: The texture instruction supports reads from multi-sample and multisample array textures. Extends .section debugging directive to include label + immediate expressions.'},\n",
       " {'id': 713,\n",
       "  'content': 'Extends .file directive to include timestamp and file size information. Semantic Changes and Clarifications The vavrg2 and vavrg4 instruction semantics were updated to indicate that instruction adds 1 only if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than being divided by 2). 12.28. Changes in PTX ISA Version 3.1 \\uf0c1 New Features PTX ISA version 3.1 introduces the following new features: Support for sm_35 target architecture. Support for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work. ld.global.nc for loading read-only global data though the non-coherent texture cache. A new funnel shift instruction, shf . Extends atomic and reduction instructions to perform 64-bit {and, or, xor} operations, and 64-bit integer {min, max} operations. Adds support for mipmaps . Adds support for indirect access to textures and surfaces. Extends support for generic addressing to include the .const state space, and adds a new operator, generic() , to form a generic address for .global or .const variables used in initializers. A new .weak directive to permit linking multiple object files containing declarations of the same symbol. Semantic Changes and Clarifications PTX 3.1 redefines the default addressing for global variables in initializers, from generic addresses to offsets in the global state space. Instruction mad.f32 requires a rounding modifier for sm_20 and higher targets. 12.29. Changes in PTX ISA Version 3.0 \\uf0c1 New Features PTX ISA version 3.0 introduces the following new features: Support for sm_30 target architectures. SIMD video instructions.'},\n",
       " {'id': 714,\n",
       "  'content': 'A new warp shuffle instruction. Instructions mad.cc and madc for efficient, extended-precision integer multiplication. Surface instructions with 3D and array geometries. The texture instruction supports reads from cubemap and cubemap array textures. Platform option .target debug to declare that a PTX module contains DWARF debug information. pmevent.mask , for triggering multiple performance monitor events. Performance monitor counter special registers %pm4..%pm7 . Semantic Changes and Clarifications Special register %gridid has been extended from 32-bits to 64-bits. PTX ISA version 3.0 deprecates module-scoped .reg and .local variables when compiling to the Application Binary Interface (ABI). The shfl instruction semantics were updated to clearly indicate that value of source operand a is unpredictable for inactive and predicated-off threads within the warp. PTX modules no longer allow duplicate .version directives. This feature was unimplemented, so there is no semantic change.'},\n",
       " {'id': 715,\n",
       "  'content': 'Unimplemented instructions suld.p and sust.p. {u32,s32,f32} have been removed.'},\n",
       " {'id': 716,\n",
       "  'content': '12.30. Changes in PTX ISA Version 2.3 \\uf0c1 New Features PTX 2.3 adds support for texture arrays. The texture array feature supports access to an array of 1D or 2D textures, where an integer indexes into the array of textures, and then one or two single-precision floating point coordinates are used to address within the selected 1D or 2D texture. PTX 2.3 adds a new directive, .address_size , for specifying the size of addresses. Variables in .const and .global state spaces are initialized to zero by default. Semantic Changes and Clarifications The semantics of the .maxntid directive have been updated to match the current implementation. Specifically, .maxntid only guarantees that the total number of threads in a thread block does not exceed the maximum. Previously, the semantics indicated that the maximum was enforced separately in each dimension, which is not the case. Bit field extract and insert instructions BFE and BFI now indicate that the len and pos operands are restricted to the value range 0..255 . Unimplemented instructions {atom,red}. {min,max}.f32 have been removed.'},\n",
       " {'id': 717,\n",
       "  'content': '12.31. Changes in PTX ISA Version 2.2 \\uf0c1 New Features PTX 2.2 adds a new directive for specifying kernel parameter attributes; specifically, there is a new directives for specifying that a kernel parameter is a pointer, for specifying to which state space the parameter points, and for optionally specifying the alignment of the memory to which the parameter points. PTX 2.2 adds a new field named force_unnormalized_coords to the .samplerref opaque type. This field is used in the independent texturing mode to override the normalized_coords field in the texture header. This field is needed to support languages such as OpenCL, which represent the property of normalized/unnormalized coordinates in the sampler header rather than in the texture header. PTX 2.2 deprecates explicit constant banks and supports a large, flat address space for the .const state space. Legacy PTX that uses explicit constant banks is still supported. PTX 2.2 adds a new tld4 instruction for loading a component ( r , g , b , or a ) from the four texels compising the bilinear interpolation footprint of a given texture location. This instruction may be used to compute higher-precision bilerp results in software, or for performing higher-bandwidth texture loads. 12.32. Changes in PTX ISA Version 2.1 \\uf0c1 New Features The underlying, stack-based ABI is supported in PTX ISA version 2.1 for sm_2x targets. Support for indirect calls has been implemented for sm_2x targets. New directives, .branchtargets and .calltargets , have been added for specifying potential targets for indirect branches and indirect function calls. A .callprototype directive has been added for declaring the type signatures for indirect function calls. The names of .global and .const variables can now be specified in variable initializers to represent their addresses. A set of thirty-two driver-specific execution environment special registers has been added. These are named %envreg0..%envreg31 . Textures and surfaces have new fields for channel data type and channel order, and the txq and suq instructions support queries for these fields. Directive .minnctapersm has replaced the .maxnctapersm directive. Directive .reqntid has been added to allow specification of exact CTA dimensions. A new instruction, rcp.approx.ftz.f64 , has been added to compute a fast, gross approximate reciprocal. Semantic Changes and Clarifications A warning is emitted if .minnctapersm is specified without also specifying .maxntid .'},\n",
       " {'id': 718,\n",
       "  'content': '12.33. Changes in PTX ISA Version 2.0 \\uf0c1 New Features Floating Point Extensions This section describes the floating-point changes in PTX ISA version 2.0 for sm_20 targets. The goal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility with legacy PTX ISA version 1.x code and sm_1x targets. The changes from PTX ISA version 1.x are as follows: Single-precision instructions support subnormal numbers by default for sm_20 targets. The .ftz modifier may be used to enforce backward compatibility with sm_1x . Single-precision add , sub , and mul now support .rm and .rp rounding modifiers for sm_20 targets. A single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754 compliant rounding modifiers and support for subnormal numbers. The fma.f32 instruction also supports .ftz and .sat modifiers. fma.f32 requires sm_20 . The mad.f32 instruction has been extended with rounding modifiers so that it’s synonymous with fma.f32 for sm_20 targets. Both fma.f32 and mad.f32 require a rounding modifier for sm_20 targets. The mad.f32 instruction without rounding is retained so that compilers can generate code for sm_1x targets. When code compiled for sm_1x is executed on sm_20 devices, mad.f32 maps to fma.rn.f32 . Single- and double-precision div , rcp , and sqrt with IEEE 754 compliant rounding have been added. These are indicated by the use of a rounding modifier and require sm_20 . Instructions testp and copysign have been added. New Instructions A load uniform instruction, ldu , has been added. Surface instructions support additional .clamp modifiers, .clamp and .zero . Instruction sust now supports formatted surface stores. A count leading zeros instruction, clz , has been added. A find leading non-sign bit instruction , bfind , has been added. A bit reversal instruction, brev , has been added. Bit field extract and insert instructions, bfe and bfi , have been added. A population count instruction, popc , has been added. A vote ballot instruction, vote.ballot.b32 , has been added. Instructions {atom,red}.add.f32 have been implemented. Instructions {atom,red} .shared have been extended to handle 64-bit data types for sm_20 targets. A system-level membar instruction, membar.sys , has been added. The bar instruction has been extended as follows: A bar.arrive instruction has been added. Instructions bar.red.popc.u32 and bar.red. {and,or}.pred have been added. bar now supports optional thread count and register operands. Scalar video instructions (includes prmt ) have been added. Instruction isspacep for querying whether a generic address falls within a specified state space window has been added. Instruction cvta for converting global, local, and shared addresses to generic address and vice-versa has been added. Other New Features Instructions ld , ldu , st , prefetch , prefetchu , isspacep , cvta , atom , and red now support generic addressing. New special registers %nwarpid , %nsmid , %clock64 , %lanemask_{eq,le,lt,ge,gt} have been added. Cache operations have been added to instructions ld , st , suld , and sust , e.g., for prefetching to specified level of memory hierarchy. Instructions prefetch and prefetchu have also been added. The .maxnctapersm directive was deprecated and replaced with .minnctapersm to better match its behavior and usage. A new directive, .section , has been added to replace the @@DWARF syntax for passing DWARF-format debugging information through PTX. A new directive, .pragma nounroll , has been added to allow users to disable loop unrolling. Semantic Changes and Clarifications The errata in cvt.ftz for PTX ISA versions 1.4 and earlier, where single-precision subnormal inputs and results were not flushed to zero if either source or destination type size was 64-bits, has been fixed. In PTX ISA version 1.5 and later, cvt.ftz (and cvt for .target sm_1x , where .ftz is implied) instructions flush single-precision subnormal inputs and results to sign-preserving zero for all combinations of floating-point instruction types. To maintain compatibility with legacy PTX code, if .version is 1.4 or earlier, single-precision subnormal inputs and results are flushed to sign-preserving zero only when neither source nor destination type size is 64-bits. Components of special registers %tid , %ntid , %ctaid , and %nctaid have been extended from 16-bits to 32-bits. These registers now have type .v4.u32 . The number of samplers available in independent texturing mode was incorrectly listed as thirty-two in PTX ISA version 1.5; the correct number is sixteen. 14. Descriptions of .pragma Strings \\uf0c1 This section describes the .pragma strings defined by ptxas. 14.1. Pragma Strings: “nounroll” \\uf0c1 “nounroll” Disable loop unrolling in optimizing the backend compiler. Syntax .pragma \"nounroll\"; Description The \"nounroll\" pragma is a directive to disable loop unrolling in the optimizing backend compiler. The \"nounroll\" pragma is allowed at module, entry-function, and statement levels, with the following meanings: module scope disables unrolling for all loops in module, including loops preceding the .pragma . entry-function scope disables unrolling for all loops in the entry function body. statement-level pragma disables unrolling of the loop for which the current block is the loop header. Note that in order to have the desired effect at statement level, the \"nounroll\" directive must appear before any instruction statements in the loop header basic block for the desired loop. The loop header block is defined as the block that dominates all blocks in the loop body and is the target of the loop backedge. Statement-level \"nounroll\" directives appearing outside of loop header blocks are silently ignored. Ignored for sm_1x targets. Examples .entry foo (...) .pragma \"nounroll\"; // do not unroll any loop in this function { ... } .func bar (...) { ... L1_head: .pragma \"nounroll\"; // do not unroll this loop ... @p bra L1_end; L1_body: ... L1_continue: bra L1_head; L1_end: ... } 14.2. Pragma Strings: “used_bytes_mask” \\uf0c1 “used_bytes_mask” Mask for indicating used bytes in data of ld operation. Syntax .pragma \"used_bytes_mask mask\"; Description The \"used_bytes_mask\" pragma is a directive that specifies used bytes in a load operation based on the mask provided. \"used_bytes_mask\" pragma needs to be specified prior to a load instruction for which information about bytes used from the load operation is needed. Pragma is ignored if instruction following it is not a load instruction. For a load instruction without this pragma, all bytes from the load operation are assumed to be used. Operand mask is a 32-bit integer with set bits indicating the used bytes in data of load operation. Semantics Each bit in mask operand corresponds to a byte data where each set bit represents the used byte. Most-significant bit corresponds to most-significant byte of data. // For 4 bytes load with only lower 3 bytes used .pragma \"used_bytes_mask 0x7\"; ld.global.u32 %r0, [gbl]; // Higher 1 byte from %r0 is unused // For vector load of 16 bytes with lower 12 bytes used .pragma \"used_bytes_mask 0xfff\"; ld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // %r3 unused PTX ISA Notes Introduced in PTX ISA version 8.3. Examples .pragma \"used_bytes_mask 0xfff\"; ld.global.v4.u32 {%r0, %r1, %r2, %r3}, [gbl]; // Only lower 12 bytes used 15. Notices \\uf0c1 15.1.'},\n",
       " {'id': 719,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 15.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 720,\n",
       "  'content': '15.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 721,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Introduction 2.'},\n",
       " {'id': 722,\n",
       "  'content': 'Data Representation 2.1. Fundamental Types 2.2. Aggregates and Unions 2.3. Bit Fields 2.4. Texture, Sampler, and Surface Types 3. Function Calling Sequence 3.1. Registers 3.2. Stack Frame 3.3. Parameter Passing 4. System Calls 5. Debug Information 5.1.'},\n",
       " {'id': 723,\n",
       "  'content': 'Generation of Debug Information 5.2. CUDA-Specific DWARF Definitions 6. Example 7.'},\n",
       " {'id': 724,\n",
       "  'content': 'C++ 8. Notices 8.1. Notice 8.2. OpenCL 8.3. Trademarks PTX Interoperability » 1. Introduction v12.5 | PDF | Archive PTX Writer’s Guide to Interoperability The guide to writing ABI-compliant PTX. Introduction \\uf0c1 This document defines the Application Binary Interface (ABI) for the CUDA ® architecture when generating PTX. By following the ABI, external developers can generate compliant PTX code that can be linked with other code. PTX is a low-level parallel-thread-execution virtual machine and ISA (Instruction Set Architecture). PTX can be output from multiple tools or written directly by developers. PTX is meant to be GPU-architecture independent, so that the same code can be reused for different GPU architectures. For more information on PTX, refer to the latest version of the PTX ISA reference document . There are multiple CUDA architecture families, each with their own ISA; e.g. SM 5.x is the Maxwell family, SM 6.x is the Pascal family. This document describes the high-level ABI for all architectures. Programs conforming to an ABI are expected to be executed on the appropriate architecture GPU, and can assume that instructions from that ISA are available. 2.'},\n",
       " {'id': 725,\n",
       "  'content': 'Data Representation \\uf0c1 2.1. Fundamental Types \\uf0c1 The below table shows the native scalar PTX types that are supported. Any PTX producer must use these sizes and alignments in order for its PTX to be compatible with PTX generated by other producers. PTX also supports native vector types, which are discussed in Aggregates and Unions . The sizes of types are defined by the host. For example, pointer size and long int size are dictated by the hosts ABI. PTX has an .address_size directive that specifies the address size used throughout the PTX code. The size of pointers is 32 bits on a 32-bit host or 64 bits on a 64-bit host. However, addresses of the local and shared memory spaces are always 32 bits in size. During separate compilation we store info about the host platform in each object file. The linker will fail to link object files generated for incompatible host platforms. PTX Type Size (bytes) Align (bytes) Hardware Representation .b8 1 1 untyped byte .b16 2 2 untyped halfword .b32 4 4 untyped word .b64 8 8 untyped doubleword .s8 1 1 signed integral byte .s16 2 2 signed integral halfword .s32 4 4 signed integral word .s64 8 8 signed integral doubleword .u8 1 1 unsigned integral byte .u16 2 2 unsigned integral halfword .u32 4 4 unsigned integral word .u64 8 8 unsigned integral doubleword .f16 2 2 IEEE half precision .f32 4 4 IEEE single precision .f64 8 8 IEEE double precision 2.2. Aggregates and Unions \\uf0c1 Beyond the scalar types, PTX also supports native-vector types of these scalar types, with both its vector syntax and its byte-array syntax. For scalar types with a size no greater than four bytes, vector types with 1, 2, 3, and 4 elements exist; for all other types, only 1 and 2 element vector types exist. All aggregates and unions can be supported in PTX with its byte-array syntax. The following are the size-and-alignment rules for all aggregates and unions. For a non-native-vector type, an entire aggregate or union is aligned on the same boundary as its most strictly aligned member. This rule is not followed if the alignments are defined by the input language. For example, in OpenCL built-in vector data types have their alignment set to the size of the built-in data type in bytes. For a native vector type – discussed at the start of this section – the alignment is defined as follows. (For the definitions below, the native vector has n elements and has an element type t.) For a vector with an odd number of elements, its alignment is the same as its member: alignof(t). For a vector with an even number of elements, its alignment is set to number of elements times the alignment of its member: n*alignof(t). Each member is assigned to the lowest available offset with the appropriate alignment. This may require internal padding, depending on the previous member. The size of an aggregate or union, if necessary, is increased to make it a multiple of the alignment of the aggregate or union. This may require tail padding, depending on the last member.'},\n",
       " {'id': 726,\n",
       "  'content': '2.3. Bit Fields \\uf0c1 C structure and union definitions may have bit fields that define integral objects with a specified number of bits. Bit Field Type Width w Range signed char 1 to 8 -2 w-1 to 2 w-1 - 1 unsigned char 1 to 8 0 to 2 w - 1 signed short 1 to 16 -2 w-1 to 2 w-1 - 1 unsigned short 1 to 16 0 to 2 w - 1 signed int 1 to 32 -2 w-1 to 2 w-1 - 1 unsigned int 1 to 32 0 to 2 w - 1 signed long long 1 to 64 -2 w-1 to 2 w-1 - 1 unsigned long long 1 to 64 0 to 2 w - 1 Current GPUs only support little-endian memory, so the below assumes little-endian layout. The following are rules that apply to bit fields. Plain bit fields (neither signed nor unsigned is specified) are treated as signed. When no type is provided (e.g., signed : 6 is specified), the type defaults to int. Bit fields obey the same size and alignment rules as other structure and union members, with the following modifications. Bit fields are allocated in memory from right to left (least to more significant) for little endian. A bit field must entirely reside in a storage unit appropriate for its declared type. A bit field should never cross its unit boundary. Bit fields may share a storage unit with other structure and union members, including members that are not bit fields, as long as there is enough space within the storage unit. Unnamed bit fields do not affect the alignment of a structure or union. Zero-length bit fields force the alignment of the following member of a structure to the next alignment boundary corresponding to the bit-field type. An unnamed, zero-length bit field will not force the external alignment of the structure to that boundary. If an unnamed, zero-length bit field has a stricter alignment than the external alignment, there is no guarantee that the stricter alignment will be maintained when the structure or union gets allocated to memory. The following figures contain examples of bit fields. Figure 1 shows the byte offsets (upper corners) and the bit numbers (lower corners) that are used in the examples. The remaining figures show different bit-field examples. Bit Numbering \\uf0c1 Bit-field Allocation \\uf0c1 Boundary Alignment \\uf0c1 Storage Unit Sharing \\uf0c1 Union Allocation \\uf0c1 Unnamed Bit Fields \\uf0c1 2.4. Texture, Sampler, and Surface Types \\uf0c1 Texture, sampler and surface types are used to define references to texture and surface memory. The CUDA architecture provides hardware and instructions to efficiently read data from texture or surface memory as opposed to global memory. References to textures are bound through runtime functions to device read-only regions of memory, called a texture memory, before they can be used by a kernel. A texture reference has several attributes e.g. normalized mode, addressing mode, and texture filtering etc. A sampler reference can be used to sample a texture when read in a kernel. A surface reference is used to read or write data from and to the surface memory. It also has various attributes similar to a texture. At the PTX level objects that access texture or surface memory are referred to as opaque objects. Textures are expressed by either a .texref or .samplerref type and surfaces are expressed by the .surfref type. The data of opaque objects can be accessed by specific instructions (TEX for .texref/.samplerref and SULD/SUST for .surfref). The attributes of opaque objects are implemented by allocating a descriptor in memory which is populated by the driver. PTX TXQ/SUQ instructions get translated into memory reads of fields of the descriptor. The internal format of the descriptor varies with each architecture and should not be relied on by the user. The data and the attributes of an opaque object may be accessed directly if the texture or surface reference is known at compile time or indirectly. If the reference is not known during compile time all information required to read data and attributes is contained in a .b64 value called the handle. The handle can be used to pass and return oqaque object references to and from functions as well as to reference external textures, samplers and surfaces. 3. Function Calling Sequence \\uf0c1 This section describes the PTX-level function calling sequence, including register usage, stack-frame layout, and parameter passing. The PTX-level function calling sequence describes what gets represented in PTX to enable function calls. There is an abstraction at this level. Most of the details associated with the function calling sequence are handled at the SASS level. PTX versions earlier than 2.0 do not conform to the ABI defined in this document, and cannot perform ABI compatible function calls. For the calling convention to work PTX version 2.0 or greater must be used. 3.1. Registers \\uf0c1 At the PTX level, the registers that are specified are virtual. Register allocation occurs during PTX-to-SASS translation. The PTX-to-SASS translation also converts parameters and return values to physical registers or stack locations. 3.2. Stack Frame \\uf0c1 The PTX level has no concept of the software stack. Manipulation of the stack is completely defined at the SASS level, and gets allocated during the PTX-to-SASS translation process. 3.3. Parameter Passing \\uf0c1 At the PTX level, all parameters and return values present in a device function use the parameter state space (.param). The below table contains the rules for handling parameters and return values that are defined at the source level. For each source-level type, the corresponding PTX-level type that should be used is provided. Source Type Size in Bits PTX Type Integral types 8 to 32 (A) .u32 (if unsigned) or .s32 (if signed) Integral types 64 .u64 (if unsigned) or .s64 (if signed) Pointers (B) 32 .u32 Pointers (B) 64 .u64 Floating-point types (C) 32 .f32 Floating-point types (C) 64 .f64 Aggregates or unions Any size .align align .b8 name [ size ] Where align is overall aggregate-or-union alignment in bytes (D), name is variable name associated with aggregate or union, and size is the aggregate-or-union size in bytes. Handles (E) 64 .b64 (assigned from .texref, .sampleref, .surfref) NOTES: Values shorter than 32-bits are sign extended or zero extended, depending on whether they are signed or unsigned types. Unless the memory type is specified in the function declaration, all pointers passed at the PTX level must use a generic address. 16-bit floating-point types are only used for storage. Therefore, they cannot be used for parameters or return values. The alignment must be 1, 2, 4, 8, 16, 32, 64, or 128 bytes. The PTX built-in opaque types such as texture, sampler, and surface types are can be passed into functions as parameters and be returned by them through 64-bit handles. The handle contains the necessary information to access the actual data from the texture or surface memory as well as the attributes of the object stored in its type descriptor. See section Texture, Sampler, and Surface Types for more information on handles. 4. System Calls \\uf0c1 System calls are calls into the driver operating system code. In PTX they look like regular calls, but the function definition is not given. A prototype must be provided in the PTX file, but the implementation of the function is provided by the driver. The prototype for the vprintf system call is: . extern . func (.'},\n",
       " {'id': 727,\n",
       "  'content': 'param . s32 status ) vprintf (. param t1 format , . param t2 valist ) The following are the definitions for the vprintf parameters and return value. status : The status value that is returned by vprintf. format : A pointer to the format specifier input. For 32-bit addresses, type t1 is .b32. For 64-bit addresses, type t1 is .b64. valist : A pointer to the valist input. For 32-bit addresses, type t2 is .b32. For 64-bit addresses, type t2 is .b64. A call to vprintf using 32-bit addresses looks like: cvta . global . b32 % r2 , _fmt ; st . b32 [ param0 ], % r2 ; cvta . local . b32 % r3 , _valist_array ; st . b32 [ param1 ], % r3 ; call . uni ( _ ), vprintf , ( param0 , param1 ); For this code, _fmt is the format string in global memory, and _valist_array is the valist of arguments. Note that any pointers must be converted to generic space. The vprintf syscall is emitted as part of the printf function defined in “stdio.h”. The prototype for the malloc system call is: . param t1 ptr ) malloc (. param t2 size ) The following are the definitions for the malloc parameters and return value. ptr : The pointer to the memory that was allocated by malloc. size : The size of memory needed from malloc. This size is defined by the type size_t. When size_t is 32 bits, type t2 is .b32. When size_t is 64 bits, type t2 is .b64. The prototype for the free system call is: . func free (. param t1 ptr ) The following is the definition for the free parameter. ptr : The pointer to the memory that should be freed. The malloc and free system calls are emitted as part of the malloc and free functions defined in “malloc.h”. In order to support assert, the PTX function call __assertfail is used whenever the assert expression produces a false value. The prototype for the __assertfail system call is: . func __assertfail (. param t1 message , .'},\n",
       " {'id': 728,\n",
       "  'content': 'param t1 file , . b32 line , . param t1 function , . param t2 charSize ) The following are the definitions for the __assertfail parameters. message : The pointer to the string that should be output. file : The pointer to the file name string associated with the assert. line : The line number associated with the assert. function : The pointer to the function name string associated with the assert. charSize : The size in bytes of the characters contained in the __assertfail parameter strings. The only supported character size is 1. The character size is defined by the type size_t. The __assertfail system call is emitted as part of the assert macro defined in “assert.h”. 5. Debug Information \\uf0c1 Debug information is encoded in DWARF (Debug With Arbitrary Record Format). 5.1. Generation of Debug Information \\uf0c1 The responsibility for generating debug information is split between the PTX producer and the PTX-to-SASS backend. The PTX producer is responsible for emitting binary DWARF into the PTX file, using the .section and .b8-.b16-.b32-and-.b64 directives in PTX. This should contain the .debug_info and .debug_abbrev sections, and possibly optional sections .debug_pubnames and .debug_aranges. These sections are standard DWARF2 sections that refer to labels and registers in the PTX. The PTX-to-SASS backend is responsible for generating the .debug_line section from the .file and .loc directives in the PTX file. This section maps source lines to SASS addresses. The backend also generates the .debug_frame section.'},\n",
       " {'id': 729,\n",
       "  'content': '5.2. CUDA-Specific DWARF Definitions \\uf0c1 In order to support debugging of multiple memory segments, address class codes are defined to reflect the memory space of variables. The address-class values are emitted as the DW_AT_address_class attribute for all variable and parameter Debugging Information Entries. The address class codes are defined in the below table. Code Value Description ADDR_code_space 1 Code storage ADDR_reg_space 2 Register storage ADDR_sreg_space 3 Special register storage ADDR_const_space 4 Constant storage ADDR_global_space 5 Global storage ADDR_local_space 6 Local storage ADDR_param_space 7 Parameter storage ADDR_shared_space 8 Shared storage ADDR_surf_space 9 Surface storage ADDR_tex_space 10 Texture storage ADDR_tex_sampler_space 11 Texture sampler storage ADDR_generic_space 12 Generic-address storage 6. Example \\uf0c1 The following is example PTX with debug information for implementing the following program that makes a call: __device__ __noinline__ int foo ( int i , int j ) { return i + j ; } __global__ void test ( int * p ) { * p = foo ( 1 , 2 ); } The resulting PTX would be something like: . version 4.2 .'},\n",
       " {'id': 730,\n",
       "  'content': 'target sm_20 , debug . address_size 64 . file 1 \"call_example.cu\" . visible . b32 func_retval0 ) // return value _Z3fooii ( . b32 _Z3fooii_param_0 , // parameter \"i\" . b32 _Z3fooii_param_1 ) // parameter \"j\" { . reg . s32 % r ; . loc 1 1 1 // following instructions are for line 1 func_begin0 : ld . u32 % r1 , [ _Z3fooii_param_0 ]; // load 1st param ld . u32 % r2 , [ _Z3fooii_param_1 ]; // load 2nd param . loc 1 3 1 // following instructions are for line 3 add . s32 % r3 , % r1 , % r2 ; st . b32 [ func_retval0 + 0 ], % r3 ; // store return value ret ; func_end0 : } . entry _Z4testPi ( . u64 _Z4testPi_param_0 ) // parameter *p { . s64 % rd ; . loc 1 6 1 func_begin1 : ld . u64 % rd1 , [ _Z4testPi_param_0 ]; // load *p mov . u32 % r1 , 1 ; mov . u32 % r2 , 2 ; . loc 1 8 9 . b32 param0 ; st . b32 [ param0 + 0 ], % r1 ; // store 1 . b32 param1 ; st . b32 [ param1 + 0 ], % r2 ; // store 2 . b32 retval0 ; call . uni ( retval0 ), _Z3fooii , ( param0 , param1 ); // call foo ld . b32 % r3 , [ retval0 + 0 ]; // get return value st . u32 [ % rd1 ], % r3 ; // *p = return value . loc 1 9 2 ret ; func_end1 : } . section . debug_info { . b32 262 . b8 2 , 0 . b32 . debug_abbrev . b8 8 , 1 , 108 , 103 , 101 , 110 , 102 , 101 , 58 , 32 , 69 , 68 , 71 , 32 , 52 , 46 , 57 . b8 0 , 4 , 99 , 97 , 108 , 108 , 49 , 46 , 99 , 117 , 0 .'},\n",
       " {'id': 731,\n",
       "  'content': 'b64 0 . debug_line // the .debug_line section will be created by ptxas from the .loc . b8 47 , 104 , 111 , 109 , 101 , 47 , 109 , 109 , 117 , 114 , 112 , 104 , 121 , 47 , 116 .'},\n",
       " {'id': 732,\n",
       "  'content': 'b8 101 , 115 , 116 , 0 , 2 , 95 , 90 , 51 , 102 , 111 , 111 , 105 , 105 , 0 , 95 , 90 . b8 51 , 102 , 111 , 111 , 105 , 105 , 0 . b32 1 , 1 , 164 .'},\n",
       " {'id': 733,\n",
       "  'content': 'b8 1 . b64 func_begin0 // start and end location of foo . b64 func_end0 . b8 1 , 156 , 3 , 105 , 0 . b8 5 , 144 , 177 , 228 , 149 , 1 , 2 , 3 , 106 , 0 . b8 5 , 144 , 178 , 228 , 149 , 1 , 2 , 0 , 4 , 105 , 110 , 116 , 0 , 5 . b32 4 . b8 2 , 95 , 90 , 52 , 116 , 101 , 115 , 116 , 80 , 105 , 0 , 95 , 90 , 52 , 116 , 101 . b8 115 , 116 , 80 , 105 , 0 . b32 1 , 6 , 253 . b64 func_begin1 // start and end location of test . b64 func_end1 . b8 1 , 156 , 3 , 112 , 0 . b32 1 , 6 , 259 . b8 9 , 3 . b64 _Z4testPi_param_0 . b8 7 , 0 , 5 , 118 , 111 , 105 , 100 , 0 , 6 . b32 164 .'},\n",
       " {'id': 734,\n",
       "  'content': 'b8 12 , 0 } . debug_abbrev { . b8 1 , 17 , 1 , 37 , 8 , 19 , 11 , 3 , 8 , 17 , 1 , 16 , 6 , 27 , 8 , 0 , 0 , 2 , 46 , 1 , 135 . b8 64 , 8 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 63 , 12 , 17 , 1 , 18 , 1 , 64 , 10 , 0 , 0 . b8 3 , 5 , 0 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 2 , 10 , 51 , 11 , 0 , 0 , 4 , 36 , 0 , 3 . b8 8 , 62 , 11 , 11 , 6 , 0 , 0 , 5 , 59 , 0 , 3 , 8 , 0 , 0 , 6 , 15 , 0 , 73 , 19 , 51 , 11 . b8 0 , 0 , 0 } . debug_pubnames { . b32 41 . debug_info . b32 262 , 69 . b8 95 , 90 , 51 , 102 , 111 , 111 , 105 , 105 , 0 . b32 174 . b8 95 , 90 , 52 , 116 , 101 , 115 , 116 , 80 , 105 , 0 .'},\n",
       " {'id': 735,\n",
       "  'content': 'b32 0 } 7. C++ \\uf0c1 The C++ implementation for device functions follows the Itanium C++ ABI. However, not everything in C++ is supported. In particular, the following are not supported in device code. Exceptions and try/catch blocks RTTI STL library Global constructors and destructors Virtual functions and classes across host and device (i.e., vtables cannot be used across host and device) There are also a few C features that are not currently supported: stdio other than printf 8. Notices \\uf0c1 8.1.'},\n",
       " {'id': 736,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 737,\n",
       "  'content': '8.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 738,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Using Inline PTX Assembly in CUDA 1.1.'},\n",
       " {'id': 739,\n",
       "  'content': 'Assembler (ASM) Statements 1.1.1. Parameters 1.1.2. Constraints 1.2. Pitfalls 1.2.1. Namespace Conflicts 1.2.2. Memory Space Conflicts 1.2.3. Incorrect Optimization 1.2.4. Incorrect PTX 1.3. Error Checking 2. Notices 2.1.'},\n",
       " {'id': 740,\n",
       "  'content': 'Notice 2.2. OpenCL 2.3. Trademarks Inline PTX Assembly in CUDA » 1. Using Inline PTX Assembly in CUDA v12.5 | PDF | Archive Inline PTX Assembly in CUDA The reference guide for inlining PTX (parallel thread execution) assembly statements into CUDA. Using Inline PTX Assembly in CUDA \\uf0c1 The NVIDIA ® CUDA ® programming environment provides a parallel thread execution (PTX) instruction set architecture (ISA) for using the GPU as a data-parallel computing device. For more information on the PTX ISA, refer to the latest version of the PTX ISA reference document . This application note describes how to inline PTX assembly language statements into CUDA code. 1.1. Assembler (ASM) Statements \\uf0c1 Assembler statements, asm() , provide a way to insert arbitrary PTX code into your CUDA program. A simple example is: asm ( \"membar.gl;\" ); This inserts a PTX membar.gl into your generated PTX code at the point of the asm() statement. 1.1.1. Parameters \\uf0c1 An asm() statement becomes more complicated, and more useful, when we pass values in and out of the asm. The basic syntax is as follows: asm ( \"template-string\" : \"constraint\" ( output ) : \"constraint\" ( input )); where you can have multiple input or output operands separated by commas. The template string contains PTX instructions with references to the operands. Multiple PTX instructions can be given by separating them with semicolons. A simple example is as follows: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); Each %n in the template string is an index into the following list of operands, in text order. So %0 refers to the first operand, %1 to the second operand, and so on. Since the output operands are always listed ahead of the input operands, they are assigned the smallest indices. This example is conceptually equivalent to the following: add . s32 i , j , k ; Note that the numbered references in the string can be in arbitrary order. The following is equivalent to the above example: asm ( \"add.s32 %0, %2, %1;\" : \"=r\" ( i ) : \"r\" ( k ), \"r\" ( j )); You can also repeat a reference, e.g. : asm ( \"add.s32 %0, %1, %1;\" : \"=r\" ( i ) : \"r\" ( k )); is conceptually add . s32 i , k , k ; If there is no input operand, you can drop the final colon, e.g. : asm ( \"mov.s32 %0, 2;\" : \"=r\" ( i )); If there is no output operand, the colon separators are adjacent, e.g. : asm ( \"mov.s32 r1, %0;\" :: \"r\" ( i )); If you want the % in a ptx instruction, then you should escape it with double %% , e.g. : asm ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x )); The above was simplified to explain the ordering of the string % references. In reality, the operand values are passed via whatever mechanism the constraint specifies. The full list of constraints will be explained later, but the “r” constraint refers to a 32bit integer register. So the earlier example asm() statement: asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); produces the following code sequence in the output generated by the compiler: ld . s32 r1 , [ j ]; ld . s32 r2 , [ k ]; add . s32 r3 , r1 , r2 ; st . s32 [ i ], r3 ; This is where the distinction between input and output operands becomes important. The input operands are loaded into registers before the asm() statement, then the result register is stored to the output operand. The “=” modifier in “=r” specifies that the register is written to. There is also available a “+” modifier that specifies the register is both read and written, e.g. : asm ( \"add.s32 %0, %0, %1;\" : \"+r\" ( i ) : \"r\" ( j )); Multiple instructions can be combined into a single asm() statement; basically, anything legal can be put into the asm string. Multiple instructions can be split across multiple lines by making use of C/C++’s implicit string concatenation. Both C++ style line end comments “//” and classical C-style comments “/**/” can be interspersed with these strings. To generate readable output in the PTX intermediate file it is best practice to terminate each instruction string except the last one with “nt”. For example, a cube routine could be written as: __device__ int cube ( int x ) { int y ; asm ( \".reg .u32 t1; \\\\n\\\\t \" // temp reg t1 \" mul.lo.u32 t1, %1, %1; \\\\n\\\\t \" // t1 = x * x \" mul.lo.u32 %0, t1, %1;\" // y = t1 * x : \"=r\" ( y ) : \"r\" ( x )); return y ; } If an output operand is conditionally updated by the asm instructions, then the “+” modifier should be used. There is an implicit use of the output operand in such a case. For example, __device__ int cond ( int x ) { int y = 0 ; asm ( \"{ \\\\n\\\\t \" \" .reg .pred %p; \\\\n\\\\t \" \" setp.eq.s32 %p, %1, 34; \\\\n\\\\t \" // x == 34? \"\\n@%p mov.s32 %0, 1; \\\\n\\\\t \" // set y to 1 if true \"}\" // conceptually y = (x==34)?1:y : \"+r\" ( y ) : \"r\" ( x )); return y ; } 1.1.2. Constraints \\uf0c1 There is a separate constraint letter for each PTX register type: \"h\" = . u16 reg \"r\" = .'},\n",
       " {'id': 741,\n",
       "  'content': 'u32 reg \"l\" = . u64 reg \"f\" = . f32 reg \"d\" = . f64 reg Example: asm ( \"cvt.f32.s64 %0, %1;\" : \"=f\" ( x ) : \"l\" ( y )); generates: ld . s64 rd1 , [ y ]; cvt . f32 .'},\n",
       " {'id': 742,\n",
       "  'content': 's64 f1 , rd1 ; st . f32 [ x ], f1 ; The constraint \"n\" may be used for immediate integer operands with a known value. Example: asm ( \"add.u32 %0, %0, %1;\" : \"=r\" ( x ) : \"n\" ( 42 )); generates: add . u32 r1 , r1 , 42 ; The constraint \"C\" can be used for operand of type ‘array of const char’, where the array contents are known at compile time. It is intended to allow customization of PTX instruction modes based on compile time computation (see examples). Here is the specification for the \"C\" constraint: \\'C\\' ( constant - expression ) The constant-expression is evaluated during compilation and shall generate the address of a variable V , where: V has static storage duration . V has type ‘array of const char’. V is constant-initialized . If V is a static class member, then V ’s initializing declaration is the declaration within the class. During translation, the compiler will replace a reference to the operand within the Assembler Template with the contents of V ’s initializer, except for the last trailing zero. No constraint modifiers are allowed for this constraint. This constraint can only be used in device code. (terms in italics are C++ standard terms and/or terms from the GNU inline asm specification). Here’s an example of the use of C constraint to generate different PTX instruction modes based on compile time computation: constexpr int mode_rz = 0 ; constexpr int mode_rn = 1 ; template struct helper ; template <> struct helper { static constexpr const char mode [] = \".rz\" ; }; template <> struct helper { static constexpr const char mode [] = \".rn\" ; }; template __device__ float compute_add ( float a , float b ) { float result ; asm ( \"add.f32%1 %0,%2,%3;\" : \"=f\" ( result ) : \"C\" ( helper :: mode ), \"f\" ( a ), \"f\" ( b )); return result ; } __global__ void kern ( float * result , float a , float b ) { * result ++ = compute_add ( a , b ); // generates add.f32.rn * result = compute_add ( a , b ); // generates add.f32.rz } Other examples (compile in C++17 or later dialect): struct S1 { static constexpr char buf1 [] = \"Jumped\" ; static constexpr char buf2 [] = { \\'O\\' , \\'v\\' , \\'e\\' , \\'r\\' , 0 }; }; template __device__ void doit () { asm volatile ( \"%0 %1 %2\" : : \"C\" ( p1 ), \"C\" ( p2 ), \"C\" ( p3 )); } struct S2 { static const char buf []; }; const char S2 :: buf [] = \"this\" ; const char buf3 [] = \"Jumped\" ; extern const char buf4 []; __global__ void foo () { static const char v1 [] = \"The\" ; static constexpr char v2 [] = \"Quick\" ; static const char v3 [] = { \\'B\\' , \\'r\\' , \\'o\\' , \\'w\\' , \\'n\\' , 0 }; static constexpr char v4 [] = { \\'F\\' , \\'o\\' , \\'x\\' , 0 }; //OK: generates \\'The Quick Brown Fox Jumped Over\\' in PTX asm volatile ( \"%0 %1 %2 %3 %4 %5\" : : \"C\" ( v1 ) , \"C\" ( v2 ), \"C\" ( v3 ), \"C\" ( v4 ), \"C\" ( S1 :: buf1 ), \"C\" ( S1 :: buf2 ) ); //OK: generates \\'Brown Fox Jumped\\' in PTX doit (); //error cases const char n1 [] = \"hi\" ; //error: argument to \"C\" constraint is not a constant expression asm volatile ( \"%0\" :: \"C\" ( n1 )); //error: S2::buf was not initialized at point of declaration asm volatile ( \"%0\" :: \"C\" ( S2 :: buf )); //error: buf4 was not initialized asm volatile ( \"%0\" :: \"C\" ( buf4 )); } There is no constraint letter for 8-bit wide PTX registers. PTX instructions types accepting 8-bit wide types permit operands to be wider than the instruction-type size . Example: __device__ void copy_u8 ( char * in , char * out ) { int d ; asm ( \"ld.u8 %0, [%1];\" : \"=r\" ( d ) : \"l\" ( in )); * out = d ; } generates: ld . u8 r1 , [ rd1 ]; st . u8 [ rd2 ], r1 ; The behavior of using a constraint string that is not one of those specified above is undefined. 1.2. Pitfalls \\uf0c1 Although asm() statements are very flexible and powerful, you may encounter some pitfalls—these are listed in this section. 1.2.1. Namespace Conflicts \\uf0c1 If the cube function (described before) is called and inlined multiple times in the code, it generates an error about duplicate definitions of the temp register t1. To avoid this error you need to: not inline the cube function, or, nest the t1 use inside {} so that it has a separate scope for each invocation, e.g. : __device__ int cube ( int x ) { int y ; asm ( \"{ \\\\n\\\\t \" // use braces for local scope \" reg .u32 t1; \\\\n\\\\t \" // temp reg t1, \" mul.lo.u32 t1, %1, %1; \\\\n\\\\t \" // t1 = x * x \" mul.lo.u32 %0, t1, %1; \\\\n\\\\t \" // y = t1 * x \"}\" : \"=r\" ( y ) : \"r\" ( x )); return y ; } Note that you can similarly use braces for local labels inside the asm() statement. 1.2.2. Memory Space Conflicts \\uf0c1 Since asm() statements have no way of knowing what memory space a register is in, the user must make sure that the appropriate PTX instruction is used. For sm_20 and greater, any pointer argument to an asm() statement is passed as a generic address. 1.2.3. Incorrect Optimization \\uf0c1 The compiler assumes that an asm() statement has no side effects except to change the output operands. To ensure that the asm is not deleted or moved during generation of PTX, you should use the volatile keyword, e.g. : asm volatile ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x )); Normally any memory that is written to will be specified as an out operand, but if there is a hidden side effect on user memory (for example, indirect access of a memory location via an operand), or if you want to stop any memory optimizations around the asm() statement performed during generation of PTX, you can add a “memory” clobbers specification after a 3rd colon, e.g. : asm volatile ( \"mov.u32 %0, %%clock;\" : \"=r\" ( x ) :: \"memory\" ); asm ( \"st.u32 [%0], %1;\" :: \"l\" ( p ), \"r\" ( x ) : \"memory\" ); 1.2.4. Incorrect PTX \\uf0c1 The compiler front end does not parse the asm() statement template string and does not know what it means or even whether it is valid PTX input. So if there are any errors in the string it will not show up until ptxas . For example, if you pass a value with an “r” constraint but use it in an add.f64 you will get a parse error from ptxas. Similarly, operand modifiers are not supported. For example, in asm ( \"mov.u32 %0, %n1;\" : \"=r\" ( n ) : \"r\" ( 1 )); the ‘n’ modifier in “%n1” is not supported and will be passed to ptxas , where it can cause undefined behavior. Refer to the document nvcc.pdf for further compiler related details. 1.3. Error Checking \\uf0c1 The following are some of the error checks that the compiler will do on inlinePTXasm: Multiple constraint letters for a single asm operand are not allowed, e.g. : asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i ) : \"rf\" ( j ), \"r\" ( k )); error: an asm operand may specify only one constraint letter in a __device__/__global__ function Only scalar variables are allowed as asm operands. Specifically aggregates like ‘struct’ type variables are not allowed, e.g. int4 i4 ; asm ( \"add.s32 %0, %1, %2;\" : \"=r\" ( i4 ) : \"r\" ( j ), \"r\" ( k )); error: an asm operand must have scalar type The type and size implied by a PTX asm constraint must match that of the associated operand. Example where size does not match: For ‘char’ type variable “ci”, asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( ci ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(1) does not match type/size implied by constraint ‘r’ In order to use ‘char’ type variables “ci”, “cj”, and “ck” in the above asm statement, code segment similar to the following may be used, int temp = ci ; asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( temp ) : \"r\" (( int ) cj ), \"r\" (( int ) ck )); ci = temp ; Another example where type does not match: For ‘float’ type variable “fi”, asm ( \"add.s32 %0,%1,%2;\" : \"=r\" ( fi ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(4) does not match type/size implied by constraint ‘r’ 2. Notices \\uf0c1 2.1.'},\n",
       " {'id': 743,\n",
       "  'content': 'Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 2.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 744,\n",
       "  'content': '2.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 745,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 CUDA Runtime API 1. Difference between the driver and runtime APIs 2.'},\n",
       " {'id': 746,\n",
       "  'content': 'API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6. Modules 6.1. Device Management 6.2. Device Management [DEPRECATED] 6.3. Thread Management [DEPRECATED] 6.4. Error Handling 6.5. Stream Management 6.6. Event Management 6.7. External Resource Interoperability 6.8. Execution Control 6.9. Execution Control [DEPRECATED] 6.10. Occupancy 6.11. Memory Management 6.12. Memory Management [DEPRECATED] 6.13. Stream Ordered Memory Allocator 6.14. Unified Addressing 6.15. Peer Device Memory Access 6.16. OpenGL Interoperability 6.17. OpenGL Interoperability [DEPRECATED] 6.18. Direct3D 9 Interoperability 6.19. Direct3D 9 Interoperability [DEPRECATED] 6.20. Direct3D 10 Interoperability 6.21. Direct3D 10 Interoperability [DEPRECATED] 6.22. Direct3D 11 Interoperability 6.23. Direct3D 11 Interoperability [DEPRECATED] 6.24. VDPAU Interoperability 6.25. EGL Interoperability 6.26. Graphics Interoperability 6.27. Texture Object Management 6.28. Surface Object Management 6.29. Version Management 6.30. Graph Management 6.31. Driver Entry Point Access 6.32. C++ API Routines 6.33. Interactions with the CUDA Driver API 6.34. Profiler Control 6.35. Data types used by CUDA Runtime 7. Data Structures 7.1. __cudaOccupancyB2DHelper 7.2. cudaAccessPolicyWindow 7.3. cudaArrayMemoryRequirements 7.4. cudaArraySparseProperties 7.5. cudaAsyncNotificationInfo_t 7.6. cudaChannelFormatDesc 7.7. cudaChildGraphNodeParams 7.8. cudaConditionalNodeParams 7.9. cudaDeviceProp 7.10. cudaEglFrame 7.11. cudaEglPlaneDesc 7.12. cudaEventRecordNodeParams 7.13. cudaEventWaitNodeParams 7.14. cudaExtent 7.15. cudaExternalMemoryBufferDesc 7.16. cudaExternalMemoryHandleDesc 7.17. cudaExternalMemoryMipmappedArrayDesc 7.18. cudaExternalSemaphoreHandleDesc 7.19. cudaExternalSemaphoreSignalNodeParams 7.20. cudaExternalSemaphoreSignalNodeParamsV2 7.21. cudaExternalSemaphoreSignalParams 7.22. cudaExternalSemaphoreSignalParams_v1 7.23. cudaExternalSemaphoreWaitNodeParams 7.24. cudaExternalSemaphoreWaitNodeParamsV2 7.25. cudaExternalSemaphoreWaitParams 7.26. cudaExternalSemaphoreWaitParams_v1 7.27. cudaFuncAttributes 7.28. cudaGraphEdgeData 7.29. cudaGraphExecUpdateResultInfo 7.30. cudaGraphInstantiateParams 7.31. cudaGraphKernelNodeUpdate 7.32. cudaGraphNodeParams 7.33. cudaHostNodeParams 7.34. cudaHostNodeParamsV2 7.35. cudaIpcEventHandle_t 7.36. cudaIpcMemHandle_t 7.37. cudaKernelNodeParams 7.38. cudaKernelNodeParamsV2 7.39. cudaLaunchAttribute 7.40. cudaLaunchAttributeValue 7.41. cudaLaunchConfig_t 7.42. cudaLaunchMemSyncDomainMap 7.43. cudaLaunchParams 7.44. cudaMemAccessDesc 7.45. cudaMemAllocNodeParams 7.46. cudaMemAllocNodeParamsV2 7.47. cudaMemcpy3DParms 7.48. cudaMemcpy3DPeerParms 7.49. cudaMemcpyNodeParams 7.50. cudaMemFreeNodeParams 7.51. cudaMemLocation 7.52. cudaMemPoolProps 7.53. cudaMemPoolPtrExportData 7.54. cudaMemsetParams 7.55. cudaMemsetParamsV2 7.56. cudaPitchedPtr 7.57. cudaPointerAttributes 7.58. cudaPos 7.59. cudaResourceDesc 7.60. cudaResourceViewDesc 7.61. cudaTextureDesc 7.62. CUuuid_st 8. Data Fields 9. Deprecated List Search Results CUDA Runtime API ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback Table of Contents 1. Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 CUDA Driver API 1. Difference between the driver and runtime APIs 2.'},\n",
       " {'id': 747,\n",
       "  'content': 'API synchronization behavior 3. Stream synchronization behavior 4. Graph object thread safety 5. Rules for version mixing 6.'},\n",
       " {'id': 748,\n",
       "  'content': 'Modules 6.1. Data types used by CUDA driver 6.2. Error Handling 6.3.'},\n",
       " {'id': 749,\n",
       "  'content': 'Initialization 6.4. Version Management 6.5. Device Management 6.6. Device Management [DEPRECATED] 6.7. Primary Context Management 6.8. Context Management 6.9. Context Management [DEPRECATED] 6.10. Module Management 6.11. Module Management [DEPRECATED] 6.12. Library Management 6.13. Memory Management 6.14. Virtual Memory Management 6.15. Stream Ordered Memory Allocator 6.16. Multicast Object Management 6.17. Unified Addressing 6.18. Stream Management 6.19. Event Management 6.20. External Resource Interoperability 6.21. Stream Memory Operations 6.22. Execution Control 6.23.'},\n",
       " {'id': 750,\n",
       "  'content': 'Execution Control [DEPRECATED] 6.24. Graph Management 6.25. Occupancy 6.26. Texture Reference Management [DEPRECATED] 6.27. Surface Reference Management [DEPRECATED] 6.28. Texture Object Management 6.29. Surface Object Management 6.30. Tensor Map Object Managment 6.31. Peer Context Memory Access 6.32. Graphics Interoperability 6.33. Driver Entry Point Access 6.34. Coredump Attributes Control API 6.35. Green Contexts 6.36. Profiler Control [DEPRECATED] 6.37. Profiler Control 6.38. OpenGL Interoperability 6.38.1. OpenGL Interoperability [DEPRECATED] 6.39. Direct3D 9 Interoperability 6.39.1. Direct3D 9 Interoperability [DEPRECATED] 6.40. Direct3D 10 Interoperability 6.40.1. Direct3D 10 Interoperability [DEPRECATED] 6.41. Direct3D 11 Interoperability 6.41.1. Direct3D 11 Interoperability [DEPRECATED] 6.42. VDPAU Interoperability 6.43. EGL Interoperability 7. Data Structures 7.1. CUaccessPolicyWindow_v1 7.2. CUarrayMapInfo_v1 7.3. CUasyncNotificationInfo 7.4. CUctxCigParam 7.5. CUctxCreateParams 7.6. CUDA_ARRAY3D_DESCRIPTOR_v2 7.7. CUDA_ARRAY_DESCRIPTOR_v2 7.8. CUDA_ARRAY_MEMORY_REQUIREMENTS_v1 7.9. CUDA_ARRAY_SPARSE_PROPERTIES_v1 7.10. 7.11. CUDA_CHILD_GRAPH_NODE_PARAMS 7.12. CUDA_CONDITIONAL_NODE_PARAMS 7.13. CUDA_EVENT_RECORD_NODE_PARAMS 7.14. CUDA_EVENT_WAIT_NODE_PARAMS 7.15. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v1 7.16. CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v2 7.17. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v1 7.18. CUDA_EXT_SEM_WAIT_NODE_PARAMS_v2 7.19. CUDA_EXTERNAL_MEMORY_BUFFER_DESC_v1 7.20. CUDA_EXTERNAL_MEMORY_HANDLE_DESC_v1 7.21. CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_v1 7.22. CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_v1 7.23. CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS_v1 7.24. CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS_v1 7.25. CUDA_GRAPH_INSTANTIATE_PARAMS 7.26. CUDA_HOST_NODE_PARAMS_v1 7.27. CUDA_HOST_NODE_PARAMS_v2 7.28. CUDA_KERNEL_NODE_PARAMS_v1 7.29. CUDA_KERNEL_NODE_PARAMS_v2 7.30. CUDA_KERNEL_NODE_PARAMS_v3 7.31. CUDA_LAUNCH_PARAMS_v1 7.32. CUDA_MEM_ALLOC_NODE_PARAMS_v1 7.33. CUDA_MEM_ALLOC_NODE_PARAMS_v2 7.34. CUDA_MEM_FREE_NODE_PARAMS 7.35. CUDA_MEMCPY2D_v2 7.36. CUDA_MEMCPY3D_PEER_v1 7.37. CUDA_MEMCPY3D_v2 7.38. CUDA_MEMCPY_NODE_PARAMS 7.39. CUDA_MEMSET_NODE_PARAMS_v1 7.40. CUDA_MEMSET_NODE_PARAMS_v2 7.41. CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_v1 7.42. CUDA_RESOURCE_DESC_v1 7.43. CUDA_RESOURCE_VIEW_DESC_v1 7.44. CUDA_TEXTURE_DESC_v1 7.45. CUdevprop_v1 7.46. CUdevResource 7.47. CUdevSmResource 7.48. CUeglFrame_v1 7.49. CUexecAffinityParam_v1 7.50. CUexecAffinitySmCount_v1 7.51. CUgraphEdgeData 7.52. CUgraphExecUpdateResultInfo_v1 7.53. CUgraphNodeParams 7.54. CUipcEventHandle_v1 7.55. CUipcMemHandle_v1 7.56. CUlaunchAttribute 7.57. CUlaunchAttributeValue 7.58. CUlaunchConfig 7.59. CUlaunchMemSyncDomainMap 7.60. CUmemAccessDesc_v1 7.61. CUmemAllocationProp_v1 7.62. CUmemFabricHandle_v1 7.63. CUmemLocation_v1 7.64. CUmemPoolProps_v1 7.65. CUmemPoolPtrExportData_v1 7.66. CUmulticastObjectProp_v1 7.67. CUstreamBatchMemOpParams_v1 7.68. CUtensorMap 8. Data Fields 9. Deprecated List Search Results CUDA Driver API ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback Table of Contents 1. Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();1. FP8 Intrinsics 2.'},\n",
       " {'id': 751,\n",
       "  'content': 'Half Precision Intrinsics 3. Bfloat16 Precision Intrinsics 4. Single Precision Mathematical Functions 5. Single Precision Intrinsics 6. Double Precision Mathematical Functions 7. Double Precision Intrinsics 8. Type Casting Intrinsics 9. Integer Mathematical Functions 10. Integer Intrinsics 11. SIMD Intrinsics 12. Structs 13. Notices CUDA Math API Reference Manual » CUDA Math API Reference Manual v12.5 | PDF | Archive CUDA Math API Reference Manual \\uf0c1 CUDA mathematical functions are always available in device code. Host implementations of the common mathematical functions are mapped in a platform-specific way to standard math library functions, provided by the host compiler and respective host libm where available. Some functions, not available with the host compilers, are implemented in crt/math_functions.hpp header file. For example, see erfinv() . Other, less common functions, like rhypot() , cyl_bessel_i0() are only available in device code. CUDA Math device functions are no-throw for well-formed CUDA programs. Note that many floating-point and integer functions names are overloaded for different argument types. For example, the log() function has the following prototypes: double log ( double x ); float log ( float x ); float logf ( float x ); Note also that due to implementation constraints, certain math functions from std:: namespace may be callable in device code even via explicitly qualified std:: names. However, such use is discouraged, since this capability is unsupported, unverified, undocumented, not portable, and may change without notice.'},\n",
       " {'id': 752,\n",
       "  'content': 'Notices Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 753,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); });1. Introduction 1.1.'},\n",
       " {'id': 754,\n",
       "  'content': 'Data Layout 1.2. New and Legacy cuBLAS API 1.3. Example Code 2. Using the cuBLAS API 2.1. General Description 2.1.1. Error Status 2.1.2. cuBLAS Context 2.1.3. Thread Safety 2.1.4. Results Reproducibility 2.1.5. Scalar Parameters 2.1.6. Parallelism with Streams 2.1.7. Batching Kernels 2.1.8. Cache Configuration 2.1.9.'},\n",
       " {'id': 755,\n",
       "  'content': 'Static Library Support 2.1.10. GEMM Algorithms Numerical Behavior 2.1.11. Tensor Core Usage 2.1.12. CUDA Graphs Support 2.1.13. 64-bit Integer Interface 2.2. cuBLAS Datatypes Reference 2.2.1. cublasHandle_t 2.2.2. cublasStatus_t 2.2.3. cublasOperation_t 2.2.4. cublasFillMode_t 2.2.5. cublasDiagType_t 2.2.6. cublasSideMode_t 2.2.7. cublasPointerMode_t 2.2.8. cublasAtomicsMode_t 2.2.9. cublasGemmAlgo_t 2.2.10. cublasMath_t 2.2.11. cublasComputeType_t 2.3. CUDA Datatypes Reference 2.3.1. cudaDataType_t 2.3.2. libraryPropertyType_t 2.4. cuBLAS Helper Function Reference 2.4.1. cublasCreate() 2.4.2. cublasDestroy() 2.4.3. cublasGetVersion() 2.4.4. cublasGetProperty() 2.4.5. cublasGetStatusName() 2.4.6. cublasGetStatusString() 2.4.7. cublasSetStream() 2.4.8. cublasSetWorkspace() 2.4.9. cublasGetStream() 2.4.10. cublasGetPointerMode() 2.4.11. cublasSetPointerMode() 2.4.12. cublasSetVector() 2.4.13. cublasGetVector() 2.4.14. cublasSetMatrix() 2.4.15. cublasGetMatrix() 2.4.16. cublasSetVectorAsync() 2.4.17. cublasGetVectorAsync() 2.4.18. cublasSetMatrixAsync() 2.4.19. cublasGetMatrixAsync() 2.4.20. cublasSetAtomicsMode() 2.4.21. cublasGetAtomicsMode() 2.4.22. cublasSetMathMode() 2.4.23. cublasGetMathMode() 2.4.24. cublasSetSmCountTarget() 2.4.25. cublasGetSmCountTarget() 2.4.26. cublasLoggerConfigure() 2.4.27. cublasGetLoggerCallback() 2.4.28. cublasSetLoggerCallback() 2.5. cuBLAS Level-1 Function Reference 2.5.1. cublasIamax() 2.5.2. cublasIamin() 2.5.3. cublasasum() 2.5.4. cublasaxpy() 2.5.5. cublascopy() 2.5.6. cublasdot() 2.5.7. cublasnrm2() 2.5.8. cublasrot() 2.5.9. cublasrotg() 2.5.10. cublasrotm() 2.5.11. cublasrotmg() 2.5.12. cublasscal() 2.5.13. cublasswap() 2.6. cuBLAS Level-2 Function Reference 2.6.1. cublasgbmv() 2.6.2. cublasgemv() 2.6.3. cublasger() 2.6.4. cublassbmv() 2.6.5. cublasspmv() 2.6.6. cublasspr() 2.6.7. cublasspr2() 2.6.8. cublassymv() 2.6.9. cublassyr() 2.6.10. cublassyr2() 2.6.11. cublastbmv() 2.6.12. cublastbsv() 2.6.13. cublastpmv() 2.6.14. cublastpsv() 2.6.15. cublastrmv() 2.6.16. cublastrsv() 2.6.17. cublashemv() 2.6.18. cublashbmv() 2.6.19. cublashpmv() 2.6.20. cublasher() 2.6.21. cublasher2() 2.6.22. cublashpr() 2.6.23. cublashpr2() 2.6.24. cublasgemvBatched() 2.6.25. cublasgemvStridedBatched() 2.7. cuBLAS Level-3 Function Reference 2.7.1. cublasgemm() 2.7.2. cublasgemm3m() 2.7.3. cublasgemmBatched() 2.7.4. cublasgemmStridedBatched() 2.7.5. cublasgemmGroupedBatched() 2.7.6. cublassymm() 2.7.7. cublassyrk() 2.7.8. cublassyr2k() 2.7.9. cublassyrkx() 2.7.10. cublastrmm() 2.7.11. cublastrsm() 2.7.12. cublastrsmBatched() 2.7.13. cublashemm() 2.7.14. cublasherk() 2.7.15. cublasher2k() 2.7.16. cublasherkx() 2.8. BLAS-like Extension 2.8.1. cublasgeam() 2.8.2. cublasdgmm() 2.8.3. cublasgetrfBatched() 2.8.4. cublasgetrsBatched() 2.8.5. cublasgetriBatched() 2.8.6. cublasmatinvBatched() 2.8.7. cublasgeqrfBatched() 2.8.8. cublasgelsBatched() 2.8.9. cublastpttr() 2.8.10. cublastrttp() 2.8.11. cublasgemmEx() 2.8.12. cublasGemmEx() 2.8.13. cublasGemmBatchedEx() 2.8.14. cublasGemmStridedBatchedEx() 2.8.15. cublasGemmGroupedBatchedEx() 2.8.16. cublasCsyrkEx() 2.8.17. cublasCsyrk3mEx() 2.8.18. cublasCherkEx() 2.8.19. cublasCherk3mEx() 2.8.20. cublasNrm2Ex() 2.8.21. cublasAxpyEx() 2.8.22. cublasDotEx() 2.8.23. cublasRotEx() 2.8.24. cublasScalEx() 3. Using the cuBLASLt API 3.1. General Description 3.1.1.'},\n",
       " {'id': 756,\n",
       "  'content': 'Problem Size Limitations 3.1.2. Heuristics Cache 3.1.3. cuBLASLt Logging 3.1.4. 8-bit Floating Point Data Types (FP8) Usage 3.1.5. Disabling CPU Instructions 3.1.6. Atomics Synchronization 3.2. cuBLASLt Code Examples 3.3. cuBLASLt Datatypes Reference 3.3.1. cublasLtClusterShape_t 3.3.2. cublasLtEpilogue_t 3.3.3. cublasLtHandle_t 3.3.4. cublasLtLoggerCallback_t 3.3.5. cublasLtMatmulAlgo_t 3.3.6. cublasLtMatmulAlgoCapAttributes_t 3.3.7. cublasLtMatmulAlgoConfigAttributes_t 3.3.8. cublasLtMatmulDesc_t 3.3.9. cublasLtMatmulDescAttributes_t 3.3.10. cublasLtMatmulHeuristicResult_t 3.3.11. cublasLtMatmulInnerShape_t 3.3.12. cublasLtMatmulPreference_t 3.3.13. cublasLtMatmulPreferenceAttributes_t 3.3.14. cublasLtMatmulSearch_t 3.3.15. cublasLtMatmulTile_t 3.3.16. cublasLtMatmulStages_t 3.3.17. cublasLtNumericalImplFlags_t 3.3.18. cublasLtMatrixLayout_t 3.3.19. cublasLtMatrixLayoutAttribute_t 3.3.20. cublasLtMatrixTransformDesc_t 3.3.21. cublasLtMatrixTransformDescAttributes_t 3.3.22. cublasLtOrder_t 3.3.23. cublasLtPointerMode_t 3.3.24. cublasLtPointerModeMask_t 3.3.25. cublasLtReductionScheme_t 3.4. cuBLASLt API Reference 3.4.1. cublasLtCreate() 3.4.2. cublasLtDestroy() 3.4.3. cublasLtDisableCpuInstructionsSetMask() 3.4.4. cublasLtGetCudartVersion() 3.4.5. cublasLtGetProperty() 3.4.6. cublasLtGetStatusName() 3.4.7. cublasLtGetStatusString() 3.4.8. cublasLtHeuristicsCacheGetCapacity() 3.4.9. cublasLtHeuristicsCacheSetCapacity() 3.4.10. cublasLtGetVersion() 3.4.11. cublasLtLoggerSetCallback() 3.4.12. cublasLtLoggerSetFile() 3.4.13. cublasLtLoggerOpenFile() 3.4.14. cublasLtLoggerSetLevel() 3.4.15. cublasLtLoggerSetMask() 3.4.16. cublasLtLoggerForceDisable() 3.4.17. cublasLtMatmul() 3.4.18. cublasLtMatmulAlgoCapGetAttribute() 3.4.19. cublasLtMatmulAlgoCheck() 3.4.20. cublasLtMatmulAlgoConfigGetAttribute() 3.4.21. cublasLtMatmulAlgoConfigSetAttribute() 3.4.22. cublasLtMatmulAlgoGetHeuristic() 3.4.23. cublasLtMatmulAlgoGetIds() 3.4.24. cublasLtMatmulAlgoInit() 3.4.25. cublasLtMatmulDescCreate() 3.4.26. cublasLtMatmulDescInit() 3.4.27. cublasLtMatmulDescDestroy() 3.4.28. cublasLtMatmulDescGetAttribute() 3.4.29. cublasLtMatmulDescSetAttribute() 3.4.30. cublasLtMatmulPreferenceCreate() 3.4.31. cublasLtMatmulPreferenceInit() 3.4.32. cublasLtMatmulPreferenceDestroy() 3.4.33. cublasLtMatmulPreferenceGetAttribute() 3.4.34. cublasLtMatmulPreferenceSetAttribute() 3.4.35. cublasLtMatrixLayoutCreate() 3.4.36. cublasLtMatrixLayoutInit() 3.4.37. cublasLtMatrixLayoutDestroy() 3.4.38. cublasLtMatrixLayoutGetAttribute() 3.4.39. cublasLtMatrixLayoutSetAttribute() 3.4.40. cublasLtMatrixTransform() 3.4.41. cublasLtMatrixTransformDescCreate() 3.4.42. cublasLtMatrixTransformDescInit() 3.4.43. cublasLtMatrixTransformDescDestroy() 3.4.44. cublasLtMatrixTransformDescGetAttribute() 3.4.45. cublasLtMatrixTransformDescSetAttribute() 4. Using the cuBLASXt API 4.1. General description 4.1.1. Tiling design approach 4.1.2. Hybrid CPU-GPU computation 4.1.3. Results reproducibility 4.2. cuBLASXt API Datatypes Reference 4.2.1. cublasXtHandle_t 4.2.2. cublasXtOpType_t 4.2.3. cublasXtBlasOp_t 4.2.4. cublasXtPinningMemMode_t 4.3. cuBLASXt API Helper Function Reference 4.3.1. cublasXtCreate() 4.3.2. cublasXtDestroy() 4.3.3. cublasXtDeviceSelect() 4.3.4. cublasXtSetBlockDim() 4.3.5. cublasXtGetBlockDim() 4.3.6. cublasXtSetCpuRoutine() 4.3.7. cublasXtSetCpuRatio() 4.3.8. cublasXtSetPinningMemMode() 4.3.9. cublasXtGetPinningMemMode() 4.4. cuBLASXt API Math Functions Reference 4.4.1. cublasXtgemm() 4.4.2. cublasXthemm() 4.4.3. cublasXtsymm() 4.4.4. cublasXtsyrk() 4.4.5. cublasXtsyr2k() 4.4.6. cublasXtsyrkx() 4.4.7. cublasXtherk() 4.4.8. cublasXther2k() 4.4.9. cublasXtherkx() 4.4.10. cublasXttrsm() 4.4.11. cublasXttrmm() 4.4.12. cublasXtspmm() 5. Using the cuBLASDx API 6. Using the cuBLAS Legacy API 6.1. Error Status 6.2. Initialization and Shutdown 6.3. Thread Safety 6.4. Memory Management 6.5. Scalar Parameters 6.6. Helper Functions 6.7.'},\n",
       " {'id': 757,\n",
       "  'content': 'Level-1,2,3 Functions 6.8. Converting Legacy to the cuBLAS API 6.9. Examples 7. cuBLAS Fortran Bindings 8. Interaction with Other Libraries and Tools 8.1. nvprune 9.'},\n",
       " {'id': 758,\n",
       "  'content': 'Acknowledgements 10. Notices 10.1. Notice 10.2. OpenCL 10.3.'},\n",
       " {'id': 759,\n",
       "  'content': 'Trademarks cuBLAS » 1. Introduction v12.5 | PDF | Archive cuBLAS The API Reference guide for cuBLAS, the CUDA Basic Linear Algebra Subroutine library. Introduction \\uf0c1 The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA®CUDA™ runtime. It allows the user to access the computational resources of NVIDIA Graphics Processing Unit (GPU). The cuBLAS Library exposes four sets of APIs: The cuBLAS API , which is simply called cuBLAS API in this document (starting with CUDA 6.0), The cuBLASXt API (starting with CUDA 6.0), and The cuBLASLt API (starting with CUDA 10.1) The cuBLASDx API (not shipped with the CUDA Toolkit) To use the cuBLAS API, the application must allocate the required matrices and vectors in the GPU memory space, fill them with data, call the sequence of desired cuBLAS functions, and then upload the results from the GPU memory space back to the host. The cuBLAS API also provides helper functions for writing and retrieving data from the GPU. To use the cuBLASXt API, the application may have the data on the Host or any of the devices involved in the computation, and the Library will take care of dispatching the operation to, and transferring the data to, one or multiple GPUs present in the system, depending on the user request. The cuBLASLt is a lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API. This library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. After a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs. This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data.'},\n",
       " {'id': 760,\n",
       "  'content': '1.1. Data Layout \\uf0c1 For maximum compatibility with existing Fortran environments, the cuBLAS library uses column-major storage, and 1-based indexing. Since C and C++ use row-major storage, applications written in these languages can not use the native array semantics for two-dimensional arrays. Instead, macros or inline functions should be defined to implement matrices on top of one-dimensional arrays. For Fortran code ported to C in mechanical fashion, one may chose to retain 1-based indexing to avoid the need to transform loops. In this case, the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) Here, ld refers to the leading dimension of the matrix, which in the case of column-major storage is the number of rows of the allocated matrix (even if only a submatrix of it is being used). For natively written C and C++ code, one would most likely choose 0-based indexing, in which case the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2C(i,j,ld) (((j)*(ld))+(i)) 1.2. New and Legacy cuBLAS API \\uf0c1 Starting with version 4.0, the cuBLAS Library provides a new API, in addition to the existing legacy API. This section discusses why a new API is provided, the advantages of using it, and the differences with the existing legacy API. Warning The legacy cuBLAS API is deprecated and will be removed in future release. The new cuBLAS library API can be used by including the header file cublas_v2.h . It has the following features that the legacy cuBLAS API does not have: The handle to the cuBLAS library context is initialized using the function and is explicitly passed to every subsequent library function call. This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs. This also allows the cuBLAS APIs to be reentrant. The scalars \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host. This change allows library functions to execute asynchronously using streams even when \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are generated by a previous kernel. When a library routine returns a scalar result, it can be returned by reference on the host or the device, instead of only being allowed to be returned by value only on the host. This change allows library routines to be called asynchronously when the scalar result is generated and returned by reference on the device resulting in maximum parallelism. The error status cublasStatus_t is returned by all cuBLAS library function calls. This change facilitates debugging and simplifies software development. Note that cublasStatus was renamed cublasStatus_t to be more consistent with other types in the cuBLAS library. The cublasAlloc() and cublasFree() functions have been deprecated. This change removes these unnecessary wrappers around cudaMalloc() and cudaFree() , respectively. The function cublasSetKernelStream() was renamed cublasSetStream() to be more consistent with the other CUDA libraries. The legacy cuBLAS API, explained in more detail in Using the cuBLAS Legacy API , can be used by including the header file cublas.h . Since the legacy API is identical to the previously released cuBLAS library API, existing applications will work out of the box and automatically use this legacy API without any source code changes. The current and the legacy cuBLAS APIs cannot be used simultaneously in a single translation unit: including both cublas.h and cublas_v2.h header files will lead to compilation errors due to incompatible symbol redeclarations. In general, new applications should not use the legacy cuBLAS API, and existing applications should convert to using the new API if it requires sophisticated and optimal stream parallelism, or if it calls cuBLAS routines concurrently from multiple threads. For the rest of the document, the new cuBLAS Library API will simply be referred to as the cuBLAS Library API. As mentioned earlier the interfaces to the legacy and the cuBLAS library APIs are the header file cublas.h and cublas_v2.h , respectively. In addition, applications using the cuBLAS library need to link against: The DSO cublas.so for Linux, The DLL cublas.dll for Windows, or The dynamic library cublas.dylib for Mac OS X. Note The same dynamic library implements both the new and legacy cuBLAS APIs. 1.3. Example Code \\uf0c1 For sample code references please see the two examples below. They show an application written in C using the cuBLAS library API with two indexing styles (Example 1. “Application Using C and cuBLAS: 1-based indexing” and Example 2. “Application Using C and cuBLAS: 0-based Indexing”). //Example 1. Application Using C and cuBLAS: 1-based indexing //----------------------------------------------------------- #include #include #include #include #include \"cublas_v2.h\" #define M 6 #define N 5 #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) static __inline__ void modify ( cublasHandle_t handle , float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( handle , n - q + 1 , & alpha , & m [ IDX2F ( p , q , ldm )], ldm ); cublasSscal ( handle , ldm - p + 1 , & beta , & m [ IDX2F ( p , q , ldm )], 1 ); } int main ( void ){ cudaError_t cudaStat ; cublasStatus_t stat ; cublasHandle_t handle ; int i , j ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 1 ; j #include #include #include #include \"cublas_v2.h\" #define M 6 #define N 5 #define IDX2C(i,j,ld) (((j)*(ld))+(i)) static __inline__ void modify ( cublasHandle_t handle , float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( handle , n - q , & alpha , & m [ IDX2C ( p , q , ldm )], ldm ); cublasSscal ( handle , ldm - p , & beta , & m [ IDX2C ( p , q , ldm )], 1 ); } int main ( void ){ cudaError_t cudaStat ; cublasStatus_t stat ; cublasHandle_t handle ; int i , j ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 0 ; j symv and cublashemv , an alternate significantly faster routine can be chosen using the routine cublasSetAtomicsMode() . In that case, the results are not guaranteed to be bit-wise reproducible because atomics are used for the computation.'},\n",
       " {'id': 761,\n",
       "  'content': '2.1.5. Scalar Parameters \\uf0c1 There are two categories of the functions that use scalar parameters : Functions that take alpha and/or beta parameters by reference on the host or the device as scaling factors, such as gemm . Functions that return a scalar result on the host or the device such as amax() , amin , asum() , rotg() , rotmg() , dot() and nrm2() . For the functions of the first category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , the scalar parameters alpha and/or beta can be on the stack or allocated on the heap, shouldn’t be placed in managed memory. Underneath, the CUDA kernels related to those functions will be launched with the value of alpha and/or beta . Therefore if they were allocated on the heap, they can be freed just after the return of the call even though the kernel launch is asynchronous. When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , alpha and/or beta must be accessible on the device and their values should not be modified until the kernel is done. Note that since cudaFree() does an implicit cudaDeviceSynchronize() , cudaFree() can still be called on alpha and/or beta just after the call but it would defeat the purpose of using this pointer mode in that case. For the functions of the second category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , these functions block the CPU, until the GPU has completed its computation and the results have been copied back to the Host. When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , these functions return immediately. In this case, similar to matrix and vector results, the scalar result is ready only when execution of the routine on the GPU has completed. This requires proper synchronization in order to read the result from the host. In either case, the pointer mode CUBLAS_POINTER_MODE_DEVICE allows the library functions to execute completely asynchronously from the Host even when alpha and/or beta are generated by a previous kernel. For example, this situation can arise when iterative methods for solution of linear systems and eigenvalue problems are implemented using the cuBLAS library.'},\n",
       " {'id': 762,\n",
       "  'content': '2.1.6. Parallelism with Streams \\uf0c1 If the application uses the results computed by multiple independent tasks, CUDA™ streams can be used to overlap the computation performed in these tasks. The application can conceptually associate each stream with each task. In order to achieve the overlap of computation between the tasks, the user should create CUDA™ streams using the function cudaStreamCreate() and set the stream to be used by each individual cuBLAS library routine by calling cublasSetStream() just before calling the actual cuBLAS routine. Note that cublasSetStream() resets the user-provided workspace to the default workspace pool; see cublasSetWorkspace() . Then, the computation performed in separate streams would be overlapped automatically when possible on the GPU. This approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the GPU with work. We recommend using the new cuBLAS API with scalar parameters and results passed by reference in the device memory to achieve maximum overlap of the computation when using streams. A particular application of streams, batching of multiple small kernels, is described in the following section. 2.1.7. Batching Kernels \\uf0c1 In this section, we explain how to use streams to batch the execution of small kernels. For instance, suppose that we have an application where we need to make many small independent matrix-matrix multiplications with dense matrices. It is clear that even with millions of small independent matrices we will not be able to achieve the same GFLOPS rate as with a one large matrix. For example, a single \\\\(n \\\\times n\\\\) large matrix-matrix multiplication performs \\\\(n^{3}\\\\) operations for \\\\(n^{2}\\\\) input size, while 1024 \\\\(\\\\frac{n}{32} \\\\times \\\\frac{n}{32}\\\\) small matrix-matrix multiplications perform \\\\(1024\\\\left( \\\\frac{n}{32} \\\\right)^{3} = \\\\frac{n^{3}}{32}\\\\) operations for the same input size. However, it is also clear that we can achieve a significantly better performance with many small independent matrices compared with a single small matrix. The architecture family of GPUs allows us to execute multiple kernels simultaneously. Hence, in order to batch the execution of independent kernels, we can run each of them in a separate stream. In particular, in the above example we could create 1024 CUDA™ streams using the function cudaStreamCreate() , then preface each call to cublasgemm() with a call to cublasSetStream() with a different stream for each of the matrix-matrix multiplications (note that cublasSetStream() resets user-provided workspace to the default workspace pool, see cublasSetWorkspace() ). This will ensure that when possible the different computations will be executed concurrently. Although the user can create many streams, in practice it is not possible to have more than 32 concurrent kernels executing at the same time. 2.1.8. Cache Configuration \\uf0c1 On some devices, L1 cache and shared memory use the same hardware resources. The cache configuration can be set directly with the CUDA Runtime function cudaDeviceSetCacheConfig. The cache configuration can also be set specifically for some functions using the routine cudaFuncSetCacheConfig. Please refer to the CUDA Runtime API documentation for details about the cache configuration settings. Because switching from one configuration to another can affect kernels concurrency, the cuBLAS Library does not set any cache configuration preference and relies on the current setting. However, some cuBLAS routines, especially Level-3 routines, rely heavily on shared memory. Thus the cache preference setting might affect adversely their performance. 2.1.9. Static Library Support \\uf0c1 The cuBLAS Library is also delivered in a static form as libcublas_static.a on Linux. The static cuBLAS library and all other static math libraries depend on a common thread abstraction layer library called libculibos.a . For example, on Linux, to compile a small application using cuBLAS, against the dynamic library, the following command can be used: nvcc myCublasApp . c - lcublas - o myCublasApp Whereas to compile against the static cuBLAS library, the following command must be used: nvcc myCublasApp . c - lcublas_static - lculibos - o myCublasApp It is also possible to use the native Host C++ compiler. Depending on the Host operating system, some additional libraries like pthread or dl might be needed on the linking line. The following command on Linux is suggested : g ++ myCublasApp . c - lcublas_static - lculibos - lcudart_static - lpthread - ldl - I / include - L / lib64 - o myCublasApp Note that in the latter case, the library cuda is not needed. The CUDA Runtime will try to open explicitly the cuda library if needed. In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available. Starting with release 11.2, using the typed functions instead of the extension functions (cublas**Ex()) helps in reducing the binary size when linking to static cuBLAS Library. 2.1.10. GEMM Algorithms Numerical Behavior \\uf0c1 Some GEMM algorithms split the computation along the dimension K to increase the GPU occupancy, especially when the dimension K is large compared to dimensions M and N. When this type of algorithm is chosen by the cuBLAS heuristics or explicitly by the user, the results of each split is summed deterministically into the resulting matrix to get the final result. For the routines cublasgemmEx and cublasGemmEx() , when the compute type is greater than the output type, the sum of the split chunks can potentially lead to some intermediate overflows thus producing a final resulting matrix with some overflows. Those overflows might not have occurred if all the dot products had been accumulated in the compute type before being converted at the end in the output type. This computation side-effect can be easily exposed when the computeType is CUDA_R_32F and Atype, Btype and Ctype are in CUDA_R_16F. This behavior can be controlled using the compute precision mode CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION with cublasSetMathMode() 2.1.11. Tensor Core Usage \\uf0c1 Tensor cores were first introduced with Volta GPUs (compute capability 7.0 and above) and significantly accelerate matrix multiplications. Starting with cuBLAS version 11.0.0, the library may automatically make use of Tensor Core capabilities wherever possible, unless they are explicitly disabled by selecting pedantic compute modes in cuBLAS (see cublasSetMathMode() , cublasMath_t ). It should be noted that the library will pick a Tensor Core enabled implementation wherever it determines that it would provide the best performance. The best performance when using Tensor Cores can be achieved when the matrix dimensions and pointers meet certain memory alignment requirements. Specifically, all of the following conditions must be satisfied to get the most performance out of Tensor Cores: ((op_A == CUBLAS_OP_N ? m : k) * AtypeSize) % 16 == 0 ((op_B == CUBLAS_OP_N ? k : n) * BtypeSize) % 16 == 0 (m * CtypeSize) % 16 == 0 (lda * AtypeSize) % 16 == 0 (ldb * BtypeSize) % 16 == 0 (ldc * CtypeSize) % 16 == 0 intptr_t(A) % 16 == 0 intptr_t(B) % 16 == 0 intptr_t(C) % 16 == 0 To conduct matrix multiplication with FP8 types (see 8-bit Floating Point Data Types (FP8) Usage ), you must ensure that your matrix dimensions and pointers meet the optimal requirements listed above. Aside from FP8, there are no longer any restrictions on matrix dimensions and memory alignments to use Tensor Cores (starting with cuBLAS version 11.0.0). 2.1.12. CUDA Graphs Support \\uf0c1 cuBLAS routines can be captured in CUDA Graph stream capture without restrictions in most situations. The exception are routines that output results into host buffers (e.g. cublasdot while pointer mode CUBLAS_POINTER_MODE_HOST is configured), as it enforces synchronization. For input coefficients (such as alpha , beta ) behavior depends on the pointer mode setting: In the case of CUBLAS(LT)_POINTER_MODE_HOST , coefficient values are captured in the graph. In the case of pointer modes with device pointers, coefficient value is accessed using the device pointer at the time of graph execution. Note When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync . However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail. To avoid this issue, use the cublasSetWorkspace() function to provide user-owned workspace memory. 2.1.13. 64-bit Integer Interface \\uf0c1 cuBLAS version 12 introduced 64-bit integer capable functions. Each 64-bit integer function is equivalent to a 32-bit integer function with the following changes: The function name has _64 suffix. The dimension (problem size) data type changed from int to int64_t . Examples of dimension: m , n , and k . The leading dimension data type changed from int to int64_t . Examples of leading dimension: lda , ldb , and ldc . The vector increment data type changed from int to int64_t . Examples of vector increment: incx and incy . For example, consider the following 32-bit integer functions: cublasStatus_t cublasSetMatrix ( int rows , int cols , int elemSize , const void * A , int lda , void * B , int ldb ); cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ); cublasStatus_t cublasSsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * A , int lda ); The equivalent 64-bit integer functions are: cublasStatus_t cublasSetMatrix_64 ( int64_t rows , int64_t cols , int64_t elemSize , const void * A , int64_t lda , void * B , int64_t ldb ); cublasStatus_t cublasIsamax_64 ( cublasHandle_t handle , int64_t n , const float * x , int64_t incx , int64_t * result ); cublasStatus_t cublasSsyr_64 ( cublasHandle_t handle , cublasFillMode_t uplo , int64_t n , const float * alpha , const float * x , int64_t incx , float * A , int64_t lda ); Not every function has a 64-bit integer equivalent. For instance, cublasSetMathMode() doesn’t have any arguments that could meaningfully be int64_t . For documentation brevity, the 64-bit integer APIs are not explicitly listed, but only mentioned that they exist for the relevant functions. 2.2. cuBLAS Datatypes Reference \\uf0c1 2.2.1. cublasHandle_t \\uf0c1 The cublasHandle_t type is a pointer type to an opaque structure holding the cuBLAS library context. The cuBLAS library context must be initialized using cublasCreate() and the returned handle must be passed to all subsequent library function calls. The context should be destroyed at the end using cublasDestroy() . 2.2.2. cublasStatus_t \\uf0c1 The type is used for function status returns. All cuBLAS library functions return their status, which can have the following values. Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The cuBLAS library was not initialized. This is usually caused by the lack of a prior cublasCreate() call, an error in the CUDA Runtime API called by the cuBLAS routine, or an error in the hardware setup. To correct: call cublasCreate() before the function call; and check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. CUBLAS_STATUS_ALLOC_FAILED Resource allocation failed inside the cuBLAS library. This is usually caused by a cudaMalloc() failure. To correct: prior to the function call, deallocate previously allocated memory as much as possible. CUBLAS_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example). To correct: ensure that all the parameters being passed have valid values. CUBLAS_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture; usually caused by compute capability lower than 5.0. To correct: compile and run the application on a device with appropriate compute capability. CUBLAS_STATUS_MAPPING_ERROR An access to GPU memory space failed, which is usually caused by a failure to bind a texture. To correct: before the function call, unbind any previously bound textures. CUBLAS_STATUS_EXECUTION_FAILED The GPU program failed to execute. This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons. To correct: check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed. CUBLAS_STATUS_INTERNAL_ERROR An internal cuBLAS operation failed. This error is usually caused by a cudaMemcpyAsync() failure. Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine’s completion. CUBLAS_STATUS_NOT_SUPPORTED The functionality requested is not supported. CUBLAS_STATUS_LICENSE_ERROR The functionality requested requires some license and an error was detected when trying to check the current licensing. This error can happen if the license is not present or is expired or if the environment variable NVIDIA_LICENSE_FILE is not set properly. 2.2.3. cublasOperation_t \\uf0c1 The cublasOperation_t type indicates which operation needs to be performed with the dense matrix. Its values correspond to Fortran characters ‘N’ or ‘n’ (non-transpose), ‘T’ or ‘t’ (transpose) and ‘C’ or ‘c’ (conjugate transpose) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_OP_N The non-transpose operation is selected. CUBLAS_OP_T The transpose operation is selected. CUBLAS_OP_C The conjugate transpose operation is selected. 2.2.4. cublasFillMode_t \\uf0c1 The type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function. Its values correspond to Fortran characters L or l (lower) and U or u (upper) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_FILL_MODE_LOWER The lower part of the matrix is filled. CUBLAS_FILL_MODE_UPPER The upper part of the matrix is filled. CUBLAS_FILL_MODE_FULL The full matrix is filled. 2.2.5. cublasDiagType_t \\uf0c1 The type indicates whether the main diagonal of the dense matrix is unity and consequently should not be touched or modified by the function. Its values correspond to Fortran characters ‘N’ or ‘n’ (non-unit) and ‘U’ or ‘u’ (unit) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_DIAG_NON_UNIT The matrix diagonal has non-unit elements. CUBLAS_DIAG_UNIT The matrix diagonal has unit elements. 2.2.6. cublasSideMode_t \\uf0c1 The type indicates whether the dense matrix is on the left or right side in the matrix equation solved by a particular function. Its values correspond to Fortran characters ‘L’ or ‘l’ (left) and ‘R’ or ‘r’ (right) that are often used as parameters to legacy BLAS implementations. Value Meaning CUBLAS_SIDE_LEFT The matrix is on the left side in the equation. CUBLAS_SIDE_RIGHT The matrix is on the right side in the equation. 2.2.7. cublasPointerMode_t \\uf0c1 The cublasPointerMode_t type indicates whether the scalar values are passed by reference on the host or device. It is important to point out that if several scalar values are present in the function call, all of them must conform to the same single pointer mode. The pointer mode can be set and retrieved using cublasSetPointerMode() and cublasGetPointerMode() routines, respectively. Value Meaning CUBLAS_POINTER_MODE_HOST The scalars are passed by reference on the host. CUBLAS_POINTER_MODE_DEVICE The scalars are passed by reference on the device. 2.2.8. cublasAtomicsMode_t \\uf0c1 The type indicates whether cuBLAS routines which has an alternate implementation using atomics can be used. The atomics mode can be set and queried using cublasSetAtomicsMode() and cublasGetAtomicsMode() and routines, respectively. Value Meaning CUBLAS_ATOMICS_NOT_ALLOWED The usage of atomics is not allowed. CUBLAS_ATOMICS_ALLOWED The usage of atomics is allowed. 2.2.9. cublasGemmAlgo_t \\uf0c1 cublasGemmAlgo_t type is an enumerant to specify the algorithm for matrix-matrix multiplication on GPU architectures up to sm_75 . On sm_80 and newer GPU architectures, this enumarant has no effect. cuBLAS has the following algorithm options: Value Meaning CUBLAS_GEMM_DEFAULT Apply Heuristics to select the GEMM algorithm CUBLAS_GEMM_ALGO0 to CUBLAS_GEMM_ALGO23 Explicitly choose an Algorithm [0,23]. Note: Doesn’t have effect on NVIDIA Ampere architecture GPUs and newer. CUBLAS_GEMM_DEFAULT_TENSOR_OP [DEPRECATED] This mode is deprecated and will be removed in a future release. Apply Heuristics to select the GEMM algorithm, while allowing use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility). CUBLAS_GEMM_ALGO0_TENSOR_OP to CUBLAS_GEMM_ALGO15_TENSOR_OP [DEPRECATED] Those values are deprecated and will be removed in a future release. Explicitly choose a Tensor core GEMM Algorithm [0,15]. Allows use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility). 2.2.10. cublasMath_t \\uf0c1 cublasMath_t enumerate type is used in cublasSetMathMode() to choose compute precision modes as defined in the following table. Since this setting does not directly control the use of Tensor Cores, the mode CUBLAS_TENSOR_OP_MATH is being deprecated, and will be removed in a future release. Value Meaning CUBLAS_DEFAULT_MATH This is the default and highest-performance mode that uses compute and intermediate storage precisions with at least the same number of mantissa and exponent bits as requested. Tensor Cores will be used whenever possible. CUBLAS_PEDANTIC_MATH This mode uses the prescribed precision and standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging. This mode might not be as performant as the other modes. CUBLAS_TF32_TENSOR_OP_MATH Enable acceleration of single-precision routines using TF32 tensor cores. CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION Forces any reductions during matrix multiplications to use the accumulator type (that is, compute type) and not the output type in case of mixed precision routines where output type precision is less than the compute type precision. This is a flag that can be set (using a bitwise or operation) alongside any of the other values. CUBLAS_TENSOR_OP_MATH [DEPRECATED] This mode is deprecated and will be removed in a future release. Allows the library to use Tensor Core operations whenever possible. For single precision GEMM routines cuBLAS will use the CUBLAS_COMPUTE_32F_FAST_16F compute type. 2.2.11. cublasComputeType_t \\uf0c1 cublasComputeType_t enumerate type is used in cublasGemmEx() and cublasLtMatmul() (including all batched and strided batched variants) to choose compute precision modes as defined below. Value Meaning CUBLAS_COMPUTE_16F This is the default and highest-performance mode for 16-bit half precision floating point and all compute and intermediate storage precisions with at least 16-bit half precision. CUBLAS_COMPUTE_16F_PEDANTIC This mode uses 16-bit half precision floating point standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging. This mode might not be as performant as the other modes since it disables use of tensor cores. CUBLAS_COMPUTE_32F This is the default 32-bit single precision floating point and uses compute and intermediate storage precisions of at least 32-bits. CUBLAS_COMPUTE_32F_PEDANTIC Uses 32-bit single precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M). CUBLAS_COMPUTE_32F_FAST_16F Allows the library to use Tensor Cores with automatic down-conversion and 16-bit half-precision compute for 32-bit input and output matrices. CUBLAS_COMPUTE_32F_FAST_16BF Allows the library to use Tensor Cores with automatic down-convesion and bfloat16 compute for 32-bit input and output matrices. See Alternate Floating Point section for more details on bfloat16. CUBLAS_COMPUTE_32F_FAST_TF32 Allows the library to use Tensor Cores with TF32 compute for 32-bit input and output matrices. See Alternate Floating Point section for more details on TF32 compute. CUBLAS_COMPUTE_64F This is the default 64-bit double precision floating point and uses compute and intermediate storage precisions of at least 64-bits. CUBLAS_COMPUTE_64F_PEDANTIC Uses 64-bit double precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M). CUBLAS_COMPUTE_32I This is the default 32-bit integer mode and uses compute and intermediate storage precisions of at least 32-bits. CUBLAS_COMPUTE_32I_PEDANTIC Uses 32-bit integer arithmetic for all phases of calculations. Note Setting the environment variable NVIDIA_TF32_OVERRIDE = 0 will override any defaults or programmatic configuration of NVIDIA libraries, and consequently, cuBLAS will not accelerate FP32 computations with TF32 tensor cores. 2.3. CUDA Datatypes Reference \\uf0c1 The chapter describes types shared by multiple CUDA Libraries and defined in the header file library_types.h . 2.3.1. cudaDataType_t \\uf0c1 The cudaDataType_t type is an enumerant to specify the data precision. It is used when the data reference does not carry the type itself (e.g void *) For example, it is used in the routine cublasSgemmEx() . Value Meaning CUDA_R_16F the data type is a 16-bit real half precision floating-point CUDA_C_16F the data type is a 32-bit structure comprised of two half precision floating-points representing a complex number. CUDA_R_16BF the data type is a 16-bit real bfloat16 floating-point CUDA_C_16BF the data type is a 32-bit structure comprised of two bfloat16 floating-points representing a complex number. CUDA_R_32F the data type is a 32-bit real single precision floating-point CUDA_C_32F the data type is a 64-bit structure comprised of two single precision floating-points representing a complex number. CUDA_R_64F the data type is a 64-bit real double precision floating-point CUDA_C_64F the data type is a 128-bit structure comprised of two double precision floating-points representing a complex number. CUDA_R_8I the data type is a 8-bit real signed integer CUDA_C_8I the data type is a 16-bit structure comprised of two 8-bit signed integers representing a complex number. CUDA_R_8U the data type is a 8-bit real unsigned integer CUDA_C_8U the data type is a 16-bit structure comprised of two 8-bit unsigned integers representing a complex number. CUDA_R_32I the data type is a 32-bit real signed integer CUDA_C_32I the data type is a 64-bit structure comprised of two 32-bit signed integers representing a complex number. CUDA_R_8F_E4M3 the data type is an 8-bit real floating point in E4M3 format CUDA_R_8F_E5M2 the data type is an 8-bit real floating point in E5M2 format 2.3.2. libraryPropertyType_t \\uf0c1 The libraryPropertyType_t is used as a parameter to specify which property is requested when using the routine cublasGetProperty() Value Meaning MAJOR_VERSION enumerant to query the major version MINOR_VERSION enumerant to query the minor version PATCH_LEVEL number to identify the patch level 2.4. cuBLAS Helper Function Reference \\uf0c1 2.4.1. cublasCreate() \\uf0c1 cublasStatus_t cublasCreate ( cublasHandle_t * handle ) This function initializes the cuBLAS library and creates a handle to an opaque structure holding the cuBLAS library context. It allocates hardware resources on the host and device and must be called prior to making any other cuBLAS library calls. The cuBLAS library context is tied to the current CUDA device. To use the library on multiple devices, one cuBLAS handle needs to be created for each device. Furthermore, for a given device, multiple cuBLAS handles with different configurations can be created. Because cublasCreate() allocates some internal resources and the release of those resources by calling cublasDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. For multi-threaded applications that use the same device from different threads, the recommended programming model is to create one cuBLAS handle per thread and use that cuBLAS handle for the entire life of the thread. Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_NOT_INITIALIZED the CUDA™ Runtime initialization failed CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_INVALID_VALUE handle == NULL 2.4.2. cublasDestroy() \\uf0c1 cublasStatus_t cublasDestroy ( cublasHandle_t handle ) This function releases hardware resources used by the cuBLAS library. This function is usually the last call with a particular handle to the cuBLAS library. Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.3. cublasGetVersion() \\uf0c1 cublasStatus_t cublasGetVersion ( cublasHandle_t handle , int * version ) This function returns the version number of the cuBLAS library. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the provided storage for library version number is not initialized (NULL) Note This function can be safely called with the handle set to NULL. This allows users to get the version of the library without a handle. Another way to do this is with cublasGetProperty() . 2.4.4. cublasGetProperty() \\uf0c1 cublasStatus_t cublasGetProperty ( libraryPropertyType type , int * value ) This function returns the value of the requested property in memory pointed to by value. Refer to libraryPropertyType for supported types. Return Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully CUBLAS_STATUS_INVALID_VALUE Invalid type value If invalid type value or value == NULL 2.4.5. cublasGetStatusName() \\uf0c1 const char * cublasGetStatusName ( cublasStatus_t status ) This function returns the string representation of a given status. Return Value Meaning NULL-terminated string The string representation of the status 2.4.6. cublasGetStatusString() \\uf0c1 const char * cublasGetStatusString ( cublasStatus_t status ) This function returns the description string for a given status. Return Value Meaning NULL-terminated string The description of the status 2.4.7. cublasSetStream() \\uf0c1 cublasStatus_t cublasSetStream ( cublasHandle_t handle , cudaStream_t streamId ) This function sets the cuBLAS library stream, which will be used to execute all subsequent calls to the cuBLAS library functions. If the cuBLAS library stream is not set, all kernels use the default NULL stream. In particular, this routine can be used to change the stream between kernel launches and then to reset the cuBLAS library stream back to NULL . Additionally this function unconditionally resets the cuBLAS library workspace back to the default workspace pool (see cublasSetWorkspace() ). Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.8. cublasSetWorkspace() \\uf0c1 cublasStatus_t cublasSetWorkspace ( cublasHandle_t handle , void * workspace , size_t workspaceSizeInBytes ) This function sets the cuBLAS library workspace to a user-owned device buffer, which will be used to execute all subsequent calls to the cuBLAS library functions (on the currently set stream). If the cuBLAS library workspace is not set, all kernels will use the default workspace pool allocated during the cuBLAS context creation. In particular, this routine can be used to change the workspace between kernel launches. The workspace pointer has to be aligned to at least 256 bytes, otherwise CUBLAS_STATUS_INVALID_VALUE error is returned. The cublasSetStream() function unconditionally resets the cuBLAS library workspace back to the default workspace pool. Calling this function, including with workspaceSizeInBytes equal to 0, will prevent the cuBLAS library from utilizing the default workspace. Too small workspaceSizeInBytes may cause some routines to fail with CUBLAS_STATUS_ALLOC_FAILED error returned or cause large regressions in performance. Workspace size equal to or larger than 16KiB is enough to prevent CUBLAS_STATUS_ALLOC_FAILED error, while a larger workspace can provide performance benefits for some routines. Note If the stream set by cublasSetStream() is cudaStreamPerThread and there are multiple threads using the same cuBLAS library handle, then users must manually manage synchronization to avoid possible race conditions in the user provided workspace. Alternatively, users may rely on the default workspace pool which safely guards against race conditions. The table below shows the recommended size of user-provided workspace. This is based on the cuBLAS default workspace pool size which is GPU architecture dependent. GPU Architecture Recommended workspace size NVIDIA Hopper Architecture 32 MiB Other 4 MiB The possible error values returned by this function and their meanings are listed below. Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the workspace pointer wasn’t aligned to at least 256 bytes 2.4.9. cublasGetStream() \\uf0c1 cublasStatus_t cublasGetStream ( cublasHandle_t handle , cudaStream_t * streamId ) This function gets the cuBLAS library stream, which is being used to execute all calls to the cuBLAS library functions. Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was returned successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE streamId == NULL 2.4.10. cublasGetPointerMode() \\uf0c1 cublasStatus_t cublasGetPointerMode ( cublasHandle_t handle , cublasPointerMode_t * mode ) This function obtains the pointer mode used by the cuBLAS library. Please see the section on the cublasPointerMode_t type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was obtained successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode == NULL 2.4.11. cublasSetPointerMode() \\uf0c1 cublasStatus_t cublasSetPointerMode ( cublasHandle_t handle , cublasPointerMode_t mode ) This function sets the pointer mode used by the cuBLAS library. The default is for the values to be passed by reference on the host. Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode is not CUBLAS_POINTER_MODE_HOST or CUBLAS_POINTER_MODE_DEVICE 2.4.12. cublasSetVector() \\uf0c1 cublasStatus_t cublasSetVector ( int n , int elemSize , const void * x , int incx , void * y , int incy ) This function supports the 64-bit Integer Interface . This function copies n elements from a vector x in host memory space to a vector y in GPU memory space. Elements in both vectors are assumed to have a size of elemSize bytes. The storage spacing between consecutive elements is given by incx for the source vector x and by incy for the destination vector y . Since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to 1 accesses a (partial) column of that matrix. Similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix. Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSizesymv and cublashemv have an alternate implementation that use atomics to cumulate results. This implementation is generally significantly faster but can generate results that are not strictly identical from one run to the others. Mathematically, those different results are not significant but when debugging those differences can be prejudicial. This function allows or disallows the usage of atomics in the cuBLAS library for all routines which have an alternate implementation. When not explicitly specified in the documentation of any cuBLAS routine, it means that this routine does not have an alternate implementation that use atomics. When atomics mode is disabled, each cuBLAS routine should produce the same results from one run to the other when called with identical parameters on the same Hardware. The default atomics mode of default initialized cublasHandle_t object is CUBLAS_ATOMICS_NOT_ALLOWED . Please see the section on the type for more details. Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2.4.21. cublasGetAtomicsMode() \\uf0c1 cublasStatus_t cublasGetAtomicsMode ( cublasHandle_t handle , cublasAtomicsMode_t * mode ) This function queries the atomic mode of a specific cuBLAS context. Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was queried successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the argument mode is a NULL pointer 2.4.22. cublasSetMathMode() \\uf0c1 cublasStatus_t cublasSetMathMode ( cublasHandle_t handle , cublasMath_t mode ) The cublasSetMathMode() function enables you to choose the compute precision modes as defined by cublasMath_t . Users are allowed to set the compute precision mode as a logical combination of them (except the deprecated CUBLAS_TENSOR_OP_MATH ). For example, cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION) . Please note that the default math mode is CUBLAS_DEFAULT_MATH . For matrix and compute precisions allowed for cublasGemmEx() and cublasLtMatmul() APIs and their strided variants please refer to: cublasGemmEx() , cublasGemmBatchedEx() , cublasGemmStridedBatchedEx() , and cublasLtMatmul() . Return Value Meaning CUBLAS_STATUS_SUCCESS the math mode was set successfully. CUBLAS_STATUS_INVALID_VALUE an invalid value for mode was specified. CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized. 2.4.23. cublasGetMathMode() \\uf0c1 cublasStatus_t cublasGetMathMode ( cublasHandle_t handle , cublasMath_t * mode ) This function returns the math mode used by the library routines. Return Value Meaning CUBLAS_STATUS_SUCCESS the math type was returned successfully. CUBLAS_STATUS_INVALID_VALUE if mode is NULL.'},\n",
       " {'id': 763,\n",
       "  'content': '2.4.24. cublasSetSmCountTarget() \\uf0c1 cublasStatus_t cublasSetSmCountTarget ( cublasHandle_t handle , int smCountTarget ) The cublasSetSmCountTarget() function allows overriding the number of multiprocessors available to the library during kernels execution. This option can be used to improve the library performance when cuBLAS routines are known to run concurrently with other work on different CUDA streams. E.g. a NVIDIA A100 GPU has 108 SM and there is a concurrent kenrel running with grid size of 8, one can use cublasSetSmCountTarget() with value 100 to override the library heuristics to optimize for running on 100 multiprocessors. When set to 0 the library returns to its default behavior.'},\n",
       " {'id': 764,\n",
       "  'content': 'The input value should not exceed the device’s multiprocessor count, which can be obtained using cudaDeviceGetAttribute . Negative values are not accepted. The user must ensure thread safety when modifying the library handle with this routine similar to when using cublasSetStream() , etc. Return Value Meaning CUBLAS_STATUS_SUCCESS SM count target was set successfully. CUBLAS_STATUS_INVALID_VALUE the value of smCountTarget outside of the allowed range. 2.4.25. cublasGetSmCountTarget() \\uf0c1 cublasStatus_t cublasGetSmCountTarget ( cublasHandle_t handle , int * smCountTarget ) This function obtains the value previously programmed to the library handle. CUBLAS_STATUS_INVALID_VALUE smCountTarget is NULL. 2.4.26. cublasLoggerConfigure() \\uf0c1 cublasStatus_t cublasLoggerConfigure ( int logIsOn , int logToStdOut , int logToStdErr , const char * logFileName ) This function configures logging during runtime. Besides this type of configuration, it is possible to configure logging with special environment variables which will be checked by libcublas: CUBLAS_LOGINFO_DBG - Setup env. variable to “1” means turn on logging (by default logging is off). CUBLAS_LOGDEST_DBG - Setup env. variable encodes how to log. “stdout”, “stderr” means to output log messages to stdout or stderr, respectively. In the other case, its specifies “filename” of file. Parameters logIsOn Input . Turn on/off logging completely. By default is off, but is turned on by calling cublasSetLoggerCallback() to user defined callback function. logToStdOut Input . Turn on/off logging to standard output I/O stream. By default is off. logToStdErr Input . Turn on/off logging to standard error I/O stream. logFileName Input . Turn on/off logging to file in filesystem specified by it’s name. cublasLoggerConfigure() copies the content of logFileName . You should provide null pointer if you are not interested in this type of logging. Returns CUBLAS_STATUS_SUCCESS Success. 2.4.27. cublasGetLoggerCallback() \\uf0c1 cublasStatus_t cublasGetLoggerCallback ( cublasLogCallback * userCallback ) This function retrieves function pointer to previously installed custom user defined callback function via cublasSetLoggerCallback() or zero otherwise. Parameters userCallback Output . Pointer to user defined callback function. The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE userCallback is NULL 2.4.28. cublasSetLoggerCallback() \\uf0c1 cublasStatus_t cublasSetLoggerCallback ( cublasLogCallback userCallback ) This function installs a custom user defined callback function via cublas C public API. Parameters userCallback Input .'},\n",
       " {'id': 765,\n",
       "  'content': '2.5. cuBLAS Level-1 Function Reference \\uf0c1 In this chapter we describe the Level-1 Basic Linear Algebra Subprograms (BLAS1) functions that perform scalar and vector based operations. We will use abbreviations for type and for the corresponding short type to make a more concise and clear presentation of the implemented functions. Unless otherwise specified and have the following meanings: Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision When the parameters and returned values of the function differ, which sometimes happens for complex input, the can also have the following meanings Sc , Cs , Dz and Zd . The abbreviation \\\\(\\\\mathbf{Re}(\\\\cdot)\\\\) and \\\\(\\\\mathbf{Im}(\\\\cdot)\\\\) will stand for the real and imaginary part of a number, respectively. Since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used. Also, the \\\\(\\\\bar{\\\\alpha}\\\\) will denote the complex conjugate of \\\\(\\\\alpha\\\\) . In general throughout the documentation, the lower case Greek symbols \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) will denote scalars, lower case English letters in bold type \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) will denote vectors and capital English letters \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) will denote matrices.'},\n",
       " {'id': 766,\n",
       "  'content': '2.5.1. cublasIamax() \\uf0c1 cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamax ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamax ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamax ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface . This function finds the (smallest) index of the element of the maximum magnitude. Hence, the result is the first \\\\(i\\\\) such that \\\\(\\\\left| \\\\mathbf{Im}\\\\left( {x\\\\lbrack j\\\\rbrack} \\\\right) \\\\middle| + \\\\middle| \\\\mathbf{Re}\\\\left( {x\\\\lbrack j\\\\rbrack} \\\\right) \\\\right|\\\\) is maximum for \\\\(i = 1,\\\\ldots,n\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{ incx}\\\\) . Notice that the last equation reflects 1-based indexing used for compatibility with Fortran.'},\n",
       " {'id': 767,\n",
       "  'content': 'Param. Memory In/out Meaning handle input handle to the cuBLAS library context. n input number of elements in the vector x .'},\n",
       " {'id': 768,\n",
       "  'content': 'x device input vector with elements. incx input stride between consecutive elements of x . result host or device output the resulting index, which is 0 if n,incxamin() \\uf0c1 cublasStatus_t cublasIsamin ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamin ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamin ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamin ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface . This function finds the (smallest) index of the element of the minimum magnitude. Hence, the result is the first \\\\(i\\\\) such that \\\\(\\\\left| \\\\mathbf{Im}\\\\left( {x\\\\lbrack j\\\\rbrack} \\\\right) \\\\middle| + \\\\middle| \\\\mathbf{Re}\\\\left( {x\\\\lbrack j\\\\rbrack} \\\\right) \\\\right|\\\\) is minimum for \\\\(i = 1,\\\\ldots,n\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) Notice that the last equation reflects 1-based indexing used for compatibility with Fortran. result host or device output the resulting index, which is 0 if n,incxasum() \\uf0c1 cublasStatus_t cublasSasum ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDasum ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScasum ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDzasum ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface . This function computes the sum of the absolute values of the elements of vector x .'},\n",
       " {'id': 769, 'content': 'Hence, the result is \\\\(\\\\left.'},\n",
       " {'id': 770,\n",
       "  'content': '\\\\sum_{i = 1}^{n} \\\\middle| \\\\mathbf{Im}\\\\left( {x\\\\lbrack j\\\\rbrack} \\\\right) \\\\middle| + \\\\middle| \\\\mathbf{Re}\\\\left( {x\\\\lbrack j\\\\rbrack} \\\\right) \\\\right|\\\\) where \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) . result host or device output the resulting index, which is 0.0 if n,incxaxpy() \\uf0c1 cublasStatus_t cublasSaxpy ( cublasHandle_t handle , int n , const float * alpha , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDaxpy ( cublasHandle_t handle , int n , const double * alpha , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCaxpy ( cublasHandle_t handle , int n , const cuComplex * alpha , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZaxpy ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function multiplies the vector x by the scalar \\\\(\\\\alpha\\\\) and adds it to the vector y overwriting the latest vector with the result.'},\n",
       " {'id': 771,\n",
       "  'content': 'Hence, the performed operation is \\\\(\\\\mathbf{y}\\\\lbrack j\\\\rbrack = \\\\alpha \\\\times \\\\mathbf{x}\\\\lbrack k\\\\rbrack + \\\\mathbf{y}\\\\lbrack j\\\\rbrack\\\\) for \\\\(i = 1,\\\\ldots,n\\\\) , \\\\(k = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incy}\\\\) . Notice that the last two equations reflect 1-based indexing used for compatibility with Fortran. alpha host or device input scalar used for multiplication. n input number of elements in the vector x and y . x device input vector with n elements. y device in/out vector with n elements. incy input stride between consecutive elements of y . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: saxpy , daxpy , caxpy , zaxpy 2.5.5. cublascopy() \\uf0c1 cublasStatus_t cublasScopy ( cublasHandle_t handle , int n , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDcopy ( cublasHandle_t handle , int n , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCcopy ( cublasHandle_t handle , int n , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZcopy ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function copies the vector x into the vector y . Hence, the performed operation is \\\\(\\\\mathbf{y}\\\\lbrack j\\\\rbrack = \\\\mathbf{x}\\\\lbrack k\\\\rbrack\\\\) for \\\\(i = 1,\\\\ldots,n\\\\) , \\\\(k = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incy}\\\\) .'},\n",
       " {'id': 772,\n",
       "  'content': 'y device output vector with n elements. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: scopy , dcopy , ccopy , zcopy 2.5.6. cublasdot() \\uf0c1 cublasStatus_t cublasSdot ( cublasHandle_t handle , int n , const float * x , int incx , const float * y , int incy , float * result ) cublasStatus_t cublasDdot ( cublasHandle_t handle , int n , const double * x , int incx , const double * y , int incy , double * result ) cublasStatus_t cublasCdotu ( cublasHandle_t handle , int n , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * result ) cublasStatus_t cublasCdotc ( cublasHandle_t handle , int n , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * result ) cublasStatus_t cublasZdotu ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * result ) cublasStatus_t cublasZdotc ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * result ) This function supports the 64-bit Integer Interface . This function computes the dot product of vectors x and y .'},\n",
       " {'id': 773,\n",
       "  'content': 'Hence, the result is \\\\(\\\\sum_{i = 1}^{n}\\\\left( {\\\\mathbf{x}\\\\lbrack k\\\\rbrack \\\\times \\\\mathbf{y}\\\\lbrack j\\\\rbrack} \\\\right)\\\\) where \\\\(k = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incy}\\\\) . Notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character ‘c’ and that the last two equations reflect 1-based indexing used for compatibility with Fortran. n input number of elements in the vectors x and y .'},\n",
       " {'id': 774,\n",
       "  'content': 'y device input vector with n elements. result host or device output the resulting dot product, which is 0.0 if nnrm2() \\uf0c1 cublasStatus_t cublasSnrm2 ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDnrm2 ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScnrm2 ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDznrm2 ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface . This function computes the Euclidean norm of the vector x . The code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to \\\\(\\\\sqrt{\\\\sum_{i = 1}^{n}\\\\left( {\\\\mathbf{x}\\\\lbrack j\\\\rbrack \\\\times \\\\mathbf{x}\\\\lbrack j\\\\rbrack} \\\\right)}\\\\) where \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) in exact arithmetic. result host or device output the resulting norm, which is 0.0 if n,incxrot() \\uf0c1 cublasStatus_t cublasSrot ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * c , const float * s ) cublasStatus_t cublasDrot ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * c , const double * s ) cublasStatus_t cublasCrot ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy , const float * c , const cuComplex * s ) cublasStatus_t cublasCsrot ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy , const float * c , const float * s ) cublasStatus_t cublasZrot ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy , const double * c , const cuDoubleComplex * s ) cublasStatus_t cublasZdrot ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy , const double * c , const double * s ) This function supports the 64-bit Integer Interface . This function applies Givens rotation matrix (i.e., rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \\\\(G = \\\\begin{pmatrix} c & s \\\\\\\\ {- s} & c \\\\\\\\ \\\\end{pmatrix}\\\\) to vectors x and y .'},\n",
       " {'id': 775,\n",
       "  'content': 'Hence, the result is \\\\(\\\\mathbf{x}\\\\lbrack k\\\\rbrack = c \\\\times \\\\mathbf{x}\\\\lbrack k\\\\rbrack + s \\\\times \\\\mathbf{y}\\\\lbrack j\\\\rbrack\\\\) and \\\\(\\\\mathbf{y}\\\\lbrack j\\\\rbrack = - s \\\\times \\\\mathbf{x}\\\\lbrack k\\\\rbrack + c \\\\times \\\\mathbf{y}\\\\lbrack j\\\\rbrack\\\\) where \\\\(k = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incy}\\\\) . x device in/out vector with n elements. c host or device input cosine element of the rotation matrix.'},\n",
       " {'id': 776,\n",
       "  'content': 's host or device input sine element of the rotation matrix. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.5.9. cublasrotg() \\uf0c1 cublasStatus_t cublasSrotg ( cublasHandle_t handle , float * a , float * b , float * c , float * s ) cublasStatus_t cublasDrotg ( cublasHandle_t handle , double * a , double * b , double * c , double * s ) cublasStatus_t cublasCrotg ( cublasHandle_t handle , cuComplex * a , cuComplex * b , float * c , cuComplex * s ) cublasStatus_t cublasZrotg ( cublasHandle_t handle , cuDoubleComplex * a , cuDoubleComplex * b , double * c , cuDoubleComplex * s ) This function supports the 64-bit Integer Interface . This function constructs the Givens rotation matrix \\\\(G = \\\\begin{pmatrix} c & s \\\\\\\\ {- s} & c \\\\\\\\ \\\\end{pmatrix}\\\\) that zeros out the second entry of a \\\\(2 \\\\times 1\\\\) vector \\\\(\\\\left( {a,b} \\\\right)^{T}\\\\) . Then, for real numbers we can write \\\\(\\\\begin{pmatrix} c & s \\\\\\\\ {- s} & c \\\\\\\\ \\\\end{pmatrix}\\\\begin{pmatrix} a \\\\\\\\ b \\\\\\\\ \\\\end{pmatrix} = \\\\begin{pmatrix} r \\\\\\\\ 0 \\\\\\\\ \\\\end{pmatrix}\\\\) where \\\\(c^{2} + s^{2} = 1\\\\) and \\\\(r = a^{2} + b^{2}\\\\) . The parameters \\\\(a\\\\) and \\\\(b\\\\) are overwritten with \\\\(r\\\\) and \\\\(z\\\\) , respectively. The value of \\\\(z\\\\) is such that \\\\(c\\\\) and \\\\(s\\\\) may be recovered using the following rules: \\\\(\\\\left( {c,s} \\\\right) = \\\\begin{cases} \\\\left( {\\\\sqrt{1 - z^{2}},z} \\\\right) & {\\\\text{ if }\\\\left| z \\\\middle| 1 \\\\right.}\\n\\\\\\\\ \\\\end{cases}\\\\) For complex numbers we can write \\\\(\\\\begin{pmatrix} c & s \\\\\\\\ {- \\\\bar{s}} & c \\\\\\\\ \\\\end{pmatrix}\\\\begin{pmatrix} a \\\\\\\\ b \\\\\\\\ \\\\end{pmatrix} = \\\\begin{pmatrix} r \\\\\\\\ 0 \\\\\\\\ \\\\end{pmatrix}\\\\) where \\\\(c^{2} + \\\\left( {\\\\bar{s} \\\\times s} \\\\right) = 1\\\\) and \\\\(r = \\\\frac{a}{|a|} \\\\times \\\\parallel \\\\left( {a,b} \\\\right)^{T} \\\\parallel_{2}\\\\) with \\\\(\\\\parallel \\\\left( {a,b} \\\\right)^{T} \\\\parallel_{2} = \\\\sqrt{\\\\left| a|^{2} + \\\\middle| b|^{2} \\\\right. }\\\\) for \\\\(a \\\\neq 0\\\\) and \\\\(r = b\\\\) for \\\\(a = 0\\\\) .'},\n",
       " {'id': 777,\n",
       "  'content': 'Finally, the parameter \\\\(a\\\\) is overwritten with \\\\(r\\\\) on exit. a host or device in/out scalar that is overwritten with \\\\(r\\\\) . b host or device in/out scalar that is overwritten with \\\\(z\\\\) . c host or device output cosine element of the rotation matrix. s host or device output sine element of the rotation matrix. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotg , drotg , crotg , zrotg 2.5.10. cublasrotm() \\uf0c1 cublasStatus_t cublasSrotm ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * param ) cublasStatus_t cublasDrotm ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * param ) This function supports the 64-bit Integer Interface . This function applies the modified Givens transformation \\\\(H = \\\\begin{pmatrix} h_{11} & h_{12} \\\\\\\\ h_{21} & h_{22} \\\\\\\\ \\\\end{pmatrix}\\\\) to vectors x and y . Hence, the result is \\\\(\\\\mathbf{x}\\\\lbrack k\\\\rbrack = h_{11} \\\\times \\\\mathbf{x}\\\\lbrack k\\\\rbrack + h_{12} \\\\times \\\\mathbf{y}\\\\lbrack j\\\\rbrack\\\\) and \\\\(\\\\mathbf{y}\\\\lbrack j\\\\rbrack = h_{21} \\\\times \\\\mathbf{x}\\\\lbrack k\\\\rbrack + h_{22} \\\\times \\\\mathbf{y}\\\\lbrack j\\\\rbrack\\\\) where \\\\(k = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incy}\\\\) . The elements , , and of matrix \\\\(H\\\\) are stored in param[1] , param[2] , param[3] and param[4] , respectively. The flag=param[0] defines the following predefined values for the matrix \\\\(H\\\\) entries flag=-1.0 flag= 0.0 flag= 1.0 flag=-2.0 \\\\(\\\\begin{pmatrix} h_{11} & h_{12} \\\\\\\\ h_{21} & h_{22} \\\\\\\\ \\\\end{pmatrix}\\\\) \\\\(\\\\begin{pmatrix} {1.0} & h_{12} \\\\\\\\ h_{21} & {1.0} \\\\\\\\ \\\\end{pmatrix}\\\\) \\\\(\\\\begin{pmatrix} h_{11} & {1.0} \\\\\\\\ {- 1.0} & h_{22} \\\\\\\\ \\\\end{pmatrix}\\\\) \\\\(\\\\begin{pmatrix} {1.0} & {0.0} \\\\\\\\ {0.0} & {1.0} \\\\\\\\ \\\\end{pmatrix}\\\\) Notice that the values -1.0, 0.0 and 1.0 implied by the flag are not stored in param. param host or device input vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\\\(H\\\\) .'},\n",
       " {'id': 778,\n",
       "  'content': 'Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotm , drotm 2.5.11. cublasrotmg() \\uf0c1 cublasStatus_t cublasSrotmg ( cublasHandle_t handle , float * d1 , float * d2 , float * x1 , const float * y1 , float * param ) cublasStatus_t cublasDrotmg ( cublasHandle_t handle , double * d1 , double * d2 , double * x1 , const double * y1 , double * param ) This function supports the 64-bit Integer Interface . This function constructs the modified Givens transformation \\\\(H = \\\\begin{pmatrix} h_{11} & h_{12} \\\\\\\\ h_{21} & h_{22} \\\\\\\\ \\\\end{pmatrix}\\\\) that zeros out the second entry of a \\\\(2 \\\\times 1\\\\) vector \\\\(\\\\left( {\\\\sqrt{d1}*x1,\\\\sqrt{d2}*y1} \\\\right)^{T}\\\\) . d1 host or device in/out scalar that is overwritten on exit. d2 host or device in/out scalar that is overwritten on exit. x1 host or device in/out scalar that is overwritten on exit. y1 host or device input scalar. param host or device output vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\\\(H\\\\) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotmg , drotmg 2.5.12. cublasscal() \\uf0c1 cublasStatus_t cublasSscal ( cublasHandle_t handle , int n , const float * alpha , float * x , int incx ) cublasStatus_t cublasDscal ( cublasHandle_t handle , int n , const double * alpha , double * x , int incx ) cublasStatus_t cublasCscal ( cublasHandle_t handle , int n , const cuComplex * alpha , cuComplex * x , int incx ) cublasStatus_t cublasCsscal ( cublasHandle_t handle , int n , const float * alpha , cuComplex * x , int incx ) cublasStatus_t cublasZscal ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , cuDoubleComplex * x , int incx ) cublasStatus_t cublasZdscal ( cublasHandle_t handle , int n , const double * alpha , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function scales the vector x by the scalar \\\\(\\\\alpha\\\\) and overwrites it with the result. Hence, the performed operation is \\\\(\\\\mathbf{x}\\\\lbrack j\\\\rbrack = \\\\alpha \\\\times \\\\mathbf{x}\\\\lbrack j\\\\rbrack\\\\) for \\\\(i = 1,\\\\ldots,n\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) . :class: table-no-stripes \\uf0c1 Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 2.5.13. cublasswap() \\uf0c1 cublasStatus_t cublasSswap ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy ) cublasStatus_t cublasDswap ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy ) cublasStatus_t cublasCswap ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZswap ( cublasHandle_t handle , int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function interchanges the elements of vector x and y .'},\n",
       " {'id': 779, 'content': 'Hence, the performed operation is \\\\(\\\\left.'},\n",
       " {'id': 780,\n",
       "  'content': '\\\\mathbf{y}\\\\lbrack j\\\\rbrack\\\\Leftrightarrow\\\\mathbf{x}\\\\lbrack k\\\\rbrack \\\\right.\\\\) for \\\\(i = 1,\\\\ldots,n\\\\) , \\\\(k = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incx}\\\\) and \\\\(j = 1 + \\\\left( {i - 1} \\\\right)*\\\\text{incy}\\\\) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sswap , dswap , cswap , zswap 2.6. cuBLAS Level-2 Function Reference \\uf0c1 In this chapter we describe the Level-2 Basic Linear Algebra Subprograms (BLAS2) functions that perform matrix-vector operations. 2.6.1. cublasgbmv() \\uf0c1 cublasStatus_t cublasSgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZgbmv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int kl , int ku , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the banded matrix-vector multiplication \\\\(\\\\mathbf{y} = \\\\alpha\\\\text{ op}(A)\\\\mathbf{x} + \\\\beta\\\\mathbf{y}\\\\) where \\\\(A\\\\) is a banded matrix with \\\\(kl\\\\) subdiagonals and \\\\(ku\\\\) superdiagonals, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars.'},\n",
       " {'id': 781,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{ op}(A) = \\\\begin{cases} A & \\\\text{ if transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$} \\\\\\\\ A^{T} & \\\\text{ if transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$} \\\\\\\\ A^{H} & \\\\text{ if transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$} \\\\\\\\ \\\\end{cases}\\\\) The banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal stored in row \\\\(ku + 1\\\\) (starting in first position), the first superdiagonal stored in row \\\\(ku\\\\) (starting in second position), the first subdiagonal stored in row \\\\(ku + 2\\\\) (starting in first position), etc. So that in general, the element \\\\(A\\\\left( {i,j} \\\\right)\\\\) is stored in the memory location A(ku+1+i-j,j) for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\in \\\\left\\\\lbrack {\\\\max\\\\left( {1,j - ku} \\\\right),\\\\min\\\\left( {m,j + kl} \\\\right)} \\\\right\\\\rbrack\\\\) . Also, the elements in the array \\\\(A\\\\) that do not conceptually correspond to the elements in the banded matrix (the top left \\\\(ku \\\\times ku\\\\) and bottom right \\\\(kl \\\\times kl\\\\) triangles) are not referenced. trans input operation op( A ) that is non- or (conj.)\\ntranspose. m input number of rows of matrix A . n input number of columns of matrix A . kl input number of subdiagonals of matrix A . ku input number of superdiagonals of matrix A . A device input array of dimension lda x n with lda>=kl+ku+1 . lda input leading dimension of two-dimensional array used to store matrix A . x device input vector with n elements if transa == CUBLAS_OP_N and m elements otherwise. beta host or device input scalar used for multiplication, if beta == 0 then y does not have to be a valid input. y device in/out vector with m elements if transa == CUBLAS_OP_N and n elements otherwise. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m, n, kl, ku gemv() \\uf0c1 cublasStatus_t cublasSgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZgemv ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication \\\\(\\\\textbf{y} = \\\\alpha\\\\text{ op}(A)\\\\textbf{x} + \\\\beta\\\\textbf{y}\\\\) where \\\\(A\\\\) is a \\\\(m \\\\times n\\\\) matrix stored in column-major format, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars.'},\n",
       " {'id': 782,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{ op}(A) = \\\\begin{cases} A & \\\\text{ if transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$} \\\\\\\\ A^{T} & \\\\text{ if transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$} \\\\\\\\ A^{H} & \\\\text{ if transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$} \\\\\\\\ \\\\end{cases}\\\\) Param. A device input array of dimension lda x n with lda >= max(1,m) . Before entry, the leading m by n part of the array A must contain the matrix of coefficients. Unchanged on exit. lda must be at least max(1,m) . x device input vector at least (1+(n-1)*abs(incx)) elements if transa==CUBLAS_OP_N and at least (1+(m-1)*abs(incx)) elements otherwise. beta host or device input scalar used for multiplication, if beta==0 then y does not have to be a valid input. y device in/out vector at least (1+(m-1)*abs(incy)) elements if transa==CUBLAS_OP_N and at least (1+(n-1)*abs(incy)) elements otherwise. incy input stride between consecutive elements of y The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,nger() \\uf0c1 cublasStatus_t cublasSger ( cublasHandle_t handle , int m , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * A , int lda ) cublasStatus_t cublasDger ( cublasHandle_t handle , int m , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * A , int lda ) cublasStatus_t cublasCgeru ( cublasHandle_t handle , int m , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasCgerc ( cublasHandle_t handle , int m , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasZgeru ( cublasHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) cublasStatus_t cublasZgerc ( cublasHandle_t handle , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the rank-1 update \\\\(A = \\\\begin{cases} {\\\\alpha\\\\mathbf{xy}^{T} + A} & \\\\text{if ger(),geru() is called} \\\\\\\\ {\\\\alpha\\\\mathbf{xy}^{H} + A} & \\\\text{if gerc() is called} \\\\\\\\ \\\\end{cases}\\\\) where \\\\(A\\\\) is a \\\\(m \\\\times n\\\\) matrix stored in column-major format, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) is a scalar.'},\n",
       " {'id': 783, 'content': 'x device input vector with m elements.'},\n",
       " {'id': 784,\n",
       "  'content': 'A device in/out array of dimension lda x n with lda >= max(1,m) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m sbmv() \\uf0c1 cublasStatus_t cublasSsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric banded matrix-vector multiplication \\\\(\\\\textbf{y} = \\\\alpha A\\\\textbf{x} + \\\\beta\\\\textbf{y}\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric banded matrix with \\\\(k\\\\) subdiagonals and superdiagonals, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the symmetric banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal of the matrix stored in row 1, the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. So that in general, the element \\\\(A(i,j)\\\\) is stored in the memory location A(1+i-j,j) for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\in \\\\lbrack j,\\\\min(m,j + k)\\\\rbrack\\\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\\\(k \\\\times k\\\\) triangle) are not referenced. If uplo == CUBLAS_FILL_MODE_UPPER then the symmetric banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\\\(A(i,j)\\\\) is stored in the memory location A(1+k+i-j,j) for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\in \\\\lbrack\\\\max(1,j - k),j\\\\rbrack\\\\) . Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\\\(k \\\\times k\\\\) triangle) are not referenced. uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix A . k input number of sub- and super-diagonals of matrix A . A device input array of dimension lda x n with lda >= k+1 . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spmv() \\uf0c1 cublasStatus_t cublasSspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * AP , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * AP , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric packed matrix-vector multiplication \\\\(\\\\textbf{y} = \\\\alpha A\\\\textbf{x} + \\\\beta\\\\textbf{y}\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric matrix stored in packed format, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\geq j\\\\) . Consequently, the packed format requires only \\\\(\\\\frac{n(n + 1)}{2}\\\\) elements for storage. If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\leq j\\\\) . uplo input indicates if matrix \\\\(A\\\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows and columns of matrix \\\\(A\\\\) .'},\n",
       " {'id': 785,\n",
       "  'content': 'AP device input array with \\\\(A\\\\) stored in packed format. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spr() \\uf0c1 cublasStatus_t cublasSspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * AP ) cublasStatus_t cublasDspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , double * AP ) This function supports the 64-bit Integer Interface . This function performs the packed symmetric rank-1 update \\\\(A = \\\\alpha\\\\textbf{x}\\\\textbf{x}^{T} + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric matrix stored in packed format, \\\\(\\\\mathbf{x}\\\\) is a vector, and \\\\(\\\\alpha\\\\) is a scalar. AP device in/out array with \\\\(A\\\\) stored in packed format. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spr2() \\uf0c1 cublasStatus_t cublasSspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * AP ) cublasStatus_t cublasDspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * AP ) This function supports the 64-bit Integer Interface . This function performs the packed symmetric rank-2 update \\\\(A = \\\\alpha\\\\left( {\\\\textbf{x}\\\\textbf{y}^{T} + \\\\textbf{y}\\\\textbf{x}^{T}} \\\\right) + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric matrix stored in packed format, \\\\(\\\\mathbf{x}\\\\) is a vector, and \\\\(\\\\alpha\\\\) is a scalar. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n symv() \\uf0c1 cublasStatus_t cublasSsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t cublasDsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) cublasStatus_t cublasCsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , /* host or device pointer */ const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZsymv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the symmetric matrix-vector multiplication.'},\n",
       " {'id': 786,\n",
       "  'content': '\\\\(\\\\textbf{y} = \\\\alpha A\\\\textbf{x} + \\\\beta\\\\textbf{y}\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric matrix stored in lower or upper mode, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. This function has an alternate faster implementation using atomics that can be enabled with cublasSetAtomicsMode() . Please see the section on the function cublasSetAtomicsMode() for more details about the usage of atomics. uplo input indicates if matrix lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. A device input array of dimension lda x n with lda>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n syr() \\uf0c1 cublasStatus_t cublasSsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * A , int lda ) cublasStatus_t cublasDsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , double * A , int lda ) cublasStatus_t cublasCsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZsyr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank-1 update \\\\(A = \\\\alpha\\\\textbf{x}\\\\textbf{x}^{T} + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric matrix stored in column-major format, \\\\(\\\\mathbf{x}\\\\) is a vector, and \\\\(\\\\alpha\\\\) is a scalar. A device in/out array of dimensions lda x n , with lda>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n syr2() \\uf0c1 cublasStatus_t cublasSsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * A , int lda cublasStatus_t cublasDsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * A , int lda cublasStatus_t cublasCsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda cublasStatus_t cublasZsyr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda This function supports the 64-bit Integer Interface . This function performs the symmetric rank-2 update \\\\(A = \\\\alpha\\\\left( {\\\\textbf{x}\\\\textbf{y}^{T} + \\\\textbf{y}\\\\textbf{x}^{T}} \\\\right) + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric matrix stored in column-major format, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) is a scalar. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n tbmv() \\uf0c1 cublasStatus_t cublasStbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtbmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular banded matrix-vector multiplication \\\\(\\\\textbf{x} = \\\\text{op}(A)\\\\textbf{x}\\\\) where \\\\(A\\\\) is a triangular banded matrix, and \\\\(\\\\mathbf{x}\\\\) is a vector.'},\n",
       " {'id': 787,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. If uplo == CUBLAS_FILL_MODE_UPPER then the triangular banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. So that in general, the element \\\\(A(i,j)\\\\) is stored in the memory location A(1+k+i-j,j) for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\in \\\\lbrack\\\\max(1,j - k,j)\\\\rbrack\\\\) . uplo input indicates if matrix A lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed. k input number of sub- and super-diagonals of matrix . A device input array of dimension lda x n , with lda>=k+1 . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n tbsv() \\uf0c1 cublasStatus_t cublasStbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtbsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , int k , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the triangular banded linear system with a single right-hand-side \\\\(\\\\text{op}(A)\\\\textbf{x} = \\\\textbf{b}\\\\) where \\\\(A\\\\) is a triangular banded matrix, and \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{b}\\\\) are vectors.'},\n",
       " {'id': 788,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) The solution \\\\(\\\\mathbf{x}\\\\) overwrites the right-hand-sides \\\\(\\\\mathbf{b}\\\\) on exit. No test for singularity or near-singularity is included in this function. If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. A device input array of dimension lda x n , with lda >= k+1 . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n tpmv() \\uf0c1 cublasStatus_t cublasStpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * AP , float * x , int incx ) cublasStatus_t cublasDtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * AP , double * x , int incx ) cublasStatus_t cublasCtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * AP , cuComplex * x , int incx ) cublasStatus_t cublasZtpmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * AP , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular packed matrix-vector multiplication \\\\(\\\\textbf{x} = \\\\text{op}(A)\\\\textbf{x}\\\\) where \\\\(A\\\\) is a triangular matrix stored in packed format, and \\\\(\\\\mathbf{x}\\\\) is a vector.'},\n",
       " {'id': 789,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\geq j\\\\) . If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\\\(A(i,j)\\\\) and \\\\(i \\\\leq j\\\\) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n tpsv() \\uf0c1 cublasStatus_t cublasStpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * AP , float * x , int incx ) cublasStatus_t cublasDtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * AP , double * x , int incx ) cublasStatus_t cublasCtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * AP , cuComplex * x , int incx ) cublasStatus_t cublasZtpsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * AP , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the packed triangular linear system with a single right-hand-side \\\\(\\\\text{op}(A)\\\\textbf{x} = \\\\textbf{b}\\\\) where \\\\(A\\\\) is a triangular matrix stored in packed format, and \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{b}\\\\) are vectors.'},\n",
       " {'id': 790,\n",
       "  'content': 'If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\geq j\\\\) . If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\leq j\\\\) . diag input indicates if the elements on the main diagonal of matrix are unity and should not be accessed. AP device input array with A stored in packed format. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n trmv() \\uf0c1 cublasStatus_t cublasStrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtrmv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function performs the triangular matrix-vector multiplication \\\\(\\\\textbf{x} = \\\\text{op}(A)\\\\textbf{x}\\\\) where \\\\(A\\\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\\\(\\\\mathbf{x}\\\\) is a vector.'},\n",
       " {'id': 791,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Param. trans input operation op( A ) (that is, non- or conj.)\\nA device input array of dimensions lda x n , with lda>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n trsv() \\uf0c1 cublasStatus_t cublasStrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const float * A , int lda , float * x , int incx ) cublasStatus_t cublasDtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const double * A , int lda , double * x , int incx ) cublasStatus_t cublasCtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuComplex * A , int lda , cuComplex * x , int incx ) cublasStatus_t cublasZtrsv ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface . This function solves the triangular linear system with a single right-hand-side \\\\(\\\\text{op}(A)\\\\textbf{x} = \\\\textbf{b}\\\\) where \\\\(A\\\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{b}\\\\) are vectors. A device input array of dimension lda x n , with lda>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hemv() \\uf0c1 cublasStatus_t cublasChemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian matrix-vector multiplication \\\\(\\\\textbf{y} = \\\\alpha A\\\\textbf{x} + \\\\beta\\\\textbf{y}\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) Hermitian matrix stored in lower or upper mode, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars.'},\n",
       " {'id': 792,\n",
       "  'content': 'This function has an alternate faster implementation using atomics that can be enabled with Please see the section on the for more details about the usage of atomics Param. uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements. The imaginary parts of the diagonal elements are assumed to be zero. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hbmv() \\uf0c1 cublasStatus_t cublasChbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian banded matrix-vector multiplication \\\\(\\\\textbf{y} = \\\\alpha A\\\\textbf{x} + \\\\beta\\\\textbf{y}\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) Hermitian banded matrix with \\\\(k\\\\) subdiagonals and superdiagonals, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the Hermitian banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc. If uplo == CUBLAS_FILL_MODE_UPPER then the Hermitian banded matrix \\\\(A\\\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc. A device input array of dimensions lda x n , with lda>=k+1 . beta host or device input scalar used for multiplication, if beta==0 then does not have to be a valid input. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpmv() \\uf0c1 cublasStatus_t cublasChpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy ) cublasStatus_t cublasZhpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface . This function performs the Hermitian packed matrix-vector multiplication \\\\(\\\\textbf{y} = \\\\alpha A\\\\textbf{x} + \\\\beta\\\\textbf{y}\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) Hermitian matrix stored in packed format, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\geq j\\\\) . If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\\\(A\\\\) are packed together column by column without gaps, so that the element \\\\(A(i,j)\\\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\\\(j = 1,\\\\ldots,n\\\\) and \\\\(i \\\\leq j\\\\) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her() \\uf0c1 cublasStatus_t cublasCher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank-1 update \\\\(A = \\\\alpha\\\\textbf{x}\\\\textbf{x}^{H} + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) Hermitian matrix stored in column-major format, \\\\(\\\\mathbf{x}\\\\) is a vector, and \\\\(\\\\alpha\\\\) is a scalar. The imaginary parts of the diagonal elements are assumed and set to zero. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her2() \\uf0c1 cublasStatus_t cublasCher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda ) cublasStatus_t cublasZher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank-2 update \\\\(A = \\\\alpha\\\\textbf{x}\\\\textbf{y}^{H} + \\\\overset{ˉ}{\\\\alpha}\\\\textbf{y}\\\\textbf{x}^{H} + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) Hermitian matrix stored in column-major format, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) is a scalar. A device in/out array of dimension lda x n with lda>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpr() \\uf0c1 cublasStatus_t cublasChpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * AP ) cublasStatus_t cublasZhpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface . This function performs the packed Hermitian rank-1 update \\\\(A = \\\\alpha\\\\textbf{x}\\\\textbf{x}^{H} + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) Hermitian matrix stored in packed format, \\\\(\\\\mathbf{x}\\\\) is a vector, and \\\\(\\\\alpha\\\\) is a scalar. AP device in/out array with A stored in packed format. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpr2() \\uf0c1 cublasStatus_t cublasChpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * AP ) cublasStatus_t cublasZhpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface . This function performs the packed Hermitian rank-2 update \\\\(A = \\\\alpha\\\\textbf{x}\\\\textbf{y}^{H} + \\\\overset{ˉ}{\\\\alpha}\\\\textbf{y}\\\\textbf{x}^{H} + A\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) Hermitian matrix stored in packed format, \\\\(\\\\mathbf{x}\\\\) and \\\\(\\\\mathbf{y}\\\\) are vectors, and \\\\(\\\\alpha\\\\) is a scalar. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n gemvBatched() \\uf0c1 cublasStatus_t cublasSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * const Aarray [], int lda , const float * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) cublasStatus_t cublasDgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * const Aarray [], int lda , const double * const xarray [], int incx , const double * beta , double * const yarray [], int incy , int batchCount ) cublasStatus_t cublasCgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * const Aarray [], int lda , const cuComplex * const xarray [], int incx , const cuComplex * beta , cuComplex * const yarray [], int incy , int batchCount ) cublasStatus_t cublasZgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * const Aarray [], int lda , const cuDoubleComplex * const xarray [], int incx , const cuDoubleComplex * beta , cuDoubleComplex * const yarray [], int incy , int batchCount ) #if defined(__cplusplus) cublasStatus_t cublasHSHgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * const Aarray [], int lda , const __half * const xarray [], int incx , const float * beta , __half * const yarray [], int incy , int batchCount ) cublasStatus_t cublasHSSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * const Aarray [], int lda , const __half * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) cublasStatus_t cublasTSTgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * const Aarray [], int lda , const __nv_bfloat16 * const xarray [], int incx , const float * beta , __nv_bfloat16 * const yarray [], int incy , int batchCount ) cublasStatus_t cublasTSSgemvBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * const Aarray [], int lda , const __nv_bfloat16 * const xarray [], int incx , const float * beta , float * const yarray [], int incy , int batchCount ) #endif This function supports the 64-bit Integer Interface . This function performs the matrix-vector multiplication of a batch of matrices and vectors.'},\n",
       " {'id': 793,\n",
       "  'content': 'The batch is considered to be “uniform”, i.e. all instances have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective A matrix, x and y vectors. The address of the input matrix and vector, and the output vector of each instance of the batch are read from arrays of pointers passed to the function by the caller. \\\\(\\\\textbf{y}\\\\lbrack i\\\\rbrack = \\\\alpha\\\\text{op}(A\\\\lbrack i\\\\rbrack)\\\\textbf{x}\\\\lbrack i\\\\rbrack + \\\\beta\\\\textbf{y}\\\\lbrack i\\\\rbrack,\\\\text{ for i} \\\\in \\\\lbrack 0,batchCount - 1\\\\rbrack\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) is an array of pointers to matrice \\\\(A\\\\lbrack i\\\\rbrack\\\\) stored in column-major format with dimension \\\\(m \\\\times n\\\\) , and \\\\(\\\\textbf{x}\\\\) and \\\\(\\\\textbf{y}\\\\) are arrays of pointers to vectors. Also, for matrix \\\\(A\\\\lbrack i\\\\rbrack\\\\) , \\\\(\\\\text{op}(A\\\\lbrack i\\\\rbrack) = \\\\left\\\\{ \\\\begin{matrix} {A\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A\\\\lbrack i\\\\rbrack}^{T} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ {A\\\\lbrack i\\\\rbrack}^{H} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Note \\\\(\\\\textbf{y}\\\\lbrack i\\\\rbrack\\\\) vectors must not overlap, i.e. the individual gemv operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublasgemv in different CUDA streams, rather than use this API. trans input operation op( A[i] ) that is non- or (conj.)\\nm input number of rows of matrix A[i] .'},\n",
       " {'id': 794,\n",
       "  'content': 'n input number of columns of matrix A[i] . Aarray device input array of pointers to array, with each array of dim. lda x n with lda>=max(1,m) . All pointers must meet certain alignment criteria. Please see below for details. lda input leading dimension of two-dimensional array used to store each matrix A[i] . xarray device input array of pointers to array, with each dimension n if trans==CUBLAS_OP_N and m otherwise. incx input stride of each one-dimensional array x[i]. beta host or device input scalar used for multiplication. If beta == 0 , y does not have to be a valid input. yarray device in/out array of pointers to array. It has dimensions m if trans==CUBLAS_OP_N and n otherwise. Vectors y[i] should not overlap; otherwise, undefined behavior is expected. incy input stride of each one-dimensional array y[i]. batchCount input number of pointers contained in Aarray, xarray and yarray. If math mode enables fast math modes when using cublasSgemvBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Ideally all pointers are aligned to at least 16 Bytes. Otherwise it is recommended that they meet the following rule: if k % 4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,batchCountgemvStridedBatched() \\uf0c1 cublasStatus_t cublasSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const float * A , int lda , long long int strideA , const float * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasDgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const double * alpha , const double * A , int lda , long long int strideA , const double * x , int incx , long long int stridex , const double * beta , double * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasCgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * x , int incx , long long int stridex , const cuComplex * beta , cuComplex * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasZgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , long long int strideA , const cuDoubleComplex * x , int incx , long long int stridex , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasHSHgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * A , int lda , long long int strideA , const __half * x , int incx , long long int stridex , const float * beta , __half * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasHSSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __half * A , int lda , long long int strideA , const __half * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasTSTgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * A , int lda , long long int strideA , const __nv_bfloat16 * x , int incx , long long int stridex , const float * beta , __nv_bfloat16 * y , int incy , long long int stridey , int batchCount ) cublasStatus_t cublasTSSgemvStridedBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , const float * alpha , const __nv_bfloat16 * A , int lda , long long int strideA , const __nv_bfloat16 * x , int incx , long long int stridex , const float * beta , float * y , int incy , long long int stridey , int batchCount ) This function supports the 64-bit Integer Interface . Input matrix A and vector x, and output vector y for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance.'},\n",
       " {'id': 795,\n",
       "  'content': 'Pointers to A matrix, x and y vectors for the first instance are passed to the function by the user along with offsets in number of elements - strideA, stridex and stridey that determine the locations of input matrices and vectors, and output vectors in future instances. \\\\(\\\\textbf{y} + i*{stridey} = \\\\alpha\\\\text{op}(A + i*{strideA})(\\\\textbf{x} + i*{stridex}) + \\\\beta(\\\\textbf{y} + i*{stridey}),\\\\text{ for i } \\\\in \\\\lbrack 0,batchCount - 1\\\\rbrack\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) is an array of pointers to matrix stored in column-major format with dimension \\\\(A\\\\lbrack i\\\\rbrack\\\\) \\\\(m \\\\times n\\\\) , and \\\\(\\\\textbf{x}\\\\) and \\\\(\\\\textbf{y}\\\\) are arrays of pointers to vectors. Also, for matrix \\\\(A\\\\lbrack i\\\\rbrack\\\\) \\\\(\\\\text{op}(A\\\\lbrack i\\\\rbrack) = \\\\left\\\\{ \\\\begin{matrix} {A\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A\\\\lbrack i\\\\rbrack}^{T} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ {A\\\\lbrack i\\\\rbrack}^{H} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Note \\\\(\\\\textbf{y}\\\\lbrack i\\\\rbrack\\\\) matrices must not overlap, i.e. Note In the table below, we use A[i], x[i], y[i] as notation for A matrix, and x and y vectors in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, stridex, stridey away from A[i-1], x[i-1], y[i-1] .'},\n",
       " {'id': 796,\n",
       "  'content': 'The unit for the offset is number of elements and must not be zero . A device input * pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x n with lda>=max(1,m) . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] x device input * pointer to the x vector corresponding to the first instance of the batch, with each dimension n if trans==CUBLAS_OP_N and m otherwise. stridex input Value of type long long int that gives the offset in number of elements between x[i] and x[i+1] beta host or device input scalar used for multiplication. y device in/out * pointer to the y vector corresponding to the first instance of the batch, with each dimension m if trans==CUBLAS_OP_N and n otherwise. stridey input Value of type long long int that gives the offset in number of elements between y[i] and y[i+1] batchCount input number of GEMVs to perform in the batch. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,batchCountgemm() \\uf0c1 cublasStatus_t cublasSgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) cublasStatus_t cublasHgemm ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * A , int lda , const __half * B , int ldb , const __half * beta , __half * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B) + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) are matrices stored in column-major format with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(m \\\\times k\\\\) , \\\\(\\\\text{op}(B)\\\\) \\\\(k \\\\times n\\\\) and \\\\(C\\\\) \\\\(m \\\\times n\\\\) , respectively.'},\n",
       " {'id': 797,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) and \\\\(\\\\text{op}(B)\\\\) is defined similarly for matrix \\\\(B\\\\) . transa input operation op( A ) that is non- or (conj.)\\ntransb input operation op( B ) that is non- or (conj.)\\nm input number of rows of matrix op( A ) and C . n input number of columns of matrix op( B ) and C . k input number of columns of op( A ) and rows of op( B ). A device input array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store the matrix A . B device input array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. ldb input leading dimension of two-dimensional array used to store matrix B . If beta==0 , C does not have to be a valid input. C device in/out array of dimensions ldc x n with ldc>=max(1,m) . ldc input leading dimension of a two-dimensional array used to store the matrix C . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k gemm3m() \\uf0c1 cublasStatus_t cublasCgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the complex matrix-matrix multiplication, using Gauss complexity reduction algorithm.'},\n",
       " {'id': 798,\n",
       "  'content': 'This can lead to an increase in performance up to 25% \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B) + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) are matrices stored in column-major format with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(m \\\\times k\\\\) , \\\\(\\\\text{op}(B)\\\\) \\\\(k \\\\times n\\\\) and \\\\(C\\\\) \\\\(m \\\\times n\\\\) , respectively. Note These 2 routines are only supported on GPUs with architecture capabilities equal to or greater than 5.0 Param. Memory In/out Meaning handle input Handle to the cuBLAS library context. transa input Operation op( A ) that is non- or (conj.)\\ntransb input Operation op( B ) that is non- or (conj.)\\nm input Number of rows of matrix op( A ) and C . n input Number of columns of matrix op( B ) and C . k input Number of columns of op( A ) and rows of op( B ). lda input Leading dimension of two-dimensional array used to store the matrix A . ldb input Leading dimension of two-dimensional array used to store matrix B . ldc input Leading dimension of a two-dimensional array used to store the matrix C . The possible error values returned by this function and their meanings are listed in the following table: Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The library was not initialized. CUBLAS_STATUS_INVALID_VALUE If m , n , k gemmBatched() \\uf0c1 cublasStatus_t cublasHgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * const Aarray [], int lda , const __half * const Barray [], int ldb , const __half * beta , __half * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasSgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * const Aarray [], int lda , const float * const Barray [], int ldb , const float * beta , float * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasDgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * const Aarray [], int lda , const double * const Barray [], int ldb , const double * beta , double * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasCgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * const Aarray [], int lda , const cuComplex * const Barray [], int ldb , const cuComplex * beta , cuComplex * const Carray [], int ldc , int batchCount ) cublasStatus_t cublasZgemmBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * const Aarray [], int lda , const cuDoubleComplex * const Barray [], int ldb , const cuDoubleComplex * beta , cuDoubleComplex * const Carray [], int ldc , int batchCount ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication of a batch of matrices.'},\n",
       " {'id': 799,\n",
       "  'content': 'all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices. The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller. \\\\(C\\\\lbrack i\\\\rbrack = \\\\alpha\\\\text{op}(A\\\\lbrack i\\\\rbrack)\\\\text{op}(B\\\\lbrack i\\\\rbrack) + \\\\beta C\\\\lbrack i\\\\rbrack,\\\\text{ for i } \\\\in \\\\lbrack 0,batchCount - 1\\\\rbrack\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) are arrays of pointers to matrices stored in column-major format with dimensions \\\\(\\\\text{op}(A\\\\lbrack i\\\\rbrack)\\\\) \\\\(m \\\\times k\\\\) , \\\\(\\\\text{op}(B\\\\lbrack i\\\\rbrack)\\\\) \\\\(k \\\\times n\\\\) and \\\\(C\\\\lbrack i\\\\rbrack\\\\) \\\\(m \\\\times n\\\\) , respectively. Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) and \\\\(\\\\text{op}(B\\\\lbrack i\\\\rbrack)\\\\) is defined similarly for matrix \\\\(B\\\\lbrack i\\\\rbrack\\\\) . Note \\\\(C\\\\lbrack i\\\\rbrack\\\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. On certain problem sizes, it might be advantageous to make multiple calls to cublasgemm in different CUDA streams, rather than use this API. transa input operation op( A[i] ) that is non- or (conj.)\\ntransb input operation op( B[i] ) that is non- or (conj.)\\nm input number of rows of matrix op( A[i] ) and C[i] .'},\n",
       " {'id': 800,\n",
       "  'content': 'n input number of columns of op( B[i] ) and C[i] . k input number of columns of op( A[i] ) and rows of op( B[i] ). lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Barray device input array of pointers to array, with each array of dim. ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise. ldb input leading dimension of two-dimensional array used to store each matrix B[i] . If beta == 0 , C does not have to be a valid input. Carray device in/out array of pointers to array. It has dimensions ldc x n with ldc>=max(1,m) . Matrices C[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix C[i] . batchCount input number of pointers contained in Aarray, Barray and Carray. If math mode enables fast math modes when using cublasSgemmBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Otherwise it is recommended that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k , batchCount gemmStridedBatched() \\uf0c1 cublasStatus_t cublasHgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const __half * alpha , const __half * A , int lda , long long int strideA , const __half * B , int ldb , long long int strideB , const __half * beta , __half * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasSgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const float * A , int lda , long long int strideA , const float * B , int ldb , long long int strideB , const float * beta , float * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasDgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , long long int strideA , const double * B , int ldb , long long int strideB , const double * beta , double * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasCgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * B , int ldb , long long int strideB , const cuComplex * beta , cuComplex * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasCgemm3mStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , long long int strideA , const cuComplex * B , int ldb , long long int strideB , const cuComplex * beta , cuComplex * C , int ldc , long long int strideC , int batchCount ) cublasStatus_t cublasZgemmStridedBatched ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , long long int strideA , const cuDoubleComplex * B , int ldb , long long int strideB , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc , long long int strideC , int batchCount ) This function supports the 64-bit Integer Interface . Input matrices A, B and output matrix C for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance.'},\n",
       " {'id': 801,\n",
       "  'content': 'Pointers to A, B and C matrices for the first instance are passed to the function by the user along with offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances. \\\\(C + i*{strideC} = \\\\alpha\\\\text{op}(A + i*{strideA})\\\\text{op}(B + i*{strideB}) + \\\\beta(C + i*{strideC}),\\\\text{ for i } \\\\in \\\\lbrack 0,batchCount - 1\\\\rbrack\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) are arrays of pointers to matrices stored in column-major format with dimensions \\\\(\\\\text{op}(A\\\\lbrack i\\\\rbrack)\\\\) \\\\(m \\\\times k\\\\) , \\\\(\\\\text{op}(B\\\\lbrack i\\\\rbrack)\\\\) \\\\(k \\\\times n\\\\) and \\\\(C\\\\lbrack i\\\\rbrack\\\\) \\\\(m \\\\times n\\\\) , respectively. Note \\\\(C\\\\lbrack i\\\\rbrack\\\\) matrices must not overlap, i.e. the individual gemm operations must be computable independently; otherwise, undefined behavior is expected. Note In the table below, we use A[i], B[i], C[i] as notation for A, B and C matrices in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, strideB, strideC away from A[i-1], B[i-1], C[i-1] . A device input * pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] B device input * pointer to the B matrix corresponding to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise. strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] beta host or device input scalar used for multiplication. C device in/out * pointer to the C matrix corresponding to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) . strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] batchCount input number of GEMMs to perform in the batch. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k , batchCount gemmGroupedBatched() \\uf0c1 cublasStatus_t cublasSgemmGroupedBatched ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const float alpha_array [], const float * const Aarray [], const int lda_array [], const float * const Barray [], const int ldb_array [], const float beta_array [], float * const Carray [], const int ldc_array [], int group_count , const int group_size []) cublasStatus_t cublasDgemmGroupedBatched ( cublasHandle_t handle , const cublasOperation_t transa_array [], const cublasOperation_t transb_array [], const int m_array [], const int n_array [], const int k_array [], const double alpha_array [], const double * const Aarray [], const int lda_array [], const double * const Barray [], const int ldb_array [], const double beta_array [], double * const Carray [], const int ldc_array [], int group_count , const int group_size []) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication on groups of matrices.'},\n",
       " {'id': 802,\n",
       "  'content': 'A given group is considered to be “uniform”, i.e. However, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups. This is functionally equivalent to the following: idx = 0 ; for i = 0 : group_count - 1 for j = 0 : group_size [ i ] - 1 gemm ( transa_array [ i ], transb_array [ i ], m_array [ i ], n_array [ i ], k_array [ i ], alpha_array [ i ], Aarray [ idx ], lda_array [ i ], Barray [ idx ], ldb_array [ i ], beta_array [ i ], Carray [ idx ], ldc_array [ i ]); idx += 1 ; end end where \\\\(\\\\text{$\\\\mathrm{alpha\\\\_array}$}\\\\) and \\\\(\\\\text{$\\\\mathrm{beta\\\\_array}$}\\\\) are arrays of scaling factors, and \\\\(\\\\text{Aarray}\\\\) , \\\\(\\\\text{Barray}\\\\) and \\\\(\\\\text{Carray}\\\\) are arrays of pointers to matrices stored in column-major format. For a given index, \\\\(\\\\text{idx}\\\\) , that is part of group \\\\(i\\\\) , the dimensions are: \\\\(\\\\text{op}(\\\\text{Aarray}\\\\lbrack\\\\text{idx}\\\\rbrack)\\\\) : \\\\(\\\\text{$\\\\mathrm{m\\\\_array}$}\\\\lbrack i\\\\rbrack \\\\times \\\\text{$\\\\mathrm{k\\\\_array}$}\\\\lbrack i\\\\rbrack\\\\) \\\\(\\\\text{op}(\\\\text{Barray}\\\\lbrack\\\\text{idx}\\\\rbrack)\\\\) : \\\\(\\\\text{$\\\\mathrm{k\\\\_array}$}\\\\lbrack i\\\\rbrack \\\\times \\\\text{$\\\\mathrm{n\\\\_array}$}\\\\lbrack i\\\\rbrack\\\\) \\\\(\\\\text{Carray}\\\\lbrack\\\\text{idx}\\\\rbrack\\\\) : \\\\(\\\\text{$\\\\mathrm{m\\\\_array}$}\\\\lbrack i\\\\rbrack \\\\times \\\\text{$\\\\mathrm{n\\\\_array}$}\\\\lbrack i\\\\rbrack\\\\) Note This API takes arrays of two different lengths. The arrays of dimensions, leading dimensions, transpositions, and scaling factors are of length group_count and the arrays of matrices are of length problem_count where \\\\(\\\\text{$\\\\mathrm{problem\\\\_count}$} = \\\\sum_{i = 0}^{\\\\text{$\\\\mathrm{group\\\\_count}$} - 1} \\\\text{$\\\\mathrm{group\\\\_size}$}\\\\lbrack i\\\\rbrack\\\\) For matrix \\\\(A[\\\\text{idx}]\\\\) in group \\\\(i\\\\) \\\\(\\\\text{op}(A[\\\\text{idx}]) = \\\\left\\\\{ \\\\begin{matrix} A[\\\\text{idx}] & {\\\\text{if }\\\\textsf{$\\\\mathrm{transa\\\\_array}\\\\lbrack i\\\\rbrack$ == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A[\\\\text{idx}]^{T} & {\\\\text{if }\\\\textsf{$\\\\mathrm{transa\\\\_array}\\\\lbrack i\\\\rbrack$ == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A[\\\\text{idx}]^{H} & {\\\\text{if }\\\\textsf{$\\\\mathrm{transa\\\\_array}\\\\lbrack i\\\\rbrack$ == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) and \\\\(\\\\text{op}(B[\\\\text{idx}])\\\\) is defined similarly for matrix \\\\(B[\\\\text{idx}]\\\\) in group \\\\(i\\\\) . Note \\\\(C\\\\lbrack\\\\text{idx}\\\\rbrack\\\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected.'},\n",
       " {'id': 803,\n",
       "  'content': 'On certain problem sizes, it might be advantageous to make multiple calls to cublasgemmBatched in different CUDA streams, rather than use this API. Memory In/out Meaning Array Length handle input handle to the cuBLAS library context. transa_array host input array containing the operations, op( A[idx] ), that is non- or (conj.)\\ntranspose for each group. group_count transb_array host input array containing the operations, op( B[idx] ), that is non- or (conj.)\\ngroup_count m_array host input array containing the number of rows of matrix op( A[idx] ) and C[idx] for each group. group_count n_array host input array containing the number of columns of op( B[idx] ) and C[idx] for each group. group_count k_array host input array containing the number of columns of op( A[idx] ) and rows of op( B[idx] ) for each group. group_count alpha_array host input array containing the scalar used for multiplication for each group. group_count Aarray device input array of pointers to array, with each array of dim. lda[i] x k[i] with lda[i]>=max(1,m[i]) if transa[i]==CUBLAS_OP_N and lda[i] x m[i] with lda[i]>=max(1,k[i]) otherwise. problem_count lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group. group_count Barray device input array of pointers to array, with each array of dim. ldb[i] x n[i] with ldb[i]>=max(1,k[i]) if transb[i]==CUBLAS_OP_N and ldb[i] x k[i] with ldb[i]>=max(1,n[i]) otherwise. problem_count ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group. group_count beta_array host input array containing the scalar used for multiplication for each group. group_count Carray device in/out array of pointers to array. It has dimensions ldc[i] x n[i] with ldc[i]>=max(1,m[i]) . Matrices C[idx] should not overlap; otherwise, undefined behavior is expected. problem_count ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group. group_count group_count host input number of groups group_size host input array containg the number of pointers contained in Aarray, Barray and Carray for each group. group_count If math mode enables fast math modes when using cublasSgemmGroupedBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Otherwise it is required that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count symm() \\uf0c1 cublasStatus_t cublasSsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsymm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {\\\\alpha AB + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {\\\\alpha BA + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a symmetric matrix stored in lower or upper mode, \\\\(B\\\\) and \\\\(C\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars.'},\n",
       " {'id': 804,\n",
       "  'content': 'side input indicates if matrix A is on the left or right of B . m input number of rows of matrix C and B , with matrix A sized accordingly. n input number of columns of matrix C and B , with matrix A sized accordingly. A device input array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. B device input array of dimension ldb x n with ldb>=max(1,m) . beta host or device input scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C device in/out array of dimension ldc x n with ldc>=max(1,m) . ldc input leading dimension of two-dimensional array used to store matrix C . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n syrk() \\uf0c1 cublasStatus_t cublasSsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyrk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank- \\\\(k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(A)^{T} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a symmetric matrix stored in lower or upper mode, and \\\\(A\\\\) is a matrix with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) .'},\n",
       " {'id': 805,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Param. uplo input indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements. trans input operation op( A ) that is non- or transpose. n input number of rows of matrix op( A ) and C . k input number of columns of matrix op( A ). A device input array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. lda input leading dimension of two-dimensional array used to store matrix A. beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input. C device in/out array of dimension ldc x n , with ldc>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k syr2k() \\uf0c1 cublasStatus_t cublasSsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyr2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the symmetric rank- \\\\(2k\\\\) update \\\\(C = \\\\alpha(\\\\text{op}(A)\\\\text{op}(B)^{T} + \\\\text{op}(B)\\\\text{op}(A)^{T}) + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a symmetric matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively.'},\n",
       " {'id': 806,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) and \\\\(B\\\\) \\\\(\\\\text{op(}A\\\\text{) and op(}B\\\\text{)} = \\\\left\\\\{ \\\\begin{matrix} {A\\\\text{ and }B} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A^{T}\\\\text{ and }B^{T}} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Param. uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements. n input number of rows of matrix op( A ), op( B ) and C . k input number of columns of matrix op( A ) and op( B ). A device input array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise. B device input array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. beta host or device input scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C device in/out array of dimensions ldc x n with ldc>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k syrkx() \\uf0c1 cublasStatus_t cublasSsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasDsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasCsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZsyrkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs a variation of the symmetric rank- \\\\(k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B)^{T} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a symmetric matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively.'},\n",
       " {'id': 807,\n",
       "  'content': 'Also, for matrices \\\\(A\\\\) and \\\\(B\\\\) \\\\(\\\\text{op(}A\\\\text{) and op(}B\\\\text{)} = \\\\left\\\\{ \\\\begin{matrix} {A\\\\text{ and }B} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A^{T}\\\\text{ and }B^{T}} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric. A usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasdgmm . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n , k trmm() \\uf0c1 cublasStatus_t cublasStrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * A , int lda , const float * B , int ldb , float * C , int ldc ) cublasStatus_t cublasDtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * A , int lda , const double * B , int ldb , double * C , int ldc ) cublasStatus_t cublasCtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , cuComplex * C , int ldc ) cublasStatus_t cublasZtrmm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the triangular matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {\\\\alpha\\\\text{op}(A)B} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {\\\\alpha B\\\\text{op}(A)} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\\\(B\\\\) and \\\\(C\\\\) are \\\\(m \\\\times n\\\\) matrix, and \\\\(\\\\alpha\\\\) is a scalar.'},\n",
       " {'id': 808,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Notice that in order to achieve better parallelism cuBLAS differs from the BLAS API only for this routine. The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLAS API assumes an out-of-place implementation (with results written into C). The application can obtain the in-place functionality of BLAS in the cuBLAS API by passing the address of the matrix B in place of the matrix C. No other overlapping in the input parameters is supported. m input number of rows of matrix B , with matrix A sized accordingly. n input number of columns of matrix B , with matrix A sized accordingly. alpha host or device input scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n trsm() \\uf0c1 cublasStatus_t cublasStrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * A , int lda , float * B , int ldb ) cublasStatus_t cublasDtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * A , int lda , double * B , int ldb ) cublasStatus_t cublasCtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , cuComplex * B , int ldb ) cublasStatus_t cublasZtrsm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb ) This function supports the 64-bit Integer Interface . This function solves the triangular linear system with multiple right-hand-sides \\\\(\\\\left\\\\{ \\\\begin{matrix} {\\\\text{op}(A)X = \\\\alpha B} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {X\\\\text{op}(A) = \\\\alpha B} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\\\(X\\\\) and \\\\(B\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) is a scalar.'},\n",
       " {'id': 809,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) The solution \\\\(X\\\\) overwrites the right-hand-sides \\\\(B\\\\) on exit. side input indicates if matrix A is on the left or right of X .'},\n",
       " {'id': 810,\n",
       "  'content': 'n input number of columns of matrix B , with matrix A is sized accordingly. B device in/out array. It has dimensions ldb x n with ldb>=max(1,m) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m trsmBatched() \\uf0c1 cublasStatus_t cublasStrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const float * alpha , const float * const A [], int lda , float * const B [], int ldb , int batchCount ); cublasStatus_t cublasDtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const double * alpha , const double * const A [], int lda , double * const B [], int ldb , int batchCount ); cublasStatus_t cublasCtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuComplex * alpha , const cuComplex * const A [], int lda , cuComplex * const B [], int ldb , int batchCount ); cublasStatus_t cublasZtrsmBatched ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * const A [], int lda , cuDoubleComplex * const B [], int ldb , int batchCount ); This function supports the 64-bit Integer Interface . This function solves an array of triangular linear systems with multiple right-hand-sides \\\\(\\\\left\\\\{ \\\\begin{matrix} {\\\\text{op}(A\\\\lbrack i\\\\rbrack)X\\\\lbrack i\\\\rbrack = \\\\alpha B\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {X\\\\lbrack i\\\\rbrack\\\\text{op}(A\\\\lbrack i\\\\rbrack) = \\\\alpha B\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\lbrack i\\\\rbrack\\\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\\\(X\\\\lbrack i\\\\rbrack\\\\) and \\\\(B\\\\lbrack i\\\\rbrack\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) is a scalar.'},\n",
       " {'id': 811,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A\\\\lbrack i\\\\rbrack) = \\\\left\\\\{ \\\\begin{matrix} {A\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A^{T}\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ {A^{H}\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) The solution \\\\(X\\\\lbrack i\\\\rbrack\\\\) overwrites the right-hand-sides \\\\(B\\\\lbrack i\\\\rbrack\\\\) on exit. This function works for any sizes but is intended to be used for matrices of small sizes where the launch overhead is a significant factor.'},\n",
       " {'id': 812,\n",
       "  'content': 'For bigger sizes, it might be advantageous to call batchCount times the regular cublastrsm within a set of CUDA streams. The current implementation is limited to devices with compute capability above or equal 2.0.'},\n",
       " {'id': 813,\n",
       "  'content': 'side input indicates if matrix A[i] is on the left or right of X[i] . uplo input indicates if matrix A[i] lower or upper part is stored, the other part is not referenced and is inferred from the stored elements. diag input indicates if the elements on the main diagonal of matrix A[i] are unity and should not be accessed. m input number of rows of matrix B[i] , with matrix A[i] sized accordingly. n input number of columns of matrix B[i] , with matrix A[i] is sized accordingly. alpha host or device input scalar used for multiplication, if alpha==0 then A[i] is not referenced and B[i] does not have to be a valid input. A device input array of pointers to array, with each array of dim. lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. lda input leading dimension of two-dimensional array used to store matrix A[i] . B device in/out array of pointers to array, with each array of dim. ldb x n with ldb>=max(1,m) . Matrices B[i] should not overlap; otherwise, undefined behavior is expected. ldb input leading dimension of two-dimensional array used to store matrix B[i] . batchCount input number of pointers contained in A and B. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m hemm() \\uf0c1 cublasStatus_t cublasChemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZhemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {\\\\alpha AB + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {\\\\alpha BA + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a Hermitian matrix stored in lower or upper mode, \\\\(B\\\\) and \\\\(C\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars.'},\n",
       " {'id': 814,\n",
       "  'content': 'A device input array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. beta input scalar used for multiplication, if beta==0 then C does not have to be a valid input. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m herk() \\uf0c1 cublasStatus_t cublasCherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank- \\\\(k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(A)^{H} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a Hermitian matrix stored in lower or upper mode, and \\\\(A\\\\) is a matrix with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) . Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Param. uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her2k() \\uf0c1 cublasStatus_t cublasCher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the Hermitian rank- \\\\(2k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B)^{H} + \\\\overset{ˉ}{\\\\alpha}\\\\text{op}(B)\\\\text{op}(A)^{H} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a Hermitian matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively.'},\n",
       " {'id': 815,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) and \\\\(B\\\\) \\\\(\\\\text{op(}A\\\\text{) and op(}B\\\\text{)} = \\\\left\\\\{ \\\\begin{matrix} {A\\\\text{ and }B} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A^{H}\\\\text{ and }B^{H}} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Param. B device input array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n herkx() \\uf0c1 cublasStatus_t cublasCherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs a variation of the Hermitian rank- \\\\(k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B)^{H} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a Hermitian matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively.'},\n",
       " {'id': 816,\n",
       "  'content': 'Also, for matrix \\\\(A\\\\) and \\\\(B\\\\) \\\\(\\\\text{op(}A\\\\text{) and op(}B\\\\text{)} = \\\\left\\\\{ \\\\begin{matrix} {A\\\\text{ and }B} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A^{H}\\\\text{ and }B^{H}} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) This routine can be used when the matrix B is in such way that the result is guaranteed to be hermitian. An usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix. beta host or device input real scalar used for multiplication, if beta==0 then C does not have to be a valid input. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n geam() \\uf0c1 cublasStatus_t cublasSgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const float * alpha , const float * A , int lda , const float * beta , const float * B , int ldb , float * C , int ldc ) cublasStatus_t cublasDgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const double * alpha , const double * A , int lda , const double * beta , const double * B , int ldb , double * C , int ldc ) cublasStatus_t cublasCgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , const cuComplex * B , int ldb , cuComplex * C , int ldc ) cublasStatus_t cublasZgeam ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , const cuDoubleComplex * B , int ldb , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix addition/transposition \\\\(C = \\\\alpha\\\\text{op}(A) + \\\\beta\\\\text{op}(B)\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) are matrices stored in column-major format with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(m \\\\times n\\\\) , \\\\(\\\\text{op}(B)\\\\) \\\\(m \\\\times n\\\\) and \\\\(C\\\\) \\\\(m \\\\times n\\\\) , respectively.'},\n",
       " {'id': 817,\n",
       "  'content': 'The operation is out-of-place if C does not overlap A or B. The in-place mode supports the following two operations, \\\\(C = \\\\alpha\\\\text{*}C + \\\\beta\\\\text{op}(B)\\\\) \\\\(C = \\\\alpha\\\\text{op}(A) + \\\\beta\\\\text{*}C\\\\) For in-place mode, if C = A , ldc = lda and transa = CUBLAS_OP_N . If C = B , ldc = ldb and transb = CUBLAS_OP_N . If the user does not meet above requirements, CUBLAS_STATUS_INVALID_VALUE is returned. The operation includes the following special cases: the user can reset matrix C to zero by setting *alpha=*beta=0 . the user can transpose matrix A by setting *alpha=1 and *beta=0 . If *alpha == 0 , A does not have to be a valid input. A device input array of dimensions lda x n with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,n) otherwise. B device input array of dimension ldb x n with ldb>=max(1,m) if transb == CUBLAS_OP_N and ldb x m with ldb>=max(1,n) otherwise. If *beta == 0 , B does not have to be a valid input. C device output array of dimensions ldc x n with ldc>=max(1,m) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m dgmm() \\uf0c1 cublasStatus_t cublasSdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const float * A , int lda , const float * x , int incx , float * C , int ldc ) cublasStatus_t cublasDdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const double * A , int lda , const double * x , int incx , double * C , int ldc ) cublasStatus_t cublasCdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const cuComplex * A , int lda , const cuComplex * x , int incx , cuComplex * C , int ldc ) cublasStatus_t cublasZdgmm ( cublasHandle_t handle , cublasSideMode_t mode , int m , int n , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface . This function performs the matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {A \\\\times diag(X)} & {\\\\text{if }\\\\textsf{mode == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ {diag(X) \\\\times A} & {\\\\text{if }\\\\textsf{mode == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) and \\\\(C\\\\) are matrices stored in column-major format with dimensions \\\\(m \\\\times n\\\\) .'},\n",
       " {'id': 818,\n",
       "  'content': '\\\\(X\\\\) is a vector of size \\\\(n\\\\) if mode == CUBLAS_SIDE_RIGHT and of size \\\\(m\\\\) if mode == CUBLAS_SIDE_LEFT . \\\\(X\\\\) is gathered from one-dimensional array x with stride incx . The absolute value of incx is the stride and the sign of incx is direction of the stride. If incx is positive, then we forward x from the first element.'},\n",
       " {'id': 819,\n",
       "  'content': 'Otherwise, we backward x from the last element. The formula of X is \\\\(X\\\\lbrack j\\\\rbrack = \\\\left\\\\{ \\\\begin{matrix} {x\\\\lbrack j \\\\times incx\\\\rbrack} & {\\\\text{if }incx \\\\geq 0} \\\\\\\\ {x\\\\lbrack(\\\\chi - 1) \\\\times |incx| - j \\\\times |incx|\\\\rbrack} & {\\\\text{if }incx geam() with *beta=0 and transa == CUBLAS_OP_N or cublasdgmm() with incx=0 and x[0]=alpha . The operation is out-of-place. The in-place only works if lda = ldc . mode input left multiply if mode == CUBLAS_SIDE_LEFT or right multiply if mode == CUBLAS_SIDE_RIGHT m input number of rows of matrix A and C . n input number of columns of matrix A and C . A device input array of dimensions lda x n with lda>=max(1,m) lda input leading dimension of two-dimensional array used to store the matrix A . x device input one-dimensional array of size \\\\(|inc| \\\\times m\\\\) if mode == CUBLAS_SIDE_LEFT and \\\\(|inc| \\\\times n\\\\) if mode == CUBLAS_SIDE_RIGHT incx input stride of one-dimensional array x . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m getrfBatched() \\uf0c1 cublasStatus_t cublasSgetrfBatched ( cublasHandle_t handle , int n , float * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasDgetrfBatched ( cublasHandle_t handle , int n , double * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasCgetrfBatched ( cublasHandle_t handle , int n , cuComplex * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); cublasStatus_t cublasZgetrfBatched ( cublasHandle_t handle , int n , cuDoubleComplex * const Aarray [], int lda , int * PivotArray , int * infoArray , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format with dimensions nxn and leading dimension lda . This function performs the LU factorization of each Aarray[i] for i = 0, …, batchSize-1 by the following equation \\\\(\\\\text{P}\\\\text{*}{Aarray}\\\\lbrack i\\\\rbrack = L\\\\text{*}U\\\\) where P is a permutation matrix which represents partial pivoting with row interchanges.'},\n",
       " {'id': 820,\n",
       "  'content': 'L is a lower triangular matrix with unit diagonal and U is an upper triangular matrix.'},\n",
       " {'id': 821,\n",
       "  'content': 'Formally P is written by a product of permutation matrices Pj , for j = 1,2,...,n , say P = P1 * P2 * P3 * .... * Pn . Pj is a permutation matrix which interchanges two rows of vector x when performing Pj*x . Pj can be constructed by j element of PivotArray[i] by the following Matlab code // In Matlab PivotArray[i] is an array of base-1. // In C, PivotArray[i] is base-0. Pj = eye ( n ); swap Pj ( j , : ) and Pj ( PivotArray [ i ][ j ] , : ) L and U are written back to original matrix A , and diagonal elements of L are discarded. The L and U can be constructed by the following Matlab code // A is a matrix of nxn after getrf. L = eye ( n ); for j = 1 : n L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : n U ( i , i : n ) = A ( i , i : n ) end If matrix A(=Aarray[i]) is singular, getrf still works and the value of info(=infoArray[i]) reports first row index that LU factorization cannot proceed. If info is k , U(k,k) is zero. The equation P*A=L*U still holds, however L and U reconstruction needs different Matlab code as follows: // A is a matrix of nxn after getrf. // info is k, which means U(k,k) is zero.'},\n",
       " {'id': 822,\n",
       "  'content': 'L = eye ( n ); for j = 1 : k -1 L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : k -1 U ( i , i : n ) = A ( i , i : n ) end for i = k : n U ( i , k : n ) = A ( i , k : n ) end This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. cublasgetrfBatched supports non-pivot LU factorization if PivotArray is NULL. cublasgetrfBatched supports arbitrary dimension. cublasgetrfBatched only supports compute capability 2.0 or above. n input number of rows and columns of Aarray[i] . Aarray device input/output array of pointers to array, with each array of dim. n x n with lda>=max(1,n) . Matrices Aarray[i] should not overlap; otherwise, undefined behavior is expected. lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] . PivotArray device output array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If PivotArray is NULL, pivoting is disabled. infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of factorization of Aarray[i] . If info=0, the execution is successful.'},\n",
       " {'id': 823,\n",
       "  'content': 'If info = -j, the j-th parameter had an illegal value. If info = k, U(k,k) is 0. The factorization has been completed, but U is exactly singular. batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,batchSize,lda getrsBatched() \\uf0c1 cublasStatus_t cublasSgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const float * const Aarray [], int lda , const int * devIpiv , float * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasDgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const double * const Aarray [], int lda , const int * devIpiv , double * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasCgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const cuComplex * const Aarray [], int lda , const int * devIpiv , cuComplex * const Barray [], int ldb , int * info , int batchSize ); cublasStatus_t cublasZgetrsBatched ( cublasHandle_t handle , cublasOperation_t trans , int n , int nrhs , const cuDoubleComplex * const Aarray [], int lda , const int * devIpiv , cuDoubleComplex * const Barray [], int ldb , int * info , int batchSize ); This function solves an array of systems of linear equations of the form: \\\\(\\\\text{op}(A\\\\lbrack i \\\\rbrack) X\\\\lbrack i\\\\rbrack = B\\\\lbrack i\\\\rbrack\\\\) where \\\\(A\\\\lbrack i\\\\rbrack\\\\) is a matrix which has been LU factorized with pivoting, \\\\(X\\\\lbrack i\\\\rbrack\\\\) and \\\\(B\\\\lbrack i\\\\rbrack\\\\) are \\\\(n \\\\times {nrhs}\\\\) matrices. Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A\\\\lbrack i\\\\rbrack) = \\\\left\\\\{ \\\\begin{matrix} {A\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A^{T}\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ {A^{H}\\\\lbrack i\\\\rbrack} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor.'},\n",
       " {'id': 824,\n",
       "  'content': 'cublasgetrsBatched supports non-pivot LU factorization if devIpiv is NULL. cublasgetrsBatched supports arbitrary dimension. cublasgetrsBatched only supports compute capability 2.0 or above. nrhs input number of columns of Barray[i] . devIpiv device input array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. If devIpiv is NULL, pivoting for all Aarray[i] is ignored. Barray device input/output array of pointers to array, with each array of dim. n x nrhs with ldb>=max(1,n) . Matrices Barray[i] should not overlap; otherwise, undefined behavior is expected. ldb input leading dimension of two-dimensional array used to store each solution matrix Barray[i] . info host output If info=0, the execution is successful. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n getriBatched() \\uf0c1 cublasStatus_t cublasSgetriBatched ( cublasHandle_t handle , int n , const float * const Aarray [], int lda , int * PivotArray , float * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasDgetriBatched ( cublasHandle_t handle , int n , const double * const Aarray [], int lda , int * PivotArray , double * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasCgetriBatched ( cublasHandle_t handle , int n , const cuComplex * const Aarray [], int lda , int * PivotArray , cuComplex * const Carray [], int ldc , int * infoArray , int batchSize ); cublasStatus_t cublasZgetriBatched ( cublasHandle_t handle , int n , const cuDoubleComplex * const Aarray [], int lda , int * PivotArray , cuDoubleComplex * const Carray [], int ldc , int * infoArray , int batchSize ); Aarray and Carray are arrays of pointers to matrices stored in column-major format with dimensions n*n and leading dimension lda and ldc respectively. This function performs the inversion of matrices A[i] for i = 0, …, batchSize-1 .'},\n",
       " {'id': 825,\n",
       "  'content': 'Prior to calling cublasgetriBatched, the matrix A[i] must be factorized first using the routine cublasgetrfBatched. After the call of cublasgetrfBatched, the matrix pointing by Aarray[i] will contain the LU factors of the matrix A[i] and the vector pointing by (PivotArray+i) will contain the pivoting sequence. Following the LU factorization, cublasgetriBatched uses forward and backward triangular solvers to complete inversion of matrices A[i] for i = 0, …, batchSize-1 . The inversion is out-of-place, so memory space of Carray[i] cannot overlap memory space of Array[i]. Typically all parameters in cublasgetrfBatched would be passed into cublasgetriBatched. For example, // step 1: perform in-place LU decomposition, P*A = L*U. // Aarray[i] is n*n matrix A[i] cublasDgetrfBatched ( handle , n , Aarray , lda , PivotArray , infoArray , batchSize ); // check infoArray[i] to see if factorization of A[i] is successful or not. // Array[i] contains LU factorization of A[i] // step 2: perform out-of-place inversion, Carray[i] = inv(A[i]) cublasDgetriBatched ( handle , n , Aarray , lda , PivotArray , Carray , ldc , infoArray , batchSize ); // check infoArray[i] to see if inversion of A[i] is successful or not. The user can check singularity from either cublasgetrfBatched or cublasgetriBatched. This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor. If cublasgetrfBatched is performed by non-pivoting, PivotArray of cublasgetriBatched should be NULL. cublasgetriBatched supports arbitrary dimension. cublasgetriBatched only supports compute capability 2.0 or above. Aarray device input array of pointers to array, with each array of dimension n*n with lda>=max(1,n) . PivotArray device output array of size n*batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion. Carray device output array of pointers to array, with each array of dimension n*n with ldc>=max(1,n) . Matrices Carray[i] should not overlap; otherwise, undefined behavior is expected. ldc input leading dimension of two-dimensional array used to store each matrix Carray[i] . infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of inversion of A[i] . The U is exactly singular and the inversion failed. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n matinvBatched() \\uf0c1 cublasStatus_t cublasSmatinvBatched ( cublasHandle_t handle , int n , const float * const A [], int lda , float * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasDmatinvBatched ( cublasHandle_t handle , int n , const double * const A [], int lda , double * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasCmatinvBatched ( cublasHandle_t handle , int n , const cuComplex * const A [], int lda , cuComplex * const Ainv [], int lda_inv , int * info , int batchSize ); cublasStatus_t cublasZmatinvBatched ( cublasHandle_t handle , int n , const cuDoubleComplex * const A [], int lda , cuDoubleComplex * const Ainv [], int lda_inv , int * info , int batchSize ); A and Ainv are arrays of pointers to matrices stored in column-major format with dimensions n*n and leading dimension lda and lda_inv respectively. This function is a short cut of cublasgetrfBatched plus cublasgetriBatched .'},\n",
       " {'id': 826,\n",
       "  'content': 'However it doesn’t work if n is greater than 32. If not, the user has to go through cublasgetrfBatched and cublasgetriBatched . If the matrix A[i] is singular, then info[i] reports singularity, the same as cublasgetrfBatched . n input number of rows and columns of A[i] . A device input array of pointers to array, with each array of dimension n*n with lda>=max(1,n) . Ainv device output array of pointers to array, with each array of dimension n*n with lda_inv>=max(1,n) . Matrices Ainv[i] should not overlap; otherwise, undefined behavior is expected. lda_inv input leading dimension of two-dimensional array used to store each matrix Ainv[i] . info device output array of size batchSize that info[i] contains the information of inversion of A[i] . If info[i]=0, the execution is successful. If info[i]=k, U(k,k) is 0. batchSize input number of pointers contained in A. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n 32 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2.8.7. cublasgeqrfBatched() \\uf0c1 cublasStatus_t cublasSgeqrfBatched ( cublasHandle_t handle , int m , int n , float * const Aarray [], int lda , float * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasDgeqrfBatched ( cublasHandle_t handle , int m , int n , double * const Aarray [], int lda , double * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasCgeqrfBatched ( cublasHandle_t handle , int m , int n , cuComplex * const Aarray [], int lda , cuComplex * const TauArray [], int * info , int batchSize ); cublasStatus_t cublasZgeqrfBatched ( cublasHandle_t handle , int m , int n , cuDoubleComplex * const Aarray [], int lda , cuDoubleComplex * const TauArray [], int * info , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format with dimensions m x n and leading dimension lda . TauArray is an array of pointers to vectors of dimension of at least max (1, min(m, n) .'},\n",
       " {'id': 827,\n",
       "  'content': 'This function performs the QR factorization of each Aarray[i] for i = 0, ...,batchSize-1 using Householder reflections. Each matrix Q[i] is represented as a product of elementary reflectors and is stored in the lower part of each Aarray[i] as follows : Q[j] = H[j][1] H[j][2] . .'},\n",
       " {'id': 828,\n",
       "  'content': \"H[j](k), where k = min(m,n). Each H[j][i] has the form H[j][i] = I - tau[j] * v * v' where tau[j] is a real scalar, and v is a real vector with v(1:i-1) = 0 and v(i) = 1 ; v(i+1:m) is stored on exit in Aarray[j][i+1:m,i] , and tau in TauArray[j][i] . cublasgeqrfBatched supports arbitrary dimension. cublasgeqrfBatched only supports compute capability 2.0 or above. m input number of rows Aarray[i] .\"},\n",
       " {'id': 829,\n",
       "  'content': 'n input number of columns of Aarray[i] . m x n with lda>=max(1,m) . TauArray device output array of pointers to vector, with each vector of dim. max(1,min(m,n)) . info host output If info=0, the parameters passed to the function are valid If infogelsBatched() \\uf0c1 cublasStatus_t cublasSgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , float * const Aarray [], int lda , float * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasDgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , double * const Aarray [], int lda , double * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasCgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , cuComplex * const Aarray [], int lda , cuComplex * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); cublasStatus_t cublasZgelsBatched ( cublasHandle_t handle , cublasOperation_t trans , int m , int n , int nrhs , cuDoubleComplex * const Aarray [], int lda , cuDoubleComplex * const Carray [], int ldc , int * info , int * devInfoArray , int batchSize ); Aarray is an array of pointers to matrices stored in column-major format. Carray is an array of pointers to matrices stored in column-major format.'},\n",
       " {'id': 830,\n",
       "  'content': 'This function find the least squares solution of a batch of overdetermined systems: it solves the least squares problem described as follows : minimize || Carray [ i ] - Aarray [ i ] * Xarray [ i ] || , with i = 0 , ..., batchSize -1 On exit, each Aarray[i] is overwritten with their QR factorization and each Carray[i] is overwritten with the least square solution cublasgelsBatched supports only the non-transpose operation and only solves over-determined systems (m >= n). cublasgelsBatched only supports compute capability 2.0 or above. trans input operation op( Aarray[i] ) that is non- or (conj.)\\nOnly non-transpose operation is currently supported. m input number of rows of each Aarray[i] and Carray[i] if trans == CUBLAS_OP_N , numbers of columns of each Aarray[i] otherwise (not supported currently). n input number of columns of each Aarray[i] if trans == CUBLAS_OP_N , and number of rows of each Aarray[i] and Carray[i] otherwise (not supported currently). nrhs input number of columns of each Carray[i] . m x n with lda>=max(1,m) if trans == CUBLAS_OP_N , and n x m with lda>=max(1,n) otherwise (not supported currently). Carray device input/output array of pointers to array, with each array of dim. m x nrhs with ldc>=max(1,m) if trans == CUBLAS_OP_N , and n x nrhs with lda>=max(1,n) otherwise (not supported currently). info host output If info=0, the parameters passed to the function are valid If info 0 : the V-th diagonal element of the Aarray[i] is zero. Aarray[i] does not have full rank. batchSize input number of pointers contained in Aarray and Carray The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m tpttr() \\uf0c1 cublasStatus_t cublasStpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * AP , float * A , int lda ); cublasStatus_t cublasDtpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * AP , double * A , int lda ); cublasStatus_t cublasCtpttr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * AP , cuComplex * A , int lda ); cublasStatus_t cublasZtpttr ( cublasHandle_t handle , cublasFillMode_t uplo int n , const cuDoubleComplex * AP , cuDoubleComplex * A , int lda ); This function performs the conversion from the triangular packed format to the triangular format If uplo == CUBLAS_FILL_MODE_LOWER then the elements of AP are copied into the lower triangular part of the triangular matrix A and the upper part of A is left untouched. If uplo == CUBLAS_FILL_MODE_UPPER then the elements of AP are copied into the upper triangular part of the triangular matrix A and the lower part of A is left untouched.'},\n",
       " {'id': 831,\n",
       "  'content': 'uplo input indicates if matrix AP contains lower or upper part of matrix A . A device output array of dimensions lda x n , with lda>=max(1,n) . The opposite side of A is left untouched. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n trttp() \\uf0c1 cublasStatus_t cublasStrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * A , int lda , float * AP ); cublasStatus_t cublasDtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * A , int lda , double * AP ); cublasStatus_t cublasCtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * A , int lda , cuComplex * AP ); cublasStatus_t cublasZtrttp ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * A , int lda , cuDoubleComplex * AP ); This function performs the conversion from the triangular format to the triangular packed format If uplo == CUBLAS_FILL_MODE_LOWER then the lower triangular part of the triangular matrix A is copied into the array AP . If uplo == CUBLAS_FILL_MODE_UPPER then then the upper triangular part of the triangular matrix A is copied into the array AP .'},\n",
       " {'id': 832,\n",
       "  'content': 'uplo input indicates which matrix A lower or upper part is referenced. AP device output array with \\\\(A\\\\) stored in packed format. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n gemmEx() \\uf0c1 cublasStatus_t cublasSgemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const float * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const float * beta , void * C , cudaDataType_t Ctype , int ldc ) cublasStatus_t cublasCgemmEx ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const void * A , cudaDataType_t Atype , int lda , const void * B , cudaDataType_t Btype , int ldb , const cuComplex * beta , void * C , cudaDataType_t Ctype , int ldc ) This function supports the 64-bit Integer Interface . This function is an extension of cublasgemm .'},\n",
       " {'id': 833,\n",
       "  'content': 'In this function the input matrices and output matrices can have a lower precision but the computation is still done in the type . For example, in the type float for cublasSgemmEx() and in the type cuComplex for cublasCgemmEx() . \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B) + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) are matrices stored in column-major format with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(m \\\\times k\\\\) , \\\\(\\\\text{op}(B)\\\\) \\\\(k \\\\times n\\\\) and \\\\(C\\\\) \\\\(m \\\\times n\\\\) , respectively. Atype input enumerant specifying the datatype of matrix A . Btype input enumerant specifying the datatype of matrix B . Ctype input enumerant specifying the datatype of matrix C . The matrix types combinations supported for cublasSgemmEx() are listed below: C A/B CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_8I CUDA_R_16BF CUDA_R_16F CUDA_R_32F The matrix types combinations supported for cublasCgemmEx() are listed below : C A/B CUDA_C_32F CUDA_C_8I CUDA_C_32F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ARCH_MISMATCH cublasCgemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0 CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters Atype , Btype and Ctype is not supported CUBLAS_STATUS_INVALID_VALUE If m gemm that allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run. Supported combinations of arguments are listed further down in this section. Note The second variant of cublasGemmEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t . C applications would still compile with the updated function signature.'},\n",
       " {'id': 834,\n",
       "  'content': 'This function is only supported on devices with compute capability 5.0 or later. alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details. Atype input Enumerant specifying the datatype of matrix A . Btype input Enumerant specifying the datatype of matrix B . beta host or device input Scaling factor for C of the type that corresponds to the computeType and Ctype, see the table below for details. Ctype input Enumerant specifying the datatype of matrix C . computeType input Enumerant specifying the computation type. algo input Enumerant specifying the algorithm. See cublasGemmAlgo_t . cublasGemmEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F Note CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC compute types are only supported with A, B being 4-byte aligned and lda, ldb being multiples of 4. For better performance, it is also recommended that IMMA kernels requirements for a regular data ordering listed here are met.'},\n",
       " {'id': 835,\n",
       "  'content': 'The possible error values returned by this function and their meanings are listed in the following table. Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmEx() is only supported for GPU with architecture capabilities equal or greater than 5.0. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported. CUBLAS_STATUS_INVALID_VALUE If m gemmBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrix arrays, the precision of computation and the GEMM algorithm to be run. Like cublasgemmBatched , the batch is considered to be “uniform”, i.e. Note The second variant of cublasGemmBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t . transa input Operation op( A[i] ) that is non- or (conj.)\\ntransb input Operation op( B[i] ) that is non- or (conj.)\\nm input Number of rows of matrix op( A[i] ) and C[i] .'},\n",
       " {'id': 836,\n",
       "  'content': 'n input Number of columns of matrix op( B[i] ) and C[i] . k input Number of columns of op( A[i] ) and rows of op( B[i] ). Aarray device input Array of pointers to array, with each array of dim. lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of Aarray . lda input Leading dimension of two-dimensional array used to store the matrix A[i] . Barray device input Array of pointers to array, with each array of dim. ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input Enumerant specifying the datatype of Barray . ldb input Leading dimension of two-dimensional array used to store matrix B[i] . If beta==0 , C[i] does not have to be a valid input. Carray device in/out Array of pointers to array. Ctype input Enumerant specifying the datatype of Carray . ldc input Leading dimension of a two-dimensional array used to store each matrix C[i] . batchCount input Number of pointers contained in Aarray, Barray and Carray. cublasGemmBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F If Atype is CUDA_R_16F or CUDA_R_16BF , or computeType is any of the FAST options, or when math mode or algo enable fast math modes, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Otherwise it is recommended that they meet the following rule: if k%8==0 then ensure intptr_t(ptr) % 16 == 0 , if k%2==0 then ensure intptr_t(ptr) % 4 == 0 . Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4. For a better performance, it is also recommended that IMMA kernels requirements for the regular data ordering listed here are met. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal to or greater than 5.0. CUBLAS_STATUS_INVALID_VALUE If m gemmStridedBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run. Like cublasgemmStridedBatched , the batch is considered to be “uniform”, i.e. Pointers to A, B and C matrices for the first instance are passed to the function by the user along with the offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances. Note The second variant of cublasGemmStridedBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType_t instead of cublasComputeType_t . A device input Pointer to matrix, A, corresponds to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. Atype input Enumerant specifying the datatype of A . strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] . B device input Pointer to matrix, B, corresponds to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. Btype input Enumerant specifying the datatype of B . strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] . C device in/out Pointer to matrix, C, corresponds to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m) . Ctype input Enumerant specifying the datatype of C . strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] . batchCount input Number of GEMMs to perform in the batch. cublasGemmStridedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_8I CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4. CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal or greater than 5.0. CUBLAS_STATUS_INVALID_VALUE If m scalar used for multiplication for each group. problem_count Atype input Enumerant specifying the datatype of A . lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group. problem_count Btype input Enumerant specifying the datatype of B . ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group. problem_count Ctype input Enumerant specifying the datatype of C . ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group. group_count computeType input Enumerant specifying the computation type. cublasGemmGroupedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F If Atype is CUDA_R_16F or CUDA_R_16BF or if the computeType is any of the FAST options, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors. Otherwise it is required that they meet the following rule: if (k * AtypeSize) % 16 == 0 then ensure intptr_t(ptr) % 16 == 0 , if (k * AtypeSize) % 4 == 0 then ensure intptr_t(ptr) % 4 == 0 . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count scalar used for multiplication. lda input Leading dimension of two-dimensional array used to store matrix A. beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input. ldc input Leading dimension of two-dimensional array used to store matrix C . The matrix types combinations supported for cublasCsyrkEx() are listed below: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. CUBLAS_STATUS_INVALID_VALUE If n scalar used for multiplication. The matrix types combinations supported for cublasCsyrk3mEx() are listed below : A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. lda input Leading dimension of two-dimensional array used to store matrix A . The matrix types combinations supported for cublasCherkEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. The matrix types combinations supported for cublasCherk3mEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below. CUBLAS_STATUS_INVALID_VALUE If n nrm2 where input data, output data and compute type can be specified independently. xType input enumerant specifying the datatype of vector x . result host or device output the resulting norm, which is 0.0 if n,incxaxpy where input data, output data and compute type can be specified independently. n input Number of elements in the vector x and y . alphaType input Enumerant specifying the datatype of scalar alpha . xType input Enumerant specifying the datatype of vector x . incx input Stride between consecutive elements of x . yType input Enumerant specifying the datatype of vector y . incy input Stride between consecutive elements of y . executionType input Enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasAxpyEx() are listed in the following table: alpha x y execution CUDA_R_32F CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters xType , yType , and executionType is not supported. CUBLAS_STATUS_EXECUTION_FAILED The function failed to launch on the GPU. CUBLAS_STATUS_INVALID_VALUE alphaType or xType or yType or executionType is not supported. For references please refer to: saxpy , daxpy , caxpy , zaxpy 2.8.22. cublasDotEx() \\uf0c1 cublasStatus_t cublasDotEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); cublasStatus_t cublasDotcEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); These functions support the 64-bit Integer Interface . These functions are an API generalization of the routines cublasdot and cublasdotc where input data, output data and compute type can be specified independently. Note: cublasdotc is dot product conjugated, cublasdotu is dot product unconjugated. n input Number of elements in the vectors x and y . result host or device output The resulting dot product, which is 0.0 if nrot where input data, output data, cosine/sine type, and compute type can be specified independently. yType input enumerant specifying the datatype of vector y . csType input enumerant specifying the datatype of c and s . executionType input enumerant specifying the datatype in which the computation is executed. The datatypes combinations currently supported for cublasRotEx() are listed below : executionType xType / yType csType CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_R_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2.8.24. cublasScalEx() \\uf0c1 cublasStatus_t cublasScalEx ( cublasHandle_t handle , int n , const void * alpha , cudaDataType alphaType , void * x , cudaDataType xType , int incx , cudaDataType executionType ); This function supports the 64-bit Integer Interface . alphaType input enumerant specifying the datatype of scalar alpha . The datatypes combinations currently supported for cublasScalEx() are listed below : alpha x execution CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters xType and executionType is not supported CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE alphaType or xType or executionType is not supported For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 3. Using the cuBLASLt API \\uf0c1 3.1. General Description \\uf0c1 The cuBLASLt library is a new lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API. This new library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability. Once a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs. Note The cuBLASLt library does not guarantee the support of all possible sizes and configurations, however, since CUDA 12.2 update 2, the problem size limitations on m, n, and batch size have been largely resolved. The main focus of the library is to provide the most performant kernels, which might have some implied limitations. Some non-standard configurations may require a user to handle them manually, typically by decomposing the problem into smaller parts (see Problem Size Limitations ). 3.1.1. Problem Size Limitations \\uf0c1 There are inherent problem size limitations that are a result of limitations in CUDA grid dimensions. For example, many kernels do not support batch sizes greater than 65535 due to a limitation on the z dimension of a grid. There are similar restriction on the m and n values for a given problem. In cases where a problem cannot be run by a single kernel, cuBLASLt will attempt to decompose the problem into multiple sub-problems and solve it by running the kernel on each sub-problem. There are some restrictions on cuBLASLt internal problem decomposition which are summarized below: Amax computations are not supported. This means that CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER must be left unset (see cublasLtMatmulDescAttributes_t ) All matrix layouts must have CUBLASLT_MATRIX_LAYOUT_ORDER set to CUBLASLT_ORDER_COL (see cublasLtOrder_t ) cuBLASLt will not partition along the n dimension when CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_DRELU_BGRAD or CUBLASLT_EPILOGUE_DGELU_BGRAD (see cublasLtEpilogue_t ) To overcome these limitations, a user may want to partition the problem themself, launch kernels for each sub-problem, and compute any necessary reductions to combine the results. 3.1.2. Heuristics Cache \\uf0c1 cuBLASLt uses heuristics to pick the most suitable matmul kernel for execution based on the problem sizes, GPU configuration, and other parameters. This requires performing some computations on the host CPU, which could take tens of microseconds. To overcome this overhead, it is recommended to query the heuristics once using cublasLtMatmulAlgoGetHeuristic() and then reuse the result for subsequent computations using cublasLtMatmul() . For the cases where querying heuristics once and then reusing them is not feasible, cuBLASLt implements a heuristics cache that maps matmul problems to kernels previously selected by heuristics. The heuristics cache uses an LRU-like eviction policy and is thread-safe. The user can control the heuristics cache capacity with the CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable or with the cublasLtHeuristicsCacheSetCapacity() function which has higher precedence. The capacity is measured in number of entries and might be rounded up to the nearest multiple of some factor for performance reasons.'},\n",
       " {'id': 837,\n",
       "  'content': 'Each entry takes about 360 bytes but is subject to change. The default capacity is 8192 entries. Note Setting capacity to zero disables the cache completely. This can be useful for workloads that do not have a steady state and for which cache operations may have higher overhead than regular heuristics computations. Note The cache is not ideal for performance reasons, so it is sometimes necessary to increase its capacity 1.5x-2.x over the anticipated number of unique matmul problems to achieve a nearly perfect hit rate. See also: cublasLtHeuristicsCacheGetCapacity() , cublasLtHeuristicsCacheSetCapacity() .'},\n",
       " {'id': 838,\n",
       "  'content': '3.1.3. cuBLASLt Logging \\uf0c1 cuBLASLt logging mechanism can be enabled by setting the following environment variables before launching the target application: CUBLASLT_LOG_LEVEL= , where is one of the following levels: “0” - Off - logging is disabled (default) “1” - Error - only errors will be logged “2” - Trace - API calls that launch CUDA kernels will log their parameters and important information “3” - Hints - hints that can potentially improve the application’s performance “4” - Info - provides general information about the library execution, may contain details about heuristic status “5” - API Trace - API calls will log their parameter and important information CUBLASLT_LOG_MASK= , where is a combination of the following flags: “0” - Off “1” - Error “2” - Trace “4” - Hints “8” - Info “16” - API Trace For example, use CUBLASLT_LOG_MASK=5 to enable Error and Hints messages. CUBLASLT_LOG_FILE= , where is a path to a logging file. File name may contain %i , that will be replaced with the process ID. For example _%i.log . If CUBLASLT_LOG_FILE is not defined, the log messages are printed to stdout. Another option is to use the experimental cuBLASLt logging API.'},\n",
       " {'id': 839,\n",
       "  'content': 'See: cublasLtLoggerSetCallback() , cublasLtLoggerSetFile() , cublasLtLoggerOpenFile() , cublasLtLoggerSetLevel() , cublasLtLoggerSetMask() , cublasLtLoggerForceDisable() 3.1.4. 8-bit Floating Point Data Types (FP8) Usage \\uf0c1 FP8 was first introduced with Ada and Hopper GPUs (compute capability 8.9 and above) and is designed to further accelerate matrix multiplications. There are two types of FP8 available: CUDA_R_8F_E4M3 is designed to be accurate at a smaller dynamic range than half precision. The E4 and M3 represent a 4-bit exponent and a 3-bit mantissa respectively. For more details, see __nv__fp8__e4m3 . CUDA_R_8F_E5M2 is designed to be accurate at a similar dynamic range as half precision. The E5 and M2 represent a 5-bit exponent and a 2-bit mantissa respectively. For more information see __nv__fp8__e5m2 . Note Unless otherwise stated, FP8 refers to both CUDA_R_8F_E4M3 and CUDA_R_8F_E5M2 . In order to maintain accurate FP8 matrix multiplications, we define native compute FP8 matrix multiplication as follows: \\\\[D = scale_D \\\\cdot (\\\\alpha \\\\cdot scale_A \\\\cdot scale_B \\\\cdot \\\\text{op}(A) \\\\text{op}(B) + \\\\beta \\\\cdot scale_C \\\\cdot C)\\\\] where A, B, and C are input matrices, and scaleA, scaleB, scaleC, scaleD, alpha, and beta are input scalars. This differs from the other matrix multiplication routines because of this addition of scaling factors for each matrix. The scaleA, scaleB, and scaleC are used for de-quantization, and scaleD is used for quantization. Note that all the scaling factors are applied multiplicatively. This means that sometimes it is necessary to use a scaling factor or its reciprocal depending on the context in which it is applied. For more information on FP8, see cublasLtMatmul() and cublasLtMatmulDescAttributes_t . For FP8 matrix multiplications, epilogues and amaxD may be computed as follows: \\\\[\\\\begin{split}D_{temp}, Aux_{temp} & = \\\\mathop{Epilogue}(\\\\alpha \\\\cdot scale_A \\\\cdot scale_B \\\\cdot \\\\text{op}(A) \\\\text{op}(B) + \\\\beta \\\\cdot scale_C \\\\cdot C) \\\\\\\\ amax_{D} & = \\\\mathop{absmax}(D_{temp}) \\\\\\\\ amax_{Aux} & = \\\\mathop{absmax}(Aux_{temp}) \\\\\\\\ D & = scale_D * D_{temp} \\\\\\\\ Aux & = scale_{Aux} * Aux_{temp} \\\\\\\\\\\\end{split}\\\\] Here Aux is an auxiliary output of an epilogue function like GELU, scaleAux is an optional scaling factor that can be applied to Aux, and amaxAux is the maximum absolute value in Aux before scaling. For more information, see attributes CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER in cublasLtMatmulDescAttributes_t .'},\n",
       " {'id': 840,\n",
       "  'content': '3.1.5. Disabling CPU Instructions \\uf0c1 As mentioned in the Heuristics Cache section, cuBLASLt heuristics perform some compute-intensive operations on the host CPU. To speed-up the operations, the implementation detects CPU capabilities and may use special instructions, such as Advanced Vector Extensions (AVX) on x86-64 CPUs. However, in some rare cases this might be not desirable. For instance, using advanced instructions may result in CPU running at a lower frequency, which would affect performance of the other host code. The user can optionally instruct the cuBLASLt library to not use some CPU instructions with the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable or with the cublasLtDisableCpuInstructionsSetMask() function which has higher precedence. The default mask is 0, meaning that there are no restrictions. Please check cublasLtDisableCpuInstructionsSetMask() for more information.'},\n",
       " {'id': 841,\n",
       "  'content': '3.1.6. Atomics Synchronization \\uf0c1 Atomics synchronization allows optimizing matmul workloads by enabling cublasLtMatmul() to have a producer or consumer relationship with another concurrently running kernel. This allows overlapping computation and communication with a finer granularity. Conceptually, matmul is provided with an array containing 32-bit integer counters, and then: In the consumer mode, either matrix A is partitioned into chunks by rows, or matrix B is partitioned into chunks by columns 1 . A chunk can be read from memory and used in computations only when the corresponding atomic counter reaches value of 0. The producer should execute a memory fence to ensure that the written value is visible to the concurrently running matmul kernel 2 . In the producer mode, the output matrix C (or D in the out-of-place mode), is partitioned by rows or columns, and after a chunk is computed, the corresponding atomic counter is set to 0. Each counter must be initialized to 1 before the matmul kernel runs. 1 The current implementation allows partitioning either the rows or the columns of the matrixes, but not both. Batched cases are not supported. 2 One possible implementation of a memory fence is cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope::thread_scope_device) (see cuda::atomic_thread_fence() for more details). The array of counters are passed to matmuls via the CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER compute descriptor attributes for the consumer and producer modes respectively 3 . The arrays must have a sufficient number of elements for all the chunks. 3 The current implementation allows to only enable either the producer or the consumer mode, but not both. Matmul will return an error if both input and output counter pointers to a non-NULL value. The number of chunks is controlled by CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS compute descriptor attributes. Both of these attributes must be set to a value greater than zero for the feature to be enabled. For the column-major layout, the number of chunks must satisfy: \\\\[\\\\begin{split}0 \\\\leq \\\\text{$\\\\mathrm{NUM\\\\_CHUNKS\\\\_ROWS}$} \\\\leq & \\\\mathop{\\\\text{floor}}\\\\left( \\\\frac{\\\\text{M}}{\\\\text{$\\\\mathrm{TILE\\\\_SIZE\\\\_M}$} * \\\\text{$\\\\mathrm{CLUSTER\\\\_SHAPE\\\\_M}$}} \\\\right) \\\\\\\\ 0 \\\\leq \\\\text{$\\\\mathrm{NUM\\\\_CHUNKS\\\\_COLS}$} \\\\leq & \\\\mathop{\\\\text{floor}}\\\\left( \\\\frac{\\\\text{N}}{\\\\text{$\\\\mathrm{TILE\\\\_SIZE\\\\_N}$} * \\\\text{$\\\\mathrm{CLUSTER\\\\_SHAPE\\\\_N}$}} \\\\right)\\\\end{split}\\\\] For row-major layout, M and N in tile size and cluster shape must be swapped. These restrictions mean that it is required to first query heuristic via cublasLtMatmulAlgoGetHeuristic() and inspect the result for tile and cluster shapes, and only then set the number of chunks. The pseudocode below shows the principles of operation: // The code below shows operation when partitioning over // rows assuming column-major layout and TN case. // // The case when partitioning is done over columns or // row-major case are handled in a similar fashion, // with the main difference being the offsets // computations. // // Note that the actual implementation does not // guarantee in which order the chunks are computed, // and may employ various optimizations to improve // overall performance. // // Here: // - A, B, C -- input matrices in the column-major layout // - lda -- leading dimension of matrix A // - M, N, K -- the original problem dimensions // - counters_in[] and counters_out[] -- the arrays of // input and output atomic counters // for ( int i = 0 ; i 1. Default: CUBLASLT_REDUCTION_SCHEME_NONE .'},\n",
       " {'id': 842,\n",
       "  'content': 'See cublasLtReductionScheme_t . uint32_t CUBLASLT_ALGO_CONFIG_CTA_SWIZZLING Enable/Disable CTA swizzling. Change mapping from CUDA grid coordinates to parts of the matrices. Possible values: 0 and 1; other values reserved. uint32_t CUBLASLT_ALGO_CONFIG_CUSTOM_OPTION Custom option value. Each algorithm can support some custom options that don’t fit the description of the other configuration attributes. See the CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX of cublasLtMatmulAlgoCapAttributes_t for the accepted range for a specific case. uint32_t CUBLASLT_ALGO_CONFIG_INNER_SHAPE_ID Inner shape ID. Refer to cublasLtMatmulInnerShape_t. Default: CUBLASLT_MATMUL_INNER_SHAPE_UNDEFINED . uint16_t CUBLASLT_ALGO_CONFIG_CLUSTER_SHAPE_ID Cluster shape ID. Refer to cublasLtClusterShape_t. Default: CUBLASLT_CLUSTER_SHAPE_AUTO . uint16_t 3.3.8. cublasLtMatmulDesc_t \\uf0c1 The cublasLtMatmulDesc_t is a pointer to an opaque structure holding the description of the matrix multiplication operation cublasLtMatmul() . A descriptor can be created by calling cublasLtMatmulDescCreate() and destroyed by calling cublasLtMatmulDescDestroy() . 3.3.9. cublasLtMatmulDescAttributes_t \\uf0c1 cublasLtMatmulDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix multiply operation. Use cublasLtMatmulDescGetAttribute() and cublasLtMatmulDescSetAttribute() to get and set the attribute value of a matmul descriptor. Attribute Name Description Data Type CUBLASLT_MATMUL_DESC_COMPUTE_TYPE Compute type. Defines the data type used for multiply and accumulate operations, and the accumulator during the matrix multiplication. See cublasComputeType_t . int32_t CUBLASLT_MATMUL_DESC_SCALE_TYPE Scale type. Defines the data type of the scaling factors alpha and beta . The accumulator value and the value from matrix C are typically converted to scale type before final scaling. The value is then converted from scale type to the type of matrix D before storing in memory. Default value is aligned with CUBLASLT_MATMUL_DESC_COMPUTE_TYPE. See cudaDataType_t . int32_t CUBLASLT_MATMUL_DESC_POINTER_MODE Specifies alpha and beta are passed by reference, whether they are scalars on the host or on the device, or device vectors. Default value is: CUBLASLT_POINTER_MODE_HOST (i.e., on the host). See cublasLtPointerMode_t . int32_t CUBLASLT_MATMUL_DESC_TRANSA Specifies the type of transformation operation that should be performed on matrix A. Default value is: CUBLAS_OP_N (i.e., non-transpose operation). See cublasOperation_t . int32_t CUBLASLT_MATMUL_DESC_TRANSB Specifies the type of transformation operation that should be performed on matrix B. int32_t CUBLASLT_MATMUL_DESC_TRANSC Specifies the type of transformation operation that should be performed on matrix C. Currently only CUBLAS_OP_N is supported. int32_t CUBLASLT_MATMUL_DESC_FILL_MODE Indicates whether the lower or upper part of the dense matrix was filled, and consequently should be used by the function. Default value is: CUBLAS_FILL_MODE_FULL .See cublasFillMode_t . int32_t CUBLASLT_MATMUL_DESC_EPILOGUE Epilogue function. See cublasLtEpilogue_t . Default value is: CUBLASLT_EPILOGUE_DEFAULT . uint32_t CUBLASLT_MATMUL_DESC_BIAS_POINTER Bias or Bias gradient vector pointer in the device memory. Input vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BIAS , CUBLASLT_EPILOGUE_RELU_BIAS , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_GELU_BIAS , CUBLASLT_EPILOGUE_GELU_AUX_BIAS . Output vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_DRELU_BGRAD , CUBLASLT_EPILOGUE_DGELU_BGRAD , CUBLASLT_EPILOGUE_BGRADA . Output vector with length that matches the number of columns of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BGRADB . Bias vector elements are the same type as alpha and beta (see CUBLASLT_MATMUL_DESC_SCALE_TYPE in this table) when matrix D datatype is CUDA_R_8I and same as matrix D datatype otherwise. See the datatypes table under cublasLtMatmul() for detailed mapping. Default value is: NULL. void * / const void * CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE Stride (in elements) to the next bias or bias gradient vector for strided batch operations. The default value is 0. int64_t CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER Pointer for epilogue auxiliary buffer. Output vector for ReLu bit-mask in forward pass when CUBLASLT_EPILOGUE_RELU_AUX or CUBLASLT_EPILOGUE_RELU_AUX_BIAS epilogue is used. Input vector for ReLu bit-mask in backward pass when CUBLASLT_EPILOGUE_DRELU or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Output of GELU input matrix in forward pass when CUBLASLT_EPILOGUE_GELU_AUX_BIAS epilogue is used. Input of GELU input matrix for backward pass when CUBLASLT_EPILOGUE_DGELU or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue is used. For aux data type, see CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE . Routines that don’t dereference this pointer, like cublasLtMatmulAlgoGetHeuristic() depend on its value to determine expected pointer alignment. Requires setting the CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD attribute. void * / const void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD Leading dimension for epilogue auxiliary buffer. ReLu bit-mask matrix leading dimension in elements (i.e. bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU_BGRAD , or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Must be divisible by 128 and be no less than the number of rows in the output matrix. GELU input matrix leading dimension in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DGELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used. Must be divisible by 8 and be no less than the number of rows in the output matrix. int64_t CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_BATCH_STRIDE Batch stride for epilogue auxiliary buffer. ReLu bit-mask matrix batch stride in elements (i.e. bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used. Must be divisible by 128. GELU input matrix batch stride in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used. Must be divisible by 8. Default value: 0. int64_t CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE Batch stride for alpha vector. Used together with CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST when matrix D’s CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT is greater than 1. If CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO is set then CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE must be set to 0 as this mode doesn’t support batched alpha vector. Default value: 0. int64_t CUBLASLT_MATMUL_DESC_SM_COUNT_TARGET Number of SMs to target for parallel execution. Optimizes heuristics for execution on a different number of SMs when user expects a concurrent stream to be using some of the device resources. Default value: 0. int32_t CUBLASLT_MATMUL_DESC_A_SCALE_POINTER Device pointer to the scale factor value that converts data in matrix A to the compute data type range. The scaling factor must have the same type as the compute type. If not specified, or set to NULL, the scaling factor is assumed to be 1. If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE . Default value: NULL const void* CUBLASLT_MATMUL_DESC_B_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix B. Default value: NULL const void* CUBLASLT_MATMUL_DESC_C_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix C. Default value: NULL const void* CUBLASLT_MATMUL_DESC_D_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix D. Default value: NULL const void* CUBLASLT_MATMUL_DESC_AMAX_D_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the output matrix. The computed value has the same type as the compute type. If not specified, or set to NULL, the maximum absolute value is not computed. Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE The type of the data that will be stored in CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . If unset (or set to the default value of -1), the data type is set to be the output matrix element data type (DType) with some exceptions: ReLu uses a bit-mask. For FP8 kernels with an output type (DType) of CUDA_R_8F_E4M3 , the data type can be set to a non-default value if: AType and BType are CUDA_R_8F_E4M3 . Bias Type is CUDA_R_16F . CType is CUDA_R_16BF or CUDA_R_16F CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_GELU_AUX When CType is CUDA_R_16BF , the data type may be set to CUDA_R_16BF or CUDA_R_8F_E4M3 . When CType is CUDA_R_16F , the data type may be set to CUDA_R_16F . Otherwise, the data type should be left unset or set to the default value of -1. Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER Device pointer to the scaling factor value to convert results from compute type data range to storage data range in the auxiliary matrix that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . The scaling factor value must have the same type as the compute type. Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER Device pointer to the memory location that on completion will be set to the maximum of absolute values in the buffer that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER . If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE. Default value: NULL void * CUBLASLT_MATMUL_DESC_FAST_ACCUM Flag for managing FP8 fast accumulation mode. When enabled, problem execution might be faster but at the cost of lower accuracy because intermediate results will not periodically be promoted to a higher precision. Default value: 0 - fast accumulation mode is disabled int8_t CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE Type of the bias or bias gradient vector in the device memory. Bias case: see CUBLASLT_EPILOGUE_BIAS . If unset (or set to the default value of -1), the bias vector elements are the same type as the elements of the output matrix (Dtype) with the following exceptions: IMMA kernels with computeType= CUDA_R_32I and Ctype=CUDA_R_8I where the bias vector elements are the same type as alpha, beta ( CUBLASLT_MATMUL_DESC_SCALE_TYPE=CUDA_R_32F ) For FP8 kernels with an output type of CUDA_R_32F , CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 . See cublasLtMatmul() for more details. Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER Pointer to a device array of input atomic counters consumed by a matmul. When a counter reaches zero, computation of the corresponding chunk of the output tensor is allowed to start. Default: NULL. See Atomics Synchronization . int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER Pointer to a device array of output atomic counters produced by a matmul. A matmul kernel sets a counter to zero when the computations of the corresponding chunk of the output tensor have completed. All the counters must be initialized to 1 before a matmul kernel is run. int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS Number of atomic synchronization chunks in the row dimension of the output matrix D. Each chunk corresponds to a single atomic counter. Default: 0 (atomics synchronization disabled). int32_t CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS Number of atomic synchronization chunks in the column dimension of the output matrix D. Each chunk corresponds to a single atomic counter. int32_t 3.3.10. cublasLtMatmulHeuristicResult_t \\uf0c1 cublasLtMatmulHeuristicResult_t is a descriptor that holds the configured matrix multiplication algorithm descriptor and its runtime properties. Member Description cublasLtMatmulAlgo_t algo Must be initialized with cublasLtMatmulAlgoInit() if the preference CUBLASLT_MATMUL_PERF_SEARCH_MODE is set to CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID. See cublasLtMatmulSearch_t . size_t workspaceSize; Actual size of workspace memory required. cublasStatus_t state; Result status. Other fields are valid only if, after call to cublasLtMatmulAlgoGetHeuristic() , this member is set to CUBLAS_STATUS_SUCCESS. float wavesCount; Waves count is a device utilization metric. A wavesCount value of 1.0f suggests that when the kernel is launched it will fully occupy the GPU. int reserved[4]; Reserved.'},\n",
       " {'id': 843,\n",
       "  'content': '3.3.11. cublasLtMatmulInnerShape_t \\uf0c1 cublasLtMatmulInnerShape_t is an enumerated type used to configure various aspects of the internal kernel design. This does not impact the CUDA grid size. Value Description CUBLASLT_MATMUL_INNER_SHAPE_UNDEFINED Inner shape is undefined. CUBLASLT_MATMUL_INNER_SHAPE_MMA884 Inner shape is MMA884. CUBLASLT_MATMUL_INNER_SHAPE_MMA1684 Inner shape is MMA1684. CUBLASLT_MATMUL_INNER_SHAPE_MMA1688 Inner shape is MMA1688. CUBLASLT_MATMUL_INNER_SHAPE_MMA16816 Inner shape is MMA16816. 3.3.12. cublasLtMatmulPreference_t \\uf0c1 The cublasLtMatmulPreference_t is a pointer to an opaque structure holding the description of the preferences for cublasLtMatmulAlgoGetHeuristic() configuration. Use cublasLtMatmulPreferenceCreate() to create one instance of the descriptor and cublasLtMatmulPreferenceDestroy() to destroy a previously created descriptor and release the resources. 3.3.13. cublasLtMatmulPreferenceAttributes_t \\uf0c1 cublasLtMatmulPreferenceAttributes_t is an enumerated type used to apply algorithm search preferences while fine-tuning the heuristic function. Use cublasLtMatmulPreferenceGetAttribute() and cublasLtMatmulPreferenceSetAttribute() to get and set the attribute value of a matmul preference descriptor. Value Description Data Type CUBLASLT_MATMUL_PREF_SEARCH_MODE Search mode. Default is CUBLASLT_SEARCH_BEST_FIT. uint32_t CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES Maximum allowed workspace memory. Default is 0 (no workspace memory allowed). uint64_t CUBLASLT_MATMUL_PREF_REDUCTION_SCHEME_MASK Reduction scheme mask. Only algorithm configurations specifying CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME that is not masked out by this attribute are allowed. For example, a mask value of 0x03 will allow only INPLACE and COMPUTE_TYPE reduction schemes. Default is CUBLASLT_REDUCTION_SCHEME_MASK (i.e., allows all reduction schemes). uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_A_BYTES Minimum buffer alignment for matrix A (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix A, which is not as strictly aligned as the algorithms need. Default is 256 bytes. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_B_BYTES Minimum buffer alignment for matrix B (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix B, which is not as strictly aligned as the algorithms need. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_C_BYTES Minimum buffer alignment for matrix C (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix C, which is not as strictly aligned as the algorithms need. uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_D_BYTES Minimum buffer alignment for matrix D (in bytes). Selecting a smaller value will exclude algorithms that can not work with matrix D, which is not as strictly aligned as the algorithms need. uint32_t CUBLASLT_MATMUL_PREF_MAX_WAVES_COUNT Maximum wave count. See cublasLtMatmulHeuristicResult_t ::wavesCount. Selecting a non-zero value will exclude algorithms that report device utilization higher than specified. Default is 0.0f. float CUBLASLT_MATMUL_PREF_IMPL_MASK Numerical implementation details mask. See cublasLtNumericalImplFlags_t . Filters heuristic result to only include algorithms that use the allowed implementations. default: uint64_t(-1) (allow everything) uint64_t 3.3.14. cublasLtMatmulSearch_t \\uf0c1 cublasLtMatmulSearch_t is an enumerated type that contains the attributes for heuristics search type. Value Description Data Type CUBLASLT_SEARCH_BEST_FIT Request heuristics for the best algorithm for the given use case. CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID Request heuristics only for the pre-configured algo id. 3.3.15. cublasLtMatmulTile_t \\uf0c1 cublasLtMatmulTile_t is an enumerated type used to set the tile size in rows x columns. See also CUTLASS: Fast Linear Algebra in CUDA C++ . Value Description CUBLASLT_MATMUL_TILE_UNDEFINED Tile size is undefined. CUBLASLT_MATMUL_TILE_8x8 Tile size is 8 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x16 Tile size is 8 rows x 16 columns. CUBLASLT_MATMUL_TILE_16x8 Tile size is 16 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x32 Tile size is 8 rows x 32 columns. CUBLASLT_MATMUL_TILE_16x16 Tile size is 16 rows x 16 columns. CUBLASLT_MATMUL_TILE_32x8 Tile size is 32 rows x 8 columns. CUBLASLT_MATMUL_TILE_8x64 Tile size is 8 rows x 64 columns. CUBLASLT_MATMUL_TILE_16x32 Tile size is 16 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x16 Tile size is 32 rows x 16 columns. CUBLASLT_MATMUL_TILE_64x8 Tile size is 64 rows x 8 columns. CUBLASLT_MATMUL_TILE_32x32 Tile size is 32 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x64 Tile size is 32 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x32 Tile size is 64 rows x 32 columns. CUBLASLT_MATMUL_TILE_32x128 Tile size is 32 rows x 128 columns. CUBLASLT_MATMUL_TILE_64x64 Tile size is 64 rows x 64 columns. CUBLASLT_MATMUL_TILE_128x32 Tile size is 128 rows x 32 columns. CUBLASLT_MATMUL_TILE_64x128 Tile size is 64 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x64 Tile size is 128 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x256 Tile size is 64 rows x 256 columns. CUBLASLT_MATMUL_TILE_128x128 Tile size is 128 rows x 128 columns. CUBLASLT_MATMUL_TILE_256x64 Tile size is 256 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x512 Tile size is 64 rows x 512 columns. CUBLASLT_MATMUL_TILE_128x256 Tile size is 128 rows x 256 columns. CUBLASLT_MATMUL_TILE_256x128 Tile size is 256 rows x 128 columns. CUBLASLT_MATMUL_TILE_512x64 Tile size is 512 rows x 64 columns. CUBLASLT_MATMUL_TILE_64x96 Tile size is 64 rows x 96 columns. CUBLASLT_MATMUL_TILE_96x64 Tile size is 96 rows x 64 columns. CUBLASLT_MATMUL_TILE_96x128 Tile size is 96 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x160 Tile size is 128 rows x 160 columns. CUBLASLT_MATMUL_TILE_160x128 Tile size is 160 rows x 128 columns. CUBLASLT_MATMUL_TILE_192x128 Tile size is 192 rows x 128 columns. CUBLASLT_MATMUL_TILE_128x192 Tile size is 128 rows x 192 columns. CUBLASLT_MATMUL_TILE_128x96 Tile size is 128 rows x 96 columns. 3.3.16. cublasLtMatmulStages_t \\uf0c1 cublasLtMatmulStages_t is an enumerated type used to configure the size and number of shared memory buffers where input elements are staged. Number of staging buffers defines kernel’s pipeline depth. Value Description CUBLASLT_MATMUL_STAGES_UNDEFINED Stage size is undefined. CUBLASLT_MATMUL_STAGES_16x1 Stage size is 16, number of stages is 1. CUBLASLT_MATMUL_STAGES_16x2 Stage size is 16, number of stages is 2. CUBLASLT_MATMUL_STAGES_16x3 Stage size is 16, number of stages is 3. CUBLASLT_MATMUL_STAGES_16x4 Stage size is 16, number of stages is 4. CUBLASLT_MATMUL_STAGES_16x5 Stage size is 16, number of stages is 5. CUBLASLT_MATMUL_STAGES_16x6 Stage size is 16, number of stages is 6. CUBLASLT_MATMUL_STAGES_32x1 Stage size is 32, number of stages is 1. CUBLASLT_MATMUL_STAGES_32x2 Stage size is 32, number of stages is 2. CUBLASLT_MATMUL_STAGES_32x3 Stage size is 32, number of stages is 3. CUBLASLT_MATMUL_STAGES_32x4 Stage size is 32, number of stages is 4. CUBLASLT_MATMUL_STAGES_32x5 Stage size is 32, number of stages is 5. CUBLASLT_MATMUL_STAGES_32x6 Stage size is 32, number of stages is 6. CUBLASLT_MATMUL_STAGES_64x1 Stage size is 64, number of stages is 1. CUBLASLT_MATMUL_STAGES_64x2 Stage size is 64, number of stages is 2. CUBLASLT_MATMUL_STAGES_64x3 Stage size is 64, number of stages is 3. CUBLASLT_MATMUL_STAGES_64x4 Stage size is 64, number of stages is 4. CUBLASLT_MATMUL_STAGES_64x5 Stage size is 64, number of stages is 5. CUBLASLT_MATMUL_STAGES_64x6 Stage size is 64, number of stages is 6. CUBLASLT_MATMUL_STAGES_128x1 Stage size is 128, number of stages is 1. CUBLASLT_MATMUL_STAGES_128x2 Stage size is 128, number of stages is 2. CUBLASLT_MATMUL_STAGES_128x3 Stage size is 128, number of stages is 3. CUBLASLT_MATMUL_STAGES_128x4 Stage size is 128, number of stages is 4. CUBLASLT_MATMUL_STAGES_128x5 Stage size is 128, number of stages is 5. CUBLASLT_MATMUL_STAGES_128x6 Stage size is 128, number of stages is 6. CUBLASLT_MATMUL_STAGES_32x10 Stage size is 32, number of stages is 10. CUBLASLT_MATMUL_STAGES_8x4 Stage size is 8, number of stages is 4. CUBLASLT_MATMUL_STAGES_16x10 Stage size is 16, number of stages is 10. CUBLASLT_MATMUL_STAGES_8x5 Stage size is 8, number of stages is 5. CUBLASLT_MATMUL_STAGES_8x3 Stage size is 8, number of stages is 3. CUBLASLT_MATMUL_STAGES_8xAUTO Stage size is 8, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_16xAUTO Stage size is 16, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_32xAUTO Stage size is 32, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_64xAUTO Stage size is 64, number of stages is selected automatically. CUBLASLT_MATMUL_STAGES_128xAUTO Stage size is 128, number of stages is selected automatically. 3.3.17. cublasLtNumericalImplFlags_t \\uf0c1 cublasLtNumericalImplFlags_t : a set of bit-flags that can be specified to select implementation details that may affect numerical behavior of algorithms. Flags below can be combined using the bit OR operator “|”. Value Description CUBLASLT_NUMERICAL_IMPL_FLAGS_FMA Specify that the implementation is based on [H,F,D]FMA (fused multiply-add) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_HMMA Specify that the implementation is based on HMMA (tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_IMMA Specify that the implementation is based on IMMA (integer tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_DMMA Specify that the implementation is based on DMMA (double precision tensor operation) family instructions. CUBLASLT_NUMERICAL_IMPL_FLAGS_TENSOR_OP_MASK Mask to filter implementations using any of the above kinds of tensor operations. CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_TYPE_MASK Mask to filter implementation details about multiply-accumulate instructions used. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_16F Specify that the implementation’s inner dot product is using half precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32F Specify that the implementation’s inner dot product is using single precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_64F Specify that the implementation’s inner dot product is using double precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32I Specify that the implementation’s inner dot product is using 32 bit signed integer precision accumulator. CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_TYPE_MASK Mask to filter implementation details about accumulator used. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16F Specify that the implementation’s inner dot product multiply-accumulate instruction is using half-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16BF Specify that the implementation’s inner dot product multiply-accumulate instruction is using bfloat16 inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_TF32 Specify that the implementation’s inner dot product multiply-accumulate instruction is using TF32 inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_32F Specify that the implementation’s inner dot product multiply-accumulate instruction is using single-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_64F Specify that the implementation’s inner dot product multiply-accumulate instruction is using double-precision inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_8I Specify that the implementation’s inner dot product multiply-accumulate instruction is using 8-bit integer inputs. CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_INPUT_TYPE_MASK Mask to filter implementation details about accumulator input used. CUBLASLT_NUMERICAL_IMPL_FLAGS_GAUSSIAN Specify that the implementation applies Gauss complexity reduction algorithm to reduce arithmetic complexity of the complex matrix multiplication problem 3.3.18. cublasLtMatrixLayout_t \\uf0c1 The cublasLtMatrixLayout_t is a pointer to an opaque structure holding the description of a matrix layout. Use cublasLtMatrixLayoutCreate() to create one instance of the descriptor and cublasLtMatrixLayoutDestroy() to destroy a previously created descriptor and release the resources. 3.3.19. cublasLtMatrixLayoutAttribute_t \\uf0c1 cublasLtMatrixLayoutAttribute_t is a descriptor structure containing the attributes that define the details of the matrix operation. Use cublasLtMatrixLayoutGetAttribute() and cublasLtMatrixLayoutSetAttribute() to get and set the attribute value of a matrix layout descriptor. Attribute Name Description Data Type CUBLASLT_MATRIX_LAYOUT_TYPE Specifies the data precision type. uint32_t CUBLASLT_MATRIX_LAYOUT_ORDER Specifies the memory order of the data of the matrix. Default value is CUBLASLT_ORDER_COL. See cublasLtOrder_t . int32_t CUBLASLT_MATRIX_LAYOUT_ROWS Describes the number of rows in the matrix. Normally only values that can be expressed as int32_t are supported. uint64_t CUBLASLT_MATRIX_LAYOUT_COLS Describes the number of columns in the matrix. uint64_t CUBLASLT_MATRIX_LAYOUT_LD The leading dimension of the matrix. For CUBLASLT_ORDER_COL this is the stride (in elements) of matrix column. See also cublasLtOrder_t . Currently only non-negative values are supported. Must be large enough so that matrix memory locations are not overlapping (e.g., greater or equal to CUBLASLT_MATRIX_LAYOUT_ROWS in case of CUBLASLT_ORDER_COL). int64_t CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT Number of matmul operations to perform in the batch. Default value is 1. See also CUBLASLT_ALGO_CAP_STRIDED_BATCH_SUPPORT in cublasLtMatmulAlgoCapAttributes_t . int32_t CUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET Stride (in elements) to the next matrix for the strided batch operation. Default value is 0. When matrix type is planar-complex (CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET != 0), batch stride is interpreted by cublasLtMatmul() in number of real valued sub-elements. for data of type CUDA_C_16F, offset of 1024B is encoded as a stride of value 512 (since each element of the real and imaginary matrices is a 2B (16bit) floating point type). NOTE: A bug in cublasLtMatrixTransform() causes it to interpret the batch stride for a planar-complex matrix as if it was specified in number of complex elements. Therefore an offset of 1024B must be encoded as stride value 256 when calling cublasLtMatrixTransform() (each complex element is 4B with real and imaginary values 2B each). This behavior is expected to be corrected in the next major cuBLAS version. int64_t CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET Stride (in bytes) to the imaginary plane for planar-complex layout. Default value is 0, indicating that the layout is regular (real and imaginary parts of complex numbers are interleaved in memory for each element). int64_t 3.3.20. cublasLtMatrixTransformDesc_t \\uf0c1 The cublasLtMatrixTransformDesc_t is a pointer to an opaque structure holding the description of a matrix transformation operation. Use cublasLtMatrixTransformDescCreate() to create one instance of the descriptor and cublasLtMatrixTransformDescDestroy() to destroy a previously created descriptor and release the resources. 3.3.21. cublasLtMatrixTransformDescAttributes_t \\uf0c1 cublasLtMatrixTransformDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix transform operation. Use cublasLtMatrixTransformDescGetAttribute() and cublasLtMatrixTransformDescSetAttribute() to set the attribute value of a matrix transform descriptor. Transform Attribute Name Description Data Type CUBLASLT_MATRIX_TRANSFORM_DESC_SCALE_TYPE Scale type. Inputs are converted to the scale type for scaling and summation, and results are then converted to the output type to store in the memory. For the supported data types see cudaDataType_t . int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_POINTER_MODE Specifies the scalars alpha and beta are passed by reference whether on the host or on the device. int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSA Specifies the type of operation that should be performed on the matrix A. int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSB Specifies the type of operation that should be performed on the matrix B. int32_t 3.3.22. cublasLtOrder_t \\uf0c1 cublasLtOrder_t is an enumerated type used to indicate the data ordering of the matrix. Value Data Order Description CUBLASLT_ORDER_COL Data is ordered in column-major format. The leading dimension is the stride (in elements) to the beginning of next column in memory. CUBLASLT_ORDER_ROW Data is ordered in row-major format. The leading dimension is the stride (in elements) to the beginning of next row in memory. CUBLASLT_ORDER_COL32 Data is ordered in column-major ordered tiles of 32 columns. The leading dimension is the stride (in elements) to the beginning of next group of 32-columns. For example, if the matrix has 33 columns and 2 rows, then the leading dimension must be at least (32) * 2 = 64. CUBLASLT_ORDER_COL4_4R2_8C Data is ordered in column-major ordered tiles of composite tiles with total 32 columns and 8 rows. A tile is composed of interleaved inner tiles of 4 columns within 4 even or odd rows in an alternating pattern. The leading dimension is the stride (in elements) to the beginning of the first 32 column x 8 row tile for the next 32-wide group of columns. For example, if the matrix has 33 columns and 1 row, the leading dimension must be at least (32 * 8) * 1 = 256. CUBLASLT_ORDER_COL32_2R_4R4 Data is ordered in column-major ordered tiles of composite tiles with total 32 columns ands 32 rows. Element offset within the tile is calculated as (((row%8)/2*4+row/8)*2+row%2)*32+col. Leading dimension is the stride (in elements) to the beginning of the first 32 column x 32 row tile for the next 32-wide group of columns. if matrix has 33 columns and 1 row, ld must be at least (32*32)*1 = 1024. 3.3.23. cublasLtPointerMode_t \\uf0c1 cublasLtPointerMode_t is an enumerated type used to set the pointer mode for the scaling factors alpha and beta . Value Description CUBLASLT_POINTER_MODE_HOST = CUBLAS_POINTER_MODE_HOST Matches CUBLAS_POINTER_MODE_HOST, and the pointer targets a single value host memory. CUBLASLT_POINTER_MODE_DEVICE = CUBLAS_POINTER_MODE_DEVICE Matches CUBLAS_POINTER_MODE_DEVICE, and the pointer targets a single value device memory. CUBLASLT_POINTER_MODE_DEVICE_VECTOR = 2 Pointers target device memory vectors of length equal to the number of rows of matrix D. CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO = 3 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is zero. CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST = 4 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is a single value in host memory. 3.3.24. cublasLtPointerModeMask_t \\uf0c1 cublasLtPointerModeMask_t is an enumerated type used to define and query the pointer mode capability. Value Description CUBLASLT_POINTER_MODE_MASK_HOST = 1 See CUBLASLT_POINTER_MODE_HOST in cublasLtPointerMode_t . CUBLASLT_POINTER_MODE_MASK_DEVICE = 2 See CUBLASLT_POINTER_MODE_DEVICE in cublasLtPointerMode_t . CUBLASLT_POINTER_MODE_MASK_DEVICE_VECTOR = 4 See CUBLASLT_POINTER_MODE_DEVICE_VECTOR in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_ZERO = 8 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_HOST = 16 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST in cublasLtPointerMode_t 3.3.25. cublasLtReductionScheme_t \\uf0c1 cublasLtReductionScheme_t is an enumerated type used to specify a reduction scheme for the portions of the dot-product calculated in parallel (i.e., “split - K”). Value Description CUBLASLT_REDUCTION_SCHEME_NONE Do not apply reduction. The dot-product will be performed in one sequence. CUBLASLT_REDUCTION_SCHEME_INPLACE Reduction is performed “in place” using the output buffer, parts are added up in the output data type. Workspace is only used for counters that guarantee sequentiality. CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE Reduction done out of place in a user-provided workspace. The intermediate results are stored in the compute type in the workspace and reduced in a separate step. CUBLASLT_REDUCTION_SCHEME_OUTPUT_TYPE Reduction done out of place in a user-provided workspace. The intermediate results are stored in the output type in the workspace and reduced in a separate step. CUBLASLT_REDUCTION_SCHEME_MASK Allows all reduction schemes. 3.4.'},\n",
       " {'id': 844,\n",
       "  'content': 'cuBLASLt API Reference \\uf0c1 3.4.1. cublasLtCreate() \\uf0c1 cublasStatus_t cublasLtCreate ( cublasLtHandle_t * lighthandle ) This function initializes the cuBLASLt library and creates a handle to an opaque structure holding the cuBLASLt library context. It allocates light hardware resources on the host and device, and must be called prior to making any other cuBLASLt library calls. The cuBLASLt library context is tied to the current CUDA device. To use the library on multiple devices, one cuBLASLt handle should be created for each device. Parameters: Parameter Memory Input / Output Description lightHandle Output Pointer to the allocated cuBLASLt handle for the created cuBLASLt context. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The allocation completed successfully. CUBLAS_STATUS_NOT_INITIALIZED The cuBLASLt library was not initialized. This usually happens: when cublasLtCreate() is not called first an error in the CUDA Runtime API called by the cuBLASLt routine, or an error in the hardware setup. CUBLAS_STATUS_ALLOC_FAILED Resource allocation failed inside the cuBLASLt library. To correct: prior to the function call, deallocate the previously allocated memory as much as possible. CUBLAS_STATUS_INVALID_VALUE lighthandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.2. cublasLtDestroy() \\uf0c1 cublasStatus_t cublasLtDestroy ( cublasLtHandle_t lightHandle ) This function releases hardware resources used by the cuBLASLt library. This function is usually the last call with a particular handle to the cuBLASLt library. Because cublasLtCreate() allocates some internal resources and the release of those resources by calling cublasLtDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the cuBLASLt handle to be destroyed. Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The cuBLASLt context was successfully destroyed. CUBLAS_STATUS_INVALID_VALUE lightHandle == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.3. cublasLtDisableCpuInstructionsSetMask() \\uf0c1 unsigned cublasLtDisableCpuInstructionsSetMask ( unsigned mask ); Instructs cuBLASLt library to not use CPU instructions specified by the flags in the mask . The function takes precedence over the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable. Parameters: mask – the flags combined with bitwise OR(|) operator that specify which CPU instructions should not be used. Supported flags: Value Description 0x1 x86-64 AVX512 ISA. Returns: the previous value of the mask .'},\n",
       " {'id': 845,\n",
       "  'content': '3.4.4. cublasLtGetCudartVersion() \\uf0c1 size_t cublasLtGetCudartVersion ( void ); This function returns the version number of the CUDA Runtime library. Parameters: None. Returns: size_t - The version number of the CUDA Runtime library. 3.4.5. cublasLtGetProperty() \\uf0c1 cublasStatus_t cublasLtGetProperty ( libraryPropertyType type , int * value ); This function returns the value of the requested property by writing it to the memory location pointed to by the value parameter. Parameters : Parameter Memory Input / Output Description type Input Of the type libraryPropertyType , whose value is requested from the property. See libraryPropertyType_t . value Output Pointer to the host memory location where the requested information should be written. Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The requested libraryPropertyType information is successfully written at the provided address. CUBLAS_STATUS_INVALID_VALUE If invalid value of the type input argument or value == NULL See cublasStatus_t for a complete list of valid return codes. 3.4.6. cublasLtGetStatusName() \\uf0c1 const char * cublasLtGetStatusName ( cublasStatus_t status ); Returns the string representation of a given status. Parameters: cublasStatus_t - the status. Returns: const char* - the NULL-terminated string. 3.4.7. cublasLtGetStatusString() \\uf0c1 const char * cublasLtGetStatusString ( cublasStatus_t status ); Returns the description string for a given status. 3.4.8. cublasLtHeuristicsCacheGetCapacity() \\uf0c1 cublasStatus_t cublasLtHeuristicsCacheGetCapacity ( size_t * capacity ); Returns the Heuristics Cache capacity. Parameters: Parameter Description capacity The pointer to the returned capacity value. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully written. CUBLAS_STATUS_INVALID_VALUE The capacity was successfully set. 3.4.9. cublasLtHeuristicsCacheSetCapacity() \\uf0c1 cublasStatus_t cublasLtHeuristicsCacheSetCapacity ( size_t capacity ); Sets the Heuristics Cache capacity. Set the capacity to 0 to disable the heuristics cache. This function takes precedence over CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable. Parameters: Parameter Description capacity The desirable heuristics cache capacity. Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully set. 3.4.10. cublasLtGetVersion() \\uf0c1 size_t cublasLtGetVersion ( void ); This function returns the version number of cuBLASLt library. Returns: size_t - The version number of cuBLASLt library. 3.4.11. cublasLtLoggerSetCallback() \\uf0c1 cublasStatus_t cublasLtLoggerSetCallback ( cublasLtLoggerCallback_t callback ); Experimental: This function sets the logging callback function. Parameters : Parameter Memory Input / Output Description callback Input Pointer to a callback function. See cublasLtLoggerCallback_t . Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the callback function was successfully set. See cublasStatus_t for a complete list of valid return codes. 3.4.12. cublasLtLoggerSetFile() \\uf0c1 cublasStatus_t cublasLtLoggerSetFile ( FILE * file ); Experimental: This function sets the logging output file. Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle. Parameters : Parameter Memory Input / Output Description file Input Pointer to an open file. File should have write permission. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging file was successfully set. 3.4.13. cublasLtLoggerOpenFile() \\uf0c1 cublasStatus_t cublasLtLoggerOpenFile ( const char * logFile ); Experimental: This function opens a logging output file in the given path. Parameters : Parameter Memory Input / Output Description logFile Input Path of the logging output file. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging file was successfully opened. 3.4.14. cublasLtLoggerSetLevel() \\uf0c1 cublasStatus_t cublasLtLoggerSetLevel ( int level ); Experimental: This function sets the value of the logging level. Parameters : Parameter Memory Input / Output Description level Input Value of the logging level. See cuBLASLt Logging . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If the value was not a valid logging level. CUBLAS_STATUS_SUCCESS If the logging level was successfully set. 3.4.15. cublasLtLoggerSetMask() \\uf0c1 cublasStatus_t cublasLtLoggerSetMask ( int mask ); Experimental: This function sets the value of the logging mask. Parameters : Parameter Memory Input / Output Description mask Input Value of the logging mask. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging mask was successfully set. 3.4.16. cublasLtLoggerForceDisable() \\uf0c1 cublasStatus_t cublasLtLoggerForceDisable (); Experimental: This function disables logging for the entire run. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If logging was successfully disabled.'},\n",
       " {'id': 846,\n",
       "  'content': '3.4.17. cublasLtMatmul() \\uf0c1 cublasStatus_t cublasLtMatmul ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t computeDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * B , cublasLtMatrixLayout_t Bdesc , const void * beta , const void * C , cublasLtMatrixLayout_t Cdesc , void * D , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , void * workspace , size_t workspaceSizeInBytes , cudaStream_t stream ); This function computes the matrix multiplication of matrices A and B to produce the output matrix D, according to the following operation: D = alpha*(A*B) + beta*(C), where A , B , and C are input matrices, and alpha and beta are input scalars. Note This function supports both in-place matrix multiplication ( C == D and Cdesc == Ddesc ) and out-of-place matrix multiplication ( C != D , both matrices must have the same data type, number of rows, number of columns, batch size, and memory order). In the out-of-place case, the leading dimension of C can be different from the leading dimension of D. Specifically the leading dimension of C can be 0 to achieve row or column broadcast. If Cdesc is omitted, this function assumes it to be equal to Ddesc . The workspace pointer must be aligned to at least a multiple of 256 bytes. The recommendations on workspaceSizeInBytes are the same as mentioned in the cublasSetWorkspace() section. Datatypes Supported: cublasLtMatmul() supports the following computeType, scaleType, Atype/Btype, and Ctype. Footnotes can be found at the end of this section. Table 1. When A, B, C, and D are Regular Column- or Row-major Matrices \\uf0c1 computeType scaleType Atype/Btype Ctype Bias Type 5 CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported. CUDA_R_32F CUDA_R_8I CUDA_R_8I Non-default epilogue not supported. CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUDA_R_8I CUDA_R_32F Non-default epilogue not supported. CUDA_R_16BF CUDA_R_32F CUDA_R_32F 5 CUDA_R_16F CUDA_R_32F CUDA_R_32F 5 CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_8I 6 CUDA_C_32F 6 Non-default epilogue not supported. CUDA_C_32F 6 CUDA_C_32F 6 CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_32F 6 CUDA_C_32F 6 Non-default epilogue not supported. CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F 5 CUDA_C_64F 6 CUDA_C_64F 6 CUDA_C_64F 6 Non-default epilogue not supported. To use IMMA kernels, one of the following sets of requirements, with the first being the preferred one, must be met: Using a regular data ordering: All matrix pointers must be 4-byte aligned. For even better performance, this condition should hold with 16 instead of 4.'},\n",
       " {'id': 847,\n",
       "  'content': 'Leading dimensions of matrices A, B, C must be multiples of 4. Only the “TN” format is supported - A must be transposed and B non-transposed. Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST . With the latter mode, the kernels support the CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE attribute. Dimensions m and k must be multiples of 4. Using the IMMA-specific data ordering on Ampere or Turing (but not Hopper) architecture - CUBLASLT_ORDER_COL32` for matrices A, C, D, and CUBLASLT_ORDER_COL4_4R2_8C (on Turing or Ampere architecture) or CUBLASLT_ORDER_COL32_2R_4R4 (on Ampere architecture) for matrix B: Leading dimensions of matrices A, B, C must fulfill conditions specific to the memory ordering (see cublasLtOrder_t ). Matmul descriptor must specify CUBLAS_OP_T on matrix B and CUBLAS_OP_N (default) on matrix A and C. If scaleType CUDA_R_32I is used, the only supported values for alpha and beta are 0 or 1 . Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE , CUBLASLT_POINTER_MODE_DEVICE_VECTOR or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO . These kernels do not support CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE . Only the “NT” format is supported - A must be transposed and B non-transposed.'},\n",
       " {'id': 848,\n",
       "  'content': 'Table 2. When A, B, C, and D Use Layouts for IMMA \\uf0c1 computeType scaleType Atype/Btype Ctype Bias Type CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported. CUDA_R_32F CUDA_R_8I CUDA_R_8I CUDA_R_32F To use FP8 kernels, the following set of requirements must be satisfied: All matrix dimensions must meet the optimal requirements listed in Tensor Core Usage (i.e. pointers and matrix dimension must support 16-byte alignment). A must be transposed and B non-transposed (The “TN” format). The compute type must be CUBLAS_COMPUTE_32F . The scale type must be CUDA_R_32F . See the table below when using FP8 kernels: Table 3. When A, B, C, and D Use Layouts for FP8 \\uf0c1 AType BType CType DType Bias Type CUDA_R_8F_E4M3 CUDA_R_8F_E4M3 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_8F_E5M2 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_8F_E4M3 CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_8F_E4M3 CUDA_R_16BF 5 CUDA_R_8F_E5M2 CUDA_R_16BF 5 CUDA_R_16F CUDA_R_8F_E4M3 CUDA_R_16F 5 CUDA_R_8F_E5M2 CUDA_R_16F 5 CUDA_R_16F CUDA_R_16F 5 CUDA_R_32F CUDA_R_32F CUDA_R_16BF 5 And finally, see below table when A,B,C,D are planar-complex matrices ( CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET != 0 , see cublasLtMatrixLayoutAttribute_t ) to make use of mixed precision tensor core acceleration. Table 4. When A, B, C, and D are Planar-Complex Matrices \\uf0c1 computeType scaleType Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_C_32F CUDA_C_16F 6 CUDA_C_16F 6 CUDA_C_32F 6 CUDA_C_16BF 6 CUDA_C_16BF 6 CUDA_C_32F 6 NOTES: 5 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 ) ReLU, dReLu, GELU, dGELU and Bias epilogue modes (see CUBLASLT_MATMUL_DESC_EPILOGUE in cublasLtMatmulDescAttributes_t ) are not supported when D matrix memory order is defined as CUBLASLT_ORDER_ROW . For best performance when using the bias vector, specify zero beta and set pointer mode to CUBLASLT_POINTER_MODE_HOST . 6 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 ) Use of CUBLAS_ORDER_ROW together with CUBLAS_OP_C (Hermitian operator) is not supported unless all of A, B, C, and D matrices use the CUBLAS_ORDER_ROW ordering. Parameters: Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. See cublasLtHandle_t . computeDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . alpha, beta Device or host Input Pointers to the scalars used in the multiplication. A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc, Bdesc and Cdesc. Adesc, Bdesc and Cdesc. Input Handles to the previous created descriptors of the type cublasLtMatrixLayout_t . D Device Output Pointer to the GPU memory associated with the descriptor Ddesc. Ddesc Input Handle to the previous created descriptor of the type cublasLtMatrixLayout_t . algo Input Handle for matrix multiplication algorithm to be used. See cublasLtMatmulAlgo_t . When NULL, an implicit heuritics query with default search preferences will be performed to determine actual algorithm to use. workspace Device Pointer to the workspace buffer allocated in the GPU memory. Must be 256B aligned (i.e. lowest 8 bits of address must be 0). workspaceSizeInBytes Input Size of the workspace. stream Host Input The CUDA stream where all the GPU work will be submitted. Returns: Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized.'},\n",
       " {'id': 849,\n",
       "  'content': 'CUBLAS_STATUS_INVALID_VALUE If the parameters are unexpectedly NULL, in conflict or in an impossible configuration. For example, when workspaceSizeInBytes is less than workspace required by the configured algo. CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device doesn’t support the configured operation. CUBLAS_STATUS_ARCH_MISMATCH If the configured operation cannot be run using the selected device. CUBLAS_STATUS_EXECUTION_FAILED If CUDA reported an execution error from the device. CUBLAS_STATUS_SUCCESS If the operation completed successfully. 3.4.18. cublasLtMatmulAlgoCapGetAttribute() \\uf0c1 cublasStatus_t cublasLtMatmulAlgoCapGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoCapAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried capability attribute for an initialized cublasLtMatmulAlgo_t descriptor structure. The capability attribute value is retrieved from the enumerated type cublasLtMatmulAlgoCapAttributes_t . For example, to get list of supported Tile IDs: cublasLtMatmulTile_t tiles [ CUBLASLT_MATMUL_TILE_END ]; size_t num_tiles , size_written ; if ( cublasLtMatmulAlgoCapGetAttribute ( algo , CUBLASLT_ALGO_CAP_TILE_IDS , tiles , sizeof ( tiles ), & size_written ) == CUBLAS_STATUS_SUCCESS ) { num_tiles = size_written / sizeof ( tiles [ 0 ]);} Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. attr Input The capability attribute whose value will be retrieved by this function. See cublasLtMatmulAlgoCapAttributes_t . buf Output The attribute value returned by this function. sizeInBytes Input Size of buf buffer (in bytes) for verification. sizeWritten Output Valid only when the return value is CUBLAS_STATUS_SUCCESS. If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If sizeInBytes is 0 and sizeWritten is NULL, or if sizeInBytes is non-zero and buf is NULL, or sizeInBytes doesn’t match size of internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If attribute’s value was successfully written to user memory. 3.4.19. cublasLtMatmulAlgoCheck() \\uf0c1 cublasStatus_t cublasLtMatmulAlgoCheck ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , cublasLtMatmulHeuristicResult_t * result ); This function performs the correctness check on the matrix multiply algorithm descriptor for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D. It checks whether the descriptor is supported on the current device, and returns the result containing the required workspace and the calculated wave count. Note CUBLAS_STATUS_SUCCESS doesn’t fully guarantee that the algo will run. The algo will fail if, for example, the buffers are not correctly aligned. However, if cublasLtMatmulAlgoCheck() fails, the algo will not run. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. operationDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t . Adesc, Bdesc, Cdesc, and Ddesc Input Handles to the previously created matrix layout descriptors of the type cublasLtMatrixLayout_t . algo Input Descriptor which specifies which matrix multiplication algorithm should be used. May point to result->algo .'},\n",
       " {'id': 850,\n",
       "  'content': 'result Output Pointer to the structure holding the results returned by this function. The results comprise of the required workspace and the calculated wave count. The algo field is never updated. See cublasLtMatmulHeuristicResult_t . Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If matrix layout descriptors or the operation descriptor do not match the algo descriptor. CUBLAS_STATUS_NOT_SUPPORTED If the algo configuration or data type combination is not currently supported on the given device. CUBLAS_STATUS_ARCH_MISMATCH If the algo configuration cannot be run using the selected device. CUBLAS_STATUS_SUCCESS If the check was successful. 3.4.20. cublasLtMatmulAlgoConfigGetAttribute() \\uf0c1 cublasStatus_t cublasLtMatmulAlgoConfigGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor. The configuration attribute value is retrieved from the enumerated type cublasLtMatmulAlgoConfigAttributes_t . Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor. attr Input The configuration attribute whose value will be retrieved by this function. See cublasLtMatmulAlgoConfigAttributes_t . 3.4.21. cublasLtMatmulAlgoConfigSetAttribute() \\uf0c1 cublasStatus_t cublasLtMatmulAlgoConfigSetAttribute ( cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor. The configuration attribute is an enumerant of the type cublasLtMatmulAlgoConfigAttributes_t . attr Input The configuration attribute whose value will be set by this function. buf Input The value to which the configuration attribute should be set. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If the attribute was set successfully.'},\n",
       " {'id': 851,\n",
       "  'content': '3.4.22. cublasLtMatmulAlgoGetHeuristic() \\uf0c1 cublasStatus_t cublasLtMatmulAlgoGetHeuristic ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , cublasLtMatmulPreference_t preference , int requestedAlgoCount , cublasLtMatmulHeuristicResult_t heuristicResultsArray [] int * returnAlgoCount ); This function retrieves the possible algorithms for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D. The output is placed in heuristicResultsArray[] in the order of increasing estimated compute time. preference Input Pointer to the structure holding the heuristic search preferences descriptor. See cublasLtMatmulPreference_t . requestedAlgoCount Input Size of the heuristicResultsArray (in elements). This is the requested maximum number of algorithms to return. heuristicResultsArray[] Output Array containing the algorithm heuristics and associated runtime characteristics, returned by this function, in the order of increasing estimated compute time. returnAlgoCount Output Number of algorithms returned by this function. This is the number of heuristicResultsArray elements written. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If requestedAlgoCount is less or equal to zero. CUBLAS_STATUS_NOT_SUPPORTED If no heuristic function available for current configuration. CUBLAS_STATUS_SUCCESS If query was successful. Inspect heuristicResultsArray[0 to (returnAlgoCount -1)].state for the status of the results. Note This function may load some kernels using CUDA Driver API which may fail when there is no available GPU memory. Do not allocate the entire VRAM before running cublasLtMatmulAlgoGetHeuristic() . 3.4.23. cublasLtMatmulAlgoGetIds() \\uf0c1 cublasStatus_t cublasLtMatmulAlgoGetIds ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int requestedAlgoCount , int algoIdsArray [], int * returnAlgoCount ); This function retrieves the IDs of all the matrix multiply algorithms that are valid, and can potentially be run by the cublasLtMatmul() function, for given types of the input matrices A, B and C, and of the output matrix D. Note The IDs are returned in no particular order.'},\n",
       " {'id': 852,\n",
       "  'content': 'To make sure the best possible algo is contained in the list, make requestedAlgoCount large enough to receive the full list. The list is guaranteed to be full if returnAlgoCount 0. algoIdsArray[] Output Array containing the algorithm IDs returned by this function. returnAlgoCount Output Number of algorithms actually returned by this function. Inspect returnAlgoCount to get actual number of IDs available.'},\n",
       " {'id': 853,\n",
       "  'content': '3.4.24. cublasLtMatmulAlgoInit() \\uf0c1 cublasStatus_t cublasLtMatmulAlgoInit ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int algoId , cublasLtMatmulAlgo_t * algo ); This function initializes the matrix multiply algorithm structure for the cublasLtMatmul() , for a specified matrix multiply algorithm and input matrices A, B and C, and the output matrix D. Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context. computeType Input Compute type. See CUBLASLT_MATMUL_DESC_COMPUTE_TYPE of cublasLtMatmulDescAttributes_t . scaleType Input Scale type. See CUBLASLT_MATMUL_DESC_SCALE_TYPE of cublasLtMatmulDescAttributes_t . Usually same as computeType. Atype, Btype, Ctype, and Dtype Input Datatype precision for the input and output matrices. algoId Input Specifies the algorithm being initialized. Should be a valid algoId returned by the cublasLtMatmulAlgoGetIds() function. algo Input Pointer to the opaque structure to be initialized. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If algo is NULL or algoId is outside the recognized range. CUBLAS_STATUS_NOT_SUPPORTED If algoId is not supported for given combination of data types. CUBLAS_STATUS_SUCCESS If the structure was successfully initialized. 3.4.25. cublasLtMatmulDescCreate() \\uf0c1 cublasStatus_t cublasLtMatmulDescCreate ( cublasLtMatmulDesc_t * matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function creates a matrix multiply descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor created by this function. See cublasLtMatmulDesc_t . computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function creates. scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function creates. See cudaDataType . Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If memory could not be allocated. CUBLAS_STATUS_SUCCESS If the descriptor was created successfully. 3.4.26. cublasLtMatmulDescInit() \\uf0c1 cublasStatus_t cublasLtMatmulDescInit ( cublasLtMatmulDesc_t matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function initializes a matrix multiply descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor initialized by this function. computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function initializes. scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function initializes. 3.4.27. cublasLtMatmulDescDestroy() \\uf0c1 cublasStatus_t cublasLtMatmulDescDestroy ( cublasLtMatmulDesc_t matmulDesc ); This function destroys a previously created matrix multiply descriptor object. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the structure holding the matrix multiply descriptor that should be destroyed by this function. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If operation was successful. 3.4.28. cublasLtMatmulDescGetAttribute() \\uf0c1 cublasStatus_t cublasLtMatmulDescGetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply descriptor. Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the previously created structure holding the matrix multiply descriptor queried by this function. attr Input The attribute that will be retrieved by this function. See cublasLtMatmulDescAttributes_t . buf Output Memory address containing the attribute value retrieved by this function. 3.4.29. cublasLtMatmulDescSetAttribute() \\uf0c1 cublasStatus_t cublasLtMatmulDescSetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply descriptor. attr Input The attribute that will be set by this function.'},\n",
       " {'id': 854,\n",
       "  'content': 'buf Input The value to which the specified attribute should be set.'},\n",
       " {'id': 855,\n",
       "  'content': '3.4.30. cublasLtMatmulPreferenceCreate() \\uf0c1 cublasStatus_t cublasLtMatmulPreferenceCreate ( cublasLtMatmulPreference_t * pref ); This function creates a matrix multiply heuristic search preferences descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description pref Output Pointer to the structure holding the matrix multiply preferences descriptor created by this function. See cublasLtMatrixLayout_t . 3.4.31. cublasLtMatmulPreferenceInit() \\uf0c1 cublasStatus_t cublasLtMatmulPreferenceInit ( cublasLtMatmulPreference_t pref ); This function initializes a matrix multiply heuristic search preferences descriptor in a previously allocated one. 3.4.32. cublasLtMatmulPreferenceDestroy() \\uf0c1 cublasStatus_t cublasLtMatmulPreferenceDestroy ( cublasLtMatmulPreference_t pref ); This function destroys a previously created matrix multiply preferences descriptor object. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the structure holding the matrix multiply preferences descriptor that should be destroyed by this function. Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the operation was successful. 3.4.33. cublasLtMatmulPreferenceGetAttribute() \\uf0c1 cublasStatus_t cublasLtMatmulPreferenceGetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply heuristic search preferences descriptor. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply heuristic search preferences descriptor queried by this function. attr Input The attribute that will be queried by this function. See cublasLtMatmulPreferenceAttributes_t . 3.4.34. cublasLtMatmulPreferenceSetAttribute() \\uf0c1 cublasStatus_t cublasLtMatmulPreferenceSetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply preferences descriptor. Parameters : Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply preferences descriptor queried by this function. 3.4.35. cublasLtMatrixLayoutCreate() \\uf0c1 cublasStatus_t cublasLtMatrixLayoutCreate ( cublasLtMatrixLayout_t * matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function creates a matrix layout descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor created by this function. type Input Enumerant that specifies the data precision for the matrix layout descriptor this function creates. rows, cols Input Number of rows and columns of the matrix. ld Input The leading dimension of the matrix. In column major layout, this is the number of elements to jump to reach the next column. Thus ld >= m (number of rows). Returns : Return Value Description CUBLAS_STATUS_ALLOC_FAILED If the memory could not be allocated. 3.4.36. cublasLtMatrixLayoutInit() \\uf0c1 cublasStatus_t cublasLtMatrixLayoutInit ( cublasLtMatrixLayout_t matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function initializes a matrix layout descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor initialized by this function. type Input Enumerant that specifies the data precision for the matrix layout descriptor this function initializes. 3.4.37. cublasLtMatrixLayoutDestroy() \\uf0c1 cublasStatus_t cublasLtMatrixLayoutDestroy ( cublasLtMatrixLayout_t matLayout ); This function destroys a previously created matrix layout descriptor object. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the structure holding the matrix layout descriptor that should be destroyed by this function. 3.4.38. cublasLtMatrixLayoutGetAttribute() \\uf0c1 cublasStatus_t cublasLtMatrixLayoutGetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to the specified matrix layout descriptor. Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the previously created structure holding the matrix layout descriptor queried by this function. attr Input The attribute being queried for. See cublasLtMatrixLayoutAttribute_t . 3.4.39. cublasLtMatrixLayoutSetAttribute() \\uf0c1 cublasStatus_t cublasLtMatrixLayoutSetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix layout descriptor. sizeInBytes Input Size of buf , the attribute buffer. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match size of internal storage for the selected attribute. CUBLAS_STATUS_SUCCESS If attribute was set successfully.'},\n",
       " {'id': 856,\n",
       "  'content': '3.4.40. cublasLtMatrixTransform() \\uf0c1 cublasStatus_t cublasLtMatrixTransform ( cublasLtHandle_t lightHandle , cublasLtMatrixTransformDesc_t transformDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * beta , const void * B , cublasLtMatrixLayout_t Bdesc , void * C , cublasLtMatrixLayout_t Cdesc , cudaStream_t stream ); This function computes the matrix transformation operation on the input matrices A and B, to produce the output matrix C, according to the below operation: C = alpha*transformation(A) + beta*transformation(B), where A , B are input matrices, and alpha and beta are input scalars. The transformation operation is defined by the transformDesc pointer. This function can be used to change the memory order of data or to scale and shift the values. transformDesc Input Pointer to the opaque descriptor holding the matrix transformation operation. See cublasLtMatrixTransformDesc_t . A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc , Bdesc and Cdesc . Adesc or Bdesc can be NULL if corresponding pointer is NULL and corresponding scalar is zero. Returns : Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized. CUBLAS_STATUS_INVALID_VALUE If the parameters are in conflict or in an impossible configuration. For example, when A is not NULL, but Adesc is NULL. CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device does not support the configured operation.'},\n",
       " {'id': 857,\n",
       "  'content': '3.4.41. cublasLtMatrixTransformDescCreate() \\uf0c1 cublasStatus_t cublasLtMatrixTransformDescCreate ( cublasLtMatrixTransformDesc_t * transformDesc , cudaDataType scaleType ); This function creates a matrix transform descriptor by allocating the memory needed to hold its opaque structure. Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor created by this function. 3.4.42. cublasLtMatrixTransformDescInit() \\uf0c1 cublasStatus_t cublasLtMatrixTransformDescInit ( cublasLtMatrixTransformDesc_t transformDesc , cudaDataType scaleType ); This function initializes a matrix transform descriptor in a previously allocated one. Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor initialized by this function. 3.4.43. cublasLtMatrixTransformDescDestroy() \\uf0c1 cublasStatus_t cublasLtMatrixTransformDescDestroy ( cublasLtMatrixTransformDesc_t transformDesc ); This function destroys a previously created matrix transform descriptor object. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the structure holding the matrix transform descriptor that should be destroyed by this function. 3.4.44. cublasLtMatrixTransformDescGetAttribute() \\uf0c1 cublasStatus_t cublasLtMatrixTransformDescGetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix transform descriptor. Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the previously created structure holding the matrix transform descriptor queried by this function. See cublasLtMatrixTransformDescAttributes_t . 3.4.45. cublasLtMatrixTransformDescSetAttribute() \\uf0c1 cublasStatus_t cublasLtMatrixTransformDescSetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix transform descriptor. Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes does not match size of the internal storage for the selected attribute. 4.'},\n",
       " {'id': 858,\n",
       "  'content': 'Using the cuBLASXt API \\uf0c1 4.1. General description \\uf0c1 The cuBLASXt API of cuBLAS exposes a multi-GPU capable host interface: when using this API the application only needs to allocate the required matrices on the host memory space. Additionally, the current implementation supports managed memory on Linux with GPU devices that have compute capability 6.x or greater but treats it as host memory. Managed memory is not supported on Windows. There are no restriction on the sizes of the matrices as long as they can fit into the host memory. The cuBLASXt API takes care of allocating the memory across the designated GPUs and dispatched the workload between them and finally retrieves the results back to the host. The cuBLASXt API supports only the compute-intensive BLAS3 routines (e.g matrix-matrix operations) where the PCI transfers back and forth from the GPU can be amortized. The cuBLASXt API has its own header file cublasXt.h . Starting with release 8.0, cuBLASXt API allows any of the matrices to be located on a GPU device. Note : The cuBLASXt API is only supported on 64-bit platforms. 4.1.1. Tiling design approach \\uf0c1 To be able to share the workload between multiples GPUs, the cuBLASXt API uses a tiling strategy : every matrix is divided in square tiles of user-controllable dimension BlockDim x BlockDim. The resulting matrix tiling defines the static scheduling policy : each resulting tile is affected to a GPU in a round robin fashion One CPU thread is created per GPU and is responsible to do the proper memory transfers and cuBLAS operations to compute all the tiles that it is responsible for. From a performance point of view, due to this static scheduling strategy, it is better that compute capabilites and PCI bandwidth are the same for every GPU. The figure below illustrates the tiles distribution between 3 GPUs. To compute the first tile G0 from C, the CPU thread 0 responsible of GPU0, have to load 3 tiles from the first row of A and tiles from the first columun of B in a pipeline fashion in order to overlap memory transfer and computations and sum the results into the first tile G0 of C before to move on to the next tile G0. Example of cublasXtgemm tiling for 3 Gpus \\uf0c1 When the tile dimension is not an exact multiple of the dimensions of C, some tiles are partially filled on the right border or/and the bottom border. The current implementation does not pad the incomplete tiles but simply keep track of those incomplete tiles by doing the right reduced cuBLAS opearations : this way, no extra computation is done. However it still can lead to some load unbalance when all GPUS do not have the same number of incomplete tiles to work on. When one or more matrices are located on some GPU devices, the same tiling approach and workload sharing is applied. The memory transfers are in this case done between devices. However, when the computation of a tile and some data are located on the same GPU device, the memory transfer to/from the local data into tiles is bypassed and the GPU operates directly on the local data. This can lead to a significant performance increase, especially when only one GPU is used for the computation. The matrices can be located on any GPU device, and do not have to be located on the same GPU device. Furthermore, the matrices can even be located on a GPU device that do not participate to the computation. On the contrary of the cuBLAS API, even if all matrices are located on the same device, the cuBLASXt API is still a blocking API from the host point of view : the data results wherever located will be valid on the call return and no device synchronization is required. 4.1.2. Hybrid CPU-GPU computation \\uf0c1 In the case of very large problems, the cuBLASXt API offers the possibility to offload some of the computation to the host CPU. This feature can be setup with the routines cublasXtSetCpuRoutine() and cublasXtSetCpuRatio() The workload affected to the CPU is put aside : it is simply a percentage of the resulting matrix taken from the bottom and the right side whichever dimension is bigger. The GPU tiling is done after that on the reduced resulting matrix. If any of the matrices is located on a GPU device, the feature is ignored and all computation will be done only on the GPUs This feature should be used with caution because it could interfere with the CPU threads responsible of feeding the GPUs. Currently, only the routine cublasXtgemm supports this feature. 4.1.3. Results reproducibility \\uf0c1 Currently all cuBLASXt API routines from a given toolkit version, generate the same bit-wise results when the following conditions are respected : all GPUs particating to the computation have the same compute capabilities and the same number of SMs. the tiles size is kept the same between run. either the CPU hybrid computation is not used or the CPU Blas provided is also guaranteed to produce reproducible results. 4.2. cuBLASXt API Datatypes Reference \\uf0c1 4.2.1. cublasXtHandle_t \\uf0c1 The cublasXtHandle_t type is a pointer type to an opaque structure holding the cuBLASXt API context. The cuBLASXt API context must be initialized using cublasXtCreate() and the returned handle must be passed to all subsequent cuBLASXt API function calls. The context should be destroyed at the end using cublasXtDestroy() . 4.2.2. cublasXtOpType_t \\uf0c1 The cublasOpType_t enumerates the four possible types supported by BLAS routines. This enum is used as parameters of the routines cublasXtSetCpuRoutine and cublasXtSetCpuRatio to setup the hybrid configuration. Value Meaning CUBLASXT_FLOAT float or single precision type CUBLASXT_DOUBLE double precision type CUBLASXT_COMPLEX single precision complex CUBLASXT_DOUBLECOMPLEX double precision complex 4.2.3. cublasXtBlasOp_t \\uf0c1 The cublasXtBlasOp_t type enumerates the BLAS3 or BLAS-like routine supported by cuBLASXt API. Value Meaning CUBLASXT_GEMM GEMM routine CUBLASXT_SYRK SYRK routine CUBLASXT_HERK HERK routine CUBLASXT_SYMM SYMM routine CUBLASXT_HEMM HEMM routine CUBLASXT_TRSM TRSM routine CUBLASXT_SYR2K SYR2K routine CUBLASXT_HER2K HER2K routine CUBLASXT_SPMM SPMM routine CUBLASXT_SYRKX SYRKX routine CUBLASXT_HERKX HERKX routine 4.2.4. cublasXtPinningMemMode_t \\uf0c1 The type is used to enable or disable the Pinning Memory mode through the routine cubasMgSetPinningMemMode Value Meaning CUBLASXT_PINNING_DISABLED the Pinning Memory mode is disabled CUBLASXT_PINNING_ENABLED the Pinning Memory mode is enabled 4.3. cuBLASXt API Helper Function Reference \\uf0c1 4.3.1. cublasXtCreate() \\uf0c1 cublasStatus_t cublasXtCreate ( cublasXtHandle_t * handle ) This function initializes the cuBLASXt API and creates a handle to an opaque structure holding the cuBLASXt API context. It allocates hardware resources on the host and device and must be called prior to making any other cuBLASXt API calls. Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_NOT_SUPPORTED cuBLASXt API is only supported on 64-bit platform 4.3.2. cublasXtDestroy() \\uf0c1 cublasStatus_t cublasXtDestroy ( cublasXtHandle_t handle ) This function releases hardware resources used by the cuBLASXt API context. The release of GPU resources may be deferred until the application exits. This function is usually the last call with a particular handle to the cuBLASXt API. Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 4.3.3. cublasXtDeviceSelect() \\uf0c1 cublasXtDeviceSelect ( cublasXtHandle_t handle , int nbDevices , int deviceId []) This function allows the user to provide the number of GPU devices and their respective Ids that will participate to the subsequent cuBLASXt API Math function calls. This function will create a cuBLAS context for every GPU provided in that list. Currently the device configuration is static and cannot be changed between Math function calls. In that regard, this function should be called only once after cublasXtCreate . To be able to run multiple configurations, multiple cuBLASXt API contexts should be created. Return Value Meaning CUBLAS_STATUS_SUCCESS User call was sucessful CUBLAS_STATUS_INVALID_VALUE Access to at least one of the device could not be done or a cuBLAS context could not be created on at least one of the device CUBLAS_STATUS_ALLOC_FAILED Some resources could not be allocated. 4.3.4. cublasXtSetBlockDim() \\uf0c1 cublasXtSetBlockDim ( cublasXtHandle_t handle , int blockDim ) This function allows the user to set the block dimension used for the tiling of the matrices for the subsequent Math function calls. Matrices are split in square tiles of blockDim x blockDim dimension. This function can be called anytime and will take effect for the following Math function calls. The block dimension should be chosen in a way to optimize the math operation and to make sure that the PCI transfers are well overlapped with the computation. Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blockDim for type and for the corresponding short type to make a more concise and clear presentation of the implemented functions. Unless otherwise specified and have the following meanings: Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision The abbreviation \\\\(\\\\mathbf{Re}(\\\\cdot)\\\\) and \\\\(\\\\mathbf{Im}(\\\\cdot)\\\\) will stand for the real and imaginary part of a number, respectively. 4.4.1. cublasXtgemm() \\uf0c1 cublasStatus_t cublasXtSgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , size_t m , size_t n , size_t k , const float * alpha , const float * A , int lda , const float * B , int ldb , const float * beta , float * C , int ldc ) cublasStatus_t cublasXtDgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const double * alpha , const double * A , int lda , const double * B , int ldb , const double * beta , double * C , int ldc ) cublasStatus_t cublasXtCgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZgemm ( cublasXtHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function performs the matrix-matrix multiplication \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B) + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, and \\\\(A\\\\) , \\\\(B\\\\) and \\\\(C\\\\) are matrices stored in column-major format with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(m \\\\times k\\\\) , \\\\(\\\\text{op}(B)\\\\) \\\\(k \\\\times n\\\\) and \\\\(C\\\\) \\\\(m \\\\times n\\\\) , respectively. Memory In/out Meaning handle input handle to the cuBLASXt API context.'},\n",
       " {'id': 859,\n",
       "  'content': 'alpha host input scalar used for multiplication. A host or device input array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise. B host or device input array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise. beta host input scalar used for multiplication. C host or device in/out array of dimensions ldc x n with ldc>=max(1,m) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n,khemm() \\uf0c1 cublasStatus_t cublasXtChemm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZhemm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the Hermitian matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {\\\\alpha AB + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {\\\\alpha BA + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a Hermitian matrix stored in lower or upper mode, \\\\(B\\\\) and \\\\(C\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. A host or device input array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise.'},\n",
       " {'id': 860,\n",
       "  'content': 'B host or device input array of dimension ldb x n with ldb>=max(1,m) . beta host input scalar used for multiplication, if beta==0 then C does not have to be a valid input. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,nsymm() \\uf0c1 cublasStatus_t cublasXtSsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsymm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the symmetric matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {\\\\alpha AB + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {\\\\alpha BA + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a symmetric matrix stored in lower or upper mode, \\\\(A\\\\) and \\\\(A\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. m input number of rows of matrix A and B , with matrix A sized accordingly.'},\n",
       " {'id': 861,\n",
       "  'content': 'n input number of columns of matrix C and A , with matrix A sized accordingly. A host or device input array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise. beta host input scalar used for multiplication, if beta == 0 then C does not have to be a valid input. C host or device in/out array of dimension ldc x n with ldc>=max(1,m) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,nsyrk() \\uf0c1 cublasStatus_t cublasXtSsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const float * A , int lda , const float * beta , float * C , int ldc ) cublasStatus_t cublasXtDsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const double * A , int lda , const double * beta , double * C , int ldc ) cublasStatus_t cublasXtCsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZsyrk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function performs the symmetric rank- \\\\(k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(A)^{T} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a symmetric matrix stored in lower or upper mode, and \\\\(A\\\\) is a matrix with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) . A host or device input array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise.'},\n",
       " {'id': 862,\n",
       "  'content': 'lda input leading dimension of two-dimensional array used to store matrix A. beta host input scalar used for multiplication, if beta==0 then C does not have to be a valid input. C host or device in/out array of dimension ldc x n , with ldc>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,ksyr2k() \\uf0c1 cublasStatus_t cublasXtSsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsyr2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs the symmetric rank- \\\\(2k\\\\) update \\\\(C = \\\\alpha(\\\\text{op}(A)\\\\text{op}(B)^{T} + \\\\text{op}(B)\\\\text{op}(A)^{T}) + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a symmetric matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively. A host or device input array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise.'},\n",
       " {'id': 863,\n",
       "  'content': 'B host or device input array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. beta host input scalar used for multiplication, if beta==0 , then C does not have to be a valid input. C host or device in/out array of dimensions ldc x n with ldc>=max(1,n) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,ksyrkx() \\uf0c1 cublasStatus_t cublasXtSsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ) cublasStatus_t cublasXtDsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ) cublasStatus_t cublasXtCsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZsyrkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ) This function performs a variation of the symmetric rank- \\\\(k\\\\) update \\\\(C = \\\\alpha(\\\\text{op}(A)\\\\text{op}(B)^{T} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a symmetric matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively. Also, for matrix \\\\(A\\\\) and \\\\(B\\\\) \\\\(\\\\text{op(}A\\\\text{) and op(}B\\\\text{)} = \\\\left\\\\{ \\\\begin{matrix} {A\\\\text{ and }B} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ {A^{T}\\\\text{ and }B^{T}} & {\\\\text{if }\\\\textsf{trans == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric.'},\n",
       " {'id': 864,\n",
       "  'content': 'An usual example is when the matrix B is a scaled form of the matrix A : this is equivalent to B being the product of the matrix A and a diagonal matrix. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,kherk() \\uf0c1 cublasStatus_t cublasXtCherk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasXtZherk ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function performs the Hermitian rank- \\\\(k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(A)^{H} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a Hermitian matrix stored in lower or upper mode, and \\\\(A\\\\) is a matrix with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) . Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,kher2k() \\uf0c1 cublasStatus_t cublasXtCher2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const float * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZher2k ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const double * beta , cuDoubleComplex * C , size_t ldc ) This function performs the Hermitian rank- \\\\(2k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B)^{H} + \\\\overset{ˉ}{\\\\alpha}\\\\text{op}(B)\\\\text{op}(A)^{H} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a Hermitian matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively. B host or device input array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,kherkx() \\uf0c1 cublasStatus_t cublasXtCherkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , const float * beta , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZherkx ( cublasXtHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , size_t n , size_t k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , const double * beta , cuDoubleComplex * C , size_t ldc ) This function performs a variation of the Hermitian rank- \\\\(k\\\\) update \\\\(C = \\\\alpha\\\\text{op}(A)\\\\text{op}(B)^{H} + \\\\beta C\\\\) where \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars, \\\\(C\\\\) is a Hermitian matrix stored in lower or upper mode, and \\\\(A\\\\) and \\\\(B\\\\) are matrices with dimensions \\\\(\\\\text{op}(A)\\\\) \\\\(n \\\\times k\\\\) and \\\\(\\\\text{op}(B)\\\\) \\\\(n \\\\times k\\\\) , respectively. For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasXtdgmm . beta host input real scalar used for multiplication, if beta==0 then C does not have to be a valid input. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters n,ktrsm() \\uf0c1 cublasStatus_t cublasXtStrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const float * alpha , const float * A , size_t lda , float * B , size_t ldb ) cublasStatus_t cublasXtDtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const double * alpha , const double * A , size_t lda , double * B , size_t ldb ) cublasStatus_t cublasXtCtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , cuComplex * B , size_t ldb ) cublasStatus_t cublasXtZtrsm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasXtDiagType_t diag , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , cuDoubleComplex * B , size_t ldb ) This function solves the triangular linear system with multiple right-hand-sides \\\\(\\\\left\\\\{ \\\\begin{matrix} {\\\\text{op}(A)X = \\\\alpha B} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {X\\\\text{op}(A) = \\\\alpha B} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\\\(X\\\\) and \\\\(B\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) is a scalar. alpha host input scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input. B host or device in/out array. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,ntrmm() \\uf0c1 cublasStatus_t cublasXtStrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const float * alpha , const float * A , size_t lda , const float * B , size_t ldb , float * C , size_t ldc ) cublasStatus_t cublasXtDtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const double * alpha , const double * A , size_t lda , const double * B , size_t ldb , double * C , size_t ldc ) cublasStatus_t cublasXtCtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const cuComplex * alpha , const cuComplex * A , size_t lda , const cuComplex * B , size_t ldb , cuComplex * C , size_t ldc ) cublasStatus_t cublasXtZtrmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , cublasDiagType_t diag , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , size_t lda , const cuDoubleComplex * B , size_t ldb , cuDoubleComplex * C , size_t ldc ) This function performs the triangular matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {\\\\alpha\\\\text{op}(A)B} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {\\\\alpha B\\\\text{op}(A)} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\\\(B\\\\) and \\\\(C\\\\) are \\\\(m \\\\times n\\\\) matrix, and \\\\(\\\\alpha\\\\) is a scalar. Also, for matrix \\\\(A\\\\) \\\\(\\\\text{op}(A) = \\\\left\\\\{ \\\\begin{matrix} A & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_N}$}} \\\\\\\\ A^{T} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_T}$}} \\\\\\\\ A^{H} & {\\\\text{if }\\\\textsf{transa == $\\\\mathrm{CUBLAS\\\\_OP\\\\_C}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) Notice that in order to achieve better parallelism, similarly to the cublas API, cuBLASXt API differs from the BLAS API for this routine.'},\n",
       " {'id': 865,\n",
       "  'content': 'The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLASXt API assumes an out-of-place implementation (with results written into C). The application can still obtain the in-place functionality of BLAS in the cuBLASXt API by passing the address of the matrix B in place of the matrix C. No other overlapping in the input parameters is supported. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,nspmm() \\uf0c1 cublasStatus_t cublasXtSspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const float * alpha , const float * AP , const float * B , size_t ldb , const float * beta , float * C , size_t ldc ); cublasStatus_t cublasXtDspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const double * alpha , const double * AP , const double * B , size_t ldb , const double * beta , double * C , size_t ldc ); cublasStatus_t cublasXtCspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * B , size_t ldb , const cuComplex * beta , cuComplex * C , size_t ldc ); cublasStatus_t cublasXtZspmm ( cublasXtHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , size_t m , size_t n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * B , size_t ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , size_t ldc ); This function performs the symmetric packed matrix-matrix multiplication \\\\(C = \\\\left\\\\{ \\\\begin{matrix} {\\\\alpha AB + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_LEFT}$}} \\\\\\\\ {\\\\alpha BA + \\\\beta C} & {\\\\text{if }\\\\textsf{side == $\\\\mathrm{CUBLAS\\\\_SIDE\\\\_RIGHT}$}} \\\\\\\\ \\\\end{matrix} \\\\right.\\\\) where \\\\(A\\\\) is a \\\\(n \\\\times n\\\\) symmetric matrix stored in packed format, \\\\(B\\\\) and \\\\(C\\\\) are \\\\(m \\\\times n\\\\) matrices, and \\\\(\\\\alpha\\\\) and \\\\(\\\\beta\\\\) are scalars. Note The packed matrix AP must be located on the host or managed memory whereas the other matrices can be located on the host or any GPU device Param. AP host input array with \\\\(A\\\\) stored in packed format. Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the parameters m,n #include #include #include \"cublas.h\" #define M 6 #define N 5 #define IDX2F(i,j,ld) ((((j)-1)*(ld))+((i)-1)) static __inline__ void modify ( float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( n - q + 1 , alpha , & m [ IDX2F ( p , q , ldm )], ldm ); cublasSscal ( ldm - p + 1 , beta , & m [ IDX2F ( p , q , ldm )], 1 ); } int main ( void ){ int i , j ; cublasStatus stat ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 1 ; j #include #include #include \"cublas.h\" #define M 6 #define N 5 #define IDX2C(i,j,ld) (((j)*(ld))+(i)) static __inline__ void modify ( float * m , int ldm , int n , int p , int q , float alpha , float beta ){ cublasSscal ( n - q , alpha , & m [ IDX2C ( p , q , ldm )], ldm ); cublasSscal ( ldm - p , beta , & m [ IDX2C ( p , q , ldm )], 1 ); } int main ( void ){ int i , j ; cublasStatus stat ; float * devPtrA ; float * a = 0 ; a = ( float * ) malloc ( M * N * sizeof ( * a )); if ( ! a ) { printf ( \"host memory allocation failed\" ); return EXIT_FAILURE ; } for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { a [ IDX2C ( i , j , M )] = ( float )( i * M + j + 1 ); } } cublasInit (); stat = cublasAlloc ( M * N , sizeof ( * a ), ( void ** ) & devPtrA ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"device memory allocation failed\" ); cublasShutdown (); return EXIT_FAILURE ; } stat = cublasSetMatrix ( M , N , sizeof ( * a ), a , M , devPtrA , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data download failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } modify ( devPtrA , M , N , 1 , 2 , 16.0f , 12.0f ); stat = cublasGetMatrix ( M , N , sizeof ( * a ), devPtrA , M , a , M ); if ( stat != cuBLAS_STATUS_SUCCESS ) { printf ( \"data upload failed\" ); cublasFree ( devPtrA ); cublasShutdown (); return EXIT_FAILURE ; } cublasFree ( devPtrA ); cublasShutdown (); for ( j = 0 ; j < N ; j ++ ) { for ( i = 0 ; i < M ; i ++ ) { printf ( \"%7.0f\" , a [ IDX2C ( i , j , M )]); } printf ( \" \\\\n \" ); } free ( a ); return EXIT_SUCCESS ; } 7. cuBLAS Fortran Bindings \\uf0c1 The cuBLAS library is implemented using the C-based CUDA toolchain.'},\n",
       " {'id': 866,\n",
       "  'content': 'Thus, it provides a C-style API. This makes interfacing to applications written in C and C++ trivial, but the library can also be used by applications written in Fortran. In particular, the cuBLAS library uses 1-based indexing and Fortran-style column-major storage for multidimensional data to simplify interfacing to Fortran applications. Unfortunately, Fortran-to-C calling conventions are not standardized and differ by platform and toolchain. In particular, differences may exist in the following areas: symbol names (capitalization, name decoration) argument passing (by value or reference) passing of string arguments (length information) passing of pointer arguments (size of the pointer) returning floating-point or compound data types (for example single-precision or complex data types) To provide maximum flexibility in addressing those differences, the cuBLAS Fortran interface is provided in the form of wrapper functions and is part of the Toolkit delivery. The C source code of those wrapper functions is located in the src directory and provided in two different forms: the thunking wrapper interface located in the file fortran_thunking.c the direct wrapper interface located in the file fortran.c The code of one of those two files needs to be compiled into an application for it to call the cuBLAS API functions. Providing source code allows users to make any changes necessary for a particular platform and toolchain. The code in those two C files has been used to demonstrate interoperability with the compilers g77 3.2.3 and g95 0.91 on 32-bit Linux, g77 3.4.5 and g95 0.91 on 64-bit Linux, Intel Fortran 9.0 and Intel Fortran 10.0 on 32-bit and 64-bit Microsoft Windows XP, and g77 3.4.0 and g95 0.92 on Mac OS X. Note that for g77, use of the compiler flag -fno-second-underscore is required to use these wrappers as provided. Also, the use of the default calling conventions with regard to argument and return value passing is expected. Using the flag -fno-f2c changes the default calling convention with respect to these two items. The thunking wrappers allow interfacing to existing Fortran applications without any changes to the application. During each call, the wrappers allocate GPU memory, copy source data from CPU memory space to GPU memory space, call cuBLAS, and finally copy back the results to CPU memory space and deallocate the GPU memory. As this process causes very significant call overhead, these wrappers are intended for light testing, not for production code. To use the thunking wrappers, the application needs to be compiled with the file fortran_thunking.c . The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all BLAS functions. To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using cuBLAS_ALLOC and cuBLAS_FREE ) and to copy data between GPU and CPU memory spaces (using cuBLAS_SET_VECTOR , cuBLAS_GET_VECTOR , cuBLAS_SET_MATRIX , and cuBLAS_GET_MATRIX ). The sample wrappers provided in fortran.c map device pointers to the OS-dependent type size_t , which is 32-bit wide on 32-bit platforms and 64-bit wide on a 64-bit platforms. One approach to deal with index arithmetic on device pointers in Fortran code is to use C-style macros, and use the C preprocessor to expand these, as shown in the example below. On Linux and Mac OS X, one way of pre-processing is to use the option -E -x f77-cpp-input when using g77 compiler, or simply the option -cpp when using g95 or gfortran.'},\n",
       " {'id': 867,\n",
       "  'content': 'On Windows platforms with Microsoft Visual C/C++, using ’cl -EP’ achieves similar results. !'},\n",
       " {'id': 868,\n",
       "  'content': 'Example B.1. Fortran 77 Application Executing on the Host ! ---------------------------------------------------------- subroutine modify ( m , ldm , n , p , q , alpha , beta ) implicit none integer ldm , n , p , q real * 4 m ( ldm , * ) , alpha , beta external cublas_sscal call cublas_sscal ( n - p + 1 , alpha , m ( p , q ), ldm ) call cublas_sscal ( ldm - p + 1 , beta , m ( p , q ), 1 ) return end program matrixmod implicit none integer M , N parameter ( M = 6 , N = 5 ) real * 4 a ( M , N ) integer i , j external cublas_init external cublas_shutdown do j = 1 , N do i = 1 , M a ( i , j ) = ( i - 1 ) * M + j enddo enddo call cublas_init call modify ( a , M , N , 2 , 3 , 1 6.0 , 1 2.0 ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , \"(F7.0$)\" ) a ( i , j ) enddo write ( * , * ) \"\" enddo stop end When traditional fixed-form Fortran 77 code is ported to use the cuBLAS library, line length often increases when the BLAS calls are exchanged for cuBLAS calls. Longer function names and possible macro expansion are contributing factors.'},\n",
       " {'id': 869,\n",
       "  'content': 'Inadvertently exceeding the maximum line length can lead to run-time errors that are difficult to find, so care should be taken not to exceed the 72-column limit if fixed form is retained. The examples in this chapter show a small application implemented in Fortran 77 on the host and the same application with the non-thunking wrappers after it has been ported to use the cuBLAS library. The second example should be compiled with ARCH_64 defined as 1 on 64-bit OS system and as 0 on 32-bit OS system. For example for g95 or gfortran, this can be done directly on the command line by using the option -cpp -DARCH_64=1 .'},\n",
       " {'id': 870,\n",
       "  'content': 'Example B.2. Same Application Using Non-thunking cuBLAS Calls !------------------------------------------------------------- #define IDX2F (i,j,ld) ((((j)-1)*(ld))+((i)-1)) subroutine modify ( devPtrM , ldm , n , p , q , alpha , beta ) implicit none integer sizeof_real parameter ( sizeof_real = 4 ) integer ldm , n , p , q #if ARCH_64 integer * 8 devPtrM #else integer * 4 devPtrM #endif real * 4 alpha , beta call cublas_sscal ( n - p + 1 , alpha , 1 devPtrM + IDX2F ( p , q , ldm ) * sizeof_real , 2 ldm ) call cublas_sscal ( ldm - p + 1 , beta , 1 devPtrM + IDX2F ( p , q , ldm ) * sizeof_real , 2 1 ) return end program matrixmod implicit none integer M , N , sizeof_real #if ARCH_64 integer * 8 devPtrA #else integer * 4 devPtrA #endif parameter ( M = 6 , N = 5 , sizeof_real = 4 ) real * 4 a ( M , N ) integer i , j , stat external cublas_init , cublas_set_matrix , cublas_get_matrix external cublas_shutdown , cublas_alloc integer cublas_alloc , cublas_set_matrix , cublas_get_matrix do j = 1 , N do i = 1 , M a ( i , j ) = ( i - 1 ) * M + j enddo enddo call cublas_init stat = cublas_alloc ( M * N , sizeof_real , devPtrA ) if ( stat . NE .'},\n",
       " {'id': 871,\n",
       "  'content': '0 ) then write ( * , * ) \"device memory allocation failed\" call cublas_shutdown stop endif stat = cublas_set_matrix ( M , N , sizeof_real , a , M , devPtrA , M ) if ( stat . 0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data download failed\" call cublas_shutdown stop endif — — Code block continues below. Space added for formatting purposes. — — call modify ( devPtrA , M , N , 2 , 3 , 16.0 , 12.0 ) stat = cublas_get_matrix ( M , N , sizeof_real , devPtrA , M , a , M ) if ( stat . NE .0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data upload failed\" call cublas_shutdown stop endif call cublas_free ( devPtrA ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , \"(F7.0$)\" ) a ( i , j ) enddo write ( * , * ) \"\" enddo stop end 8. Interaction with Other Libraries and Tools \\uf0c1 This section describes important requirements and recommendations that ensure correct use of cuBLAS with other libraries and utilities.'},\n",
       " {'id': 872,\n",
       "  'content': '8.1. nvprune \\uf0c1 nvprune enables pruning relocatable host objects and static libraries to only contain device code for the specific target architectures. In case of cuBLAS, particular care must be taken if using nvprune with compute capabilities, whose minor revision number is different than 0. To reduce binary size, cuBLAS may only store major revision equivalents of CUDA binary files for kernels reused between different minor revision versions. Therefore, to ensure that a pruned library does not fail for arbitrary problems, the user must keep binaries for a selected architecture and all prior minor architectures in its major architecture. For example, the following call prunes libcublas_static.a to contain only sm_75 (Turing) and sm_70 (Volta) cubins: nvprune -- generate - code code = sm_70 -- generate - code code = sm_75 libcublasLt_static . a - o libcublasLt_static_sm70_sm75 . a which should be used instead of: nvprune - arch = sm_75 libcublasLt_static . a - o libcublasLt_static_sm75 .'},\n",
       " {'id': 873,\n",
       "  'content': 'a 9. Acknowledgements \\uf0c1 NVIDIA would like to thank the following individuals and institutions for their contributions: Portions of the SGEMM, DGEMM, CGEMM and ZGEMM library routines were written by Vasily Volkov of the University of California. Portions of the SGEMM, DGEMM and ZGEMM library routines were written by Davide Barbieri of the University of Rome Tor Vergata. Portions of the DGEMM and SGEMM library routines optimized for Fermi architecture were developed by the University of Tennessee. Subsequently, several other routines that are optimized for the Fermi architecture have been derived from these initial DGEMM and SGEMM implementations. The substantial optimizations of the STRSV, DTRSV, CTRSV and ZTRSV library routines were developed by Jonathan Hogg of The Science and Technology Facilities Council (STFC). Subsequently, some optimizations of the STRSM, DTRSM, CTRSM and ZTRSM have been derived from these TRSV implementations. Substantial optimizations of the SYMV and HEMV library routines were developed by Ahmad Abdelfattah, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST). Substantial optimizations of the TRMM and TRSM library routines were developed by Ali Charara, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST). This product includes {fmt} - A modern formatting library https://fmt.dev Copyright (c) 2012 - present, Victor Zverovich. This product includes spdlog - Fast C++ logging library. https://github.com/gabime/spdlog The MIT License (MIT). This product includes SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT https://sleef.org Boost Software License - Version 1.0 - August 17th, 2003. This product includes Frozen - a header-only, constexpr alternative to gperf for C++14 users. https://github.com/serge-sans-paille/frozen Apache License - Version 2.0, January 2004. This product includes Boost C++ Libraries - free peer-reviewed portable C++ source libraries https://www.boost.org/ Boost Software License - Version 1.0 - August 17th, 2003. This product includes Zstandard - a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios. https://github.com/facebook/zstd The BSD License. 10.'},\n",
       " {'id': 874,\n",
       "  'content': 'Notices \\uf0c1 10.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 875,\n",
       "  'content': '10.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 876,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 cuDLA API 1. Modules 1.1.'},\n",
       " {'id': 877,\n",
       "  'content': 'Data types used by cuDLA driver 1.2. ModulesData StructuresData FieldsNotices 2. Data Structures 2.1. cudlaDevAttribute 2.2. cudlaExternalMemoryHandleDesc_t 2.3. cudlaExternalSemaphoreHandleDesc_t 2.4. CudlaFence 2.5. cudlaModuleAttribute 2.6. cudlaModuleTensorDescriptor 2.7. cudlaSignalEvents 2.8. cudlaTask 2.9. cudlaWaitEvents 3. Data Fields Search Results cuDLA API ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback 1. Modules Here is a list of all modules: Data types used by cuDLA driver cuDLA API 1.1. Data types used by cuDLA driver Classes struct CudlaFence union cudlaDevAttribute struct cudlaExternalMemoryHandleDesc_t struct cudlaExternalSemaphoreHandleDesc_t union cudlaModuleAttribute struct cudlaModuleTensorDescriptor struct cudlaSignalEvents struct cudlaTask struct cudlaWaitEvents Typedefs typedef cudlaDevHandle_t * cudlaDevHandle typedef cudlaModule_t * cudlaModule Enumerations enum cudlaAccessPermissionFlags enum cudlaDevAttributeType enum cudlaFenceType enum cudlaMode enum cudlaModuleAttributeType enum cudlaModuleLoadFlags enum cudlaNvSciSyncAttributes enum cudlaStatus enum cudlaSubmissionFlags Typedefs typedef cudlaDevHandle_t * cudlaDevHandle cuDLA Device Handle typedef cudlaModule_t * cudlaModule cuDLA Module Handle Enumerations enum cudlaAccessPermissionFlags Access permission flags for importing NvSciBuffers Values CUDLA_READ_WRITE_PERM = 0 Flag to import memory with read-write permission CUDLA_READ_ONLY_PERM = 1 Flag to import memory with read-only permission CUDLA_TASK_STATISTICS = 1<<1 Flag to indicate buffer as layerwise statistics buffer. enum cudlaDevAttributeType Device attribute type.'},\n",
       " {'id': 878,\n",
       "  'content': 'Values CUDLA_UNIFIED_ADDRESSING = 0 Flag to check for support for UVA. CUDLA_DEVICE_VERSION = 1 Flag to check for DLA HW version. enum cudlaFenceType Supported fence types. Values CUDLA_NVSCISYNC_FENCE = 1 NvSciSync fence type for EOF. CUDLA_NVSCISYNC_FENCE_SOF = 2 enum cudlaMode Device creation modes. Values CUDLA_CUDA_DLA = 0 Hyrbid mode. CUDLA_STANDALONE = 1 Standalone mode. enum cudlaModuleAttributeType Module attribute types. Values CUDLA_NUM_INPUT_TENSORS = 0 Flag to retrieve number of input tensors. CUDLA_NUM_OUTPUT_TENSORS = 1 Flag to retrieve number of output tensors. CUDLA_INPUT_TENSOR_DESCRIPTORS = 2 Flag to retrieve all the input tensor descriptors. CUDLA_OUTPUT_TENSOR_DESCRIPTORS = 3 Flag to retrieve all the output tensor descriptors. CUDLA_NUM_OUTPUT_TASK_STATISTICS = 4 Flag to retrieve total number of output task statistics buffer. CUDLA_OUTPUT_TASK_STATISTICS_DESCRIPTORS = 5 Flag to retrieve all the output task statistics descriptors. enum cudlaModuleLoadFlags Module load flags for cudlaModuleLoadFromMemory . Values CUDLA_MODULE_DEFAULT = 0 Default flag. CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS = 1 Flag to load a module that is used to perform permanent fault diagnostics for DLA HW. enum cudlaNvSciSyncAttributes cuDLA NvSciSync attributes. Values CUDLA_NVSCISYNC_ATTR_WAIT = 1 Wait attribute. CUDLA_NVSCISYNC_ATTR_SIGNAL = 2 Signal attribute. enum cudlaStatus Error codes. Values cudlaSuccess = 0 The API call returned with no errors. cudlaErrorInvalidParam = 1 This indicates that one or more parameters passed to the API is/are incorrect. cudlaErrorOutOfResources = 2 This indicates that the API call failed due to lack of underlying resources. cudlaErrorCreationFailed = 3 This indicates that an internal error occurred during creation of device handle. cudlaErrorInvalidAddress = 4 This indicates that the memory object being passed in the API call has not been registered before. cudlaErrorOs = 5 This indicates that an OS error occurred. cudlaErrorCuda = 6 This indicates that there was an error in a CUDA operation as part of the API call. cudlaErrorUmd = 7 This indicates that there was an error in the DLA runtime for the API call. cudlaErrorInvalidDevice = 8 This indicates that the device handle passed to the API call is invalid. cudlaErrorInvalidAttribute = 9 This indicates that an invalid attribute is being requested. cudlaErrorIncompatibleDlaSWVersion = 10 This indicates that the underlying DLA runtime is incompatible with the current cuDLA version. cudlaErrorMemoryRegistered = 11 This indicates that the memory object is already registered. cudlaErrorInvalidModule = 12 This indicates that the module being passed is invalid. cudlaErrorUnsupportedOperation = 13 This indicates that the operation being requested by the API call is unsupported. cudlaErrorNvSci = 14 This indicates that the NvSci operation requested by the API call failed. cudlaErrorDlaErrInvalidInput = 0x40000001 DLA HW Error. cudlaErrorDlaErrInvalidPreAction = 0x40000002 DLA HW Error. cudlaErrorDlaErrNoMem = 0x40000003 DLA HW Error. cudlaErrorDlaErrProcessorBusy = 0x40000004 DLA HW Error. cudlaErrorDlaErrTaskStatusMismatch = 0x40000005 DLA HW Error. cudlaErrorDlaErrEngineTimeout = 0x40000006 DLA HW Error. cudlaErrorDlaErrDataMismatch = 0x40000007 DLA HW Error. cudlaErrorUnknown = 0x7fffffff This indicates that an unknown error has occurred. enum cudlaSubmissionFlags Task submission flags for cudlaSubmitTask . Values CUDLA_SUBMIT_NOOP = 1 Flag to specify that the submitted task must be bypassed for execution. CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE = 1<<1 Flag to specify that the global lock acquire must be skipped. CUDLA_SUBMIT_DIAGNOSTICS_TASK = 1<<2 Flag to specify that the submitted task is to run permanent fault diagnostics for DLA HW. 2. Data Structures Here are the data structures with brief descriptions: cudlaDevAttribute cudlaExternalMemoryHandleDesc cudlaExternalSemaphoreHandleDesc CudlaFence cudlaModuleAttribute cudlaModuleTensorDescriptor cudlaSignalEvents cudlaTask cudlaWaitEvents 2.1. cudlaDevAttribute Union Reference [ Data types used by cuDLA driver ] Device attribute. Public Variables uint32_t deviceVersion uint8_t unifiedAddressingSupported Variables uint32_t cudlaDevAttribute :: deviceVersion [inherited] DLA device version. Xavier has 1.0 and Orin has 2.0. uint8_t cudlaDevAttribute :: unifiedAddressingSupported [inherited] Returns 0 if unified addressing is not supported. 2.2. cudlaExternalMemoryHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External memory handle descriptor. Public Variables const void * extBufObject unsigned long long size Variables const void * cudlaExternalMemoryHandleDesc_t :: extBufObject [inherited] A handle representing an external memory object. unsigned long long cudlaExternalMemoryHandleDesc_t :: size [inherited] Size of the memory allocation 2.3. cudlaExternalSemaphoreHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External semaphore handle descriptor. Public Variables const void * extSyncObject Variables const void * cudlaExternalSemaphoreHandleDesc_t :: extSyncObject [inherited] A handle representing an external synchronization object. 2.4. CudlaFence Struct Reference [ Data types used by cuDLA driver ] Fence description. Public Variables void * fence cudlaFenceType type Variables void * CudlaFence :: fence [inherited] Fence. cudlaFenceType CudlaFence :: type [inherited] Fence type. 2.5. cudlaModuleAttribute Union Reference [ Data types used by cuDLA driver ] Module attribute. Public Variables cudlaModuleTensorDescriptor * inputTensorDesc uint32_t numInputTensors uint32_t numOutputTensors cudlaModuleTensorDescriptor * outputTensorDesc Variables cudlaModuleTensorDescriptor * cudlaModuleAttribute :: inputTensorDesc [inherited] Returns an array of input tensor descriptors. uint32_t cudlaModuleAttribute :: numInputTensors [inherited] Returns the number of input tensors. uint32_t cudlaModuleAttribute :: numOutputTensors [inherited] Returns the number of output tensors. cudlaModuleTensorDescriptor * cudlaModuleAttribute :: outputTensorDesc [inherited] Returns an array of output tensor descriptors. 2.6. cudlaModuleTensorDescriptor Struct Reference [ Data types used by cuDLA driver ] Tensor descriptor. 2.7. cudlaSignalEvents Struct Reference [ Data types used by cuDLA driver ] Signal events for cudlaSubmitTask Public Variables const * devPtrs CudlaFence * eofFences uint32_t numEvents Variables const * cudlaSignalEvents :: devPtrs [inherited] Array of registered synchronization objects (via cudlaImportExternalSemaphore ). CudlaFence * cudlaSignalEvents :: eofFences [inherited] Array of fences pointers for all the signal events corresponding to the synchronization objects. uint32_t cudlaSignalEvents :: numEvents [inherited] Total number of signal events. 2.8. cudlaTask Struct Reference [ Data types used by cuDLA driver ] Structure of Task. Public Variables const * inputTensor cudlaModule moduleHandle uint32_t numInputTensors uint32_t numOutputTensors const * outputTensor cudlaSignalEvents * signalEvents const cudlaWaitEvents * waitEvents Variables const * cudlaTask :: inputTensor [inherited] Array of input tensors. cudlaModule cudlaTask :: moduleHandle [inherited] cuDLA module handle. uint32_t cudlaTask :: numInputTensors [inherited] Number of input tensors. uint32_t cudlaTask :: numOutputTensors [inherited] Number of output tensors. const * cudlaTask :: outputTensor [inherited] Array of output tensors. cudlaSignalEvents * cudlaTask :: signalEvents [inherited] Signal events. const cudlaWaitEvents * cudlaTask :: waitEvents [inherited] Wait events. 2.9. cudlaWaitEvents Struct Reference [ Data types used by cuDLA driver ] Wait events for cudlaSubmitTask . Public Variables uint32_t numEvents const CudlaFence * preFences Variables uint32_t cudlaWaitEvents :: numEvents [inherited] Total number of wait events. const CudlaFence * cudlaWaitEvents :: preFences [inherited] Array of fence pointers for all the wait events. 3. Data Fields Here is a list of all documented struct and union fields with links to the struct/union documentation for each field: deviceVersion cudlaDevAttribute devPtrs cudlaSignalEvents eofFences cudlaSignalEvents extBufObject cudlaExternalMemoryHandleDesc extSyncObject cudlaExternalSemaphoreHandleDesc fence CudlaFence inputTensor cudlaTask inputTensorDesc cudlaModuleAttribute moduleHandle cudlaTask numEvents cudlaWaitEvents cudlaSignalEvents numInputTensors cudlaTask cudlaModuleAttribute numOutputTensors cudlaTask cudlaModuleAttribute outputTensor cudlaTask outputTensorDesc cudlaModuleAttribute preFences cudlaWaitEvents signalEvents cudlaTask size cudlaExternalMemoryHandleDesc type CudlaFence unifiedAddressingSupported cudlaDevAttribute waitEvents cudlaTask Notices Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein.'},\n",
       " {'id': 879,\n",
       "  'content': 'NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. OpenCL OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 880,\n",
       "  'content': 'Trademarks NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Copyright © 2021 - 2024 NVIDIA Corporation & affiliates.'},\n",
       " {'id': 881,\n",
       "  'content': 'All rights reserved. This product includes software developed by the Syncro Soft SRL (http://www.sync.ro/). Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2021-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();1. Introduction 2.'},\n",
       " {'id': 882,\n",
       "  'content': 'NVBLAS Overview 3. GPU Accelerated Routines 4. BLAS Symbols Interception 5. Device Memory Support 6. Security Precaution 7.'},\n",
       " {'id': 883,\n",
       "  'content': 'Configuration 7.1. NVBLAS_CONFIG_FILE Environment Variable 7.2. Configuration Keywords 7.2.1. NVBLAS_LOGFILE 7.2.2. NVBLAS_TRACE_LOG_ENABLED 7.2.3. NVBLAS_CPU_BLAS_LIB 7.2.4. NVBLAS_GPU_LIST 7.2.5. NVBLAS_TILE_DIM 7.2.6. NVBLAS_GPU_DISABLED_ 7.2.7. NVBLAS_CPU_RATIO_ 7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED 7.2.9. Configuration File Example 8. NVBLAS Installation 9. Usage 10.'},\n",
       " {'id': 884,\n",
       "  'content': 'Notices 10.1. Notice 10.2. OpenCL 10.3. Trademarks NVBLAS » 1. Introduction v12.5 | PDF | Archive NVBLAS The User guide for NVBLAS, drop-in BLAS replacement, multi-GPUs accelerated 1. Introduction \\uf0c1 The NVBLAS Library is a GPU-accelerated Libary that implements BLAS (Basic Linear Algebra Subprograms). It can accelerate most BLAS Level-3 routines by dynamically routing BLAS calls to one or more NVIDIA GPUs present in the system, when the charateristics of the call make it speed up on a GPU. 2. NVBLAS Overview \\uf0c1 The NVBLAS Library is built on top of the cuBLAS Library using only the CUBLASXT API (refer to the CUBLASXT API section of the cuBLAS Documentation for more details). NVBLAS also requires the presence of a CPU BLAS lirbary on the system. Currently NVBLAS intercepts only compute intensive BLAS Level-3 calls (see table below). Depending on the charateristics of those BLAS calls, NVBLAS will redirect the calls to the GPUs present in the system or to CPU. That decision is based on a simple heuristic that estimates if the BLAS call will execute for long enough to amortize the PCI transfers of the input and output data to the GPU. Because NVBLAS does not support all standard BLAS routines, it might be necessary to associate it with an existing full BLAS Library. Please refer to the Usage section for more details. 3. GPU Accelerated Routines \\uf0c1 NVBLAS offloads only the compute-intensive BLAS3 routines which have the best potential for acceleration on GPUs. The following table shows the currently supported routines: Routine Types Operation gemm S,D,C,Z Multiplication of 2 matrices syrk S,D,C,Z Symmetric rank-k update herk C,Z Hermitian rank-k update syr2k S,D,C,Z Symmetric rank-2k update her2k C,Z Hermitian rank-2k update trsm S,D,C,Z Triangular solve with multiple right-hand sides trmm S,D,C,Z Triangular matrix-matrix multiplication symm S,D,C,Z Symmetric matrix-matrix multiplication hemm C,Z Hermitian matrix-matrix multiplication 4. BLAS Symbols Interception \\uf0c1 Standard BLAS Library implementations usually expose multiple symbols for the same routines. Let’s say func is a BLAS routine name, func_ or/and func are usually defined as extern symbols. Some BLAS Libraries might also expose some symbols with a proprietary appended prefix. NVBLAS intercepts only the symbols func_ and func . The user needs to make sure that the application intended to be GPU-accelerated by NVBLAS actually calls those defined symbols. Any other symbols will not be intercepted and the original BLAS routine will be executed for those cases. 5. Device Memory Support \\uf0c1 Starting with Release 8.0, data can be located on any GPU device, even on GPU devices that are not configured to be part of the computation. When any of the data is located on a GPU, the computation will be exclusively done on GPU whatever the size of the problem. Also, this feature has to be used with caution: the user has to be sure that the BLAS call will indeed be intercepted by NVBLAS, otherwise it will result in a crash when the CPU BLAS tries to execute it. 6. Security Precaution \\uf0c1 Because the NVBLAS Library relies on a symbols interception mechanism, it is essential to make sure it has not been compromised. In that regard, NVBLAS should never be used from a process running at elevated privileges, such as Administrator on Windows or root on Linux. 7. Configuration \\uf0c1 Because NVBLAS is a drop-in replacement of BLAS, it must be configured through an ASCII text file that describes how many and which GPUs can participate in the intercepted BLAS calls. The configuration file is parsed at the time of the loading of the library.'},\n",
       " {'id': 885,\n",
       "  'content': 'The format of the configuration file is based on keywords optionally followed by one or more user-defined parameters. At most one keyword per line is allowed. Blank lines or lines beginning with the character # are ignored.'},\n",
       " {'id': 886,\n",
       "  'content': '7.1. NVBLAS_CONFIG_FILE Environment Variable \\uf0c1 The location and name of the configuration file must be defined by the environment variable NVBLAS_CONFIG_FILE . By default, if NVBLAS_CONFIG_FILE is not defined, NVBLAS will try to open the file nvblas.conf in the current directory. For a safe use of NVBLAS, the configuration file should have have restricted write permissions. 7.2.'},\n",
       " {'id': 887,\n",
       "  'content': 'Configuration Keywords \\uf0c1 The configuration keywords syntax is described in the following subsections. 7.2.1. NVBLAS_LOGFILE \\uf0c1 This keyword defines the file where NVBLAS should print status and error messages. By default, if not defined, the standard error output file (eg. stderr) will be used. It is advised to define this keyword early in the configuration to capture errors in parsing that file itself.'},\n",
       " {'id': 888,\n",
       "  'content': '7.2.2. NVBLAS_TRACE_LOG_ENABLED \\uf0c1 When this keyword is defined, every intercepted BLAS calls will be logged into the NVBLAS_LOGFILE. This feature, even though intrusive, can be useful for debugging purposes. 7.2.3. NVBLAS_CPU_BLAS_LIB \\uf0c1 This keyword defines the CPU BLAS dynamic library file (for example, .so file on Linux or .dll on Windows) that NVBLAS should open to find the CPU BLAS symbols definitions. This keyword must be defined for NVBLAS to work. Because CPU Blas libraries are often composed of multiple files, even though this keyword is set to the full path to the main file of the CPU library, it might still be necessary to define the right path to find the rest of the library files in the environment of your system. On Linux, this can be done by setting the environment variable LD_LIBRARY_PATH whereas on Windows, this can be done by setting the environment variable PATH . For a safe use of NVBLAS, the following precautions are strongly advised: The CPU BLAS Library should be located where ordinary users do not have write permissions. The path specified should be absolute, not relative. 7.2.4. NVBLAS_GPU_LIST \\uf0c1 This keyword defines the list of GPUs that should participate in the computation of the intercepted BLAS calls. If not defined, only GPU device 0 is used, since that is normally the most compute-capable GPU installed in the system. This keyword can be set to a list of device numbers separated by blank characters. Also the following wildcard keywords are also accepted for simplicity : Keyword Meaning ALL All compute-capable GPUs detected on the system will be used by NVBLAS ALL0 GPU device 0, AND all others GPUs detected that have the same compute-capabilities as device 0 will be used by NVBLAS Note In the current release of CUBLAS, the CUBLASXT API supports two GPUs if they are on the same board such as Tesla K10 or GeForce GTX690 and one GPU otherwise. Because NVBLAS is built on top of the CUBLASXT API, NVBLAS has the same restriction. If access to more GPUs devices is needed, details of the licensing are described at cublasXt . 7.2.5. NVBLAS_TILE_DIM \\uf0c1 This keyword defines the tile dimension that should be used to divide the matrices involved in the computation. This definition maps directly to a call of the cublasXt API routine cublasXtSetBlockDim . Refer to cuBLAS documentation to understand the tradeoffs associated with setting this to a larger or a smaller value. 7.2.6. NVBLAS_GPU_DISABLED_ \\uf0c1 This keyword, appended with the name of a BLAS routine disables NVBLAS from running a specified routine on the GPU. This feature is intended mainly for debugging purposes. By default, all supported BLAS routines are enabled. 7.2.7. NVBLAS_CPU_RATIO_ \\uf0c1 This keyword, appended with the name of ta BLAS routine defines the ratio of the workload that should remain on the CPU in the event that the NVBLAS decides to offload work for that routine on the GPU. This functionality is directly mapped to the cublasXt API routine cublasXtSetCpuRatio . By default, the ratio is defined to zero for all routines. Please refer to the cuBLAS documentation for details and for the list of routines which support this feature.'},\n",
       " {'id': 889,\n",
       "  'content': '7.2.8. NVBLAS_AUTOPIN_MEM_ENABLED \\uf0c1 This keyword enables the Pinning Memory mode. This functionality is directly mapped to the cublasXt API routine cublasXtSetPinningMemMode . If this keyowrd is not present in the configuration file, the Pinning Memory mode will be set to CUBLASXT_PINNING_DISABLED . Note There are some restrictions to use this feature as specified in the cuBLAS documentation of the underlying routine cublasXtSetPinningMemMode . Specifically when NVBLAS is used in a multi-threaded applications, this option should not be used if there is a chance that matrices used by different threads overlaps while calling NVBLAS. Please refer to the cuBLAS Documentation of the routine `cublasXtSetPinningMemMode `__ for details. 7.2.9. Configuration File Example \\uf0c1 The following example shows a typical NVBLAS configuration file : # This is the configuration file to use NVBLAS Library # Setup the environment variable NVBLAS_CONFIG_FILE to specify your own config file. # By default, if NVBLAS_CONFIG_FILE is not defined, # NVBLAS Library will try to open the file \"nvblas.conf\" in its current directory # Example : NVBLAS_CONFIG_FILE /home/cuda_user/my_nvblas.conf # The config file should have restricted write permissions accesses # Specify which output log file (default is stderr) NVBLAS_LOGFILE nvblas.log # Enable trace log of every intercepted BLAS calls NVBLAS_TRACE_LOG_ENABLED #Put here the CPU BLAS fallback Library of your choice #It is strongly advised to use full path to describe the location of the CPU Library NVBLAS_CPU_BLAS_LIB /usr/lib/libopenblas.so #NVBLAS_CPU_BLAS_LIB /libmkl_rt.so # List of GPU devices Id to participate to the computation # Use ALL if you want all your GPUs to contribute # Use ALL0, if you want all your GPUs of the same type as device 0 to contribute # However, NVBLAS consider that all GPU have the same performance and PCI bandwidth # By default if no GPU are listed, only device 0 will be used #NVBLAS_GPU_LIST 0 2 4 #NVBLAS_GPU_LIST ALL NVBLAS_GPU_LIST ALL0 # Tile Dimension NVBLAS_TILE_DIM 2048 # Autopin Memory NVBLAS_AUTOPIN_MEM_ENABLED #List of BLAS routines that are prevented from running on GPU (use for debugging purpose # The current list of BLAS routines supported by NVBLAS are # GEMM, SYRK, HERK, TRSM, TRMM, SYMM, HEMM, SYR2K, HER2K #NVBLAS_GPU_DISABLED_SGEMM #NVBLAS_GPU_DISABLED_DGEMM #NVBLAS_GPU_DISABLED_CGEMM #NVBLAS_GPU_DISABLED_ZGEMM # Computation can be optionally hybridized between CPU and GPU # By default, GPU-supported BLAS routines are ran fully on GPU # The option NVBLAS_CPU_RATIO_ give the ratio [0,1] # of the amount of computation that should be done on CPU # CAUTION : this option should be used wisely because it can actually # significantly reduced the overall performance if too much work is given to CPU #NVBLAS_CPU_RATIO_CGEMM 0.07 8. NVBLAS Installation \\uf0c1 The NVBLAS Library is part of the CUDA Toolkit, and will be installed along all the other CUDA libraries. It is available on 64-bit operating systems. NVBLAS Library is built on top of cuBLAS, so the cuBLAS library needs to be accessible by NVBLAS. 9. Usage \\uf0c1 To use the NVBLAS Library, the user application must be relinked against NVBLAS in addition to the original CPU Blas (technically only NVBLAS is needed unless some BLAS routines not supported by NVBLAS are used by the application). To be sure that the linker links against the exposed symbols of NVBLAS and not the ones from the CPU BLAS, the NVBLAS Library needs to be put before the CPU BLAS on the linkage command line. On Linux, an alternative way to use NVBLAS Library is to use the LD_PRELOAD environment variable; this technique has the advantage of avoiding the relinkage step. However, the user should avoid defining that environment variable globally because it will cause the NVBLAS library to be loaded by every shell command executed on the system, thus leading to a lack of responsiveness of the system. Finally, mathematical tools and libraries often offer the opportunity to specify the BLAS Library to be used through an environment variable or a configuration file. Because NVBLAS does not support all the standard BLAS routines, it might be necessary to pair NVBLAS with a full BLAS library, even though your application only calls supported NVBLAS routines. Fortunately, those tools and libraries usually offer a way to specify multiple BLAS Libraries. Please refer to the documentation of the appropriate tools and libraries for details.'},\n",
       " {'id': 890, 'content': '10.'},\n",
       " {'id': 891,\n",
       "  'content': 'Notices \\uf0c1 10.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 10.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 892,\n",
       "  'content': '10.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 893,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Introduction 2.'},\n",
       " {'id': 894,\n",
       "  'content': 'Identifiers 3. High Level Structure 3.1. Linkage Types 3.2. Calling Conventions 3.2.1. Rules and Restrictions 3.3. Visibility Styles 3.4. DLL Storage Classes 3.5. Thread Local Storage Models 3.6. Runtime Preemption Specifiers 3.7. Structure Types 3.8. Non-Integral Pointer Type 3.9. Comdats 3.10. source_filename 3.11. Global Variables 3.12. Functions 3.13. Aliases 3.14. Ifuncs 3.15. Named Metadata 3.16. Parameter Attributes 3.17. Garbage Collector Strategy Names 3.18. Prefix Data 3.19.'},\n",
       " {'id': 895,\n",
       "  'content': 'Prologue Data 3.20. Attribute Groups 3.21. Function Attributes 3.22. Global Attributes 3.23. Operand Bundles 3.24. Module-Level Inline Assembly 3.25. Data Layout 3.26.'},\n",
       " {'id': 896,\n",
       "  'content': 'Target Triple 3.27. Pointer Aliasing Rules 3.28. Volatile Memory Access 3.29. Memory Model for Concurrent Operations 3.30. Atomic Memory Ordering Constraints 3.31. Fast-Math Flags 3.32. Use-list Order Directives 4. Type System 5.'},\n",
       " {'id': 897,\n",
       "  'content': 'Constants 6. Other Values 6.1. Inline Assembler Expressions 7. Metadata 7.1. Metadata Nodes and Metadata Strings 8. ThinLTO Summary 9. Intrinsic Global Variables 10. Instructions 10.1.'},\n",
       " {'id': 898,\n",
       "  'content': 'Terminator Instructions 10.2. Binary Operations 10.3. Bitwise Binary Operations 10.4. Vector Operations 10.5. Aggregate Operations 10.6. Memory Access and Addressing Operations 10.6.1. alloca Instruction 10.6.2. load Instruction 10.6.3. store Instruction 10.6.4. fence Instruction 10.6.5. cmpxchg Instruction 10.6.6. atomicrmw Instruction 10.6.7. getelementptr Instruction 10.7. Conversion Operations 10.8.'},\n",
       " {'id': 899,\n",
       "  'content': 'Other Operations 11. Intrinsic Functions 11.1. Variable Argument Handling Intrinsics 11.2. Accurate Garbage Collection Intrinsics 11.3. Code Generator Intrinsics 11.4. Standard C Library Intrinsics 11.5. Bit Manipulations Intrinsics 11.6. Specialised Arithmetic Intrinsics 11.7. Arithmetic with Overflow Intrinsics 11.8. Half Precision Floating Point Intrinsics 11.9. Debugger Intrinsics 11.10. Exception Handling Intrinsics 11.11. Trampoline Intrinsics 11.12. Masked Vector Load and Store Intrinsics 11.13. Masked Vector Expanding Load and Compressing Store Intrinsics 11.14. Experimental Vector Reduction Intrinsics 11.15. Constrained Floating Point Intrinsics 11.16. Constrained libm-equivalent Intrinsics 11.17. Masked Vector Gather and Scatter Intrinsics 11.18. Memory Use Markers 11.19. General Intrinsics 11.20. Element Wise Atomic Memory Intrinsics 11.21. Stack Map Intrinsics 12. Address Space 12.1. Address Spaces 12.2. Generic Pointers and Non-Generic Pointers 12.2.1. Generic Pointers vs. Non-generic Pointers 12.2.2. Conversion 12.2.3. No Aliasing between Two Different Specific Address Spaces 12.3. The alloca Instruction 13. Global Property Annotation 13.1. Overview 13.2. Representation of Properties 13.3. Supported Properties 14. Texture and Surface 14.1. Texture Variable and Surface Variable 14.2. Accessing Texture Memory or Surface Memory 15. NVVM Specific Intrinsic Functions 15.1. Atomic 15.2. Barrier and Memory Fence 15.3. Address space conversion 15.4. Special Registers 15.5. Texture/Surface Access 15.5.1. Texture Reads 15.5.2. Surface Loads 15.5.3. Surface Stores 15.6. Warp-level Operations 15.6.1. Barrier Synchronization 15.6.2. Data Movement 15.6.3. Vote 15.6.4. Match 15.6.5. Matrix Operation 15.6.5.1. Load Fragments 15.6.5.2. Store Fragments 15.6.5.3. Matrix Multiply-and-Accumulate 16. Source Level Debugging Support 17. NVVM ABI for PTX 17.1. Linkage Types 17.2. Parameter Passing and Return 18. Revision History 19. Notices 19.1. Notice 19.2. OpenCL 19.3. Trademarks NVVM IR Specification » 1. Introduction v12.5 | PDF | Archive NVVM IR Specification Reference guide to the NVVM compiler (intermediate representation) based on the LLVM IR. Introduction \\uf0c1 NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR. The NVVM compiler (which is based on LLVM) generates PTX code from NVVM IR. NVVM IR and NVVM compilers are mostly agnostic about the source language being used. The PTX codegen part of a NVVM compiler needs to know the source language because of the difference in DCI (driver/compiler interface). NVVM IR is a binary format and is based on a subset of LLVM IR bitcode format. This document uses only human-readable form to describe NVVM IR. Technically speaking, NVVM IR is LLVM IR with a set of rules, restrictions, and conventions, plus a set of supported intrinsic functions. A program specified in NVVM IR is always a legal LLVM program. A legal LLVM program may not be a legal NVVM program. There are three levels of support for NVVM IR. Supported: The feature is fully supported. Most IR features should fall into this category. Accepted and ignored: The NVVM compiler will accept this IR feature, but will ignore the required semantics. This applies to some IR features that do not have meaningful semantics on GPUs and that can be ignored. Calling convention markings are an example. Illegal, not supported: The specified semantics is not supported, such as a fence instruction. Future versions of NVVM may either support or accept and ignore IRs that are illegal in the current version. This document describes version 2.0 of the NVVM IR and version 3.1 of the NVVM debug metadata (see Source Level Debugging Support ). The 2.0 version of NVVM IR is incompatible with the previous version 1.11. Linking of NVVM IR Version 1.11 with 2.0 will result in compiler error. The current NVVM IR is based on LLVM 7.0.1. For the complete semantics of the IR, readers of this document should check the official LLVM Language Reference Manual ( https://releases.llvm.org/7.0.1/docs/LangRef.html ). 2. Identifiers \\uf0c1 The name of a named global identifier must have the form: @[a-zA-Z$_][a-zA-Z$_0-9]* Note that it cannot contain the . character. [@%]llvm.nvvm. * and [@%]nvvm. * are reserved words. 3.'},\n",
       " {'id': 900,\n",
       "  'content': 'High Level Structure \\uf0c1 3.1. Linkage Types \\uf0c1 Supported: private internal available_externally linkonce weak common linkonce_odr weak_odr external Not supported: appending extern_weak See NVVM ABI for PTX for details on how linkage types are translated to PTX. 3.2. Calling Conventions \\uf0c1 All LLVM calling convention markings are accepted and ignored. Functions and calls are generated according to the PTX calling convention. 3.2.1. Rules and Restrictions \\uf0c1 When an argument with width less than 32-bit is passed, the zeroext/signext parameter attribute should be set. zeroext will be assumed if not set. When a value with width less than 32-bit is returned, the zeroext/signext parameter attribute should be set. Arguments of aggregate or vector types that are passed by value can be passed by pointer with the byval attribute set (referred to as the by-pointer-byval case below). The align attribute must be set if the type requires a non-natural alignment (natural alignment is the alignment inferred for the aggregate type according to the Data Layout section). If a function has an argument of aggregate or vector type that is passed by value directly and the type has a non-natural alignment requirement, the alignment must be annotated by the global property annotation , where alignment is a 32-bit integer whose upper 16 bits represent the argument position (starting from 1) and the lower 16 bits represent the alignment. If the return type of a function is an aggregate or a vector that has a non-natural alignment, then the alignment requirement must be annotated by the global property annotation , where the upper 16 bits is 0, and the lower 16 bits represent the alignment. It is not required to annotate a function with otherwise. If annotated, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case. For an indirect call instruction of a function that has a non-natural alignment for its return value or one of its arguments that is not expressed in alignment in the by-pointer-byval case, the call instruction must have an attached metadata of kind callalign . The metadata contains a sequence of i32 fields each of which represents a non-natural alignment requirement. The upper 16 bits of an i32 field represent the argument position (0 for return value, 1 for the first argument, and so on) and the lower 16 bits represent the alignment. The i32 fields must be sorted in the increasing order. For example, % call = call % struct . S % fp1 ( % struct . S * byval align 8 % arg1p , % struct . S % arg2 ), ! callalign ! 10 !'},\n",
       " {'id': 901,\n",
       "  'content': '10 = ! { i32 8 , i32 520 }; It is not required to have an i32 metadata field for the other arguments or the return value otherwise. If presented, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case . It is not required to have a callalign metadata attached to a direct call instruction. If attached, the alignment must match the natural alignment or the alignment in the by-pointer-byval case. The absence of the metadata in an indirect call instruction means using natural alignment or the align attribute in the by-pointer-byval case. 3.3.'},\n",
       " {'id': 902,\n",
       "  'content': 'Visibility Styles \\uf0c1 All styles—default, hidden, and protected—are accepted and ignored. 3.4. DLL Storage Classes \\uf0c1 Not supported. 3.5. Thread Local Storage Models \\uf0c1 Not supported. 3.6. Runtime Preemption Specifiers \\uf0c1 Not supported. 3.7. Structure Types \\uf0c1 Fully supported. 3.8. Non-Integral Pointer Type \\uf0c1 Not supported. 3.9. Comdats \\uf0c1 Not supported. 3.10. source_filename \\uf0c1 Accepted and ignored.'},\n",
       " {'id': 903,\n",
       "  'content': '3.11. Global Variables \\uf0c1 A global variable, that is not an intrinsic global variable, may be optionally declared to reside in one of the following address spaces: global shared constant If no address space is explicitly specified, the global variable is assumed to reside in the global address space with a generic address value. See Address Space for details. thread_local variables are not supported. No explicit section (except for the metadata section) is allowed. Initializations of shared variables are not supported. Use undef initialization.'},\n",
       " {'id': 904,\n",
       "  'content': '3.12. Functions \\uf0c1 The following are not supported on functions: Alignment Explicit section Garbage collector name Prefix data Prologue Personality 3.13. Aliases \\uf0c1 Supported only as aliases of non-kernel functions. 3.14. Ifuncs \\uf0c1 Not supported. 3.15. Named Metadata \\uf0c1 Accepted and ignored, except for the following: !nvvm.annotations : see Global Property Annotation !nvvmir.version !llvm.dbg.cu !llvm.module.flags The NVVM IR version is specified using a named metadata called !nvvmir.version . The !nvvmir.version named metadata may have one metadata node that contains the NVVM IR version for that module. If multiple such modules are linked together, the named metadata in the linked module may have more than one metadata node with each node containing a version. A metadata node with NVVM IR version takes either of the following forms: It may consist of two i32 values—the first denotes the NVVM IR major version number and the second denotes the minor version number. If absent, the version number is assumed to be 1.0, which can be specified as: !nvvmir.version = ! {!0} !0 = ! {i32 1, i32 0} It may consist of four i32 values—the first two denote the NVVM IR major and minor versions respectively. The third value denotes the NVVM IR debug metadata major version number, and the fourth value denotes the corresponding minor version number. {i32 1, i32 0, i32 1, i32 0} The version of NVVM IR described in this document is 2.0. The version of NVVM IR debug metadata described in this document is 3.1. 3.16. Parameter Attributes \\uf0c1 Fully supported, except the following: Accepted and ignored: inreg nest Not supported: inalloca swiftself swifterror See Calling Conventions for the use of the attributes. 3.17.'},\n",
       " {'id': 905,\n",
       "  'content': 'Garbage Collector Strategy Names \\uf0c1 Not supported. 3.18.'},\n",
       " {'id': 906,\n",
       "  'content': 'Prefix Data \\uf0c1 Not supported. 3.19. Prologue Data \\uf0c1 Not supported. 3.20.'},\n",
       " {'id': 907,\n",
       "  'content': 'Attribute Groups \\uf0c1 Fully supported. The set of supported attributes is equal to the set of attributes accepted where the attribute group is used.'},\n",
       " {'id': 908,\n",
       "  'content': '3.21. Function Attributes \\uf0c1 Supported: allocsize alwaysinline cold convergent inaccessiblememonly inaccessiblemem_or_argmemonly inlinehint minsize no-jump-tables noduplicate noinline noreturn norecurse nounwind \"null-pointer-is-valid\" optforfuzzing optnone optsize readnone readonly writeonly argmemonly speculatable strictfp Not Supported: alignstack builtin nonlazybind naked nobuiltin noimplicitfloat noredzone \"patchable-function\" probe-stack returns_twice sanitize_address sanitize_memory sanitize_thread sanitize_hwaddress ssp sspreq sspstrong \"stack-probe-size\" \"no-stack-arg-probe\" uwtable jumptable safestack \"thunk\" nocf_check shadowcallstack 3.22. Global Attributes \\uf0c1 Not supported.'},\n",
       " {'id': 909,\n",
       "  'content': '3.23. Operand Bundles \\uf0c1 Not supported. 3.24. Module-Level Inline Assembly \\uf0c1 Supported. 3.25. Data Layout \\uf0c1 Only the following data layout is supported: 64-bit e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 The following data layouts are deprecated and will be removed in a future release. 32-bit e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 64-bit e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 3.26. Target Triple \\uf0c1 Only the following target triple is supported, where * can be any name: 64-bit: nvptx64-*-cuda The following target triple is deprecated, and will be removed in future release: 32-bit: nvptx-*-cuda 3.27.'},\n",
       " {'id': 910, 'content': 'Pointer Aliasing Rules \\uf0c1 Fully supported.'},\n",
       " {'id': 911,\n",
       "  'content': '3.28. Volatile Memory Access \\uf0c1 Fully supported. Note that for code generation: ld.volatile and st.volatile will be generated. 3.29.'},\n",
       " {'id': 912,\n",
       "  'content': 'Memory Model for Concurrent Operations \\uf0c1 Not applicable. Threads in an NVVM IR program must use atomic operations or barrier synchronization to communicate. 3.30.'},\n",
       " {'id': 913,\n",
       "  'content': 'Atomic Memory Ordering Constraints \\uf0c1 Atomic loads and stores are not supported. Other atomic operations on other than 32-bit or 64-bit operands are not supported. 3.31. Fast-Math Flags \\uf0c1 Supported. 3.32. Use-list Order Directives \\uf0c1 Not supported. 4. Type System \\uf0c1 Fully supported, except for the following: Floating point types half , fp128 , x86_fp80 and ppc_fp128 are not supported. The x86_mmx type is not supported. The token type is not supported. The non-integral pointer type is not supported. 5.'},\n",
       " {'id': 914,\n",
       "  'content': 'Constants \\uf0c1 Fully supported, except for the following: Token constants is not supported. blockaddress(@function, %block) is not supported. For a constant expression that is used as the initializer of a global variable @g1 , if the constant expression contains a global identifier @g2 , then the constant expression is supported if it can be reduced to the form of bitcast+offset , where offset is an integer number (including 0 ) 6. Other Values \\uf0c1 6.1. Inline Assembler Expressions \\uf0c1 Inline assembler of PTX instructions is supported, with the following supported constraints: Constraint Type c i8 h i16 r i32 l i64 f f32 d f64 The inline asm metadata !srcloc is accepted and ignored. The inline asm dialect inteldialect is not supported. 7.'},\n",
       " {'id': 915,\n",
       "  'content': 'Metadata \\uf0c1 7.1. Metadata Nodes and Metadata Strings \\uf0c1 Fully supported. The following metadata are understood by the NVVM compiler: Specialized Metadata Nodes llvm.loop.unroll.count llvm.loop.unroll.disable llvm.loop.unroll.full callalign (see Rules and Restrictions for Calling Conventions) Module flags metadata ( llvm.module.flags ) is supported and verified, but the metadata values will be ignored. All other metadata is accepted and ignored.'},\n",
       " {'id': 916,\n",
       "  'content': '8. ThinLTO Summary \\uf0c1 Not supported. 9. Intrinsic Global Variables \\uf0c1 The llvm.used global variable is supported. The llvm.compiler.used global variable is supported The llvm.global_ctors global variable is not supported The llvm.global_dtors global variable is not supported 10. Instructions \\uf0c1 10.1. Terminator Instructions \\uf0c1 Supported: ret br switch unreachable Unsupported: indirectbr invoke resume catchswitch catchret cleanupret 10.2. Binary Operations \\uf0c1 Supported: add fadd sub fsub mul fmul udiv sdiv fdiv urem srem frem 10.3.'},\n",
       " {'id': 917,\n",
       "  'content': 'Bitwise Binary Operations \\uf0c1 Supported: shl lshr ashr and or xor 10.4. Vector Operations \\uf0c1 Supported: extractelement insertelement shufflevector 10.5. Aggregate Operations \\uf0c1 Supported: extractvalue insertvalue 10.6. Memory Access and Addressing Operations \\uf0c1 10.6.1. alloca Instruction \\uf0c1 The alloca instruction returns a generic pointer to the local address space. The inalloca attribute is not supported. Maximum alignment supported is 2^23. The addrspace() specifier is supported only if num is 0. 10.6.2.'},\n",
       " {'id': 918,\n",
       "  'content': 'load Instruction \\uf0c1 load atomic is not supported. 10.6.3. store Instruction \\uf0c1 store atomic is not supported. 10.6.4. fence Instruction \\uf0c1 Not supported. Use NVVM intrinsic functions instead.'},\n",
       " {'id': 919,\n",
       "  'content': '10.6.5. cmpxchg Instruction \\uf0c1 Supported for i32 , i64 , and i128 types, with the following restrictions: The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space. The weak marker and the failure ordering are accepted and ignored. The i128 type is only supported on compute_90 and above. 10.6.6. atomicrmw Instruction \\uf0c1 nand is not supported. The other keywords are supported for i32 , i64 , and i128 types, with the following restrictions.'},\n",
       " {'id': 920,\n",
       "  'content': 'The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space. For i128 , only xchg is supported, and only on compute_90 and above. 10.6.7. getelementptr Instruction \\uf0c1 Fully supported. 10.7. Conversion Operations \\uf0c1 Supported: trunc .. to zext .. to sext .. to fptrunc .. to fpext .. to fptoui .. to fptosi .. to uitofp .. to sitofp .. to ptrtoint .. to inttoptr .. to addrspacecast .. to bitcast .. to See Conversion for a special use case of bitcast . 10.8. Other Operations \\uf0c1 Supported: icmp fcmp phi select va_arg call (See Calling Conventions for other rules and restrictions.)\\nUnsupported: landingpad catchpad cleanuppad 11. Intrinsic Functions \\uf0c1 11.1.'},\n",
       " {'id': 921,\n",
       "  'content': 'Variable Argument Handling Intrinsics \\uf0c1 llvm.va_start Supported. llvm.va_end Supported. llvm.va_copy Supported. 11.2. Accurate Garbage Collection Intrinsics \\uf0c1 Not supported. 11.3. Code Generator Intrinsics \\uf0c1 Not supported. 11.4. Standard C Library Intrinsics \\uf0c1 llvm.memcpy Supported. Note that the constant address space cannot be used as the destination since it is read-only. llvm.memmove Supported. Note that the constant address space cannot be used since it is read-only. llvm.memset Supported. llvm.sqrt Supported for float/double and vector of float/double. Mapped to PTX sqrt.rn.f32 and sqrt.rn.f64 . llvm.powi Not supported. llvm.sin Not supported. llvm.cos Not supported. llvm.pow Not supported. llvm.exp Not supported. llvm.exp2 Not supported. llvm.log Not supported. llvm.log10 Not supported. llvm.log2 Not supported. llvm.fma Supported for float/double and vector of float/double. Mapped to PTX fma.rn.f32 and fma.rn.f64 llvm.fabs Not supported. llvm.copysign Not supported. llvm.floor Not supported. llvm.ceil Not supported. llvm.trunc Not supported. llvm.rint Not supported. llvm.nearbyint Not supported. llvm.round Not supported. llvm.minnum Not supported. llvm.maxnum Not supported. 11.5. Bit Manipulations Intrinsics \\uf0c1 llvm.bitreverse Supported for i8 , i16 , i32 , and i64 . llvm.bswap Supported for i16 , i32 , and i64 . llvm.ctpop Supported for i8 , i16 , i32 , i64 , and vectors of these types. llvm.ctlz Supported for i8 , i16 , i32 , i64 , and vectors of these types. llvm.cttz Supported for i8 , i16 , i32 , i64 , and vectors of these types. llvm.fshl Supported for i8 , i16 , i32 , and i64 . llvm.fshr Supported for i8 , i16 , i32 , and i64 . 11.6. Specialised Arithmetic Intrinsics \\uf0c1 llvm.fmuladd Supported. llvm.canonicalize Not supported. 11.7. Arithmetic with Overflow Intrinsics \\uf0c1 Supported for i16 , i32 , and i64 . 11.8. Half Precision Floating Point Intrinsics \\uf0c1 Supported: llvm.convert.to.fp16 , llvm.convert.from.fp16 11.9. Debugger Intrinsics \\uf0c1 llvm.dbg.addr Supported. llvm.dbg.declare Supported. llvm.dbg.value Supported. 11.10. Exception Handling Intrinsics \\uf0c1 Not supported. 11.11. Trampoline Intrinsics \\uf0c1 Not supported. 11.12. Masked Vector Load and Store Intrinsics \\uf0c1 Not supported. 11.13. Masked Vector Expanding Load and Compressing Store Intrinsics \\uf0c1 Not supported. 11.14. Experimental Vector Reduction Intrinsics \\uf0c1 Not supported. 11.15. Constrained Floating Point Intrinsics \\uf0c1 Not supported. 11.16. Constrained libm-equivalent Intrinsics \\uf0c1 Not supported. 11.17. Masked Vector Gather and Scatter Intrinsics \\uf0c1 Not supported. 11.18. Memory Use Markers \\uf0c1 Supported: llvm.lifetime.start , llvm.lifetime.end , llvm.invariant.start , and llvm.invariant.end . Not supported: llvm.launder.invariant.group , llvm.strip.invariant.group . 11.19. General Intrinsics \\uf0c1 llvm.var.annotation Accepted and ignored. llvm.ptr.annotation Accepted and ignored. llvm.annotation Accepted and ignored. llvm.codeview.annotation Not supported. llvm.trap Supported. llvm.debugtrap Not supported. llvm.stackguard Not supported. llvm.stackprotector Not supported. llvm.objectsize Not supported. llvm.expect Supported. llvm.assume Supported. llvm.ssa_copy Not supported. llvm.type.test Not supported. llvm.type.checked.load Not supported. llvm.donothing Supported. llvm.experimental.deoptimize Not supported. llvm.experimental.guard Not supported. llvm.load.relative Not supported. llvm.sideeffect Supported. 11.20. Element Wise Atomic Memory Intrinsics \\uf0c1 Not supported. 11.21. Stack Map Intrinsics \\uf0c1 Not supported. 12.'},\n",
       " {'id': 922,\n",
       "  'content': 'Address Space \\uf0c1 12.1. Address Spaces \\uf0c1 NVVM IR has a set of predefined memory address spaces, whose semantics are similar to those defined in CUDA C/C++, OpenCL C and PTX. Any address space not listed below is not supported . Name Address Space Number Semantics/Example code 0 functions, code CUDA C/C++ function OpenCL C function generic 0 Can only be used to qualify the pointee of a pointer Pointers in CUDA C/C++ global 1 CUDA C/C++ __device__ OpenCL C global shared 3 CUDA C/C++ __shared__ OpenCL C local constant 4 CUDA C/C++ __constant__ OpenCL C constant local 5 CUDA C/C++ local OpenCL C private 2, 101 and above Each global variable, that is not an intrinsic global variable, can be declared to reside in a specific non-zero address space, which can only be one of the following: global , shared or constant . If a non-intrinsic global variable is declared without any address space number or with the address space number 0, then this global variable resides in address space global and the pointer of this global variable holds a generic pointer value. The predefined NVVM memory spaces are needed for the language front-ends to model the memory spaces in the source languages. For example, // CUDA C/C++ __constant__ int c; __device__ int g; ; NVVM IR @c = addrspace(4) global i32 0, align 4 @g = addrspace(1) global [2 x i32] zeroinitializer, align 4 Address space numbers 2 and 101 or higher are reserved for NVVM compiler internal use only. No language front-end should generate code that uses these address spaces directly. 12.2.'},\n",
       " {'id': 923,\n",
       "  'content': \"Generic Pointers and Non-Generic Pointers \\uf0c1 12.2.1. Generic Pointers vs. Non-generic Pointers \\uf0c1 There are generic pointers and non-generic pointers in NVVM IR. A generic pointer is a pointer that may point to memory in any address space. A non-generic pointer points to memory in a specific address space. In NVVM IR, a generic pointer has a pointer type with the address space generic , while a non-generic pointer has a pointer type with a non-generic address space. Note that the address space number for the generic address space is 0—the default in both NVVM IR and LLVM IR. The address space number for the code address space is also 0. Function pointers are qualified by address space code ( addrspace(0) ). Loads/stores via generic pointers are supported, as well as loads/stores via non-generic pointers. Loads/stores via function pointers are not supported @a = addrspace(1) global i32 0, align 4 ; 'global' addrspace, @a holds a specific value @b = global i32 0, align 4 ; 'global' addrspace, @b holds a generic value @c = addrspace(4) global i32 0, align 4 ; 'constant' addrspace, @c holds a specific value ... = load i32 addrspace(1)* @a, align 4 ; Correct ... = load i32* @a, align 4 ; Wrong ... = load i32* @b, align 4 ; Correct ... = load i32 addrspace(1)* @b, align 4 ; Wrong ... = load i32 addrspace(4)* @c, align4 ; Correct ... = load i32* @c, align 4 ; Wrong 12.2.2. Conversion \\uf0c1 The bit value of a generic pointer that points to a specific object may be different from the bit value of a specific pointer that points to the same object. The addrspacecast IR instruction should be used to perform pointer casts across address spaces (generic to non-generic or non-generic to generic). Casting a non-generic pointer to a different non-generic pointer is not supported. Casting from a generic to a non-generic pointer is undefined if the generic pointer does not point to an object in the target non-generic address space. inttoptr and ptrtoint are supported. inttoptr and ptrtoint are value preserving instructions when the two operands are of the same size. In general, using ptrtoint and inttoptr to implement an address space cast is undefined. The following intrinsic can be used to query if the argument pointer was derived from the address of a kernel function parameter that has the grid_constant property: i1 @llvm.nvvm.isspacep.grid_const(i8*) The following intrinsic can be used to query if the input generic pointer was derived from the address of a variable allocated in the shared address space, in a CTA that is part of the same cluster as the parent CTA of the invoking thread. This intrinsic is only supported for Hopper+. i1 @llvm.nvvm.isspacep.cluster_shared(i8*) The following intrinsics can be used to query if a generic pointer can be safely cast to a specific non-generic address space: i1 @llvm.nvvm.isspacep.const(i8*) i1 @llvm.nvvm.isspacep.global(i8*) i1 @llvm.nvvm.isspacep.local(i8*) i1 @llvm.nvvm.isspacep.shared(i8*) bitcast on pointers is supported, though LLVM IR forbids bitcast from being used to change the address space of a pointer. 12.2.3. No Aliasing between Two Different Specific Address Spaces \\uf0c1 Two different specific address spaces do not overlap. NVVM compiler assumes two memory accesses via non-generic pointers that point to different address spaces are not aliased. 12.3. The alloca Instruction \\uf0c1 The alloca instruction returns a generic pointer that only points to address space local . 13.\"},\n",
       " {'id': 924,\n",
       "  'content': 'Global Property Annotation \\uf0c1 13.1. Overview \\uf0c1 NVVM uses Named Metadata to annotate IR objects with properties that are otherwise not representable in the IR. The NVVM IR producers can use the Named Metadata to annotate the IR with properties, which the NVVM compiler can process. 13.2. Representation of Properties \\uf0c1 For each translation unit (that is, per bitcode file), there is a named metadata called nvvm.annotations . This named metadata contains a list of MDNodes. The first operand of each MDNode is an entity that the node is annotating using the remaining operands. Multiple MDNodes may provide annotations for the same entity, in which case their first operands will be same. The remaining operands of the MDNode are organized in order as . The property-name operand is MDString, while the value is i32 . Starting with the operand after the annotated entity, every alternate operand specifies a property. The operand after a property is its value. The following is an example. !nvvm.annotations = ! {!12, !13} !12 = ! {void (i32, i32)* @_Z6kernelii, ! \"kernel\", i32 1} !13 = ! {void ()* @_Z7kernel2v, !'},\n",
       " {'id': 925,\n",
       "  'content': '\"kernel\", i32 1, ! \"maxntidx\", i32 16} If two bitcode files are being linked and both have a named metadata nvvm.annotations , the linked file will have a single merged named metadata. If both files define properties for the same entity foo , the linked file will have two MDNodes defining properties for foo . It is illegal for the files to have conflicting properties for the same entity. 13.3. Supported Properties \\uf0c1 Property Name Annotated On Description maxntid{x, y, z} kernel function Maximum expected CTA size from any launch. reqntid{x, y, z} kernel function Minimum expected CTA size from any launch. cluster_dim_{x,y,z} kernel function Support for cluster dimensions for Hopper+. If any dimension is specified as 0, then all dimensions must be specified as 0. cluster_max_blocks kernel function Maximum number of blocks per cluster. Must be non-zero. Only supported for Hopper+. minctasm kernel function Hint/directive to the compiler/driver, asking it to put at least these many CTAs on an SM. grid_constant kernel function The argument is a metadata node, which contains a list of integers, where each integer n denotes that the nth parameter has the grid_constant annotation (numbering from 1). The parameter’s type must be of pointer type with byval attribute set. It is undefined behavior to write to memory pointed to by the parameter. This property is only supported for Volta+. maxnreg function Maximum number of registers for function. kernel function Signifies that this function is a kernel function. align function Signifies that the value in low 16-bits of the 32-bit value contains alignment of n th parameter type if its alignment is not the natural alignment. n is specified by high 16-bits of the value.'},\n",
       " {'id': 926,\n",
       "  'content': 'For return type, n is 0. texture global variable Signifies that variable is a texture. surface global variable Signifies that variable is a surface. managed global variable Signifies that variable is a UVM managed variable. 14. Texture and Surface \\uf0c1 14.1. Texture Variable and Surface Variable \\uf0c1 A texture or a surface variable can be declared/defined as a global variable of i64 type with annotation texture or surface in the global address space. A texture or surface variable must have a name, which must follow identifier naming conventions. It is illegal to store to or load from the address of a texture or surface variable. A texture or a surface variable may only have the following uses: In a metadata node As an intrinsic function argument as shown below In llvm.used Global Variable 14.2. Accessing Texture Memory or Surface Memory \\uf0c1 Texture memory and surface memory can be accessed using texture or surface handles. NVVM provides the following intrinsic function to get a texture or surface handle from a texture or surface variable. delcare i64 % llvm .'},\n",
       " {'id': 927, 'content': 'nvvm . texsurf .'},\n",
       " {'id': 928,\n",
       "  'content': 'handle . p1i64 ( metadata , i64 addrspace ( 1 ) * ) The first argument to the intrinsic is a metadata holding the texture or surface variable. Such a metadata may hold only one texture or one surface variable. The second argument to the intrinsic is the texture or surface variable itself. The intrinsic returns a handle of i64 type. The returned handle value from the intrinsic call can be used as an operand (with a constraint of l) in a PTX inline asm to access the texture or surface memory. 15.'},\n",
       " {'id': 929,\n",
       "  'content': 'NVVM Specific Intrinsic Functions \\uf0c1 15.1. Atomic \\uf0c1 Besides the atomic instructions, the following extra atomic intrinsic functions are supported. declare float @llvm.nvvm.atomic.load.add.f32.p0f32(float* address, float val) declare float @llvm.nvvm.atomic.load.add.f32.p1f32(float addrspace(1)* address, float val) declare float @llvm.nvvm.atomic.load.add.f32.p3f32(float addrspace(3)* address, float val) declare double @llvm.nvvm.atomic.load.add.f64.p0f64(double* address, double val) declare double @llvm.nvvm.atomic.load.add.f64.p1f64(double addrspace(1)* address, double val) declare double @llvm.nvvm.atomic.load.add.f64.p3f64(double addrspace(3)* address, double val) reads the single/double precision floating point value old located at the address address , computes old+val , and stores the result back to memory at the same address. These operations are performed in one atomic transaction. The function returns old . declare i32 @llvm.nvvm.atomic.load.inc.32.p0i32(i32* address, i32 val) declare i32 @llvm.nvvm.atomic.load.inc.32.p1i32(i32 addrspace(1)* address, i32 val) declare i32 @llvm.nvvm.atomic.load.inc.32.p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes ((old >= val) ? 0 : (old+1)) , and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. declare i32 @llvm.nvvm.atomic.load.dec.32.p0i32(i32* address, i32 val) declare i32 @llvm.nvvm.atomic.load.dec.32.p1i32(i32 addrspace(1)* address, i32 val) declare i32 @llvm.nvvm.atomic.load.dec.32.p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes (((old == 0) | (old > val)) ? val : (old-1) ) , and stores the result back to memory at the same address.'},\n",
       " {'id': 930,\n",
       "  'content': '15.2. Barrier and Memory Fence \\uf0c1 declare void @llvm.nvvm.barrier0() waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to llvm.nvvm.barrier0() are visible to all threads in the block. declare i32 @llvm.nvvm.barrier0.popc(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero. declare i32 @llvm.nvvm.barrier0.and(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them. declare i32 @llvm.nvvm.barrier0.or(i32) is identical to llvm.nvvm.barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them. declare void @llvm.nvvm.cluster.barrier(i32 %flags) Synchronize and communicate among threads in the same cluster. The %flags is encoded according to the following table: %flags bits Meaning 31-8 Reserved 7-4 Memory ordering (See Cluster Barrier Memory Ordering Encoding below) 3-0 Operation mode (See Cluster Barrier Operation Mode Encoding below) Cluster Barrier Operation Mode Encoding Encoding Mode Description 0 Arrive Arrive at cluster barrier 1 Wait Wait at cluster barrier 2-15 RESERVED RESERVED Cluster Barrier Memory Ordering Encoding Encoding Mode Description 0 Default All synchronous memory accesses requested by the executing entry prior to arrive are performed and are visible to all the entrys in the cluster after wait. 1 Relaxed All previously fenced memory accesses requested by the executing entry prior to arrive are performed and are visible to all the entrys in the cluster after wait. This ordering is only supported when the operation mode is Arrive. 2-15 RESERVED RESERVED declare void @llvm.nvvm.membar.cta() is a memory fence at the thread block level. This intrinsic is deprecated. Please use nvvm.membar with flags as argument instead. declare void @llvm.nvvm.membar.gl() is a memory fence at the device level. declare void @llvm.nvvm.membar.sys() is a memory fence at the system level. declare void @llvm.nvvm.membar(i32 %flags) Wait for all prior memory accesses requested by this thread to be performed at a membar level defined by the membar mode below. The memory barrier enforces vertical ordering only. It makes no guarantees as to execution synchronization with other threads. For horizontal synchronization, a barrier should be used instead, or in addition to membar. The %flags is encoded according to the following table: %flags bits Meaning 31-4 Reserved 3-0 Membar modes (See Membar Mode Encoding.)\\nMembar Mode Encoding Encoding Mode Description 0 GLOBAL Membar at the global level 1 CTA Membar at the CTA level 2 SYSTEM Membar at the system level 3 RESERVED RESERVED 4 CLUSTER Membar at the cluster level, only on Hopper+ 5-15 RESERVED RESERVED 15.3. Address space conversion \\uf0c1 Note Attention: Please use the addrspacecast IR instruction for address space conversion.'},\n",
       " {'id': 931,\n",
       "  'content': '15.4. Special Registers \\uf0c1 The following intrinsic functions are provided to support reading special PTX registers: declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() declare i32 @llvm.nvvm.read.ptx.sreg.tid.y() declare i32 @llvm.nvvm.read.ptx.sreg.tid.z() declare i32 @llvm.nvvm.read.ptx.sreg.ntid.x() declare i32 @llvm.nvvm.read.ptx.sreg.ntid.y() declare i32 @llvm.nvvm.read.ptx.sreg.ntid.z() declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.x() declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.y() declare i32 @llvm.nvvm.read.ptx.sreg.ctaid.z() declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.x() declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.y() declare i32 @llvm.nvvm.read.ptx.sreg.nctaid.z() declare i32 @llvm.nvvm.read.ptx.sreg.warpsize() 15.5. Texture/Surface Access \\uf0c1 The following intrinsic function is provided to convert a global texture/surface variable into a texture/surface handle.'},\n",
       " {'id': 932,\n",
       "  'content': 'declare i64 %llvm.nvvm.texsurf.handle.p1i64(metadata, i64 addrspace(1)*) See Accessing Texture Memory or Surface Memory for details. The following IR definitions apply to all intrinsics in this section: type %float4 = { float, float, float, float } type %long2 = { i64, i64 } type %int4 = { i32, i32, i32, i32 } type %int2 = { i32, i32 } type %short4 = { i16, i16, i16, i16 } type %short2 = { i16, i16 } 15.5.1. Texture Reads \\uf0c1 Sampling a 1D texture: %float4 @llvm.nvvm.tex.unified.1d.v4f32.s32(i64 %tex, i32 %x) %float4 @llvm.nvvm.tex.unified.1d.v4f32.f32(i64 %tex, float %x) %float4 @llvm.nvvm.tex.unified.1d.level.v4f32.f32(i64 %tex, float %x, float %level) %float4 @llvm.nvvm.tex.unified.1d.grad.v4f32.f32(i64 %tex, float %x, float %dPdx, float %dPdy) %int4 @llvm.nvvm.tex.unified.1d.v4s32.s32(i64 %tex, i32 %x) %int4 @llvm.nvvm.tex.unified.1d.v4s32.f32(i64 %tex, float %x) %int4 @llvm.nvvm.tex.unified.1d.level.v4s32.f32(i64 %tex, float %x, float %level) %int4 @llvm.nvvm.tex.unified.1d.grad.v4s32.f32(i64 %tex, float %x, float %dPdx, float %dPdy) %int4 @llvm.nvvm.tex.unified.1d.v4u32.s32(i64 %tex, i32 %x) %int4 @llvm.nvvm.tex.unified.1d.v4u32.f32(i64 %tex, float %x) %int4 @llvm.nvvm.tex.unified.1d.level.v4u32.f32(i64 %tex, float %x, float %level) %int4 @llvm.nvvm.tex.unified.1d.grad.v4u32.f32(i64 %tex, float %x, float %dPdx, float %dPdy) Sampling a 1D texture array: %float4 @llvm.nvvm.tex.unified.1d.array.v4f32.s32(i64 %tex, i32 %idx, i32 %x) %float4 @llvm.nvvm.tex.unified.1d.array.v4f32.f32(i64 %tex, i32 %idx, float %x) %float4 @llvm.nvvm.tex.unified.1d.array.level.v4f32.f32(i64 %tex, i32 %idx, float %x, float %level) %float4 @llvm.nvvm.tex.unified.1d.array.grad.v4f32.f32(i64 %tex, i32 %idx, float %x, float %dPdx, float %dPdy) %int4 @llvm.nvvm.tex.unified.1d.array.v4s32.s32(i64 %tex, i32 %idx, i32 %x) %int4 @llvm.nvvm.tex.unified.1d.array.v4s32.f32(i64 %tex, i32 %idx, float %x) %int4 @llvm.nvvm.tex.unified.1d.array.level.v4s32.f32(i64 %tex, i32 %idx, float %x, float %level) %int4 @llvm.nvvm.tex.unified.1d.array.grad.v4s32.f32(i64 %tex, i32 %idx, float %x, float %dPdx, float %dPdy) %int4 @llvm.nvvm.tex.unified.1d.array.v4u32.s32(i64 %tex, i32 %idx, i32 %x) %int4 @llvm.nvvm.tex.unified.1d.array.v4u32.f32(i64 %tex, i32 %idx, float %x) %int4 @llvm.nvvm.tex.unified.1d.array.level.v4u32.f32(i64 %tex, i32 %idx, float %x, float %level) %int4 @llvm.nvvm.tex.unified.1d.array.grad.v4u32.f32(i64 %tex, i32 %idx, float %x, float %dPdx, float %dPdy) Sampling a 2D texture: %float4 @llvm.nvvm.tex.unified.2d.v4f32.s32(i64 %tex, i32 %x, i32 %y) %float4 @llvm.nvvm.tex.unified.2d.v4f32.f32(i64 %tex, float %x, float %y) %float4 @llvm.nvvm.tex.unified.2d.level.v4f32.f32(i64 %tex, float %x, float %y, float %level) %float4 @llvm.nvvm.tex.unified.2d.grad.v4f32.f32(i64 %tex, float %x, float %y, float %dPdx_x, float %dPdx_y, float %dPdy_x, float %dPdy_y) %int4 @llvm.nvvm.tex.unified.2d.v4s32.s32(i64 %tex, i32 %x, i32 %y) %int4 @llvm.nvvm.tex.unified.2d.v4s32.f32(i64 %tex, float %x, float %y,) %int4 @llvm.nvvm.tex.unified.2d.level.v4s32.f32(i64 %tex, float %x, float %y, float %level) %int4 @llvm.nvvm.tex.unified.2d.grad.v4s32.f32(i64 %tex, float %x, float %y, float %dPdx_x, float %dPdx_y, float %dPdy_x, float %dPdy_y) %int4 @llvm.nvvm.tex.unified.2d.v4u32.s32(i64 %tex, i32 %x i32 %y) %int4 @llvm.nvvm.tex.unified.2d.v4u32.f32(i64 %tex, float %x float %y) %int4 @llvm.nvvm.tex.unified.2d.level.v4u32.f32(i64 %tex, float %x, float %y, float %level) %int4 @llvm.nvvm.tex.unified.2d.grad.v4u32.f32(i64 %tex, float %x, float %y, float %dPdx_x, float %dPdx_y, float %dPdy_x, float %dPdy_y) Sampling a 2D texture array: %float4 @llvm.nvvm.tex.unified.2d.array.v4f32.s32(i64 %tex, i32 %idx, i32 %x, i32 %y) %float4 @llvm.nvvm.tex.unified.2d.array.v4f32.f32(i64 %tex, i32 %idx, float %x, float %y) %float4 @llvm.nvvm.tex.unified.2d.array.level.v4f32.f32(i64 %tex, i32 %idx, float %x, float %y, float %level) %float4 @llvm.nvvm.tex.unified.2d.array.grad.v4f32.f32(i64 %tex, i32 %idx, float %x, float %y, float %dPdx_x, float %dPdx_y, float %dPdy_x, float %dPdy_y) %int4 @llvm.nvvm.tex.unified.2d.array.v4s32.s32(i64 %tex, i32 %idx, i32 %x, i32 %y) %int4 @llvm.nvvm.tex.unified.2d.array.v4s32.f32(i64 %tex, i32 %idx, float %x, float %y) %int4 @llvm.nvvm.tex.unified.2d.array.level.v4s32.f32(i64 %tex, i32 %idx, float %x, float %y, float %level) %int4 @llvm.nvvm.tex.unified.2d.array.grad.v4s32.f32(i64 %tex, i32 %idx, float %x, float %y, float %dPdx_x, float %dPdx_y, float %dPdy_x, float %dPdy_y) %int4 @llvm.nvvm.tex.unified.2d.array.v4u32.s32(i64 %tex, i32 %idx, i32 %x i32 %y) %int4 @llvm.nvvm.tex.unified.2d.array.v4u32.f32(i64 %tex, i32 %idx, float %x float %y) %int4 @llvm.nvvm.tex.unified.2d.array.level.v4u32.f32(i64 %tex, i32 %idx, float %x, float %y, float %level) %int4 @llvm.nvvm.tex.unified.2d.array.grad.v4u32.f32(i64 %tex, i32 %idx, float %x, float %y, float %dPdx_x, float %dPdx_y, float %dPdy_x, float %dPdy_y) Sampling a 3D texture: %float4 @llvm.nvvm.tex.unified.3d.v4f32.s32(i64 %tex, i32 %x, i32 %y, i32 %z) %float4 @llvm.nvvm.tex.unified.3d.v4f32.f32(i64 %tex, float %x, float %y, float %z) %float4 @llvm.nvvm.tex.unified.3d.level.v4f32.f32(i64 %tex,float %x, float %y, float %z, float %level) %float4 @llvm.nvvm.tex.unified.3d.grad.v4f32.f32(i64 %tex, float %x, float %y, float %z, float %dPdx_x, float %dPdx_y, float %dPdx_z, float %dPdy_x, float %dPdy_y, float %dPdy_z) %int4 @llvm.nvvm.tex.unified.3d.v4s32.s32(i64 %tex, i32 %x, i32 %y, i32 %z) %int4 @llvm.nvvm.tex.unified.3d.v4s32.f32(i64 %tex, float %x, float %y, float %z) %int4 @llvm.nvvm.tex.unified.3d.level.v4s32.f32(i64 %tex, float %x, float %y, float %z, float %level) %int4 @llvm.nvvm.tex.unified.3d.grad.v4s32.f32(i64 %tex, float %x, float %y, float %z, float %dPdx_x, float %dPdx_y, float %dPdx_z, float %dPdy_x, float %dPdy_y, float %dPdy_z) %int4 @llvm.nvvm.tex.unified.3d.v4u32.s32(i64 %tex, i32 %x i32 %y, i32 %z) %int4 @llvm.nvvm.tex.unified.3d.v4u32.f32(i64 %tex, float %x, float %y, float %z) %int4 @llvm.nvvm.tex.unified.3d.level.v4u32.f32(i64 %tex, float %x, float %y, float %z, float %level) %int4 @llvm.nvvm.tex.unified.3d.grad.v4u32.f32(i64 %tex, float %x, float %y, float %z, float %dPdx_x, float %dPdx_y, float %dPdx_z, float %dPdy_x, float %dPdy_y, float %dPdy_z) Sampling a cube texture: %float4 @llvm.nvvm.tex.unified.cube.v4f32.f32(i64 %tex, float %x, float %y, float %z) %float4 @llvm.nvvm.tex.unified.cube.level.v4f32.f32(i64 %tex,float %x, float %y, float %z, float %level) %int4 @llvm.nvvm.tex.unified.cube.v4s32.f32(i64 %tex, float %x, float %y, float %z) %int4 @llvm.nvvm.tex.unified.cube.level.v4s32.f32(i64 %tex, float %x, float %y, float %z, float %level) %int4 @llvm.nvvm.tex.unified.cube.v4u32.f32(i64 %tex, float %x, float %y, float %z) %int4 @llvm.nvvm.tex.unified.cube.level.v4u32.f32(i64 %tex, float %x, float %y, float %z, float %level) Sampling a cube texture array: %float4 @llvm.nvvm.tex.unified.cube.array.v4f32.f32(i64 %tex, i32 %idx, float %x, float %y, float %z) %float4 @llvm.nvvm.tex.unified.cube.array.level.v4f32.f32(i64 %tex, i32 %idx, float %x, float %y, float %z, float %level) %int4 @llvm.nvvm.tex.unified.cube.array.v4s32.f32(i64 %tex, i32 %idx, float %x, float %y, float %z) %int4 @llvm.nvvm.tex.unified.cube.array.level.v4s32.f32(i64 %tex, i32 %idx, float %x, float %y, float %z, float %level) %int4 @llvm.nvvm.tex.unified.cube.array.v4u32.f32(i64 %tex, i32 %idx, float %x, float %y, float %z) %int4 @llvm.nvvm.tex.unified.cube.array.level.v4u32.f32(i64 %tex, i32 %idx, float %x, float %y, float %z, float %level) Fetching a four-texel bilerp footprint: %float4 @llvm.nvvm.tld4.unified.r.2d.v4f32.f32(i64 %tex, float %x, float %y) %float4 @llvm.nvvm.tld4.unified.g.2d.v4f32.f32(i64 %tex, float %x, float %y) %float4 @llvm.nvvm.tld4.unified.b.2d.v4f32.f32(i64 %tex, float %x, float %y) %float4 @llvm.nvvm.tld4.unified.a.2d.v4f32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.r.2d.v4s32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.g.2d.v4s32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.b.2d.v4s32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.a.2d.v4s32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.r.2d.v4u32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.g.2d.v4u32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.b.2d.v4u32.f32(i64 %tex, float %x, float %y) %int4 @llvm.nvvm.tld4.unified.a.2d.v4u32.f32(i64 %tex, float %x, float %y) 15.5.2. Surface Loads \\uf0c1 In the following intrinsics, represents the surface clamp mode and can be one of the following: clamp , trap , or zero .'},\n",
       " {'id': 933,\n",
       "  'content': 'For surface load instructions that operate on 8-bit data channels, the output operands are of type i16 . The high-order eight bits are undefined. Reading a 1D surface: i16 @llvm.nvvm.suld.1d.i8. (i64 %tex, i32 %x) i16 @llvm.nvvm.suld.1d.i16. (i64 %tex, i32 %x) i32 @llvm.nvvm.suld.1d.i32. (i64 %tex, i32 %x) i64 @llvm.nvvm.suld.1d.i64. (i64 %tex, i32 %x) %short2 @llvm.nvvm.suld.1d.v2i8. (i64 %tex, i32 %x) %short2 @llvm.nvvm.suld.1d.v2i16. (i64 %tex, i32 %x) %int2 @llvm.nvvm.suld.1d.v2i32. (i64 %tex, i32 %x) %long2 @llvm.nvvm.suld.1d.v2i64. (i64 %tex, i32 %x) %short4 @llvm.nvvm.suld.1d.v4i8. (i64 %tex, i32 %x) %short4 @llvm.nvvm.suld.1d.v4i16. (i64 %tex, i32 %x) %int4 @llvm.nvvm.suld.1d.v4i32. (i64 %tex, i32 %x) Reading a 1D surface array: i16 @llvm.nvvm.suld.1d.array.i8. (i64 %tex, i32 %idx, i32 %x) i16 @llvm.nvvm.suld.1d.array.i16. (i64 %tex, i32 %idx, i32 %x) i32 @llvm.nvvm.suld.1d.array.i32. (i64 %tex, i32 %idx, i32 %x) i64 @llvm.nvvm.suld.1d.array.i64. (i64 %tex, i32 %idx, i32 %x) %short2 @llvm.nvvm.suld.1d.array.v2i8. (i64 %tex, i32 %idx, i32 %x) %short2 @llvm.nvvm.suld.1d.array.v2i16. (i64 %tex, i32 %idx, i32 %x) %int2 @llvm.nvvm.suld.1d.array.v2i32. (i64 %tex, i32 %idx, i32 %x) %long2 @llvm.nvvm.suld.1d.array.v2i64. (i64 %tex, i32 %idx, i32 %x) %short4 @llvm.nvvm.suld.1d.array.v4i8. (i64 %tex, i32 %idx, i32 %x) %short4 @llvm.nvvm.suld.1d.array.v4i16. (i64 %tex, i32 %idx, i32 %x) %int4 @llvm.nvvm.suld.1d.array.v4i32. (i64 %tex, i32 %idx, i32 %x) Reading a 2D surface: i16 @llvm.nvvm.suld.2d.i8. (i64 %tex, i32 %x, i32 %y) i16 @llvm.nvvm.suld.2d.i16. (i64 %tex, i32 %x, i32 %y) i32 @llvm.nvvm.suld.2d.i32. (i64 %tex, i32 %x, i32 %y) i64 @llvm.nvvm.suld.2d.i64. (i64 %tex, i32 %x, i32 %y) %short2 @llvm.nvvm.suld.2d.v2i8. (i64 %tex, i32 %x, i32 %y) %short2 @llvm.nvvm.suld.2d.v2i16. (i64 %tex, i32 %x, i32 %y) %int2 @llvm.nvvm.suld.2d.v2i32. (i64 %tex, i32 %x, i32 %y) %long2 @llvm.nvvm.suld.2d.v2i64. (i64 %tex, i32 %x, i32 %y) %short4 @llvm.nvvm.suld.2d.v4i8. (i64 %tex, i32 %x, i32 %y) %short4 @llvm.nvvm.suld.2d.v4i16. (i64 %tex, i32 %x, i32 %y) %int4 @llvm.nvvm.suld.2d.v4i32. (i64 %tex, i32 %x, i32 %y) Reading a 2D surface array: i16 @llvm.nvvm.suld.2d.array.i8. (i64 %tex, i32 %idx, i32 %x, i32 %y) i16 @llvm.nvvm.suld.2d.array.i16. (i64 %tex, i32 %idx, i32 %x, i32 %y) i32 @llvm.nvvm.suld.2d.array.i32. (i64 %tex, i32 %idx, i32 %x, i32 %y) i64 @llvm.nvvm.suld.2d.array.i64. (i64 %tex, i32 %idx, i32 %x, i32 %y) %short2 @llvm.nvvm.suld.2d.array.v2i8. (i64 %tex, i32 %idx, i32 %x, i32 %y) %short2 @llvm.nvvm.suld.2d.array.v2i16. (i64 %tex, i32 %idx, i32 %x, i32 %y) %int2 @llvm.nvvm.suld.2d.array.v2i32. (i64 %tex, i32 %idx, i32 %x, i32 %y) %long2 @llvm.nvvm.suld.2d.array.v2i64. (i64 %tex, i32 %idx, i32 %x, i32 %y) %short4 @llvm.nvvm.suld.2d.array.v4i8. (i64 %tex, i32 %idx, i32 %x, i32 %y) %short4 @llvm.nvvm.suld.2d.array.v4i16. (i64 %tex, i32 %idx, i32 %x, i32 %y) %int4 @llvm.nvvm.suld.2d.array.v4i32. (i64 %tex, i32 %idx, i32 %x, i32 %y) Reading a 3D surface: i16 @llvm.nvvm.suld.3d.i8. (i64 %tex, i32 %x, i32 %y, i32 %z) i16 @llvm.nvvm.suld.3d.i16. (i64 %tex, i32 %x, i32 %y, i32 %z) i32 @llvm.nvvm.suld.3d.i32. (i64 %tex, i32 %x, i32 %y, i32 %z) i64 @llvm.nvvm.suld.3d.i64. (i64 %tex, i32 %x, i32 %y, i32 %z) %short2 @llvm.nvvm.suld.3d.v2i8. (i64 %tex, i32 %x, i32 %y, i32 %z) %short2 @llvm.nvvm.suld.3d.v2i16. (i64 %tex, i32 %x, i32 %y, i32 %z) %int2 @llvm.nvvm.suld.3d.v2i32. (i64 %tex, i32 %x, i32 %y, i32 %z) %long2 @llvm.nvvm.suld.3d.v2i64. (i64 %tex, i32 %x, i32 %y, i32 %z) %short4 @llvm.nvvm.suld.3d.v4i8. (i64 %tex, i32 %x, i32 %y, i32 %z) %short4 @llvm.nvvm.suld.3d.v4i16. (i64 %tex, i32 %x, i32 %y, i32 %z) %int4 @llvm.nvvm.suld.3d.v4i32. (i64 %tex, i32 %x, i32 %y, i32 %z) 15.5.3. Surface Stores \\uf0c1 In the following intrinsics, represents the surface clamp mode. It is trap for the formatted stores, and can be one of the following for unformatted stores: clamp , trap , or zero . For surface store instructions that operate on 8-bit data channels, the input operands are of type i16 . The high-order eight bits are ignored. Writing a 1D surface: ;; Unformatted void @llvm.nvvm.sust.b.1d.i8. (i64 %tex, i32 %x, i16 %r) void @llvm.nvvm.sust.b.1d.i16. (i64 %tex, i32 %x, i16 %r) void @llvm.nvvm.sust.b.1d.i32. (i64 %tex, i32 %x, i32 %r) void @llvm.nvvm.sust.b.1d.i64. (i64 %tex, i32 %x, i64 %r) void @llvm.nvvm.sust.b.1d.v2i8. (i64 %tex, i32 %x, i16 %r, i16 %g) void @llvm.nvvm.sust.b.1d.v2i16. (i64 %tex, i32 %x, i16 %r, i16 %g) void @llvm.nvvm.sust.b.1d.v2i32. (i64 %tex, i32 %x, i32 %r, i32 %g) void @llvm.nvvm.sust.b.1d.v2i64. (i64 %tex, i32 %x, i64 %r, i64 %g) void @llvm.nvvm.sust.b.1d.v4i8. (i64 %tex, i32 %x, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.1d.v4i16. (i64 %tex, i32 %x, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.1d.v4i32. (i64 %tex, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.1d.i32. (i64 %tex, i32 %x, i32 %r) void @llvm.nvvm.sust.p.1d.v2i32. (i64 %tex, i32 %x, i32 %r, i32 %g) void @llvm.nvvm.sust.p.1d.v4i32. (i64 %tex, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 1D surface array: ;; Unformatted void @llvm.nvvm.sust.b.1d.array.i8. (i64 %tex, i32 %idx, i32 %x, i16 %r) void @llvm.nvvm.sust.b.1d.array.i16. (i64 %tex, i32 %idx, i32 %x, i16 %r) void @llvm.nvvm.sust.b.1d.array.i32. (i64 %tex, i32 %idx, i32 %x, i32 %r) void @llvm.nvvm.sust.b.1d.array.i64. (i64 %tex, i32 %idx, i32 %x, i64 %r) void @llvm.nvvm.sust.b.1d.array.v2i8. (i64 %tex, i32 %idx, i32 %x, i16 %r, i16 %g) void @llvm.nvvm.sust.b.1d.array.v2i16. (i64 %tex, i32 %idx, i32 %x, i16 %r, i16 %g) void @llvm.nvvm.sust.b.1d.array.v2i32. (i64 %tex, i32 %idx, i32 %x, i32 %r, i32 %g) void @llvm.nvvm.sust.b.1d.array.v2i64. (i64 %tex, i32 %idx, i32 %x, i64 %r, i64 %g) void @llvm.nvvm.sust.b.1d.array.v4i8. (i64 %tex, i32 %idx, i32 %x, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.1d.array.v4i16. (i64 %tex, i32 %idx, i32 %x, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.1d.array.v4i32. (i64 %tex, i32 %idx, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.1d.array.i32. (i64 %tex, i32 %idx, i32 %x, i32 %r) void @llvm.nvvm.sust.p.1d.array.v2i32. (i64 %tex, i32 %idx, i32 %x, i32 %r, i32 %g) void @llvm.nvvm.sust.p.1d.array.v4i32. (i64 %tex, i32 %idx, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface: ;; Unformatted void @llvm.nvvm.sust.b.2d.i8. (i64 %tex, i32 %x, i32 %y, i16 %r) void @llvm.nvvm.sust.b.2d.i16. (i64 %tex, i32 %x, i32 %y, i16 %r) void @llvm.nvvm.sust.b.2d.i32. (i64 %tex, i32 %x, i32 %y, i32 %r) void @llvm.nvvm.sust.b.2d.i64. (i64 %tex, i32 %x, i32 %y, i64 %r) void @llvm.nvvm.sust.b.2d.v2i8. (i64 %tex, i32 %x, i32 %y, i16 %r, i16 %g) void @llvm.nvvm.sust.b.2d.v2i16. (i64 %tex, i32 %x, i32 %y, i16 %r, i16 %g) void @llvm.nvvm.sust.b.2d.v2i32. (i64 %tex, i32 %x, i32 %y, i32 %r, i32 %g) void @llvm.nvvm.sust.b.2d.v2i64. (i64 %tex, i32 %x, i32 %y, i64 %r, i64 %g) void @llvm.nvvm.sust.b.2d.v4i8. (i64 %tex, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.v4i16. (i64 %tex, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.v4i32. (i64 %tex, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.2d.i32. (i64 %tex, i32 %x, i32 %y, i32 %r) void @llvm.nvvm.sust.p.2d.v2i32. (i64 %tex, i32 %x, i32 %y, i32 %r, i32 %g) void @llvm.nvvm.sust.p.2d.v4i32. (i64 %tex, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface array: ;; Unformatted void @llvm.nvvm.sust.b.2d.array.i8. (i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r) void @llvm.nvvm.sust.b.2d.array.i16. (i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r) void @llvm.nvvm.sust.b.2d.array.i32. (i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r) void @llvm.nvvm.sust.b.2d.array.i64. (i64 %tex, i32 %idx, i32 %x, i32 %y, i64 %r) void @llvm.nvvm.sust.b.2d.array.v2i8. (i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g) void @llvm.nvvm.sust.b.2d.array.v2i16. (i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g) void @llvm.nvvm.sust.b.2d.array.v2i32. (i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g) void @llvm.nvvm.sust.b.2d.array.v2i64. (i64 %tex, i32 %idx, i32 %x, i32 %y, i64 %r, i64 %g) void @llvm.nvvm.sust.b.2d.array.v4i8. (i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.array.v4i16. (i64 %tex, i32 %idx, i32 %x, i32 %y, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.2d.array.v4i32. (i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.2d.array.i32. (i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r) void @llvm.nvvm.sust.p.2d.array.v2i32. (i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g) void @llvm.nvvm.sust.p.2d.array.v4i32. (i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 3D surface: ;; Unformatted void @llvm.nvvm.sust.b.3d.i8. (i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r) void @llvm.nvvm.sust.b.3d.i16. (i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r) void @llvm.nvvm.sust.b.3d.i32. (i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r) void @llvm.nvvm.sust.b.3d.i64. (i64 %tex, i32 %x, i32 %y, i32 %z, i64 %r) void @llvm.nvvm.sust.b.3d.v2i8. (i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r, i16 %g) void @llvm.nvvm.sust.b.3d.v2i16. (i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r, i16 %g) void @llvm.nvvm.sust.b.3d.v2i32. (i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r, i32 %g) void @llvm.nvvm.sust.b.3d.v2i64. (i64 %tex, i32 %x, i32 %y, i32 %z, i64 %r, i64 %g) void @llvm.nvvm.sust.b.3d.v4i8. (i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.3d.v4i16. (i64 %tex, i32 %x, i32 %y, i32 %z, i16 %r, i16 %g, i16 %b, i16 %a) void @llvm.nvvm.sust.b.3d.v4i32. (i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r, i32 %g, i32 %b, i32 %a) ;; Formatted void @llvm.nvvm.sust.p.3d.i32. (i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r) void @llvm.nvvm.sust.p.3d.v2i32. (i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r, i32 %g) void @llvm.nvvm.sust.p.3d.v4i32. (i64 %tex, i32 %x, i32 %y, i32 %z, i32 %r, i32 %g, i32 %b, i32 %a) 15.6. Warp-level Operations \\uf0c1 15.6.1. Barrier Synchronization \\uf0c1 The following intrinsic performs a barrier synchronization among a subset of threads in a warp. declare void @llvm.nvvm.bar.warp.sync(i32 %membermask) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before resuming execution. The argument %membership is a 32bit mask, with each bit corresponding to a lane in the warp. 1 means the thread is in the subset. The behavior of this intrinsic is undefined if the executing thread is not in the %membermask . For compute_62 or below, all threads in %membermask must call the same @llvm.nvvm.bar.warp.sync() in convergence, and only threads belonging to the %membermask can be active when the intrinsic is called. Otherwise, the behavior is undefined. 15.6.2. Data Movement \\uf0c1 The following intrinsic synchronizes a subset of threads in a warp and then performs data movement among these threads. declare {i32, i1} @llvm.nvvm.shfl.sync.i32(i32 %membermask, i32 %mode, i32 %a, i32 %b, i32 %c) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before reading data from other threads in the same warp. Each thread in the currently executing warp will compute a source lane index j based on input arguments %b , %c , and %mode . If the computed source lane index j is in range, the returned i32 value will be the value of %a from lane j; otherwise, it will be the the value of %a from the current thread. If the thread corresponding to lane j is inactive, then the returned i32 value is undefined. The returned i1 value is set to 1 if the source lane j is in range, and otherwise set to 0. The argument %mode must be a constant and its encoding is specified in the following table. Encoding Meaning 0 IDX 1 UP 2 DOWN 3 BFLY Argument %b specifies a source lane or source lane offset, depending on %mode . Argument %c contains two packed values specifying a mask for logically splitting warps into sub-segments and an upper bound for clamping the source lane index. The following pseudo code illustrates the semantics of this intrinsic. wait until all threads in %membermask have arrived; %lane[4:0] = current_lane_id; // position of thread in warp %bval[4:0] = %b[4:0]; // source lane or lane offset (0..31) %cval[4:0] = %c[4:0]; // clamp value %mask[4:0] = %c[12:8]; %maxLane = (%lane[4:0] & %mask[4:0]) | (%cval[4:0] & ~%mask[4:0]); %minLane = (%lane[4:0] & %mask[4:0]); switch (%mode) { case UP: %j = %lane - %bval; %pval = (%j >= %maxLane); break; case DOWN: %j = %lane + %bval; %pval = (%j i32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.ld.a.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.ld.a.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); ; load fragment B declare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.ld.b.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.ld.b.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {i32, i32, i32, i32, i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.ld.b.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); ; load fragment C declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m16n16k16.ld.c.f32.pf32(float addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m32n8k16.ld.c.f32.pf32(float addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m8n32k16.ld.c.f32.pf32(float addrspace()* %ptr, i32 %ldm, i32 %rowcol); ; load fragment C declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.ld.c.f16.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.ld.c.f16.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.ld.c.f16.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol); These intrinsics load and return a matrix fragment from memory at location %ptr . The matrix in memory must be in a canonical matrix layout with leading dimension %ldm .'},\n",
       " {'id': 934,\n",
       "  'content': '%rowcol specifies which the matrix in memory is row-major (0) or column-major (1). %rowcol must be a constant value. The returned sequence of values represent the fragment held by the calling thread. How the elements of a matrix are distributed among the fragments is opaque to the user and is different for matrix A , B and the accumulator. Therefore, three variants (i.e.'},\n",
       " {'id': 935,\n",
       "  'content': 'ld.a , ld.b , and ld.c ) are provided. These intrinsics are overloaded based on the address spaces. The address space number must be either 0 (generic), 1 (global) or 3 (shared). The behavior of this intrinsic is undefined if any thread in the warp has exited. These intrinsics are only available on compute_70 or higher. 15.6.5.2. Store Fragments \\uf0c1 The following intrinsics synchronize all threads in a warp and then store a fragment of a matrix for each thread. ; The last 8 arguments are the elements of the C fragment declare void @llvm.nvvm.hmma.m16n16k16.st.c.f32.pfloat(float addrspace()* %ptr, i32 %ldm, i32 %rowcol, float, float, float, float, float, float, float, float); declare void @llvm.nvvm.hmma.m32n8k16.st.c.f32.pfloat(float addrspace()* %ptr, i32 %ldm, i32 %rowcol, float, float, float, float, float, float, float, float); declare void @llvm.nvvm.hmma.m8n32k16.st.c.f32.pfloat(float addrspace()* %ptr, i32 %ldm, i32 %rowcol, float, float, float, float, float, float, float, float); ; The last 4 arguments are the elements of the C fragment declare void @llvm.nvvm.hmma.m16n16k16.st.c.f16.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol, i32, i32, i32, i32); declare void @llvm.nvvm.hmma.m32n8k16.st.c.f16.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol, i32, i32, i32, i32); declare void @llvm.nvvm.hmma.m8n32k16.st.c.f16.pi32(i32 addrspace()* %ptr, i32 %ldm, i32 %rowcol, i32, i32, i32, i32); These intrinsics store an accumulator fragment to memory at location %ptr . 15.6.5.3.'},\n",
       " {'id': 936,\n",
       "  'content': 'Matrix Multiply-and-Accumulate \\uf0c1 The following intrinsics synchronize all threads in a warp and then perform a matrix multiply-and-accumulate operation. declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.mma.f16.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3); declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.mma.f16.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3); declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.mma.f16.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m16n16k16.mma.f32.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m32n8k16.mma.f32.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m8n32k16.mma.f32.f16(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, i32 %c0, i32 %c1, i32 %c2, i32 %c3); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m16n16k16.mma.f32.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m32n8k16.mma.f32.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7); declare {float, float, float, float, float, float, float, float} @llvm.nvvm.hmma.m8n32k16.mma.f32.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7); declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m16n16k16.mma.f16.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7); declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m32n8k16.mma.f16.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7); declare {i32, i32, i32, i32} @llvm.nvvm.hmma.m8n32k16.mma.f16.f32(i32 %rowcol, i32 %satf, i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %a4, i32 %a5, i32 %a6, i32 %a7, i32 %b0, i32 %b1, i32 %b2, i32 %b3, i32 %b4, i32 %b5, i32 %b6, i32 %b7, float %c0, float %c1, float %c2, float %c3, float %c4, float %c5, float %c6, float %c7); These intrinsics perform a matrix multiply-and-accumulate operation. %rowcol specifies the layout of A and B fragments.'},\n",
       " {'id': 937,\n",
       "  'content': 'It must be a constant value, which can have the following values and semantics. Encoding Meaning 0 A fragment is row-major, B fragment is row-major 1 A fragment is row-major, B fragment is column-major 2 A fragment is column-major, B fragment is row-major 3 A fragment is column-major, B fragment is column-major Support for %satf has been removed and this operand must be a constant zero. The behavior of these intrinsics are undefined if any thread in the warp has exited.'},\n",
       " {'id': 938,\n",
       "  'content': '16. Source Level Debugging Support \\uf0c1 To enable source level debugging of an IR module, NVVM IR supports debug intrinsics and debug information descriptors to express the debugging information. Debug information descriptors are represented using specialized metadata nodes. The current NVVM IR debug metadata version is 3.1. The current NVVM IR debugging support is based on that in LLVM 7.0.1. For the complete semantics of the IR, readers of this chapter should check the official LLVM IR specialized metadata nodes documentation ( https://releases.llvm.org/7.0.1/docs/LangRef.html#specialized-metadata-nodes ) and the Source Level Debugging with LLVM Manual ( https://releases.llvm.org/7.0.1/docs/SourceLevelDebugging.html ). The following metadata nodes need to be present in the module when debugging support is requested: Named metadata node !llvm.dbg.cu Module flags metadata for \"Debug Info Version\" flag: The behavior flag should be Error . The value of the flag should be DEBUG_METADATA_VERSION in LLVM 7.0.1, which is 3. Named metadata !nvvmir.version containing a metadata node with the NVVM IR major and minor version values followed by the NVVM IR debug metadata major and minor version values. The debug resolution (e.g., full, line info only) is controlled by the DICompileUnit’s emissionKind field: FullDebug (value: 1) : Generate symbolic debug and line information. This requires the libNVVM -g option to be specified at compile time. DebugDirectivesOnly (value: 3) : Generate line information. Source level debugging is supported only for a single debug compile unit. If there are multiple input NVVM IR modules, at most one module may have a single debug compile unit. 17. NVVM ABI for PTX \\uf0c1 17.1. Linkage Types \\uf0c1 The following table provides the mapping of NVVM IR linkage types associated with functions and global variables to PTX linker directives . LLVM Linkage Type PTX Linker Directive private , internal This is the default linkage type and does not require a linker directive. external Function with definition .visible Global variable with initialization Function without definition .extern Global variable without initialization common .common for the global address space, otherwise .weak available_externally , linkonce , linkonce_odr , weak , weak_odr .weak All other linkage types Not supported. 17.2.'},\n",
       " {'id': 939,\n",
       "  'content': 'Parameter Passing and Return \\uf0c1 The following table shows the mapping of function argument and return types in NVVM IR to PTX types. Source Type Size in Bits PTX Type Integer types <= 32 .u32 or .b32 (zero-extended if unsigned) .s32 or .b32 (sign-extended if signed) 64 .u64 or .b64 (if unsigned) .s64 or .b64 (if signed) Pointer types (without byval attribute) 32 .u32 or .b32 64 .u64 or .b64 Floating-point types 32 .f32 or .b32 64 .f64 or .b64 Aggregate types Any size .align align .b8 name [ size ] Where align is overall aggregate or vector alignment in bytes, name is variable name associated with aggregate or vector, and size is the aggregate or vector size in bytes. Pointer types to aggregate with byval attribute 32 or 64 Vector type Any size 18. Revision History \\uf0c1 Version 1.0 Initial Release. Version 1.1 Added support for UVM managed variables in global property annotation. See Supported Properties . Version 1.2 Update to LLVM 3.4 for CUDA 7.0. Remove address space intrinsics in favor of addrspacecast . Add information about source level debugging support. Version 1.3 Add support for LLVM 3.8 for CUDA 8.0. Version 1.4 Add support for warp-level intrinsics. Version 1.5 Add support for LLVM 5.0 for CUDA 9.2. Version 1.6 Update to LLVM 7.0.1 for CUDA 11.2. Version 1.7 Add support for alloca with dynamic size. Version 1.8 Add support for i128 in data layout. Version 1.9 Modified text about ignoring shared variable initializations. Version 1.10 Added support for grid_constant kernel parameters for CUDA 11.7. Version 1.11 Added support for Hopper+ cluster intrinsics and max_blocks_per_cluster kernel property for CUDA 11.8. Deprecated support for 32-bit compilation. Version 2.0 Updated the NVVM IR to version 2.0 which is incompatible with NVVM IR version 1.x Removed address space conversion intrinsics. The IR verifier on 2.0 IR will give an error when these intrinsics are present. Clients of libNVVM are advised to use addrspacecast instruction instead. Stricter error checking on the supported datalayouts. Older style loop unroll pragma metadata on loop backedges is no longer supported. Clients are advised to use the newer loop pragma metadata defined by the LLVM framework. Shared variable initialization with non-undef values is no longer supported. In 1.x versions these initializers were ignored silently. This feature makes the 2.0 version incompatible with 1.x versions. 19.'},\n",
       " {'id': 940,\n",
       "  'content': 'Notices \\uf0c1 19.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 19.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 941,\n",
       "  'content': '19.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 942,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}NVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12.5.1 libdevice User\\'s Guide 1. Introduction 1.1.'},\n",
       " {'id': 943,\n",
       "  'content': 'What Is libdevice? 2. Basic Usage 2.1. Linking with libdevice 3. Function Reference 3.1. __nv_abs 3.2. __nv_acos 3.3. __nv_acosf 3.4. __nv_acosh 3.5. __nv_acoshf 3.6. __nv_asin 3.7. __nv_asinf 3.8. __nv_asinh 3.9. __nv_asinhf 3.10. __nv_atan 3.11. __nv_atan2 3.12. __nv_atan2f 3.13. __nv_atanf 3.14. __nv_atanh 3.15. __nv_atanhf 3.16. __nv_brev 3.17. __nv_brevll 3.18. __nv_byte_perm 3.19. __nv_cbrt 3.20. __nv_cbrtf 3.21. __nv_ceil 3.22. __nv_ceilf 3.23. __nv_clz 3.24. __nv_clzll 3.25. __nv_copysign 3.26. __nv_copysignf 3.27. __nv_cos 3.28. __nv_cosf 3.29. __nv_cosh 3.30. __nv_coshf 3.31. __nv_cospi 3.32. __nv_cospif 3.33. __nv_dadd_rd 3.34. __nv_dadd_rn 3.35. __nv_dadd_ru 3.36. __nv_dadd_rz 3.37. __nv_ddiv_rd 3.38. __nv_ddiv_rn 3.39. __nv_ddiv_ru 3.40. __nv_ddiv_rz 3.41. __nv_dmul_rd 3.42. __nv_dmul_rn 3.43. __nv_dmul_ru 3.44. __nv_dmul_rz 3.45. __nv_double2float_rd 3.46. __nv_double2float_rn 3.47. __nv_double2float_ru 3.48. __nv_double2float_rz 3.49. __nv_double2hiint 3.50. __nv_double2int_rd 3.51. __nv_double2int_rn 3.52. __nv_double2int_ru 3.53. __nv_double2int_rz 3.54. __nv_double2ll_rd 3.55. __nv_double2ll_rn 3.56. __nv_double2ll_ru 3.57. __nv_double2ll_rz 3.58. __nv_double2loint 3.59. __nv_double2uint_rd 3.60. __nv_double2uint_rn 3.61. __nv_double2uint_ru 3.62. __nv_double2uint_rz 3.63. __nv_double2ull_rd 3.64. __nv_double2ull_rn 3.65. __nv_double2ull_ru 3.66. __nv_double2ull_rz 3.67. __nv_double_as_longlong 3.68. __nv_drcp_rd 3.69. __nv_drcp_rn 3.70. __nv_drcp_ru 3.71. __nv_drcp_rz 3.72. __nv_dsqrt_rd 3.73. __nv_dsqrt_rn 3.74. __nv_dsqrt_ru 3.75. __nv_dsqrt_rz 3.76. __nv_erf 3.77. __nv_erfc 3.78. __nv_erfcf 3.79. __nv_erfcinv 3.80. __nv_erfcinvf 3.81. __nv_erfcx 3.82. __nv_erfcxf 3.83. __nv_erff 3.84. __nv_erfinv 3.85. __nv_erfinvf 3.86. __nv_exp 3.87. __nv_exp10 3.88. __nv_exp10f 3.89. __nv_exp2 3.90. __nv_exp2f 3.91. __nv_expf 3.92. __nv_expm1 3.93. __nv_expm1f 3.94. __nv_fabs 3.95. __nv_fabsf 3.96. __nv_fadd_rd 3.97. __nv_fadd_rn 3.98. __nv_fadd_ru 3.99. __nv_fadd_rz 3.100. __nv_fast_cosf 3.101. __nv_fast_exp10f 3.102. __nv_fast_expf 3.103. __nv_fast_fdividef 3.104. __nv_fast_log10f 3.105. __nv_fast_log2f 3.106. __nv_fast_logf 3.107. __nv_fast_powf 3.108. __nv_fast_sincosf 3.109. __nv_fast_sinf 3.110. __nv_fast_tanf 3.111. __nv_fdim 3.112. __nv_fdimf 3.113. __nv_fdiv_rd 3.114. __nv_fdiv_rn 3.115. __nv_fdiv_ru 3.116. __nv_fdiv_rz 3.117. __nv_ffs 3.118. __nv_ffsll 3.119. __nv_finitef 3.120. __nv_float2half_rn 3.121. __nv_float2int_rd 3.122. __nv_float2int_rn 3.123. __nv_float2int_ru 3.124. __nv_float2int_rz 3.125. __nv_float2ll_rd 3.126. __nv_float2ll_rn 3.127. __nv_float2ll_ru 3.128. __nv_float2ll_rz 3.129. __nv_float2uint_rd 3.130. __nv_float2uint_rn 3.131. __nv_float2uint_ru 3.132. __nv_float2uint_rz 3.133. __nv_float2ull_rd 3.134. __nv_float2ull_rn 3.135. __nv_float2ull_ru 3.136. __nv_float2ull_rz 3.137. __nv_float_as_int 3.138. __nv_floor 3.139. __nv_floorf 3.140. __nv_fma 3.141. __nv_fma_rd 3.142. __nv_fma_rn 3.143. __nv_fma_ru 3.144. __nv_fma_rz 3.145. __nv_fmaf 3.146. __nv_fmaf_rd 3.147. __nv_fmaf_rn 3.148. __nv_fmaf_ru 3.149. __nv_fmaf_rz 3.150. __nv_fmax 3.151. __nv_fmaxf 3.152. __nv_fmin 3.153. __nv_fminf 3.154. __nv_fmod 3.155. __nv_fmodf 3.156. __nv_fmul_rd 3.157. __nv_fmul_rn 3.158. __nv_fmul_ru 3.159. __nv_fmul_rz 3.160. __nv_frcp_rd 3.161. __nv_frcp_rn 3.162. __nv_frcp_ru 3.163. __nv_frcp_rz 3.164. __nv_frexp 3.165. __nv_frexpf 3.166. __nv_frsqrt_rn 3.167. __nv_fsqrt_rd 3.168. __nv_fsqrt_rn 3.169. __nv_fsqrt_ru 3.170. __nv_fsqrt_rz 3.171. __nv_fsub_rd 3.172. __nv_fsub_rn 3.173. __nv_fsub_ru 3.174. __nv_fsub_rz 3.175. __nv_hadd 3.176. __nv_half2float 3.177. __nv_hiloint2double 3.178. __nv_hypot 3.179. __nv_hypotf 3.180. __nv_ilogb 3.181. __nv_ilogbf 3.182. __nv_int2double_rn 3.183. __nv_int2float_rd 3.184. __nv_int2float_rn 3.185. __nv_int2float_ru 3.186. __nv_int2float_rz 3.187. __nv_int_as_float 3.188. __nv_isfinited 3.189. __nv_isinfd 3.190. __nv_isinff 3.191. __nv_isnand 3.192. __nv_isnanf 3.193. __nv_j0 3.194. __nv_j0f 3.195. __nv_j1 3.196. __nv_j1f 3.197. __nv_jn 3.198. __nv_jnf 3.199. __nv_ldexp 3.200. __nv_ldexpf 3.201. __nv_lgamma 3.202. __nv_lgammaf 3.203. __nv_ll2double_rd 3.204. __nv_ll2double_rn 3.205. __nv_ll2double_ru 3.206. __nv_ll2double_rz 3.207. __nv_ll2float_rd 3.208. __nv_ll2float_rn 3.209. __nv_ll2float_ru 3.210. __nv_ll2float_rz 3.211. __nv_llabs 3.212. __nv_llmax 3.213. __nv_llmin 3.214. __nv_llrint 3.215. __nv_llrintf 3.216. __nv_llround 3.217. __nv_llroundf 3.218. __nv_log 3.219. __nv_log10 3.220. __nv_log10f 3.221. __nv_log1p 3.222. __nv_log1pf 3.223. __nv_log2 3.224. __nv_log2f 3.225. __nv_logb 3.226. __nv_logbf 3.227. __nv_logf 3.228. __nv_longlong_as_double 3.229. __nv_max 3.230. __nv_min 3.231. __nv_modf 3.232. __nv_modff 3.233. __nv_mul24 3.234. __nv_mul64hi 3.235. __nv_mulhi 3.236. __nv_nan 3.237. __nv_nanf 3.238. __nv_nearbyint 3.239. __nv_nearbyintf 3.240. __nv_nextafter 3.241. __nv_nextafterf 3.242. __nv_normcdf 3.243. __nv_normcdff 3.244. __nv_normcdfinv 3.245. __nv_normcdfinvf 3.246. __nv_popc 3.247. __nv_popcll 3.248. __nv_pow 3.249. __nv_powf 3.250. __nv_powi 3.251. __nv_powif 3.252. __nv_rcbrt 3.253. __nv_rcbrtf 3.254. __nv_remainder 3.255. __nv_remainderf 3.256. __nv_remquo 3.257. __nv_remquof 3.258. __nv_rhadd 3.259. __nv_rint 3.260. __nv_rintf 3.261. __nv_round 3.262. __nv_roundf 3.263. __nv_rsqrt 3.264. __nv_rsqrtf 3.265. __nv_sad 3.266. __nv_saturatef 3.267. __nv_scalbn 3.268. __nv_scalbnf 3.269. __nv_signbitd 3.270. __nv_signbitf 3.271. __nv_sin 3.272. __nv_sincos 3.273. __nv_sincosf 3.274. __nv_sincospi 3.275. __nv_sincospif 3.276. __nv_sinf 3.277. __nv_sinh 3.278. __nv_sinhf 3.279. __nv_sinpi 3.280. __nv_sinpif 3.281. __nv_sqrt 3.282. __nv_sqrtf 3.283. __nv_tan 3.284. __nv_tanf 3.285. __nv_tanh 3.286. __nv_tanhf 3.287. __nv_tgamma 3.288. __nv_tgammaf 3.289. __nv_trunc 3.290. __nv_truncf 3.291. __nv_uhadd 3.292. __nv_uint2double_rn 3.293. __nv_uint2float_rd 3.294. __nv_uint2float_rn 3.295. __nv_uint2float_ru 3.296. __nv_uint2float_rz 3.297. __nv_ull2double_rd 3.298. __nv_ull2double_rn 3.299. __nv_ull2double_ru 3.300. __nv_ull2double_rz 3.301. __nv_ull2float_rd 3.302. __nv_ull2float_rn 3.303. __nv_ull2float_ru 3.304. __nv_ull2float_rz 3.305. __nv_ullmax 3.306. __nv_ullmin 3.307. __nv_umax 3.308. __nv_umin 3.309. __nv_umul24 3.310. __nv_umul64hi 3.311. __nv_umulhi 3.312. __nv_urhadd 3.313. __nv_usad 3.314. __nv_y0 3.315. __nv_y0f 3.316. __nv_y1 3.317. __nv_y1f 3.318.'},\n",
       " {'id': 944,\n",
       "  'content': '__nv_yn 3.319. __nv_ynf Search Results libdevice User\\'s Guide ( PDF ) - v12.5.1 ( older ) - Last updated July 1, 2024 - Send Feedback Libdevice User\\'s Guide User\\'s guide to libdevice Table of Contents 1. __nv_ynf Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation var switchTo5x=true; stLight.options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite.pageBottom();1. libNVVM API 1.1.'},\n",
       " {'id': 945,\n",
       "  'content': 'Introduction 1.2. Thread Safety 1.3. Module 2. Error Handling 2.1. Enumerations 2.2. Functions 3. General Information Query 3.1. Functions 4. Compilation 4.1. Functions 4.2. Typedefs 5. Notices 5.1.'},\n",
       " {'id': 946,\n",
       "  'content': 'Notice 5.2. OpenCL 5.3. Trademarks libNVVM API v4.0 » 1. libNVVM API v12.5 | Archive libNVVM API libNVVM API v4.0 Reference Manual 1. libNVVM API \\uf0c1 1.1. Introduction \\uf0c1 libNVVM API provides an interface for generating PTX code from both binary and text NVVM IR inputs. Compatible input can be generated by tools and libraries that produce LLVM 7.0 IR and bitcode. Support for reading the text NVVM IR representation is deprecated and may be removed in a later release. 1.2. Thread Safety \\uf0c1 libNVVM API provides a thread-safe interface to libNVVM. Clients can take advantage of improved compilation speeds by spawning multiple compilation threads concurrently. 1.3. Module \\uf0c1 This chapter presents the API of the libNVVM library. Here is a list of all modules: Error Handling General Information Query Compilation 2. Error Handling \\uf0c1 Enumerations nvvmResult NVVM API call result code. Functions const char * nvvmGetErrorString (nvvmResult result) Get the message string for the given nvvmResult code. 2.1. Enumerations \\uf0c1 enum nvvmResult \\uf0c1 NVVM API call result code. Values: enumerator NVVM_SUCCESS \\uf0c1 enumerator NVVM_ERROR_OUT_OF_MEMORY \\uf0c1 enumerator NVVM_ERROR_PROGRAM_CREATION_FAILURE \\uf0c1 enumerator NVVM_ERROR_IR_VERSION_MISMATCH \\uf0c1 enumerator NVVM_ERROR_INVALID_INPUT \\uf0c1 enumerator NVVM_ERROR_INVALID_PROGRAM \\uf0c1 enumerator NVVM_ERROR_INVALID_IR \\uf0c1 enumerator NVVM_ERROR_INVALID_OPTION \\uf0c1 enumerator NVVM_ERROR_NO_MODULE_IN_PROGRAM \\uf0c1 enumerator NVVM_ERROR_COMPILATION \\uf0c1 2.2. Functions \\uf0c1 const char * nvvmGetErrorString ( nvvmResult result ) \\uf0c1 Get the message string for the given nvvmResult code. Parameters result – [in] NVVM API result code. Returns Message string for the given nvvmResult code. 3. General Information Query \\uf0c1 Functions nvvmResult nvvmIRVersion (int *majorIR, int *minorIR, int *majorDbg, int *minorDbg) Get the NVVM IR version. nvvmResult nvvmVersion (int *major, int *minor) Get the NVVM version. 3.1. Functions \\uf0c1 nvvmResult nvvmIRVersion ( int * majorIR , int * minorIR , int * majorDbg , int * minorDbg ) \\uf0c1 Get the NVVM IR version. Parameters majorIR – [out] NVVM IR major version number. minorIR – [out] NVVM IR minor version number. majorDbg – [out] NVVM IR debug metadata major version number. minorDbg – [out] NVVM IR debug metadata minor version number. Returns NVVM_SUCCESS nvvmResult nvvmVersion ( int * major , int * minor ) \\uf0c1 Get the NVVM version. Parameters major – [out] NVVM major version number. minor – [out] NVVM minor version number. Returns NVVM_SUCCESS 4. Compilation \\uf0c1 Functions nvvmResult nvvmAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program. nvvmResult nvvmCompileProgram (nvvmProgram prog, int numOptions, const char **options) Compile the NVVM program. nvvmResult nvvmCreateProgram (nvvmProgram *prog) Create a program, and set the value of its handle to *prog . nvvmResult nvvmDestroyProgram (nvvmProgram *prog) Destroy a program. nvvmResult nvvmGetCompiledResult (nvvmProgram prog, char *buffer) Get the compiled result. nvvmResult nvvmGetCompiledResultSize (nvvmProgram prog, size_t *bufferSizeRet) Get the size of the compiled result. nvvmResult nvvmGetProgramLog (nvvmProgram prog, char *buffer) Get the Compiler/Verifier Message. nvvmResult nvvmGetProgramLogSize (nvvmProgram prog, size_t *bufferSizeRet) Get the Size of Compiler/Verifier Message. nvvmResult nvvmLazyAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program. nvvmResult nvvmVerifyProgram (nvvmProgram prog, int numOptions, const char **options) Verify the NVVM program. Typedefs nvvmProgram NVVM Program. 4.1. Functions \\uf0c1 nvvmResult nvvmAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char * name ) \\uf0c1 Add a module level NVVM IR to a program. The buffer should contain an NVVM IR module. The module should have NVVM IR either in the LLVM 7.0.1 bitcode representation or in the LLVM 7.0.1 text representation. Support for reading the text representation of NVVM IR is deprecated and may be removed in a later version. Parameters prog – [in] NVVM program. buffer – [in] NVVM IR module in the bitcode or text representation. size – [in] Size of the NVVM IR module. name – [in] Name of the NVVM IR module. If NULL, “” is used as the name. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmCompileProgram ( nvvmProgram prog , int numOptions , const char * * options ) \\uf0c1 Compile the NVVM program. The NVVM IR modules in the program will be linked at the IR level. The linked IR program is compiled to PTX. The target datalayout in the linked IR program is used to determine the address size (32bit vs 64bit). The valid compiler options are: -g (enable generation of full debugging information).'},\n",
       " {'id': 947,\n",
       "  'content': 'Full debug support is only valid with ‘-opt=0’. Debug support requires the input module to utilize NVVM IR Debug Metadata. Line number (line info) only generation is also enabled via NVVM IR Debug Metadata, there is no specific libNVVM API flag for that case. -opt= 0 (disable optimizations) 3 (default, enable optimizations) -arch= compute_50 compute_52 (default) compute_53 compute_60 compute_61 compute_62 compute_70 compute_72 compute_75 compute_80 compute_87 compute_89 compute_90 -ftz= 0 (default, preserve denormal values, when performing single-precision floating-point operations) 1 (flush denormal values to zero, when performing single-precision floating-point operations) -prec-sqrt= 0 (use a faster approximation for single-precision floating-point square root) 1 (default, use IEEE round-to-nearest mode for single-precision floating-point square root) -prec-div= 0 (use a faster approximation for single-precision floating-point division and reciprocals) 1 (default, use IEEE round-to-nearest mode for single-precision floating-point division and reciprocals) -fma= 0 (disable FMA contraction) 1 (default, enable FMA contraction) -jump-table-density=[0-101] Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx.idx instruction) will be used to implement a switch statement. Default value is 101.'},\n",
       " {'id': 948,\n",
       "  'content': 'The percentage ranges from 0 to 101 inclusively. -gen-lto (Generate LTO IR instead of PTX). numOptions – [in] Number of compiler options passed. options – [in] Compiler options in the form of C string array. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM NVVM_ERROR_COMPILATION nvvmResult nvvmCreateProgram ( nvvmProgram * prog ) \\uf0c1 Create a program, and set the value of its handle to *prog . See also nvvmDestroyProgram() Parameters prog – [in] NVVM program. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmDestroyProgram ( nvvmProgram * prog ) \\uf0c1 Destroy a program. See also nvvmCreateProgram() Parameters prog – [in] NVVM program. Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResult ( nvvmProgram prog , char * buffer ) \\uf0c1 Get the compiled result. The result is stored in the memory pointed to by buffer . buffer – [out] Compiled result. Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResultSize ( nvvmProgram prog , size_t * bufferSizeRet ) \\uf0c1 Get the size of the compiled result. bufferSizeRet – [out] Size of the compiled result (including the trailing NULL). Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLog ( nvvmProgram prog , char * buffer ) \\uf0c1 Get the Compiler/Verifier Message. The NULL terminated message string is stored in the memory pointed to by buffer when the return value is NVVM_SUCCESS. buffer – [out] Compilation/Verification log. Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLogSize ( nvvmProgram prog , size_t * bufferSizeRet ) \\uf0c1 Get the Size of Compiler/Verifier Message. The size of the message string (including the trailing NULL) is stored into bufferSizeRet when the return value is NVVM_SUCCESS. bufferSizeRet – [out] Size of the compilation/verification log (including the trailing NULL). Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmLazyAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char * name ) \\uf0c1 Add a module level NVVM IR to a program. The module should have NVVM IR in the LLVM 7.0.1 bitcode representation. A module added using this API is lazily loaded - the only symbols loaded are those that are required by module(s) loaded using nvvmAddModuleToProgram. It is an error for a program to have all modules loaded using this API. Compiler may also optimize entities in this module by making them internal to the linked NVVM IR module, making them eligible for other optimizations. Due to these optimizations, this API to load a module is more efficient and should be used where possible. buffer – [in] NVVM IR module in the bitcode representation. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmVerifyProgram ( nvvmProgram prog , int numOptions , const char * * options ) \\uf0c1 Verify the NVVM program. The valid compiler options are: Same as for nvvmCompileProgram() . See also nvvmCompileProgram() Parameters prog – [in] NVVM program. Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_IR NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM 4.2. Typedefs \\uf0c1 typedef struct _nvvmProgram * nvvmProgram \\uf0c1 NVVM Program. An opaque handle for a program.'},\n",
       " {'id': 949, 'content': '5.'},\n",
       " {'id': 950,\n",
       "  'content': 'Notices \\uf0c1 5.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 5.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 951,\n",
       "  'content': '5.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 952,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. CUDA for Tegra 2.'},\n",
       " {'id': 953, 'content': 'Overview 3.'},\n",
       " {'id': 954,\n",
       "  'content': 'Memory Management 3.1. I/O Coherency 3.2. Estimating Total Allocatable Device Memory on an Integrated GPU Device 4. Porting Considerations 4.1. Memory Selection 4.2. Pinned Memory 4.3. Effective Usage of Unified Memory on Tegra 4.4. GPU Selection 4.5. Synchronization Mechanism Selection 4.6. CUDA Features Not Supported on Tegra 5. EGL Interoperability 5.1. EGLStream 5.1.1. EGLStream Flow 5.1.2. CUDA as Producer 5.1.3. CUDA as Consumer 5.1.4. Implicit Synchronization 5.1.5. Data Transfer Between Producer and Consumer 5.1.6. EGLStream Pipeline 5.2. EGLImage 5.2.1. CUDA interop with EGLImage 5.3. EGLSync 5.3.1. CUDA Interop with EGLSync 5.3.2. Creating EGLSync from a CUDA Event 5.3.3. Creating a CUDA Event from EGLSync 6. CUDA Upgradable Package for Jetson 6.1. Installing the CUDA Upgrade Package 6.1.1. Prerequisite 6.1.2. From Network Repositories or Local Installers 6.2. Deployment Considerations for CUDA Upgrade Package 6.2.1. Use the Right Upgrade Package 6.2.2.'},\n",
       " {'id': 955, 'content': 'Feature Exceptions 6.2.3.'},\n",
       " {'id': 956,\n",
       "  'content': 'Check for Compatibility Support 7. cuDLA 7.1. Developer Guide 7.1.1.'},\n",
       " {'id': 957,\n",
       "  'content': 'Device Model 7.1.2. Loading and Querying Modules 7.1.3. Memory Model 7.1.4. Task Execution and Synchronization Model 7.1.4.1. Task Execution 7.1.4.1.1. Multithreaded User Submission 7.1.4.2. Synchronization 7.1.4.2.1. Registering an external semaphore: 7.1.4.2.2. Events setup for cudlaSubmitTask() 7.1.4.2.3. Waiting on the signal event 7.1.4.2.4. Supported Synchronization Primitives in cuDLA 7.1.4.2.5. Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList 7.1.4.2.6. Timestamp Support for NvSciFence 7.1.4.2.7. Requesting Timestamp Support for NvSciSync Object 7.1.4.2.8. Extracting Timestamp Value from Fence 7.1.4.3. Fault Diagnostics 7.1.4.4.'},\n",
       " {'id': 958,\n",
       "  'content': 'NOOP Submission 7.1.5. Error Reporting Model 7.2. Migrating from NvMediaDla to cuDLA 7.3. Profiling a cuDLA App 7.4. cuDLA Release Notes 8. Notices 8.1. Notice 8.2. OpenCL 8.3. Trademarks CUDA for Tegra » 1. CUDA for Tegra v12.5 | PDF | Archive 1. CUDA for Tegra \\uf0c1 This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). It also discusses EGL interoperability. 2. Overview \\uf0c1 This document provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). This guide is for developers who are already familiar with programming in CUDA®, and C/C++, and who want to develop applications for the Tegra® SoC. Performance guidelines, best practices, terminology, and general information provided in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide are applicable to all CUDA-capable GPU architectures, including Tegra® devices. The CUDA C++ Programming Guide and the CUDA C Best Practices Guide are available at the following web sites: CUDA C++ Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html CUDA C++ Best Practices Guide: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html 3. Memory Management \\uf0c1 In Tegra® devices, both the CPU (Host) and the iGPU share SoC DRAM memory. A dGPU with separate DRAM memory can be connected to the Tegra device over PCIe or NVLink. It is currently supported only on the NVIDIA DRIVE platform. An overview of a dGPU-connected Tegra® memory system is shown in Figure 1 . dGPU-connected Tegra Memory System \\uf0c1 In Tegra, device memory, host memory, and unified memory are allocated on the same physical SoC DRAM. On a dGPU, device memory is allocated on the dGPU DRAM. The caching behavior in a Tegra system is different from that of an x86 system with a dGPU. The caching and accessing behavior of different memory types in a Tegra system is shown in Table 1 . Table 1. Characteristics of Different Memory Types in a Tegra System \\uf0c1 Memory Type CPU iGPU Tegra-connected dGPU Device memory Not directly accessible Cached Cached Pageable host memory Cached Not directly accessible Not directly accessible Pinned host memory Uncached where compute capability is less than 7.2. Cached where compute capability is greater than or equal to 7.2. Uncached Uncached Unified memory Cached Cached Not supported On Tegra, because device memory, host memory, and unified memory are allocated on the same physical SoC DRAM, duplicate memory allocations and data transfers can be avoided. 3.1. I/O Coherency \\uf0c1 I/O coherency (also known as one-way coherency) is a feature with which an I/O device such as a GPU can read the latest updates in CPU caches. It removes the need to perform CPU cache management operations when the same physical memory is shared between CPU and GPU. The GPU cache management operations still need to be performed because the coherency is one way. Please note that the CUDA driver internally performs the GPU cache management operations when managed memory or interop memory is used. I/O coherency is supported on Tegra devices starting with Xavier SOC. Applications should realize benefits from this HW feature without needing to make changes to the application’s code (see point 2 below). The following functionalities depend on I/O coherency support: cudaHostRegister() / cuMemHostRegister() is supported only on platforms which are I/O coherent. The host register support can be queried using the device attribute cudaDevAttrHostRegisterSupported / CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED. CPU cache for pinned memory allocated using cudaMallocHost() / cuMemHostAlloc() / cuMemAllocHost() is enabled only on platforms which are I/O coherent. 3.2. Estimating Total Allocatable Device Memory on an Integrated GPU Device \\uf0c1 The cudaMemGetInfo() API returns the snapshot of free and total amount of memory available for allocation for the GPU. The free memory could change if any other client allocate memory. The discrete GPU has the dedicated DRAM called VIDMEM which is separate from CPU memory. The snapshot of free memory in discrete GPU is returned by the cudaMemGetInfo API. The integrated GPU, on Tegra SoC, shares the DRAM with CPU and other the Tegra engines. The CPU can control the contents of DRAM and free DRAM memory by moving the contents of DMAR to SWAP area or vice versa. The cudaMemGetInfo API currently does not account for SWAP memory area. The cudaMemGetInfo API may return a smaller size than the actually allocatable memory since the CPU may be able to free up some DRAM region by moving pages to the SWAP area. In order to estimate the amount of allocatable device memory, CUDA application developers should consider following: On Linux and Android platforms: Device allocatable memory on Linux and Android depends mainly on the total and free sizes of swap space and main memory. The following points can help users to estimate the total amount of device allocatable memory in various situations: Host allocated memory = Total used physical memory – Device allocated memory If (Host allocated memory Free Swap Space) then Device allocatable memory = Total Physical Memory – (Host allocated memory - Free swap space) Here, Device allocated memory is memory already allocated on the device. It can be obtained from the NvMapMemUsed field in /proc/meminfo or from the total field of /sys/kernel/debug/nvmap/iovmm/clients . Total used physical memory can be obtained using the free -m command. The used field in row Mem represents this information. Total Physical memory is obtained from the MemTotal field in /proc/meminfo . Free swap space can be find by using the free -m command. The free field in the Swap row represents this information. If the free command is not available, the same information can be obtained from /proc/meminfo as: Total Used physical memory = MemTotal – MemFree Free swap space = SwapFree On QNX platforms: QNX does not use swap space, hence, cudaMemGetInfo.free will be a fair estimate of allocatable device memory as there is no swap space to move memory pages to swap area. 4.'},\n",
       " {'id': 959,\n",
       "  'content': \"Porting Considerations \\uf0c1 CUDA applications originally developed for dGPUs attached to x86 systems may require modifications to perform efficiently on Tegra systems. This section describes the considerations for porting such applications to a Tegra system, such as selecting an appropriate memory buffer type (pinned memory, unified memory, and others) and selecting between iGPU and dGPU, to achieve efficient performance for the application. 4.1. Memory Selection \\uf0c1 CUDA applications can use various kinds of memory buffers, such as device memory, pageable host memory, pinned memory, and unified memory. Even though these memory buffer types are allocated on the same physical device, each has different accessing and caching behaviors, as shown in Table 1 . It is important to select the most appropriate memory buffer type for efficient application execution. Device Memory Use device memory for buffers whose accessibility is limited to the iGPU. For example, in an application with multiple kernels, there may be buffers that are used only by the intermediate kernels of the application as input or output. These buffers are accessed only by the iGPU. Such buffers should be allocated with device memory. Pageable Host Memory Use pageable host memory for buffers whose accessibility is limited to the CPU. Pinned Memory Tegra® systems with different compute capabilities exhibit different behavior in terms of I/O coherency. For example, Tegra® systems with compute capability greater than or equal to 7.2 are I/O coherent and others are not I/O coherent. On Tegra® systems with I/O coherency, the CPU access time of pinned memory is as good as pageable host memory because it is cached on the CPU. However, on Tegra® systems without I/O coherency, the CPU access time of pinned memory is higher, because it is not cached on the CPU. Pinned memory is recommended for small buffers because the caching effect is negligible for such buffers and also because pinned memory does not involve any additional overhead, unlike Unified Memory. With no additional overhead, pinned memory is also preferable for large buffers if the access pattern is not cache friendly on iGPU. For large buffers, when the buffer is accessed only once on iGPU in a coalescing manner, performance on iGPU can be as good as unified memory on iGPU. Unified Memory Unified memory is cached on the iGPU and the CPU. On Tegra®, using unified memory in applications requires additional coherency and cache maintenance operations during the kernel launch, synchronization and prefetching hint calls. This coherency maintenance overhead is slightly higher on a Tegra® system with compute capability less than 7.2 as they lack I/O coherency. On Tegra® devices with I/O coherency (with a compute capability of 7.2 or greater) where unified memory is cached on both CPU and iGPU, for large buffers which are frequently accessed by the iGPU and the CPU and the accesses on iGPU are repetitive , unified memory is preferable since repetitive accesses can offset the cache maintenance cost. On Tegra® devices without I/O coherency (with a compute capability of less than 7.2), for large buffers which are frequently accessed by the CPU and the iGPU and the accesses on iGPU are not repetitive , unified memory is still preferable over pinned memory because pinned memory is not cached on both CPU and iGPU. That way, the application can take advantage of unified memory caching on the CPU. Pinned memory or unified memory can be used to reduce the data transfer overhead between CPU and iGPU as both memories are directly accessible from the CPU and the iGPU. In an application, input and output buffers that must be accessible on both the host and the iGPU can be allocated using either unified memory or pinned memory. Note The unified memory model requires the driver and system software to manage coherence on the current Tegra SOC. Software managed coherence is by nature non-deterministic and not recommended in a safe context. Zero-copy memory (pinned memory) is preferable in these applications. Evaluate the impact of unified memory overheads, pinned memory cache misses, and device memory data transfers in applications to determine the correct memory selection. 4.2. Pinned Memory \\uf0c1 This section provides guidelines for porting applications that use pinned memory allocations in x86 systems with dGPUs to Tegra®. CUDA applications developed for a dGPU attached to x86 system use pinned memory to reduce data transfer time and to overlap data transfers with kernel execution time. For specific information on this topic, see “Data Transfer Between Host and Device” and “Asynchronous and Overlapping Transfers with Computation” at the following websites. “Data Transfer Between Host and Device”: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device “Asynchronous and Overlapping Transfers with Computation”: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation On Tegra® systems with no I/O coherency, repetitive access of pinned memory degrades application performance, because pinned memory is not cached on the CPU in such systems. A sample application is shown below in which a set of filters and operations (k1, k2, and k3) are applied to an image. Pinned memory is allocated to reduce data transfer time on an x86 system with a dGPU, increasing the overall application speed. However, targeting a Tegra® device with the same code causes a drastic increase in the execution time of the readImage() function because it repeatedly accesses an uncached buffer. This increases the overall application time. If the time taken by readImage() is significantly higher compared to kernels execution time, it is recommended to use unified memory to reduce the readImage() time. Otherwise, evaluate the application with pinned memory and unified memory by removing unnecessary data transfer calls to decide best suited memory. // Sample code for an x86 system with a discrete GPU int main () { int * h_a , * d_a , * d_b , * d_c , * d_d , * h_d ; int height = 1024 ; int width = 1024 ; size_t sizeOfImage = width * height * sizeof ( int ); // 4MB image //Pinned memory allocated to reduce data transfer time cudaMallocHost ( h_a , sizeOfImage ); cudaMallocHost ( h_d , sizeOfImage ); //Allocate buffers on GPU cudaMalloc ( & d_a , sizeOfImage ); cudaMalloc ( & d_b , sizeOfImage ); cudaMalloc ( & d_c , sizeOfImage ); cudaMalloc ( & d_d , sizeOfImage ); //CPU reads Image; readImage ( h_a ); // Intialize the h_a buffer // Transfer image to GPU cudaMemcpy ( d_a , h_a , sizeOfImage , cudaMemcpyHostToDevice ); // Data transfer is fast as we used pinned memory // ----- CUDA Application pipeline start ---- k1 >> ( d_a , d_b ) // Apply filter 1 k2 >> ( d_b , d_c ) // Apply filter 2 k3 >> ( d_c , d_d ) // Some operation on image data // ----- CUDA Application pipeline end ---- // Transfer processed image to CPU cudaMemcpy ( h_d , d_d , sizeOfImage , cudaMemcpyDeviceToHost ); // Data transfer is fast as we used pinned memory // Use processed Image i.e h_d in later computations on CPU. UseImageonCPU ( h_d ); } // Porting the code on Tegra int main () { int * h_a , * d_b , * d_c , * h_d ; int height = 1024 ; int width = 1024 ; size_t sizeOfImage = width * height * sizeof ( int ); // 4MB image //Unified memory allocated for input and output //buffer of application pipeline cudaMallocManaged ( h_a , sizeOfImage , cudaMemAttachHost ); cudaMallocManaged ( h_d , sizeOfImage ); //Intermediate buffers not needed on CPU side. //So allocate them on device memory cudaMalloc ( & d_b , sizeOfImage ); cudaMalloc ( & d_c , sizeOfImage ); //CPU reads Image; readImage ( h_a ); // Intialize the h_a buffer // ----- CUDA Application pipeline start ---- // Prefetch input image data to GPU cudaStreamAttachMemAsync ( NULL , h_a , 0 , cudaMemAttachGlobal ); k1 >> ( h_a , d_b ) k2 >> ( d_b , d_c ) k3 >> ( d_c , h_d ) // Prefetch output image data to CPU cudaStreamAttachMemAsync ( NULL , h_d , 0 , cudaMemAttachHost ); cudaStreamSynchronize ( NULL ); // ----- CUDA Application pipeline end ---- // Use processed Image i.e h_d on CPU side. UseImageonCPU ( h_d ); } The cudaHostRegister() function The cudaHostRegister() function is not supported on Tegra® devices with compute capability less than 7.2, because those devices do not have I/O coherency. Use other pinned memory allocation functions such as cudaMallocHost() and cudaHostAlloc() if cudaHostRegister() is not supported on the device. GNU Atomic operations on pinned memory The GNU atomic operations on uncached memory is not supported on Tegra® CPU. As pinned memory is not cached on Tegra® devices with compute capability less than 7.2, GNU atomic operations is not supported on pinned memory. 4.3. Effective Usage of Unified Memory on Tegra \\uf0c1 Using unified memory in applications requires additional coherency and cache maintenance operations at kernel launch, synchronization, and prefetching hint calls. These operations are performed synchronously with other GPU work which can cause unpredictable latencies in the application. The performance of unified memory on Tegra® can be improved by providing data prefetching hints. The driver can use these prefetching hints to optimize the coherence operations. To prefetch the data, the cudaStreamAttachMemAsync() function can be used, in addition to the techniques described in the “Coherency and Concurrency” section of the CUDA C Programming Guide at the following link: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd to prefetch the data. The prefetching behavior of unified memory, as triggered by the changing states of the attachment flag, is shown in Table 2 . Table 2. Unified Memory Prefetching Behavior per Changing Attachment Flag States \\uf0c1 Previous Flag Current Flag Prefetching Behavior cudaMemAttachGlobal/cudaMemAttachSingle cudaMemAttachHost Causes prefetch to CPU cudaMemAttachHost cudaMemAttachGlobal/ cudaMemAttachSingle Causes prefetch to GPU cudaMemAttachGlobal cudaMemAttachSingle No prefetch to GPU cudaMemAttachSingle cudaMemAttachGlobal No prefetch to GPU The following example shows usage of cudaStreamAttachMemAsync() to prefetch data. Note However, not supported on Tegra® devices are the data prefetching techniques that use cudaMemPrefetchAsync() as described in the “Performance Tuning” section of the CUDA C++ Programming Guide at the following web site: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-performance-tuning Note There are limitations in QNX system software which prevent implementation of all UVM optimizations. Because of this, using cudaStreamAttachMemAsync() to prefetch hints on QNX does not benefit performance. __global__ void matrixMul ( int * p , int * q , int * r , int hp , int hq , int wp , int wq ) { // Matrix multiplication kernel code } void MatrixMul ( int hp , int hq , int wp , int wq ) { int * p , * q , * r ; int i ; size_t sizeP = hp * wp * sizeof ( int ); size_t sizeQ = hq * wq * sizeof ( int ); size_t sizeR = hp * wq * sizeof ( int ); //Attach buffers 'p' and 'q' to CPU and buffer 'r' to GPU cudaMallocManaged ( & p , sizeP , cudaMemAttachHost ); cudaMallocManaged ( & q , sizeQ , cudaMemAttachHost ); cudaMallocManaged ( & r , sizeR ); //Intialize with random values randFill ( p , q , hp , wp , hq , wq ); // Prefetch p,q to GPU as they are needed in computation cudaStreamAttachMemAsync ( NULL , p , 0 , cudaMemAttachGlobal ); cudaStreamAttachMemAsync ( NULL , q , 0 , cudaMemAttachGlobal ); matrixMul >> ( p , q , r , hp , hq , wp , wq ); // Prefetch 'r' to CPU as only 'r' is needed cudaStreamAttachMemAsync ( NULL , r , 0 , cudaMemAttachHost ); cudaStreamSynchronize ( NULL ); // Print buffer 'r' values for ( i = 0 ; i planeDesc[0].width = WIDTH; cudaEgl->planeDesc[0].depth = 0; cudaEgl->planeDesc[0].height = HEIGHT; cudaEgl->planeDesc[0].numChannels = 4; cudaEgl->planeDesc[0].pitch = WIDTH * cudaEgl->planeDesc[0].numChannels; cudaEgl->frameType = cudaEglFrameTypePitch; cudaEgl->planeCount = 1; cudaEgl->eglColorFormat = cudaEglColorFormatARGB; cudaEgl->planeDesc[0].channelDesc.f=cudaChannelFormatKindUnsigned cudaEgl->planeDesc[0].channelDesc.w = 8; cudaEgl->planeDesc[0].channelDesc.x = 8; cudaEgl->planeDesc[0].channelDesc.y = 8; cudaEgl->planeDesc[0].channelDesc.z = 8; size_t numElem = cudaEgl->planeDesc[0].pitch * cudaEgl->planeDesc[0].height; // Buffer allocated by producer cudaMalloc(&(cudaEgl->pPitch[0].ptr), numElem); //CUDA producer connects to EGLStream cudaEGLStreamProducerConnect(&conn, eglStream, WIDTH, HEIGHT)) // Sets all elements in the buffer to 1 K1>>(cudaEgl->pPitch[0].ptr, 1, numElem); // Present frame to EGLStream cudaEGLStreamProducerPresentFrame(&conn, *cudaEgl, NULL); cudaEGLStreamProducerReturnFrame(&conn, cudaEgl, eglStream); . .\"},\n",
       " {'id': 960,\n",
       "  'content': '//clean up cudaEGLStreamProducerDisconnect(&conn); . }\\nA frame is represented as a cudaEglFramestructure . The frameType parameter in cudaEglFrame indicates the memory layout of the frame. The supported memory layouts are CUDA Array and device pointer. Any mismatch in the width and height values of frame with the values specified in cudaEGLStreamProducerConnect() leads to undefined behavior. In the sample, the CUDA producer is sending a single frame, but it can send multiple frames over a loop. CUDA cannot present more than 64 active frames to EGLStream. The cudaEGLStreamProducerReturnFrame() call waits until it receives the released frame from the consumer. Once the CUDA producer presents the first frame to EGLstream, at least one frame is always available for consumer acquisition until the producer disconnects. This prevents the removal of the last frame from EGLStream, which would block cudaEGLStreamProducerReturnFrame (). Use the EGL_NV_stream_reset extension to set EGLStream attribute EGL_SUPPORT_REUSE_NV to false to allow the last frame to be removed from EGLStream. This allows removing or returning the last frame from EGLStream. 5.1.3. CUDA as Consumer \\uf0c1 When CUDA is the consumer, the supported producers are CUDA, OpenGL, NvMedia, Argus, and Camera. API functions to be used when CUDA is the consumer are listed in Table 3. Except for connecting and disconnecting from EGLStream, all API calls are non-blocking. The following consumer side steps are shown in the sample code that follows: Connect consumer to EGLStream (line 5). Acquire frame from EGLStream (lines 8-10). Process the frame on consumer (line 16). Release frame back to EGLStream (line 19). Disconnect the consumer after completion of the task (line 22). void ConsumerThread(EGLStreamKHR eglStream) { . //Connect consumer to EGLStream cudaEGLStreamConsumerConnect(&conn, eglStream); // consumer acquires a frame unsigned int timeout = 16000; cudaEGLStreamConsumerAcquireFrame(& conn, &cudaResource, eglStream, timeout); //consumer gets a cuda object pointer cudaGraphicsResourceGetMappedEglFrame(&cudaEgl, cudaResource, 0, 0); size_t numElem = cudaEgl->planeDesc[0].pitch * cudaEgl->planeDesc[0].height; . int checkIfOne = 1; // Checks if each value in the buffer is 1, if any value is not 1, it sets checkIfOne = 0. K2>>(cudaEgl->pPitch[0].ptr, 1, numElem, checkIfOne); . cudaEGLStreamConsumerReleaseFrame(&conn, cudaResource, &eglStream); . cudaEGLStreamConsumerDisconnect(&conn); . }\\nIn the sample code, the CUDA consumer receives a single frame, but it can also receive multiple frames over a loop. If a CUDA consumer fails to receive a new frame in the specified time limit using cudaEGLStreamConsumerAcquireFrame() , it reacquires the previous frame from EGLStream. The time limit is indicated by the timeout parameter. The application can use eglQueryStreamKHR() to query for the availability of new frames using. If the consumer uses already released frames, it results in undefined behavior. The consumer behavior is defined only for read operations. Behavior is undefined when the consumer writes to a frame. If the CUDA context is destroyed while connected to EGLStream, the stream is placed in the EGL_STREAM_STATE_DISCONNECTED_KHR state and the connection handle is invalidated. 5.1.4. Implicit Synchronization \\uf0c1 EGLStream provides implicit synchronization in an application. For example, in the previous code samples, both the producer and consumer threads are running in parallel and the K1 and K2 kernel processes access the same frame, but K2 execution in the consumer thread is guaranteed to occur only after kernel K1 in the producer thread finishes. The cudaEGLStreamConsumerAcquireFrame() function waits on the GPU side until K1 finishes and ensures synchronization between producer and consumer. The variable checkIfOne is never set to 0 inside the K2 kernel in the consumer thread. Similarly, cudaEGLStreamProducerReturnFrame() in the producer thread is guaranteed to get the frame only after K2 finishes and the consumer releases the frame. These non-blocking calls allow the CPU to do other computation in between, as synchronization is taken care of on the GPU side. The EGLStreams_CUDA_Interop CUDA sample code shows the usage of EGLStream in detail. 5.1.5. Data Transfer Between Producer and Consumer \\uf0c1 Data transfer between producer and consumer is avoided when they are present on the same device. In a Tegra® platform that includes a dGPU however, such as is in NVIDIA DRIVE™ PX 2, the producer and consumer can be present on different devices. In that case, an additional memory copy is required internally to move the frame between Tegra® SoC DRAM and dGPU DRAM. EGLStream allows producer and consumer to run on any GPU without code modification. Note On systems where a Tegra® device is connected to a dGPU, if a producer frame uses CUDA array, both producer and consumer should be on the same GPU. But if a producer frame uses CUDA device pointers, the consumer can be present on any GPU. 5.1.6. EGLStream Pipeline \\uf0c1 An application can use multiple EGL streams in a pipeline to pass the frames from one API to another. For an application where NvMedia sends a frame to CUDA for computation, CUDA sends the same frame to OpenGL for rendering after the computation. The EGLStream pipeline is illustrated in Figure 3 . EGLStream Pipeline \\uf0c1 NvMedia and CUDA connect as producer and consumer respectively to one EGLStream. CUDA and OpenGL connect as producer and consumer respectively to another EGLStream. Using multiple EGLStreams in pipeline fashion gives the flexibility to send frames across multiple APIs without allocating additional memory or requiring explicit data transfers. Sending a frame across the above EGLStream pipeline involves the following steps. NvMedia sends a frame to CUDA for processing. CUDA uses the frame for computation and sends to OpenGL for rendering. OpenGL consumes the frame and releases it back to CUDA. CUDA releases the frame back to NvMedia. The above steps can be performed in a loop to facilitate the transfer of multiple frames in the EGLStream pipeline. 5.2. EGLImage \\uf0c1 An EGLImage interop allows an EGL client API to share image data with other EGL client APIs. For example, an application can use an EGLImage interop to share an OpenGL texture with CUDA without allocating any additional memory. A single EGLImage object can be shared across multiple client APIs for modification. An EGLImage interop does not provide implicit synchronization. Applications must maintain synchronization to avoid race conditions. Note An EGLImage is created using eglCreateImageKHR() and destroyed using eglDestroyImageKHR() . For more information see the EGLImage specification at the following web site: https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_image_base.txt 5.2.1. CUDA interop with EGLImage \\uf0c1 CUDA supports interoperation with EGLImage, allowing CUDA to read or modify the data of an EGLImage. An EGLImage can be a single or multi-planar resource. In CUDA, a single-planar EGLImage object is represented as a CUDA array or device pointer. Similarly, a multi-planar EGLImage object is represented as an array of device pointers or CUDA arrays. EGLImage is supported on Tegra® devices running the Linux, QNX, or Android operating systems. Use the cudaGraphicsEGLRegisterImage() API to register an EGLImage object with CUDA. Registering an EGLImage with CUDA creates a graphics resource object. An application can use cudaGraphicsResourceGetMappedEglFrame() to get a frame from the graphics resource object. In CUDA, a frame is represented as a cudaEglFrame structure. The frameType parameter in cudaEglFrame indicates if the frame is a CUDA device pointer or a CUDA array. For a single planar graphics resource, an application can directly obtain a device pointer or CUDA array using cudaGraphicsResourceGetMappedPointer() or cudaGraphicsSubResourceGetMappedArray() respectively. A CUDA array can be bound to a texture or surface reference to access inside a kernel. Also, a multi-dimensional CUDA array can be read and written via cudaMemcpy3D() . Note An EGLImage cannot be created from a CUDA object. The cudaGraphicsEGLRegisterImage() function is only supported on Tegra® devices. Also, cudaGraphicsEGLRegisterImage() expects only the ‘0’ flag as other API flags are for future use. The following sample code shows EGLImage interoperability. In the code, an EGLImage object eglImage is created using OpenGL texture. The eglImage object is mapped as a CUDA array pArray in CUDA. The pArray array is bound to a surface object to allow modification of the OpenGL texture in the changeTexture. The function checkBuf() checks if the texture is updated with new values. int width = 256; int height = 256; int main() { . unsigned char *hostSurf; unsigned char *pSurf; CUarray pArray; unsigned int bufferSize = WIDTH * HEIGHT * 4; pSurf= (unsigned char *)malloc(bufferSize); hostSurf = (unsigned char *)malloc(bufferSize); // Initialize the buffer for(int y = 0; y >>(writeSurface, width, height); cuCtxSynchronize(); CUDA_MEMCPY3D cpdesc; memset(&cpdesc, 0, sizeof(cpdesc)); cpdesc.srcXInBytes = cpdesc.srcY = cpdesc.srcZ = cpdesc.srcLOD = 0; cpdesc.dstXInBytes = cpdesc.dstY = cpdesc.dstZ = cpdesc.dstLOD = 0; cpdesc.srcMemoryType = CU_MEMORYTYPE_ARRAY; cpdesc.dstMemoryType = CU_MEMORYTYPE_HOST; cpdesc.srcArray = pArray; cpdesc.dstHost = (void *)hostSurf; cpdesc.WidthInBytes = WIDTH * 4; cpdesc.Height = HEIGHT; cpdesc.Depth = 1; //Copy CUDA surface object values to hostSurf cuMemcpy3D(&cpdesc); cuCtxSynchronize(); unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char))); // Get the modified texture values as GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp)); glFinish(); // Check if the OpenGL texture got modified values checkbuf(temp,hostSurf); // Clean up CUDA cuGraphicsUnregisterResource(pResource); cuSurfObjectDestroy(writeSurface); . . }\\n__global__ void changeTexture(cudaSurfaceObject_t arr, unsigned int width, unsigned int height){ unsigned int x = threadIdx.x + blockIdx.x * blockDim.x; unsigned int y = threadIdx.y + blockIdx.y * blockDim.y; uchar4 data = make_uchar4(1, 2, 3, 4); surf2Dwrite(data, arr, x * 4, y); } void checkbuf(unsigned char *ref, unsigned char *hostSurf) { for(int y = 0; y >>(inputSurfObj, width, height); cuEventRecord(cuda_event, stream); //Create EGLsync object from CUDA event cuda_event eglsync2 = eglCreateSync64KHR(dpy, EGL_SYNC_CUDA_EVENT_NV, eglattrib); //waits till kernel to finish eglWaitSyncKHR(eglDisplayHandle, eglsync2, 0); . //Copy modified pArray values to hostSurf . unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char))); // Get the modified texture values GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp)); . // This function check if the OpenGL texture got modified values checkbuf(temp,hostSurf); // Clean up CUDA cudaGraphicsUnregisterResource(pResource); cudaDestroySurfaceObject(inputSurfObj); eglDestroySyncKHR(eglDisplayHandle, eglsync1); eglDestroySyncKHR(eglDisplayHandle, eglsync2); cudaEventDestroy(egl_event); cudaEventDestroy(cuda_event); . 6. CUDA Upgradable Package for Jetson \\uf0c1 CUDA introduced an upgrade path starting with JetPack SDK 5.0 which provides an option to update the CUDA driver and the CUDA toolkit to the latest version. 6.1. Installing the CUDA Upgrade Package \\uf0c1 6.1.1. Prerequisite \\uf0c1 The Jetson device must be installed with a compatible NVIDIA JetPack version. Refer to Use the Right Upgrade Package for more info.'},\n",
       " {'id': 961,\n",
       "  'content': '6.1.2. From Network Repositories or Local Installers \\uf0c1 The CUDA Downloads page provides step-by-step instructions on how to download and use the local installer or CUDA network repositories to install the latest Toolkit. The CUDA upgrade package gets downloaded and installed along with the corresponding CUDA toolkit for Linux-aarch64-jetson devices. For use cases where applications are built on the host and require just the CUDA upgrade package to be installed independently on the target, the corresponding Debians can be found in CUDA Repos . Taking 11.8 for example, this can be installed by running the command: $ sudo apt-get install -y cuda-compat-11-8 Note This is the recommended path for CUDA upgrade for devices that have disk space (secondary storage) limitations. The installed upgrade package is available in the versioned toolkit location. For example, for 11.8 it is located in /usr/local/cuda-11.8/ . The upgrade package consists of the following files: libcuda.so. * - the CUDA Driver libnvidia-nvvm.so. * - Just In Time - Link Time Optimization (CUDA 11.8 and later only) libnvidia-ptxjitcompiler.so. * - the JIT (just-in-time) compiler for PTX files nvidia-cuda-mps-control - CUDA MPS control executable nvidia-cuda-mps-server - CUDA MPS server executable These files together implement the CUDA 11.8 driver interfaces. Note This package only provides the files, and does not configure the system. Example The following commands show how the CUDA Upgrade package can be installed and used to run the applications. $ sudo apt-get -y install cuda Reading package lists... Building dependency tree... Reading state information... The following additional packages will be installed: cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8 ...... The following NEW packages will be installed: cuda cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8 ...... 0 upgraded, 48 newly installed, 0 to remove and 38 not upgraded.'},\n",
       " {'id': 962, 'content': 'Need to get 15.7 MB/1,294 MB of archives.'},\n",
       " {'id': 963,\n",
       "  'content': 'After this operation, 4,375 MB of additional disk space will be used. Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/arm64 cuda-compat-11-8 11.8.31339915-1 [15.8 MB] Fetched 15.7 MB in 12s (1,338 kB/s) Selecting previously unselected package cuda-compat-11-8. (Reading database ...'},\n",
       " {'id': 964, 'content': '...... (Reading database ...'},\n",
       " {'id': 965,\n",
       "  'content': '100% (Reading database ... 148682 files and directories currently installed.)\\nPreparing to unpack .../00-cuda-compat-11-8_11.8.30682616-1_arm64.deb ... Unpacking cuda-compat-11-8 (11.8.30682616-1) ... ...... Unpacking cuda-11-8 (11.8.0-1) ... Selecting previously unselected package cuda. Preparing to unpack .../47-cuda_11.8.0-1_arm64.deb ... Unpacking cuda (11.8.0-1) ... Setting up cuda-toolkit-config-common (11.8.56-1) ... Setting up cuda-nvml-dev-11-8 (11.8.56-1) ... Setting up cuda-compat-11-8 (11.8.30682616-1) ... ...... $ ls -l /usr/local/cuda-11.8/compat total 55300 lrwxrwxrwx 1 root root 12 Jan 6 19:14 libcuda.so -> libcuda.so.1 lrwxrwxrwx 1 root root 14 Jan 6 19:14 libcuda.so.1 -> libcuda.so.1.1 -rw-r--r-- 1 root root 21702832 Jan 6 19:14 libcuda.so.1.1 lrwxrwxrwx 1 root root 19 Jan 6 19:14 libnvidia-nvvm.so -> libnvidia-nvvm.so.4 lrwxrwxrwx 1 root root 23 Jan 6 19:14 libnvidia-nvvm.so.4 -> libnvidia-nvvm.so.4.0.0 -rw-r--r-- 1 root root 24255256 Jan 6 19:14 libnvidia-nvvm.so.4.0.0 -rw-r--r-- 1 root root 10665608 Jan 6 19:14 libnvidia-ptxjitcompiler.so lrwxrwxrwx 1 root root 27 Jan 6 19:14 libnvidia-ptxjitcompiler.so.1 -> libnvidia-ptxjitcompiler.so $ export PATH=/usr/local/cuda-11.8/bin:$PATH $ export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH The user can set LD_LIBRARY_PATH to include the libraries installed by the upgrade package before running the CUDA 11.8 application: $ LD_LIBRARY_PATH=/usr/local/cuda-11.8/compat:$LD_LIBRARY_PATH ~/Samples/1_Utilities/deviceQuery CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"Orin\" CUDA Driver Version / Runtime Version 11.8 / 11.8 CUDA Capability Major/Minor version number: 8.7 ...... deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1 Result = PASS Only a single CUDA upgrade package can be installed at any point in time on a given system.'},\n",
       " {'id': 966,\n",
       "  'content': 'While installing a new CUDA upgrade package, the previous version of the installed upgrade package will be removed and replaced with the new one. The default drivers (originally installed with the NVIDIA JetPack and part of the L4T BSP) will be retained by the installer. The application has the ability to use either the default version of CUDA (originally installed with NVIDIA JetPack) or the one installed by the upgrade package. The LD_LIBRARY_PATH variable can be used to choose the required version. In addition to LD_LIBRARY_PATH , CUDA MPS users must set the PATH variable in order to use the nvidia-cuda-mps-* executables installed by the upgrade package before starting MPS and running the CUDA applications that use MPS. The MPS executables installed with the upgrade package are only compatible with the CUDA driver installed with the same upgrade package, and vice versa, which can be checked with the version info. Installation of the upgrade package will fail if it is not compatible with the NVIDIA JetPack version. 6.2. Deployment Considerations for CUDA Upgrade Package \\uf0c1 6.2.1. Use the Right Upgrade Package \\uf0c1 The CUDA upgrade package is named after the highest toolkit that it can support. For example, if you are on the NVIDIA JetPack SDK 5.0 (11.4) driver but require 11.8 application support, install the CUDA upgrade package for 11.8. Each CUDA release will support upgrades only for a specific set of NVIDIA JetPack releases. The table below shows the NVIDIA JetPack SDK version supported by each CUDA release. JetPack SDK CUDA 11.4 CUDA 11.8 CUDA 12.0 CUDA 12.1 CUDA 12.2 CUDA 12.3 onwards 5.x default C C C C X JetPack SDK CUDA 12.2 CUDA 12.3 CUDA 12.4 CUDA 12.5 6.x default X C C The following table shows the CUDA UMD and CUDA Toolkit version compatibility on NVIDIA JetPack 5.x release: CUDA UMD CUDA Toolkit 11.4 (default - part of NVIDIA JetPack) 11.8 12.0 12.1 12.2 11.4 (default – part of NVIDIA JetPack) C C ( Minor Version Compatibility ) X X X 11.8 (through Upgrade Package) C ( Binary Compatibility ) C X X X 12.0 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C C ( Minor Version Compatibility ) C ( Minor Version Compatibility ) 12.1 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C ( Binary Compatibility ) C C ( Minor Version Compatibility ) 12.2 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C ( Binary Compatibility ) C ( Binary Compatibility ) C The following table shows the CUDA UMD and CUDA Toolkit version compatibility on NVIDIA JetPack 6.x release: CUDA UMD CUDA Toolkit 12.2 (default - part of NVIDIA JetPack) 12.4 12.5 12.2 (default – part of NVIDIA JetPack) C C ( Minor Version Compatibility ) C ( Minor Version Compatibility ) 12.4 (through Upgrade Package) C ( Binary Compatibility ) C C ( Minor Version Compatibility ) 12.5 (through Upgrade Package) C ( Binary Compatibility ) C ( Binary Compatibility ) C C - Compatible X – Not compatible Note CUDA upgrade packages on NVIDIA JetPack SDK 5.x are available from CUDA 11.8 onwards. 6.2.2. Feature Exceptions \\uf0c1 CUDA upgrade package only updates the CUDA driver interfaces while leaving the rest of the NVIDIA JetPack SDK components as is. If a new feature in the latest CUDA driver needs an updated NVIDIA JetPack SDK component/interface, it might not work and error out when used. 6.2.3. Check for Compatibility Support \\uf0c1 In addition to the CUDA driver and certain compiler components, there are other drivers in the NVIDIA JetPack that remain at the default version. The CUDA upgrade path is for CUDA only. A well-written application should use following error codes to determine if CUDA Upgrade is supported. System administrators should be aware of these error codes to determine if there are errors in the deployment. CUDA_ERROR_SYSTEM_DRIVER_MISMATCH = 803 . This error indicates that there is a mismatch between the versions of the upgraded CUDA driver and the already installed drivers on the system. CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE = 804 . This error indicates that the system was updated to run with the CUDA upgrade package but the visible hardware detected by CUDA does not support this configuration. 7. cuDLA \\uf0c1 DLA (Deep Learning Accelerator) is a fixed function accelerator present on the NVIDIA Tegra SoC and is used for inference applications. The DLA HW has superior performance/W and can natively run many of the layers in modern neural networks, thus making it an attractive value proposition for embedded AI applications. Programming the DLA typically consists of an offline and online step: in the offline step, an input network is parsed and compiled by the DLA compiler into a loadable and in the online step, that loadable is executed by the DLA HW to generate an inference result. The SW stack that is currently provided by NVIDIA to perform the online or execution step consists of NvMediaDla and the DLA runtime/KMD. Together, these APIs enable the user to submit a DLA task to the DLA HW for inferencing purposes. The main functional paths are illustrated in the figure below. DLA SW stack \\uf0c1 It follows from the model above that users wishing to use GPU and DLA together in an application would have to use interop mechanisms such as EGLStreams/NvSci to share buffers as well as synchronization primitives between the GPU and DLA. These interop mechanisms usually involve many steps for each buffer that is being shared and have limited ability to fine-tune the scheduling of tasks between the GPU and DLA. cuDLA is an extension of the CUDA programming model that integrates DLA (Deep Learning Accelerator) with CUDA thereby making it possible to submit DLA tasks using CUDA programming constructs such as streams and graphs. Managing shared buffers as well as synchronizing the tasks between GPU and DLA is transparently handled by cuDLA, freeing up the programmer to focus on the high-level usecase. 7.1. Developer Guide \\uf0c1 This section describes the key principles involved in programming the DLA HW using cuDLA APIs. The cuDLA interfaces expose mechanisms to initialize devices, manage memory and submit DLA tasks. As such, this section discusses how the cuDLA APIs can be used for these usecases. The detailed specification of these APIs is described in the API specification and should be referred while writing a cuDLA application. Since cuDLA is an extension of CUDA, it is designed to work in conjunction with CUDA APIs that perform CUDA functions such as GPU management, context management etc. Therefore, the current state of the application in terms of which GPU is selected and the current active context (and its lifecycle) are all important considerations while evaluating the behavior of a cuDLA API. 7.1.1. Device Model \\uf0c1 To perform any DLA operation, it is necessary that an application first create a cuDLA device handle. The cudlaCreateDevice() API creates a logical instance of a cuDLA device wherein the selected DLA HW instance is coupled with the current active GPU selected via CUDA. For example, the following code snippet would create a logical instance consisting of the current GPU (set via cudaSetDevice() ) and DLA HW 0. Currently, cuDLA supports only iGPU on Tegra and an attempt to create a device handle by setting the current GPU as a dGPU would result in a device creation error during cudlaCreateDevice() . cudlaDevHandle devHandle ; cudlaStatus ret ; ret = cudlaCreateDevice ( 0 , & devHandle , CUDLA_CUDA_DLA ); Device model \\uf0c1 The user can create any number of such logical instances using cudlaCreateDevice() using any combination of GPU and DLA HW instances (subject to system resource availability): Device model - multiple instances \\uf0c1 In addition, cudlaCreateDevice() supports an alternative flag during device creation - CUDLA_STANDALONE. This flag can be used by applications when they wish to create a cuDLA device in standalone mode i.e without coupling it with a GPU device. All device submissions can be accomplished using cuDLA in standalone mode as well but in this mode there is no support for CUDA interactions. Consequently, in what follows, two modes of execution are considered while describing a particular API or a particular usecase: the hybrid mode and the standalone mode.'},\n",
       " {'id': 967,\n",
       "  'content': 'The API spec has complete details about which API is supported in which mode.'},\n",
       " {'id': 968,\n",
       "  'content': '7.1.2. Loading and Querying Modules \\uf0c1 The cuDLA device handle needs an appropriate loadable to be associated with it before any DLA task submission occurs. The loadable is usually created offline using TensorRT. The loadable has information about the number of input and output tensors as well as their respective metadata and can be queried by the application to retrieve this information. A typical application flow after a successful cuDLA device initialization would look like this (interspersed with some debug logs): DPRINTF ( \"Device created successfully \\\\n \" ); // Load the loadable from \\'loadableData\\' in which the loadable binary has // been copied from the location of the loadable - disk or otherwise. err = cudlaModuleLoadFromMemory ( devHandle , loadableData , file_size , & amp ; moduleHandle , 0 ); if ( err != cudlaSuccess ) { // handle error } // Get tensor attributes. uint32_t numInputTensors = 0 ; uint32_t numOutputTensors = 0 ; cudlaModuleAttribute attribute ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_NUM_INPUT_TENSORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } numInputTensors = attribute . numInputTensors ; DPRINTF ( \"numInputTensors = %d \\\\n \" , numInputTensors ); err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_NUM_OUTPUT_TENSORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } numOutputTensors = attribute . numOutputTensors ; DPRINTF ( \"numOutputTensors = %d \\\\n \" , numOutputTensors ); cudlaModuleTensorDescriptor * inputTensorDesc = ( cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numInputTensors ); cudlaModuleTensorDescriptor * outputTensorDesc = ( cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numOutputTensors ); if (( inputTensorDesc == NULL ) || ( outputTensorDesc == NULL )) { // handle error } attribute . inputTensorDesc = inputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_INPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } attribute . outputTensorDesc = outputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_OUTPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err != cudlaSuccess ) { // handle error } Applications can use the retrieved tensor descriptors to setup their data buffers in terms of size and formats. Detailed information about the contents of the tensor descriptors is present in the API specification section under cudlaModuleGetAttributes() . 7.1.3. Memory Model \\uf0c1 The GPU and DLA have different MMUs that manage the VA to PA conversion while performing their respective functions. The figure below shows an example where the GMMU performs a translation for GPU VAs and the SMMU performs a similar function for the VAs arriving from the DLA. Virtual address to physical address conversion \\uf0c1 In hybrid mode, before a CUDA pointer can be accessed by the DLA, it is necessary that the CUDA pointer be registered with the DLA. This registration step creates an entry in the SMMU and returns the corresponding VA for use in task submissions. The following code snippet shows an example registration for a device handle created with the flag CUDLA_CUDA_DLA : // Allocate memory on GPU. void * buffer ; uint32_t size = 100 ; result = cudaMalloc ( & inputBufferGPU , size ); if ( result != cudaSuccess ) { // handle error } // Register the CUDA-allocated buffers. uint64_t * bufferRegisteredPtr = NULL ; err = cudlaMemRegister ( devHandle , ( uint64_t * ) inputBufferGPU , size , & bufferRegisteredPtr , 0 ); if ( err != cudlaSuccess ) { // handle error } In standalone mode, cuDLA functions without the underlying CUDA device. Consequently, in this mode, the memory allocations performed by the application (which need to be subsequently registered) need to come from outside CUDA. On Tegra systems, cuDLA supports registration of NvSciBuf allocations via the cudlaImportExternalMemory() API as the following code snippet shows: // Allocate the NvSciBuf object. NvSciBufObj inputBufObj ; sciError = NvSciBufObjAlloc ( reconciledInputAttrList , & inputBufObj ); if ( sciError != NvSciError_Success ) { // handle error } uint64_t * inputBufObjRegPtr = NULL ; // importing external memory cudlaExternalMemoryHandleDesc memDesc = { 0 }; memset ( & memDesc , 0 , sizeof ( memDesc )); memDesc . extBufObject = ( void * ) inputBufObj ; memDesc . size = size ; err = cudlaImportExternalMemory ( devHandle , & memDesc , & inputBufObjRegPtr , 0 ); if ( err != cudlaSuccess ) { // handle error } 7.1.4. Task Execution and Synchronization Model \\uf0c1 7.1.4.1. Task Execution \\uf0c1 Submitting a DLA task for execution is similar to submitting a CUDA kernel to the GPU. cuDLA natively supports CUDA streams and works seamlessly with the stream semantics to ensure that all tasks intended for the DLA are executed by the DLA HW only after the previous tasks on the stream have completed execution. This enables applications to setup complex processing workflows between the GPU and the DLA using familiar stream semantics without having to manage memory coherency and execution dependencies between GPU and DLA. A visual illustration of the execution model is shown in the following figure. DLA tasks can be interspersed with GPU tasks in a given stream or multiple streams and cudlaSubmitTask() handles all the memory/execution dependencies. cuDLA task execution model \\uf0c1 The submit task API needs the input and output tensors in the form of the addresses registered with the DLA (using cudlaMemRegister() ). An application can pre-register all the required pointers with cuDLA and then use the registered pointers during cudlaSubmitTask() . This API, in turn, ensures that the results of the previous operations on the underlying memory corresponding to the registered pointers is visible to the DLA before it begins execution of the current task. A typical application code consisting of CUDA and cuDLA operations is shown in the snippet below: DPRINTF ( \"ALL MEMORY REGISTERED SUCCESSFULLY \\\\n \" ); // Copy data from CPU buffers to GPU buffers. result = cudaMemcpyAsync ( inputBufferGPU , inputBuffer , inputTensorDesc [ 0 ]. size , cudaMemcpyHostToDevice , stream ); if ( result != cudaSuccess ) { // handle error } result = cudaMemsetAsync ( outputBufferGPU , 0 , outputTensorDesc [ 0 ]. size , stream ); if ( result != cudaSuccess ) { // handle error } // Enqueue a cuDLA task. cudlaTask task ; task . moduleHandle = moduleHandle ; task . outputTensor = & outputBufferRegisteredPtr ; task . numOutputTensors = 1 ; task . numInputTensors = 1 ; task . inputTensor = & inputBufferRegisteredPtr ; task . waitEvents = NULL ; task . signalEvents = NULL ; err = cudlaSubmitTask ( devHandle , & task , 1 , stream , 0 ); if ( err != cudlaSuccess ) { // handle error } DPRINTF ( \"SUBMIT IS DONE ! !'},\n",
       " {'id': 969,\n",
       "  'content': '! \\\\n \" ); result = cudaMemcpyAsync ( outputBuffer , outputBufferGPU , outputTensorDesc [ 0 ]. size , cudaMemcpyDeviceToHost , stream ); if ( result != cudaSuccess ) { // handle error } In standalone mode, the stream parameter in cudlaSubmitTask() must be specified as NULL as cuDLA is operating independently of CUDA. In this case, the tasks submitted to the DLA are executed in FIFO order. 7.1.4.1.1. Multithreaded User Submission \\uf0c1 Users can specify the CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE flag during submission to a particular device handle if they are sure that submission to this particular device handle occurs only in this thread and that there is no shared data at the application level between this device handle and any other device handle which might be used in a parallel thread for submission. This flag facilitates some optimizations in the submission path which might lead to better submission times from the application point of view.'},\n",
       " {'id': 970,\n",
       "  'content': '7.1.4.2. Synchronization \\uf0c1 Synchronization of tasks in hybrid mode does not need a different API. Since DLA tasks are submitted to CUDA streams, it is sufficient to wait on the stream to complete its work in order to ensure that all DLA tasks submitted on that stream are completed. In this regard DLA task synchronization is compatible with any of the different synchronization mechanisms available in CUDA – Event, Stream, Device – and the entire CUDA machinery is available for applications to setup different flows and usecases. In standalone mode, however, the synchronization mechanisms are different given that cuDLA operates independently of CUDA. In this mode, the cudlaTask structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively as part of cudlaSubmitTask() . Each submitted task will wait for all its wait events to be signaled before beginning execution and will provide a signal event (if one is requested for during cudlaSubmitTask() ) that the application (or any other entity) can wait on to ensure that the submitted task has completed execution. In cuDLA 1.0, only NvSciSync fences are supported as part of wait events. Furthermore, only NvSciSync objects can be registered and signaled as part of signal events and the fence corresponding to the signaled event is returned as part of cudlaSubmitTask() . Like all memory operations, the underlying backing store for the events (in this case the NvSciSync object) must be registered with cuDLA before using it in a task submission. The code snippet below shows an example flow where the application creates an input and output NvSciSync object and registers them, creates fences corresponding to them, marks the corresponding fences as wait/signal as part of cudlaSubmitTask() and then signals the input fence and waits on the output fence. 7.1.4.2.1. Registering an external semaphore: \\uf0c1 sciError = NvSciSyncObjAlloc ( nvSciSyncReconciledListObj1 , & syncObj1 ); if ( sciError != NvSciError_Success ) { // handle error } sciError = NvSciSyncObjAlloc ( nvSciSyncReconciledListObj2 , & syncObj2 ); if ( sciError != NvSciError_Success ) { // handle error } // importing external semaphore uint64_t * nvSciSyncObjRegPtr1 = NULL ; uint64_t * nvSciSyncObjRegPtr2 = NULL ; cudlaExternalSemaphoreHandleDesc semaMemDesc = { 0 }; memset ( & semaMemDesc , 0 , sizeof ( semaMemDesc )); semaMemDesc . extSyncObject = syncObj1 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr1 , 0 ); if ( err != cudlaSuccess ) { // handle error } memset ( & semaMemDesc , 0 , sizeof ( semaMemDesc )); semaMemDesc . extSyncObject = syncObj2 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr2 , 0 ); if ( err != cudlaSuccess ) { // handle error } DPRINTF ( \"ALL EXTERNAL SEMAPHORES REGISTERED SUCCESSFULLY \\\\n \" ); 7.1.4.2.2. Events setup for cudlaSubmitTask() \\uf0c1 // Wait events NvSciSyncFence preFence = NvSciSyncFenceInitializer ; sciError = NvSciSyncObjGenerateFence ( syncObj1 , & preFence ); if ( sciError != NvSciError_Success ) { // handle error } cudlaWaitEvents * waitEvents ; waitEvents = ( cudlaWaitEvents * ) malloc ( sizeof ( cudlaWaitEvents )); if ( waitEvents == NULL ) { // handle error } waitEvents -> numEvents = 1 ; CudlaFence * preFences = ( CudlaFence * ) malloc ( waitEvents -> numEvents * sizeof ( CudlaFence )); if ( preFences == NULL ) { // handle error } preFences [ 0 ]. fence = & preFence ; preFences [ 0 ]. type = CUDLA_NVSCISYNC_FENCE ; waitEvents -> preFences = preFences ; // Signal Events cudlaSignalEvents * signalEvents ; signalEvents = ( cudlaSignalEvents * ) malloc ( sizeof ( cudlaSignalEvents )); if ( signalEvents == NULL ) { // handle error } signalEvents -> numEvents = 1 ; uint64_t ** devPtrs = ( uint64_t ** ) malloc ( signalEvents -> numEvents * sizeof ( uint64_t * )); if ( devPtrs == NULL ) { // handle error } devPtrs [ 0 ] = nvSciSyncObjRegPtr2 ; signalEvents -> devPtrs = devPtrs ; signalEvents -> eofFences = ( CudlaFence * ) malloc ( signalEvents -> numEvents * sizeof ( CudlaFence )); if ( signalEvents -> eofFences == NULL ) { // handle error } NvSciSyncFence eofFence = NvSciSyncFenceInitializer ; signalEvents -> eofFences [ 0 ]. fence = & eofFence ; signalEvents -> eofFences [ 0 ]. type = CUDLA_NVSCISYNC_FENCE ; // Enqueue a cuDLA task. outputTensor = & outputBufObjRegPtr ; task .'},\n",
       " {'id': 971,\n",
       "  'content': 'inputTensor = & inputBufObjRegPtr ; task . waitEvents = waitEvents ; task . signalEvents = signalEvents ; err = cudlaSubmitTask ( devHandle , & task , 1 , NULL , 0 ); if ( err != cudlaSuccess ) { // handle error } DPRINTF ( \"SUBMIT IS DONE ! \\\\n \" ); 7.1.4.2.3. Waiting on the signal event \\uf0c1 // Signal wait events. // For illustration purposes only. In practice, this signal will be done by another // entity or driver that provides the data input for this particular submitted task. NvSciSyncObjSignal ( syncObj1 ); // Wait for operations to finish. In practice, this wait will be done by // another entity or driver that is waiting for the output of the submitted task. sciError = NvSciSyncFenceWait ( reinterpret_cast ( signalEvents -> eofFences [ 0 ]. fence ), nvSciCtx , -1 ); if ( sciError != NvSciError_Success ) { // handle error } 7.1.4.2.4. Supported Synchronization Primitives in cuDLA \\uf0c1 cuDLA supports two types of NvSciSync object primitives. These are sync point and deterministic semaphores. By default, cuDLA prioritizes sync point primitive over deterministic semaphore primitive and sets these priorities in the NvSciSync attribute list when requested by the application using cudlaGetNvSciSyncAttributes() . For Deterministic semaphore, the NvSciSync attribute list used to create the NvSciSync object must have the value of NvSciSyncAttrKey_RequireDeterministicFences key set to true. Deterministic fences allow users to enqueue a wait over the semaphore object even before corresponding signal is enqueued. For such semaphore object, cuDLA guarantees that each signal operation will increment the fence value by ‘1’. Users are expected to keep track of signals enqueued on the semaphore object and insert waits accordingly. 7.1.4.2.5. Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList \\uf0c1 // Set NvSciSyncAttrKey_RequireDeterministicFences key to true in // NvScisyncAtrrList that is used to create NvSciSync object with // Deterministic Semaphore primitive. NvSciSyncAccessPerm cpuPerm = NvSciSyncAccessPerm_SignalOnly ; keyValue [ 0 ]. attrKey = NvSciSyncAttrKey_RequiredPerm ; keyValue [ 0 ]. value = ( void * ) & cpuPerm ; keyValue [ 0 ]. len = sizeof ( cpuPerm ); bool detFenceReq = true ; keyValue [ 1 ]. attrKey = NvSciSyncAttrKey_RequireDeterministicFences ; keyValue [ 1 ]. value = ( const void * ) & detFenceReq ; keyValue [ 1 ]. len = sizeof ( detFenceReq ); return NvSciSyncAttrListSetAttrs ( list , keyValue , 2 ); 7.1.4.2.6. Timestamp Support for NvSciFence \\uf0c1 cuDLA supports the timestamp feature of NvSci in cuDLA standalone mode. Timestamp support enables users to get the time at which a particular fence has been signaled. This time value is the snapshot of the DLA clock in microseconds. cuDLA users can request timestamp support by setting the value of the NvSciSyncAttrKey_WaiterRequireTimestamps key as true while filling up the NvSci waiter attribute list. The users can use this timestamp along with SOF(Start Of Frame) fence and EOF(End OF Frame) fence to get a snapshot of DLA clock just before start of task & after task completion respectively. This enables users to calculate time taken by DLA to execute the submitted task. 7.1.4.2.7. Requesting Timestamp Support for NvSciSync Object \\uf0c1 sciError fillCpuWaiterAttrList ( NvSciSyncAttrList list ) { bool cpuWaiter = true ; NvSciSyncAttrKeyValuePair keyValue [ 3 ]; memset ( keyValue , 0 , sizeof ( keyValue )); keyValue [ 0 ]. attrKey = NvSciSyncAttrKey_NeedCpuAccess ; keyValue [ 0 ]. value = ( void * ) & cpuWaiter ; keyValue [ 0 ]. len = sizeof ( cpuWaiter ); NvSciSyncAccessPerm cpuPerm = NvSciSyncAccessPerm_WaitOnly ; keyValue [ 1 ]. attrKey = NvSciSyncAttrKey_RequiredPerm ; keyValue [ 1 ]. value = ( void * ) & cpuPerm ; keyValue [ 1 ]. len = sizeof ( cpuPerm ); bool cpuRequiresTimeStamp = true ; keyValue [ 2 ]. attrKey = NvSciSyncAttrKey_WaiterRequireTimestamps ; keyValue [ 2 ]. value = ( void * ) & cpuRequiresTimeStamp ; keyValue [ 2 ]. len = sizeof ( cpuRequiresTimeStamp ); return NvSciSyncAttrListSetAttrs ( list , keyValue , 3 ); } NvSciSyncCpuWaitContext nvSciCtx ; NvSciSyncModule syncModule ; NvSciSyncAttrList waiterAttrListObj = nullptr ; NvSciSyncAttrList signalerAttrListObj = nullptr ; NvSciSyncAttrList syncAttrListObj [ 2 ]; NvSciSyncAttrList nvSciSyncConflictListObj ; NvSciSyncAttrList nvSciSyncReconciledListObj ; sciError = NvSciSyncModuleOpen ( & syncModule ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncAttrListCreate ( syncModule , & signalerAttrListObj ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncAttrListCreate ( syncModule , & waiterAttrListObj ); if ( sciError != NvSciError_Success ) { //handle error } err = cudlaGetNvSciSyncAttributes ( reinterpret_cast ( signalerAttrListObj ), CUDLA_NVSCISYNC_ATTR_SIGNAL ); if ( err != cudlaSuccess ) { //handle error } sciError = fillCpuWaiterAttrList ( waiterAttrListObj ); if ( sciError != NvSciError_Success ) { //handle error } syncAttrListObj [ 0 ] = signalerAttrListObj ; syncAttrListObj [ 1 ] = waiterAttrListObj ; sciError = NvSciSyncAttrListReconcile ( syncAttrListObj , 2 , & nvSciSyncReconciledListObj , & nvSciSyncConflictListObj3 ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncObjAlloc ( nvSciSyncReconciledListObj , & syncObj ); if ( sciError != NvSciError_Success ) { //handle error } sciError = NvSciSyncCpuWaitContextAlloc ( syncModule , & nvSciCtx ); if ( sciError != NvSciError_Success ) { //handle error } 7.1.4.2.8. Extracting Timestamp Value from Fence \\uf0c1 Refer to these sections for more information: Registering an external semaphore: Events setup for cudlaSubmitTask() Waiting on the signal event // To extract Timestamp of the fence // Timestamp will be valid only after fence is signaled // hence Fence must be waited up on before extracting timestamp value uint64_t eofTimestampUS = 0UL ; sciError = NvSciSyncFenceGetTimestamp ( reinterpret_cast ( signalEvents -> eofFences .'},\n",
       " {'id': 972,\n",
       "  'content': 'fence ), & ( eofTimestampUS )); if (( sciError != NvSciError_Success ) || ( eofTimestampUS == 0UL )) { //handle error } 7.1.4.3. Fault Diagnostics \\uf0c1 To perform fault diagnostics for DLA HW, users should specify the CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS flag to load the module and CUDLA_SUBMIT_DIAGNOSTICS_TASK during task submission. This task can be used to probe the state of DLA HW. With this flag set, in standalone mode the user is not allowed to do event only submissions, where tensor information is NULL and only events (wait/signal or both) are present in task. This is because the task always runs on an internally loaded diagnostic module. This diagnostic module does not expect any input tensors and so does not require input tensor memory. However the user is expected to query the number of output tensors, allocate the output tensor memory, and pass the same while using the submit task. 7.1.4.4. NOOP Submission \\uf0c1 Users can mark certain tasks as noop tasks while calling cudlaSubmitTask() . This is done by passing CUDLA_SUBMIT_NOOP in the flags parameter of cudlaSubmitTask() . A noop submission implies that all the other submission semantics are maintained. Specifically, the task is submitted to DLA, wait/signal events are considered before and after and stream semantics are respected. The key difference is that the task is skipped by the DLA for execution. This is supported in both hybrid and standalone modes.'},\n",
       " {'id': 973,\n",
       "  'content': '7.1.5. Error Reporting Model \\uf0c1 The asynchronous nature of task execution results in two kinds of errors that can get reported via cuDLA APIs: Synchronous errors Asynchronous errors Synchronous errors are those that are reported by the cuDLA APIs as part of their return code when they are invoked in an application. Asynchronous errors are those that are detected later compared to sequential program execution. The typical scenario here is that each task submitted to the DLA HW executes after a particular duration of time. As a result, if there are errors in the task execution, they cannot be reported as part of the task submission APIs. Depending on the timing of the errors, they are reported during a subsequent cuDLA API call or after a synchronization operation. HW execution errors reported as part of cuDLA APIs are straightforward to handle at the application level. However, if there is a no cuDLA API call currently executing or about to execute in the application, then the application needs to perform extra steps to handle asynchronous errors. In hybrid mode, DLA HW errors can get reported via CUDA synchronization operations. As mentioned in the device model section, cuDLA logically associates DLA with a GPU for the purposes of execution. Therefore, any DLA HW errors are propagated via CUDA to the user. The user needs to check for DLA-specific errors from CUDA synchronization operations and then check the cuDLA device handle for the exact error using cudlaGetLastError() . If there are multiple cuDLA device handles in the application and each of them have submitted some tasks to cuDLA in hybrid mode, then each and every device handle much be checked for errors. The underlying model here is to use CUDA to detect DLA HW errors and then use cudlaGetLastError() on the relevant handle to report the exact error. The code snippet below shows an example: result = cudaStreamSynchronize ( stream ); if ( result != cudaSuccess ) { DPRINTF ( \"Error in synchronizing stream = %s \\\\n \" , cudaGetErrorName ( result )); if ( result == cudaErrorExternalDevice ) { cudlaStatus hwStatus = cudlaGetLastError ( devHandle ); if ( hwStatus != cudlaSuccess ) { DPRINTF ( \"Asynchronous error in HW = %u \\\\n \" , hwStatus ); } } } This error reporting model is compatible with CUDA Driver APIs as well and therefore if the application uses CUDA Driver APIs for synchronization, similar error codes and error handling flow is applicable. In standalone mode, the model is similar with the exception that there is no corresponding mechanism to detect errors as part of synchronization operations. In this mode, the only option that an application has to wait on the submitted tasks is to wait on the NvSciSync fence returned by the latest submission. As of this writing, NvSciSync does not support reporting DLA HW errors and therefore an application is expected to wait for the fence and then query cudlaGetLastError() for any errors during execution. 7.2. Migrating from NvMediaDla to cuDLA \\uf0c1 NvMediaDla and cuDLA have different programming models with some degree of overlap in the functionality exposed by the respective APIs. The following table provides a mapping from the NvMediaDla API to the equivalent cuDLA API or functionality. This is intended to be used as a reference when migrating an NvMediaDla app to a cuDLA app. NvMediaDla cuDLA NvMediaDlaGetVersion() cudlaGetVersion() NvMediaDlaPingById() Not required as ping is done inside cudlaCreateDevice and only upon successful ping does device handle creation succeed. NvMediaDlaCreate() cudlaCreateDevice() NvMediaDlaDestroy() cudlaDestroyDevice() NvMediaDlaGetUMDVersion() Not available NvMediaDlaGetNumEngines() cudlaDeviceGetCount() NvMediaDlaGetMaxOutstandingTasks() Not available NvMediaDlaInit() cudlaCreateDevice (but specifying number of input tasks is not available) NvMediaDlaGetInstanceId() Not available NvMediaDlaGetNumTasks() Not available NvMediaDlaLoadableCreate() Not required as declaring a variable of type cudlaModule is sufficient alongwith cudlaModuleLoadFromMemory() . NvMediaDlaLoadableDestroy() Not required as cuDLA modules are declared as variables of type cudlaModule . NvMediaDlaAppendLoadable() Not required as this is done inside cudlaModuleLoadFromMemory() . NvMediaDlaSetCurrentLoadable() Not required as this is done inside cudlaModuleLoadFromMemory() . NvMediaDlaGetNumOfInputTensors() cudlaModuleGetAttributes() NvMediaDlaGetInputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaGetNumOfOutputTensors() cudlaModuleGetAttributes() NvMediaDlaGetOutputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaDataRegister() cudlaMemRegister() NvMediaDlaDataUnregister() cudlaMemUnregister() NvMediaDlaLoadLoadable() cudlaModuleLoadFromMemory() NvMediaDlaRemoveLoadable() cudlaModuleUnload() NvMediaDlaSubmit() cudlaSubmitTask() NvMediaDlaNvSciSyncGetVersion() Not available NvMediaDlaFillNvSciSyncAttrList() cudlaGetNvSciSyncAttributes() NvMediaDlaRegisterNvSciSyncObj() cudlaImportExternalSemaphore() NvMediaDlaUnregisterNvSciSyncObj() cudlaMemUnregister() NvMediaDlaSetNvSciSyncObjforEOF() Not required as cudlaTask structure has the required capability to specify this. NvMediaDlaInsertPreNvSciSyncFence() Not required as cudlaTask structure has the required capability to specify this. NvMediaDlaGetEOFNvSciSyncFence() Not required as cudlaTask structure has the required capability to retrieve this. 7.3. Profiling a cuDLA App \\uf0c1 cuDLA APIs can be profiled using NVIDIA Nsight Systems. The following command can be used to generate traces for cuDLA APIs. These traces can be viewed in Nsight. $ nsys profile --trace nvtx -e CUDLA_NVTX_LEVEL=1 --output 7.4. cuDLA Release Notes \\uf0c1 Known Issues in cuDLA 1.2.1: In hybrid mode, cuDLA internally allocates memory with CUDA using the primary context. As a result, before destroying/resetting a CUDA primary context, it is mandatory that all cuDLA device initializations are destroyed. Before destroying a cuDLA device handle, it is important to ensure that all tasks submitted previously to the device are completed. Failure to do so can lead to application crashes as the internal memory allocations would still be in use. NvSciBuf buffer allocations made by the application must adhere to DLA alignment constraints. It is the application’s responsibility to ensure that there are no duplicate fences specified as part of wait events while submitting tasks. In general, any synchronous or asynchronous error returned by cuDLA APIs must be treated as a non-recoverable error. In this case, the application is expected to restart and initialize cuDLA again in order to submit DLA tasks. The exception to this rule is cudlaErrorMemoryRegistered which is returned by cuDLA when the application tries to register a particular memory again without unregistering. cuDLA does not support UVM between CUDA and DLA. cuDLA does not support CUDA Graph. cuDLA does not support per-thread default stream. cuDLA does not support CNP (DLA functions cannot be used with CNP). cuDLA does not support block linear memory. cuDLA does not support CUDA VMM APIs at the present moment. cuDLA does not support dGPU. Under certain conditions, DLA FW can hang for certain tasks. This can result in the application hanging in both hybrid as well as standalone mode. Applications are expected to detect these scenarios and respond accordingly.'},\n",
       " {'id': 974, 'content': '8.'},\n",
       " {'id': 975,\n",
       "  'content': 'Notices \\uf0c1 8.1. Notice \\uf0c1 This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality. NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice. Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete. NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document. NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk. NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs. No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA. Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices. THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product. 8.2. OpenCL \\uf0c1 OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.'},\n",
       " {'id': 976,\n",
       "  'content': '8.3. Trademarks \\uf0c1 NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates. All rights reserved.'},\n",
       " {'id': 977,\n",
       "  'content': 'Last updated on Jul 1, 2024. jQuery(function () { SphinxRtdTheme.Navigation.enable(false); }); if (typeof _satellite !== \"undefined\"){_satellite.pageBottom();}1. Introduction 2.'},\n",
       " {'id': 978,\n",
       "  'content': 'Preconditioned Iterative Methods 2.1. Algorithm 1 Conjugate Gradient (CG) 2.2. Algorithm 2 Bi-Conjugate Gradient Stabilized (BiCGStab) 3. Numerical Experiments 4.'},\n",
       " {'id': 979, 'content': 'Conclusion 5.'},\n",
       " {'id': 980,\n",
       "  'content': 'Acknowledgements 6. References 7. Notices 7.1. Notice 7.2.'},\n",
       " {'id': 981,\n",
       "  'content': 'OpenCL 7.3. Trademarks Incomplete-LU and Cholesky Preconditioned Iterative Methods » 1. Introduction v12.5 | PDF | Archive Incomplete-LU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS White paper describing how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. Introduction \\uf0c1 The solution of large sparse linear systems is an important problem in computational mechanics, atmospheric modeling, geophysics, biology, circuit simulation and many other applications in the field of computational science and engineering. In general, these linear systems can be solved using direct or preconditioned iterative methods. Although the direct methods are often more reliable, they usually have large memory requirements and do not scale well on massively parallel computer platforms. The iterative methods are more amenable to parallelism and therefore can be used to solve larger problems. Currently, the most popular iterative schemes belong to the Krylov subspace family of methods. They include Bi-Conjugate Gradient Stabilized (BiCGStab) and Conjugate Gradient (CG) iterative methods for nonsymmetric and symmetric positive definite (s.p.d.)\\nlinear systems, respectively [2] , [11] . We describe these methods in more detail in the next section. In practice, we often use a variety of preconditioning techniques to improve the convergence of the iterative methods. In this white paper we focus on the incomplete-LU and Cholesky preconditioning [11] , which is one of the most popular of these preconditioning techniques. It computes an incomplete factorization of the coefficient matrix and requires a solution of lower and upper triangular linear systems in every iteration of the iterative method. In order to implement the preconditioned BiCGStab and CG we use the sparse matrix-vector multiplication [3] , [15] and the sparse triangular solve [8] , [16] implemented in the cuSPARSE library. We point out that the underlying implementation of these algorithms takes advantage of the CUDA parallel programming paradigm [5] , [9] , [13] , which allows us to explore the computational resources of the graphical processing unit (GPU). In our numerical experiments the incomplete factorization is performed on the CPU (host) and the resulting lower and upper triangular factors are then transferred to the GPU (device) memory before starting the iterative method. However, the computation of the incomplete factorization could also be accelerated on the GPU. We point out that the parallelism available in these iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand. In our numerical experiments the incomplete-LU and Cholesky preconditioned iterative methods achieve on average more than 2x speedup using the cuSPARSE and cuBLAS libraries on the GPU over the MKL [17] implementation on the CPU. For example, the speedup for the preconditioned iterative methods with the incomplete-LU and Cholesky factorization with 0 fill-in (ilu0) is shown in Figure 1 for matrices resulting from a variety of applications. It will be described in more detail in the last section. Speedup of the Incomplete-LU Cholesky (with 0 fill-in) Prec. Iterative Methods \\uf0c1 In the next sections we briefly describe the methods of interest and comment on the role played in them by the parallel sparse matrix-vector multiplication and triangular solve algorithms. 2. Preconditioned Iterative Methods \\uf0c1 Let us consider the linear system \\\\(A\\\\mathbf{x} = \\\\mathbf{f}\\\\) where \\\\(A \\\\in \\\\mathbb{R}^{n \\\\times n}\\\\) is a nonsingular coefficient matrix and \\\\(\\\\mathbf{x},\\\\mathbf{f} \\\\in \\\\mathbb{R}^{n}\\\\) are the solution and right-hand-side vectors. In general, the iterative methods start with an initial guess and perform a series of steps that find more accurate approximations to the solution. There are two types of iterative methods: (i) the stationary iterative methods, such as the splitting-based Jacobi and Gauss-Seidel (GS), and (ii) the nonstationary iterative methods, such as the Krylov subspace family of methods, which includes CG and BiCGStab . As we mentioned earlier we focus on the latter in this white paper. The convergence of the iterative methods depends highly on the spectrum of the coefficient matrix and can be significantly improved using preconditioning. The preconditioning modifies the spectrum of the coefficient matrix of the linear system in order to reduce the number of iterative steps required for convergence. It often involves finding a preconditioning matrix \\\\(M\\\\) , such that \\\\(M^{- 1}\\\\) is a good approximation of \\\\(A^{- 1}\\\\) and the systems with \\\\(M\\\\) are relatively easy to solve. For the s.p.d. matrix \\\\(A\\\\) we can let \\\\(M\\\\) be its incomplete-Cholesky factorization, so that \\\\(A \\\\approx M = {\\\\widetilde{R}}^{T}\\\\widetilde{R}\\\\) , where \\\\(\\\\widetilde{R}\\\\) is an upper triangular matrix. Let us assume that \\\\(M\\\\) is nonsingular, then \\\\({\\\\widetilde{R}}^{- T}A{\\\\widetilde{R}}^{- 1}\\\\) is s.p.d. and instead of solving the linear system (1) , we can solve the preconditioned linear system \\\\(\\\\left( {{\\\\widetilde{R}}^{- T}A{\\\\widetilde{R}}^{- 1}} \\\\right)\\\\left( {\\\\widetilde{R}\\\\mathbf{x}} \\\\right) = {\\\\widetilde{R}}^{- T}\\\\mathbf{f}\\\\) The pseudocode for the preconditioned CG iterative method is shown in Algorithm 1 . 2.1. Algorithm 1 Conjugate Gradient (CG) \\uf0c1 1: \\\\(\\\\text{Letting initial guess be }\\\\mathbf{x}_{0}\\\\text{, compute }\\\\mathbf{r}\\\\leftarrow\\\\mathbf{f} - A\\\\mathbf{x}_{0}\\\\) 2: \\\\(\\\\textbf{for }i\\\\leftarrow 1,2,...\\\\text{ until convergence }\\\\textbf{do}\\\\) 3: \\\\(\\\\quad\\\\quad\\\\text{Solve }M\\\\mathbf{z}\\\\leftarrow\\\\mathbf{r}\\\\) \\\\(\\\\vartriangleright \\\\text{Sparse lower and upper triangular solves}\\\\) 4: \\\\(\\\\quad\\\\quad\\\\rho_{i}\\\\leftarrow\\\\mathbf{r}^{T}\\\\mathbf{z}\\\\) 5: \\\\(\\\\quad\\\\quad\\\\textbf{if }i==1\\\\textbf{ then}\\\\) 6: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\mathbf{p}\\\\leftarrow\\\\mathbf{z}\\\\) 7: \\\\(\\\\quad\\\\quad\\\\textbf{else}\\\\) 8: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\beta\\\\leftarrow\\\\frac{\\\\rho_{i}}{\\\\rho_{i - 1}}\\\\) 9: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\mathbf{p}\\\\leftarrow\\\\mathbf{z} + \\\\beta\\\\mathbf{p}\\\\) 10: \\\\(\\\\quad\\\\quad\\\\textbf{end if}\\\\) 11: \\\\(\\\\quad\\\\quad\\\\text{Compute }\\\\mathbf{q}\\\\leftarrow A\\\\mathbf{p}\\\\) \\\\(\\\\vartriangleright \\\\text{Sparse matrix-vector multiplication}\\\\) 12: \\\\(\\\\quad\\\\quad\\\\alpha\\\\leftarrow\\\\frac{\\\\rho_{i}}{\\\\mathbf{p}^{T}\\\\mathbf{q}}\\\\) 13: \\\\(\\\\quad\\\\quad\\\\mathbf{x}\\\\leftarrow\\\\mathbf{x} + \\\\alpha\\\\mathbf{p}\\\\) 14: \\\\(\\\\quad\\\\quad\\\\mathbf{r}\\\\leftarrow\\\\mathbf{r} - \\\\alpha\\\\mathbf{q}\\\\) 15: \\\\(\\\\textbf{end for}\\\\) Notice that in every iteration of the incomplete-Cholesky preconditioned CG iterative method we need to perform one sparse matrix-vector multiplication and two triangular solves. The corresponding CG code using the cuSPARSE and cuBLAS libraries in C programming language is shown below.'},\n",
       " {'id': 982,\n",
       "  'content': '/***** CG Code *****/ /* ASSUMPTIONS: 1. The cuSPARSE and cuBLAS libraries have been initialized. The appropriate memory has been allocated and set to zero.'},\n",
       " {'id': 983,\n",
       "  'content': '3. The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- Cholesky upper triangular factor R (valR, csrRowPtrR, csrColIndR) have been computed and are present in the device (GPU) memory. */ //create the info and analyse the lower and upper triangular factors cusparseCreateSolveAnalysisInfo ( & inforRt ); cusparseCreateSolveAnalysisInfo ( & inforR ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforRt ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforR ); //1: compute initial residual r = f - A x0 (using initial guess in x) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , x , 0.0 , r ); cublasDscal ( n , -1.0 , r , 1 ); cublasDaxpy ( n , 1.0 , f , 1 , r , 1 ); nrmr0 = cublasDnrm2 ( n , r , 1 ); //2: repeat until convergence (based on max. it.'},\n",
       " {'id': 984,\n",
       "  'content': 'and relative residual) for ( i = 0 ; i 1\\\\textbf{ then}\\\\) 9: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\textbf{if }\\\\omega==0.0\\\\textbf{ then}\\\\) 10: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\quad\\\\quad\\\\text{method failed}\\\\) 11: \\\\(\\\\quad\\\\quad\\\\textbf{end if}\\\\) 12: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\beta\\\\leftarrow\\\\frac{\\\\rho_{i}}{\\\\rho_{i - 1}} times \\\\frac{\\\\alpha}{\\\\omega}\\\\) 13: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\mathbf{p}\\\\leftarrow\\\\mathbf{r} + \\\\beta\\\\left( {\\\\mathbf{p} - \\\\omega\\\\mathbf{v}} \\\\right)\\\\) 14: \\\\(\\\\quad\\\\quad\\\\textbf{end if}\\\\) 15: \\\\(\\\\quad\\\\quad\\\\text{Solve }M\\\\hat{\\\\mathbf{p}}\\\\leftarrow\\\\mathbf{p}\\\\) \\\\(\\\\vartriangleright \\\\text{Sparse lower and upper triangular solves}\\\\) 16: \\\\(\\\\quad\\\\quad\\\\text{Compute }\\\\mathbf{q}\\\\leftarrow A\\\\hat{\\\\mathbf{p}}\\\\) \\\\(\\\\vartriangleright \\\\text{Sparse matrix-vector multiplication}\\\\) 17: \\\\(\\\\quad\\\\quad\\\\alpha\\\\leftarrow\\\\frac{\\\\rho_{i}}{{\\\\widetilde{\\\\mathbf{r}}}^{T}\\\\mathbf{q}}\\\\) 18: \\\\(\\\\quad\\\\quad\\\\mathbf{s}\\\\leftarrow\\\\mathbf{r} - \\\\alpha\\\\mathbf{q}\\\\) 19: \\\\(\\\\quad\\\\quad\\\\mathbf{x}\\\\leftarrow\\\\mathbf{x} + \\\\alpha\\\\hat{\\\\mathbf{p}}\\\\) 20: \\\\(\\\\quad\\\\quad\\\\textbf{if }\\\\left\\\\| s \\\\right\\\\|_{2} \\\\leq \\\\mathit{tol}\\\\textbf{ then}\\\\) 21: \\\\(\\\\quad\\\\quad\\\\quad\\\\quad\\\\text{method converged}\\\\) 22: \\\\(\\\\quad\\\\quad\\\\textbf{end if}\\\\) 23: \\\\(\\\\quad\\\\quad\\\\text{Solve }M\\\\hat{\\\\mathbf{s}}\\\\leftarrow\\\\mathbf{s}\\\\) \\\\(\\\\vartriangleright \\\\text{Sparse lower and upper triangular solves}\\\\) 24: \\\\(\\\\quad\\\\quad\\\\text{Compute }\\\\mathbf{t}\\\\leftarrow A\\\\hat{\\\\mathbf{s}}\\\\) \\\\(\\\\vartriangleright \\\\text{Sparse matrix-vector multiplication}\\\\) 25: \\\\(\\\\quad\\\\quad\\\\omega\\\\leftarrow\\\\frac{\\\\mathbf{t}^{T}\\\\mathbf{s}}{\\\\mathbf{t}^{T}\\\\mathbf{t}}\\\\) 26: \\\\(\\\\quad\\\\quad\\\\mathbf{x}\\\\leftarrow\\\\mathbf{x} + \\\\omega\\\\hat{\\\\mathbf{s}}\\\\) 27: \\\\(\\\\quad\\\\quad\\\\mathbf{r}\\\\leftarrow\\\\mathbf{s} - \\\\omega\\\\mathbf{t}\\\\) 28: \\\\(\\\\textbf{end for}\\\\) Notice that in every iteration of the incomplete-LU preconditioned BiCGStab iterative method we need to perform two sparse matrix-vector multiplications and four triangular solves. The corresponding BiCGStab code using the cuSPARSE and cuBLAS libraries in C programming language is shown below.'},\n",
       " {'id': 985,\n",
       "  'content': '/***** BiCGStab Code *****/ /* ASSUMPTIONS: 1. The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- LU lower L (valL, csrRowPtrL, csrColIndL) and upper U (valU, csrRowPtrU, csrColIndU) triangular factors have been computed and are present in the device (GPU) memory. */ //create the info and analyse the lower and upper triangular factors cusparseCreateSolveAnalysisInfo ( & infoL ); cusparseCreateSolveAnalysisInfo ( & infoU ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrL , valL , csrRowPtrL , csrColIndL , infoL ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrU , valU , csrRowPtrU , csrColIndU , infoU ); //1: compute initial residual r = b - A x0 (using initial guess in x) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , x , 0.0 , r ); cublasDscal ( n , -1.0 , r , 1 ); cublasDaxpy ( n , 1.0 , f , 1 , r , 1 ); //2: Set p=r and \\\\tilde{r}=r cublasDcopy ( n , r , 1 , p , 1 ); cublasDcopy ( n , r , 1 , rw , 1 ); nrmr0 = cublasDnrm2 ( n , r , 1 ); //3: repeat until convergence (based on max. and relative residual) for ( i = 0 ; i 0 ){ //12: \\\\beta = (\\\\rho_{i} / \\\\rho_{i-1}) ( \\\\alpha / \\\\omega ) beta = ( rho / rhop ) * ( alpha / omega ); //13: p = r + \\\\beta (p - \\\\omega v) cublasDaxpy ( n , - omega , q , 1 , p , 1 ); cublasDscal ( n , beta , p , 1 ); cublasDaxpy ( n , 1.0 , r , 1 , p , 1 ); } //15: M \\\\hat{p} = p (sparse lower and upper triangular solves) cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrL , valL , csrRowPtrL , csrColIndL , infoL , p , t ); cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrU , valU , csrRowPtrU , csrColIndU , infoU , t , ph ); //16: q = A \\\\hat{p} (sparse matrix-vector multiplication) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , ph , 0.0 , q ); //17: \\\\alpha = \\\\rho_{i} / (\\\\tilde{r}^{T} q) temp = cublasDdot ( n , rw , 1 , q , 1 ); alpha = rho / temp ; //18: s = r - \\\\alpha q cublasDaxpy ( n , - alpha , q , 1 , r , 1 ); //19: x = x + \\\\alpha \\\\hat{p} cublasDaxpy ( n , alpha , ph , 1 , x , 1 ); //20: check for convergence nrmr = cublasDnrm2 ( n , r , 1 ); if ( nrmr / nrmr0 < tol ){ break ; } //23: M \\\\hat{s} = r (sparse lower and upper triangular solves) cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrL , valL , csrRowPtrL , csrColIndL , infoL , r , t ); cusparseDcsrsv_solve ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , 1.0 , descrU , valU , csrRowPtrU , csrColIndU , infoU , t , s ); //24: t = A \\\\hat{s} (sparse matrix-vector multiplication) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1.0 , descrA , valA , csrRowPtrA , csrColIndA , s , 0.0 , t ); //25: \\\\omega = (t^{T} s) / (t^{T} t) temp = cublasDdot ( n , t , 1 , r , 1 ); temp2 = cublasDdot ( n , t , 1 , t , 1 ); omega = temp / temp2 ; //26: x = x + \\\\omega \\\\hat{s} cublasDaxpy ( n , omega , s , 1 , x , 1 ); //27: r = s - \\\\omega t cublasDaxpy ( n , - omega , t , 1 , r , 1 ); //check for convergence nrmr = cublasDnrm2 ( n , r , 1 ); if ( nrmr / nrmr0 < tol ){ break ; } } //destroy the analysis info (for lower and upper triangular factors) cusparseDestroySolveAnalysisInfo ( infoL ); cusparseDestroySolveAnalysisInfo ( infoU ); As shown in Figure 2 the majority of time in each iteration of the incomplete-LU and Cholesky preconditioned iterative methods is spent in the sparse matrix-vector multiplication and triangular solve. The sparse matrix-vector multiplication has already been extensively studied in the following references [3] , [15] .'},\n",
       " {'id': 986,\n",
       "  'content': 'The sparse triangular solve is not as well known, so we briefly point out the strategy used to explore parallelism in it and refer the reader to the NVIDIA technical report [8] for further details. The Splitting of Total Time Taken on the GPU by the Preconditioned Iterative Method \\uf0c1 To understand the main ideas behind the sparse triangular solve, notice that although the forward and back substitution is an inherently sequential algorithm for dense triangular systems, the dependencies on the previously obtained elements of the solution do not necessarily exist for the sparse triangular systems. We pursue the strategy that takes advantage of the lack of these dependencies and split the solution process into two phases as mentioned in [1] , [4] , [6] , [7] , [8] , [10] , [12] , [14] . The analysis phase builds the data dependency graph that groups independent rows into levels based on the matrix sparsity pattern. The solve phase iterates across the constructed levels one-by-one and computes all elements of the solution corresponding to the rows at a single level in parallel. Notice that by construction the rows within each level are independent of each other, but are dependent on at least one row from the previous level. The analysis phase needs to be performed only once and is usually significantly slower than the solve phase, which can be performed multiple times. This arrangement is ideally suited for the incomplete-LU and Cholesky preconditioned iterative methods. Numerical Experiments \\uf0c1 In this section we study the performance of the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods. We use twelve matrices selected from The University of Florida Sparse Matrix Collection [18] in our numerical experiments. The seven s.p.d. and five nonsymmetric matrices with the respective number of rows (m), columns (n=m) and non-zero elements (nnz) are grouped and shown according to their increasing order in Table 1 . Table 1.'},\n",
       " {'id': 987,\n",
       "  'content': 'Symmetric Positive Definite (s.p.d.)\\nand Nonsymmetric Test Matrices \\uf0c1 # Matrix m,n nnz s.p.d. Application 1 offshore 259,789 4,242,673 yes Geophysics 2 af_shell3 504,855 17,562,051 yes Mechanics 3 parabolic_fem 525,825 3,674,625 yes General 4 apache2 715,176 4,817,870 yes Mechanics 5 ecology2 999,999 4,995,991 yes Biology 6 thermal2 1,228,045 8,580,313 yes Thermal Simulation 7 G3_circuit 1,585,478 7,660,826 yes Circuit Simulation 8 FEM_3D_thermal2 147,900 3,489,300 no Mechanics 9 thermomech_dK 204,316 2,846,228 no Mechanics 10 ASIC_320ks 321,671 1,316,08511 no Circuit Simulation 11 cage13 445,315 7,479,343 no Biology 12 atmosmodd 1,270,432 8,814,880 no Atmospheric Model In the following experiments we use the hardware system with NVIDIA C2050 (ECC on) GPU and Intel Core i7 CPU 950 @ 3.07GHz, using the 64-bit Linux operating system Ubuntu 10.04 LTS, cuSPARSE library 4.0 and MKL 10.2.3.029. The MKL_NUM_THREADS and MKL_DYNAMIC environment variables are left unset to allow MKL to use the optimal number of threads.'},\n",
       " {'id': 988,\n",
       "  'content': 'We compute the incomplete-LU and Cholesky factorizations using the MKL routines csrilu0 and csrilut with 0 and threshold fill-in, respectively. In the csrilut routine we allow three different levels of fill-in denoted by (5,10 -3 ), (10,10 -5 ) and (20,10 -7 ). In general, the \\\\(\\\\left( k,\\\\mathit{tol} \\\\right)\\\\) fill-in is based on \\\\(nnz/n + k\\\\) maximum allowed number of elements per row and the dropping of elements with magnitude \\\\(\\\\left| l_{ij} \\\\middle| , \\\\middle| u_{ij} \\\\middle| < \\\\mathit{tol} \\\\times \\\\left\\\\| \\\\mathbf{a}_{i}^{T} \\\\right\\\\|_{2} \\\\right.\\\\) , where \\\\(l_{ij}\\\\) , \\\\(u_{ij}\\\\) and \\\\(\\\\mathbf{a}_{i}^{T}\\\\) are the elements of the lower \\\\(L\\\\) , upper \\\\(U\\\\) triangular factors and the i -th row of the coefficient matrix \\\\(A\\\\) , respectively. We compare the implementation of the BiCGStab and CG iterative methods using the cuSPARSE and cuBLAS libraries on the GPU and MKL on the CPU. In our experiments we let the initial guess be zero, the right-hand-side \\\\(\\\\mathbf{f} = A\\\\mathbf{e}\\\\) where \\\\(\\\\mathbf{e}^{T}{= (1,\\\\ldots,1)}^{T}\\\\) , and the stopping criteria be the maximum number of iterations 2000 or relative residual \\\\(\\\\left\\\\| \\\\mathbf{r}_{i} \\\\right\\\\|_{2}/\\\\left\\\\| \\\\mathbf{r}_{0} \\\\right\\\\|_{2} < 10^{- 7}\\\\) , where \\\\(\\\\mathbf{r}_{i} = \\\\mathbf{f} - A\\\\mathbf{x}_{i}\\\\) is the residual at i -th iteration. Table 2. csrilu0 Preconditioned CG and BiCGStab Methods \\uf0c1 ilu0 CPU GPU Speedup # fact. time(s) copy time(s) solve time(s) \\\\(\\\\frac{\\\\left\\\\| \\\\mathbf{r}_{i} \\\\right\\\\|_{2}}{\\\\left\\\\| \\\\mathbf{r}_{0} \\\\right\\\\|_{2}}\\\\) # it. solve time(s) \\\\(\\\\frac{\\\\left\\\\| \\\\mathbf{r}_{i} \\\\right\\\\|_{2}}{\\\\left\\\\| \\\\mathbf{r}_{0} \\\\right\\\\|_{2}}\\\\) # it. vs. ilu0 1 0.38 0.02 0.72 8.83E-08 25 1.52 8.83E-08 25 0.57 2 1.62 0.04 38.5 1.00E-07 569 33.9 9.69E-08 571 1.13 3 0.13 0.01 39.2 9.84E-08 1044 6.91 9.84E-08 1044 5.59 4 0.12 0.01 35.0 9.97E-08 713 12.8 9.97E-08 713 2.72 5 0.09 0.01 107 9.98E-08 1746 55.3 9.98E-08 1746 1.92 6 0.40 0.02 155 9.96E-08 1656 54.4 9.79E-08 1656 2.83 7 0.16 0.02 20.2 8.70E-08 183 8.61 8.22E-08 183 2.32 8 0.32 0.02 0.13 5.25E-08 4 0.52 5.25E-08 4 0.53 9 0.20 0.01 72.7 1.96E-04 2000 40.0 2.08E-04 2000 1.80 10 0.11 0.01 0.27 6.33E-08 6 0.12 6.33E-08 6 1.59 11 0.70 0.03 0.28 2.52E-08 2.5 0.15 2.52E-08 2.5 1.10 12 0.25 0.04 12.5 7.33E-08 76.5 4.30 9.69E-08 74.5 2.79 Table 3. csrilut (5,10 -3 ) Preconditioned CG and BiCGStab Methods \\uf0c1 ilut(5,10 -3 ) CPU GPU Speedup # fact.'},\n",
       " {'id': 989, 'content': 'vs.'},\n",
       " {'id': 990,\n",
       "  'content': 'ilut (5,10 -3 ) vs. ilu0 1 0.14 0.01 1.17 9.70E-08 32 1.82 9.70E-08 32 0.67 0.69 2 0.51 0.03 49.1 9.89E-08 748 33.6 9.89E-08 748 1.45 1.39 3 1.47 0.02 11.7 9.72E-08 216 6.93 9.72E-08 216 1.56 1.86 4 0.17 0.01 67.9 9.96E-08 1495 26.5 9.96E-08 1495 2.56 5.27 5 0.55 0.04 59.5 9.22E-08 653 71.6 9.22E-08 653 0.83 1.08 6 3.59 0.05 47.0 9.50E-08 401 90.1 9.64E-08 401 0.54 0.92 7 1.24 0.05 23.1 8.08E-08 153 24.8 8.08E-08 153 0.93 2.77 8 0.82 0.03 0.12 3.97E-08 2 1.12 3.97E-08 2 0.48 1.10 9 0.10 0.01 54.3 5.68E-04 2000 24.5 1.58E-04 2000 2.21 1.34 10 0.12 0.01 0.16 4.89E-08 4 0.08 6.45E-08 4 1.37 1.15 11 4.99 0.07 0.36 1.40E-08 2.5 0.37 1.40E-08 2.5 0.99 6.05 12 0.32 0.03 39.2 7.05E-08 278.5 10.6 8.82E-08 270.5 3.60 8.60 The results of the numerical experiments are shown in Table 2 through Table 5 , where we state the speedup obtained by the iterative method on the GPU over CPU (speedup), number of iterations required for convergence (# it. ), achieved relative residual ( \\\\(\\\\frac{\\\\left\\\\| \\\\mathbf{r}_{i} \\\\right\\\\|_{2}}{\\\\left\\\\| \\\\mathbf{r}_{0} \\\\right\\\\|_{2}}\\\\) ) and time in seconds taken by the factorization (fact.'},\n",
       " {'id': 991,\n",
       "  'content': '), iterative solution of the linear system (solve), and cudaMemcpy of the lower and upper triangular factors to the GPU (copy). We include the time taken to compute the incomplete-LU and Cholesky factorization as well as to transfer the triangular factors from the CPU to the GPU memory in the computed speedup. Table 4. csrilut (10,10 -5 ) Preconditioned CG and BiCGStab Methods \\uf0c1 ilut(10,10 -5 ) CPU GPU Speedup # fact. vs.'},\n",
       " {'id': 992,\n",
       "  'content': 'ilut (10,10 -5 ) vs. ilu0 1 0.15 0.01 1.06 8.79E-08 34 1.96 8.79E-08 34 0.57 0.63 2 0.52 0.03 60.0 9.86E-08 748 38.7 9.86E-08 748 1.54 1.70 3 3.89 0.03 9.02 9.79E-08 147 5.42 9.78E-08 147 1.38 1.83 4 1.09 0.03 34.5 9.83E-08 454 38.2 9.83E-08 454 0.91 2.76 5 3.25 0.06 26.3 9.71E-08 272 55.2 9.71E-08 272 0.51 0.53 6 11.0 0.07 44.7 9.42E-08 263 84.0 9.44E-08 263 0.59 1.02 7 5.95 0.09 8.84 8.53E-08 43 17.0 8.53E-08 43 0.64 1.68 8 2.94 0.04 0.09 2.10E-08 1.5 1.75 2.10E-08 1.5 0.64 3.54 9 0.11 0.01 53.2 4.24E-03 2000 24.4 4.92E-03 2000 2.18 1.31 10 0.12 0.01 0.16 4.89E-11 4 0.08 6.45E-11 4 1.36 1.18 11 2.89 0.09 0.44 6.10E-09 2.5 0.48 6.10E-09 2.5 1.00 33.2 12 0.36 0.03 36.6 7.05E-08 278.5 10.6 8.82E-08 270.5 3.35 8.04 Table 5. csrilut (20,10 -7 ) Preconditioned CG and BiCGStab Methods \\uf0c1 ilut(20,10 -7 ) CPU GPU Speedup # fact.'},\n",
       " {'id': 993, 'content': 'vs.'},\n",
       " {'id': 994,\n",
       "  'content': 'ilut (20,10 -7 ) vs. ilu0 1 0.82 0.02 47.6 9.90E-08 1297 159 9.86E-08 1292 0.30 25.2 2 9.21 0.11 32.1 8.69E-08 193 84.6 8.67E-08 193 0.44 1.16 3 10.04 0.04 6.26 9.64E-08 90 4.75 9.64E-08 90 1.10 2.36 4 8.12 0.10 15.7 9.02E-08 148 22.5 9.02E-08 148 0.78 1.84 5 8.60 0.10 21.2 9.52E-08 158 53.6 9.52E-08 158 0.48 0.54 6 35.2 0.11 29.2 9.88E-08 162 80.5 9.88E-08 162 0.56 1.18 7 23.1 0.14 3.79 7.50E-08 14 12.1 7.50E-08 14 0.76 3.06 8 5.23 0.05 0.14 1.19E-09 1.5 2.37 1.19E-09 1.5 0.70 6.28 9 0.12 0.01 55.1 3.91E-03 2000 24.4 2.27E-03 2000 2.25 1.36 10 0.14 0.01 0.14 9.35E-08 3.5 0.07 7.19E-08 3.5 1.28 1.18 11 218 0.12 0.43 9.80E-08 2 0.66 9.80E-08 2 1.00 12 15.0 0.21 12.2 3.45E-08 31 4.95 3.45E-08 31 1.35 5.93 The summary of performance of BiCGStab and CG iterative methods preconditioned with different incomplete factorizations on the GPU is shown in Figure 3 , where “*” indicates that the method did not converge to the required tolerance. Notice that in general in our numerical experiments the performance for the incomplete factorizations decreases as the threshold parameters are relaxed and the factorization becomes more dense, thus inhibiting parallelism due to data dependencies between rows in the sparse triangular solve.'},\n",
       " {'id': 995,\n",
       "  'content': 'For this reason, the best performance on the GPU is obtained for the incomplete-LU and Cholesky factorization with 0 fill-in, which will be our point of reference. Performance of BiCGStab and CG with Incomplete-LU Cholesky Preconditioning \\uf0c1 Although the incomplete factorizations with a more relaxed threshold are often closer to the exact factorization and thus result in fewer iterative steps, they are also much more expensive to compute. Moreover, notice that even though the number of iterative steps decreases, each step is more computationally expensive. As a result of these tradeoffs the total time, the sum of the time taken by the factorization and the iterative solve, for the iterative method does not necessarily decrease with a more relaxed threshold in our numerical experiments. The speedup based on the total time taken by the preconditioned iterative method on the GPU with csrilu0 preconditioner and CPU with all four preconditioners is shown in Figure 4 . Notice that for majority of matrices in our numerical experiments the implementation of the iterative method using the cuSPARSE and cuBLAS libraries does indeed outperform the MKL. Speedup of prec. BiCGStab and CG on GPU (with csrilu0 ) vs. CPU (with all) \\uf0c1 Finally, the average of the obtained speedups is shown in Figure 5 , where we have excluded the runs with cage13 matrix for ilut (10,10 -5 ) and runs with offshore and cage13 matrices for ilut (20,10 -7 ) incomplete factorizations because of their disproportional speedup. However, the speedup including these runs is shown in parenthesis on the same plot. Consequently, we can conclude that the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods obtain on average more than 2x speedup on the GPU over their CPU implementation. Average Speedup of BiCGStab and CG on GPU (with csrilu0 ) and CPU (with all) \\uf0c1 4. Conclusion \\uf0c1 The performance of the iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand. In our numerical experiments the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods implemented on the GPU using the cuSPARSE and cuBLAS libraries achieved an average of 2x speedup over their MKL implementation. The sparse matrix-vector multiplication and triangular solve, which is split into a slower analysis phase that needs to be performed only once and a faster solve phase that can be performed multiple times, were the essential building blocks of these iterative methods. In fact the obtained speedup was usually mostly influenced by the time taken by the solve phase of the algorithm. Finally, we point out that the use of multiple-right-hand-sides would increase the available parallelism and can result in a significant relative performance improvement in the preconditioned iterative methods. Also, the development of incomplete-LU and Cholesky factorizations using CUDA parallel programming paradigm can further improve the obtained speedup. 5.'},\n",
       " {'id': 996,\n",
       "  'content': 'Acknowledgements \\uf0c1 This white paper was authored by Maxim Naumov for NVIDIA Corporation. Permission to make digital or hard copies of all or part of this work for any use is granted without fee provided that copies bear this notice and the full citation on the first page.'},\n",
       " {'id': 997, 'content': '6.'},\n",
       " {'id': 998, 'content': 'References \\uf0c1 [1] E.'},\n",
       " {'id': 999,\n",
       "  'content': 'Anderson and Y. Saad Solving Sparse Triangular Linear Systems on Parallel Computers, Int. J.'},\n",
       " {'id': 1000, 'content': 'High Speed Comput., pp. 73-95, 1989.'},\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
