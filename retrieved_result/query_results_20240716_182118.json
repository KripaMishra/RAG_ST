{
    "timestamp": "2024-07-16T18:20:53.834550",
    "original_query": "How do I install the Toolkit in a different location?",
    "expanded_queries": [
        "How do I install the Toolkit in a different location? for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords",
        "Toolkit for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords",
        "the Toolkit for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords",
        "Toolskit for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords for: How do I generate keywords"
    ],
    "results": [
        {
            "document": "Frequently Asked Questions 15.1. How do I install the Toolkit in a different location? 15.2. Why do I see “nvcc: No such file or directory” when I try to build a CUDA application? 15.3. Why do I see “error while loading shared libraries: : cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library? 15.4. Why do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu? 15.5. How can I tell X to ignore a GPU for compute-only use? 15.6. Why doesn’t the cuda-repo package install the CUDA Toolkit and Drivers? 15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04? 15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update? 15.9. How do I install a CUDA driver with a version less than 367 using a network repo? 15.10. How do I install an older CUDA version using a network repo? 15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency? 15.12. How do I handle “Errors were encountered while processing: glx-diversions”? 16.",
            "score": 40.4514
        },
        {
            "document": "If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf , may need to be modified. In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system. For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file. Consult the xorg.conf documentation for more information. Note Installing Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries. Reboot the system to reload the graphical interface: sudo reboot Verify the device nodes are created properly. Perform the post-installation actions . 8.3. Disabling Nouveau  To install the Display Driver, the Nouveau drivers must first be disabled. Each distribution of Linux has a different method for disabling Nouveau. The Nouveau drivers are loaded if the following command prints anything: lsmod | grep nouveau 8.3.1. Fedora  Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the following command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system. 8.3.2. RHEL / Rocky and KylinOS  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force 8.3.3. OpenSUSE  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd 8.3.4. SLES  No actions to disable Nouveau are required as Nouveau is not installed on SLES. 8.3.5. WSL  No actions to disable Nouveau are required as Nouveau is not installed on WSL. 8.3.6. Ubuntu  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.3.7. Debian  Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.4. Device Node Verification  Check that the device files /dev/nvidia* exist and have the correct (0666) file permissions. These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver. Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuid nvidia-modprobe tool that is bundled with the NVIDIA Driver. However, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below: #!/bin/bash /sbin/modprobe nvidia if [ \"$?\"\n-eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\"\n-eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi 8.5. Advanced Options  Action Options Used Explanation Silent Installation --silent Required for any silent installation.",
            "score": 39.97835159301758
        },
        {
            "document": " The Runfile installation asks where you wish to install the Toolkit during an interactive install. If installing using a non-interactive install, you can use the --toolkitpath parameter to change the install location: ./runfile.run --silent \\ --toolkit --toolkitpath=/my/new/toolkit The RPM and Deb packages cannot be installed to a custom install location directly using the package managers. See the “Install CUDA to a specific directory using the Package Manager installation method” scenario in the Advanced Setup section for more information.  Your PATH environment variable is not set up correctly. Ensure that your PATH includes the bin directory where you installed the Toolkit, usually /usr/local/cuda-12.4/bin . export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} 15.3.  Your LD_LIBRARY_PATH environment variable is not set up correctly. Ensure that your LD_LIBRARY_PATH includes the lib and/or lib64 directory where you installed the Toolkit, usually /usr/local/cuda-12.4/lib{,64} : export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 15.4.  These errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the system’s sources.list file. Repositories that do not host packages for the newly added architecture will present this error. While noisy, the error itself does no harm. Please see the Advanced Setup section for details on how to modify your sources.list file to prevent these errors.  To make sure X doesn’t use a certain GPU for display, you need to specify which other GPU to use for display. For more information, please refer to the “Use a specific GPU for rendering the display” scenario in the Advanced Setup section.  When using RPM or Deb, the downloaded package is a repository package.",
            "score": 25.047205
        },
        {
            "document": "Variables marked __constant__ may not be marked as __managed__ . A valid CUDA context is necessary for the correct operation of __managed__ variables. Accessing __managed__ variables can trigger CUDA context creation if a context for the current device hasn’t already been created. In the example above, accessing x before the kernel launch triggers context creation on device 0. In the absence of that access, the kernel launch would have triggered context creation. C++ objects declared as __managed__ are subject to certain specific constraints, particularly where static initializers are concerned. Please refer to C++ Language Support for a list of these constraints. Note For devices with CUDA Managed Memory without full support , visibility of __managed__ variables for asynchronous operations executing in CUDA streams is discussed in the section on Managing Data Visibility and Concurrent CPU + GPU Access with Streams . 19.1.2.4. Difference between Unified Memory and Mapped Memory  The main difference between Unified Memory and CUDA Mapped Memory is that CUDA Mapped Memory does not guarantee that all kinds of memory accesses (for example atomics) are supported on all systems, while Unified Memory does. The limited set of memory operations that are guaranteed to be portably supported by CUDA Mapped Memory is available on more systems than Unified Memory. 19.1.2.5. Pointer Attributes  CUDA Programs may check whether a pointer addresses a CUDA Managed Memory allocation by calling cudaPointerGetAttributes() and testing whether the pointer attribute value is cudaMemoryTypeManaged . This API returns cudaMemoryTypeHost for system-allocated memory that has been registered with cudaHostRegister() and cudaMemoryTypeUnregistered for system-allocated memory that CUDA is unaware of. Pointer attributes do not state where the memory resides, they state how the memory was allocated or registered. The following example shows how to detect the type of pointer at runtime: char const * kind ( cudaPointerAttributes a , bool pma , bool cma ) { switch ( a . type ) { case cudaMemoryTypeHost : return pma ? \"Unified: CUDA Host or Registered Memory\" : \"Not Unified: CUDA Host or Registered Memory\" ; case cudaMemoryTypeDevice : return \"Not Unified: CUDA Device Memory\" ; case cudaMemoryTypeManaged : return cma ? \"Unified: CUDA Managed Memory\" : \"Not Unified: CUDA Managed Memory\" ; case cudaMemoryTypeUnregistered : return pma ? \"Unified: System-Allocated Memory\" : \"Not Unified: System-Allocated Memory\" ; default : return \"unknown\" ; } } void check_pointer ( int i , void * ptr ) { cudaPointerAttributes attr ; cudaPointerGetAttributes ( & attr , ptr ); int pma = 0 , cma = 0 , device = 0 ; cudaGetDevice ( & device ); cudaDeviceGetAttribute ( & pma , cudaDevAttrPageableMemoryAccess , device ); cudaDeviceGetAttribute ( & cma , cudaDevAttrConcurrentManagedAccess , device ); printf ( \"Pointer %d: memory is %s \\n \" , i , kind ( attr , pma , cma )); } __managed__ int managed_var = 5 ; int main () { int * ptr [ 5 ]; ptr [ 0 ] = ( int * ) malloc ( sizeof ( int )); cudaMallocManaged ( & ptr [ 1 ], sizeof ( int )); cudaMallocHost ( & ptr [ 2 ], sizeof ( int )); cudaMalloc ( & ptr [ 3 ], sizeof ( int )); ptr [ 4 ] = & managed_var ; for ( int i = 0 ; i >> ( data , N ); // execute on GPU cudaMemPrefetchAsync ( data , N , cudaCpuDeviceId , s ); // prefetch to CPU cudaStreamSynchronize ( s ); use_data ( data , N ); free ( data ); } Managed void test_prefetch_managed ( cudaStream_t s ) { char * data ; cudaMallocManaged ( & data , N ); init_data ( data , N ); // execute on CPU cudaMemPrefetchAsync ( data , N , myGpuId , s ); // prefetch to GPU mykernel >> ( data , N ); // execute on GPU cudaMemPrefetchAsync ( data , N , cudaCpuDeviceId , s ); // prefetch to CPU cudaStreamSynchronize ( s ); use_data ( data , N ); cudaFree ( data ); } 19.1.2.8.2. Data Usage Hints  When multiple processors simultaneously access the same data, cudaMemAdvise may be used to hint how the data at [devPtr, devPtr + count) will be accessed: cudaError_t cudaMemAdvise ( const void * devPtr , size_t count , enum cudaMemoryAdvise advice , int device ); Where advice may take the following values: cudaMemAdviseSetReadMostly : This implies that the data is mostly going to be read from and only occasionally written to. In general, it allows trading off read bandwidth for write bandwidth on this region. Example: void test_advise_managed ( cudaStream_t stream ) { char * dataPtr ; size_t dataSize = 64 * TPB ; // 16 KiB // Allocate memory using cudaMallocManaged // (malloc may be used on systems with full CUDA Unified memory support) cudaMallocManaged ( & dataPtr , dataSize ); // Set the advice on the memory region cudaMemAdvise ( dataPtr , dataSize , cudaMemAdviseSetReadMostly , myGpuId ); int outerLoopIter = 0 ; while ( outerLoopIter >> (( const char * ) dataPtr , dataSize ); innerLoopIter ++ ; } outerLoopIter ++ ; } cudaFree ( dataPtr ); } cudaMemAdviseSetPreferredLocation : In general, any memory may be migrated at any time to any location, for example, when a given processor is running out of physical memory. This hint tells the system that migrating this memory region away from its preferred location is undesired, by setting the preferred location for the data to be the physical memory belonging to device. Passing in a value of cudaCpuDeviceId for device sets the preferred location as CPU memory. Other hints, like cudaMemPrefetchAsync , may override this hint, leading the memory to be migrated away from its preferred location. cudaMemAdviseSetAccessedBy : In some systems, it may be beneficial for performance to establish a mapping into memory before accessing the data from a given processor. This hint tells the system that the data will be frequently accessed by device , enabling the system to assume that creating these mappings pays off. This hint does not imply where the data should reside, but it can be combined with cudaMemAdviseSetPreferredLocation to specify that. Each advice can be also unset by using one of the following values: cudaMemAdviseUnsetReadMostly , cudaMemAdviseUnsetPreferredLocation and cudaMemAdviseUnsetAccessedBy .",
            "score": 22.834575
        },
        {
            "document": "This option is also ignored for entry functions that have .maxntid directive specified. --minnctapersm ( -minnctapersm ) Specify the minimum number of CTAs to be mapped to an SM. This option is also ignored for entry functions that have .minnctapersm directive specified. --override-directive-values ( -override-directive-values ) Override the PTX directives values by the corresponding option values. This option is effective only for -minnctapersm , -maxntid and -maxregcount options. --make-errors-visible-at-exit ( -make-errors-visible-at-exit ) Generate required instructions at exit point to make memory faults and errors visible at exit. 6. Basic Usage  This section of the document uses a simple example, Vector Addition , shown in Figure 1 to explain how to use PTX Compiler APIs to compile this PTX program. For brevity and readability, error checks on the API return values are not shown. Figure 1. PTX source string for a simple vector addition const char *ptxCode = \" \\n \\ .version 7.0 \\n \\ .target sm_50 \\n \\ .address_size 64 \\n \\ .visible .entry simpleVectorAdd( \\n \\ .param .u64 simpleVectorAdd_param_0, \\n \\ .param .u64 simpleVectorAdd_param_1, \\n \\ .param .u64 simpleVectorAdd_param_2 \\n \\ ) { \\n \\ .reg .f32 %f; \\n \\ .reg .b32 %r; \\n \\ .reg .b64 %rd; \\n \\ ld.param.u64 %rd1, [simpleVectorAdd_param_0]; \\n \\ ld.param.u64 %rd2, [simpleVectorAdd_param_1]; \\n \\ ld.param.u64 %rd3, [simpleVectorAdd_param_2]; \\n \\ cvta.to.global.u64 %rd4, %rd3; \\n \\ cvta.to.global.u64 %rd5, %rd2; \\n \\ cvta.to.global.u64 %rd6, %rd1; \\n \\ mov.u32 %r1, %ctaid.x; \\n \\ mov.u32 %r2, %ntid.x; \\n \\ mov.u32 %r3, %tid.x; \\n \\ mad.lo.s32 %r4, %r2, %r1, %r3; \\n \\ mul.wide.u32 %rd7, %r4, 4; \\n \\ add.s64 %rd8, %rd6, %rd7; \\n \\ ld.global.f32 %f1, [%rd8]; \\n \\ add.s64 %rd9, %rd5, %rd7; \\n \\ ld.global.f32 %f2, [%rd9]; \\n \\ add.f32 %f3, %f1, %f2; \\n \\ add.s64 %rd10, %rd4, %rd7; \\n \\ st.global.f32 [%rd10], %f3; \\n \\ ret; \\n \\ } \"; The CUDA code corresponding to this PTX program would look like: Figure 2. Equivalent CUDA source for the simple vector addition extern \"C\" __global__ void simpleVectorAdd(float *x, float *y, float *out) { size_t tid = blockIdx.x * blockDim.x + threadIdx.x; out[tid] = x[tid] + y[tid]; } With this PTX program as a string, we can create the compiler and obtain a handle to it as shown in Figure 3 . Figure 3. Compiler creation and initialization of a program nvPTXCompilerHandle compiler; nvPTXCompilerCreate(&compiler, (size_t)strlen(ptxCode), ptxCode); Compilation can now be done by specifying the compile options as shown in Figure 4 . Figure 4. Compilation of the PTX program const char* compile_options[] = { \"--gpu-name=sm_70\", \"--verbose\" }; nvPTXCompilerCompile(compiler, 2, compile_options); The compiled GPU assembly code can now be obtained. To obtain this we first allocate memory for it. And to allocate memory, we need to query the size of the image of the compiled GPU assembly code which is done as shown in Figure 5 . Figure 5. Query size of the compiled assembly image nvPTXCompilerGetCompiledProgramSize(compiler, &elfSize); The image of the compiled GPU assembly code can now be queried as shown in Figure 6 . This image can then be executed on the GPU by passing this image to the CUDA Driver APIs. Figure 6. Query the compiled assembly image elf = (char*) malloc(elfSize); nvPTXCompilerGetCompiledProgram(compiler, (void*)elf); When the compiler is not needed anymore, it can be destroyed as shown in Figure 7 . Figure 7. Destroy the compiler nvPTXCompilerDestroy(&compiler); 7. Example: Simple Vector Addition  Code (simpleVectorAddition.c) #include #include #include \"cuda.h\" #include \"nvPTXCompiler.h\" #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define SIZE NUM_THREADS * NUM_BLOCKS #define CUDA_SAFE_CALL(x) \\ do { \\ CUresult result = x; \\ if (result != CUDA_SUCCESS) { \\ const char *msg; \\ cuGetErrorName(result, &msg); \\ printf(\"error: %s failed with error %s\\n\", #x, msg); \\ exit(1); \\ } \\ } while(0) #define NVPTXCOMPILER_SAFE_CALL(x) \\ do { \\ nvPTXCompileResult result = x; \\ if (result != NVPTXCOMPILE_SUCCESS) { \\ printf(\"error: %s failed with error code %d\\n\", #x, result); \\ exit(1); \\ } \\ } while(0) const char *ptxCode = \" \\ .version 7.0 \\n \\ .target sm_50 \\n \\ .address_size 64 \\n \\ .visible .entry simpleVectorAdd( \\n \\ .param .u64 simpleVectorAdd_param_0, \\n \\ .param .u64 simpleVectorAdd_param_1, \\n \\ .param .u64 simpleVectorAdd_param_2 \\n \\ ) { \\n \\ .reg .f32 %f; \\n \\ .reg .b32 %r; \\n \\ .reg .b64 %rd; \\n \\ ld.param.u64 %rd1, [simpleVectorAdd_param_0]; \\n \\ ld.param.u64 %rd2, [simpleVectorAdd_param_1]; \\n \\ ld.param.u64 %rd3, [simpleVectorAdd_param_2]; \\n \\ cvta.to.global.u64 %rd4, %rd3; \\n \\ cvta.to.global.u64 %rd5, %rd2; \\n \\ cvta.to.global.u64 %rd6, %rd1; \\n \\ mov.u32 %r1, %ctaid.x; \\n \\ mov.u32 %r2, %ntid.x; \\n \\ mov.u32 %r3, %tid.x; \\n \\ mad.lo.s32 %r4, %r2, %r1, %r3; \\n \\ mul.wide.u32 %rd7, %r4, 4; \\n \\ add.s64 %rd8, %rd6, %rd7; \\n \\ ld.global.f32 %f1, [%rd8]; \\n \\ add.s64 %rd9, %rd5, %rd7; \\n \\ ld.global.f32 %f2, [%rd9]; \\n \\ add.f32 %f3, %f1, %f2; \\n \\ add.s64 %rd10, %rd4, %rd7; \\n \\ st.global.f32 [%rd10], %f3; \\n \\ ret; \\n \\ } \"; int elfLoadAndKernelLaunch(void* elf, size_t elfSize) { CUdevice cuDevice; CUcontext context; CUmodule module; CUfunction kernel; CUdeviceptr dX, dY, dOut; size_t i; size_t bufferSize = SIZE * sizeof(float); float a; float hX[SIZE], hY[SIZE], hOut[SIZE]; void* args[3]; CUDA_SAFE_CALL(cuInit(0)); CUDA_SAFE_CALL(cuDeviceGet(&cuDevice, 0)); CUDA_SAFE_CALL(cuCtxCreate(&context, 0, cuDevice)); CUDA_SAFE_CALL(cuModuleLoadDataEx(&module, elf, 0, 0, 0)); CUDA_SAFE_CALL(cuModuleGetFunction(&kernel, module, \"simpleVectorAdd\")); // Generate input for execution, and create output buffers. for (i = 0; i -I $CUDA_PATH/include -L $CUDA_PATH/lib/x64/ -lcuda nvptxcompiler_static.lib Linux: gcc simpleVectorAddition.c -o simpleVectorAddition \\ -I $CUDA_PATH/include \\ -L $CUDA_PATH/lib64 \\ libnvptxcompiler_static.a -lcuda -lm -lpthread \\ -Wl,-rpath,$CUDA_PATH/lib64 7.2. Notices  7.2.1.",
            "score": 22.3944225
        }
    ],
    "answer": "Original query: How do I install the Toolkit in a different location?\nRetrieved documents:\n- Frequently Asked Questions 15.1. How do I install the Toolkit in a different location? 15.2. Why do I see “nvcc: No such file or directory” when I try to build a CUDA application? 15.3. Why do I see “error while loading shared libraries: : cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library? 15.4. Why do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu? 15.5. How can I tell X to ignore a GPU for compute-only use? 15.6. Why doesn’t the cuda-repo package install the CUDA Toolkit and Drivers? 15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04? 15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update? 15.9. How do I install a CUDA driver with a version less than 367 using a network repo? 15.10. How do I install an older CUDA version using a network repo? 15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency? 15.12. How do I handle “Errors were encountered while processing: glx-diversions”? 16.\n- If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf, may need to be modified. In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system. For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file. Consult the xorg.conf documentation for more information. Note Installing Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries. Reboot the system to reload the graphical interface: sudo reboot Verify the device nodes are created properly. Perform the post-installation actions. 8.3. Can I use a different Xserver for CUDA? 8.4. Can I use a different Xserver for CUDA? 8.5. Will CUDA work on a system with a Quadro™ or ATI™ graphics card? 8.6. Can I use a different Xserver for CUDA? 8.7. Can I use a different Xserver for CUDA? 8.8. Can I use a different Xserver for CUDA? 8.9. Can I use a different Xserver for CUDA? 8.10. Can I use a different Xserver for CUDA? 8.11. Can I use a different Xserver for CUDA? 8.12. Can I use a different Xserver for CUDA? 8.13. Can I use a different Xserver for CUDA? 8.14. Can I use a different Xserver for CUDA? 8.15. Can I use a different Xserver for CUDA? 8.16. Can I use a different Xserver for CUDA? 8.17. Can I use a different Xserver for CUDA? 8.18. Can I use a different Xserver for CUDA? 8.19. Can I use a different Xserver for CUDA? 8.20. Can I use a different Xserver for CUDA? 8.21. Can I use a different Xserver for CUDA? 8.22. Can I use a different Xserver for CUDA? 8.23. Can I use a different Xserver for CUDA? 8.24. Can I use a different Xserver for CUDA? 8.25. Can I use a different Xserver for CUDA? 8.26. Can I use a different Xserver for CUDA? 8.27. Can I use a different Xserver for CUDA? 8.28. Can I use a different Xserver for CUDA? 8.29. Can I use a different Xserver for CUDA? 8.30. Can I use a different Xserver for CUDA? 8.31. Can I use a different Xserver for CUDA? 8.32. Can I use a different Xserver for CUDA? 8.33. Can I use a different Xserver for CUDA? 8.34. Can I use a different Xserver for CUDA? 8.35. Can I use a different Xserver for CUDA? 8.36. Can I use a different Xserver for CUDA"
}