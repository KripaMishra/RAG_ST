{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/etc/mod\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the generator model and tokenizer\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def load_retrieved_data(file_path):\n",
    "    \"\"\"Load the retrieved data from a JSON file.\"\"\"\n",
    "    context=[]\n",
    "    with open(file_path, 'r') as f:\n",
    "        data= json.load(f)\n",
    "    for i in range(len(data['results'])):\n",
    "      context.append(data['results'][i]['document'])\n",
    "    return context\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    \"\"\"Generate an answer based on the given query and context.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert on the topics related to GPU and specifically CUDA and nvidia. Your task is to analyze the query and then try to use the provided context and your prior knowledge for answering the query. In case of a contradiction between your knowlege and the provided context, consider the context to be more accurate and then try to formulate your answer based on the new information. In say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = generator_model.generate(**inputs, max_length=150, num_return_sequences=1)\n",
    "    answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "def question_answering(query, file_path):\n",
    "    \"\"\"Perform question answering using the retrieved data.\"\"\"\n",
    "    context = load_retrieved_data(file_path)\n",
    "    answer = generate_answer(query, context)\n",
    "    return answer, context\n",
    "\n",
    "def save_query_results(query, answer, file_path=None):\n",
    "    \"\"\"Save the query results to a JSON file.\"\"\"\n",
    "    context=load_retrieved_data('/content/1.json')\n",
    "    retrieved_docs = context\n",
    "    query_data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_documents\": retrieved_docs\n",
    "    }\n",
    "\n",
    "    if file_path is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        file_path = f\"query_results_{timestamp}.json\"\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(query_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Query results saved to {file_path}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the retrieved data\n",
    "    # retrieved_data = load_retrieved_data('/content/1.json')\n",
    "\n",
    "    # while True:\n",
    "    #     query = input(\"Enter your question (or 'quit' to exit): \")\n",
    "    #     if query.lower() == 'quit':\n",
    "    #         break\n",
    "\n",
    "    #     answer, retrieved_docs = question_answering(query, '/content/1.json')\n",
    "\n",
    "    #     print(\"\\nAnswer:\", answer)\n",
    "    #     print(\"\\nRetrieved Documents:\")\n",
    "    #     for i, doc in enumerate(retrieved_docs, 1):\n",
    "    #         print(f\"{i}. {doc['content'][:100]}...\")  # Print first 100 characters of each document\n",
    "\n",
    "    #     save_query_results(query, answer, retrieved_docs)\n",
    "\n",
    "    #     print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    query=' How do I install the Toolkit in a different location? ? '\n",
    "    file_path='/home/ubuntu/project/Steps/retrieved_result/query_results_20240715_211004.json'\n",
    "    answer,context= question_answering(query,file_path)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/etc/mod'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frequently Asked Questions 15.1. How do I install the Toolkit in a different location? 15.2. Why do I see “nvcc: No such file or directory” when I try to build a CUDA application? 15.3. Why do I see “error while loading shared libraries: : cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library? 15.4. Why do I see multiple “404 Not Found” errors when updating my repository meta-data on Ubuntu? 15.5. How can I tell X to ignore a GPU for compute-only use? 15.6. Why doesn’t the cuda-repo package install the CUDA Toolkit and Drivers? 15.7. How do I get CUDA to work on a laptop with an iGPU and a dGPU running Ubuntu14.04? 15.8. What do I do if the display does not load, or CUDA does not work, after performing a system update? 15.9. How do I install a CUDA driver with a version less than 367 using a network repo? 15.10. How do I install an older CUDA version using a network repo? 15.11. Why does the installation on SUSE install the Mesa-dri-nouveau dependency? 15.12. How do I handle “Errors were encountered while processing: glx-diversions”? 16.',\n",
       " 'If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg.conf , may need to be modified. In some cases, nvidia-xconfig can be used to automatically generate an xorg.conf file that works for the system. For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg.conf file. Consult the xorg.conf documentation for more information. Note Installing Mesa may overwrite the /usr/lib/libGL.so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries. Reboot the system to reload the graphical interface: sudo reboot Verify the device nodes are created properly. Perform the post-installation actions . 8.3. Disabling Nouveau \\uf0c1 To install the Display Driver, the Nouveau drivers must first be disabled. Each distribution of Linux has a different method for disabling Nouveau. The Nouveau drivers are loaded if the following command prints anything: lsmod | grep nouveau 8.3.1. Fedora \\uf0c1 Create a file at /usr/lib/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the following command: sudo grub2-mkconfig -o /boot/grub2/grub.cfg Reboot the system. 8.3.2. RHEL / Rocky and KylinOS \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force 8.3.3. OpenSUSE \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd 8.3.4. SLES \\uf0c1 No actions to disable Nouveau are required as Nouveau is not installed on SLES. 8.3.5. WSL \\uf0c1 No actions to disable Nouveau are required as Nouveau is not installed on WSL. 8.3.6. Ubuntu \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.3.7. Debian \\uf0c1 Create a file at /etc/modprobe.d/blacklist-nouveau.conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8.4. Device Node Verification \\uf0c1 Check that the device files /dev/nvidia* exist and have the correct (0666) file permissions. These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver. Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuid nvidia-modprobe tool that is bundled with the NVIDIA Driver. However, some systems disallow setuid binaries, so if these files do not exist, you can create them manually by using a startup script such as the one below: #!/bin/bash /sbin/modprobe nvidia if [ \"$?\"\\n-eq 0 ]; then # Count the number of NVIDIA controllers found. NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$?\"\\n-eq 0 ]; then # Find out the major device number used by the nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk \\'{print $1}\\'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi 8.5. Advanced Options \\uf0c1 Action Options Used Explanation Silent Installation --silent Required for any silent installation.',\n",
       " '\\uf0c1 The Runfile installation asks where you wish to install the Toolkit during an interactive install. If installing using a non-interactive install, you can use the --toolkitpath parameter to change the install location: ./runfile.run --silent \\\\ --toolkit --toolkitpath=/my/new/toolkit The RPM and Deb packages cannot be installed to a custom install location directly using the package managers. See the “Install CUDA to a specific directory using the Package Manager installation method” scenario in the Advanced Setup section for more information. \\uf0c1 Your PATH environment variable is not set up correctly. Ensure that your PATH includes the bin directory where you installed the Toolkit, usually /usr/local/cuda-12.4/bin . export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}} 15.3. \\uf0c1 Your LD_LIBRARY_PATH environment variable is not set up correctly. Ensure that your LD_LIBRARY_PATH includes the lib and/or lib64 directory where you installed the Toolkit, usually /usr/local/cuda-12.4/lib{,64} : export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib\\\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 15.4. \\uf0c1 These errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the system’s sources.list file. Repositories that do not host packages for the newly added architecture will present this error. While noisy, the error itself does no harm. Please see the Advanced Setup section for details on how to modify your sources.list file to prevent these errors. \\uf0c1 To make sure X doesn’t use a certain GPU for display, you need to specify which other GPU to use for display. For more information, please refer to the “Use a specific GPU for rendering the display” scenario in the Advanced Setup section. \\uf0c1 When using RPM or Deb, the downloaded package is a repository package.',\n",
       " 'Variables marked __constant__ may not be marked as __managed__ . A valid CUDA context is necessary for the correct operation of __managed__ variables. Accessing __managed__ variables can trigger CUDA context creation if a context for the current device hasn’t already been created. In the example above, accessing x before the kernel launch triggers context creation on device 0. In the absence of that access, the kernel launch would have triggered context creation. C++ objects declared as __managed__ are subject to certain specific constraints, particularly where static initializers are concerned. Please refer to C++ Language Support for a list of these constraints. Note For devices with CUDA Managed Memory without full support , visibility of __managed__ variables for asynchronous operations executing in CUDA streams is discussed in the section on Managing Data Visibility and Concurrent CPU + GPU Access with Streams . 19.1.2.4. Difference between Unified Memory and Mapped Memory \\uf0c1 The main difference between Unified Memory and CUDA Mapped Memory is that CUDA Mapped Memory does not guarantee that all kinds of memory accesses (for example atomics) are supported on all systems, while Unified Memory does. The limited set of memory operations that are guaranteed to be portably supported by CUDA Mapped Memory is available on more systems than Unified Memory. 19.1.2.5. Pointer Attributes \\uf0c1 CUDA Programs may check whether a pointer addresses a CUDA Managed Memory allocation by calling cudaPointerGetAttributes() and testing whether the pointer attribute value is cudaMemoryTypeManaged . This API returns cudaMemoryTypeHost for system-allocated memory that has been registered with cudaHostRegister() and cudaMemoryTypeUnregistered for system-allocated memory that CUDA is unaware of. Pointer attributes do not state where the memory resides, they state how the memory was allocated or registered. The following example shows how to detect the type of pointer at runtime: char const * kind ( cudaPointerAttributes a , bool pma , bool cma ) { switch ( a . type ) { case cudaMemoryTypeHost : return pma ? \"Unified: CUDA Host or Registered Memory\" : \"Not Unified: CUDA Host or Registered Memory\" ; case cudaMemoryTypeDevice : return \"Not Unified: CUDA Device Memory\" ; case cudaMemoryTypeManaged : return cma ? \"Unified: CUDA Managed Memory\" : \"Not Unified: CUDA Managed Memory\" ; case cudaMemoryTypeUnregistered : return pma ? \"Unified: System-Allocated Memory\" : \"Not Unified: System-Allocated Memory\" ; default : return \"unknown\" ; } } void check_pointer ( int i , void * ptr ) { cudaPointerAttributes attr ; cudaPointerGetAttributes ( & attr , ptr ); int pma = 0 , cma = 0 , device = 0 ; cudaGetDevice ( & device ); cudaDeviceGetAttribute ( & pma , cudaDevAttrPageableMemoryAccess , device ); cudaDeviceGetAttribute ( & cma , cudaDevAttrConcurrentManagedAccess , device ); printf ( \"Pointer %d: memory is %s \\\\n \" , i , kind ( attr , pma , cma )); } __managed__ int managed_var = 5 ; int main () { int * ptr [ 5 ]; ptr [ 0 ] = ( int * ) malloc ( sizeof ( int )); cudaMallocManaged ( & ptr [ 1 ], sizeof ( int )); cudaMallocHost ( & ptr [ 2 ], sizeof ( int )); cudaMalloc ( & ptr [ 3 ], sizeof ( int )); ptr [ 4 ] = & managed_var ; for ( int i = 0 ; i >> ( data , N ); // execute on GPU cudaMemPrefetchAsync ( data , N , cudaCpuDeviceId , s ); // prefetch to CPU cudaStreamSynchronize ( s ); use_data ( data , N ); free ( data ); } Managed void test_prefetch_managed ( cudaStream_t s ) { char * data ; cudaMallocManaged ( & data , N ); init_data ( data , N ); // execute on CPU cudaMemPrefetchAsync ( data , N , myGpuId , s ); // prefetch to GPU mykernel >> ( data , N ); // execute on GPU cudaMemPrefetchAsync ( data , N , cudaCpuDeviceId , s ); // prefetch to CPU cudaStreamSynchronize ( s ); use_data ( data , N ); cudaFree ( data ); } 19.1.2.8.2. Data Usage Hints \\uf0c1 When multiple processors simultaneously access the same data, cudaMemAdvise may be used to hint how the data at [devPtr, devPtr + count) will be accessed: cudaError_t cudaMemAdvise ( const void * devPtr , size_t count , enum cudaMemoryAdvise advice , int device ); Where advice may take the following values: cudaMemAdviseSetReadMostly : This implies that the data is mostly going to be read from and only occasionally written to. In general, it allows trading off read bandwidth for write bandwidth on this region. Example: void test_advise_managed ( cudaStream_t stream ) { char * dataPtr ; size_t dataSize = 64 * TPB ; // 16 KiB // Allocate memory using cudaMallocManaged // (malloc may be used on systems with full CUDA Unified memory support) cudaMallocManaged ( & dataPtr , dataSize ); // Set the advice on the memory region cudaMemAdvise ( dataPtr , dataSize , cudaMemAdviseSetReadMostly , myGpuId ); int outerLoopIter = 0 ; while ( outerLoopIter >> (( const char * ) dataPtr , dataSize ); innerLoopIter ++ ; } outerLoopIter ++ ; } cudaFree ( dataPtr ); } cudaMemAdviseSetPreferredLocation : In general, any memory may be migrated at any time to any location, for example, when a given processor is running out of physical memory. This hint tells the system that migrating this memory region away from its preferred location is undesired, by setting the preferred location for the data to be the physical memory belonging to device. Passing in a value of cudaCpuDeviceId for device sets the preferred location as CPU memory. Other hints, like cudaMemPrefetchAsync , may override this hint, leading the memory to be migrated away from its preferred location. cudaMemAdviseSetAccessedBy : In some systems, it may be beneficial for performance to establish a mapping into memory before accessing the data from a given processor. This hint tells the system that the data will be frequently accessed by device , enabling the system to assume that creating these mappings pays off. This hint does not imply where the data should reside, but it can be combined with cudaMemAdviseSetPreferredLocation to specify that. Each advice can be also unset by using one of the following values: cudaMemAdviseUnsetReadMostly , cudaMemAdviseUnsetPreferredLocation and cudaMemAdviseUnsetAccessedBy .',\n",
       " 'This option is also ignored for entry functions that have .maxntid directive specified. --minnctapersm ( -minnctapersm ) Specify the minimum number of CTAs to be mapped to an SM. This option is also ignored for entry functions that have .minnctapersm directive specified. --override-directive-values ( -override-directive-values ) Override the PTX directives values by the corresponding option values. This option is effective only for -minnctapersm , -maxntid and -maxregcount options. --make-errors-visible-at-exit ( -make-errors-visible-at-exit ) Generate required instructions at exit point to make memory faults and errors visible at exit. 6. Basic Usage \\uf0c1 This section of the document uses a simple example, Vector Addition , shown in Figure 1 to explain how to use PTX Compiler APIs to compile this PTX program. For brevity and readability, error checks on the API return values are not shown. Figure 1. PTX source string for a simple vector addition const char *ptxCode = \" \\\\n \\\\ .version 7.0 \\\\n \\\\ .target sm_50 \\\\n \\\\ .address_size 64 \\\\n \\\\ .visible .entry simpleVectorAdd( \\\\n \\\\ .param .u64 simpleVectorAdd_param_0, \\\\n \\\\ .param .u64 simpleVectorAdd_param_1, \\\\n \\\\ .param .u64 simpleVectorAdd_param_2 \\\\n \\\\ ) { \\\\n \\\\ .reg .f32 %f; \\\\n \\\\ .reg .b32 %r; \\\\n \\\\ .reg .b64 %rd; \\\\n \\\\ ld.param.u64 %rd1, [simpleVectorAdd_param_0]; \\\\n \\\\ ld.param.u64 %rd2, [simpleVectorAdd_param_1]; \\\\n \\\\ ld.param.u64 %rd3, [simpleVectorAdd_param_2]; \\\\n \\\\ cvta.to.global.u64 %rd4, %rd3; \\\\n \\\\ cvta.to.global.u64 %rd5, %rd2; \\\\n \\\\ cvta.to.global.u64 %rd6, %rd1; \\\\n \\\\ mov.u32 %r1, %ctaid.x; \\\\n \\\\ mov.u32 %r2, %ntid.x; \\\\n \\\\ mov.u32 %r3, %tid.x; \\\\n \\\\ mad.lo.s32 %r4, %r2, %r1, %r3; \\\\n \\\\ mul.wide.u32 %rd7, %r4, 4; \\\\n \\\\ add.s64 %rd8, %rd6, %rd7; \\\\n \\\\ ld.global.f32 %f1, [%rd8]; \\\\n \\\\ add.s64 %rd9, %rd5, %rd7; \\\\n \\\\ ld.global.f32 %f2, [%rd9]; \\\\n \\\\ add.f32 %f3, %f1, %f2; \\\\n \\\\ add.s64 %rd10, %rd4, %rd7; \\\\n \\\\ st.global.f32 [%rd10], %f3; \\\\n \\\\ ret; \\\\n \\\\ } \"; The CUDA code corresponding to this PTX program would look like: Figure 2. Equivalent CUDA source for the simple vector addition extern \"C\" __global__ void simpleVectorAdd(float *x, float *y, float *out) { size_t tid = blockIdx.x * blockDim.x + threadIdx.x; out[tid] = x[tid] + y[tid]; } With this PTX program as a string, we can create the compiler and obtain a handle to it as shown in Figure 3 . Figure 3. Compiler creation and initialization of a program nvPTXCompilerHandle compiler; nvPTXCompilerCreate(&compiler, (size_t)strlen(ptxCode), ptxCode); Compilation can now be done by specifying the compile options as shown in Figure 4 . Figure 4. Compilation of the PTX program const char* compile_options[] = { \"--gpu-name=sm_70\", \"--verbose\" }; nvPTXCompilerCompile(compiler, 2, compile_options); The compiled GPU assembly code can now be obtained. To obtain this we first allocate memory for it. And to allocate memory, we need to query the size of the image of the compiled GPU assembly code which is done as shown in Figure 5 . Figure 5. Query size of the compiled assembly image nvPTXCompilerGetCompiledProgramSize(compiler, &elfSize); The image of the compiled GPU assembly code can now be queried as shown in Figure 6 . This image can then be executed on the GPU by passing this image to the CUDA Driver APIs. Figure 6. Query the compiled assembly image elf = (char*) malloc(elfSize); nvPTXCompilerGetCompiledProgram(compiler, (void*)elf); When the compiler is not needed anymore, it can be destroyed as shown in Figure 7 . Figure 7. Destroy the compiler nvPTXCompilerDestroy(&compiler); 7. Example: Simple Vector Addition \\uf0c1 Code (simpleVectorAddition.c) #include #include #include \"cuda.h\" #include \"nvPTXCompiler.h\" #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define SIZE NUM_THREADS * NUM_BLOCKS #define CUDA_SAFE_CALL(x) \\\\ do { \\\\ CUresult result = x; \\\\ if (result != CUDA_SUCCESS) { \\\\ const char *msg; \\\\ cuGetErrorName(result, &msg); \\\\ printf(\"error: %s failed with error %s\\\\n\", #x, msg); \\\\ exit(1); \\\\ } \\\\ } while(0) #define NVPTXCOMPILER_SAFE_CALL(x) \\\\ do { \\\\ nvPTXCompileResult result = x; \\\\ if (result != NVPTXCOMPILE_SUCCESS) { \\\\ printf(\"error: %s failed with error code %d\\\\n\", #x, result); \\\\ exit(1); \\\\ } \\\\ } while(0) const char *ptxCode = \" \\\\ .version 7.0 \\\\n \\\\ .target sm_50 \\\\n \\\\ .address_size 64 \\\\n \\\\ .visible .entry simpleVectorAdd( \\\\n \\\\ .param .u64 simpleVectorAdd_param_0, \\\\n \\\\ .param .u64 simpleVectorAdd_param_1, \\\\n \\\\ .param .u64 simpleVectorAdd_param_2 \\\\n \\\\ ) { \\\\n \\\\ .reg .f32 %f; \\\\n \\\\ .reg .b32 %r; \\\\n \\\\ .reg .b64 %rd; \\\\n \\\\ ld.param.u64 %rd1, [simpleVectorAdd_param_0]; \\\\n \\\\ ld.param.u64 %rd2, [simpleVectorAdd_param_1]; \\\\n \\\\ ld.param.u64 %rd3, [simpleVectorAdd_param_2]; \\\\n \\\\ cvta.to.global.u64 %rd4, %rd3; \\\\n \\\\ cvta.to.global.u64 %rd5, %rd2; \\\\n \\\\ cvta.to.global.u64 %rd6, %rd1; \\\\n \\\\ mov.u32 %r1, %ctaid.x; \\\\n \\\\ mov.u32 %r2, %ntid.x; \\\\n \\\\ mov.u32 %r3, %tid.x; \\\\n \\\\ mad.lo.s32 %r4, %r2, %r1, %r3; \\\\n \\\\ mul.wide.u32 %rd7, %r4, 4; \\\\n \\\\ add.s64 %rd8, %rd6, %rd7; \\\\n \\\\ ld.global.f32 %f1, [%rd8]; \\\\n \\\\ add.s64 %rd9, %rd5, %rd7; \\\\n \\\\ ld.global.f32 %f2, [%rd9]; \\\\n \\\\ add.f32 %f3, %f1, %f2; \\\\n \\\\ add.s64 %rd10, %rd4, %rd7; \\\\n \\\\ st.global.f32 [%rd10], %f3; \\\\n \\\\ ret; \\\\n \\\\ } \"; int elfLoadAndKernelLaunch(void* elf, size_t elfSize) { CUdevice cuDevice; CUcontext context; CUmodule module; CUfunction kernel; CUdeviceptr dX, dY, dOut; size_t i; size_t bufferSize = SIZE * sizeof(float); float a; float hX[SIZE], hY[SIZE], hOut[SIZE]; void* args[3]; CUDA_SAFE_CALL(cuInit(0)); CUDA_SAFE_CALL(cuDeviceGet(&cuDevice, 0)); CUDA_SAFE_CALL(cuCtxCreate(&context, 0, cuDevice)); CUDA_SAFE_CALL(cuModuleLoadDataEx(&module, elf, 0, 0, 0)); CUDA_SAFE_CALL(cuModuleGetFunction(&kernel, module, \"simpleVectorAdd\")); // Generate input for execution, and create output buffers. for (i = 0; i -I $CUDA_PATH/include -L $CUDA_PATH/lib/x64/ -lcuda nvptxcompiler_static.lib Linux: gcc simpleVectorAddition.c -o simpleVectorAddition \\\\ -I $CUDA_PATH/include \\\\ -L $CUDA_PATH/lib64 \\\\ libnvptxcompiler_static.a -lcuda -lm -lpthread \\\\ -Wl,-rpath,$CUDA_PATH/lib64 7.2. Notices \\uf0c1 7.2.1.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class CustomRetrieval:\n",
    "    def retrieve(self, query: str) -> Dict:\n",
    "        # Placeholder for your custom retrieval logic\n",
    "        # This should return the results in the format you provided\n",
    "        pass\n",
    "\n",
    "class QueryEnhancer:\n",
    "    def enhance(self, query: str) -> List[str]:\n",
    "        # Placeholder for your query enhancement logic\n",
    "        # This should return a list of expanded queries\n",
    "        pass\n",
    "\n",
    "class RAGModel:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.retriever = CustomRetrieval()\n",
    "        self.query_enhancer = QueryEnhancer()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def generate_answer(self, query: str) -> str:\n",
    "        # Perform custom retrieval\n",
    "        retrieval_results = self.retriever.retrieve(query)\n",
    "\n",
    "        # Enhance the query\n",
    "        expanded_queries = self.query_enhancer.enhance(query)\n",
    "\n",
    "        # Combine original query, expanded queries, and retrieved documents\n",
    "        context = f\"Original query: {query}\\n\"\n",
    "        context += f\"Expanded queries: {', '.join(expanded_queries)}\\n\"\n",
    "        context += \"Retrieved documents:\\n\"\n",
    "        for result in retrieval_results['results']:\n",
    "            context += f\"- {result['document']}\\n\"\n",
    "\n",
    "        # Generate answer using the language model\n",
    "        prompt = f\"{context}\\nBased on the above information, please answer the following question: {query}\"\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = self.model.generate(input_ids, max_length=200, num_return_sequences=1, temperature=0.7)\n",
    "        answer = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        retrieval_results = self.retriever.retrieve(query)\n",
    "        expanded_queries = self.query_enhancer.enhance(query)\n",
    "        answer = self.generate_answer(query)\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"original_query\": query,\n",
    "            \"expanded_queries\": expanded_queries,\n",
    "            \"results\": retrieval_results['results'],\n",
    "            \"answer\": answer\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"gpt2\"  # Replace with your preferred model\n",
    "    rag_model = RAGModel(model_name)\n",
    "    \n",
    "    query = \"How do I install the Toolkit in a different location?\"\n",
    "    result = rag_model.process_query(query)\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections: ['example_collection', 'documents']\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, Collection, utility\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# List collections\n",
    "collections = utility.list_collections()\n",
    "print(\"Available collections:\", collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection information: {'auto_id': False, 'description': 'documents', 'fields': [{'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}, {'name': 'content', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}], 'enable_dynamic_field': False}\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"documents\"\n",
    "collection = Collection(collection_name)\n",
    "\n",
    "# Get collection information\n",
    "print(\"Collection information:\", collection.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    expr=\"id >= 0\",  # Query condition, this will return all entities\n",
    "    output_fields=[\"id\"],  # Specify the fields you want to retrieve\n",
    "    limit=10  # Limit the number of results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [\"{'id': 1}\", \"{'id': 2}\", \"{'id': 3}\", \"{'id': 4}\", \"{'id': 5}\", \"{'id': 6}\", \"{'id': 7}\", \"{'id': 8}\", \"{'id': 9}\", \"{'id': 10}\"] , extra_info: {'cost': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stepsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
