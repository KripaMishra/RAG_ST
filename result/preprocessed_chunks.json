[
  {
    "id": 1,
    "content": "With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application Using built-in capabilities for"
  },
  {
    "id": 2,
    "content": "distributing computations across multi-GPU configurations, scientists and researchers can develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs EULA The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated"
  },
  {
    "id": 4,
    "content": "If you do not agree with the terms and conditions of the license agreement, then do not download or use the software"
  },
  {
    "id": 5,
    "content": "Installation Guides  Quick Start Guide This guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system Installation Guide Windows This guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems Installation Guide Linux This guide discusses how to install and check for correct"
  },
  {
    "id": 6,
    "content": "operation of the CUDA Development Tools on GNU/Linux systems Programming Guides  Programming Guide This guide provides a detailed discussion of the CUDA programming model and programming interface"
  },
  {
    "id": 7,
    "content": "It then describes the hardware implementation, and provides guidance on how to achieve maximum performance"
  },
  {
    "id": 8,
    "content": "The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API"
  },
  {
    "id": 9,
    "content": "Best Practices Guide This guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures"
  },
  {
    "id": 10,
    "content": "The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit"
  },
  {
    "id": 11,
    "content": "Maxwell Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture This document provides guidance to ensure that your software applications are compatible with Maxwell Pascal Compatibility Guide This application note is intended to help developers ensure that their NVIDIA"
  },
  {
    "id": 12,
    "content": "CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture This document provides guidance to ensure that your software applications are compatible with Pascal Volta Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture This document provides"
  },
  {
    "id": 13,
    "content": "guidance to ensure that your software applications are compatible with Volta Turing Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture This document provides guidance to ensure that your software applications are compatible with Turing NVIDIA Ampere GPU Architecture"
  },
  {
    "id": 14,
    "content": "Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture Hopper Compatibility Guide This application note is intended to help developers ensure"
  },
  {
    "id": 15,
    "content": "that their NVIDIA CUDA applications will run properly on the Hopper GPUs This document provides guidance to ensure that your software applications are compatible with Hopper architecture Ada Compatibility Guide This application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs This document provides guidance to ensure that your"
  },
  {
    "id": 16,
    "content": "software applications are compatible with Ada architecture Maxwell Tuning Guide Maxwell is NVIDIA’s 4th-generation architecture for CUDA compute applications Applications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes This guide summarizes the ways that applications can be fine-tuned to gain additional"
  },
  {
    "id": 17,
    "content": "speedups by leveraging Maxwell architectural features Pascal Tuning Guide Pascal is NVIDIA’s 5th-generation architecture for CUDA compute applications Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes This guide summarizes the ways that applications can be fine-tuned to gain additional speedups"
  },
  {
    "id": 18,
    "content": "by leveraging Pascal architectural features Volta Tuning Guide Volta is NVIDIA’s 6th-generation architecture for CUDA compute applications Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging"
  },
  {
    "id": 19,
    "content": "Volta architectural features Turing Tuning Guide Turing is NVIDIA’s 7th-generation architecture for CUDA compute applications Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing"
  },
  {
    "id": 20,
    "content": "architectural features NVIDIA Ampere GPU Architecture Tuning Guide NVIDIA Ampere GPU Architecture is NVIDIA’s 8th-generation architecture for CUDA compute applications Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes This guide summarizes the ways that applications can be"
  },
  {
    "id": 21,
    "content": "fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture’s features Hopper Tuning Guide Hopper GPU Architecture is NVIDIA’s 9th-generation architecture for CUDA compute applications Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes This guide summarizes the"
  },
  {
    "id": 22,
    "content": "ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture’s features Ada Tuning Guide The NVIDIA® Ada GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and"
  },
  {
    "id": 23,
    "content": "applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features"
  },
  {
    "id": 24,
    "content": "PTX ISA This guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA)"
  },
  {
    "id": 26,
    "content": "PTX Interoperability This document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code Inline PTX Assembly This document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code"
  },
  {
    "id": 27,
    "content": "It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter"
  },
  {
    "id": 28,
    "content": "CUDA API References  CUDA Runtime API Fields in structures might appear in order that is different from the order of declaration CUDA Driver API Fields in structures might appear in order that is different from the order of declaration"
  },
  {
    "id": 29,
    "content": "cuBLAS The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime"
  },
  {
    "id": 30,
    "content": "It allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs"
  },
  {
    "id": 31,
    "content": "NVBLAS The NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library"
  },
  {
    "id": 32,
    "content": "nvJPEG The nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications"
  },
  {
    "id": 33,
    "content": "cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology"
  },
  {
    "id": 34,
    "content": "The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas"
  },
  {
    "id": 35,
    "content": "NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains"
  },
  {
    "id": 36,
    "content": "It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API"
  },
  {
    "id": 37,
    "content": "This facility can often provide optimizations and performance not possible in a purely offline static compilation"
  },
  {
    "id": 38,
    "content": "PTX Compiler API References  PTX Compiler APIs This guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library"
  },
  {
    "id": 39,
    "content": "Miscellaneous  CUDA Demo Suite This document describes the demo applications shipped with the CUDA Demo Suite"
  },
  {
    "id": 40,
    "content": "CUDA on WSL This guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2)"
  },
  {
    "id": 42,
    "content": "Multi-Instance GPU (MIG) This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA® A100 GPU"
  },
  {
    "id": 43,
    "content": "CUDA Compatibility This document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade"
  },
  {
    "id": 44,
    "content": "The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications"
  },
  {
    "id": 46,
    "content": "0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express"
  },
  {
    "id": 47,
    "content": "This document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model"
  },
  {
    "id": 48,
    "content": "nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process"
  },
  {
    "id": 49,
    "content": "CUDA-GDB The NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware"
  },
  {
    "id": 50,
    "content": "Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Installation Guide Nsight Eclipse Plugins Edition Nsight Eclipse Plugins Edition getting started guide Nsight Systems The documentation for Nsight Systems Nsight Compute The NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications"
  },
  {
    "id": 51,
    "content": "It provides detailed performance metrics and API debugging via a user interface and command line tool"
  },
  {
    "id": 52,
    "content": "White Papers  Floating Point and IEEE 754 A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide Incomplete-LU and Cholesky Preconditioned Iterative Methods In this"
  },
  {
    "id": 53,
    "content": "white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively"
  },
  {
    "id": 54,
    "content": "Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms"
  },
  {
    "id": 55,
    "content": "Application Notes  CUDA for Tegra This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU)"
  },
  {
    "id": 56,
    "content": "libdevice User’s Guide The libdevice library is an LLVM bitcode library that implements common functions for GPU kernels"
  },
  {
    "id": 57,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 59,
    "content": "5 Update 1 Release Notes v12 5 | PDF | Archive NVIDIA CUDA Toolkit Release Notes The Release Notes for the CUDA Toolkit CUDA 12 5 Update 1 Release Notes  The release notes for the NVIDIA® CUDA® Toolkit can be found online at https: docs nvidia com/cuda/cuda-toolkit-release-notes/index"
  },
  {
    "id": 61,
    "content": "Note The release notes have been reorganized into two major sections: the general CUDA release notes, and the CUDA libraries release notes including historical information for 12"
  },
  {
    "id": 65,
    "content": "CUDA Toolkit Major Component Versions  CUDA Components Starting with CUDA 11, the various components in the toolkit are versioned independently For more information various GPU products that are CUDA capable, visit https: developer"
  },
  {
    "id": 68,
    "content": "The CUDA driver is backward compatible, meaning that applications compiled against a particular version of the CUDA will continue to work on subsequent (later) driver releases"
  },
  {
    "id": 74,
    "content": "0, the toolkit components are individually versioned, and the toolkit itself is versioned as shown in the table below"
  },
  {
    "id": 77,
    "content": "com/deploy/cuda-compatibility/index html Table 2 CUDA Toolkit and Minimum Required Driver Version for CUDA Minor Version Compatibility  CUDA Toolkit Minimum Required Driver Version for CUDA Minor Version Compatibility* Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 12"
  },
  {
    "id": 105,
    "content": "22** * Using a Minimum Required Version that is different from Toolkit Driver Version could be allowed in compatibility mode – please read the CUDA Compatibility Guide for details"
  },
  {
    "id": 112,
    "content": "The version of the development NVIDIA GPU Driver packaged in each CUDA Toolkit release is shown below Note that this driver is for development purposes and is not recommended for use in production with Tesla GPUs For running CUDA applications in production with Tesla GPUs, it is recommended to download the latest driver for Tesla GPUs from the NVIDIA driver downloads site at https: www nvidia"
  },
  {
    "id": 114,
    "content": "During the installation of the CUDA Toolkit, the installation of the NVIDIA driver may be skipped on Windows (when using the interactive or silent installation) or on Linux (by using meta packages)"
  },
  {
    "id": 129,
    "content": "General CUDA  In an upcoming CUDA release the NVIDIA Open GPU kernel module flavor will be the default and recommended installation option"
  },
  {
    "id": 130,
    "content": "End-users with Maxwell, Pascal, or Volta GPUs may need to take action to install the NVIDIA proprietary kernel modules"
  },
  {
    "id": 141,
    "content": "CUDA Developer Tools  For changes to nvprof and Visual Profiler, see the changelog For new features, improvements, and bug fixes in Nsight Visual Studio Edition, see the changelog For new features, improvements, and bug fixes in CUDA-GDB, see the changelog"
  },
  {
    "id": 153,
    "content": "Also added a new environment variable NVCC_REPORT_ALLERROR to emit error messages if the error is coming from a system header, instead of aborting the compiler"
  },
  {
    "id": 154,
    "content": "Resolved a compiler issue that caused different results when compiling with the -G flag than without the flag"
  },
  {
    "id": 155,
    "content": "Fixed the incorrect control flow transformation in the compiler caused by optimizations applied to multi-block loops"
  },
  {
    "id": 156,
    "content": "Resolved issues seen when compiling cuBLASDx device functions, in some conditions leading to “Misaligned shared or local address”"
  },
  {
    "id": 164,
    "content": "Deprecated or Dropped Features  Features deprecated in the current release of the CUDA software still work in the current release, but their documentation may have been removed, and they will become officially unsupported in a future release We recommend that developers employ alternative solutions to these features in their software"
  },
  {
    "id": 168,
    "content": "Deprecated or Dropped Architectures  NVIDIA CUDA support for the PowerPC architecture is removed in CUDA 12"
  },
  {
    "id": 173,
    "content": "Deprecated Operating Systems  NVIDIA CUDA support for Red Hat Enterprise Linux 7 and CentOS 7 is removed in CUDA 12"
  },
  {
    "id": 180,
    "content": "4 deprecated support for the following host compilers: Microsoft Visual C/C++ (MSVC) 2017 All GCC versions prior to GCC 7"
  },
  {
    "id": 188,
    "content": "CUDA Math Libraries toolchain uses C++11 features, and a C++11-compatible standard library (libstdc++ >= 20150422) is required on the host"
  },
  {
    "id": 195,
    "content": "5 Update 1  New Features Performance improvement to matrix multiplication targeting large language models, specifically for small batch sizes on Hopper GPUs Known Issues The bias epilogue (without ReLU or GeLU) may be not supported on Hopper GPUs for strided batch cases"
  },
  {
    "id": 196,
    "content": "Resolved Issues Under rare circumstances, executing SYMM/HEMM concurrently with GEMM on Hopper GPUs might have caused race conditions in the host code, which could lead to an Illegal Memory Access CUDA error cublasLtMatmul could produce an Illegal Instruction CUDA error on Pascal GPUs under the following conditions: batch is greater than 1, and beta is not equal to 0, and the computations are"
  },
  {
    "id": 202,
    "content": "cuBLAS: Release 12 5  New Features cuBLAS adds an experimental API to support mixed precision grouped batched GEMMs This enables grouped batched GEMMs with FP16 or BF16 inputs/outputs with the FP32 compute type"
  },
  {
    "id": 203,
    "content": "Known Issues cublasLtMatmul ignores inputs to CUBLASLT_MATMUL_DESC_D_SCALE_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER if the elements of the respective matrix are not of FP8 types Resolved Issues cublasLtMatmul ignored the mismatch between the provided scale type and the implied by the documentation, assuming the latter For instance, an unsupported configuration of cublasLtMatmul"
  },
  {
    "id": 204,
    "content": "with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 and produce incorrect results"
  },
  {
    "id": 205,
    "content": "cuBLAS SYMV failed for large n dimension: 131072 and above for ssymv, 92673 and above for csymv and dsymv, and 65536 and above for zsymv"
  },
  {
    "id": 210,
    "content": "4 Update 1  Known Issues Setting a cuBLAS handle stream to cudaStreamPerThread and setting the workspace via cublasSetWorkspace will cause any subsequent cublasSetWorkspace calls to fail"
  },
  {
    "id": 211,
    "content": "cublasLtMatmul ignores mismatches between the provided scale type and the scale type implied by the documentation and assumes the latter For example, an unsupported configuration of cublasLtMatmul with the scale type being FP32 and all other types being FP16 would run with the implicit assumption that the scale type is FP16 which can produce incorrect results Resolved Issues cublasLtMatmul"
  },
  {
    "id": 212,
    "content": "ignored the CUBLASLT_MATMUL_DESC_AMAX_D_POINTER for unsupported configurations instead of returning an error"
  },
  {
    "id": 213,
    "content": "In particular, computing absolute maximum of D is currently supported only for FP8 Matmul when the output data type is also FP8 ( CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2 )"
  },
  {
    "id": 214,
    "content": "Reduced host-side overheads for some of the cuBLASLt APIs: cublasLtMatmul() , cublasLtMatmulAlgoCheck() , and cublasLtMatmulAlgoGetHeuristic() cublasLtMatmul() and cublasLtMatmulAlgoGetHeuristic() could have resulted in floating point exceptions (FPE) on some Hopper-based GPUs, including Multi-Instance GPU (MIG)"
  },
  {
    "id": 221,
    "content": "4  New Features cuBLAS adds experimental APIs to support grouped batched GEMM for single precision and double precision"
  },
  {
    "id": 222,
    "content": "Grouped batch mode allows you to concurrently solve GEMMs of different dimensions (m, n, k), leading dimensions (lda, ldb, ldc), transpositions (transa, transb), and scaling factors (alpha, beta)"
  },
  {
    "id": 223,
    "content": "Known Issues When the current context has been created using cuGreenCtxCreate() , cuBLAS does not properly detect the number of SMs available"
  },
  {
    "id": 225,
    "content": "BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE"
  },
  {
    "id": 226,
    "content": "cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU}_BGRAD could out-of-bound access the workspace cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_D{RELU,GELU} could produce illegal memory access if no workspace is provided"
  },
  {
    "id": 227,
    "content": "When captured in CUDA Graph stream capture, cuBLAS routines can create memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail"
  },
  {
    "id": 233,
    "content": "3 Update 1  New Features Improved performance of heuristics cache for workloads that have a high eviction rate"
  },
  {
    "id": 234,
    "content": "Known Issues BLAS level 2 and 3 functions might not treat alpha in a BLAS compliant manner when alpha is zero and the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE"
  },
  {
    "id": 235,
    "content": "You may encounter the following issues: (1) HER{,2,X,K,2K} may zero the imaginary part on the diagonal elements of the output matrix; and (2) HER{,2,X,K,2K}, SYR{,2,X,K,2K} and others may produce NaN resulting from performing computation on matrices A and B which would otherwise be skipped"
  },
  {
    "id": 236,
    "content": "If strict compliance with BLAS is required, the user may manually check for alpha value before invoking the functions or switch to CUBLAS_POINTER_MODE_HOST"
  },
  {
    "id": 237,
    "content": "Resolved Issues cuBLASLt matmul operations might have computed the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1 0, the C and D matrices are the same, the epilogue contains GELU activation function"
  },
  {
    "id": 238,
    "content": "When an application compiled with cuBLASLt from CUDA Toolkit 12 2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12 2 update 2 or CUDA Toolkit 12"
  },
  {
    "id": 239,
    "content": "3, matrix multiply descriptors initialized using cublasLtMatmulDescInit() sometimes did not respect attribute changes using cublasLtMatmulDescSetAttribute()"
  },
  {
    "id": 241,
    "content": "cublasLtMatmul with K equals 1 and epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} might have returned incorrect results for the bias gradient"
  },
  {
    "id": 247,
    "content": "Known Issues cuBLASLt matmul operations may compute the output incorrectly under the following conditions: the data type of matrices A and B is FP8, the data type of matrices C and D is FP32, FP16, or BF16, the beta value is 1 0, the C and D matrices are the same, the epilogue contains GELU activation function"
  },
  {
    "id": 248,
    "content": "When an application compiled with cuBLASLt from CUDA Toolkit 12 2 update 1 or earlier runs with cuBLASLt from CUDA Toolkit 12"
  },
  {
    "id": 249,
    "content": "2 update 2 or later, matrix multiply descriptors initialized using cublasLtMatmulDescInit() may not respect attribute changes using cublasLtMatmulDescSetAttribute() To workaround this issue, create the matrix multiply descriptor using cublasLtMatmulDescCreate() instead of cublasLtMatmulDescInit()"
  },
  {
    "id": 254,
    "content": "2 Update 2  New Features cuBLASLt will now attempt to decompose problems that cannot be run by a single gemm kernel It does this by partitioning the problem into smaller chunks and executing the gemm kernel multiple times"
  },
  {
    "id": 255,
    "content": "This improves functional coverage for very large m, n, or batch size cases and makes the transition from the cuBLAS API to the cuBLASLt API more reliable"
  },
  {
    "id": 260,
    "content": "2  Known Issues cuBLAS initialization fails on Hopper architecture GPUs when MPS is in use with CUDA_MPS_ACTIVE_THREAD_PERCENTAGE set to a value less than 100%"
  },
  {
    "id": 261,
    "content": "Some Hopper kernels produce incorrect results for batched matmuls with CUBLASLT_EPILOGUE_RELU_BIAS or CUBLASLT_EPILOGUE_GELU_BIAS and a non-zero CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE"
  },
  {
    "id": 268,
    "content": "This is useful in some rare cases where certain CPU instructions used by cuBLASLt heuristics negatively impact CPU performance"
  },
  {
    "id": 269,
    "content": "Known Issues When creating a matrix layout using the cublasLtMatrixLayoutCreate() function, the object pointed at by cublasLtMatrixLayout_t is smaller than cublasLtMatrixLayoutOpaque_t (but enough to hold the internal structure)"
  },
  {
    "id": 270,
    "content": "As a result, the object should not be dereferenced or copied explicitly, as this might lead to out of bound accesses"
  },
  {
    "id": 271,
    "content": "If one needs to serialize the layout or copy it, it is recommended to manually allocate an object of size sizeof(cublasLtMatrixLayoutOpaque_t) bytes, and initialize it using cublasLtMatrixLayoutInit() function The issue will be fixed in future releases by ensuring that cublasLtMatrixLayoutCreate() allocates at least sizeof(cublasLtMatrixLayoutOpaque_t) bytes"
  },
  {
    "id": 277,
    "content": "Known Issues For optimal performance on NVIDIA Hopper architecture, cuBLAS needs to allocate a bigger internal workspace (64 MiB) than on the previous architectures (8 MiB) A possible workaround is to set the CUBLAS_WORKSPACE_CONFIG environment variable to :32768:2 when running cuBLAS on NVIDIA Hopper architecture"
  },
  {
    "id": 278,
    "content": "Resolved Issues Reduced cuBLAS host-side overheads caused by not using the cublasLt heuristics cache"
  },
  {
    "id": 285,
    "content": "Added more Hopper-specific kernels for cublasLtMatmul with epilogues: CUBLASLT_EPILOGUE_BGRAD{A,B} CUBLASLT_EPILOGUE_{RELU,GELU}_AUX CUBLASLT_EPILOGUE_D{RELU,GELU} Improved Hopper performance on arm64-sbsa by adding Hopper kernels that were previously supported only on the x86_64 architecture for Windows and Linux"
  },
  {
    "id": 286,
    "content": "Known Issues There are no forward compatible kernels for single precision complex gemms that do not require workspace"
  },
  {
    "id": 287,
    "content": "Resolved Issues Fixed an issue on NVIDIA Ampere architecture and newer GPUs where cublasLtMatmul with epilogue CUBLASLT_EPILOGUE_BGRAD{A,B} and a nontrivial reduction scheme (that is, not CUBLASLT_REDUCTION_SCHEME_NONE ) could return incorrect results for the bias gradient cublasLtMatmul for gemv-like cases (that is, m or n equals 1) might ignore bias with the CUBLASLT_EPILOGUE_RELU_BIAS and"
  },
  {
    "id": 290,
    "content": "cublasLt3mMode_t , CUBLASLT_MATMUL_PREF_MATH_MODE_MASK , and CUBLASLT_MATMUL_PREF_GAUSSIAN_MODE_MASK from cublasLtMatmulPreferenceAttributes_t CUBLASLT_MATMUL_PREF_POINTER_MODE_MASK , CUBLASLT_MATMUL_PREF_EPILOGUE_MASK , and CUBLASLT_MATMUL_PREF_SM_COUNT_TARGET from cublasLtMatmulPreferenceAttributes_t This mask was only applicable to CUBLASLT_MATMUL_PREF_MATH_MODE_MASK which was removed"
  },
  {
    "id": 297,
    "content": "5  New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in R2C and C2R FFTs for many sizes We recommend testing your R2C / C2R use cases with and without JIT LTO kernels and comparing the resulting performance You can enable JIT LTO kernels using the per-plan properties cuFFT API"
  },
  {
    "id": 301,
    "content": "cuFFT: Release 12 4 Update 1  Resolved Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt h ) in CUDA 12"
  },
  {
    "id": 308,
    "content": "4  New Features Added Just-In-Time Link-Time Optimized (JIT LTO) kernels for improved performance in FFTs with 64-bit indexing"
  },
  {
    "id": 310,
    "content": "Improved accuracy for certain single-precision (fp32) FFT cases, especially involving FFTs for larger sizes"
  },
  {
    "id": 311,
    "content": "Known Issues A routine from the cuFFT LTO EA library was added by mistake to the cuFFT Advanced API header ( cufftXt"
  },
  {
    "id": 314,
    "content": "Resolved Issues Fixed an issue that could cause overwriting of user data when performing out-of-place real-to-complex (R2C) transforms with user-specified output strides (i"
  },
  {
    "id": 316,
    "content": "Fixed inconsistent behavior between libcufftw and FFTW when both inembed and onembed are nullptr / NULL From now on, as in FFTW, passing nullptr / NULL as inembed/onembed parameter is equivalent to passing n, that is, the logical size for that dimension"
  },
  {
    "id": 321,
    "content": "3 Update 1  Known Issues Executing a real-to-complex (R2C) or complex-to-real (C2R) plan in a context different to the one used to create the plan could cause undefined behavior Resolved Issues Complex-to-complex (C2C) execution functions ( cufftExec and similar) now properly error-out in case of error during kernel launch, for example due to a missing CUDA context"
  },
  {
    "id": 326,
    "content": "3  New Features Callback kernels are more relaxed in terms of resource usage, and will use fewer registers"
  },
  {
    "id": 327,
    "content": "Improved accuracy for double precision prime and composite FFT sizes with factors larger than 127 Slightly improved planning times for some FFT sizes"
  },
  {
    "id": 332,
    "content": "2  New Features cufftSetStream can be used in multi-GPU plans with a stream from any GPU context, instead of from the primary context of the first GPU listed in cufftXtSetGPUs"
  },
  {
    "id": 333,
    "content": "The improved performance spans hundreds of single precision and double precision cases for FFTs with contiguous data layout, across multiple GPU architectures (from Maxwell to Hopper GPUs) via PTX JIT"
  },
  {
    "id": 334,
    "content": "Resolved Issues cuFFT no longer exhibits a race condition when threads simultaneously create and access plans with more than 1023 plans alive cuFFT no longer exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently"
  },
  {
    "id": 339,
    "content": "1 Update 1  Known Issues cuFFT exhibits a race condition when one thread calls cufftCreate (or cufftDestroy ) and another thread calls any API (except cufftCreate or cufftDestroy ), and when the total number of plans alive exceeds 1023 cuFFT exhibits a race condition when multiple threads call cufftXtSetGPUs concurrently on different plans"
  },
  {
    "id": 344,
    "content": "1  New Features Improved performance on Hopper GPUs for hundreds of FFTs of sizes ranging from 14 to 28800 The improved performance spans over 542 cases across single and double precision for FFTs with contiguous data layout"
  },
  {
    "id": 346,
    "content": "8, CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms"
  },
  {
    "id": 347,
    "content": "An upcoming release will update the cuFFT callback implementation, removing this limitation cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11"
  },
  {
    "id": 349,
    "content": "Resolved Issues cuFFT no longer produces errors with compute-sanitizer at program exit if the CUDA context used at plan creation was destroyed prior to program exit"
  },
  {
    "id": 354,
    "content": "0 Update 1  Resolved Issues Scratch space requirements for multi-GPU, single-batch, 1D FFTs were reduced"
  },
  {
    "id": 359,
    "content": "0  New Features PTX JIT kernel compilation allowed the addition of many new accelerated cases for Maxwell, Pascal, Volta and Turing architectures"
  },
  {
    "id": 362,
    "content": "5 Update 1  Resolved Issues The potential out-of-bound accesses on bufferOnDevice by calls of cusolverDnXlarft have been resolved"
  },
  {
    "id": 371,
    "content": "4 Update 1, values ldt > k in calls of cusolverDnXlarft can result in out-of-bound memory accesses on bufferOnDevice"
  },
  {
    "id": 372,
    "content": "As a workaround it is possible to allocate a larger device workspace buffer of size workspaceInBytesOnDevice=ALIGN_32((ldt*k + n*k)*sizeofCudaDataType(dataTypeT)) , with auto ALIGN_32 = []( int64_t val ) { return (( val + 31 ) / 32 ) * 32 ; }; and auto sizeofCudaDataType = []( cudaDataType dt ) { if ( dt == CUDA_R_32F ) return sizeof ( float ); if ( dt == CUDA_R_64F ) return sizeof ( double ); if"
  },
  {
    "id": 373,
    "content": "( dt == CUDA_C_32F ) return sizeof ( cuComplex ); if ( dt == CUDA_C_64F ) return sizeof ( cuDoubleComplex ); }; 2"
  },
  {
    "id": 377,
    "content": "4 Update 1  New Features The performance of cusolverDnXlarft has been improved The change in cusolverDnXlarft also results in a modest speedup in cusolverDnormqr , cusolverDnormtr , and cusolverDnXsyevd"
  },
  {
    "id": 381,
    "content": "Deprecations Using long-deprecated cusolverDnPotrf , cusolverDnPotrs , cusolverDnGeqrf , cusolverDnGetrf , cusolverDnGetrs , cusolverDnSyevd , cusolverDnSyevdx , cusolverDnGesvd , and their accompanying bufferSize functions will result in a deprecation warning"
  },
  {
    "id": 382,
    "content": "The warning can be turned off by using the -DDISABLE_CUSOLVER_DEPRECATED flag while compiling; however, users should use cusolverDnXpotrf , cusolverDnXpotrs , cusolverDnXgeqrf , cusolverDnXgetrf , cusolverDnXgetrs , cusolverDnXsyevd , cusolverDnXsyevdx , cusolverDnXgesvd , and the corresponding bufferSize functions instead"
  },
  {
    "id": 387,
    "content": "4  New Features cusolverDnXlarft and cusolverDnXlarft_bufferSize APIs were introduced cusolverDnXlarft forms the triangular factor of a real block reflector, while cusolverDnXlarft_bufferSize returns its required workspace sizes in bytes Known Issues cusolverDnXtrtri_bufferSize` returns an incorrect required device workspace size As a workaround the returned size can be multiplied by the size of"
  },
  {
    "id": 388,
    "content": "the data type (for example, 8 bytes if matrix A is of type double) to obtain the correct workspace size"
  },
  {
    "id": 393,
    "content": "2 Update 2  Resolved Issues Fixed an issue with cusolverDngesvd() , cusolverDnGesvd() , and cusolverDnXgesvd() , which could cause wrong results for matrices larger than 18918 if jobu or jobvt was unequal to ‘ N ’"
  },
  {
    "id": 398,
    "content": "2  New Features A new API to ensure deterministic results or allow non-deterministic results for improved performance"
  },
  {
    "id": 399,
    "content": "Affected functions are: cusolverDngeqrf() , cusolverDnsyevd() , cusolverDnsyevdx() , cusolverDngesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvdr() , and cusolverDnXgesvdp()"
  },
  {
    "id": 400,
    "content": "Known Issues Concurrent executions of cusolverDngetrf() or cusolverDnXgetrf() in different non-blocking CUDA streams on the same device might result in a deadlock"
  },
  {
    "id": 408,
    "content": "Resolved Issues cusparseSpMM() would sometimes get incorrect results when alpha=0 , num_batches>1 , batch_stride indicates that there is padding between batches"
  },
  {
    "id": 409,
    "content": "cusparseSpMM_bufferSize() would return the wrong size when the sparse matrix is Blocked Ellpack and the dense matrices have only a single column (n=1) cusparseCreateSlicedEll would return an error when the slice size is greater than the matrix number of rows Sliced-ELLPACK cusparseSpSV_analysis() failed due to insufficient resources for some matrices and some slice sizes"
  },
  {
    "id": 414,
    "content": "5  New Features Added support for mixed input types in SpMV: single precision input matrix, double precision input vector, double precision output vector"
  },
  {
    "id": 415,
    "content": "Resolved Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes"
  },
  {
    "id": 420,
    "content": "4  New Features Added the preprocessing step for sparse matrix-vector multiplication cusparseSpMV_preprocess() Added a new API cusparseSpSM_updateMatrix() to update the sparse matrix between the analysis and solving phase of cusparseSpSM()"
  },
  {
    "id": 421,
    "content": "Known Issues cusparseSpMV() introduces invalid memory accesses when the output vector is not aligned to 16 bytes Resolved Issues cusparseSpVV() provided incorrect results when the sparse vector has many non-zeros"
  },
  {
    "id": 426,
    "content": "3 Update 1  New Features Added support for block sizes of 64 and 128 in cusparseSDDMM() Added a preprocessing step cusparseSDDMM_preprocess() for BSR cusparseSDDMM() that helps improve performance of the main computing stage"
  },
  {
    "id": 431,
    "content": "3  New Features The cusparseSpSV_bufferSize() and cusparseSpSV_analysis() routines now accept NULL pointers for the dense vector The cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines now accept dense matrix descriptors with NULL pointer for values Known Issues The cusparseSpSV_analysis() and cusparseSpSM_analysis() routines are blocking calls/not asynchronous"
  },
  {
    "id": 432,
    "content": "Wrong results can occur for cusparseSpSV() using sliced ELLPACK format and transpose/transpose conjugate operation on matrix A"
  },
  {
    "id": 433,
    "content": "Fixed an issue that caused cusparseSpSV_analysis() to hang sometimes in a multi-thread environment Fixed an issue with cusparseSpSV() and cusparseSpSV() that sometimes yielded wrong output when the output vector/matrix or input matrix contained NaN"
  },
  {
    "id": 438,
    "content": "2 Update 1  New Features The library now provides the opportunity to dump sparse matrices to files during the creation of the descriptor for debugging purposes"
  },
  {
    "id": 439,
    "content": "Resolved Issues Removed CUSPARSE_SPMM_CSR_ALG3 fallback to avoid confusion in the algorithm selection process"
  },
  {
    "id": 446,
    "content": "1 Update 1  New Features Introduced Block Sparse Row (BSR) sparse matrix storage for the Generic APIs with support for SDDMM routine ( cusparseSDDMM ) Introduced Sliced Ellpack (SELL) sparse matrix storage format for the Generic APIs with support for sparse matrix-vector multiplication ( cusparseSpMV ) and triangular solver with a single right-hand side ( cusparseSpSV ) Added a new API call ("
  },
  {
    "id": 447,
    "content": "cusparseSpSV_updateMatrix ) to update matrix values and/or the matrix diagonal in the sparse triangular solver with a single right-hand side after the analysis step"
  },
  {
    "id": 452,
    "content": "0 Update 1  New Features cusparseSDDMM() now supports mixed precision computation Improved cusparseSpMM() alg2 mixed-precision performance on some matrices on NVIDIA Ampere architecture GPUs cusparseSpSV() and cusparseSpSM() now support in-place computation, namely the output and input vectors/matrices have the same memory address"
  },
  {
    "id": 453,
    "content": "Resolved Issues cusparseSpSM() could produce wrong results if the leading dimension (ld) of the RHS matrix is greater than the number of columns/rows"
  },
  {
    "id": 458,
    "content": "0  New Features JIT LTO functionalities ( cusparseSpMMOp() ) switched from driver to nvJitLto library"
  },
  {
    "id": 462,
    "content": "Now the Generic APIs interface clearly declares when a descriptor and its data are modified by the cuSPARSE functions"
  },
  {
    "id": 463,
    "content": "The first algorithm computes a strict bound on the number of intermediate product, while the second one allows partitioning the computation in chunks"
  },
  {
    "id": 472,
    "content": "5  Known Issues As a result of ongoing testing we updated the interval bounds in which double precision lgamma() function may experience greater than the documented 4 ulp accuracy loss"
  },
  {
    "id": 479,
    "content": "4  Resolved Issues Host-specific code in cuda_fp16/bf16 headers is now free from type-punning and shall work correctly in the presence of optimizations based on strict-aliasing rules"
  },
  {
    "id": 483,
    "content": "CUDA Math: Release 12 3  New Features Performance of SIMD Integer CUDA Math APIs was improved Resolved Issues The __hisinf() Math APIs from cuda_fp16"
  },
  {
    "id": 485,
    "content": "h headers were silently producing wrong results if compiled with the -std=c++20 compiler option because of an underlying nvcc compiler issue, resolved in version 12"
  },
  {
    "id": 491,
    "content": "pass -fno-strict-aliasing to host GCC compiler) as these may interfere with the type-punning idioms used in the __half , __half2 , __nv_bfloat16 , __nv_bfloat162 types implementations and expose the user program to undefined behavior Note, the headers suppress GCC diagnostics through: #pragma GCC diagnostic ignored -Wstrict-aliasing"
  },
  {
    "id": 496,
    "content": "CUDA Math: Release 12 2  New Features CUDA Math APIs for __half and __nv_bfloat16 types received usability improvements, including host side support for many of the arithmetic operations and conversions __half and __nv_bfloat16 types have implicit conversions to/from integral types, which are now available with host compilers by default"
  },
  {
    "id": 497,
    "content": "To opt-out user may want to define the following macros (these macros will be removed in the future CUDA release): __CUDA_FP16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ __CUDA_BF16_DISABLE_IMPLICIT_INTEGER_CONVERTS_FOR_HOST_COMPILERS__ Resolved Issues During ongoing testing, NVIDIA identified that due to an algorithm error the results of 64-bit floating-point division in default"
  },
  {
    "id": 501,
    "content": "The affected algorithm was present in both offline compilation as well as just-in-time (JIT) compilation As JIT compilation is handled by the driver, NVIDIA recommends updating to driver version greater than or equal to R535 (R536 on Windows) when IEEE754 compliance is required and when using JIT"
  },
  {
    "id": 502,
    "content": "Updated the observed worst case error bounds for single precision intrinsic functions __expf() , __exp10f() and double precision functions asinh() , acosh()"
  },
  {
    "id": 507,
    "content": "1  New Features Performance and accuracy improvements in atanf , acosf , asinf , sinpif , cospif , powf , erff , and tgammaf"
  },
  {
    "id": 511,
    "content": "CUDA Math: Release 12 0  New Features Introduced new integer/fp16/bf16 CUDA Math APIs to help expose performance benefits of new DPX instructions"
  },
  {
    "id": 512,
    "content": "Known Issues Double precision inputs that cause the double precision division algorithm in the default ‘round to nearest even mode’ produce spurious overflow: an infinite result is delivered where DBL_MAX 0x7FEF_FFFF_FFFF_FFFF is expected"
  },
  {
    "id": 527,
    "content": "Resolved Issues A performance issue with the NPP ResizeSqrPixel API is now fixed and shows improved performance"
  },
  {
    "id": 535,
    "content": "Zero Copy behavior has been changed: Setting NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY flag will no longer enable NVJPEG_FLAGS_REDUCED_MEMORY_DECODE"
  },
  {
    "id": 540,
    "content": "3 Update 1  New Features New APIs: nvjpegBufferPinnedResize and nvjpegBufferDeviceResize which can be used to resize pinned and device buffers before using them"
  },
  {
    "id": 551,
    "content": "Resolved Issues An issue that causes runtime failures when nvJPEGDecMultipleInstances was tested with a large number of threads is resolved"
  },
  {
    "id": 552,
    "content": "Known Issues Backend NVJPEG_BACKEND_GPU_HYBRID - Unable to handle bistreams with extra scans lengths"
  },
  {
    "id": 554,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 555,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 557,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 558,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 559,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 560,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 561,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 562,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 563,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 564,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 565,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 566,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 567,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 574,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 576,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 577,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 584,
    "content": "A future CUDA release will have the Nsight Visual Studio installer with VS2022 support integrated into it"
  },
  {
    "id": 588,
    "content": "New instructions in public PTX  New instructions for bit mask creation—BMSK, and sign extension—SZEXT, are added to the public PTX ISA You can find documentation for these instructions in the PTX ISA guide: BMSK and SZEXT"
  },
  {
    "id": 592,
    "content": "Unused Kernel Optimization  In CUDA 11 5, unused kernel pruning was introduced with the potential benefits of reducing binary size and improving performance through more efficient optimizations As mentioned in the 11"
  },
  {
    "id": 593,
    "content": "5 blog, there is an opt-out flag that can be used in case it becomes necessary for debug purposes or for other special situations"
  },
  {
    "id": 594,
    "content": "New -arch=native option  In addition to the -arch=all and -arch=all-major options added in CUDA 11 5, NVCC introduced -arch= native in CUDA 11"
  },
  {
    "id": 596,
    "content": "This -arch=native option is a convenient way for users to let NVCC determine the right target architecture to compile the CUDA device code to based on the GPU installed on the system"
  },
  {
    "id": 597,
    "content": "This can be particularly helpful for testing when applications are run on the same system they are compiled in"
  },
  {
    "id": 601,
    "content": "Generate PTX from nvlink:  Using the following command line, device linker, nvlink will produce PTX as an output in addition to CUBIN: nvcc -dlto -dlink -ptx Device linking by nvlink is the final stage in the CUDA compilation process"
  },
  {
    "id": 602,
    "content": "Applications that have multiple source translation units have to be compiled in separate compilation mode"
  },
  {
    "id": 604,
    "content": "4) allowed nvlink to perform optimizations at device link time instead of at compile time so that separately compiled applications with several translation units can be optimized to the same level as whole program compilations with a single translation unit However, without the option to output PTX, applications that cared about forward compatibility of device code could not benefit from Link"
  },
  {
    "id": 605,
    "content": "Time Optimization or had to constrain the device code to a single source file With the option for nvlink that performs LTO to generate the output in PTX, customer applications that require forward compatibility across GPU architectures can span across multiple files and can also take advantage of Link Time Optimization"
  },
  {
    "id": 609,
    "content": "Bullseye support  NVCC compiled source code now works with the code coverage tool Bullseye Code coverage for device function is not supported through bullseye"
  },
  {
    "id": 619,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 620,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 622,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 623,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 624,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 625,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 626,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 627,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 628,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 629,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 630,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 631,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 632,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 639,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 641,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 642,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 649,
    "content": "License Agreement for NVIDIA Software Development Kits v12 5 | PDF | Archive End User License Agreement NVIDIA Software License Agreement and CUDA Supplement to Software License Agreement Last updated: October 8, 2021 The CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and"
  },
  {
    "id": 650,
    "content": "the associated documentation on CUDA APIs, programming model and development tools If you do not agree with the terms and conditions of the license agreement, then do not download or use the software Preface The Software License Agreement in Chapter 1 and the Supplement in Chapter 2 contain license terms and conditions that govern the use of NVIDIA CUDA toolkit By accepting this agreement, you"
  },
  {
    "id": 651,
    "content": "agree to comply with all the terms and conditions applicable to the product(s) included herein NVIDIA Driver Description This package contains the operating system driver and fundamental system software components for NVIDIA GPUs NVIDIA CUDA Toolkit Description The NVIDIA CUDA Toolkit provides command-line and graphical tools for building, debugging and optimizing the performance of applications"
  },
  {
    "id": 652,
    "content": "accelerated by NVIDIA GPUs, runtime and math libraries, and documentation including programming guides, user manuals, and API references Default Install Location of CUDA Toolkit Windows platform: %ProgramFiles%\\NVIDIA GPU Computing Toolkit\\CUDA\\v# # Linux platform: /usr/local/cuda-# # Mac platform: /Developer/NVIDIA/CUDA-# # NVIDIA CUDA Samples Description CUDA Samples are now located in https:"
  },
  {
    "id": 653,
    "content": "github com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples NVIDIA Nsight Visual Studio Edition (Windows only) Description NVIDIA Nsight Development Platform, Visual Studio Edition is a development environment integrated into Microsoft Visual Studio that provides tools for debugging, profiling, analyzing and optimizing your GPU computing and"
  },
  {
    "id": 654,
    "content": "graphics applications Default Install Location of Nsight Visual Studio Edition Windows platform: %ProgramFiles(x86)%\\NVIDIA Corporation\\Nsight Visual Studio Edition #"
  },
  {
    "id": 656,
    "content": "License Agreement for NVIDIA Software Development Kits  Important Notice—Read before downloading, installing, copying or using the licensed software: This license agreement, including exhibits attached (“Agreement”) is a legal agreement between you and NVIDIA Corporation (“NVIDIA”) and governs your use of a NVIDIA software development kit (“SDK”)"
  },
  {
    "id": 657,
    "content": "Each SDK has its own set of software and materials, but here is a description of the types of items that may be included in a SDK: source code, header files, APIs, data sets and assets (examples include images, textures, models, scenes, videos, native API input/output files), binary software, sample code, libraries, utility programs, programming code and documentation"
  },
  {
    "id": 658,
    "content": "This Agreement can be accepted only by an adult of legal age of majority in the country in which the SDK is used If you are entering into this Agreement on behalf of a company or other legal entity, you represent that you have the legal authority to bind the entity to this Agreement, in which case “you” will mean the entity you represent If you don’t have the required age or authority to accept"
  },
  {
    "id": 659,
    "content": "this Agreement, or if you don’t accept all the terms and conditions of this Agreement, do not download, install or use the SDK You agree to use the SDK only for purposes that are permitted by (a) this Agreement, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions"
  },
  {
    "id": 665,
    "content": "License Grant  Subject to the terms of this Agreement, NVIDIA hereby grants you a non-exclusive, non-transferable license, without the right to sublicense (except as expressly provided in this Agreement) to: Install and use the SDK, Modify and create derivative works of sample source code delivered in the SDK, and Distribute those portions of the SDK that are identified in this Agreement as"
  },
  {
    "id": 666,
    "content": "distributable, as incorporated in object code format into a software application that meets the distribution requirements indicated in this Agreement"
  },
  {
    "id": 670,
    "content": "Distribution Requirements  These are the distribution requirements for you to exercise the distribution grant: Your application must have material additional functionality, beyond the included portions of the SDK"
  },
  {
    "id": 671,
    "content": "The following notice shall be included in modifications and derivative works of sample source code distributed: “This software contains source code provided by NVIDIA Corporation"
  },
  {
    "id": 672,
    "content": "” Unless a developer tool is identified in this Agreement as distributable, it is delivered for your internal use only"
  },
  {
    "id": 673,
    "content": "The terms under which you distribute your application must be consistent with the terms of this Agreement, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA’s intellectual property rights Additionally, you agree that you will protect the privacy, security and legal rights of your application users You agree to notify NVIDIA in"
  },
  {
    "id": 674,
    "content": "writing of any known or suspected distribution or use of the SDK not in compliance with the requirements of this Agreement, and to enforce the terms of your agreements with respect to distributed SDK"
  },
  {
    "id": 678,
    "content": "Authorized Users  You may allow employees and contractors of your entity or of your subsidiary(ies) to access and use the SDK from your secure network to perform work on your behalf If you are an academic institution you may allow users enrolled or employed by the academic institution to access and use the SDK from your secure network You are responsible for the compliance with the terms of this"
  },
  {
    "id": 679,
    "content": "Agreement by your authorized users If you become aware that your authorized users didn’t follow the terms of this Agreement, you agree to take reasonable steps to resolve the non-compliance and prevent new occurrences"
  },
  {
    "id": 683,
    "content": "Pre-Release SDK  The SDK versions identified as alpha, beta, preview or otherwise as pre-release, may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, accessibility, availability, and reliability standards relative to commercial versions of NVIDIA software and materials Use of a pre-release SDK may result in unexpected results,"
  },
  {
    "id": 684,
    "content": "loss of data, project delays or other unpredictable damage or loss You may use a pre-release SDK at your own risk, understanding that pre-release SDKs are not intended for use in production or business-critical systems NVIDIA may also choose to abandon development and terminate the availability of a pre-release SDK at any time without liability"
  },
  {
    "id": 688,
    "content": "Updates  NVIDIA may, at its option, make available patches, workarounds or other updates to this SDK Unless the updates are provided with their separate governing terms, they are deemed part of the SDK licensed to you as provided in this Agreement You agree that the form and content of the SDK that NVIDIA provides may change without prior notice to you While NVIDIA generally maintains"
  },
  {
    "id": 689,
    "content": "compatibility between versions, NVIDIA may in some cases make changes that introduce incompatibilities in future versions of the SDK"
  },
  {
    "id": 693,
    "content": "Components Under Other Licenses  The SDK may come bundled with, or otherwise include or be distributed with, NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SDK If and to the extent there is a conflict between the terms in this Agreement and the license terms associated with the component, the license terms"
  },
  {
    "id": 694,
    "content": "associated with the components control only to the extent necessary to resolve the conflict Subject to the other terms of this Agreement, you may use the SDK to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses"
  },
  {
    "id": 698,
    "content": "Reservation of Rights  NVIDIA reserves all rights, title, and interest in and to the SDK, not expressly granted to you under this Agreement"
  },
  {
    "id": 701,
    "content": "Limitations  The following license limitations apply to your use of the SDK: You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SDK or copies of the SDK Except as expressly provided in this Agreement, you may not copy, sell, rent, sublicense, transfer, distribute, modify, or create derivative works of any portion of"
  },
  {
    "id": 702,
    "content": "the SDK Unless you have an agreement with NVIDIA for this purpose, you may not indicate that an application created with the SDK is sponsored or endorsed by NVIDIA You may not bypass, disable, or circumvent any encryption, security, digital rights management or authentication mechanism in the SDK You may not use the SDK in any manner that would cause it to become subject to an open source software"
  },
  {
    "id": 703,
    "content": "license As examples, licenses that require as a condition of use, modification, and/or distribution that the SDK be: Disclosed or distributed in source code form; Licensed for the purpose of making derivative works; or Redistributable at no charge You acknowledge that the SDK as delivered is not tested or certified by NVIDIA for use in connection with the design, construction, maintenance, and/or"
  },
  {
    "id": 704,
    "content": "operation of any system where the use or failure of such system could result in a situation that threatens the safety of human life or results in catastrophic damages (each, a “Critical Application”) Examples of Critical Applications include use in avionics, navigation, autonomous vehicle applications, ai solutions for automotive products, military, medical, life support or other life critical"
  },
  {
    "id": 705,
    "content": "applications NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses You are solely responsible for ensuring that any product or service developed with the SDK as a whole includes sufficient features to comply with all applicable legal and regulatory standards and requirements You agree to defend, indemnify and hold harmless"
  },
  {
    "id": 706,
    "content": "NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney’s fees and costs incident to establishing the right of indemnification) arising out of or related to products or services that"
  },
  {
    "id": 707,
    "content": "use the SDK in or for Critical Applications, and for use of the SDK outside of the scope of this Agreement or not in compliance with its terms You may not reverse engineer, decompile or disassemble any portion of the output generated using SDK elements for the purpose of translating such output artifacts to target a non-NVIDIA platform"
  },
  {
    "id": 710,
    "content": "Ownership  NVIDIA or its licensors hold all rights, title and interest in and to the SDK and its modifications and derivative works, including their respective intellectual property rights, subject to your rights under Section 1"
  },
  {
    "id": 713,
    "content": "This SDK may include software and materials from NVIDIA’s licensors, and these licensors are intended third party beneficiaries that may enforce this Agreement with respect to their intellectual property rights You hold all rights, title and interest in and to your applications and your derivative works of the sample source code delivered in the SDK, including their respective intellectual"
  },
  {
    "id": 717,
    "content": "You may, but don’t have to, provide to NVIDIA suggestions, feature requests or other feedback regarding the SDK, including possible enhancements or modifications to the SDK For any feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of"
  },
  {
    "id": 718,
    "content": "sublicensees), and distribute (through multiple tiers of distributors) it without the payment of any royalties or fees to you NVIDIA is constantly looking for ways to improve its products, so you may send feedback to NVIDIA through the developer portal at https: developer nvidia"
  },
  {
    "id": 722,
    "content": "No Warranties  THE SDK IS PROVIDED BY NVIDIA “AS IS” AND “WITH ALL FAULTS ” TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES EXPRESSLY DISCLAIM ALL WARRANTIES OF ANY KIND OR NATURE, WHETHER EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE, NON-INFRINGEMENT, OR THE ABSENCE OF ANY DEFECTS"
  },
  {
    "id": 727,
    "content": "Limitation of Liability  TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY (I) SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR (II) DAMAGES FOR (A) ANY LOST PROFITS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR (B) THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE USE OR PERFORMANCE"
  },
  {
    "id": 728,
    "content": "OF THE SDK, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY IN NO EVENT WILL NVIDIA’S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS AGREEMENT EXCEED US$10"
  },
  {
    "id": 730,
    "content": "THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT These exclusions and limitations of liability shall apply regardless if NVIDIA or its affiliates have been advised of the possibility of such damages, and regardless of whether a remedy fails its essential purpose These exclusions and limitations of liability form an essential basis of the bargain"
  },
  {
    "id": 731,
    "content": "between the parties, and, absent any of these exclusions or limitations of liability, the provisions of this Agreement, including, without limitation, the economic terms, would be substantially different"
  },
  {
    "id": 734,
    "content": "Termination  This Agreement will continue to apply until terminated by either you or NVIDIA as described below NVIDIA may, at any time, terminate this Agreement if: (i) you fail to comply with any term of this Agreement and the non-compliance is not fixed within thirty (30) days following notice from NVIDIA (or immediately if you violate NVIDIA’s intellectual property rights); (ii) you commence"
  },
  {
    "id": 735,
    "content": "or participate in any legal proceeding against NVIDIA with respect to the SDK; or (iii) NVIDIA decides to no longer provide the SDK in a country or, in NVIDIA’s sole discretion, the continued use of it is no longer commercially viable Upon any termination of this Agreement, you agree to promptly discontinue use of the SDK and destroy all copies in your possession or control Your prior"
  },
  {
    "id": 736,
    "content": "distributions in accordance with this Agreement are not affected by the termination of this Agreement Upon written request, you will certify in writing that you have complied with your commitments under this section Upon any termination of this Agreement all provisions survive except for the license grant provisions"
  },
  {
    "id": 739,
    "content": "General  If you wish to assign this Agreement or your rights and obligations, including by merger, consolidation, dissolution or operation of law, contact NVIDIA to ask for permission"
  },
  {
    "id": 741,
    "content": "NVIDIA may assign, delegate or transfer this Agreement and its rights and obligations, and if to a non-affiliate you will be notified You agree to cooperate with NVIDIA and provide reasonably requested information to verify your compliance with this Agreement"
  },
  {
    "id": 742,
    "content": "This Agreement will be governed in all respects by the laws of the United States and of the State of Delaware, without regard to the conflicts of laws principles"
  },
  {
    "id": 743,
    "content": "The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed"
  },
  {
    "id": 744,
    "content": "The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this Agreement Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction"
  },
  {
    "id": 745,
    "content": "If any court of competent jurisdiction determines that any provision of this Agreement is illegal, invalid or unenforceable, such provision will be construed as limited to the extent necessary to be consistent with and fully enforceable under the law and the remaining provisions will remain in full force and effect"
  },
  {
    "id": 746,
    "content": "Each party acknowledges and agrees that the other is an independent contractor in the performance of this Agreement"
  },
  {
    "id": 747,
    "content": "The SDK has been developed entirely at private expense and is “commercial items” consisting of “commercial computer software” and “commercial computer software documentation” provided with RESTRICTED RIGHTS"
  },
  {
    "id": 749,
    "content": "7202-3(a) or as set forth in subparagraphs (c)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52"
  },
  {
    "id": 751,
    "content": "You agree that you will not ship, transfer or export the SDK into any country, or use the SDK in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U"
  },
  {
    "id": 753,
    "content": "Department of Treasury’s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations"
  },
  {
    "id": 754,
    "content": "By accepting this Agreement, you confirm that you are not located in a country currently embargoed by the U"
  },
  {
    "id": 757,
    "content": "You agree that any notices that NVIDIA sends you electronically will satisfy any legal communication requirements Please direct your legal notices or other correspondence to NVIDIA Corporation, 2788 San Tomas Expressway, Santa Clara, California 95051, United States of America, Attention: Legal Department"
  },
  {
    "id": 758,
    "content": "This Agreement and any exhibits incorporated into this Agreement constitute the entire agreement of the parties with respect to the subject matter of this Agreement and supersede all prior negotiations or documentation exchanged between the parties relating to this SDK license"
  },
  {
    "id": 760,
    "content": "Any amendment or waiver under this Agreement shall be in writing and signed by representatives of both parties"
  },
  {
    "id": 762,
    "content": "CUDA Toolkit Supplement to Software License Agreement for NVIDIA Software Development Kits  The terms in this supplement govern your use of the NVIDIA CUDA Toolkit SDK under the terms of your license agreement (“Agreement”) as modified by this supplement Capitalized terms used but not defined below have the meaning assigned to them in the Agreement This supplement is an exhibit to the Agreement"
  },
  {
    "id": 763,
    "content": "and is incorporated as an integral part of the Agreement In the event of conflict between the terms in this supplement and the terms in the Agreement, the terms in this supplement govern"
  },
  {
    "id": 766,
    "content": "License Scope  The SDK is licensed for you to develop applications only for use in systems with NVIDIA GPUs"
  },
  {
    "id": 769,
    "content": "Distribution  The portions of the SDK that are distributable under the Agreement are listed in Attachment A"
  },
  {
    "id": 772,
    "content": "Operating Systems  Those portions of the SDK designed exclusively for use on the Linux or FreeBSD operating systems, or other operating systems derived from the source code to these operating systems, may be copied and redistributed for use in accordance with this Agreement, provided that the object code files are not modified in any way (except for unzipping of compressed files)"
  },
  {
    "id": 775,
    "content": "Audio and Video Encoders and Decoders  You acknowledge and agree that it is your sole responsibility to obtain any additional third-party licenses required to make, have made, use, have used, sell, import, and offer for sale your products or services that include or incorporate any third-party software and content relating to audio and/or video encoders and decoders from, including but not"
  },
  {
    "id": 780,
    "content": "NVIDIA does not grant to you under this Agreement any necessary patent or other rights with respect to any audio and/or video encoders and decoders"
  },
  {
    "id": 783,
    "content": "Licensing  If the distribution terms in this Agreement are not suitable for your organization, or for any questions regarding this Agreement, please contact NVIDIA at nvidia-compute-license-questions @ nvidia"
  },
  {
    "id": 787,
    "content": "Attachment A  The following CUDA Toolkit files may be distributed with applications developed by you, including certain variations of these files that have version number or architecture specific information embedded in the file name - as an example only, for release version 9"
  },
  {
    "id": 792,
    "content": "org/licenses/gpl txt Consistent with these licensing requirements, the software listed below is provided under the terms of the specified open source software licenses To obtain source code for software provided under licenses that require redistribution of source code, including the GNU General Public License (GPL) and GNU Lesser General Public License (LGPL), contact oss-requests @ nvidia"
  },
  {
    "id": 793,
    "content": "This offer is valid for a period of three (3) years from the date of the distribution of this product by NVIDIA CORPORATION"
  },
  {
    "id": 794,
    "content": "Component License CUDA-GDB GPL v3 Licensee represents and warrants that any and all third party licensing and/or royalty payment obligations in connection with Licensee’s use of the H 264 video codecs are solely the responsibility of Licensee Licensee’s use of the Thrust library is subject to the terms and conditions of the Apache License Version 2"
  },
  {
    "id": 802,
    "content": "html In addition, Licensee acknowledges the following notice: Thrust includes source code from the Boost Iterator, Tuple, System, and Random Number libraries"
  },
  {
    "id": 803,
    "content": "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN"
  },
  {
    "id": 805,
    "content": "Licensee’s use of the LLVM third party component is subject to the following terms and conditions: = LLVM Release License = University of Illinois/NCSA Open Source License Copyright (c) 2003-2010 University of Illinois at Urbana-Champaign Developed by: LLVM Team University of Illinois at Urbana-Champaign http: llvm org Permission is hereby granted, free of charge, to any person obtaining a copy"
  },
  {
    "id": 806,
    "content": "of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must"
  },
  {
    "id": 807,
    "content": "retain the above copyright notice, this list of conditions and the following disclaimers * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution * Neither the names of the LLVM Team, University of Illinois at Urbana- Champaign, nor the names of its"
  },
  {
    "id": 808,
    "content": "contributors may be used to endorse or promote products derived from this Software without specific prior written permission THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY"
  },
  {
    "id": 809,
    "content": "CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE Licensee’s use of the PCRE third party component is subject to the following terms and conditions: - PCRE LICENCE - PCRE is a library of functions to support regular expressions whose syntax and semantics are"
  },
  {
    "id": 810,
    "content": "as close as possible to those of the Perl 5 language Release 8 of PCRE is distributed under the terms of the \"BSD\" licence, as specified below The documentation for PCRE, supplied in the \"doc\" directory, is distributed under the same terms as the software itself Also included in the distribution is a set of C++ wrapper functions, and a just- in-time compiler that can be used to optimize pattern"
  },
  {
    "id": 815,
    "content": "PCRE JUST-IN-TIME COMPILATION SUPPORT - Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail hu Copyright(c) 2010-2012 Zoltan Herczeg All rights reserved STACK-LESS JUST-IN-TIME COMPILER - Written by: Zoltan Herczeg Email local part: hzmester Emain domain: freemail hu Copyright(c) 2009-2012 Zoltan Herczeg All rights reserved"
  },
  {
    "id": 816,
    "content": "THE \"BSD\" LICENCE - Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the"
  },
  {
    "id": 818,
    "content": "nor the names of their contributors may be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 819,
    "content": "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,"
  },
  {
    "id": 820,
    "content": "BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE"
  },
  {
    "id": 821,
    "content": "Some of the cuBLAS library routines were written by or derived from code written by Vasily Volkov and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2007-2009, Regents of the University of California All rights reserved"
  },
  {
    "id": 822,
    "content": "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer"
  },
  {
    "id": 823,
    "content": "* Neither the name of the University of California, Berkeley nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 824,
    "content": "THIS SOFTWARE IS PROVIDED BY THE AUTHOR \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS"
  },
  {
    "id": 825,
    "content": "OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE"
  },
  {
    "id": 826,
    "content": "Some of the cuBLAS library routines were written by or derived from code written by Davide Barbieri and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2008-2009 Davide Barbieri @ University of Rome Tor Vergata * The name of the author may not be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 827,
    "content": "Some of the cuBLAS library routines were derived from code developed by the University of Tennessee and are subject to the Modified Berkeley Software Distribution License as follows: Copyright (c) 2010 The University of Tennessee * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer listed in this license in the"
  },
  {
    "id": 828,
    "content": "documentation and/or other materials provided with the distribution * Neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission Some of the cuBLAS library routines were written by or derived from code written by Jonathan Hogg and are subject to the Modified Berkeley"
  },
  {
    "id": 829,
    "content": "Software Distribution License as follows: Copyright (c) 2012, The Science and Technology Facilities Council (STFC) * Neither the name of the STFC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission IN NO EVENT SHALL THE STFC BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR"
  },
  {
    "id": 830,
    "content": "CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE Some of the"
  },
  {
    "id": 833,
    "content": "0, as follows: -- (C) Copyright 2013 King Abdullah University of Science and Technology Authors: Ahmad Abdelfattah (ahmad ahmad@kaust"
  },
  {
    "id": 839,
    "content": "sa) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer"
  },
  {
    "id": 840,
    "content": "* Neither the name of the King Abdullah University of Science and Technology nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 841,
    "content": "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES"
  },
  {
    "id": 842,
    "content": "(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE Some of the cuSPARSE library"
  },
  {
    "id": 843,
    "content": "routines were written by or derived from code written by Li-Wen Chang and are subject to the NCSA Open Source License as follows: Copyright (c) 2012, University of Illinois"
  },
  {
    "id": 847,
    "content": "edu Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do"
  },
  {
    "id": 848,
    "content": "so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer * Neither the names of IMPACT Group, University of Illinois, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission"
  },
  {
    "id": 849,
    "content": "Some of the cuRAND library routines were written by or derived from code written by Mutsuo Saito and Makoto Matsumoto and are subject to the following license: Copyright (c) 2009, 2010 Mutsuo Saito, Makoto Matsumoto and Hiroshima University Copyright (c) 2011 Mutsuo Saito, Makoto Matsumoto, Hiroshima University and University of Tokyo * Neither the name of the Hiroshima University nor the names"
  },
  {
    "id": 850,
    "content": "of its contributors may be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 851,
    "content": "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer * Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer"
  },
  {
    "id": 853,
    "content": "Shaw Research nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 854,
    "content": "Some of the Math library routines were written by or derived from code developed by Norbert Juffa and are subject to the following license: Copyright (c) 2015-2017, Norbert Juffa All rights reserved"
  },
  {
    "id": 855,
    "content": "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1 Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in"
  },
  {
    "id": 857,
    "content": "IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE"
  },
  {
    "id": 858,
    "content": "OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE"
  },
  {
    "id": 859,
    "content": "Licensee’s use of the lz4 third party component is subject to the following terms and conditions: Copyright (C) 2011-2013, Yann Collet"
  },
  {
    "id": 863,
    "content": "php) Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer"
  },
  {
    "id": 864,
    "content": "The NPP library uses code from the Boost Math Toolkit, and is subject to the following license: Boost Software License - Version 1"
  },
  {
    "id": 866,
    "content": "Portions of the Nsight Eclipse Edition is subject to the following license: The Eclipse Foundation makes available all content in this plug-in (\"Content\") Unless otherwise indicated below, the Content is provided to you under the terms and conditions of the Eclipse Public License Version 1"
  },
  {
    "id": 868,
    "content": "If you did not receive this Content directly from the Eclipse Foundation, the Content is being redistributed by another party (\"Redistributor\") and different terms and conditions may apply to your use of any object code in the Content Unless otherwise indicated below, the terms and conditions of the EPL still apply to any source code in the Content and such source code may be obtained at http:"
  },
  {
    "id": 871,
    "content": "Some of the cuBLAS library routines uses code from OpenAI, which is subject to the following license: License URL https: github com/openai/openai-gemm/blob/master/LICENSE License Text The MIT License Copyright (c) 2016 OpenAI (http: openai"
  },
  {
    "id": 873,
    "content": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,"
  },
  {
    "id": 874,
    "content": "subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER"
  },
  {
    "id": 876,
    "content": "Licensee’s use of the Visual Studio Setup Configuration Samples is subject to the following license: The MIT License (MIT) Copyright (C) Microsoft Corporation Licensee’s use of linmath h header for CPU functions for GL vector/matrix operations from lunarG is subject to the Apache License Version 2"
  },
  {
    "id": 878,
    "content": "Components of the driver and compiler used for binary management, including nvFatBin, nvcc, and cuobjdump, use the Zstandard library which is subject to the following license: BSD License For Zstandard software Copyright (c) Meta Platforms, Inc"
  },
  {
    "id": 879,
    "content": "* Neither the name Facebook, nor Meta, nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 880,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation"
  },
  {
    "id": 882,
    "content": "5 | PDF | Archive CUDA Quick Start Guide Minimal first-steps instructions to get CUDA running on a standard system Introduction  This guide covers the basic instructions needed to install CUDA and verify that a CUDA application can run on each supported platform These instructions are intended to be used on a clean installation of a supported platform For questions which are not answered in this"
  },
  {
    "id": 883,
    "content": "document, please refer to the Windows Installation Guide and Linux Installation Guide The CUDA installation packages can be found on the CUDA Downloads Page"
  },
  {
    "id": 885,
    "content": "Windows  When installing CUDA on Windows, you can choose between the Network Installer and the Local Installer For more details, refer to the Windows Installation Guide"
  },
  {
    "id": 888,
    "content": "Network Installer  Perform the following steps to install CUDA and verify the installation Once the installation completes, click “next” to acknowledge the Nsight Visual Studio Edition installation summary"
  },
  {
    "id": 889,
    "content": "Navigate to the Samples’ nbody directory in https: github com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/nbody"
  },
  {
    "id": 890,
    "content": "Open the nbody Visual Studio solution file for the version of Visual Studio you have installed, for example, nbody_vs2019"
  },
  {
    "id": 892,
    "content": "Note Run samples by navigating to the executable’s location, otherwise it will fail to locate dependent resources"
  },
  {
    "id": 895,
    "content": "Local Installer  Perform the following steps to install CUDA and verify the installation Once the installation completes, click next to acknowledge the Nsight Visual Studio Edition installation summary Open the nbody Visual Studio solution file for the version of Visual Studio you have installed"
  },
  {
    "id": 898,
    "content": "Pip Wheels - Windows  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python"
  },
  {
    "id": 899,
    "content": "These packages are intended for runtime use and do not currently include developer tools (these can be installed separately)"
  },
  {
    "id": 900,
    "content": "Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA"
  },
  {
    "id": 901,
    "content": "NGC PyPI repo If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules If these Python modules are out-of-date then the commands which follow later in this section may fail py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module py -m pip install nvidia-pyindex If your project"
  },
  {
    "id": 902,
    "content": "is using a requirements txt file, then you can add the following line to your requirements txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https: pypi ngc nvidia com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install"
  },
  {
    "id": 903,
    "content": "nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12"
  },
  {
    "id": 904,
    "content": "nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125"
  },
  {
    "id": 907,
    "content": "Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3 Linux  CUDA on Linux can be installed using an RPM, Debian, Runfile, or Conda package, depending on the platform being installed on"
  },
  {
    "id": 910,
    "content": "Linux x86_64  For development on the x86_64 architecture See the Linux Installation Guide for more details"
  },
  {
    "id": 914,
    "content": "Redhat / CentOS  When installing CUDA on Redhat or CentOS, you can choose between the Runfile Installer and the RPM Installer"
  },
  {
    "id": 923,
    "content": "Enable optional repos : On RHEL 8 Linux only, execute the following steps to enable optional repositories"
  },
  {
    "id": 924,
    "content": "Disable the Nouveau drivers: Create a file at /etc/modprobe d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters"
  },
  {
    "id": 925,
    "content": "Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux"
  },
  {
    "id": 927,
    "content": "conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12 5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12 5/lib64\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples"
  },
  {
    "id": 928,
    "content": "from https: github com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https: github com/NVIDIA/cuda-samples/tree/master/Samples/nbody"
  },
  {
    "id": 932,
    "content": "Fedora  When installing CUDA on Fedora, you can choose between the Runfile Installer and the RPM Installer"
  },
  {
    "id": 937,
    "content": "Disable the Nouveau drivers: Create a file at /usr/lib/modprobe d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the below command: sudo grub2-mkconfig -o /boot/grub2/grub"
  },
  {
    "id": 938,
    "content": "cfg Reboot the system: sudo reboot Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters"
  },
  {
    "id": 939,
    "content": "Run the installer silently to install with the default selections (implies acceptance of the EULA): sudo sh cuda__linux"
  },
  {
    "id": 941,
    "content": "conf file to use the NVIDIA GPU for display: sudo nvidia-xconfig Reboot the system to load the graphical interface"
  },
  {
    "id": 942,
    "content": "Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables: export PATH=/usr/local/cuda-12 5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12 5/lib64\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https: github com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https: github"
  },
  {
    "id": 947,
    "content": "SUSE Linux Enterprise Server  When installing CUDA on SUSE Linux Enterprise Server, you can choose between the Runfile Installer and the RPM Installer"
  },
  {
    "id": 952,
    "content": "Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters"
  },
  {
    "id": 957,
    "content": "Install the repository meta-data, refresh the Zypper cache, and install CUDA: sudo rpm --install cuda-repo-- rpm sudo rpm --erase gpg-pubkey-7fa2af80* sudo zypper refresh sudo zypper install cuda Add the user to the video group: sudo usermod -a -G video Reboot the system to load the NVIDIA drivers: sudo reboot Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:"
  },
  {
    "id": 958,
    "content": "export PATH=/usr/local/cuda-12 5/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12 5/lib64\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Install a writable copy of the samples from https: github com/nvidia/cuda-samples , then build and run the nbody sample using the Linux instructions in https: github com/NVIDIA/cuda-samples/tree/master/Samples/nbody"
  },
  {
    "id": 963,
    "content": "Disable the Nouveau drivers: Create a file at /etc/modprobe d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd Reboot into runlevel 3 by temporarily adding the number “3” and the word “nomodeset” to the end of the system’s kernel boot parameters"
  },
  {
    "id": 964,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo"
  },
  {
    "id": 969,
    "content": "Local Repo Installation for Amazon Linux  Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*"
  },
  {
    "id": 975,
    "content": "Network Repo Installation for Amazon Linux  Enable the network repository and clean the DN cache: sudo dnf config-manager --add-repo https: developer"
  },
  {
    "id": 983,
    "content": "Common Installation Instructions for Amazon Linux  These instructions apply to both local and network installation for Amazon Linux"
  },
  {
    "id": 984,
    "content": "Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda"
  },
  {
    "id": 987,
    "content": "For pre-existing projects which use libcuda so , it may be useful to add a symbolic link from libcuda"
  },
  {
    "id": 993,
    "content": "Pip Wheels - Linux  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module python3 -m pip install nvidia-pyindex If your project is using a requirements txt file, then you can add the following line to your requirements txt file as an"
  },
  {
    "id": 997,
    "content": "com Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12"
  },
  {
    "id": 998,
    "content": "nvidia-cuda-nvcc-cu12 nvidia-nvml-dev-cu12 nvidia-cuda-nvrtc-cu12 nvidia-nvtx-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cublas-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvjpeg-cu12 nvidia-opencl-cu12 nvidia-nvjitlink-cu12 These metapackages install the following packages: nvidia-nvml-dev-cu125 nvidia-cuda-nvcc-cu125"
  },
  {
    "id": 999,
    "content": "nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-nvtx-cu125 nvidia-cuda-nvrtc-cu125 nvidia-npp-cu125 nvidia-cusparse-cu125 nvidia-cusolver-cu125 nvidia-curand-cu125 nvidia-cufft-cu125 nvidia-nvjpeg-cu125 nvidia-opencl-cu125 nvidia-nvjitlink-cu125 3"
  },
  {
    "id": 1002,
    "content": "Installation To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia Uninstallation To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 3"
  },
  {
    "id": 1005,
    "content": "Install repository meta-data sudo dpkg -i cuda-repo-__ deb Update the CUDA public GPG key sudo apt-key del 7fa2af80 When installing using the local repo: sudo cp /var/cuda-repo-ubuntu2004-12-0-local/cuda-*-keyring gpg /usr/share/keyrings/ When installing using the network repo: wget https: developer"
  },
  {
    "id": 1016,
    "content": "pin sudo mv cuda- pin /etc/apt/preferences d/cuda-repository-pin-600 Update the Apt repository cache and install CUDA sudo apt-get update sudo apt-get install cuda 3"
  },
  {
    "id": 1019,
    "content": "Ubuntu  When installing CUDA on Ubuntu, you can choose between the Runfile Installer and the Debian Installer In the case of the Debian installers, the instructions for the Local and Network variants are the same"
  },
  {
    "id": 1027,
    "content": "10 Debian  When installing CUDA on Debian 10, you can choose between the Runfile Installer and the Debian Installer"
  },
  {
    "id": 1028,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 1029,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 1031,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 1032,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 1033,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 1034,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 1035,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 1036,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 1037,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 1038,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 1039,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 1040,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 1041,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 1048,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 1050,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 1051,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2015-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 1057,
    "content": "5 | PDF | Archive CUDA Installation Guide for Microsoft Windows The installation instructions for the CUDA Toolkit on Microsoft Windows systems"
  },
  {
    "id": 1059,
    "content": "It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU)"
  },
  {
    "id": 1060,
    "content": "CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms With CUDA C/C++, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation"
  },
  {
    "id": 1062,
    "content": "This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources"
  },
  {
    "id": 1064,
    "content": "The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus"
  },
  {
    "id": 1067,
    "content": "CUDA Driver will continue to support running 32-bit application binaries on GeForce GPUs until Ada Support for running x86 32-bit applications on x86_64 Windows is limited to use with: CUDA Driver CUDA Runtime (cudart) CUDA Math Library (math"
  },
  {
    "id": 1070,
    "content": "About This Document  This document is intended for readers familiar with Microsoft Windows operating systems and the Microsoft Visual Studio environment"
  },
  {
    "id": 1073,
    "content": "Installing CUDA Development Tools  Basic instructions can be found in the Quick Start Guide The setup of CUDA development tools on a system running the appropriate version of Windows consists of a few simple steps: Verify the system has a CUDA-capable GPU"
  },
  {
    "id": 1077,
    "content": "Verify You Have a CUDA-Capable GPU  You can verify that you have a CUDA-capable GPU through the Display Adapters section in the Windows Device Manager If you have an NVIDIA card that is listed in https: developer nvidia com/cuda-gpus , that GPU is CUDA-capable"
  },
  {
    "id": 1078,
    "content": "The Windows Device Manager can be opened via the following steps: Open a run window from the Start Menu Run: control /name Microsoft"
  },
  {
    "id": 1082,
    "content": "com/cuda-downloads Choose the platform you are using and one of the following installer formats: Network Installer: A minimal installer which later downloads packages required for installation Full Installer: An installer which contains all the components of the CUDA Toolkit and does not require any further download This installer is useful for systems which lack network access and for enterprise"
  },
  {
    "id": 1084,
    "content": "The CUDA Toolkit installs the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources"
  },
  {
    "id": 1085,
    "content": "Download Verification The download can be verified by comparing the MD5 checksum posted at https: developer download"
  },
  {
    "id": 1090,
    "content": "txt with that of the downloaded file If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again"
  },
  {
    "id": 1093,
    "content": "Install the CUDA Software  Before installing the toolkit, you should read the Release Notes, as they provide details on installation and software functionality If you have not installed a stand-alone driver, install the driver from the NVIDIA CUDA Toolkit Graphical Installation Install the CUDA Software by executing the CUDA installer and following the on-screen prompts Silent Installation The"
  },
  {
    "id": 1096,
    "content": "Table 2 Possible Subpackage Names  Subpackage Name Subpackage Description Toolkit Subpackages (defaults to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12"
  },
  {
    "id": 1100,
    "content": "5 The CUDA Profiling Tools Interface for creating profiling and tracing tools that target CUDA applications"
  },
  {
    "id": 1102,
    "content": "5 CUDA HTML and PDF documentation files including the CUDA C++ Programming Guide, CUDA C++ Best Practices Guide, CUDA library documentation, etc"
  },
  {
    "id": 1112,
    "content": "Driver Use the -n option if you do not want to reboot automatically after install or uninstall, even if reboot is required"
  },
  {
    "id": 1113,
    "content": "Extracting and Inspecting the Files Manually Sometimes it may be desirable to extract or inspect the installable files directly, such as in enterprise deployment, or to browse the files before installation"
  },
  {
    "id": 1114,
    "content": "The full installation package can be extracted using a decompression tool which supports the LZMA compression method, such as 7-zip or WinZip"
  },
  {
    "id": 1115,
    "content": "Once extracted, the CUDA Toolkit files will be in the CUDAToolkit folder, and similarily for CUDA Visual Studio Integration"
  },
  {
    "id": 1119,
    "content": "Note Accessing the files in this manner does not set up any environment settings, such as variables or Visual Studio integration"
  },
  {
    "id": 1124,
    "content": "Uninstalling the CUDA Software  All subpackages can be uninstalled through the Windows Control Panel by using the Programs and Features widget"
  },
  {
    "id": 1127,
    "content": "Using Conda to Install the CUDA Software  This section describes the installation and configuration of CUDA when using the Conda installer The Conda packages are available at https: anaconda"
  },
  {
    "id": 1137,
    "content": "Installation  To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda - c nvidia 2"
  },
  {
    "id": 1140,
    "content": "Uninstallation  To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 2"
  },
  {
    "id": 1143,
    "content": "Installing Previous CUDA Releases  All Conda packages released under a specific CUDA version are labeled with that release version To install a previous version, include that label in the install command such as: conda install cuda - c nvidia / label / cuda -11"
  },
  {
    "id": 1145,
    "content": "0 Note Some CUDA releases do not move to new versions of all installable components When this is the case these components will be moved to the new label, and you may need to modify the install command to include both labels such as: conda install cuda - c nvidia / label / cuda -11"
  },
  {
    "id": 1154,
    "content": "Use a Suitable Driver Model  On Windows 10 and later, the operating system provides two driver models under which the NVIDIA Driver may operate: The WDDM driver model is used for display devices The Tesla Compute Cluster (TCC) mode of the NVIDIA Driver is available for non-display devices such as NVIDIA Tesla GPUs and the GeForce GTX Titan GPUs; it uses the Windows WDM driver model To check"
  },
  {
    "id": 1155,
    "content": "which driver mode is in use and/or to switch driver modes, use the nvidia-smi tool that is included with the NVIDIA Driver installation (see nvidia-smi -h for details) Note Keep in mind that when TCC mode is enabled for a particular GPU, that GPU cannot be used as a display device Note NVIDIA GeForce GPUs (excluding GeForce GTX Titan GPUs) do not support TCC mode"
  },
  {
    "id": 1158,
    "content": "Verify the Installation  Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware"
  },
  {
    "id": 1163,
    "content": "Running the Compiled Examples  The version of the CUDA Toolkit can be checked by running nvcc -V in a Command Prompt window You can display a Command Prompt window by going to: Start > All Programs > Accessories > Command Prompt CUDA Samples are located in https: github com/nvidia/cuda-samples To use the samples, clone the project, build the samples, and run them using the instructions on the"
  },
  {
    "id": 1164,
    "content": "Github page To verify a correct configuration of the hardware and software, it is highly recommended that you build and run the deviceQuery sample program"
  },
  {
    "id": 1165,
    "content": "If CUDA is installed and configured correctly, the output should look similar to Figure 1 Figure 1 Valid Results from deviceQuery CUDA Sample  The exact appearance and the output lines might be different on your system"
  },
  {
    "id": 1166,
    "content": "The important outcomes are that a device was found, that the device(s) match what is installed in your system, and that the test passed If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, ensure the deivce and driver are properly installed Running the bandwidthTest program, located in the same directory as deviceQuery above,"
  },
  {
    "id": 1167,
    "content": "ensures that the system and the CUDA-capable device are able to communicate correctly Figure 2 Valid Results from bandwidthTest CUDA Sample  The device name (second line) and the bandwidth numbers vary from system to system The important items are the second line, which confirms a CUDA device was found, and the second-to-last line, which confirms that all necessary tests passed If the tests do"
  },
  {
    "id": 1168,
    "content": "not pass, make sure you do have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed"
  },
  {
    "id": 1169,
    "content": "To see a graphical representation of what CUDA can do, run the particles sample at https: github com/NVIDIA/cuda-samples/tree/master/Samples/particles 3"
  },
  {
    "id": 1170,
    "content": "Pip Wheels  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python"
  },
  {
    "id": 1171,
    "content": "These packages are intended for runtime use and do not currently include developer tools (these can be installed separately)"
  },
  {
    "id": 1172,
    "content": "Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA"
  },
  {
    "id": 1173,
    "content": "NGC PyPI repo If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules If these Python modules are out-of-date then the commands which follow later in this section may fail py -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module py -m pip install nvidia-pyindex If your project"
  },
  {
    "id": 1174,
    "content": "is using a requirements txt file, then you can add the following line to your requirements txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https: pypi ngc nvidia com Procedure Install the CUDA runtime package: py -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: py -m pip install"
  },
  {
    "id": 1175,
    "content": "nvidia- Metapackages The following metapackages will install the latest version of the named component on Windows for the indicated CUDA version nvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-npp-cu12"
  },
  {
    "id": 1176,
    "content": "nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 nvidia-opencl-cu12 These metapackages install the following packages: nvidia-cublas-cu125 nvidia-cuda-runtime-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-nvrtc-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125"
  },
  {
    "id": 1177,
    "content": "nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 nvidia-opencl-cu125 4"
  },
  {
    "id": 1178,
    "content": "Compiling CUDA Programs  The project files in the CUDA Samples have been designed to provide simple, one-click builds of the programs that include all source code"
  },
  {
    "id": 1187,
    "content": "com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/bandwidthTest If you elected to use the default installation location, the output is placed in CUDA Samples\\v12"
  },
  {
    "id": 1192,
    "content": "Sample Projects  The sample projects come in two configurations: debug and release (where release contains no debugging information) and different Visual Studio projects These sample projects also make use of the $CUDA_PATH environment variable to locate where the CUDA Toolkit and the associated"
  },
  {
    "id": 1199,
    "content": "props file Install Directory Visual Studio 2015 (deprecated) C:Program Files (x86)\\MSBuild\\Microsoft"
  },
  {
    "id": 1201,
    "content": "0\\V140\\BuildCustomizations Visual Studio 2017 \\Common7\\IDE\\VC\\VCTargets\\BuildCustomizations Visual Studio 2019 C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\MSBuild\\Microsoft\\VC\\v160\\BuildCustomizations Visual Studio 2022 C:\\Program Files\\Microsoft Visual Studio\\2022\\Professional\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations You can reference this CUDA 12"
  },
  {
    "id": 1206,
    "content": "Build Customizations for New Projects  When creating a new CUDA application, the Visual Studio project file must be configured to include CUDA build customizations To accomplish this, click File-> New | Project… NVIDIA-> CUDA->, then select a template for your CUDA Toolkit version"
  },
  {
    "id": 1207,
    "content": "For example, selecting the “CUDA 12 5 Runtime” template will configure your project for use with the CUDA 12"
  },
  {
    "id": 1211,
    "content": "To specify a custom CUDA Toolkit location, under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field as desired"
  },
  {
    "id": 1215,
    "content": "Build Customizations for Existing Projects  When adding CUDA acceleration to existing applications, the relevant Visual Studio project files must be updated to include CUDA build customizations This can be done using one of the following two methods: Open the Visual Studio project, right click on the project name, and select Build Dependencies > Build Customizations… , then select the CUDA"
  },
  {
    "id": 1216,
    "content": "Toolkit version you would like to target Alternatively, you can configure your project always to build with the most recently installed version of the CUDA Toolkit Under CUDA C/C++ , select Common , and set the CUDA Toolkit Custom Dir field to $(CUDA_PATH) While Option 2 will allow your project to automatically use any new CUDA Toolkit version you may install in the future, selecting the toolkit"
  },
  {
    "id": 1217,
    "content": "version explicitly as in Option 1 is often better in practice, because if there are new CUDA configuration options added to the build customization rules accompanying the newer toolkit, you would not see those new options using Option 2 If you use the $(CUDA_PATH) environment variable to target a version of the CUDA Toolkit for building, and you perform an installation or uninstallation of any"
  },
  {
    "id": 1218,
    "content": "version of the CUDA Toolkit, you should validate that the $(CUDA_PATH) environment variable points to the correct installation directory of the CUDA Toolkit for your purposes You can access the value of the $(CUDA_PATH) environment variable via the following steps: Open a run window from the Start Menu This can done when adding the file by right clicking the project you wish to add the file to,"
  },
  {
    "id": 1219,
    "content": "selecting Add New Item , selecting NVIDIA CUDA 12 5\\CodeCUDA C/C++ File , and then selecting the file you wish to add For advanced users, if you wish to try building your project against a newer CUDA Toolkit without making changes to any of your project files, go to the Visual Studio command prompt, change the current directory to the location of your project, and execute a command such as the"
  },
  {
    "id": 1220,
    "content": "following: msbuild /t:Rebuild /p:CudaToolkitDir=\"drive:/path/to/new/toolkit/\" 5 Additional Considerations  Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C Programming Guide, located in the CUDA Toolkit documentation"
  },
  {
    "id": 1221,
    "content": "directory A number of helpful development tools are included in the CUDA Toolkit or are available for download from the NVIDIA Developer Zone to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Visual Studio Edition, and NVIDIA Visual Profiler For technical support on programming questions, consult and participate in the developer forums at https: developer nvidia com/cuda/"
  },
  {
    "id": 1225,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 1226,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 1228,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 1229,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 1230,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 1231,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 1232,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 1233,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 1234,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 1235,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 1236,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 1237,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 1238,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 1245,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 1247,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 1248,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 1256,
    "content": "Why do I see “error while loading shared libraries: : cannot open shared object file: No such file or directory” when I try to run a CUDA application that uses a CUDA library"
  },
  {
    "id": 1266,
    "content": "5 | PDF | Archive NVIDIA CUDA Installation Guide for Linux The installation instructions for the CUDA Toolkit on Linux"
  },
  {
    "id": 1268,
    "content": "It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU)"
  },
  {
    "id": 1269,
    "content": "CUDA was developed with several design goals in mind: Provide a small set of extensions to standard programming languages, like C, that enable a straightforward implementation of parallel algorithms With CUDA C/C﻿+﻿+, programmers can focus on the task of parallelization of the algorithms rather than spending time on their implementation"
  },
  {
    "id": 1271,
    "content": "This configuration also allows simultaneous computation on the CPU and GPU without contention for memory resources"
  },
  {
    "id": 1273,
    "content": "The on-chip shared memory allows parallel tasks running on these cores to share data without sending it over the system memory bus"
  },
  {
    "id": 1274,
    "content": "This guide will show you how to install and check the correct operation of the CUDA development tools"
  },
  {
    "id": 1277,
    "content": "System Requirements  To use NVIDIA CUDA on your system, you will need the following installed: CUDA-capable GPU A supported version of Linux with a gcc compiler and toolchain CUDA Toolkit (available at https: developer"
  },
  {
    "id": 1279,
    "content": "com/cuda-downloads ) The CUDA development environment relies on tight integration with the host development environment, including the host compiler and C runtime libraries, and is therefore only supported on distribution versions that have been qualified for this CUDA Toolkit release"
  },
  {
    "id": 1280,
    "content": "Table 1 Native Linux Distribution Support in CUDA 12 5 Update 1  Distribution Kernel 1 Default GCC GLIBC x86_64 RHEL 9"
  },
  {
    "id": 1286,
    "content": "About This Document  This document is intended for readers familiar with the Linux environment and the compilation of C programs from the command line"
  },
  {
    "id": 1289,
    "content": "Pre-installation Actions  Some actions must be taken before the CUDA Toolkit and Driver can be installed on Linux: Verify the system has a CUDA-capable GPU"
  },
  {
    "id": 1290,
    "content": "Note You can override the install-time prerequisite checks by running the installer with the -override flag"
  },
  {
    "id": 1294,
    "content": "Verify You Have a CUDA-Capable GPU  To verify that your GPU is CUDA-capable, go to your distribution’s equivalent of System Properties, or, from the command line, enter: lspci | grep -i nvidia If you do not see any settings, update the PCI hardware database that Linux maintains by entering update-pciids (generally found in /sbin ) at the command line and rerun the previous lspci command"
  },
  {
    "id": 1295,
    "content": "If your graphics card is from NVIDIA and it is listed in https: developer nvidia com/cuda-gpus , your GPU is CUDA-capable"
  },
  {
    "id": 1299,
    "content": "Verify You Have a Supported Version of Linux  The CUDA Development Tools are only supported on some specific distributions of Linux"
  },
  {
    "id": 1300,
    "content": "To determine which distribution and release number you’re running, type the following at the command line: uname -m && cat /etc/*release You should see output similar to the following, modified for your particular system: x86_64 Red Hat Enterprise Linux Workstation release 6 0 (Santiago) The x86_64 line indicates you are running on a 64-bit system"
  },
  {
    "id": 1304,
    "content": "Verify the System Has gcc Installed  The gcc compiler is required for development using the CUDA Toolkit It is generally installed as part of the Linux installation, and in most cases the version of gcc installed with a supported version of Linux will work correctly To verify the version of gcc installed on your system, type the following on the command line: gcc --version If an error message"
  },
  {
    "id": 1305,
    "content": "displays, you need to install the development tools from your Linux distribution or obtain a version of gcc and its accompanying toolchain from the Web"
  },
  {
    "id": 1308,
    "content": "Verify the System has the Correct Kernel Headers and Development Packages Installed  The CUDA Driver requires that the kernel headers and development packages for the running version of the kernel be installed at the time of the driver installation, as well whenever the driver is rebuilt For example, if your system is running kernel version 3"
  },
  {
    "id": 1312,
    "content": "4-301 kernel headers and development packages must also be installed While the Runfile installation performs no package validation, the RPM and Deb installations of the driver will make an attempt to install the kernel header and development packages if no version of these packages is currently installed However, it will install the latest version of these packages, which may or may not match the"
  },
  {
    "id": 1313,
    "content": "version of the kernel your system is using Therefore, it is best to manually ensure the correct version of the kernel headers and development packages are installed prior to installing the CUDA Drivers, as well as whenever you change the kernel version The version of the kernel your system is running can be found by running the following command: uname -r This is the version of the kernel headers"
  },
  {
    "id": 1314,
    "content": "and development packages that must be installed prior to installing the CUDA Drivers This command will be used multiple times below to specify the version of the packages to install More advanced cases, such as custom kernel branches, should ensure that their kernel headers and sources match the kernel build they are running Note If you perform a system update which changes the version of the"
  },
  {
    "id": 1315,
    "content": "Linux kernel being used, make sure to rerun the commands below to ensure you have the correct kernel headers and kernel development packages installed Otherwise, the CUDA Driver will fail to work with the new kernel"
  },
  {
    "id": 1318,
    "content": "Install GPUDirect Storage  If you intend to use GPUDirectStorage (GDS), you must install the CUDA package and MLNX_OFED package GDS is supported in two different modes: GDS (default/full perf mode) and Compatibility mode Compatibility mode is the only mode that is supported on certain distributions due to software dependency limitations Full GDS support is restricted to the following Linux"
  },
  {
    "id": 1326,
    "content": "5-1) and above is only supported with the NVIDIA open kernel driver Follow the instructions in Removing CUDA Toolkit and Driver to remove existing NVIDIA driver packages and then follow instructions in NVIDIA Open GPU Kernel Modules to install NVIDIA open kernel driver packages"
  },
  {
    "id": 1329,
    "content": "Choose an Installation Method  The CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages) The distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution’s native package"
  },
  {
    "id": 1330,
    "content": "management system The distribution-specific packages interface with the distribution’s native package management system Note For both native as well as cross development, the toolkit must be installed using the distribution-specific installer See the CUDA Cross-Platform Installation section for more details"
  },
  {
    "id": 1335,
    "content": "The CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources"
  },
  {
    "id": 1336,
    "content": "Download Verification The download can be verified by comparing the MD5 checksum posted at https: developer download"
  },
  {
    "id": 1341,
    "content": "txt with that of the downloaded file If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again"
  },
  {
    "id": 1342,
    "content": "Address Custom xorg conf, If Applicable  The driver relies on an automatically generated xorg conf file at /etc/X11/xorg conf If a custom-built xorg conf file is present, this functionality will be disabled and the driver may not work You can try removing the existing xorg conf file, or adding the contents of /etc/X11/xorg conf"
  },
  {
    "id": 1344,
    "content": "conf to the xorg conf file The xorg conf file will most likely need manual tweaking for systems with a non-trivial GPU configuration"
  },
  {
    "id": 1347,
    "content": "Handle Conflicting Installation Methods  Before installing CUDA, any previous installations that could conflict should be uninstalled"
  },
  {
    "id": 1348,
    "content": "This will not affect systems which have not had CUDA installed previously, or systems where the installation method has been preserved (RPM/Deb vs"
  },
  {
    "id": 1352,
    "content": "Overview  Installation using RPM or Debian packages interfaces with your system’s package management system When using RPM or Debian local repo installers, the downloaded package contains a repository snapshot stored on the local filesystem in /var/ Such a package only informs the package manager where to find the actual installation packages, but will not install them If the online network"
  },
  {
    "id": 1353,
    "content": "repository is enabled, RPM or Debian packages will be automatically downloaded at installation time using the package manager: apt-get, dnf, yum, or zypper"
  },
  {
    "id": 1354,
    "content": "Distribution-specific instructions detail how to install CUDA: RHEL 8 / Rocky Linux 8 RHEL 9 / Rocky Linux 9 KylinOS 10 Fedora SLES OpenSUSE WSL Ubuntu Debian Amazon Linux 2023 Finally, some helpful package manager capabilities are detailed"
  },
  {
    "id": 1355,
    "content": "Note Optional components such as nvidia-fs , libnvidia_nscq , and fabricmanager are not installed by default and will have to be installed separately as needed"
  },
  {
    "id": 1361,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) If matching kernel-headers and kernel-devel packages are not available for the currently running kernel version, you may need to use the previously shipped version of these packages"
  },
  {
    "id": 1362,
    "content": "Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau Any such third-party repositories must be added to the package manager repository database before installing the NVIDIA driver RPM packages, or missing dependencies will prevent the installation from proceeding"
  },
  {
    "id": 1367,
    "content": "rpm Enable optional repos: On RHEL 8 Linux only, execute the following steps to enable optional repositories On x86_64 systems: subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase"
  },
  {
    "id": 1372,
    "content": "Local Repo Installation for RHEL 8 / Rocky 8  Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-* rpm 3"
  },
  {
    "id": 1375,
    "content": "Network Repo Installation for RHEL 8 / Rocky 8  Enable the network repo: sudo dnf config-manager --add-repo https: developer"
  },
  {
    "id": 1378,
    "content": "com/compute/cuda/repos/$distro/$arch/cuda-$distro repo where $distro/$arch should be replaced by one of the following: rhel8/cross-linux-sbsa rhel8/sbsa rhel8/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685"
  },
  {
    "id": 1379,
    "content": "On a fresh installation of RHEL, the dnf package manager will prompt the user to accept new keys when installing packages the first time"
  },
  {
    "id": 1388,
    "content": "Common Instructions for RHEL 8 / Rocky 8  These instructions apply to both local and network installation"
  },
  {
    "id": 1389,
    "content": "Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda"
  },
  {
    "id": 1392,
    "content": "For pre-existing projects which use libcuda so , it may be useful to add a symbolic link from libcuda"
  },
  {
    "id": 1400,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Satisfy third-party package dependency: Satisfy DKMS dependency : The NVIDIA driver RPM packages depend on other external packages, such as DKMS and libvdpau"
  },
  {
    "id": 1405,
    "content": "rpm Enable optional repos: On RHEL 9 Linux only, execute the following steps to enable optional repositories On x86_64 systems: subscription-manager repos --enable=rhel-9-for-x86_64-appstream-rpms subscription-manager repos --enable=rhel-9-for-x86_64-baseos-rpms subscription-manager repos --enable=codeready-builder-for-rhel-9-x86_64-rpms Remove Outdated Signing Key: sudo rpm --erase"
  },
  {
    "id": 1410,
    "content": "Local Repo Installation for RHEL 9 / Rocky 9  Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-* rpm 3"
  },
  {
    "id": 1413,
    "content": "Network Repo Installation for RHEL 9 / Rocky 9  Enable the network repo: sudo dnf config-manager --add-repo https: developer"
  },
  {
    "id": 1416,
    "content": "com/compute/cuda/repos/$distro/$arch/cuda-$distro repo where $distro/$arch should be replaced by one of the following: rhel9/cross-linux-sbsa rhel9/sbsa rhel9/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685"
  },
  {
    "id": 1425,
    "content": "Common Instructions for RHEL 9 / Rocky 9  These instructions apply to both local and network installation"
  },
  {
    "id": 1431,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Choose an installation method: local repo or network repo"
  },
  {
    "id": 1435,
    "content": "Local Repo Installation for KylinOS  Install local repository on file system: sudo rpm --install cuda-repo-kylin10-X-Y-local-* rpm 3"
  },
  {
    "id": 1438,
    "content": "Network Repo Installation for KylinOS  Enable the network repo: sudo dnf config-manager --add-repo https: developer"
  },
  {
    "id": 1442,
    "content": "repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685"
  },
  {
    "id": 1443,
    "content": "Common Instructions for KylinOS 10  These instructions apply to both local and network installation"
  },
  {
    "id": 1449,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo"
  },
  {
    "id": 1453,
    "content": "Local Repo Installation for Fedora  Install local repository on file system: sudo rpm --install cuda-repo--X-Y-local-*"
  },
  {
    "id": 1459,
    "content": "Network Repo Installation for Fedora  Enable the network repo: sudo dnf config-manager --add-repo https: developer"
  },
  {
    "id": 1462,
    "content": "com/compute/cuda/repos/$distro/x86_64/cuda-$distro repo where $distro should be replaced by one of the following: fedora37 fedora39 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685"
  },
  {
    "id": 1463,
    "content": "On a fresh installation of Fedora, the dnf package manager will prompt the user to accept new keys when installing packages the first time"
  },
  {
    "id": 1472,
    "content": "Common Installation Instructions for Fedora  These instructions apply to both local and network installation for Fedora"
  },
  {
    "id": 1473,
    "content": "Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Note The CUDA driver installation may fail if the RPMFusion non-free repository is enabled In this case, CUDA installations should temporarily disable the RPMFusion non-free repository sudo dnf --disablerepo=\"rpmfusion-nonfree*\" install cuda It may be necessary to rebuild the grub configuration"
  },
  {
    "id": 1475,
    "content": "If so, then run this below command, and reboot the system: sudo grub2-mkconfig -o /boot/grub2/grub cfg Reboot the system: sudo reboot Add libcuda so symbolic link, if necessary: The libcuda"
  },
  {
    "id": 1477,
    "content": "The kernel development packages for the currently running kernel can be installed with: sudo zypper install -y kernel--devel= To run the above command, you will need the variant and version of the currently running kernel Use the output of the uname command to determine the currently running kernel’s variant and version: $ uname -r 3"
  },
  {
    "id": 1482,
    "content": "The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\\-default ') The kernel headers and development packages for the currently running kernel can be installed with: sudo zypper install -y kernel--devel= On SLES12 SP4, install the Mesa-libgl-devel Linux packages before proceeding"
  },
  {
    "id": 1483,
    "content": "Add the user to the video group: sudo usermod -a -G video Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo"
  },
  {
    "id": 1487,
    "content": "Local Repo Installation for SLES  Install local repository on file system: sudo rpm --install cuda-repo-sles15-X-Y-local-*"
  },
  {
    "id": 1495,
    "content": "com/compute/cuda/repos/$distro/$arch/cuda-$distro repo where $distro/$arch should be replaced by one of the following: sles15/cross-linux-sbsa sles15/sbsa sles15/x86_64 Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685"
  },
  {
    "id": 1496,
    "content": "On a fresh installation of SLES, the zypper package manager will prompt the user to accept new keys when installing packages the first time"
  },
  {
    "id": 1502,
    "content": "repo Refresh Zypper repository cache: sudo SUSEConnect --product PackageHub/15/ sudo zypper refresh 3"
  },
  {
    "id": 1505,
    "content": "Common Installation Instructions for SLES  These instructions apply to both local and network installation for SLES"
  },
  {
    "id": 1506,
    "content": "Install CUDA SDK: sudo zypper install cuda-toolkit Install CUDA Samples GL dependencies: Refer to CUDA Cross-Platform Samples"
  },
  {
    "id": 1512,
    "content": "The kernel development packages for the default kernel variant can be installed with: sudo zypper install -y kernel-default-devel=$(uname -r | sed 's/\\-default ') Add the user to the video group: sudo usermod -a -G video Remove Outdated Signing Key: sudo rpm --erase gpg-pubkey-7fa2af80* Choose an installation method: local repo or network repo"
  },
  {
    "id": 1516,
    "content": "Local Repo Installation for OpenSUSE  Install local repository on file system: sudo rpm --install cuda-repo-opensuse15-"
  },
  {
    "id": 1521,
    "content": "Network Repo Installation for OpenSUSE  Enable the network repo: sudo zypper addrepo https: developer"
  },
  {
    "id": 1525,
    "content": "repo Install the new CUDA public GPG key: The new GPG public key for the CUDA repository (RPM-based distros) is d42d0685"
  },
  {
    "id": 1526,
    "content": "On fresh installation of openSUSE, the zypper package manager will prompt the user to accept new keys when installing packages the first time"
  },
  {
    "id": 1535,
    "content": "Common Installation Instructions for OpenSUSE  These instructions apply to both local and network installation for OpenSUSE"
  },
  {
    "id": 1536,
    "content": "Install CUDA SDK: sudo zypper install cuda-toolkit Reboot the system: sudo reboot Perform the post-installation actions"
  },
  {
    "id": 1539,
    "content": "WSL  These instructions must be used if you are installing in a WSL environment Do not use the Ubuntu instructions in this case; it is important to not install the cuda-drivers packages within the WSL environment"
  },
  {
    "id": 1544,
    "content": "Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo"
  },
  {
    "id": 1548,
    "content": "Local Repo Installation for WSL  Install local repositiry on file system: sudo dpkg -i cuda-repo-wsl-ubuntu-X-Y-local_*_x86_64 deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo-wsl-ubuntu-X-Y-local/cuda-*-keyring"
  },
  {
    "id": 1552,
    "content": "Network Repo Installation for WSL  The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc"
  },
  {
    "id": 1553,
    "content": "This must be enrolled on the system, either using the cuda-keyring package or manually; the apt-key command is deprecated and not recommended"
  },
  {
    "id": 1554,
    "content": "Common Installation Instructions for WSL  These instructions apply to both local and network installation for WSL"
  },
  {
    "id": 1555,
    "content": "Update the Apt repository cache: sudo apt-get update Install CUDA SDK: sudo apt-get install cuda-toolkit Perform the post-installation actions"
  },
  {
    "id": 1561,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo"
  },
  {
    "id": 1565,
    "content": "Local Repo Installation for Ubuntu  Install local repository on file system: sudo dpkg -i cuda-repo-__ deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo--X-Y-local/cuda-*-keyring"
  },
  {
    "id": 1573,
    "content": "Network Repo Installation for Ubuntu  The new GPG public key for the CUDA repository is 3bf863cc Common Installation Instructions for Ubuntu  These instructions apply to both local and network installation for Ubuntu"
  },
  {
    "id": 1574,
    "content": "Update the Apt repository cache: sudo apt-get update Install CUDA SDK: Note These two commands must be executed separately sudo apt-get install cuda-toolkit To include all GDS packages: sudo apt-get install nvidia-gds Reboot the system sudo reboot Perform the Post-installation Actions 3"
  },
  {
    "id": 1576,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo apt-get install linux-headers-$(uname -r) Enable the contrib repository: sudo add-apt-repository contrib Remove Outdated Signing Key: sudo apt-key del 7fa2af80 Choose an installation method: local repo or network repo"
  },
  {
    "id": 1580,
    "content": "Local Repo Installation for Debian  Install local repository on file system: sudo dpkg -i cuda-repo--X-Y-local_*_x86_64 deb Enroll ephemeral public GPG key: sudo cp /var/cuda-repo--X-Y-local/cuda-*-keyring"
  },
  {
    "id": 1584,
    "content": "Network Repo Installation for Debian  The new GPG public key for the CUDA repository (Debian-based distros) is 3bf863cc"
  },
  {
    "id": 1590,
    "content": "deb where $distro/$arch should be replaced by one of the following: debian10/x86_64 debian11/x86_64 sudo dpkg -i cuda-keyring_1"
  },
  {
    "id": 1592,
    "content": "deb Or if you are unable to install the cuda-keyring package, you can optionally: Enroll the new signing key manually: wget https: developer"
  },
  {
    "id": 1595,
    "content": "com/compute/cuda/repos x86_64/cuda-archive-keyring gpg sudo mv cuda-archive-keyring gpg /usr/share/keyrings/cuda-archive-keyring gpg Enable the network repository: echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring gpg] https: developer"
  },
  {
    "id": 1604,
    "content": "Common Installation Instructions for Debian  These instructions apply to both local and network installation for Debian"
  },
  {
    "id": 1605,
    "content": "Update the Apt repository cache: sudo apt-get update Note If you are using Debian 10, you may instead need to run: sudo apt-get --allow-releaseinfo-change update Install CUDA SDK: sudo apt-get -y install cuda Reboot the system: sudo reboot Perform the post-installation actions"
  },
  {
    "id": 1611,
    "content": "The kernel headers and development packages for the currently running kernel can be installed with: sudo dnf install kernel-devel-$(uname -r) kernel-headers-$(uname -r) kernel-modules-extra-$(uname -r) Choose an installation method: local repo or network repo"
  },
  {
    "id": 1615,
    "content": "Local Repo Installation for Amazon Linux  Install local repository on file system: sudo rpm --install cuda-repo-amzn2023-X-Y-local-*"
  },
  {
    "id": 1620,
    "content": "Network Repo Installation for Amazon Linux  Enable the network repository: sudo dnf config-manager --add-repo https: developer"
  },
  {
    "id": 1627,
    "content": "Common Installation Instructions for Amazon Linux  These instructions apply to both local and network installation for Amazon Linux"
  },
  {
    "id": 1628,
    "content": "Install CUDA SDK: sudo dnf module install nvidia-driver:latest-dkms sudo dnf install cuda-toolkit Install GPUDirect Filesystem: sudo dnf install nvidia-gds Add libcuda"
  },
  {
    "id": 1633,
    "content": "Additional Package Manager Capabilities  Below are some additional capabilities of the package manager that users can take advantage of"
  },
  {
    "id": 1634,
    "content": "This package will install the full set of other CUDA packages required for native development and should cover most scenarios On supported platforms, the cuda-cross-aarch64 and cuda-cross-sbsa packages install all the packages required for cross-platform development to arm64-Jetson and arm64-Server, respectively The libraries and header files of the target architecture’s display driver package"
  },
  {
    "id": 1635,
    "content": "are also installed to enable the cross compilation of driver applications Note 32-bit compilation native and cross-compilation is removed from CUDA 12"
  },
  {
    "id": 1637,
    "content": "CUDA Driver will continue to support running existing 32-bit applications on existing GPUs except Hopper"
  },
  {
    "id": 1638,
    "content": "The packages installed by the packages above can also be installed individually by specifying their names explicitly The list of available packages be can obtained with: yum --disablerepo=\"*\" --enablerepo=\"cuda*\" list available # RedHat dnf --disablerepo=\"*\" --enablerepo=\"cuda*\" list available # Fedora zypper packages -r cuda # OpenSUSE & SLES cat /var/lib/apt/lists/*cuda*Packages | grep"
  },
  {
    "id": 1642,
    "content": "Meta Packages  Meta packages are RPM/Deb/Conda packages which contain no (or few) files but have multiple dependencies They are used to install many CUDA packages when you may not know the details of the packages you want Table 5 Meta Packages Available for CUDA 12 4  Meta Package Purpose cuda Installs all CUDA Toolkit and Driver packages cuda-toolkit-12-5 Installs all CUDA Toolkit packages"
  },
  {
    "id": 1643,
    "content": "required to develop CUDA applications cuda-runtime-12-5 Installs all CUDA Toolkit packages required to run CUDA applications, as well as the Driver packages"
  },
  {
    "id": 1650,
    "content": "rpm  These packages provide 32-bit driver libraries needed for things such as Steam (popular game app store/launcher), older video games, and some compute applications"
  },
  {
    "id": 1651,
    "content": "For Debian 10 and Debian 11: sudo dpkg --add-architecture i386 sudo apt-get update sudo apt-get install libcuda1-i386 nvidia-driver-libs-i386 For Debian 12: sudo dpkg --add-architecture i386 sudo apt-get update apt install nvidia-driver-libs:i386 For Ubuntu: sudo dpkg --add-architecture i386 sudo apt-get update sudo apt-get install libnvidia-compute-:i386 libnvidia-decode-:i386 \\"
  },
  {
    "id": 1652,
    "content": "libnvidia-encode-:i386 libnvidia-extra-:i386 libnvidia-fbc1-:i386 \\ libnvidia-gl-:i386 Where is the driver version, for example 495 For Fedora and RHEL8+: sudo dnf install nvidia-driver-cuda-libs"
  },
  {
    "id": 1653,
    "content": "i686 nvidia-driver-devel i686 \\ nvidia-driver-libs i686 nvidia-driver-NvFBCOpenGL i686 nvidia-driver-NVML i686 Note There is no modularity profile support"
  },
  {
    "id": 1654,
    "content": "For openSUSE/SLES: sudo zypper install nvidia-compute-G06-32bit nvidia-gl-G06-32bit nvidia-video-G06-32bit 3"
  },
  {
    "id": 1657,
    "content": "Package Upgrades  The cuda package points to the latest stable release of the CUDA Toolkit When a new version is available, use the following commands to upgrade the toolkit and driver: sudo dnf install cuda-toolkit # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-toolkit # OpenSUSE and SLES sudo apt-get install cuda-toolkit # Ubuntu and Debian The cuda-cross- packages can also be"
  },
  {
    "id": 1658,
    "content": "upgraded in the same manner The cuda-drivers package points to the latest driver release available in the CUDA repository When a new version is available, use the following commands to upgrade the driver: sudo dnf module install nvidia-driver:latest-dkms # Fedora, RHEL9, RHEL8, and KylinOS sudo zypper install cuda-drivers nvidia-gfxG04-kmp-default # OpenSUSE and SLES sudo apt-get install"
  },
  {
    "id": 1659,
    "content": "cuda-drivers # Ubuntu and Debian Some desktop environments, such as GNOME or KDE, will display a notification alert when new packages are available To avoid any automatic upgrade, and lock down the toolkit installation to the X Y release, install the cuda-X-Y or cuda-cross--X-Y package For instance, to install both the X Y CUDA Toolkit and the X Y+1 CUDA Toolkit, install the cuda-X Y and cuda-X"
  },
  {
    "id": 1662,
    "content": "Driver Installation  This section is for users who want to install a specific driver version For Debian and Ubuntu: sudo apt-get install cuda-drivers- For example: sudo apt-get install cuda-drivers-535 For OpenSUSE and SLES: sudo zypper -v install cuda-drivers- For example: sudo zypper -v install cuda-drivers-550 This allows you to get the highest version in the specified branch For Fedora and"
  },
  {
    "id": 1663,
    "content": "RHEL8+: sudo dnf module install nvidia-driver:/ where profile by default is “ default ” and does not need to be specified"
  },
  {
    "id": 1664,
    "content": "Example dkms streams: 450-dkms or latest-dkms Example precompiled streams: 450 or latest Note Precompiled streams are only supported on RHEL8 x86_64 and RHEL9 x86_64"
  },
  {
    "id": 1665,
    "content": "To uninstall or change streams on Fedora and RHEL8: sudo dnf module remove --all nvidia-driver sudo dnf module reset nvidia-driver 5 NVIDIA Open GPU Kernel Modules  The NVIDIA Linux GPU Driver contains several kernel modules: nvidia"
  },
  {
    "id": 1666,
    "content": "ko nvidia-modeset ko nvidia-uvm ko nvidia-drm ko nvidia-peermem ko Starting in the 515 driver release series, two “flavors” of these kernel modules are provided: Proprietary - this is the flavor that NVIDIA has historically shipped With every driver release, the source code to the open kernel modules will be published on https: github com/NVIDIA/open-gpu-kernel-modules and a tarball will be"
  },
  {
    "id": 1668,
    "content": "lspci | grep VGA Experimental support for GeForce and Quadro SKUs can be enabled with: echo \"options nvidia NVreg_OpenRmEnableUnsupportedGpus=1\" | sudo tee /etc/modprobe"
  },
  {
    "id": 1673,
    "content": "CUDA Runfile  Pass the CLI argument to the CUDA runfile to opt in to NVIDIA Open GPU Kernel Modules: sh cuda___linux run -m=kernel-open 5"
  },
  {
    "id": 1675,
    "content": "Debian  Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-kernel-open-dkms Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-kernel-open-dkms=-1 Install the rest of the NVIDIA driver packages: sudo apt-get install"
  },
  {
    "id": 1680,
    "content": "Fedora  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5"
  },
  {
    "id": 1682,
    "content": "KylinOS 10  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5"
  },
  {
    "id": 1684,
    "content": "RHEL 9 and Rocky 9  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5"
  },
  {
    "id": 1686,
    "content": "RHEL 8 and Rocky 8  Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:open-dkms OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package and the rest of the NVIDIA driver packages: sudo dnf module install nvidia-driver:-open 5"
  },
  {
    "id": 1688,
    "content": "OpenSUSE and SLES  Install the NVIDIA Open GPU Kernel Modules package: sudo zypper install nvidia-open-driver-G06-kmp-default Install the rest of the NVIDIA driver packages: sudo zypper install cuda-drivers OR to install a specific driver version Install the NVIDIA Open GPU Kernel Modules package: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp- | sed 's| ||g' | awk -F '|' '"
  },
  {
    "id": 1689,
    "content": "{print $2\"=\"$4}') Install the rest of the NVIDIA driver packages: sudo zypper -v install cuda-drivers- For example: sudo zypper -v install $(zypper search -s nvidia-open-driver-G06-kmp-default | sed 's| ||g' | awk -F '|' '/550/ {print $2\"=\"$4}') sudo zypper -v install cuda-drivers-550 5"
  },
  {
    "id": 1691,
    "content": "Ubuntu  Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install nvidia-driver--open Install the rest of the NVIDIA driver packages: sudo apt-get install cuda-drivers- Note End-users on Ubuntu should upgrade their NVIDIA Open GPU kernel modules using the following: sudo apt-get install --verbose-versions nvidia-kernel-source-550-open cuda-drivers-550 OR to install a specific"
  },
  {
    "id": 1692,
    "content": "driver version Install the NVIDIA Open GPU Kernel Modules package: sudo apt-get install -v nvidia-driver--open Install the rest of the NVIDIA driver packages: sudo apt-get install -v cuda-drivers- For example: sudo apt-get install -v nvidia-driver-550-open sudo apt-get install -v cuda-drivers-550 6"
  },
  {
    "id": 1693,
    "content": "Precompiled Streams  Precompiled streams offer an optional method of streamlining the installation process The advantages of precompiled streams: Precompiled: faster boot up after driver and/or kernel updates Pre-tested: kernel and driver combination has been validated Removes gcc dependency: no compiler installation required Removes dkms dependency: enabling EPEL repository not required Removes"
  },
  {
    "id": 1694,
    "content": "kernel-devel and kernel-headers dependencies: no black screen if matching packages are missing When using precompiled drivers, a plugin for the dnf package manager is enabled that cleans up stale"
  },
  {
    "id": 1696,
    "content": "To prevent system breakages, the NVIDIA dnf plugin also prevents upgrading to a kernel for which no precompiled driver yet exists"
  },
  {
    "id": 1697,
    "content": "This can delay the application of security fixes but ensures that a tested kernel and driver combination is always used A warning is displayed by dnf during that upgrade situation: NOTE: Skipping kernel installation since no NVIDIA driver kernel module package kmod-nvidia-${driver}-${kernel} latest-dkms always updates to the highest versioned driver (non-precompiled): sudo dnf module install"
  },
  {
    "id": 1698,
    "content": "nvidia-driver:latest-dkms Note This is the default stream -dkms locks the driver updates to the specified driver branch (non-precompiled): sudo dnf module install nvidia-driver:-dkms Note Valid streams include 520-dkms , 515-dkms , 470-dkms , and 450-dkms"
  },
  {
    "id": 1701,
    "content": "Precompiled Streams Support Matrix  This table shows the supported precompiled and legacy DKMS streams for each driver NVIDIA Driver Precompiled Stream Legacy DKMS Stream Open DKMS Stream Highest version latest latest-dkms open-dkms Locked at 520 x 520 520-dkms 520-open Locked at 515 x 515 515-dkms 515-open Prior to switching between module streams, first reset: sudo dnf module reset"
  },
  {
    "id": 1703,
    "content": "Modularity Profiles  Modularity profiles work with any supported modularity stream and allow for additional use cases List of nvidia-driver Module Profiles  Stream Profile Use Case Default /default Installs all the driver packages in a stream NVSwitch Fabric /fm Installs all the driver packages plus components required for bootstrapping an NVSwitch system (including the Fabric Manager and NSCQ"
  },
  {
    "id": 1704,
    "content": "telemetry) For example: sudo dnf module nvidia-driver:/default sudo dnf module nvidia-driver:/ks sudo dnf module nvidia-driver:/fm sudo dnf module nvidia-driver:/src You can install multiple modularity profiles using BASH curly brace expansion, for example: sudo dnf module install nvidia-driver:latest/{default,src} See https: developer nvidia"
  },
  {
    "id": 1705,
    "content": "com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams in the Developer Blog and https: developer"
  },
  {
    "id": 1712,
    "content": "This section describes the installation and configuration of CUDA when using the standalone installer The standalone installer is a “"
  },
  {
    "id": 1716,
    "content": "Runfile Overview  The Runfile installation installs the NVIDIA Driver and CUDA Toolkit via an interactive ncurses-based interface"
  },
  {
    "id": 1717,
    "content": "Distribution-specific instructions on disabling the Nouveau drivers as well as steps for verifying device node creation are also provided"
  },
  {
    "id": 1722,
    "content": "This can usually be accomplished by adding the number “3” to the end of the system’s kernel boot parameters"
  },
  {
    "id": 1724,
    "content": "Consult your system’s bootloader documentation for information on how to make the above boot parameter changes"
  },
  {
    "id": 1725,
    "content": "The reboot is required to completely unload the Nouveau drivers and prevent the graphical interface from loading The CUDA driver cannot be installed while the Nouveau drivers are loaded or while the graphical interface is active If the Nouveau drivers are still loaded, consult your distribution’s documentation to see if further steps are needed to disable Nouveau"
  },
  {
    "id": 1727,
    "content": "run The installer will prompt for the following: EULA Acceptance CUDA Driver installation CUDA Toolkit installation, location, and /usr/local/cuda symbolic link The default installation location for the toolkit is /usr/local/cuda-12 4 : The /usr/local/cuda symbolic link points to the location where the CUDA Toolkit was installed This link allows projects to use the latest CUDA Toolkit without any"
  },
  {
    "id": 1729,
    "content": "When the current privileges are insufficient to perform an action, the installer will ask for the user’s password to attempt to install with root privileges Actions that cause the installer to attempt to install with root privileges are: installing the CUDA Driver installing the CUDA Toolkit to a location the user does not have permission to write to creating the /usr/local/cuda symbolic link"
  },
  {
    "id": 1730,
    "content": "Running the installer with sudo , as shown above, will give permission to install to directories that require root permissions Directories and files created while running the installer with sudo will have root ownership If installing the driver, the installer will also ask if the openGL libraries should be installed If the GPU used for display is not an NVIDIA GPU, the NVIDIA openGL libraries"
  },
  {
    "id": 1731,
    "content": "should not be installed Otherwise, the openGL libraries used by the graphics driver of the non-NVIDIA GPU will be overwritten and the GUI will not work If performing a silent installation, the --no-opengl-libs option should be used to prevent the openGL libraries from being installed If the GPU used for display is an NVIDIA GPU, the X server configuration file, /etc/X11/xorg"
  },
  {
    "id": 1735,
    "content": "For non-standard systems, such as those with more than one GPU, it is recommended to manually edit the xorg"
  },
  {
    "id": 1738,
    "content": "so that was previously installed by the NVIDIA driver, so a reinstallation of the NVIDIA driver might be required after installing these libraries"
  },
  {
    "id": 1739,
    "content": "Reboot the system to reload the graphical interface: sudo reboot Verify the device nodes are created properly"
  },
  {
    "id": 1743,
    "content": "Disabling Nouveau  To install the Display Driver, the Nouveau drivers must first be disabled The Nouveau drivers are loaded if the following command prints anything: lsmod | grep nouveau 8"
  },
  {
    "id": 1747,
    "content": "d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force Run the following command: sudo grub2-mkconfig -o /boot/grub2/grub"
  },
  {
    "id": 1753,
    "content": "d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo dracut --force 8"
  },
  {
    "id": 1757,
    "content": "d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initrd: sudo /sbin/mkinitrd 8"
  },
  {
    "id": 1769,
    "content": "d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8"
  },
  {
    "id": 1773,
    "content": "d/blacklist-nouveau conf with the following contents: blacklist nouveau options nouveau modeset=0 Regenerate the kernel initramfs: sudo update-initramfs -u 8"
  },
  {
    "id": 1775,
    "content": "Device Node Verification  Check that the device files /dev/nvidia* exist and have the correct (0666) file permissions"
  },
  {
    "id": 1776,
    "content": "These files are used by the CUDA Driver to communicate with the kernel-mode portion of the NVIDIA Driver Applications that use the NVIDIA driver, such as a CUDA application or the X server (if any), will normally automatically create these files if they are missing using the setuid nvidia-modprobe tool that is bundled with the NVIDIA Driver However, some systems disallow setuid binaries, so if"
  },
  {
    "id": 1777,
    "content": "these files do not exist, you can create them manually by using a startup script such as the one below: # /bin/bash /sbin/modprobe nvidia if [ \"$"
  },
  {
    "id": 1779,
    "content": "NVDEVS=`lspci | grep -i NVIDIA` N3D=`echo \"$NVDEVS\" | grep \"3D controller\" | wc -l` NVGA=`echo \"$NVDEVS\" | grep \"VGA compatible controller\" | wc -l` N=`expr $N3D + $NVGA - 1` for i in `seq 0 $N`; do mknod -m 666 /dev/nvidia$i c 195 $i done mknod -m 666 /dev/nvidiactl c 195 255 else exit 1 fi /sbin/modprobe nvidia-uvm if [ \"$ \" -eq 0 ]; then # Find out the major device number used by the"
  },
  {
    "id": 1780,
    "content": "nvidia-uvm driver D=`grep nvidia-uvm /proc/devices | awk '{print $1}'` mknod -m 666 /dev/nvidia-uvm c $D 0 else exit 1 fi 8"
  },
  {
    "id": 1782,
    "content": "Advanced Options  Action Options Used Explanation Silent Installation --silent Required for any silent installation Performs an installation with no further user-input and minimal command-line output based on the options provided below"
  },
  {
    "id": 1783,
    "content": "At least one of --driver , --uninstall , and --toolkit must be passed if running with non-root permissions"
  },
  {
    "id": 1784,
    "content": "Extraction --extract= Extracts to the the following: the driver runfile, the raw files of the toolkit to This is especially useful when one wants to install the driver using one or more of the command-line options provided by the driver installer which are not exposed in this installer"
  },
  {
    "id": 1785,
    "content": "Overriding Installation Checks --override Ignores compiler, third-party library, and toolkit detection checks which would prevent the CUDA Toolkit from installing"
  },
  {
    "id": 1786,
    "content": "No OpenGL Libraries --no-opengl-libs Prevents the driver installation from installing NVIDIA’s GL libraries"
  },
  {
    "id": 1787,
    "content": "Overriding Kernel Source --kernel-source-path= Tells the driver installation to use as the kernel source directory when building the NVIDIA kernel module Running nvidia-xconfig --run-nvidia-xconfig Tells the driver installation to run nvidia-xconfig to update the system X configuration file so that the NVIDIA X driver is used This option should only be used to work around failures to build or"
  },
  {
    "id": 1789,
    "content": "Custom Temporary Directory Selection --tmpdir= Performs any temporary actions within instead of /tmp"
  },
  {
    "id": 1795,
    "content": "Uninstallation  To uninstall the CUDA Toolkit, run the uninstallation script provided in the bin directory of the toolkit"
  },
  {
    "id": 1796,
    "content": "By default, it is located in /usr/local/cuda-12 4/bin : sudo /usr/local/cuda-12 4/bin/cuda-uninstaller To uninstall the NVIDIA Driver, run nvidia-uninstall : sudo /usr/bin/nvidia-uninstall To enable the Nouveau drivers, remove the blacklist file created in the Disabling Nouveau section, and regenerate the kernel initramfs/initrd again as described in that section"
  },
  {
    "id": 1798,
    "content": "Conda Installation  This section describes the installation and configuration of CUDA when using the Conda installer The Conda packages are available at https: anaconda"
  },
  {
    "id": 1805,
    "content": "Installing CUDA Using Conda  To perform a basic install of all CUDA Toolkit components using Conda, run the following command: conda install cuda -c nvidia 9"
  },
  {
    "id": 1807,
    "content": "Uninstalling CUDA Using Conda  To uninstall the CUDA Toolkit using Conda, run the following command: conda remove cuda 9"
  },
  {
    "id": 1809,
    "content": "Installing Previous CUDA Releases  All Conda packages released under a specific CUDA version are labeled with that release version To install a previous version, include that label in the install command such as: conda install cuda -c nvidia/label/cuda-11"
  },
  {
    "id": 1813,
    "content": "Upgrading from cudatoolkit Package  If you had previously installed CUDA using the cudatoolkit package and want to maintain a similar install footprint, you can limit your installation to the following packages: cuda-libraries-dev cuda-nvcc cuda-nvtx cuda-cupti Note Some extra files, such as headers, will be included in this installation which were not included in the cudatoolkit package If you"
  },
  {
    "id": 1814,
    "content": "need to reduce your installation further, replace cuda-libraries-dev with the specific libraries you need"
  },
  {
    "id": 1816,
    "content": "Pip Wheels  NVIDIA provides Python Wheels for installing CUDA through pip, primarily for using CUDA with Python"
  },
  {
    "id": 1817,
    "content": "These packages are intended for runtime use and do not currently include developer tools (these can be installed separately)"
  },
  {
    "id": 1818,
    "content": "Please note that with this installation method, CUDA installation environment is managed via pip and additional care must be taken to set up your host environment to use CUDA outside the pip environment Prerequisites To install Wheels, you must first install the nvidia-pyindex package, which is required in order to set up your pip installation to fetch additional Python modules from the NVIDIA"
  },
  {
    "id": 1819,
    "content": "NGC PyPI repo If your pip and setuptools Python modules are not up-to-date, then use the following command to upgrade these Python modules If these Python modules are out-of-date then the commands which follow later in this section may fail python3 -m pip install --upgrade setuptools pip wheel You should now be able to install the nvidia-pyindex module python3 -m pip install nvidia-pyindex If your"
  },
  {
    "id": 1820,
    "content": "project is using a requirements txt file, then you can add the following line to your requirements txt file as an alternative to installing the nvidia-pyindex package: --extra-index-url https: pypi org/simple Procedure Install the CUDA runtime package: python3 -m pip install nvidia-cuda-runtime-cu12 Optionally, install additional packages as listed below using the following command: python3 -m pip"
  },
  {
    "id": 1821,
    "content": "install nvidia- Metapackages The following metapackages will install the latest version of the named component on Linux for the indicated CUDA version nvidia-cuda-runtime-cu12 nvidia-cuda-cccl-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvcc-cu12 nvidia-cuda-opencl-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cublas-cu12 nvidia-cuda-sanitizer-api-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12"
  },
  {
    "id": 1822,
    "content": "nvidia-cusparse-cu12 nvidia-npp-cu12 nvidia-nvfatbin-cu12 nvidia-nvjitlink-cu12 nvidia-nvjpeg-cu12 nvidia-nvml-dev-cu12 nvidia-nvtx-cu12 These metapackages install the following packages: nvidia-cuda-runtime-cu125 nvidia-cuda-cccl-cu125 nvidia-cuda-cupti-cu125 nvidia-cuda-nvcc-cu125 nvidia-cuda-opencl-cu125 nvidia-cublas-cu125 nvidia-cuda-sanitizer-api-cu125 nvidia-cuda-nvrtc-cu125"
  },
  {
    "id": 1823,
    "content": "nvidia-cufft-cu125 nvidia-curand-cu125 nvidia-cusolver-cu125 nvidia-cusparse-cu125 nvidia-npp-cu125 nvidia-nvfatbin-cu125 nvidia-nvjitlink-cu125 nvidia-nvjpeg-cu125 nvidia-nvml-dev-cu125 nvidia-nvtx-cu125 11"
  },
  {
    "id": 1824,
    "content": "Tarball and Zip Archive Deliverables  In an effort to meet the needs of a growing customer base requiring alternative installer packaging formats, as well as a means of input into community CI/CD systems, tarball and zip archives are available for each component These tarball and zip archives, known as binary archives, are provided at https: developer"
  },
  {
    "id": 1840,
    "content": "4 update 2) which includes the release date, the name of each component, license name, relative URL for each platform and checksums Package maintainers are advised to check the provided LICENSE for each component prior to redistribution"
  },
  {
    "id": 1844,
    "content": "Parsing Redistrib JSON  The following example of a JSON manifest contains keys for each component: name, license, version, and a platform array which includes relative_path, sha256, md5, and size (bytes) for each archive"
  },
  {
    "id": 1845,
    "content": "Importing Tarballs into CMake  The recommended module for importing these tarballs into the CMake build system is via FindCUDAToolkit (3"
  },
  {
    "id": 1847,
    "content": "The path to the extraction location can be specified with the CUDAToolkit_ROOT environmental variable"
  },
  {
    "id": 1852,
    "content": "Importing Tarballs into Bazel  The recommended method of importing these tarballs into the Bazel build system is using http_archive and pkg_tar For an example, see bazel/1_pkg_tar/"
  },
  {
    "id": 1854,
    "content": "CUDA Cross-Platform Environment  Cross development for arm64-sbsa is supported on Ubuntu 20 04, Ubuntu 22 04, RHEL 8, RHEL 9, and SLES 15 Cross development for arm64-Jetson is only supported on Ubuntu 20 04 We recommend selecting a host development environment that matches the supported cross-target environment"
  },
  {
    "id": 1855,
    "content": "This selection helps prevent possible host/target incompatibilities, such as GCC or GLIBC version mismatches"
  },
  {
    "id": 1858,
    "content": "CUDA Cross-Platform Installation  Some of the following steps may have already been performed as part of the native Ubuntu installation To install the native CUDA Toolkit on the target system, refer to the native Ubuntu installation section"
  },
  {
    "id": 1861,
    "content": "Update the Apt repository cache: sudo apt-get update Install the appropriate cross-platform CUDA Toolkit: For aarch64: sudo apt-get install cuda-cross-aarch64 For QNX: sudo apt-get install cuda-cross-qnx Perform the post-installation actions"
  },
  {
    "id": 1864,
    "content": "CUDA Cross-Platform Samples  CUDA Samples are now located in https: github com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples"
  },
  {
    "id": 1868,
    "content": "Mandatory Actions  Some actions must be taken after the installation before the CUDA Toolkit and Driver can be used"
  },
  {
    "id": 1872,
    "content": "Environment Setup  The PATH variable needs to include export PATH=/usr/local/cuda-12 4/bin${PATH:+:${PATH}}"
  },
  {
    "id": 1874,
    "content": "To add this path to the PATH variable: export PATH=/usr/local/cuda-12 4/bin${PATH:+:${PATH}} In addition, when using the runfile installation method, the LD_LIBRARY_PATH variable needs to contain /usr/local/cuda-12 4/lib64 on a 64-bit system, or /usr/local/cuda-12 4/lib on a 32-bit system To change the environment variables for 64-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12"
  },
  {
    "id": 1875,
    "content": "4/lib64\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} To change the environment variables for 32-bit operating systems: export LD_LIBRARY_PATH=/usr/local/cuda-12 4/lib\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Note that the above paths change when using a custom install path with the runfile installation method"
  },
  {
    "id": 1882,
    "content": "Install Persistence Daemon  NVIDIA is providing a user-space daemon on Linux to support persistence of driver state across CUDA job runs The daemon approach provides a more elegant and robust solution to this problem than persistence mode The NVIDIA Persistence Daemon can be started as the root user by running: /usr/bin/nvidia-persistenced --verbose This command should be run on boot"
  },
  {
    "id": 1887,
    "content": "Install Writable Samples  CUDA Samples are now located in https: github com/nvidia/cuda-samples , which includes instructions for obtaining, building, and running the samples"
  },
  {
    "id": 1891,
    "content": "Verify the Installation  Before continuing, it is important to verify that the CUDA toolkit can find and communicate correctly with the CUDA-capable hardware"
  },
  {
    "id": 1894,
    "content": "Note Ensure the PATH and, if using the runfile installation method, LD_LIBRARY_PATH variables are set correctly"
  },
  {
    "id": 1899,
    "content": "Verify the Driver Version  If you installed the driver, verify that the correct version of it is loaded If you did not install the driver, or are using an operating system where the driver is not loaded via a kernel module, such as L4T, skip this step When the driver is loaded, the driver version can be found by executing the command cat /proc/driver/nvidia/version Note that this command will"
  },
  {
    "id": 1907,
    "content": "If the CUDA software is installed and configured correctly, the output for deviceQuery should look similar to that shown in Figure 1 Valid Results from deviceQuery CUDA Sample  The exact appearance and the output lines might be different on your system"
  },
  {
    "id": 1908,
    "content": "The important outcomes are that a device was found (the first highlighted line), that the device matches the one on your system (the second highlighted line), and that the test passed (the final highlighted line)"
  },
  {
    "id": 1909,
    "content": "If a CUDA-capable device and the CUDA Driver are installed but deviceQuery reports that no CUDA-capable devices are present, this likely means that the /dev/nvidia* files are missing or have the wrong permissions"
  },
  {
    "id": 1910,
    "content": "On systems where SELinux is enabled, you might need to temporarily disable this security feature to run deviceQuery"
  },
  {
    "id": 1911,
    "content": "Running the bandwidthTest program ensures that the system and the CUDA-capable device are able to communicate correctly Valid Results from bandwidthTest CUDA Sample  Note that the measurements for your CUDA-capable device description will vary from system to system The important point is that you obtain measurements, and that the second-to-last line (in Figure 2 ) confirms that all necessary"
  },
  {
    "id": 1912,
    "content": "tests passed Should the tests not pass, make sure you have a CUDA-capable NVIDIA GPU on your system and make sure it is properly installed"
  },
  {
    "id": 1913,
    "content": "If you run into difficulties with the link step (such as libraries not being found), consult the Linux Release Notes found in https: github"
  },
  {
    "id": 1918,
    "content": "Install Nsight Eclipse Plugins  To install Nsight Eclipse plugins, an installation script is provided: /usr/local/cuda-12"
  },
  {
    "id": 1924,
    "content": "Local Repo Removal  Removal of the local repo installer is recommended after installation of CUDA SDK Ubuntu and Debian sudo apt-get remove --purge \"cuda-repo--X-Y-local*\" Fedora sudo dnf remove \"cuda-repo--X-Y-local*\" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove \"cuda-repo--X-Y-local*\" openSUSE 15 and SLES 15 sudo zypper remove \"cuda-repo--X-Y-local*\" Removal of the local"
  },
  {
    "id": 1925,
    "content": "repo installer is recommended after installation of NVIDA driver Ubuntu and Debian sudo apt-get remove --purge \"nvidia-driver-local-repo-*\" Fedora sudo dnf remove \"nvidia-driver-local-repo-*\" RHEL 9 / Rocky Linux 9 and RHEL 8 / Rocky Linux 8 sudo dnf remove \"nvidia-driver-local-repo-*\" openSUSE 15 and SLES 15 sudo zypper remove \"nvidia-driver-local-repo-*\" 13"
  },
  {
    "id": 1927,
    "content": "Optional Actions  Other options are not necessary to use the CUDA Toolkit, but are available to provide additional features"
  },
  {
    "id": 1931,
    "content": "Install Third-party Libraries  Some CUDA samples use third-party libraries which may not be installed by default on your system"
  },
  {
    "id": 1932,
    "content": "Install the Source Code for cuda-gdb  The cuda-gdb source must be explicitly selected for installation with the runfile installation method During the installation, in the component selection page, expand the component “CUDA Tools 12 4” and select cuda-gdb-src for installation To obtain a copy of the source code for cuda-gdb using the RPM and Debian installation methods, the cuda-gdb-src package"
  },
  {
    "id": 1938,
    "content": "Select the Active Version of CUDA  For applications that rely on the symlinks /usr/local/cuda and /usr/local/cuda-MAJOR , you may wish to change to a different installed version of CUDA using the provided alternatives To show the active version of CUDA and all available versions: update-alternatives --display cuda To show the active minor version of a given major CUDA release:"
  },
  {
    "id": 1939,
    "content": "update-alternatives --display cuda-12 To update the active version of CUDA: sudo update-alternatives --config cuda 14"
  },
  {
    "id": 1940,
    "content": "Advanced Setup  Below is information on some advanced setup scenarios which are not covered in the basic instructions above Table 8 Advanced Setup Scenarios when Installing CUDA  Scenario Instructions Install CUDA using the Package Manager installation method without installing the NVIDIA GL libraries Fedora Install CUDA using the following command: sudo dnf install cuda-toolkit-12-4 \\"
  },
  {
    "id": 1941,
    "content": "nvidia-driver-cuda akmod-nvidia Follow the instructions here to ensure that Nouveau is disabled If performing an upgrade over a previous installation, the NVIDIA kernel module may need to be rebuilt by following the instructions here OpenSUSE/SLES On some system configurations the NVIDIA GL libraries may need to be locked before installation using: sudo zypper addlock nvidia-glG04 Install CUDA"
  },
  {
    "id": 1942,
    "content": "using the following command: sudo zypper install --no-recommends cuda-toolkit-12-4 \\ nvidia-computeG04 \\ nvidia-gfxG04-kmp-default Follow the instructions here to ensure that Nouveau is disabled"
  },
  {
    "id": 1943,
    "content": "Instead, the driver packages integrate with the Bumblebee framework to provide a solution for users who wish to control what applications the NVIDIA drivers are used for Upgrade from a RPM/Deb driver installation which includes the diagnostic driver packages to a driver installation which does not include the diagnostic driver packages RHEL/CentOS Remove diagnostic packages using the following"
  },
  {
    "id": 1944,
    "content": "command: sudo yum remove cuda-drivers-diagnostic \\ xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal Fedora Remove diagnostic packages using the following command: sudo dnf remove cuda-drivers-diagnostic \\ xorg-x11-drv-nvidia-diagnostic Follow the instructions here to continue installation as normal OpenSUSE/SLES Remove diagnostic packages using the"
  },
  {
    "id": 1945,
    "content": "following command: sudo zypper remove cuda-drivers-diagnostic \\ nvidia-diagnosticG04 Follow the instructions here to continue installation as normal Ubuntu Remove diagnostic packages using the following command: sudo apt-get purge cuda-drivers-diagnostic \\ nvidia-384-diagnostic Follow the instructions here to continue installation as normal"
  },
  {
    "id": 1946,
    "content": "The Device entry should resemble the following: Section \"Device\" Identifier \"Device0\" Driver \"driver_name\" VendorName \"vendor_name\" BusID \"bus_id\" EndSection The details will you will need to add differ on a case-by-case basis For example, if you have two NVIDIA GPUs and you want the first GPU to be used for display, you would replace “ driver_name ” with “ nvidia ”, “ vendor_name ” with “ NVIDIA"
  },
  {
    "id": 1948,
    "content": "RPM The RPM packages don’t support custom install locations through the package managers (Yum and Zypper), but it is possible to install the RPM packages to a custom location using rpm’s --relocate parameter: sudo rpm --install --relocate /usr/local/cuda-12"
  },
  {
    "id": 1950,
    "content": "rpm You will need to install the packages in the correct dependency order; this task is normally taken care of by the package managers For example, if package “foo” has a dependency on package “bar”, you should install package “bar” first, and package “foo” second You can check the dependencies of a RPM package as follows: rpm -qRp package rpm Note that the driver packages cannot be relocated It"
  },
  {
    "id": 1951,
    "content": "is however possible to extract the contents of the Deb packages and move the files to the desired install location Runfile The Runfile can be extracted into the standalone Toolkit and Driver Runfiles by using the --extract parameter The Toolkit standalone Runfiles can be further extracted by running: /runfile run --tar mxvf The Driver Runfile can be extracted by running: /runfile run -x RPM The"
  },
  {
    "id": 1952,
    "content": "RPM packages can be extracted by running: rpm2cpio package rpm | cpio -idmv Deb The Deb packages can be extracted by running: dpkg-deb -x package deb output_dir Modify Ubuntu’s apt package manager to query specific architectures for specific repositories"
  },
  {
    "id": 1953,
    "content": "This is useful when a foreign architecture has been added, causing “404 Not Found” errors to appear when the repository meta-data is updated"
  },
  {
    "id": 1956,
    "content": "This is done by modifying the /etc/apt/sources list file and any files containing repositories you wish to restrict under the /etc/apt/sources list"
  },
  {
    "id": 1959,
    "content": "list An architecture-restricted repository entry looks like: deb [arch=,] For example, if you wanted to restrict a repository to only the amd64 and i386 architectures, it would look like: deb [arch=amd64,i386] It is not necessary to restrict the deb-src repositories, as these repositories don’t provide architecture-specific packages"
  },
  {
    "id": 1960,
    "content": "For example: nvidia: Unknown symbol drm_open (err 0) Check to see if there are any optionally installable modules that might provide these symbols which are not currently installed For the example of the drm_open symbol, check to see if there are any packages which provide drm_open and are not already installed"
  },
  {
    "id": 1962,
    "content": "04, the linux-image-extra package provides the DRM kernel module (which provides drm_open ) This package is optional even though the kernel headers reflect the availability of DRM regardless of whether this package is installed or not"
  },
  {
    "id": 1963,
    "content": "This can occur on systems with limited storage in the TMP directory (usually /tmp ), or on systems which use a tmpfs in memory to handle temporary storage"
  },
  {
    "id": 1964,
    "content": "In this case, the --tmpdir command-line option should be used to instruct the runfile to use a directory with sufficient space to extract into"
  },
  {
    "id": 1965,
    "content": "Wayland is disabled during installation of the Fedora driver RPM due to compatability issues To re-enable Wayland, comment out this line in /etc/gdm/custom"
  },
  {
    "id": 1966,
    "content": "conf : WaylandEnable=false In case of the error: E: Failed to fetch file:/var/cuda-repo File not found Debian and Ubuntu This can occur when installing CUDA after uninstalling a different version Use the following command before installation: sudo rm -v /var/lib/apt/lists/*cuda* /var/lib/apt/lists/*nvidia* Verbose installation on Debian and Ubuntu Use the --verbose-versions flag, for example:"
  },
  {
    "id": 1968,
    "content": " The Runfile installation asks where you wish to install the Toolkit during an interactive install If installing using a non-interactive install, you can use the --toolkitpath parameter to change the install location: /runfile run --silent \\ --toolkit --toolkitpath=/my/new/toolkit The RPM and Deb packages cannot be installed to a custom install location directly using the package managers See"
  },
  {
    "id": 1969,
    "content": "the “Install CUDA to a specific directory using the Package Manager installation method” scenario in the Advanced Setup section for more information Ensure that your PATH includes the bin directory where you installed the Toolkit, usually /usr/local/cuda-12 4/bin Ensure that your LD_LIBRARY_PATH includes the lib and/or lib64 directory where you installed the Toolkit, usually /usr/local/cuda-12"
  },
  {
    "id": 1970,
    "content": "4/lib{,64} : export LD_LIBRARY_PATH=/usr/local/cuda-12 4/lib\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 15"
  },
  {
    "id": 1972,
    "content": " These errors occur after adding a foreign architecture because apt is attempting to query for each architecture within each repository listed in the system’s sources"
  },
  {
    "id": 1977,
    "content": " To make sure X doesn’t use a certain GPU for display, you need to specify which other GPU to use for display For more information, please refer to the “Use a specific GPU for rendering the display” scenario in the Advanced Setup section"
  },
  {
    "id": 1978,
    "content": " After installing CUDA, set the driver value for the intel device in /etc/X11/xorg conf to ‘ modesetting ’ as shown below: Section \"Device\" Identifier \"intel\" Driver \"modesetting\" EndSection To prevent Ubuntu from reverting the change in xorg"
  },
  {
    "id": 1980,
    "content": "In many cases, a new Linux kernel will be installed without properly updating the required Linux kernel headers and development packages To ensure the CUDA driver continues to work when performing a system update, rerun the commands in the Kernel Headers and Development Packages section Additionally, on Fedora, the Akmods framework will sometimes fail to correctly rebuild the NVIDIA kernel module"
  },
  {
    "id": 1981,
    "content": "packages when a new Linux kernel is installed When this happens, it is usually sufficient to invoke Akmods manually and regenerate the module mapping files by running the following commands in a virtual console, and then rebooting: sudo akmods --force sudo depmod You can reach a virtual console by hitting ctrl+alt+f2 at the same time  To install a CUDA driver at a version earlier than 367 using a"
  },
  {
    "id": 1983,
    "content": "For example, to install 352 99, instead of installing the cuda-drivers metapackage at version 352 99, you will need to install all required packages of cuda-drivers at version 352 99  Depending on your system configuration, you may not be able to install old versions of CUDA using the cuda metapackage In order to install a specific version of CUDA, you may need to specify all of the packages"
  },
  {
    "id": 1984,
    "content": "that would normally be installed by the cuda metapackage at the version you want to install If you are using yum to install certain packages at an older version, the dependencies may not resolve as expected In this case you may need to pass “ --setopt=obsoletes=0 ” to yum to allow an install of packages which are obsoleted at a later version than you are trying to install"
  },
  {
    "id": 1985,
    "content": " This dependency comes from the SUSE repositories and shouldn’t affect the use of the NVIDIA driver or the CUDA Toolkit"
  },
  {
    "id": 1986,
    "content": "To disable this dependency, you can lock that package with the following command: sudo zypper al Mesa-dri-nouveau 15"
  },
  {
    "id": 1988,
    "content": "Run the following commands: sudo apt-get install glx-diversions --reinstall sudo apt-get remove nvidia-alternative Then re-run the commands from Removing CUDA Toolkit and Driver"
  },
  {
    "id": 1989,
    "content": "Additional Considerations  Now that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs"
  },
  {
    "id": 1990,
    "content": "To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C++ Programming Guide, located in /usr/local/cuda-12"
  },
  {
    "id": 1992,
    "content": "A number of helpful development tools are included in the CUDA Toolkit to assist you as you develop your CUDA programs, such as NVIDIA ® Nsight™ Eclipse Edition, NVIDIA Visual Profiler, CUDA-GDB, and CUDA-MEMCHECK"
  },
  {
    "id": 1993,
    "content": "For technical support on programming questions, consult and participate in the developer forums at https: forums developer"
  },
  {
    "id": 1997,
    "content": "Switching between Driver Module Flavors  Use the following steps to switch between the NVIDIA driver legacy and open module flavors on your system Note If switching to open module, experimental support for GeForce and Quadro SKUs can be enabled with: echo \"options nvidia NVreg_OpenRmEnableUnsupportedGpus=1\" | sudo tee /etc/modprobe"
  },
  {
    "id": 2000,
    "content": "Fedora, RHEL 9 / Rocky Linux 9, RHEL 8 / Rocky Linux 8 To switch between legacy and open: uninstall, then reinstall"
  },
  {
    "id": 2001,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 2002,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 2004,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 2005,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 2006,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 2007,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 2008,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 2009,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 2010,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 2011,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 2012,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 2013,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 2014,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 2021,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 2023,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 2025,
    "content": "Copyright  © 2009-2024 NVIDIA Corporation & affiliates Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2009-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 2030,
    "content": "Added Unified Memory Programming guide supporting Grace Hopper with Address Translation Service (ATS) and Heterogeneous Memory Management (HMM ) on x86"
  },
  {
    "id": 2031,
    "content": "The Benefits of Using GPUs  The Graphics Processing Unit (GPU) 1 provides much higher instruction throughput and memory bandwidth than the CPU within a similar price and power envelope Many applications leverage these higher capabilities to run faster on the GPU than on the CPU (see GPU Applications )"
  },
  {
    "id": 2032,
    "content": "Other computing devices, like FPGAs, are also very energy efficient, but offer much less programming flexibility than GPUs"
  },
  {
    "id": 2033,
    "content": "This difference in capabilities between the GPU and the CPU exists because they are designed with different goals in mind While the CPU is designed to excel at executing a sequence of operations, called a thread , as fast as possible and can execute a few tens of these threads in parallel, the GPU is designed to excel at executing thousands of them in parallel (amortizing the slower single-thread"
  },
  {
    "id": 2034,
    "content": "performance to achieve greater throughput) The GPU is specialized for highly parallel computations and therefore designed such that more transistors are devoted to data processing rather than data caching and flow control"
  },
  {
    "id": 2036,
    "content": "Figure 1 The GPU Devotes More Transistors to Data Processing  Devoting more transistors to data processing, for example, floating-point computations, is beneficial for highly parallel computations; the GPU can hide memory access latencies with computation, instead of relying on large data caches and complex flow control to avoid long memory access latencies, both of which are expensive in terms"
  },
  {
    "id": 2038,
    "content": "In general, an application has a mix of parallel parts and sequential parts, so systems are designed with a mix of GPUs and CPUs in order to maximize overall performance"
  },
  {
    "id": 2039,
    "content": "Applications with a high degree of parallelism can exploit this massively parallel nature of the GPU to achieve higher performance than on the CPU"
  },
  {
    "id": 2042,
    "content": "CUDA®: A General-Purpose Parallel Computing Platform and Programming Model  In November 2006, NVIDIA ® introduced CUDA ® , a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU"
  },
  {
    "id": 2043,
    "content": "CUDA comes with a software environment that allows developers to use C++ as a high-level programming language"
  },
  {
    "id": 2044,
    "content": "As illustrated by Figure 2 , other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC CUDA is designed to support various languages and application programming interfaces"
  },
  {
    "id": 2047,
    "content": "A Scalable Programming Model  The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems The challenge is to develop application software that transparently scales its parallelism to leverage the increasing number of processor cores, much as 3D graphics applications transparently scale their parallelism to manycore GPUs with widely varying"
  },
  {
    "id": 2049,
    "content": "The CUDA parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such as C"
  },
  {
    "id": 2050,
    "content": "At its core are three key abstractions — a hierarchy of thread groups, shared memories, and barrier synchronization — that are simply exposed to the programmer as a minimal set of language extensions"
  },
  {
    "id": 2051,
    "content": "These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism"
  },
  {
    "id": 2052,
    "content": "They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time"
  },
  {
    "id": 2053,
    "content": "enables automatic scalability Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors as illustrated by Figure 3 , and only the runtime system needs to know the physical multiprocessor count"
  },
  {
    "id": 2054,
    "content": "This scalable programming model allows the GPU architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions: from the high-performance enthusiast GeForce GPUs and professional Quadro and Tesla computing products to a variety of inexpensive, mainstream GeForce GPUs (see CUDA-Enabled GPUs for a list of all CUDA-enabled GPUs)"
  },
  {
    "id": 2055,
    "content": "Figure 3 Automatic Scalability  Note A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation for more details) A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors"
  },
  {
    "id": 2058,
    "content": "Document Structure  This document is organized into the following sections: Introduction is a general introduction to CUDA"
  },
  {
    "id": 2060,
    "content": "Stream Ordered Memory Allocator describes how applications can order memory allocation and deallocation"
  },
  {
    "id": 2061,
    "content": "Compute Capabilities gives the technical specifications of various devices, as well as more architectural details"
  },
  {
    "id": 2062,
    "content": "1 The graphics qualifier comes from the fact that when the GPU was originally created, two decades ago, it was designed as a specialized processor to accelerate graphics rendering Driven by the insatiable market demand for real-time, high-definition, 3D graphics, it has evolved into a general processor used for many more workloads than just graphics rendering"
  },
  {
    "id": 2064,
    "content": "Programming Model  This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++ Full code for the vector addition example used in this chapter and the next can be found in the vectorAdd CUDA sample"
  },
  {
    "id": 2067,
    "content": "Kernels  CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels , that, when called, are executed N times in parallel by N different CUDA threads , as opposed to only once like regular C++ functions"
  },
  {
    "id": 2068,
    "content": "A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new >> execution configuration syntax (see C++ Language Extensions ) Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables"
  },
  {
    "id": 2069,
    "content": "As an illustration, the following sample code, using the built-in variable threadIdx , adds two vectors A and B of size N and stores the result into vector C :   Kernel definition __global__ void VecAdd ( float * A , float * B , float * C ) { int i = threadIdx"
  },
  {
    "id": 2073,
    "content": "Thread Hierarchy  For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index , forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block"
  },
  {
    "id": 2074,
    "content": "This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume"
  },
  {
    "id": 2075,
    "content": "The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy) , the thread ID of a thread of index (x, y) is (x + y Dx) ; for a three-dimensional block of size (Dx, Dy, Dz) , the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy)"
  },
  {
    "id": 2076,
    "content": "As an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C : Kernel definition __global__ void MatAdd ( float A [ N ][ N ], float B [ N ][ N ], float C [ N ][ N ]) { int i = threadIdx Kernel invocation with one block of N * N * 1 threads int numBlocks = 1 ; dim3 threadsPerBlock ( N , N ); MatAdd >> ( A , B , C ); } There is a limit to the number of"
  },
  {
    "id": 2077,
    "content": "threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks Blocks are organized into a one-dimensional,"
  },
  {
    "id": 2078,
    "content": "two-dimensional, or three-dimensional grid of thread blocks as illustrated by Figure 4 The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system Figure 4 Grid of Thread Blocks  The number of threads per block and the number of blocks per grid specified in the >> syntax can be of type int or"
  },
  {
    "id": 2079,
    "content": "dim3 Each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in blockIdx variable The dimension of the thread block is accessible within the kernel through the built-in blockDim variable Extending the previous MatAdd() example to handle multiple blocks, the code becomes as follows Kernel"
  },
  {
    "id": 2080,
    "content": "definition __global__ void MatAdd ( float A [ N ][ N ], float B [ N ][ N ], float C [ N ][ N ]) { int i = blockIdx } A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not"
  },
  {
    "id": 2081,
    "content": "be the case Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated by Figure 3 , enabling programmers to write code that scales with the number of cores Threads within a block can cooperate by sharing data"
  },
  {
    "id": 2082,
    "content": "through some shared memory and by synchronizing their execution to coordinate memory accesses More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed In addition to __syncthreads() , the Cooperative Groups API provides a rich"
  },
  {
    "id": 2083,
    "content": "set of thread-synchronization primitives For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads() is expected to be lightweight"
  },
  {
    "id": 2087,
    "content": "Thread Block Clusters  With the introduction of NVIDIA Compute Capability 9 0 , the CUDA programming model introduces an optional level of hierarchy called Thread Block Clusters that are made up of thread blocks Similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a GPU"
  },
  {
    "id": 2088,
    "content": "Processing Cluster (GPC) in the GPU Similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or three-dimension as illustrated by Figure 5 The number of thread blocks in a cluster can be user-defined, and a maximum of 8 thread blocks in a cluster is supported as a portable cluster size in CUDA Note that on GPU hardware or MIG configurations which are too small to"
  },
  {
    "id": 2089,
    "content": "support 8 multiprocessors the maximum cluster size will be reduced accordingly Identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is architecture-specific and can be queried using the cudaOccupancyMaxPotentialClusterSize API Figure 5 Grid of Thread Block Clusters  Note In a kernel launched using cluster support, the"
  },
  {
    "id": 2090,
    "content": "gridDim variable still denotes the size in terms of number of thread blocks, for compatibility purposes A thread block cluster can be enabled in a kernel either using a compiler time kernel attribute using __cluster_dims__(X,Y,Z) or using the CUDA kernel launch API cudaLaunchKernelEx The cluster size using kernel attribute is fixed at compile time and then the kernel can be launched using the"
  },
  {
    "id": 2091,
    "content": "classical >> If a kernel uses compile-time cluster size, the cluster size cannot be modified when launching the kernel Kernel definition Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension __global__ void __cluster_dims__ ( 2 , 1 , 1 ) cluster_kernel ( float * input , float * output ) { } int main () { float * input , * output ; Kernel invocation with compile time cluster size"
  },
  {
    "id": 2092,
    "content": "dim3 threadsPerBlock ( 16 , 16 ); dim3 numBlocks ( N / threadsPerBlock y ); The grid dimension is not affected by cluster launch, and is still enumerated using number of blocks cluster_kernel >> ( input , output ); } A thread block cluster size can also be set at runtime and the kernel can be launched using the CUDA kernel launch API cudaLaunchKernelEx Kernel definition No compile time attribute"
  },
  {
    "id": 2093,
    "content": "attached to the kernel __global__ void cluster_kernel ( float * input , float * output ) { } int main () { float * input , * output ; dim3 threadsPerBlock ( 16 , 16 ); dim3 numBlocks ( N / threadsPerBlock y ); Kernel invocation with runtime cluster size { cudaLaunchConfig_t config = { 0 }; The grid dimension is not affected by cluster launch, and is still enumerated using number of blocks numAttrs"
  },
  {
    "id": 2094,
    "content": "= 1 ; cudaLaunchKernelEx ( & config , cluster_kernel , input , output ); } } In GPUs with compute capability 9 0, all the thread blocks in the cluster are guaranteed to be co-scheduled on a single GPU Processing Cluster (GPC) and allow thread blocks in the cluster to perform hardware-supported synchronization using the Cluster Group API cluster"
  },
  {
    "id": 2096,
    "content": "Cluster group also provides member functions to query cluster group size in terms of number of threads or number of blocks using num_threads() and num_blocks() API respectively The rank of a thread or block in the cluster group can be queried using dim_threads() and dim_blocks() API respectively Thread blocks in a cluster have the ability to read, write, and perform atomics to any address in the"
  },
  {
    "id": 2097,
    "content": "distributed shared memory Distributed Shared Memory gives an example of performing histograms in distributed shared memory"
  },
  {
    "id": 2100,
    "content": "Memory Hierarchy  CUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 6"
  },
  {
    "id": 2101,
    "content": "Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block Thread blocks in a thread block cluster can perform read, write, and atomics operations on each other’s shared memory"
  },
  {
    "id": 2102,
    "content": "There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses ) Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory ) The global,"
  },
  {
    "id": 2104,
    "content": "Heterogeneous Programming  As illustrated by Figure 7 , the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program This is the case, for example, when the kernels execute on a GPU and the rest of the C++ program executes on a CPU The CUDA programming model also assumes that both the host and"
  },
  {
    "id": 2105,
    "content": "the device maintain their own separate memory spaces in DRAM, referred to as host memory and device memory , respectively Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime (described in Programming Interface ) This includes device memory allocation and deallocation as well as data transfer between host and device"
  },
  {
    "id": 2106,
    "content": "memory Managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device Figure 7 Heterogeneous Programming  Note Serial code executes on the host"
  },
  {
    "id": 2110,
    "content": "Asynchronous SIMT Programming Model  In the CUDA programming model a thread is the lowest level of abstraction for doing a computation or a memory operation Starting with devices based on the NVIDIA Ampere GPU architecture, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model The asynchronous programming model defines the behavior of"
  },
  {
    "id": 2111,
    "content": "asynchronous operations with respect to CUDA threads The asynchronous programming model defines the behavior of Asynchronous Barrier for synchronization between CUDA threads The model also explains and defines how cuda::memcpy_async can be used to move data asynchronously from global memory while computing in the GPU"
  },
  {
    "id": 2115,
    "content": "Asynchronous Operations  An asynchronous operation is defined as an operation that is initiated by a CUDA thread and is executed asynchronously as-if by another thread In a well formed program one or more CUDA threads synchronize with the asynchronous operation The CUDA thread that initiated the asynchronous operation is not required to be among the synchronizing threads Such an asynchronous"
  },
  {
    "id": 2116,
    "content": "thread (an as-if thread) is always associated with the CUDA thread that initiated the asynchronous operation An asynchronous operation uses a synchronization object to synchronize the completion of the operation Such a synchronization object can be explicitly managed by a user (e"
  },
  {
    "id": 2121,
    "content": "These objects are explained in detail in Asynchronous Barrier and Asynchronous Data Copies using cuda::pipeline"
  },
  {
    "id": 2122,
    "content": "A scope defines the set of threads that may use the synchronization object to synchronize with the asynchronous operation The following table defines the thread scopes available in CUDA C++ and the threads that can be synchronized with each Thread Scope Description cuda::thread_scope::thread_scope_thread Only the CUDA thread which initiated asynchronous operations synchronizes"
  },
  {
    "id": 2123,
    "content": "cuda::thread_scope::thread_scope_block All or any CUDA threads within the same thread block as the initiating thread synchronizes cuda::thread_scope::thread_scope_device All or any CUDA threads in the same GPU device as the initiating thread synchronizes cuda::thread_scope::thread_scope_system All or any CUDA or CPU threads in the same system as the initiating thread synchronizes These thread"
  },
  {
    "id": 2127,
    "content": "Compute Capability  The compute capability of a device is represented by a version number, also sometimes called its “SM version”"
  },
  {
    "id": 2128,
    "content": "This version number identifies the features supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU"
  },
  {
    "id": 2129,
    "content": "The compute capability comprises a major revision number X and a minor revision number Y and is denoted by X"
  },
  {
    "id": 2131,
    "content": "The major revision number is 9 for devices based on the NVIDIA Hopper GPU architecture, 8 for devices based on the NVIDIA Ampere GPU architecture, 7 for devices based on the Volta architecture, 6 for devices based on the Pascal architecture, 5 for devices based on the Maxwell architecture, and 3 for devices based on the Kepler architecture The minor revision number corresponds to an incremental"
  },
  {
    "id": 2132,
    "content": "improvement to the core architecture, possibly including new features Turing is the architecture for devices of compute capability 7 5, and is an incremental update based on the Volta architecture Note The compute capability version of a particular GPU should not be confused with the CUDA version (for example, CUDA 7 5, CUDA 8, CUDA 9), which is the version of the CUDA software platform The CUDA"
  },
  {
    "id": 2133,
    "content": "platform is used by application developers to create applications that run on many generations of GPU architectures, including future GPU architectures yet to be invented While new versions of the CUDA platform often add native support for a new GPU architecture by supporting the compute capability version of that architecture, new versions of the CUDA platform typically also include software"
  },
  {
    "id": 2134,
    "content": "features that are independent of hardware generation The Tesla and Fermi architectures are no longer supported starting with CUDA 7 0 and CUDA 9"
  },
  {
    "id": 2137,
    "content": "Programming Interface  CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device"
  },
  {
    "id": 2138,
    "content": "They allow programmers to define a kernel as a C++ function and use some new syntax to specify the grid and block dimension each time the function is called"
  },
  {
    "id": 2139,
    "content": "Any source file that contains some of these extensions must be compiled with nvcc as outlined in Compilation with NVCC"
  },
  {
    "id": 2140,
    "content": "It provides C and C++ functions that execute on the host to allocate and deallocate device memory, transfer data between host memory and device memory, manage systems with multiple devices, etc"
  },
  {
    "id": 2141,
    "content": "The runtime is built on top of a lower-level C API, the CUDA driver API, which is also accessible by the application The driver API provides an additional level of control by exposing lower-level concepts such as CUDA contexts - the analogue of host processes for the device - and CUDA modules - the analogue of dynamically loaded libraries for the device Most applications do not use the driver API"
  },
  {
    "id": 2142,
    "content": "as they do not need this additional level of control and when using the runtime, context and module management are implicit, resulting in more concise code As the runtime is interoperable with the driver API, most applications that need some driver API features can default to use the runtime API and only use the driver API where needed The driver API is introduced in Driver API and fully described"
  },
  {
    "id": 2146,
    "content": "Compilation with NVCC  Kernels can be written using the CUDA instruction set architecture, called PTX , which is described in the PTX reference manual"
  },
  {
    "id": 2149,
    "content": "nvcc is a compiler driver that simplifies the process of compiling C++ or PTX code: It provides simple and familiar command line options and executes them by invoking the collection of tools that implement the different compilation stages"
  },
  {
    "id": 2162,
    "content": ", code that executes on the device) nvcc ’s basic workflow consists in separating device code from host code and then: compiling the device code into an assembly form ( PTX code) and/or binary form ( cubin object), and modifying the host code by replacing the >> syntax introduced in Kernels (and described in more details in Execution Configuration ) by the necessary CUDA runtime function calls to"
  },
  {
    "id": 2163,
    "content": "load and launch each compiled kernel from the PTX code and/or cubin object The modified host code is output either as C++ code that is left to be compiled using another tool or as object code directly by letting nvcc invoke the host compiler during the last compilation stage Applications can then: Either link to the compiled host code (this is the most common case), Or ignore the modified host"
  },
  {
    "id": 2164,
    "content": "code (if any) and use the CUDA driver API (see Driver API ) to load and execute the PTX code or cubin object"
  },
  {
    "id": 2169,
    "content": "Just-in-Time Compilation  Any PTX code loaded by an application at runtime is compiled further to binary code by the device driver Just-in-time compilation increases application load time, but allows the application to benefit from any new compiler improvements coming with each new device driver It is also the only way for applications to run on devices that did not exist at the time the"
  },
  {
    "id": 2170,
    "content": "application was compiled, as detailed in Application Compatibility When the device driver just-in-time compiles some PTX code for some application, it automatically caches a copy of the generated binary code in order to avoid repeating the compilation in subsequent invocations of the application The cache - referred to as compute cache - is automatically invalidated when the device driver is"
  },
  {
    "id": 2171,
    "content": "upgraded, so that applications can benefit from the improvements in the new just-in-time compiler built into the device driver Environment variables are available to control just-in-time compilation as described in CUDA Environment Variables As an alternative to using nvcc to compile CUDA C++ device code, NVRTC can be used to compile CUDA C++ device code to PTX at runtime NVRTC is a runtime"
  },
  {
    "id": 2176,
    "content": "Binary Compatibility  Binary code is architecture-specific A cubin object is generated using the compiler option -code that specifies the targeted architecture: For example, compiling with -code=sm_80 produces binary code for devices of compute capability 8"
  },
  {
    "id": 2178,
    "content": "Binary compatibility is guaranteed from one minor revision to the next one, but not from one minor revision to the previous one or across major revisions"
  },
  {
    "id": 2179,
    "content": "In other words, a cubin object generated for compute capability X y will only execute on devices of compute capability X"
  },
  {
    "id": 2185,
    "content": "PTX Compatibility  Some PTX instructions are only supported on devices of higher compute capabilities For example, Warp Shuffle Functions are only supported on devices of compute capability 5"
  },
  {
    "id": 2187,
    "content": "The -arch compiler option specifies the compute capability that is assumed when compiling C++ to PTX code So, code that contains warp shuffle, for example, must be compiled with -arch=compute_50 (or higher) PTX code produced for some specific compute capability can always be compiled to binary code of greater or equal compute capability Note that a binary compiled from an earlier PTX version may"
  },
  {
    "id": 2188,
    "content": "not make use of some hardware features For example, a binary targeting devices of compute capability 7 0 (Volta) compiled from PTX generated for compute capability 6 0 (Pascal) will not make use of Tensor Core instructions, since these were not available on Pascal As a result, the final binary may perform worse than would be possible if the binary were generated using the latest version of PTX PTX"
  },
  {
    "id": 2189,
    "content": "code compiled to target architecture conditional features only run on the exact same physical architecture and nowhere else Example code compiled with sm_90a or compute_90a only runs on devices with compute capability 9"
  },
  {
    "id": 2194,
    "content": "Application Compatibility  To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability as described in Binary Compatibility and PTX Compatibility In particular, to be able to execute code on future architectures with higher compute capability (for which no binary code can be generated yet), an application"
  },
  {
    "id": 2195,
    "content": "must load PTX code that will be just-in-time compiled for these devices (see Just-in-Time Compilation ) Which PTX and binary code gets embedded in a CUDA C++ application is controlled by the -arch and -code compiler options or the -gencode compiler option as detailed in the nvcc user manual"
  },
  {
    "id": 2197,
    "content": "cu -gencode arch=compute_50,code=sm_50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=\\\"compute_70,sm_70\\\" embeds binary code compatible with compute capability 5"
  },
  {
    "id": 2198,
    "content": "0 and 6 0 (first and second -gencode options) and PTX and binary code compatible with compute capability 7 0 (third -gencode option)"
  },
  {
    "id": 2199,
    "content": "Host code is generated to automatically select at runtime the most appropriate code to load and execute, which, in the above example, will be: 5"
  },
  {
    "id": 2212,
    "content": "cu can have an optimized code path that uses warp reduction operations, for example, which are only supported in devices of compute capability 8"
  },
  {
    "id": 2216,
    "content": "cu is compiled for architecture conditional features example with sm_90a or compute_90a , the code can only run on devices with compute capability 9"
  },
  {
    "id": 2218,
    "content": "Applications using the driver API must compile code to separate files and explicitly load and execute the most appropriate file at runtime"
  },
  {
    "id": 2219,
    "content": "The Volta architecture introduces Independent Thread Scheduling which changes the way threads are scheduled on the GPU For code relying on specific behavior of SIMT scheduling in previous architectures, Independent Thread Scheduling may alter the set of participating threads, leading to incorrect results To aid migration while implementing the corrective actions detailed in Independent Thread"
  },
  {
    "id": 2220,
    "content": "Scheduling , Volta developers can opt-in to Pascal’s thread scheduling with the compiler option combination -arch=compute_60 -code=sm_70"
  },
  {
    "id": 2221,
    "content": "The nvcc user manual lists various shorthands for the -arch , -code , and -gencode compiler options For example, -arch=sm_70 is a shorthand for -arch=compute_70 -code=compute_70,sm_70 (which is the same as -gencode arch=compute_70,code=\\\"compute_70,sm_70\\\" )"
  },
  {
    "id": 2225,
    "content": "C++ Compatibility  The front end of the compiler processes CUDA source files according to C++ syntax rules"
  },
  {
    "id": 2226,
    "content": "However, only a subset of C++ is fully supported for the device code as described in C++ Language Support"
  },
  {
    "id": 2232,
    "content": ", pointers are 64-bit) Device code compiled in 64-bit mode is only supported with host code compiled in 64-bit mode"
  },
  {
    "id": 2235,
    "content": "CUDA Runtime  The runtime is implemented in the cudart library, which is linked to the application, either statically via cudart"
  },
  {
    "id": 2242,
    "content": "It is only safe to pass the address of CUDA runtime symbols between components that link to the same instance of the CUDA runtime"
  },
  {
    "id": 2243,
    "content": "As mentioned in Heterogeneous Programming , the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory"
  },
  {
    "id": 2244,
    "content": "Shared Memory illustrates the use of shared memory, introduced in Thread Hierarchy , to maximize performance"
  },
  {
    "id": 2245,
    "content": "Page-Locked Host Memory introduces page-locked host memory that is required to overlap kernel execution with data transfers between host and device memory"
  },
  {
    "id": 2246,
    "content": "Asynchronous Concurrent Execution describes the concepts and API used to enable asynchronous concurrent execution at various levels in the system"
  },
  {
    "id": 2247,
    "content": "Multi-Device System shows how the programming model extends to a system with multiple devices attached to the same host"
  },
  {
    "id": 2248,
    "content": "Texture and Surface Memory presents the texture and surface memory spaces that provide another way to access device memory; they also expose a subset of the GPU texturing hardware"
  },
  {
    "id": 2249,
    "content": "Graphics Interoperability introduces the various functions the runtime provides to interoperate with the two main graphics APIs, OpenGL and Direct3D"
  },
  {
    "id": 2254,
    "content": "0, the cudaInitDevice() and cudaSetDevice() calls initialize the runtime and the primary context associated with the specified device Absent these calls, the runtime will implicitly use device 0 and self-initialize as needed to process other runtime API requests One needs to keep this in mind when timing runtime function calls and when interpreting the error code from the first call into the"
  },
  {
    "id": 2257,
    "content": "0, cudaSetDevice() would not initialize the runtime and applications would often use the no-op runtime call cudaFree(0) to isolate the runtime initialization from other api activity (both for the sake of timing and error handling) The runtime creates a CUDA context for each device in the system (see Context for more details on CUDA contexts) This context is the primary context for this device and"
  },
  {
    "id": 2258,
    "content": "is initialized at the first runtime function which requires an active context on this device As part of this context creation, the device code is just-in-time compiled if necessary (see Just-in-Time Compilation ) and loaded into device memory If needed, for example, for driver API interoperability, the primary context of a device can be accessed from the driver API as described in Interoperability"
  },
  {
    "id": 2259,
    "content": "between Runtime and Driver APIs When a host thread calls cudaDeviceReset() , this destroys the primary context of the device the host thread currently operates on (i"
  },
  {
    "id": 2261,
    "content": ", the current device as defined in Device Selection ) The next runtime function call made by any host thread that has this device as current will create a new primary context for this device"
  },
  {
    "id": 2262,
    "content": "Note The CUDA interfaces use global state that is initialized during host program initiation and destroyed during host program termination The CUDA runtime and driver cannot detect if this state is invalid, so using any of these interfaces (implicitly or explicitly) during program initiation or termination after main) will result in undefined behavior"
  },
  {
    "id": 2264,
    "content": "0, cudaSetDevice() will now explicitly initialize the runtime after changing the current device for the host thread Previous versions of CUDA delayed runtime initialization on the new device until the first runtime call was made after cudaSetDevice() This change means that it is now very important to check the return value of cudaSetDevice() for initialization errors The runtime functions from"
  },
  {
    "id": 2265,
    "content": "the error handling and version management sections of the reference manual do not initialize the runtime"
  },
  {
    "id": 2269,
    "content": "Device Memory  As mentioned in Heterogeneous Programming , the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory Kernels operate out of device memory, so the runtime provides functions to allocate, deallocate, and copy device memory, as well as transfer data between host memory and device memory"
  },
  {
    "id": 2270,
    "content": "Linear memory is allocated in a single unified address space, which means that separately allocated entities can reference one another via pointers, for example, in a binary tree or linked list"
  },
  {
    "id": 2271,
    "content": "The size of the address space depends on the host system (CPU) and the compute capability of the used GPU: Table 1 Linear Memory Address Space  x86_64 (AMD64) POWER (ppc64le) ARM64 up to compute capability 5"
  },
  {
    "id": 2274,
    "content": "3 (Maxwell) and earlier, the CUDA driver creates an uncommitted 40bit virtual address reservation to ensure that memory allocations (pointers) fall into the supported range This reservation appears as reserved virtual memory, but does not occupy any physical memory until the program actually allocates memory"
  },
  {
    "id": 2275,
    "content": "Linear memory is typically allocated using cudaMalloc() and freed using cudaFree() and data transfer between host memory and device memory are typically done using cudaMemcpy()"
  },
  {
    "id": 2276,
    "content": "In the vector addition code sample of Kernels , the vectors need to be copied from host memory to device memory:   Device code __global__ void VecAdd ( float * A , float * B , float * C , int N ) { int i = blockDim"
  },
  {
    "id": 2277,
    "content": "x ; if ( i >> ( d_A , d_B , d_C , N );   Copy result from device memory to host memory   h_C contains the result in host memory cudaMemcpy ( h_C , d_C , size , cudaMemcpyDeviceToHost );   Free device memory cudaFree ( d_A ); cudaFree ( d_B ); cudaFree ( d_C );   Free host memory"
  },
  {
    "id": 2278,
    "content": "These functions are recommended for allocations of 2D or 3D arrays as it makes sure that the allocation is appropriately padded to meet the alignment requirements described in Device Memory Accesses , therefore ensuring best performance when accessing the row addresses or performing copies between 2D arrays and other regions of device memory (using the cudaMemcpy2D() and cudaMemcpy3D() functions)"
  },
  {
    "id": 2279,
    "content": "The following code sample allocates a width x height 2D array of floating-point values and shows how to loop over the array elements in device code: Host code int width = 64 , height = 64 ; float * devPtr ; size_t pitch ; cudaMallocPitch ( & devPtr , & pitch , width * sizeof ( float ), height ); MyKernel >> ( devPtr , pitch , width , height ); Device code __global__ void MyKernel ( float * devPtr"
  },
  {
    "id": 2280,
    "content": ", size_t pitch , int width , int height ) { for ( int r = 0 ; r >> ( devPitchedPtr , width , height , depth ); Device code __global__ void MyKernel ( cudaPitchedPtr devPitchedPtr , int width , int height , int depth ) { char * devPtr = devPitchedPtr pitch ; size_t slicePitch = pitch * height ; for ( int z = 0 ; z ( ptr ); Global Memory data pointer stream_attribute"
  },
  {
    "id": 2282,
    "content": "Set the attributes to a CUDA stream of type cudaStream_t cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); When a kernel subsequently executes in CUDA stream , memory accesses within the global memory extent [ptr ptr+num_bytes) are more likely to persist in the L2 cache than accesses to other global memory locations"
  },
  {
    "id": 2283,
    "content": "L2 persistence can also be set for a CUDA Graph Kernel Node as shown in the example below: CUDA GraphKernelNode Example cudaKernelNodeAttrValue node_attribute ; Kernel level attributes data structure node_attribute hitProp = cudaAccessPropertyPersisting ; Type of access property on cache hit node_attribute Set the attributes to a CUDA Graph Kernel node of type cudaGraphNode_t"
  },
  {
    "id": 2284,
    "content": "cudaGraphKernelNodeSetAttribute ( node , cudaKernelNodeAttributeAccessPolicyWindow , & node_attribute ); The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property"
  },
  {
    "id": 2285,
    "content": "In both of the examples above, 60% of the memory accesses in the global memory region [ptr ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property Which specific memory accesses are classified as persisting (the hitProp ) is random with a probability of approximately hitRatio ; the probability distribution depends upon the hardware architecture and"
  },
  {
    "id": 2286,
    "content": "the memory extent For example, if the L2 set-aside cache size is 16KB and the num_bytes in the accessPolicyWindow is 32KB: With a hitRatio of 0 5, the hardware will select, at random, 16KB of the 32KB window to be designated as persisting and cached in the set-aside L2 cache area With a hitRatio of 1 0, the hardware will attempt to cache the whole 32KB window in the set-aside L2 cache area Since"
  },
  {
    "id": 2287,
    "content": "the set-aside area is smaller than the window, cache lines will be evicted to keep the most recently used 16KB of the 32KB data in the set-aside portion of the L2 cache The hitRatio can therefore be used to avoid thrashing of cache lines and overall reduce the amount of data moved into and out of the L2 cache A hitRatio value below 1 0 can be used to manually control the amount of data different"
  },
  {
    "id": 2288,
    "content": "accessPolicyWindow s from concurrent CUDA streams can cache in L2 For example, let the L2 set-aside cache size be 16KB; two concurrent kernels in two different CUDA streams, each with a 16KB accessPolicyWindow , and both with hitRatio value 1 0, might evict each others’ cache lines when competing for the shared L2 resource However, if both accessPolicyWindows have a hitRatio value of 0 5, they"
  },
  {
    "id": 2294,
    "content": "L2 Access Properties  Three types of access properties are defined for different global memory data accesses: cudaAccessPropertyStreaming : Memory accesses that occur with the streaming property are less likely to persist in the L2 cache because these accesses are preferentially evicted cudaAccessPropertyPersisting : Memory accesses that occur with the persisting property are more likely to"
  },
  {
    "id": 2295,
    "content": "persist in the L2 cache because these accesses are preferentially retained in the set-aside portion of L2 cache cudaAccessPropertyNormal : This access property forcibly resets previously applied persisting access property to a normal status Memory accesses with the persisting property from previous CUDA kernels may be retained in L2 cache long after their intended use This persistence-after-use"
  },
  {
    "id": 2296,
    "content": "reduces the amount of L2 cache available to subsequent kernels that do not use the persisting property Resetting an access property window with the cudaAccessPropertyNormal property removes the persisting (preferential retention) status of the prior access, as if the prior access had been without an access property"
  },
  {
    "id": 2301,
    "content": "L2 Persistence Example  The following example shows how to set-aside L2 cache for persistent accesses, use the set-aside L2 cache in CUDA kernels via CUDA Stream and then reset the L2 cache"
  },
  {
    "id": 2302,
    "content": "cudaStream_t stream ; cudaStreamCreate ( & stream ); Create CUDA stream cudaDeviceProp prop ; CUDA device properties variable cudaGetDeviceProperties ( & prop , device_id ); Query GPU properties size_t size = min ( int ( prop persistingL2CacheMaxSize ); cudaDeviceSetLimit ( cudaLimitPersistingL2CacheSize , size ); set-aside 3/4 of L2 cache for persisting accesses or the max allowed size_t"
  },
  {
    "id": 2304,
    "content": "accessPolicyMaxWindowSize , num_bytes );   Select minimum of user defined num_bytes and max window size"
  },
  {
    "id": 2305,
    "content": "cudaStreamAttrValue stream_attribute ; Stream level attributes data structure stream_attribute missProp = cudaAccessPropertyStreaming ; Type of access property on cache miss cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); Set the attributes to a CUDA Stream for ( int i = 0 ; i >> ( data1 ); This data1 is used by a kernel multiple times } [data1 +"
  },
  {
    "id": 2306,
    "content": "num_bytes) benefits from L2 persistence cuda_kernelB >> ( data1 ); A different kernel in the same stream can also benefit from the persistence of data1 stream_attribute num_bytes = 0 ; Setting the window size to 0 disable it cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); Overwrite the access policy attribute to a CUDA Stream"
  },
  {
    "id": 2307,
    "content": "cudaCtxResetPersistingL2Cache (); Remove any persistent lines in L2 cuda_kernelC >> ( data2 ); data2 can now benefit from full L2 in normal mode 3"
  },
  {
    "id": 2311,
    "content": "Reset L2 Access to Normal  A persisting L2 cache line from a previous CUDA kernel may persist in L2 long after it has been used Hence, a reset to normal for L2 cache is important for streaming or normal memory accesses to utilize the L2 cache with normal priority Reset a previous persisting memory region with the access property, cudaAccessPropertyNormal Reset all persisting L2 cache lines to"
  },
  {
    "id": 2312,
    "content": "normal by calling cudaCtxResetPersistingL2Cache() Reliance on automatic reset is strongly discouraged because of the undetermined length of time required for automatic reset to occur"
  },
  {
    "id": 2317,
    "content": "Manage Utilization of L2 set-aside cache  Multiple CUDA kernels executing concurrently in different CUDA streams may have a different access policy window assigned to their streams However, the L2 set-aside cache portion is shared among all these concurrent CUDA kernels As a result, the net utilization of this set-aside cache portion is the sum of all the concurrent kernels’ individual use The"
  },
  {
    "id": 2318,
    "content": "benefits of designating memory accesses as persisting diminish as the volume of persisting accesses exceeds the set-aside L2 cache capacity To manage utilization of the set-aside L2 cache portion, an application must consider the following: Size of L2 set-aside cache When and how L2 reset is required to allow normal or streaming accesses to utilize the previously set-aside L2 cache with equal"
  },
  {
    "id": 2324,
    "content": "Query L2 cache Properties  Properties related to L2 cache are a part of cudaDeviceProp struct and can be queried using CUDA runtime API cudaGetDeviceProperties CUDA Device Properties include: l2CacheSize : The amount of available L2 cache on the GPU persistingL2CacheMaxSize : The maximum amount of L2 cache that can be set-aside for persisting memory accesses"
  },
  {
    "id": 2330,
    "content": "Control L2 Cache Set-Aside Size for Persisting Memory Access  The L2 set-aside cache size for persisting memory accesses is queried using CUDA runtime API cudaDeviceGetLimit and set using CUDA runtime API cudaDeviceSetLimit as a cudaLimit"
  },
  {
    "id": 2331,
    "content": "Shared Memory  As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier Shared memory is expected to be much faster than global memory as mentioned in Thread Hierarchy and detailed in Shared Memory It can be used as scratchpad memory (or software managed cache) to minimize global memory accesses from a CUDA block as illustrated by the"
  },
  {
    "id": 2332,
    "content": "following matrix multiplication example The following code sample is a straightforward implementation of matrix multiplication that does not take advantage of shared memory"
  },
  {
    "id": 2333,
    "content": "Each thread reads one row of A and one column of B and computes the corresponding element of C as illustrated in Figure 8 Matrices are stored in row-major order:   M(row, col) = *(M elements + row * M"
  },
  {
    "id": 2334,
    "content": "width + col) typedef struct { int width ; int height ; float * elements ; } Matrix ; Thread block size #define BLOCK_SIZE 16 Forward declaration of the matrix multiplication kernel __global__ void MatMulKernel ( const Matrix , const Matrix , Matrix ); Matrix multiplication - Host code Matrix dimensions are assumed to be multiples of BLOCK_SIZE void MatMul ( const Matrix A , const Matrix B ,"
  },
  {
    "id": 2335,
    "content": "Matrix C ) { Load A and B to device memory Matrix d_A ; d_A elements , size , cudaMemcpyHostToDevice ); Allocate C in device memory Matrix d_C ; d_C elements , size ); Invoke kernel dim3 dimBlock ( BLOCK_SIZE , BLOCK_SIZE ); dim3 dimGrid ( B elements ); } Matrix multiplication kernel called by MatMul() __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { Each thread computes one"
  },
  {
    "id": 2336,
    "content": "element of C by accumulating results into Cvalue float Cvalue = 0 ; int row = blockIdx x ; for ( int e = 0 ; e >> ( d_A , d_B , d_C ); Read C from device memory cudaMemcpy ( C elements ); } Matrix multiplication kernel called by MatMul() __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { Block row and column int blockRow = blockIdx x ; Each thread block computes one sub-matrix Csub"
  },
  {
    "id": 2337,
    "content": "of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); Each thread computes one element of Csub by accumulating results into Cvalue float Cvalue = 0 ; Thread row and column within Csub int row = threadIdx x ; Loop over all the sub-matrices of A and B that are required to compute Csub Multiply each pair of sub-matrices together and accumulate the results for ( int m = 0 ; m Distributed Shared"
  },
  {
    "id": 2338,
    "content": "memory histogram kernel __global__ void clusterHist_kernel ( int * bins , const int nbins , const int bins_per_block , const int * __restrict__ input , size_t array_size ) { extern __shared__ int smem []; namespace cg = cooperative_groups ; int tid = cg :: this_grid () cg :: cluster_group cluster = cg :: this_cluster (); unsigned int clusterBlockRank = cluster x ; i = nbins ) binid = nbins - 1 ;"
  },
  {
    "id": 2339,
    "content": "Find destination block rank and offset for computing distributed shared memory histogram int dst_block_rank = ( int )( binid / bins_per_block ); int dst_offset = binid % bins_per_block ; Pointer to target block shared memory int * dst_smem = cluster map_shared_rank ( smem , dst_block_rank ); Perform atomic update of the histogram bin atomicAdd ( dst_smem + dst_offset , 1 ); } cluster"
  },
  {
    "id": 2340,
    "content": "synchronization is required to ensure all distributed shared memory operations are completed and no thread block exits while other thread blocks are still accessing distributed shared memory cluster sync (); Perform global memory histogram, using the local distributed memory histogram int * lbins = bins + cluster x ; i a ( 0 ); __managed__ cuda :: atomic b ( 0 ); Thread 1 (SM) x = 1 ; a = 1 ;"
  },
  {
    "id": 2341,
    "content": "Thread 2 (SM) while ( a = 1 ) ; assert ( x == 1 ); b = 1 ; Thread 3 (CPU) while ( b = 1 ) ; assert ( x == 1 ); Consider the example above The CUDA memory consistency model guarantees that the asserted condition will be true, so the write to x from thread 1 must be visible to thread 3, before the write to b from thread 2 The memory ordering provided by the release and acquire of a is only"
  },
  {
    "id": 2342,
    "content": "sufficient to make x visible to thread 2, not thread 3, as it is a device-scope operation The system-scope ordering provided by release and acquire of b , therefore, needs to ensure not only writes issued from thread 2 itself are visible to thread 3, but also writes from other threads that are visible to thread 2 As the GPU cannot know at the time of execution which writes have been guaranteed at"
  },
  {
    "id": 2343,
    "content": "the source level to be visible and which are visible only by chance timing, it must cast a conservatively wide net for in-flight memory operations This sometimes leads to interference: because the GPU is waiting on memory operations it is not required to at the source level, the fence/flush may take longer than necessary"
  },
  {
    "id": 2344,
    "content": "Note that fences may occur explicitly as intrinsics or atomics in code, like in the example, or implicitly to implement synchronizes-with relationships at task boundaries"
  },
  {
    "id": 2345,
    "content": "A common example is when a kernel is performing computation in local GPU memory, and a parallel kernel (e"
  },
  {
    "id": 2347,
    "content": "Upon completion, the local kernel will implicitly flush its writes to satisfy any synchronizes-with relationships to downstream work"
  },
  {
    "id": 2348,
    "content": "This may unnecessarily wait, fully or partially, on slower nvlink or PCIe writes from the communication kernel"
  },
  {
    "id": 2353,
    "content": "Isolating Traffic with Domains  Beginning with Hopper architecture GPUs and CUDA 12 0, the memory synchronization domains feature provides a way to alleviate such interference"
  },
  {
    "id": 2355,
    "content": "Writes and fences are tagged with the ID, and a fence will only order writes matching the fence’s domain"
  },
  {
    "id": 2356,
    "content": "In the concurrent compute vs communication example, the communication kernels can be placed in a different domain"
  },
  {
    "id": 2357,
    "content": "When using domains, code must abide by the rule that ordering or synchronization between distinct domains on the same GPU requires system-scope fencing"
  },
  {
    "id": 2358,
    "content": "This is necessary for cumulativity as one kernel’s writes will not be encompassed by a fence issued from a kernel in another domain In essence, cumulativity is satisfied by ensuring that cross-domain traffic is flushed to the system scope ahead of time However, because kernels will default to domain 0 as described below, backward compatibility is maintained"
  },
  {
    "id": 2363,
    "content": "Using Domains in CUDA  Domains are accessible via the new launch attributes cudaLaunchAttributeMemSyncDomain and cudaLaunchAttributeMemSyncDomainMap The former selects between logical domains cudaLaunchMemSyncDomainDefault and cudaLaunchMemSyncDomainRemote , and the latter provides a mapping from logical to physical domains"
  },
  {
    "id": 2364,
    "content": "The remote domain is intended for kernels performing remote memory access in order to isolate their memory traffic from local kernels Note, however, the selection of a particular domain does not affect what memory access a kernel may legally perform"
  },
  {
    "id": 2365,
    "content": "To facilitate portable code, domains functionality can be used on all devices and CUDA will report a count of 1 prior to Hopper"
  },
  {
    "id": 2366,
    "content": "An individual kernel launch at a low level in the stack, such as from NCCL, can select a semantic logical domain without concern for the surrounding application architecture The default value for the logical domain if it is not set is the default domain, and the default mapping is to map the default domain to 0 and the remote domain to 1 (on GPUs with more than 1 domain) Specific libraries may"
  },
  {
    "id": 2369,
    "content": "Together, this provides a beneficial use pattern for common applications out of the box, with no code changes needed in other components, frameworks, or at application level An alternative use pattern, for example in an application using nvshmem or with no clear separation of kernel types, could be to partition parallel streams"
  },
  {
    "id": 2370,
    "content": "Example of launching a kernel with the remote logical domain cudaLaunchAttribute domainAttr ; domainAttr"
  },
  {
    "id": 2371,
    "content": "val = cudaLaunchMemSyncDomainRemote ; cudaLaunchConfig_t config ;   Fill out other config fields config numAttrs = 1 ; cudaLaunchKernelEx ( & config , myKernel , kernelArg1 , kernelArg2"
  },
  {
    "id": 2372,
    "content": "); Example of setting a mapping for a stream (This mapping is the default for streams starting on Hopper if not explicitly set, and provided for illustration) cudaLaunchAttributeValue mapAttr ; mapAttr remote = 1 ; cudaStreamSetAttribute ( stream , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); Example of mapping different streams to different physical domains, ignoring logical domain"
  },
  {
    "id": 2373,
    "content": "settings cudaLaunchAttributeValue mapAttr ; mapAttr remote = 0 ; cudaStreamSetAttribute ( streamA , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); mapAttr remote = 1 ; cudaStreamSetAttribute ( streamB , cudaLaunchAttributeMemSyncDomainMap , & mapAttr ); As with other launch attributes, these are exposed uniformly on CUDA streams, individual launches using cudaLaunchKernelEx , and kernel nodes"
  },
  {
    "id": 2374,
    "content": "in CUDA graphs A typical use would set the mapping at stream level and the logical domain at launch level (or bracketing a section of stream use) as described above"
  },
  {
    "id": 2375,
    "content": "Graphs take both attributes from the node itself, essentially an indirect way of specifying a physical domain Domain-related attributes set on the stream a graph is launched into are not used in execution of the graph"
  },
  {
    "id": 2379,
    "content": "Asynchronous Concurrent Execution  CUDA exposes the following operations as independent tasks that can operate concurrently with one another: Computation on the host; Computation on the device; Memory transfers from the host to the device; Memory transfers from the device to the host; Memory transfers within the memory of a given device; Memory transfers among devices The level of concurrency"
  },
  {
    "id": 2380,
    "content": "achieved between these operations will depend on the feature set and compute capability of the device as described below"
  },
  {
    "id": 2385,
    "content": "Concurrent Execution between Host and Device  Concurrent host execution is facilitated through asynchronous library functions that return control to the host thread before the device completes the requested task"
  },
  {
    "id": 2386,
    "content": "Using asynchronous calls, many device operations can be queued up together to be executed by the CUDA driver when appropriate device resources are available"
  },
  {
    "id": 2387,
    "content": "This relieves the host thread of much of the responsibility to manage the device, leaving it free for other tasks"
  },
  {
    "id": 2388,
    "content": "The following device operations are asynchronous with respect to the host: Kernel launches; Memory copies within a single device’s memory; Memory copies from host to device of a memory block of 64 KB or less; Memory copies performed by functions that are suffixed with Async ; Memory set function calls"
  },
  {
    "id": 2389,
    "content": "Programmers can globally disable asynchronicity of kernel launches for all CUDA applications running on a system by setting the CUDA_LAUNCH_BLOCKING environment variable to 1"
  },
  {
    "id": 2390,
    "content": "This feature is provided for debugging purposes only and should not be used as a way to make production software run reliably"
  },
  {
    "id": 2391,
    "content": "Kernel launches are synchronous if hardware counters are collected via a profiler (Nsight, Visual Profiler) unless concurrent kernel profiling is enabled"
  },
  {
    "id": 2399,
    "content": "Applications may query this capability by checking the concurrentKernels device property (see Device Enumeration ), which is equal to 1 for devices that support it"
  },
  {
    "id": 2400,
    "content": "The maximum number of kernel launches that a device can execute concurrently depends on its compute capability and is listed in Table 21 A kernel from one CUDA context cannot execute concurrently with a kernel from another CUDA context"
  },
  {
    "id": 2402,
    "content": "Kernels that use many textures or a large amount of local memory are less likely to execute concurrently with other kernels"
  },
  {
    "id": 2407,
    "content": "Overlap of Data Transfer and Kernel Execution  Some devices can perform an asynchronous memory copy to or from the GPU concurrently with kernel execution"
  },
  {
    "id": 2408,
    "content": "Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration ), which is greater than zero for devices that support it It is also possible to perform an intra-device copy simultaneously with kernel execution (on devices that support the concurrentKernels device property) and/or with copies to or from the device (for devices that support the"
  },
  {
    "id": 2409,
    "content": "asyncEngineCount property) Intra-device copies are initiated using the standard memory copy functions with destination and source addresses residing on the same device"
  },
  {
    "id": 2416,
    "content": "Applications may query this capability by checking the asyncEngineCount device property (see Device Enumeration ), which is equal to 2 for devices that support it"
  },
  {
    "id": 2423,
    "content": "A stream is a sequence of commands (possibly issued by different host threads) that execute in order Different streams, on the other hand, may execute their commands out of order with respect to one another or concurrently; this behavior is not guaranteed and should therefore not be relied upon for correctness (for example, inter-kernel communication is undefined) The commands issued on a stream"
  },
  {
    "id": 2424,
    "content": "may execute when all the dependencies of the command are met The dependencies could be previously launched commands on same stream or dependencies from other streams The successful completion of synchronize call guarantees that all the commands launched are completed"
  },
  {
    "id": 2430,
    "content": "Creation and Destruction of Streams  A stream is defined by creating a stream object and specifying it as the stream parameter to a sequence of kernel launches and host device memory copies"
  },
  {
    "id": 2431,
    "content": "The following code sample creates two streams and allocates an array hostPtr of float in page-locked memory cudaStream_t stream [ 2 ]; for ( int i = 0 ; i >> ( outputDevPtr + i * size , inputDevPtr + i * size , size ); cudaMemcpyAsync ( hostPtr + i * size , outputDevPtr + i * size , size , cudaMemcpyDeviceToHost , stream [ i ]); } Each stream copies its portion of input array hostPtr to array"
  },
  {
    "id": 2432,
    "content": "inputDevPtr in device memory, processes inputDevPtr on the device by calling MyKernel() , and copies the result outputDevPtr back to the same portion of hostPtr"
  },
  {
    "id": 2433,
    "content": "Overlapping Behavior describes how the streams overlap in this example depending on the capability of the device"
  },
  {
    "id": 2434,
    "content": "for ( int i = 0 ; i device memory copies that do not specify any stream parameter, or equivalently that set the stream parameter to zero, are issued to the default stream For code that is compiled using the --default-stream per-thread compilation flag (or that defines the CUDA_API_PER_THREAD_DEFAULT_STREAM macro before including CUDA headers ( cuda"
  },
  {
    "id": 2437,
    "content": "Note #define CUDA_API_PER_THREAD_DEFAULT_STREAM 1 cannot be used to enable this behavior when the code is compiled by nvcc as nvcc implicitly includes cuda_runtime"
  },
  {
    "id": 2439,
    "content": "In this case the --default-stream per-thread compilation flag needs to be used or the CUDA_API_PER_THREAD_DEFAULT_STREAM macro needs to be defined with the -DCUDA_API_PER_THREAD_DEFAULT_STREAM=1 compiler flag For code that is compiled using the --default-stream legacy compilation flag, the default stream is a special stream called the NULL stream and each device has a single NULL stream used for"
  },
  {
    "id": 2440,
    "content": "all host threads The NULL stream is special as it causes implicit synchronization as described in Implicit Synchronization For code that is compiled without specifying a --default-stream compilation flag, --default-stream legacy is assumed as the default"
  },
  {
    "id": 2447,
    "content": "cudaDeviceSynchronize() waits until all preceding commands in all streams of all host threads have completed cudaStreamSynchronize() takes a stream as a parameter and waits until all preceding commands in the given stream have completed"
  },
  {
    "id": 2448,
    "content": "It can be used to synchronize the host with a specific stream, allowing other streams to continue executing on the device"
  },
  {
    "id": 2449,
    "content": "cudaStreamWaitEvent() takes a stream and an event as parameters (see Events for a description of events)and makes all the commands added to the given stream after the call to cudaStreamWaitEvent() delay their execution until the given event has completed cudaStreamQuery() provides applications with a way to know if all preceding commands in a stream have completed"
  },
  {
    "id": 2455,
    "content": "Implicit Synchronization  Two commands from different streams cannot run concurrently if any one of the following operations is issued in-between them by the host thread: a page-locked host memory allocation, a device memory allocation, a device memory set, a memory copy between two addresses to the same device memory, any CUDA command to the NULL stream, a switch between the L1/shared memory"
  },
  {
    "id": 2458,
    "content": "Operations that require a dependency check include any other commands within the same stream as the launch being checked and any call to cudaStreamQuery() on that stream"
  },
  {
    "id": 2459,
    "content": "Therefore, applications should follow these guidelines to improve their potential for concurrent kernel execution: All independent operations should be issued before dependent operations, Synchronization of any kind should be delayed as long as possible"
  },
  {
    "id": 2465,
    "content": "Overlapping Behavior  The amount of execution overlap between two streams depends on the order in which the commands are issued to each stream and whether or not the device supports overlap of data transfer and kernel execution (see Overlap of Data Transfer and Kernel Execution ), concurrent kernel execution (see Concurrent Kernel Execution ), and/or concurrent data transfers (see Concurrent"
  },
  {
    "id": 2466,
    "content": "Data Transfers ) For example, on devices that do not support concurrent data transfers, the two streams of the code sample of Creation and Destruction do not overlap at all because the memory copy from host to device is issued to stream[1] after the memory copy from device to host is issued to stream[0], so it can only start once the memory copy from device to host issued to stream[0] has"
  },
  {
    "id": 2467,
    "content": "completed If the code is rewritten the following way (and assuming the device supports overlap of data transfer and kernel execution) for ( int i = 0 ; i >> ( outputDevPtr + i * size , inputDevPtr + i * size , size ); for ( int i = 0 ; i >> ( devPtrOut [ i ], devPtrIn [ i ], size ); cudaMemcpyAsync ( hostPtr [ i ], devPtrOut [ i ], size , cudaMemcpyDeviceToHost , stream [ i ]); cudaLaunchHostFunc"
  },
  {
    "id": 2468,
    "content": "( stream [ i ], MyCallback , ( void * ) i ); } The commands that are issued in a stream after a host function do not start executing before the function has completed"
  },
  {
    "id": 2469,
    "content": "A host function enqueued into a stream must not make CUDA API calls (directly or indirectly), as it might end up waiting on itself if it makes such a call leading to a deadlock"
  },
  {
    "id": 2475,
    "content": "Stream Priorities  The relative priorities of streams can be specified at creation using cudaStreamCreateWithPriority() The range of allowable priorities, ordered as [ highest priority, lowest priority ] can be obtained using the cudaDeviceGetStreamPriorityRange() function At runtime, pending work in higher-priority streams takes preference over pending work in low-priority streams The following"
  },
  {
    "id": 2476,
    "content": "code sample obtains the allowable range of priorities for the current device, and creates streams with the highest and lowest available priorities get the range of stream priorities for this device int priority_high , priority_low ; cudaDeviceGetStreamPriorityRange ( & priority_low , & priority_high ); create streams with highest and lowest available priorities cudaStream_t st_high , st_low ;"
  },
  {
    "id": 2477,
    "content": "cudaStreamCreateWithPriority ( & st_high , cudaStreamNonBlocking , priority_high ); cudaStreamCreateWithPriority ( & st_low , cudaStreamNonBlocking , priority_low ); 3"
  },
  {
    "id": 2481,
    "content": "Programmatic Dependent Launch and Synchronization  The Programmatic Dependent Launch mechanism allows for a dependent secondary kernel to launch before the primary kernel it depends on in the same CUDA stream has finished executing"
  },
  {
    "id": 2483,
    "content": "0, this technique can provide performance benefits when the secondary kernel can complete significant work that does not depend on the results of the primary kernel"
  },
  {
    "id": 2490,
    "content": "Figure 10 GPU activity timeline  Here, secondary_kernel is launched after primary_kernel finishes its execution Serialized execution is usually necessary because secondary_kernel depends on result data produced by primary_kernel If secondary_kernel has no dependency on primary_kernel , both of them can be launched concurrently by using CUDA streams Even if secondary_kernel is dependent on"
  },
  {
    "id": 2492,
    "content": "For example, almost all the kernels have some sort of preamble section during which tasks such as zeroing buffers or loading constant values are performed"
  },
  {
    "id": 2493,
    "content": "Figure 11 Preamble section of secondary_kernel  Figure 11 demonstrates the portion of secondary_kernel that could be executed concurrently without impacting the application Note that concurrent launch also allows us to hide the launch latency of secondary_kernel behind the execution of primary_kernel Figure 12 Concurrent execution of primary_kernel and secondary_kernel  The concurrent launch"
  },
  {
    "id": 2494,
    "content": "and execution of secondary_kernel shown in Figure 12 is achievable using Programmatic Dependent Launch Programmatic Dependent Launch introduces changes to the CUDA kernel launch APIs as explained in following section"
  },
  {
    "id": 2502,
    "content": "API Description  In Programmatic Dependent Launch, a primary and a secondary kernel are launched in the same CUDA stream The primary kernel should execute cudaTriggerProgrammaticLaunchCompletion with all thread blocks when it’s ready for the secondary kernel to launch __global__ void primary_kernel () { Initial work that should finish before starting secondary kernel Trigger the secondary kernel"
  },
  {
    "id": 2503,
    "content": "cudaTriggerProgrammaticLaunchCompletion (); Work that can coincide with the secondary kernel } __global__ void secondary_kernel () { Independent work Will block until all primary kernels the secondary kernel is dependent on have completed and flushed results to global memory cudaGridDependencySynchronize (); Dependent work } cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ] numAttrs = 1 ;"
  },
  {
    "id": 2504,
    "content": "primary_kernel >> (); cudaLaunchKernelEx ( & configSecondary , secondary_kernel ); When the secondary kernel is launched using the cudaLaunchAttributeProgrammaticStreamSerialization attribute, the CUDA driver is safe to launch the secondary kernel early and not wait on the completion and memory flush of the primary before launching the secondary The CUDA driver can launch the secondary kernel when"
  },
  {
    "id": 2505,
    "content": "all primary thread blocks have launched and executed cudaTriggerProgrammaticLaunchCompletion If the primary kernel doesn’t execute the trigger, it implicitly occurs after all thread blocks in the primary kernel exit In either case, the secondary thread blocks might launch before data written by the primary kernel is visible As such, when the secondary kernel is configured with Programmatic"
  },
  {
    "id": 2506,
    "content": "Dependent Launch , it must always use cudaGridDependencySynchronize or other means to verify that the result data from the primary is available Please note that these methods provide the opportunity for the primary and secondary kernels to execute concurrently, however this behavior is opportunistic and not guaranteed to lead to concurrent kernel execution Reliance on concurrent execution in this"
  },
  {
    "id": 2513,
    "content": "Use in CUDA Graphs  Programmatic Dependent Launch can be used in CUDA Graphs via stream capture or directly via edge data To program this feature in a CUDA Graph with edge data, use a cudaGraphDependencyType value of cudaGraphDependencyTypeProgrammatic on an edge connecting two kernel nodes This edge type makes the upstream kernel visible to a cudaGridDependencySynchronize() in the downstream"
  },
  {
    "id": 2515,
    "content": "This type must be used with an outgoing port of either cudaGraphKernelNodePortLaunchCompletion or cudaGraphKernelNodePortProgrammatic"
  },
  {
    "id": 2516,
    "content": "The resulting graph equivalents for stream capture are as follows: Stream code (abbreviated) Resulting graph edge cudaLaunchAttribute attribute ; attribute from_port = cudaGraphKernelNodePortProgrammatic ; cudaLaunchAttribute attribute ; attribute"
  },
  {
    "id": 2517,
    "content": "A graph is a series of operations, such as kernel launches, connected by dependencies, which is defined separately from its execution"
  },
  {
    "id": 2518,
    "content": "Separating out the definition of a graph from its execution enables a number of optimizations: first, CPU launch costs are reduced compared to streams, because much of the setup is done in advance; second, presenting the whole workflow to CUDA enables optimizations which might not be possible with the piecewise work submission mechanism of streams To see the optimizations possible with graphs,"
  },
  {
    "id": 2519,
    "content": "consider what happens in a stream: when you place a kernel into a stream, the host driver performs a sequence of operations in preparation for the execution of the kernel on the GPU These operations, necessary for setting up and launching the kernel, are an overhead cost which must be paid for each kernel that is issued For a GPU kernel with a short execution time, this overhead cost can be a"
  },
  {
    "id": 2520,
    "content": "significant fraction of the overall end-to-end execution time Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution During the definition phase, a program creates a description of the operations in the graph along with the dependencies between them Instantiation takes a snapshot of the graph template, validates it, and performs much of the"
  },
  {
    "id": 2521,
    "content": "setup and initialization of work with the aim of minimizing what needs to be done at launch It may be launched any number of times without repeating the instantiation"
  },
  {
    "id": 2536,
    "content": "Node Types  A graph node can be one of: kernel CPU function call memory copy memset empty node waiting on an event recording an event signalling an external semaphore waiting on an external semaphore conditional node child graph: To execute a separate nested graph, as shown in the following figure"
  },
  {
    "id": 2537,
    "content": "Edge data modifies a dependency specified by an edge and consists of three parts: an outgoing port, an incoming port, and a type Port values are specific to node type and direction, and edge types may be restricted to specific node types Outgoing port 0 waits on an entire task, incoming port 0 blocks an entire task, and edge type 0 is associated with a full dependency with memory synchronizing"
  },
  {
    "id": 2538,
    "content": "behavior Edge data is optionally specified in various graph APIs via a parallel array to the associated nodes If it is omitted as an output (query) parameter, the API accepts this if the edge data being ignored is all zero-initialized, and returns cudaErrorLossyQuery if the call would discard information Edge data is also available in some stream capture APIs: cudaStreamBeginCaptureToGraph() ,"
  },
  {
    "id": 2539,
    "content": "cudaStreamGetCaptureInfo() , and cudaStreamUpdateCaptureDependencies() The data is associated with a dangling edge (half edge) which will either be connected to a future captured node or discarded at termination of stream capture These edges are ignored when considering if a stream capture has been fully rejoined to the origin stream, and cannot be discarded at the end of capture Currently, no"
  },
  {
    "id": 2541,
    "content": "There is one non-default dependency type, cudaGraphDependencyTypeProgrammatic , which enables Programmatic Dependent Launch between two kernel nodes"
  },
  {
    "id": 2547,
    "content": "Creating a Graph Using Graph APIs  Graphs can be created via two mechanisms: explicit API and stream capture Figure 14 Creating a Graph Using Graph APIs Example  Create the graph - it starts out empty cudaGraphCreate ( & graph , 0 ); For the purpose of this example, we'll create the nodes separately from the dependencies to demonstrate that it can be done in two stages cudaGraphAddKernelNode ("
  },
  {
    "id": 2548,
    "content": "& a , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & b , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & c , graph , NULL , 0 , & nodeParams ); cudaGraphAddKernelNode ( & d , graph , NULL , 0 , & nodeParams ); Now set up dependencies on each node cudaGraphAddDependencies ( graph , & a , & b , 1 ); A->B cudaGraphAddDependencies ( graph , & a , & c , 1 ); A->C"
  },
  {
    "id": 2549,
    "content": "cudaGraphAddDependencies ( graph , & b , & d , 1 ); B->D cudaGraphAddDependencies ( graph , & c , & d , 1 ); C->D 3"
  },
  {
    "id": 2554,
    "content": "Creating a Graph Using Stream Capture  Stream capture provides a mechanism to create a graph from existing stream-based APIs"
  },
  {
    "id": 2555,
    "content": "A section of code which launches work into streams, including existing code, can be bracketed with calls to cudaStreamBeginCapture() and cudaStreamEndCapture() cudaGraph_t graph ; cudaStreamBeginCapture ( stream ); kernel_A >> ("
  },
  {
    "id": 2557,
    "content": "); libraryCall ( stream ); kernel_C >> ( ); cudaStreamEndCapture ( stream , & graph ); A call to cudaStreamBeginCapture() places a stream in capture mode When a stream is being captured, work launched into the stream is not enqueued for execution This graph is then returned by calling cudaStreamEndCapture() , which also ends capture mode for the stream A graph which is actively being constructed"
  },
  {
    "id": 2558,
    "content": "by stream capture is referred to as a capture graph Stream capture can be used on any CUDA stream except cudaStreamLegacy (the “NULL stream”) If a program is using the legacy stream, it may be possible to redefine stream 0 to be the per-thread stream with no functional change Instead of capturing to an internal graph, work is captured to a graph provided by the user"
  },
  {
    "id": 2565,
    "content": "Cross-stream Dependencies and Events  Stream capture can handle cross-stream dependencies expressed with cudaEventRecord() and cudaStreamWaitEvent() , provided the event being waited upon was recorded into the same capture graph When an event is recorded in a stream that is in capture mode, it results in a captured event When a captured event is waited on by a stream, it places the stream in"
  },
  {
    "id": 2566,
    "content": "capture mode if it is not already, and the next item in the stream will have additional dependencies on the nodes in the captured event When cross-stream dependencies are present in stream capture, cudaStreamEndCapture() must still be called in the same stream where cudaStreamBeginCapture() was called; this is the origin stream Any other streams which are being captured to the same capture graph,"
  },
  {
    "id": 2567,
    "content": "due to event-based dependencies, must also be joined back to the origin stream All streams being captured to the same capture graph are taken out of capture mode upon cudaStreamEndCapture() Failure to rejoin to the origin stream will result in failure of the overall capture operation stream1 is the origin stream cudaStreamBeginCapture ( stream1 ); kernel_A >> ( ); Fork into stream2 cudaEventRecord"
  },
  {
    "id": 2570,
    "content": ");   Join stream2 back to origin stream (stream1) cudaEventRecord ( event2 , stream2 ); cudaStreamWaitEvent ( stream1 , event2 ); kernel_D >> ( );   End capture in the origin stream cudaStreamEndCapture ( stream1 , & graph );   stream1 and stream2 no longer in capture mode Graph returned by the above code is shown in Figure 14"
  },
  {
    "id": 2571,
    "content": "Note When a stream is taken out of capture mode, the next non-captured item in the stream (if any) will still have a dependency on the most recent prior non-captured item, despite intermediate items having been removed"
  },
  {
    "id": 2578,
    "content": "Prohibited and Unhandled Operations  It is invalid to synchronize or query the execution status of a stream which is being captured or a captured event, because they do not represent items scheduled for execution It is also invalid to query the execution status of or synchronize a broader handle which encompasses an active stream capture, such as a device or context handle when any associated"
  },
  {
    "id": 2579,
    "content": "stream is in capture mode When any stream in the same context is being captured, and it was not created with cudaStreamNonBlocking , any attempted use of the legacy stream is invalid This is because the legacy stream handle at all times encompasses these other streams; enqueueing to the legacy stream would create a dependency on the streams being captured, and querying it or synchronizing it would"
  },
  {
    "id": 2580,
    "content": "query or synchronize the streams being captured Synchronous APIs, such as cudaMemcpy() , enqueue work to the legacy stream and synchronize it before returning Note As a general rule, when a dependency relation would connect something that is captured with something that was not captured and instead enqueued for execution, CUDA prefers to return an error rather than ignore the dependency An"
  },
  {
    "id": 2581,
    "content": "exception is made for placing a stream into or out of capture mode; this severs a dependency relation between items added to the stream immediately before and after the mode transition It is invalid to merge two separate capture graphs by waiting on a captured event from a stream which is being captured and is associated with a different capture graph than the event It is invalid to wait on a"
  },
  {
    "id": 2582,
    "content": "non-captured event from a stream which is being captured without specifying the cudaEventWaitExternal flag A small number of APIs that enqueue asynchronous operations into streams are not currently supported in graphs and will return an error if called with a stream which is being captured, such as cudaStreamAttachMemAsync()"
  },
  {
    "id": 2589,
    "content": "Invalidation  When an invalid operation is attempted during stream capture, any associated capture graphs are invalidated When a capture graph is invalidated, further use of any streams which are being captured or captured events associated with the graph is invalid and will return an error, until stream capture is ended with cudaStreamEndCapture() This call will take the associated streams out"
  },
  {
    "id": 2596,
    "content": "CUDA User Objects  CUDA User Objects can be used to help manage the lifetime of resources used by asynchronous work in CUDA"
  },
  {
    "id": 2597,
    "content": "Consider for example an event-based pool or a synchronous-create, asynchronous-destroy scheme Library API with pool allocation void libraryWork ( cudaStream_t stream ) { auto & resource = pool recordReadyEvent ( stream ); } Library API with asynchronous resource deletion void libraryWork ( cudaStream_t stream ) { Resource * resource = new Resource ( ); launchWork ( stream , resource );"
  },
  {
    "id": 2598,
    "content": "cudaStreamAddCallback ( stream , []( cudaStream_t , cudaError_t , void * resource ) { delete static_cast ( resource ); }, resource , 0 ); Error handling considerations not shown } These schemes are difficult with CUDA graphs because of the non-fixed pointer or handle for the resource which requires indirection or graph update, and the synchronous CPU code needed each time the work is submitted"
  },
  {
    "id": 2599,
    "content": "They also do not work with stream capture if these considerations are hidden from the caller of the library, and because of use of disallowed APIs during capture"
  },
  {
    "id": 2600,
    "content": "A CUDA user object associates a user-specified destructor callback with an internal refcount, similar to C++ shared_ptr"
  },
  {
    "id": 2601,
    "content": "Note that for user-owned references, unlike C++ smart pointers, there is no object representing the reference; users must track user-owned references manually A typical use case would be to immediately move the sole user-owned reference to a CUDA graph after the user object is created When a reference is associated to a CUDA graph, CUDA will manage the graph operations automatically A cloned"
  },
  {
    "id": 2602,
    "content": "cudaGraph_t retains a copy of every reference owned by the source cudaGraph_t , with the same multiplicity An instantiated cudaGraphExec_t retains a copy of every reference in the source cudaGraph_t When a cudaGraphExec_t is destroyed without being synchronized, the references are retained until the execution is completed"
  },
  {
    "id": 2603,
    "content": "If the destructor callback had signaled a synchronization object, it would   be safe to wait on it at this point"
  },
  {
    "id": 2604,
    "content": "References owned by graphs in child graph nodes are associated to the child graphs, not the parents If an executable graph or child graph is updated with cudaGraphExecUpdate or cudaGraphExecChildGraphNodeSetParams , the references in the new source graph are cloned and replace the references in the target graph"
  },
  {
    "id": 2605,
    "content": "In either case, if previous launches are not synchronized, any references which would be released are held until the launches have finished executing"
  },
  {
    "id": 2606,
    "content": "In addition, it is not legal to call CUDA APIs from the destructor, similar to the restriction on cudaLaunchHostFunc It is legal to signal another thread to perform an API call, if the dependency is one way and the thread doing the call cannot block forward progress of CUDA work"
  },
  {
    "id": 2607,
    "content": "User objects are created with cudaUserObjectCreate , which is a good starting point to browse related APIs"
  },
  {
    "id": 2613,
    "content": "Updating Instantiated Graphs  Work submission using graphs is separated into three distinct stages: definition, instantiation, and execution In situations where the workflow is not changing, the overhead of definition and instantiation can be amortized over many executions, and graphs provide a clear advantage over streams"
  },
  {
    "id": 2614,
    "content": "A graph is a snapshot of a workflow, including kernels, parameters, and dependencies, in order to replay it as rapidly and efficiently as possible In situations where the workflow changes the graph becomes out of date and must be modified Major changes to graph structure such as topology or types of nodes will require re-instantiation of the source graph because various topology-related"
  },
  {
    "id": 2615,
    "content": "optimization techniques must be re-applied The cost of repeated instantiation can reduce the overall performance benefit from graph execution, but it is common for only node parameters, such as kernel parameters and cudaMemcpy addresses, to change while graph topology remains the same For this case, CUDA provides a lightweight mechanism known as “Graph Update,” which allows certain node parameters"
  },
  {
    "id": 2616,
    "content": "to be modified in-place without having to rebuild the entire graph Updates will take effect the next time the graph is launched, so they will not impact previous graph launches, even if they are running at the time of the update A graph may be updated and relaunched repeatedly, so multiple updates/launches can be queued on a stream CUDA provides two mechanisms for updating instantiated graph"
  },
  {
    "id": 2617,
    "content": "parameters, whole graph update and individual node update Whole graph update allows the user to supply a topologically identical cudaGraph_t object whose nodes contain updated parameters Individual node update allows the user to explicitly update the parameters of individual nodes Using an updated cudaGraph_t is more convenient when a large number of nodes are being updated, or when the graph"
  },
  {
    "id": 2621,
    "content": "Using individual node update is preferred when the number of changes is small and the user has the handles to the nodes requiring updates Individual node update skips the topology checks and comparisons for unchanged nodes, so it can be more efficient in many cases"
  },
  {
    "id": 2622,
    "content": "CUDA also provides a mechanism for enabling and disabling individual nodes without affecting their current parameters"
  },
  {
    "id": 2631,
    "content": "A node whose function originally did not use CUDA dynamic parallelism cannot be updated to a function which uses CUDA dynamic parallelism"
  },
  {
    "id": 2632,
    "content": "cudaMemset and cudaMemcpy nodes: The CUDA device(s) to which the operand(s) was allocated/mapped cannot change"
  },
  {
    "id": 2633,
    "content": "The source/destination memory must be allocated from the same context as the original source/destination memory Additional memcpy node restrictions: Changing either the source or destination memory type (i"
  },
  {
    "id": 2636,
    "content": "External semaphore wait nodes and record nodes: Changing the number of semaphores is not supported Conditional nodes: The order of handle creation and assignment must match between the graphs Changing parameters of nodes within the conditional body graph is subject to the rules above There are no restrictions on updates to host nodes, event record nodes, or event wait nodes"
  },
  {
    "id": 2643,
    "content": "Whole Graph Update  cudaGraphExecUpdate() allows an instantiated graph (the “original graph”) to be updated with the parameters from a topologically identical graph (the “updating” graph) The topology of the updating graph must be identical to the original graph used to instantiate the cudaGraphExec_t More explicitly, following the following rules will cause cudaGraphExecUpdate() to pair the"
  },
  {
    "id": 2644,
    "content": "nodes in the original graph and the updating graph deterministically: For any capturing stream, the API calls operating on that stream must be made in the same order, including event wait and other api calls not directly corresponding to node creation The API calls which directly manipulate a given graph node’s incoming edges (including captured stream APIs, node add APIs, and edge addition /"
  },
  {
    "id": 2645,
    "content": "removal APIs) must be made in the same order Moreover, when dependencies are specified in arrays to these APIs, the order in which the dependencies are specified inside those arrays must match Sink nodes are nodes without dependent nodes / outgoing edges in the final graph at the time of the cudaGraphExecUpdate() invocation The following operations affect sink node ordering (if present) and must"
  },
  {
    "id": 2646,
    "content": "(as a combined set) be made in the same order: Node add APIs resulting in a sink node cudaStreamUpdateCaptureDependencies() , if it removes a sink node from a capturing stream’s dependency set The following example shows how the API could be used to update an instantiated graph: cudaGraphExec_t graphExec = NULL ; for ( int i = 0 ; i >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 );"
  },
  {
    "id": 2647,
    "content": "cudaGraphInstantiate ( & gExec1 , g1 ); Launch the host graph, which will in turn launch the device graph cudaGraphLaunch ( gExec1 , stream ); } A graph can have up to 120 total fire-and-forget graphs during the course of its execution This total resets between launches of the same parent graph"
  },
  {
    "id": 2656,
    "content": "Graph Execution Environments  In order to fully understand the device-side synchronization model, it is first necessary to understand the concept of an execution environment When a graph is launched from the device, it is launched into its own execution environment The execution environment of a given graph encapsulates all work in the graph as well as all generated fire and forget work The"
  },
  {
    "id": 2657,
    "content": "graph can be considered complete when it has completed execution and when all generated child work is complete The below diagram shows the environment encapsulation that would be generated by the fire-and-forget sample code in the previous section Figure 16 Fire and forget launch, with execution environments  These environments are also hierarchical, so a graph environment can include multiple"
  },
  {
    "id": 2658,
    "content": "levels of child-environments from fire and forget launches Figure 17 Nested fire and forget environments  When a graph is launched from the host, there exists a stream environment that parents the execution environment of the launched graph downstream dependent work may now run) when the overall stream environment is marked as complete"
  },
  {
    "id": 2659,
    "content": "Tail Launch  Unlike on the host, it is not possible to synchronize with device graphs from the GPU via traditional methods such as cudaDeviceSynchronize() or cudaStreamSynchronize() Rather, in order to enable serial work dependencies, a different launch mode - tail launch - is offered, to provide similar functionality A tail launch executes when a graph’s environment is considered complete - ie,"
  },
  {
    "id": 2660,
    "content": "when the graph and all its children are complete When a graph completes, the environment of the next graph in the tail launch list will replace the completed environment as a child of the parent environment Figure 19 A simple tail launch  The above execution flow can be generated by the code below: __global__ void launchTailGraph ( cudaGraphExec_t graph ) { cudaGraphLaunch ( graph ,"
  },
  {
    "id": 2661,
    "content": "cudaStreamGraphTailLaunch ); } void graphSetup () { cudaGraphExec_t gExec1 , gExec2 ; cudaGraph_t g1 , g2 ; Create, instantiate, and upload the device graph create_graph ( & g2 ); cudaGraphInstantiate ( & gExec2 , g2 , cudaGraphInstantiateFlagDeviceLaunch ); cudaGraphUpload ( gExec2 , stream ); Create and instantiate the launching graph cudaStreamBeginCapture ( stream , cudaStreamCaptureModeGlobal"
  },
  {
    "id": 2662,
    "content": "); launchTailGraph >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 ); Launch the host graph, which will in turn launch the device graph cudaGraphLaunch ( gExec1 , stream ); } Tail launches enqueued by a given graph will execute one at a time, in order of when they were enqueued Figure 20 Tail launch ordering  Tail launches enqueued by a tail graph will"
  },
  {
    "id": 2663,
    "content": "execute before tail launches enqueued by previous graphs in the tail launch list Figure 21 Tail launch ordering when enqueued from multiple graphs  A graph can have up to 255 pending tail launches"
  },
  {
    "id": 2673,
    "content": "Tail Self-launch  It is possible for a device graph to enqueue itself for a tail launch, although a given graph can only have one self-launch enqueued at a time In order to query the currently running device graph so that it can be relaunched, a new device-side function is added: cudaGraphExec_t cudaGetCurrentGraphExec (); This function returns the handle of the currently running graph if it is"
  },
  {
    "id": 2674,
    "content": "a device graph If the currently executing kernel is not a node within a device graph, this function will return NULL"
  },
  {
    "id": 2675,
    "content": "Below is sample code showing usage of this function for a relaunch loop: __device__ int relaunchCount = 0 ; __global__ void relaunchSelf () { int relaunchMax = 100 ; if ( threadIdx"
  },
  {
    "id": 2676,
    "content": "x == 0 ) { if ( relaunchCount >> ( gExec2 ); cudaStreamEndCapture ( stream , & g1 ); cudaGraphInstantiate ( & gExec1 , g1 );   Launch the host graph, which will in turn launch the device graph cudaGraphLaunch ( gExec1 , stream ); } Since sibling launches are not launched into the launching graph’s execution environment, they will not gate tail launches enqueued by the launching graph"
  },
  {
    "id": 2682,
    "content": "Conditional Graph Nodes  Conditional nodes allow conditional execution and looping of a graph contained within the conditional node"
  },
  {
    "id": 2683,
    "content": "This allows dynamic and iterative workflows to be represented completely within a graph and frees up the host CPU to perform other work in parallel"
  },
  {
    "id": 2684,
    "content": "Evaluation of the condition value is performed on the device when the dependencies of the conditional node have been met Conditional nodes can be one of the following types: Conditional IF nodes execute their body graph once if the condition value is non-zero when the node is executed Conditional WHILE nodes execute their body graph if the condition value is non-zero when the node is executed and"
  },
  {
    "id": 2685,
    "content": "will continue to execute their body graph until the condition value is zero A condition value is accessed by a conditional handle , which must be created before the node A default value, applied on each graph launch, can also be specified when the handle is created When the conditional node is created, an empty graph is created and the handle is returned to the user so that the graph can be"
  },
  {
    "id": 2686,
    "content": "populated This conditional body graph can be populated using either the graph APIs or cudaStreamBeginCaptureToGraph() Conditional nodes can be nested"
  },
  {
    "id": 2693,
    "content": "Conditional Handles  A condition value is represented by cudaGraphConditionalHandle and is created by cudaGraphConditionalHandleCreate() If cudaGraphCondAssignDefault is specified when the handle is created, the condition value will be initialized to the specified default before every graph launch If this flag is not provided, it is up to the user to initialize the condition value in a kernel"
  },
  {
    "id": 2694,
    "content": "upstream of the conditional node which tests it If the condition value is not initialized by one of these methods, its value is undefined The default value and flags associated with a handle will be updated during whole graph update"
  },
  {
    "id": 2701,
    "content": "Condtional Node Body Graph Requirements  General requirements: The graph’s nodes must all reside on a single device The graph can only contain kernel nodes, empty nodes, memcpy nodes, memset nodes, child graph nodes, and conditional nodes Memcpy/Memset nodes: Only copies/memsets involving device memory and/or pinned device-mapped host memory are permitted Note that the copy operation will be"
  },
  {
    "id": 2702,
    "content": "performed from the device on which the graph resides, even if it is targeting memory on another device"
  },
  {
    "id": 2709,
    "content": "Conditional IF Nodes  The body graph of an IF node will be executed once if the condition is non-zero when the node is executed The following diagram depicts a 3 node graph where the middle node, B, is a conditional node: Figure 23 Conditional IF Node  The following code illustrates the creation of a graph containing an IF conditional node } void graphSetup () { cudaGraph_t graph ;"
  },
  {
    "id": 2710,
    "content": "cudaGraphExec_t graphExec ; cudaGraphNode_t node ; void * kernelArgs [ 1 ]; int value = 1 ; cudaGraphCreate ( & graph , 0 ); cudaGraphConditionalHandle handle ; cudaGraphConditionalHandleCreate ( & handle , graph ); Use a kernel upstream of the conditional to set the handle value cudaGraphNodeParams params = { cudaGraphNodeTypeKernel }; params kernelParams = kernelArgs ; kernelArgs [ 0 ] = &"
  },
  {
    "id": 2711,
    "content": "handle ; cudaGraphAddNode ( & node , graph , NULL , 0 , & params ); cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional }; cParams size = 1 ; cudaGraphAddNode ( & node , graph , & node , 1 , & cParams ); cudaGraph_t bodyGraph = cParams cudaGraphAddNode ( & node , bodyGraph , NULL , 0 , & params ); cudaGraphInstantiate ( & graphExec , graph , NULL , NULL , 0 ); cudaGraphLaunch ( graphExec"
  },
  {
    "id": 2712,
    "content": ", 0 ); cudaDeviceSynchronize (); cudaGraphExecDestroy ( graphExec ); cudaGraphDestroy ( graph ); } 3"
  },
  {
    "id": 2718,
    "content": "Conditional WHILE Nodes  The body graph of a WHILE node will be executed until the condition is non-zero The condition will be evaluated when the node is executed and after completion of the body graph The following diagram depicts a 3 node graph where the middle node, B, is a conditional node: Figure 24 Conditional WHILE Node  The following code illustrates the creation of a graph containing a"
  },
  {
    "id": 2721,
    "content": "__global__ void loopKernel ( cudaGraphConditionalHandle handle ) { static int count = 10 ; cudaGraphSetConditional ( handle , -- count 1 : 0 ); } void graphSetup () { cudaGraph_t graph ; cudaGraphExec_t graphExec ; cudaGraphNode_t node ; void * kernelArgs [ 1 ]; cuGraphCreate ( & graph , 0 ); cudaGraphConditionalHandle handle ; cudaGraphConditionalHandleCreate ( & handle , graph , 1 ,"
  },
  {
    "id": 2722,
    "content": "cudaGraphCondAssignDefault ); cudaGraphNodeParams cParams = { cudaGraphNodeTypeConditional }; cParams size = 1 ; cudaGraphAddNode ( & node , graph , NULL , 0 , & cParams ); cudaGraph_t bodyGraph = cParams kernelParams = kernelArgs ; kernelArgs [ 0 ] = & handle ; cudaGraphAddNode ( & node , bodyGraph , NULL , 0 , & params ); cudaGraphInstantiate ( & graphExec , graph , NULL , NULL , 0 );"
  },
  {
    "id": 2723,
    "content": "cudaGraphLaunch ( graphExec , 0 ); cudaDeviceSynchronize (); cudaGraphExecDestroy ( graphExec ); cudaGraphDestroy ( graph ); } 3"
  },
  {
    "id": 2727,
    "content": "Events  The runtime also provides a way to closely monitor the device’s progress, as well as perform accurate timing, by letting the application asynchronously record events at any point in the program, and query when these events are completed"
  },
  {
    "id": 2728,
    "content": "An event has completed when all tasks - or optionally, all commands in a given stream - preceding the event have completed Events in stream zero are completed after all preceding tasks and commands in all streams are completed"
  },
  {
    "id": 2734,
    "content": "Creation and Destruction of Events  The following code sample creates two events: cudaEvent_t start , stop ; cudaEventCreate ( & start ); cudaEventCreate ( & stop ); They are destroyed this way: cudaEventDestroy ( start ); cudaEventDestroy ( stop ); 3"
  },
  {
    "id": 2739,
    "content": "Elapsed Time  The events created in Creation and Destruction can be used to time the code sample of Creation and Destruction the following way: cudaEventRecord ( start , 0 ); for ( int i = 0 ; i >> ( outputDev + i * size , inputDev + i * size , size ); cudaMemcpyAsync ( outputHost + i * size , outputDev + i * size , size , cudaMemcpyDeviceToHost , stream [ i ]); } cudaEventRecord ( stop , 0 );"
  },
  {
    "id": 2740,
    "content": "cudaEventSynchronize ( stop ); float elapsedTime ; cudaEventElapsedTime ( & elapsedTime , start , stop ); 3"
  },
  {
    "id": 2744,
    "content": "Synchronous Calls  When a synchronous function is called, control is not returned to the host thread before the device has completed the requested task Whether the host thread will then yield, block, or spin can be specified by calling cudaSetDeviceFlags() with some specific flags (see reference manual for details) before any other CUDA call is performed by the host thread"
  },
  {
    "id": 2752,
    "content": "The following code sample shows how to enumerate these devices, query their properties, and determine the number of CUDA-enabled devices"
  },
  {
    "id": 2753,
    "content": "int deviceCount ; cudaGetDeviceCount ( & deviceCount ); int device ; for ( device = 0 ; device >> ( p0 );   Launch kernel on device 0 cudaSetDevice ( 1 );   Set device 1 as current float * p1 ; cudaMalloc ( & p1 , size );   Allocate memory on device 1 MyKernel >> ( p1 );   Launch kernel on device 1 3"
  },
  {
    "id": 2757,
    "content": "Stream and Event Behavior  A kernel launch will fail if it is issued to a stream that is not associated to the current device as illustrated in the following code sample cudaSetDevice ( 0 ); Set device 0 as current cudaStream_t s0 ; cudaStreamCreate ( & s0 ); Create stream s0 on device 0 MyKernel >> (); Launch kernel on device 0 in s0 cudaSetDevice ( 1 ); Set device 1 as current cudaStream_t s1"
  },
  {
    "id": 2758,
    "content": "; cudaStreamCreate ( & s1 ); Create stream s1 on device 1 MyKernel >> (); Launch kernel on device 1 in s1 This kernel launch will fail: MyKernel >> (); Launch kernel on device 1 in s0 A memory copy will succeed even if it is issued to a stream that is not associated to the current device cudaEventRecord() will fail if the input event and input stream are associated to different devices"
  },
  {
    "id": 2759,
    "content": "cudaEventElapsedTime() will fail if the two input events are associated to different devices cudaEventSynchronize() and cudaEventQuery() will succeed even if the input event is associated to a device that is different from the current device cudaStreamWaitEvent() will succeed even if the input stream and input event are associated to different devices cudaStreamWaitEvent() can therefore be used to"
  },
  {
    "id": 2760,
    "content": "synchronize multiple devices with each other Each device has its own default stream (see Default Stream ), so commands issued to the default stream of a device may execute out of order or concurrently with respect to commands issued to the default stream of any other device"
  },
  {
    "id": 2765,
    "content": "Peer-to-Peer Memory Access  Depending on the system properties, specifically the PCIe and/or NVLINK topology, devices are able to address each other’s memory (i"
  },
  {
    "id": 2768,
    "content": "This peer-to-peer memory access feature is supported between two devices if cudaDeviceCanAccessPeer() returns true for these two devices Peer-to-peer memory access is only supported in 64-bit applications and must be enabled between two devices by calling cudaDeviceEnablePeerAccess() as illustrated in the following code sample On non-NVSwitch enabled systems, each device can support a system-wide"
  },
  {
    "id": 2769,
    "content": "maximum of eight peer connections A unified address space is used for both devices (see Unified Virtual Address Space ), so the same pointer can be used to address memory from both devices as shown in the code sample below cudaSetDevice ( 0 ); Set device 0 as current float * p0 ; size_t size = 1024 * sizeof ( float ); cudaMalloc ( & p0 , size ); Allocate memory on device 0 MyKernel >> ( p0 );"
  },
  {
    "id": 2770,
    "content": "Launch kernel on device 0 cudaSetDevice ( 1 ); Set device 1 as current cudaDeviceEnablePeerAccess ( 0 , 0 ); Enable peer-to-peer access with device 0 Launch kernel on device 1 This kernel launch can access memory on device 0 at address p0 MyKernel >> ( p0 ); 3"
  },
  {
    "id": 2775,
    "content": "IOMMU on Linux  On Linux only, CUDA and the display driver does not support IOMMU-enabled bare-metal PCIe peer to peer memory copy As a consequence, users on Linux, when running on a native bare metal system, should disable the IOMMU The IOMMU should be enabled and the VFIO driver be used as a PCIe pass through for virtual machines"
  },
  {
    "id": 2781,
    "content": "Peer-to-Peer Memory Copy  Memory copies can be performed between the memories of two different devices When a unified address space is used for both devices (see Unified Virtual Address Space ), this is done using the regular memory copy functions mentioned in Device Memory"
  },
  {
    "id": 2782,
    "content": "Otherwise, this is done using cudaMemcpyPeer() , cudaMemcpyPeerAsync() , cudaMemcpy3DPeer() , or cudaMemcpy3DPeerAsync() as illustrated in the following code sample"
  },
  {
    "id": 2783,
    "content": "cudaSetDevice ( 0 ); Set device 0 as current float * p0 ; size_t size = 1024 * sizeof ( float ); cudaMalloc ( & p0 , size ); Allocate memory on device 0 cudaSetDevice ( 1 ); Set device 1 as current float * p1 ; cudaMalloc ( & p1 , size ); Allocate memory on device 1 cudaSetDevice ( 0 ); Set device 0 as current MyKernel >> ( p0 ); Launch kernel on device 0 cudaSetDevice ( 1 ); Set device 1 as"
  },
  {
    "id": 2784,
    "content": "current cudaMemcpyPeer ( p1 , 1 , p0 , 0 , size ); Copy p0 to p1 MyKernel >> ( p1 ); Launch kernel on device 1 A copy (in the implicit NULL stream) between the memories of two different devices: does not start until all commands previously issued to either device have completed and runs to completion before any commands (see Asynchronous Concurrent Execution ) issued after the copy to either"
  },
  {
    "id": 2786,
    "content": "Consistent with the normal behavior of streams, an asynchronous copy between the memories of two devices may overlap with copies or kernels in another stream"
  },
  {
    "id": 2787,
    "content": "Note that if peer-to-peer access is enabled between two devices via cudaDeviceEnablePeerAccess() as described in Peer-to-Peer Memory Access , peer-to-peer memory copy between these two devices no longer needs to be staged through the host and is therefore faster"
  },
  {
    "id": 2791,
    "content": "Unified Virtual Address Space  When the application is run as a 64-bit process, a single address space is used for the host and all the devices of compute capability 2"
  },
  {
    "id": 2793,
    "content": "All host memory allocations made via CUDA API calls and all device memory allocations on supported devices are within this virtual address range As a consequence: The location of any memory on the host allocated through CUDA, or on any of the devices which use the unified address space, can be determined from the value of the pointer using cudaPointerGetAttributes() When copying to or from the"
  },
  {
    "id": 2794,
    "content": "memory of any device which uses the unified address space, the cudaMemcpyKind parameter of cudaMemcpy*() can be set to cudaMemcpyDefault to determine locations from the pointers This also works for host pointers not allocated through CUDA, as long as the current device uses unified addressing Allocations via cudaHostAlloc() are automatically portable (see Portable Memory ) across all the devices"
  },
  {
    "id": 2795,
    "content": "for which the unified address space is used, and pointers returned by cudaHostAlloc() can be used directly from within kernels running on these devices (i"
  },
  {
    "id": 2797,
    "content": ", there is no need to obtain a device pointer via cudaHostGetDevicePointer() as described in Mapped Memory"
  },
  {
    "id": 2798,
    "content": "Applications may query if the unified address space is used for a particular device by checking that the unifiedAddressing device property (see Device Enumeration ) is equal to 1"
  },
  {
    "id": 2802,
    "content": "Interprocess Communication  Any device memory pointer or event handle created by a host thread can be directly referenced by any other thread within the same process It is not valid outside this process however, and therefore cannot be directly referenced by threads belonging to a different process To share device memory pointers and events across processes, an application must use the Inter"
  },
  {
    "id": 2803,
    "content": "Process Communication API, which is described in detail in the reference manual The IPC API is only supported for 64-bit processes on Linux and for devices of compute capability 2"
  },
  {
    "id": 2805,
    "content": "Using this API, an application can get the IPC handle for a given device memory pointer using cudaIpcGetMemHandle() , pass it to another process using standard IPC mechanisms (for example, interprocess shared memory or files), and use cudaIpcOpenMemHandle() to retrieve a device pointer from the IPC handle that is a valid pointer within this other process"
  },
  {
    "id": 2806,
    "content": "Note that allocations made by cudaMalloc() may be sub-allocated from a larger block of memory for performance reasons In such case, CUDA IPC APIs will share the entire underlying memory block which may cause other sub-allocations to be shared, which can potentially lead to information disclosure between processes To prevent this behavior, it is recommended to only share allocations with a 2MiB"
  },
  {
    "id": 2808,
    "content": "An example of using the IPC API is where a single primary process generates a batch of input data, making the data available to multiple secondary processes without requiring regeneration or copying"
  },
  {
    "id": 2809,
    "content": "Applications using CUDA IPC to communicate with each other should be compiled, linked, and run with the same CUDA driver and runtime"
  },
  {
    "id": 2811,
    "content": "5, only events-sharing IPC APIs are supported on L4T and embedded Linux Tegra devices with compute capability 7"
  },
  {
    "id": 2817,
    "content": "Error Checking  All runtime functions return an error code, but for an asynchronous function (see Asynchronous Concurrent Execution ), this error code cannot possibly report any of the asynchronous errors that could occur on the device since the function returns before the device has completed the task; the error code only reports errors that occur on the host prior to executing the task,"
  },
  {
    "id": 2818,
    "content": "typically related to parameter validation; if an asynchronous error occurs, it will be reported by some subsequent unrelated runtime function call The only way to check for asynchronous errors just after some asynchronous function call is therefore to synchronize just after the call by calling cudaDeviceSynchronize() (or by using any other synchronization mechanisms described in Asynchronous"
  },
  {
    "id": 2819,
    "content": "Concurrent Execution ) and checking the error code returned by cudaDeviceSynchronize() The runtime maintains an error variable for each host thread that is initialized to cudaSuccess and is overwritten by the error code every time an error occurs (be it a parameter validation error or an asynchronous error) Kernel launches do not return any error code, so cudaPeekAtLastError() or"
  },
  {
    "id": 2820,
    "content": "cudaGetLastError() must be called just after the kernel launch to retrieve any pre-launch errors To ensure that any error returned by cudaPeekAtLastError() or cudaGetLastError() does not originate from calls prior to the kernel launch, one has to make sure that the runtime error variable is set to cudaSuccess just before the kernel launch, for example, by calling cudaGetLastError() just before the"
  },
  {
    "id": 2821,
    "content": "kernel launch Kernel launches are asynchronous, so to check for asynchronous errors, the application must synchronize in-between the kernel launch and the call to cudaPeekAtLastError() or cudaGetLastError() Note that cudaErrorNotReady that may be returned by cudaStreamQuery() and cudaEventQuery() is not considered an error and is therefore not reported by cudaPeekAtLastError() or"
  },
  {
    "id": 2826,
    "content": "Call Stack  On devices of compute capability 2 x and higher, the size of the call stack can be queried using cudaDeviceGetLimit() and set using cudaDeviceSetLimit() When the call stack overflows, the kernel call fails with a stack overflow error if the application is run via a CUDA debugger (CUDA-GDB, Nsight) or an unspecified launch error, otherwise When the compiler cannot determine the stack"
  },
  {
    "id": 2827,
    "content": "size, it issues a warning saying Stack size cannot be statically determined Once this warning is issued, user will need to set stack size manually if default stack size is not sufficient"
  },
  {
    "id": 2831,
    "content": "Texture and Surface Memory  CUDA supports a subset of the texturing hardware that the GPU uses for graphics to access texture and surface memory Reading data from texture or surface memory instead of global memory can have several performance benefits as described in Device Memory Accesses"
  },
  {
    "id": 2836,
    "content": "Texture Memory  Texture memory is read from kernels using the device functions described in Texture Functions The process of reading a texture calling one of these functions is called a texture fetch Each texture fetch specifies a parameter called a texture object for the texture object API The texture object specifies: The texture , which is the piece of texture memory that is fetched Texture"
  },
  {
    "id": 2837,
    "content": "objects are created at runtime and the texture is specified when creating the texture object as described in Texture Object API Its dimensionality that specifies whether the texture is addressed as a one dimensional array using one texture coordinate, a two-dimensional array using two texture coordinates, or a three-dimensional array using three texture coordinates Table 21 lists the maximum"
  },
  {
    "id": 2839,
    "content": "The type of a texel, which is restricted to the basic integer and single-precision floating-point types and any of the 1-, 2-, and 4-component vector types defined in Built-in Vector Types that are derived from the basic integer and single-precision floating-point types"
  },
  {
    "id": 2840,
    "content": "The read mode , which is equal to cudaReadModeNormalizedFloat or cudaReadModeElementType If it is cudaReadModeNormalizedFloat and the type of the texel is a 16-bit or 8-bit integer type, the value returned by the texture fetch is actually returned as floating-point type and the full range of the integer type is mapped to [0"
  },
  {
    "id": 2844,
    "content": "0] for signed integer type; for example, an unsigned 8-bit texture element with the value 0xff reads as 1"
  },
  {
    "id": 2845,
    "content": "By default, textures are referenced (by the functions of Texture Functions ) using floating-point coordinates in the range [0, N-1] where N is the size of the texture in the dimension corresponding to the coordinate For example, a texture that is 64x32 in size will be referenced with coordinates in the range [0, 63] and [0, 31] for the x and y dimensions, respectively Normalized texture"
  },
  {
    "id": 2848,
    "content": "0-1/N] instead of [0, N-1], so the same 64x32 texture would be addressed by normalized coordinates in the range [0, 1-1/N] in both the x and y dimensions Normalized texture coordinates are a natural fit to some applications’ requirements, if it is preferable for the texture coordinates to be independent of the texture size"
  },
  {
    "id": 2850,
    "content": "8 with coordinates that are out of range The default addressing mode is to clamp the coordinates to the valid range: [0, N) for non-normalized coordinates and [0"
  },
  {
    "id": 2852,
    "content": "0) for normalized coordinates If the border mode is specified instead, texture fetches with out-of-range texture coordinates return zero"
  },
  {
    "id": 2853,
    "content": "When using the wrap mode, each coordinate x is converted to frac(x)=x - floor(x) where floor(x) is the largest integer not greater than x When using the mirror mode, each coordinate x is converted to frac(x) if floor(x) is even and 1-frac(x) if floor(x) is odd"
  },
  {
    "id": 2854,
    "content": "The addressing mode is specified as an array of size three whose first, second, and third elements specify the addressing mode for the first, second, and third texture coordinates, respectively; the addressing mode are cudaAddressModeBorder , cudaAddressModeClamp , cudaAddressModeWrap , and cudaAddressModeMirror ; cudaAddressModeWrap and cudaAddressModeMirror are only supported for normalized"
  },
  {
    "id": 2855,
    "content": "texture coordinates The filtering mode which specifies how the value returned when fetching the texture is computed based on the input texture coordinates Linear texture filtering may be done only for textures that are configured to return floating-point data When enabled, the texels surrounding a texture fetch location are read and the return value of the texture fetch is interpolated based on"
  },
  {
    "id": 2856,
    "content": "where the texture coordinates fell between the texels Simple linear interpolation is performed for one-dimensional textures, bilinear interpolation for two-dimensional textures, and trilinear interpolation for three-dimensional textures If it is cudaFilterModePoint , the returned value is the texel whose texture coordinates are the closest to the input texture coordinates If it is"
  },
  {
    "id": 2857,
    "content": "cudaFilterModeLinear , the returned value is the linear interpolation of the two (for a one-dimensional texture), four (for a two dimensional texture), or eight (for a three dimensional texture) texels whose texture coordinates are the closest to the input texture coordinates Cubemap Textures and Cubemap Layered Textures describe a special type of texture, the cubemap texture"
  },
  {
    "id": 2858,
    "content": "Simple transformation kernel __global__ void transformKernel ( float * output , cudaTextureObject_t texObj , int width , int height , float theta ) {   Calculate normalized texture coordinates unsigned int x = blockIdx"
  },
  {
    "id": 2859,
    "content": "16-Bit Floating-Point Textures  The 16-bit floating-point or half format supported by CUDA arrays is the same as the IEEE 754-2008 binary2 format CUDA C++ does not support a matching data type, but provides intrinsic functions to convert to and from the 32-bit floating-point format via the unsigned short type: __float2half_rn(float) and __half2float(unsigned short) 16-bit floating-point"
  },
  {
    "id": 2860,
    "content": "components are promoted to 32 bit float during texture fetching before any filtering is performed A channel description for the 16-bit floating-point format can be created by calling one of the cudaCreateChannelDescHalf*() functions"
  },
  {
    "id": 2866,
    "content": "Layered Textures  A one-dimensional or two-dimensional layered texture (also known as texture array in Direct3D and array texture in OpenGL) is a texture made up of a sequence of layers, all of which are regular textures of same dimensionality, size, and data type A one-dimensional layered texture is addressed using an integer index and a floating-point texture coordinate; the index denotes a"
  },
  {
    "id": 2867,
    "content": "layer within the sequence and the coordinate addresses a texel within that layer A two-dimensional layered texture is addressed using an integer index and two floating-point texture coordinates; the index denotes a layer within the sequence and the coordinates address a texel within that layer A layered texture can only be a CUDA array by calling cudaMalloc3DArray() with the cudaArrayLayered flag"
  },
  {
    "id": 2868,
    "content": "(and a height of zero for one-dimensional layered texture) Layered textures are fetched using the device functions described in tex1DLayered() and tex2DLayered() Layered textures are only supported on devices of compute capability 2"
  },
  {
    "id": 2875,
    "content": "Cubemap Textures  A cubemap texture is a special type of two-dimensional layered texture that has six layers representing the faces of a cube: The width of a layer is equal to its height The cubemap is addressed using three texture coordinates x , y , and z that are interpreted as a direction vector emanating from the center of the cube and pointing to one face of the cube and a texel within the"
  },
  {
    "id": 2876,
    "content": "layer corresponding to that face More specifically, the face is selected by the coordinate with largest magnitude m and the corresponding layer is addressed using coordinates (s/m+1)/2 and (t/m+1)/2 where s and t are defined in Table 3"
  },
  {
    "id": 2877,
    "content": "Table 3 Cubemap Fetch  face m s t |x| > |y| and |x| > |z| x ≥ 0 0 x -z -y x |x| and |y| > |z| y ≥ 0 2 y x z y |x| and |z| > |y| z ≥ 0 4 z x -y z >> ( inputSurfObj , outputSurfObj , width , height ); Copy data from device back to host cudaMemcpy2DFromArray ( h_data , spitch , cuOutputArray , 0 , 0 , 4 * width * sizeof ( unsigned char ), height , cudaMemcpyDeviceToHost ); Destroy surface objects"
  },
  {
    "id": 2878,
    "content": "cudaDestroySurfaceObject ( inputSurfObj ); cudaDestroySurfaceObject ( outputSurfObj ); Free device memory cudaFreeArray ( cuInputArray ); cudaFreeArray ( cuOutputArray ); Free host memory free ( h_data ); return 0 ; } 3"
  },
  {
    "id": 2883,
    "content": "Cubemap Surfaces  Cubemap surfaces are accessed using surfCubemapread() and surfCubemapwrite() ( surfCubemapread and surfCubemapwrite ) as a two-dimensional layered surface, i"
  },
  {
    "id": 2885,
    "content": ", using an integer index denoting a face and two floating-point texture coordinates addressing a texel within the layer corresponding to this face"
  },
  {
    "id": 2892,
    "content": "Cubemap Layered Surfaces  Cubemap layered surfaces are accessed using surfCubemapLayeredread() and surfCubemapLayeredwrite() ( surfCubemapLayeredread() and surfCubemapLayeredwrite() ) as a two-dimensional layered surface, i"
  },
  {
    "id": 2894,
    "content": ", using an integer index denoting a face of one of the cubemaps and two floating-point texture coordinates addressing a texel within the layer corresponding to this face Faces are ordered as indicated in Table 3 , so index ((2 * 6) + 3), for example, accesses the fourth face of the third cubemap"
  },
  {
    "id": 2900,
    "content": "They are one dimensional, two dimensional, or three-dimensional and composed of elements, each of which has 1, 2 or 4 components that may be signed or unsigned 8-, 16-, or 32-bit integers, 16-bit floats, or 32-bit floats"
  },
  {
    "id": 2901,
    "content": "CUDA arrays are only accessible by kernels through texture fetching as described in Texture Memory or surface reading and writing as described in Surface Memory"
  },
  {
    "id": 2906,
    "content": "Read/Write Coherency  The texture and surface memory is cached (see Device Memory Accesses ) and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global write or a surface write in the same kernel call returns undefined data In other words,"
  },
  {
    "id": 2907,
    "content": "a thread can safely read some texture or surface memory location only if this memory location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call"
  },
  {
    "id": 2911,
    "content": "Graphics Interoperability  Some resources from OpenGL and Direct3D may be mapped into the address space of CUDA, either to enable CUDA to read data written by OpenGL or Direct3D, or to enable CUDA to write data for consumption by OpenGL or Direct3D A resource must be registered to CUDA before it can be mapped using the functions mentioned in OpenGL Interoperability and Direct3D Interoperability"
  },
  {
    "id": 2912,
    "content": "These functions return a pointer to a CUDA graphics resource of type struct cudaGraphicsResource Registering a resource is potentially high-overhead and therefore typically called only once per resource Each CUDA context which intends to use the resource is required to register it separately Once a resource is registered to CUDA, it can be mapped and unmapped as many times as necessary using"
  },
  {
    "id": 2913,
    "content": "cudaGraphicsMapResources() and cudaGraphicsUnmapResources() cudaGraphicsResourceSetMapFlags() can be called to specify usage hints (write-only, read-only) that the CUDA driver can use to optimize resource management A mapped resource can be read from or written to by kernels using the device memory address returned by cudaGraphicsResourceGetMappedPointer() for buffers and"
  },
  {
    "id": 2914,
    "content": "cudaGraphicsSubResourceGetMappedArray() for CUDA arrays Accessing a resource through OpenGL, Direct3D, or another CUDA context while it is mapped produces undefined results OpenGL Interoperability and Direct3D Interoperability give specifics for each graphics API and some code samples SLI Interoperability gives specifics for when the system is in SLI mode"
  },
  {
    "id": 2919,
    "content": "OpenGL Interoperability  The OpenGL resources that may be mapped into the address space of CUDA are OpenGL buffer, texture, and renderbuffer objects"
  },
  {
    "id": 2920,
    "content": "In CUDA, it appears as a device pointer and can therefore be read and written by kernels or via cudaMemcpy() calls"
  },
  {
    "id": 2921,
    "content": "They can also write to it via the surface write functions if the resource has been registered with the cudaGraphicsRegisterFlagsSurfaceLoadStore flag"
  },
  {
    "id": 2922,
    "content": "cudaGraphicsGLRegisterImage() supports all texture formats with 1, 2, or 4 components and an internal type of float (for example, GL_RGBA_FLOAT32 ), normalized integer (for example, GL_RGBA8, GL_INTENSITY16 ), and unnormalized integer (for example, GL_RGBA8UI ) (please note that since unnormalized integer formats require OpenGL 3"
  },
  {
    "id": 2924,
    "content": "The OpenGL context whose resources are being shared has to be current to the host thread making any OpenGL interoperability API calls"
  },
  {
    "id": 2925,
    "content": "Please note: When an OpenGL texture is made bindless (say for example by requesting an image or texture handle using the glGetTextureHandle */ glGetImageHandle * APIs) it cannot be registered with CUDA The application needs to register the texture for interop before requesting an image or texture handle"
  },
  {
    "id": 2926,
    "content": "The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object: GLuint positionsVBO ; struct cudaGraphicsResource * positionsVBO_CUDA ; int main () { Initialize OpenGL and GLUT for device 0 and make the OpenGL context current glutDisplayFunc ( display ); Explicitly set device 0 cudaSetDevice ( 0 ); Create buffer object and"
  },
  {
    "id": 2927,
    "content": "register it with CUDA glGenBuffers ( 1 , & positionsVBO ); glBindBuffer ( GL_ARRAY_BUFFER , positionsVBO ); unsigned int size = width * height * 4 * sizeof ( float ); glBufferData ( GL_ARRAY_BUFFER , size , 0 , GL_DYNAMIC_DRAW ); glBindBuffer ( GL_ARRAY_BUFFER , 0 ); cudaGraphicsGLRegisterBuffer ( & positionsVBO_CUDA , positionsVBO , cudaGraphicsMapFlagsWriteDiscard ); Launch rendering loop"
  },
  {
    "id": 2928,
    "content": "glutMainLoop (); } void display () { Map buffer object for writing from CUDA float4 * positions ; cudaGraphicsMapResources ( 1 , & positionsVBO_CUDA , 0 ); size_t num_bytes ; cudaGraphicsResourceGetMappedPointer (( void ** ) & positions , & num_bytes , positionsVBO_CUDA )); Execute kernel dim3 dimBlock ( 16 , 16 , 1 ); dim3 dimGrid ( width / dimBlock y , 1 ); createVertices >> ( positions , time ,"
  },
  {
    "id": 2929,
    "content": "width , height ); Unmap buffer object cudaGraphicsUnmapResources ( 1 , & positionsVBO_CUDA , 0 ); Render from buffer object glClear ( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT ); glBindBuffer ( GL_ARRAY_BUFFER , positionsVBO ); glVertexPointer ( 4 , GL_FLOAT , 0 , 0 ); glEnableClientState ( GL_VERTEX_ARRAY ); glDrawArrays ( GL_POINTS , 0 , width * height ); glDisableClientState ( GL_VERTEX_ARRAY"
  },
  {
    "id": 2930,
    "content": "); Swap buffers glutSwapBuffers (); glutPostRedisplay (); } void deleteVBO () { cudaGraphicsUnregisterResource ( positionsVBO_CUDA ); glDeleteBuffers ( 1 , & positionsVBO ); } __global__ void createVertices ( float4 * positions , float time , unsigned int width , unsigned int height ) { unsigned int x = blockIdx y ; Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float )"
  },
  {
    "id": 2932,
    "content": "0f - 1 0f ; v = v * 2 0f - 1 0f ;   calculate simple sine wave pattern float freq = 4 0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0"
  },
  {
    "id": 2934,
    "content": "0f ); } On Windows and for Quadro GPUs, cudaWGLGetDevice() can be used to retrieve the CUDA device associated to the handle returned by wglEnumGpusNV() Quadro GPUs offer higher performance OpenGL interoperability than GeForce and Tesla GPUs in a multi-GPU configuration where OpenGL rendering is performed on the Quadro GPU and CUDA computations are performed on other GPUs in the system"
  },
  {
    "id": 2939,
    "content": "Direct3D Interoperability  Direct3D interoperability is supported for Direct3D 9Ex, Direct3D 10, and Direct3D 11 A CUDA context may interoperate only with Direct3D devices that fulfill the following criteria: Direct3D 9Ex devices must be created with DeviceType set to D3DDEVTYPE_HAL and BehaviorFlags with the D3DCREATE_HARDWARE_VERTEXPROCESSING flag; Direct3D 10 and Direct3D 11 devices must be"
  },
  {
    "id": 2940,
    "content": "created with DriverType set to D3D_DRIVER_TYPE_HARDWARE The Direct3D resources that may be mapped into the address space of CUDA are Direct3D buffers, textures, and surfaces"
  },
  {
    "id": 2941,
    "content": "These resources are registered using cudaGraphicsD3D9RegisterResource() , cudaGraphicsD3D10RegisterResource() , and cudaGraphicsD3D11RegisterResource()"
  },
  {
    "id": 2942,
    "content": "The following code sample uses a kernel to dynamically modify a 2D width x height grid of vertices stored in a vertex buffer object"
  },
  {
    "id": 2948,
    "content": "Direct3D 9 Version  IDirect3D9 * D3D ; IDirect3DDevice9 * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; IDirect3DVertexBuffer9 * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ; Initialize Direct3D D3D = Direct3DCreate9Ex ( D3D_SDK_VERSION ); Get a CUDA-enabled adapter unsigned int adapter = 0 ; for (; adapter GetAdapterCount (); adapter"
  },
  {
    "id": 2949,
    "content": "++ ) { D3DADAPTER_IDENTIFIER9 adapterId ; g_pD3D -> GetAdapterIdentifier ( adapter , 0 , & adapterId ); if ( cudaD3D9GetDevice ( & dev , adapterId D3D -> CreateDeviceEx ( adapter , D3DDEVTYPE_HAL , hWnd , D3DCREATE_HARDWARE_VERTEXPROCESSING , & params , NULL , & device ); Use the same device cudaSetDevice ( dev ); Create vertex buffer and register it with CUDA unsigned int size = width * height *"
  },
  {
    "id": 2950,
    "content": "sizeof ( CUSTOMVERTEX ); device -> CreateVertexBuffer ( size , 0 , D3DFVF_CUSTOMVERTEX , D3DPOOL_DEFAULT , & positionsVB , 0 ); cudaGraphicsD3D9RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard ); Launch rendering loop while ("
  },
  {
    "id": 2954,
    "content": "} void Render () { Map vertex buffer for writing from CUDA float4 * positions ; cudaGraphicsMapResources ( 1 , & positionsVB_CUDA , 0 ); size_t num_bytes ; cudaGraphicsResourceGetMappedPointer (( void ** ) & positions , & num_bytes , positionsVB_CUDA )); Execute kernel dim3 dimBlock ( 16 , 16 , 1 ); dim3 dimGrid ( width / dimBlock y , 1 ); createVertices >> ( positions , time , width , height );"
  },
  {
    "id": 2955,
    "content": "Unmap vertex buffer cudaGraphicsUnmapResources ( 1 , & positionsVB_CUDA , 0 ); Draw and present } void releaseVB () { cudaGraphicsUnregisterResource ( positionsVB_CUDA ); positionsVB -> Release (); } __global__ void createVertices ( float4 * positions , float time , unsigned int width , unsigned int height ) { unsigned int x = blockIdx y ; Calculate uv coordinates float u = x / ( float ) width ;"
  },
  {
    "id": 2957,
    "content": "0f - 1 0f ; v = v * 2 0f - 1 0f ;   Calculate simple sine wave pattern float freq = 4 0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0"
  },
  {
    "id": 2958,
    "content": "5f ;   Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3"
  },
  {
    "id": 2963,
    "content": "Direct3D 10 Version  ID3D10Device * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; ID3D10Buffer * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ; Get a CUDA-enabled adapter IDXGIFactory * factory ; CreateDXGIFactory ( __uuidof ( IDXGIFactory ), ( void ** ) & factory ); IDXGIAdapter * adapter = 0 ; for ( unsigned int i = 0 ; adapter ; ++ i"
  },
  {
    "id": 2964,
    "content": ") { if ( FAILED ( factory -> EnumAdapters ( i , & adapter )) break ; if ( cudaD3D10GetDevice ( & dev , adapter ) == cudaSuccess ) break ; adapter -> Release (); } factory -> Release (); Create swap chain and device D3D10CreateDeviceAndSwapChain ( adapter , D3D10_DRIVER_TYPE_HARDWARE , 0 , D3D10_CREATE_DEVICE_DEBUG , D3D10_SDK_VERSION , & swapChainDesc , & swapChain , & device ); adapter -> Release"
  },
  {
    "id": 2965,
    "content": "(); Use the same device cudaSetDevice ( dev ); Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); D3D10_BUFFER_DESC bufferDesc ; bufferDesc MiscFlags = 0 ; device -> CreateBuffer ( & bufferDesc , 0 , & positionsVB ); cudaGraphicsD3D10RegisterResource ( & positionsVB_CUDA , positionsVB , cudaGraphicsRegisterFlagsNone );"
  },
  {
    "id": 2966,
    "content": "cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard ); Launch rendering loop while ("
  },
  {
    "id": 2968,
    "content": "y ;   Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2"
  },
  {
    "id": 2969,
    "content": "0f - 1 0f ; v = v * 2 0f - 1 0f ;   Calculate simple sine wave pattern float freq = 4 0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0"
  },
  {
    "id": 2970,
    "content": "5f ;   Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3"
  },
  {
    "id": 2975,
    "content": "Direct3D 11 Version  ID3D11Device * device ; struct CUSTOMVERTEX { FLOAT x , y , z ; DWORD color ; }; ID3D11Buffer * positionsVB ; struct cudaGraphicsResource * positionsVB_CUDA ; int main () { int dev ; Get a CUDA-enabled adapter IDXGIFactory * factory ; CreateDXGIFactory ( __uuidof ( IDXGIFactory ), ( void ** ) & factory ); IDXGIAdapter * adapter = 0 ; for ( unsigned int i = 0 ; adapter ; ++ i"
  },
  {
    "id": 2976,
    "content": ") { if ( FAILED ( factory -> EnumAdapters ( i , & adapter )) break ; if ( cudaD3D11GetDevice ( & dev , adapter ) == cudaSuccess ) break ; adapter -> Release (); } factory -> Release (); Create swap chain and device sFnPtr_D3D11CreateDeviceAndSwapChain ( adapter , D3D11_DRIVER_TYPE_HARDWARE , 0 , D3D11_CREATE_DEVICE_DEBUG , featureLevels , 3 , D3D11_SDK_VERSION , & swapChainDesc , & swapChain , &"
  },
  {
    "id": 2977,
    "content": "device , & featureLevel , & deviceContext ); adapter -> Release (); Use the same device cudaSetDevice ( dev ); Create vertex buffer and register it with CUDA unsigned int size = width * height * sizeof ( CUSTOMVERTEX ); D3D11_BUFFER_DESC bufferDesc ; bufferDesc MiscFlags = 0 ; device -> CreateBuffer ( & bufferDesc , 0 , & positionsVB ); cudaGraphicsD3D11RegisterResource ( & positionsVB_CUDA ,"
  },
  {
    "id": 2978,
    "content": "positionsVB , cudaGraphicsRegisterFlagsNone ); cudaGraphicsResourceSetMapFlags ( positionsVB_CUDA , cudaGraphicsMapFlagsWriteDiscard ); Launch rendering loop while ("
  },
  {
    "id": 2980,
    "content": "y ;   Calculate uv coordinates float u = x / ( float ) width ; float v = y / ( float ) height ; u = u * 2"
  },
  {
    "id": 2981,
    "content": "0f - 1 0f ; v = v * 2 0f - 1 0f ;   Calculate simple sine wave pattern float freq = 4 0f ; float w = sinf ( u * freq + time ) * cosf ( v * freq + time ) * 0"
  },
  {
    "id": 2982,
    "content": "5f ;   Write positions positions [ y * width + x ] = make_float4 ( u , w , v , __int_as_float ( 0xff00ff00 )); } 3"
  },
  {
    "id": 2986,
    "content": "SLI Interoperability  In a system with multiple GPUs, all CUDA-enabled GPUs are accessible via the CUDA driver and runtime as separate devices There are however special considerations as described below when the system is in SLI mode First, an allocation in one CUDA device on one GPU will consume memory on other GPUs that are part of the SLI configuration of the Direct3D or OpenGL device Second,"
  },
  {
    "id": 2989,
    "content": "The application can use the cudaD3D[9|10|11]GetDevices() for Direct3D and cudaGLGetDevices() for OpenGL set of calls to identify the CUDA device handle(s) for the device(s) that are performing the rendering in the current and next frame Given this information the application will typically choose the appropriate device and map Direct3D or OpenGL resources to the CUDA device returned by"
  },
  {
    "id": 2990,
    "content": "cudaD3D[9|10|11]GetDevices() or cudaGLGetDevices() when the deviceList parameter is set to cudaD3D[9|10|11]DeviceListCurrentFrame or cudaGLDeviceListCurrentFrame"
  },
  {
    "id": 2991,
    "content": "Please note that resource returned from cudaGraphicsD9D[9|10|11]RegisterResource and cudaGraphicsGLRegister[Buffer|Image] must be only used on device the registration happened"
  },
  {
    "id": 2992,
    "content": "Therefore on SLI configurations when data for different frames is computed on different CUDA devices it is necessary to register the resources for each separately"
  },
  {
    "id": 2993,
    "content": "See Direct3D Interoperability and OpenGL Interoperability for details on how the CUDA runtime interoperate with Direct3D and OpenGL, respectively"
  },
  {
    "id": 2997,
    "content": "External Resource Interoperability  External resource interoperability allows CUDA to import certain resources that are explicitly exported by other APIs These objects are typically exported by other APIs using handles native to the Operating System, like file descriptors on Linux or NT handles on Windows They could also be exported using other unified interfaces such as the NVIDIA Software"
  },
  {
    "id": 3000,
    "content": "An imported memory object can be accessed from within kernels using device pointers mapped onto the memory object via cudaExternalMemoryGetMappedBuffer() or CUDA mipmapped arrays mapped via cudaExternalMemoryGetMappedMipmappedArray() Depending on the type of memory object, it may be possible for more than one mapping to be setup on a single memory object Therefore, any device pointers mapped onto"
  },
  {
    "id": 3001,
    "content": "that object must be explicitly freed using cudaFree() and any CUDA mipmapped arrays mapped onto that object must be explicitly freed using cudaFreeMipmappedArray() An imported synchronization object can then be signaled using cudaSignalExternalSemaphoresAsync() and waited on using cudaWaitExternalSemaphoresAsync() Also, depending on the type of the imported synchronization object, there may be"
  },
  {
    "id": 3002,
    "content": "additional constraints imposed on how they can be signaled and waited on, as described in subsequent sections"
  },
  {
    "id": 3013,
    "content": "Matching device UUIDs  When importing memory and synchronization objects exported by Vulkan, they must be imported and mapped on the same device as they were created on The CUDA device that corresponds to the Vulkan physical device on which the objects were created can be determined by comparing the UUID of a CUDA device with that of the Vulkan physical device, as shown in the following code"
  },
  {
    "id": 3014,
    "content": "sample Note that the Vulkan physical device should not be part of a device group that contains more than one Vulkan physical device The device group as returned by vkEnumeratePhysicalDeviceGroups that contains the given Vulkan physical device must have a physical device count of 1"
  },
  {
    "id": 3015,
    "content": "int getCudaDeviceForVulkanPhysicalDevice ( VkPhysicalDevice vkPhysicalDevice ) { VkPhysicalDeviceIDProperties vkPhysicalDeviceIDProperties = {}; vkPhysicalDeviceIDProperties"
  },
  {
    "id": 3016,
    "content": "pNext = NULL ; VkPhysicalDeviceProperties2 vkPhysicalDeviceProperties2 = {}; vkPhysicalDeviceProperties2"
  },
  {
    "id": 3017,
    "content": "pNext = & vkPhysicalDeviceIDProperties ; vkGetPhysicalDeviceProperties2 ( vkPhysicalDevice , & vkPhysicalDeviceProperties2 ); int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice GetAdapterLuid (); int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice QueryInterface ( __uuidof ( IDXGIDevice ), ( void"
  },
  {
    "id": 3018,
    "content": "** ) & dxgiDevice ); IDXGIAdapter * dxgiAdapter ; dxgiDevice -> GetAdapter ( & dxgiAdapter ); DXGI_ADAPTER_DESC dxgiAdapterDesc ; dxgiAdapter -> GetDesc ( & dxgiAdapterDesc ); LUID d3d11Luid = dxgiAdapterDesc AdapterLuid ; int cudaDeviceCount ; cudaGetDeviceCount ( & cudaDeviceCount ); for ( int cudaDevice = 0 ; cudaDevice ( bufattrs [ 0 ]"
  },
  {
    "id": 3019,
    "content": "value ));   Note cache and compression are per GPU attributes, so read values for specific gpu by comparing UUID   Read cacheability granted by NvSciBuf int numGpus = bufattrs [ 1 ]"
  },
  {
    "id": 3020,
    "content": "len / sizeof ( NvSciBufAttrValGpuCache ); NvSciBufAttrValGpuCache [] cacheVal = ( NvSciBufAttrValGpuCache * ) bufattrs [ 1 ]"
  },
  {
    "id": 3021,
    "content": "value ; bool ret_cacheVal ; for ( int i = 0 ; i >> ( array , arrayCount ); cudaDeviceSynchronize ();   If interested, the occupancy can be calculated with   cudaOccupancyMaxActiveBlocksPerMultiprocessor return 0 ; } The following code sample shows how to use the cluster occupancy API to find the max number of active clusters of a given size"
  },
  {
    "id": 3023,
    "content": "0, except on GPU hardware or MIG configurations which are too small to support 8 multiprocessors in which case the maximum cluster size will be reduced But it is recommended that the users query the maximum cluster size before launching a cluster kernel"
  },
  {
    "id": 3024,
    "content": "dynamicSmemBytes = dynamic_shared_memory_size ; cudaLaunchAttribute attribute [ 1 ]; attribute [ 0 ]"
  },
  {
    "id": 3025,
    "content": "numAttrs = 1 ; int max_cluster_size = 0 ; cudaOccupancyMaxPotentialClusterSize ( & max_cluster_size , ( void * ) kernel , & config ); int max_active_clusters = 0 ; cudaOccupancyMaxActiveClusters ( & max_active_clusters , ( void * ) kernel , & config ); std :: cout /include/cuda_occupancy"
  },
  {
    "id": 3027,
    "content": "The Nsight Compute version of the occupancy calculator is particularly useful as a learning tool that visualizes the impact of changes to the parameters that affect occupancy (block size, registers per thread, and shared memory per thread)"
  },
  {
    "id": 3030,
    "content": "Maximize Memory Throughput  The first step in maximizing overall memory throughput for the application is to minimize data transfers with low bandwidth That means minimizing data transfers between the host and the device, as detailed in Data Transfer between Host and Device , since these have much lower bandwidth than data transfers between global memory and the device That also means minimizing"
  },
  {
    "id": 3031,
    "content": "data transfers between global memory and the device by maximizing use of on-chip memory: shared memory and caches (i"
  },
  {
    "id": 3033,
    "content": ", L1 cache and L2 cache available on devices of compute capability 2 x and higher, texture cache and constant cache available on all devices) Shared memory is equivalent to a user-managed cache: The application explicitly allocates and accesses it As illustrated in CUDA Runtime , a typical programming pattern is to stage data coming from device memory into shared memory; in other words, to have"
  },
  {
    "id": 3034,
    "content": "each thread of a block: Load data from device memory to shared memory, Synchronize with all the other threads of the block so that each thread can safely read shared memory locations that were populated by different threads, Process the data in shared memory, Synchronize again if necessary to make sure that shared memory has been updated with the results, Write the results back to device memory"
  },
  {
    "id": 3035,
    "content": "For some applications (for example, for which global memory access patterns are data-dependent), a traditional hardware-managed cache is more appropriate to exploit data locality As mentioned in Compute Capability 7 x , Compute Capability 8 x and Compute Capability 9 0 , for devices of compute capability 7"
  },
  {
    "id": 3037,
    "content": "x and 9 0, the same on-chip memory is used for both L1 and shared memory, and how much of it is dedicated to L1 versus shared memory is configurable for each kernel call The throughput of memory accesses by a kernel can vary by an order of magnitude depending on access pattern for each type of memory The next step in maximizing memory throughput is therefore to organize memory accesses as"
  },
  {
    "id": 3038,
    "content": "optimally as possible based on the optimal memory access patterns described in Device Memory Accesses This optimization is especially important for global memory accesses as global memory bandwidth is low compared to available on-chip bandwidths and arithmetic instruction throughput, so non-optimal global memory accesses generally have a high impact on performance"
  },
  {
    "id": 3042,
    "content": "Data Transfer between Host and Device  Applications should strive to minimize data transfer between the host and the device One way to accomplish this is to move more code from the host to the device, even if that means running kernels that do not expose enough parallelism to execute on the device with full efficiency Intermediate data structures may be created in device memory, operated on by"
  },
  {
    "id": 3043,
    "content": "the device, and destroyed without ever being mapped by the host or copied to host memory Also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always performs better than making each transfer separately On systems with a front-side bus, higher performance for data transfers between host and device is achieved by using page-locked"
  },
  {
    "id": 3044,
    "content": "host memory as described in Page-Locked Host Memory In addition, when using mapped page-locked memory ( Mapped Memory ), there is no need to allocate any device memory and explicitly copy data between device and host memory For maximum performance, these memory accesses must be coalesced as with accesses to global memory (see Device Memory Accesses ) Assuming that they are and that the mapped"
  },
  {
    "id": 3045,
    "content": "memory is read or written only once, using mapped page-locked memory instead of explicit copies between device and host memory can be a win for performance On integrated systems where device memory and host memory are physically the same, any copy between host and device memory is superfluous and mapped page-locked memory should be used instead Applications may query a device is integrated by"
  },
  {
    "id": 3052,
    "content": ", global, local, shared, constant, or texture memory) might need to be re-issued multiple times depending on the distribution of the memory addresses across the threads within the warp How the distribution affects the instruction throughput this way is specific to each type of memory and described in the following sections For example, for global memory, as a general rule, the more scattered the"
  },
  {
    "id": 3053,
    "content": "addresses are, the more reduced the throughput is Global Memory Global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i"
  },
  {
    "id": 3055,
    "content": ", whose first address is a multiple of their size) can be read or written by memory transactions When a warp executes an instruction that accesses global memory, it coalesces the memory accesses of the threads within the warp into one or more of these memory transactions depending on the size of the word accessed by each thread and the distribution of the memory addresses across the threads In"
  },
  {
    "id": 3056,
    "content": "general, the more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly"
  },
  {
    "id": 3057,
    "content": "For example, if a 32-byte memory transaction is generated for each thread’s 4-byte access, throughput is divided by 8"
  },
  {
    "id": 3058,
    "content": "How many transactions are necessary and how much throughput is ultimately affected varies with the compute capability of the device Compute Capability 5 x , Compute Capability 6 x , Compute Capability 7 x , Compute Capability 8 x and Compute Capability 9 0 give more details on how global memory accesses are handled for various compute capabilities To maximize global memory throughput, it is"
  },
  {
    "id": 3059,
    "content": "therefore important to maximize coalescing by: Following the most optimal access patterns based on Compute Capability 5 x , Compute Capability 6 x , Compute Capability 7 x , Compute Capability 8 x and Compute Capability 9"
  },
  {
    "id": 3060,
    "content": "0 Using data types that meet the size and alignment requirement detailed in the section Size and Alignment Requirement below, Padding data in some cases, for example, when accessing a two-dimensional array as described in the section Two-Dimensional Arrays below Size and Alignment Requirement Global memory instructions support reading or writing words of size equal to 1, 2, 4, 8, or 16 bytes Any"
  },
  {
    "id": 3061,
    "content": "access (via a variable or a pointer) to data residing in global memory compiles to a single global memory instruction if and only if the size of the data type is 1, 2, 4, 8, or 16 bytes and the data is naturally aligned (i"
  },
  {
    "id": 3063,
    "content": ", its address is a multiple of that size) If this size and alignment requirement is not fulfilled, the access compiles to multiple instructions with interleaved access patterns that prevent these instructions from fully coalescing It is therefore recommended to use types that meet this requirement for data that resides in global memory"
  },
  {
    "id": 3064,
    "content": "For structures, the size and alignment requirements can be enforced by the compiler using the alignment specifiers __align__(8) or __align__(16) , such as struct __align__ ( 8 ) { float x ; float y ; }; or struct __align__ ( 16 ) { float x ; float y ; float z ; }; Any address of a variable residing in global memory or returned by one of the memory allocation routines from the driver or runtime"
  },
  {
    "id": 3065,
    "content": "API is always aligned to at least 256 bytes Reading non-naturally aligned 8-byte or 16-byte words produces incorrect results (off by a few words), so special care must be taken to maintain alignment of the starting address of any value or array of values of these types A typical case where this might be easily overlooked is when using some custom global memory allocation scheme, whereby the"
  },
  {
    "id": 3066,
    "content": "allocations of multiple arrays (with multiple calls to cudaMalloc() or cuMemAlloc() ) is replaced by the allocation of a single large block of memory partitioned into multiple arrays, in which case the starting address of each array is offset from the block’s starting address Two-Dimensional Arrays A common global memory access pattern is when each thread of index (tx,ty) uses the following"
  },
  {
    "id": 3067,
    "content": "address to access one element of a 2D array of width width , located at address BaseAddress of type type* (where type meets the requirement described in Maximize Utilization ): BaseAddress + width * ty + tx For these accesses to be fully coalesced, both the width of the thread block and the width of the array must be a multiple of the warp size In particular, this means that an array whose width"
  },
  {
    "id": 3068,
    "content": "is not a multiple of this size will be accessed much more efficiently if it is actually allocated with a width rounded up to the closest multiple of this size and its rows padded accordingly"
  },
  {
    "id": 3069,
    "content": "The cudaMallocPitch() and cuMemAllocPitch() functions and associated memory copy functions described in the reference manual enable programmers to write non-hardware-dependent code to allocate arrays that conform to these constraints"
  },
  {
    "id": 3070,
    "content": "Local Memory Local memory accesses only occur for some automatic variables as mentioned in Variable Memory Space Specifiers Automatic variables that the compiler is likely to place in local memory are: Arrays for which it cannot determine that they are indexed with constant quantities, Large structures or arrays that would consume too much register space, Any variable if the kernel uses more"
  },
  {
    "id": 3071,
    "content": "registers than available (this is also known as register spilling ) Inspection of the PTX assembly code (obtained by compiling with the -ptx or -keep option) will tell if a variable has been placed in local memory during the first compilation phases as it will be declared using the local mnemonic and accessed using the ld local and st local mnemonics Even if it has not, subsequent compilation"
  },
  {
    "id": 3072,
    "content": "phases might still decide otherwise though if they find it consumes too much register space for the targeted architecture: Inspection of the cubin object using cuobjdump will tell if this is the case Also, the compiler reports total local memory usage per kernel ( lmem ) when compiling with the --ptxas-options=-v option Note that some mathematical functions have implementation paths that might"
  },
  {
    "id": 3073,
    "content": "access local memory The local memory space resides in device memory, so local memory accesses have the same high latency and low bandwidth as global memory accesses and are subject to the same requirements for memory coalescing as described in Device Memory Accesses Local memory is however organized such that consecutive 32-bit words are accessed by consecutive thread IDs Accesses are therefore"
  },
  {
    "id": 3074,
    "content": "fully coalesced as long as all threads in a warp access the same relative address (for example, same index in an array variable, same member in a structure variable)"
  },
  {
    "id": 3075,
    "content": "On devices of compute capability 5 x onwards, local memory accesses are always cached in L2 in the same way as global memory accesses (see Compute Capability 5 x and Compute Capability 6"
  },
  {
    "id": 3077,
    "content": "Shared Memory Because it is on-chip, shared memory has much higher bandwidth and much lower latency than local or global memory To achieve high bandwidth, shared memory is divided into equally-sized memory modules, called banks, which can be accessed simultaneously Any memory read or write request made of n addresses that fall in n distinct memory banks can therefore be serviced simultaneously,"
  },
  {
    "id": 3078,
    "content": "yielding an overall bandwidth that is n times as high as the bandwidth of a single module However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of"
  },
  {
    "id": 3079,
    "content": "separate memory requests If the number of separate memory requests is n , the initial memory request is said to cause n -way bank conflicts To get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts"
  },
  {
    "id": 3080,
    "content": "This is described in Compute Capability 5 x , Compute Capability 6 x , Compute Capability 7 x , Compute Capability 8 x , and Compute Capability 9 0 for devices of compute capability 5"
  },
  {
    "id": 3086,
    "content": "Constant Memory The constant memory space resides in device memory and is cached in the constant cache"
  },
  {
    "id": 3087,
    "content": "A request is then split into as many separate requests as there are different memory addresses in the initial request, decreasing throughput by a factor equal to the number of separate requests The resulting requests are then serviced at the throughput of the constant cache in case of a cache hit, or at the throughput of device memory otherwise Texture and Surface Memory The texture and surface"
  },
  {
    "id": 3088,
    "content": "memory spaces reside in device memory and are cached in texture cache, so a texture fetch or surface read costs one memory read from device memory only on a cache miss, otherwise it just costs one read from texture cache The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture or surface addresses that are close together in 2D will achieve best"
  },
  {
    "id": 3089,
    "content": "performance Also, it is designed for streaming fetches with a constant latency; a cache hit reduces DRAM bandwidth demand but not fetch latency"
  },
  {
    "id": 3090,
    "content": "In this section, throughputs are given in number of operations per clock cycle per multiprocessor For a warp size of 32, one instruction corresponds to 32 operations, so if N is the number of operations per clock cycle, the instruction throughput is N/32 instructions per clock cycle"
  },
  {
    "id": 3091,
    "content": "They must be multiplied by the number of multiprocessors in the device to get throughput for the whole device"
  },
  {
    "id": 3095,
    "content": "Arithmetic Instructions  The following table gives the throughputs of the arithmetic instructions that are natively supported in hardware for devices of various compute capabilities"
  },
  {
    "id": 3107,
    "content": "0 16-bit floating-point add, multiply, multiply-add N/A 256 128 2 256 128 256 3 128 256 32-bit floating-point add, multiply, multiply-add 128 64 128 64 128 64-bit floating-point add, multiply, multiply-add 4 32 4 32 5 32 2 64 32-bit floating-point reciprocal, reciprocal square root, base-2 logarithm ( __log2f ), base 2 exponential ( exp2f ), sine ( __sinf ), cosine ( __cosf ) 32 16 32 16 32-bit"
  },
  {
    "id": 3108,
    "content": "integer add, extended-precision add, subtract, extended-precision subtract 128 64 128 64 32-bit integer multiply, multiply-add, extended-precision multiply-add Multiple instruct 32-bit integer shift 64 32 64 compare, minimum, maximum 64 32 64 32-bit integer bit reverse 64 32 64 16 Bit field extract/insert 64 32 64 Multiple Instruct 64 32-bit bitwise AND, OR, XOR 128 64 128 64 count of leading"
  },
  {
    "id": 3109,
    "content": "zeros, most significant non-sign bit 32 16 32 16 population count 32 16 32 16 warp shuffle 32 32 8 32 warp reduce Multiple instruct 16 warp vote 64 sum of absolute difference 64 32 64 SIMD video instructions vabsdiff2 Multiple instruct Type conversions from 8-bit and 16-bit integer to 32-bit integer types 32 16 32 64 Type conversions from and to 64-bit types 4 16 4 16 10 16 2 2 16 All other type"
  },
  {
    "id": 3111,
    "content": "The implementation may be different for devices of different compute capabilities, and the number of native instructions after compilation may fluctuate with every compiler version"
  },
  {
    "id": 3112,
    "content": "The implementation of some functions are readily available on the CUDA header files ( math_functions"
  },
  {
    "id": 3115,
    "content": "In general, code compiled with -ftz=true (denormalized numbers are flushed to zero) tends to have higher performance than code compiled with -ftz=false Similarly, code compiled with -prec-div=false (less precise division) tends to have higher performance code than code compiled with -prec-div=true , and code compiled with -prec-sqrt=false (less precise square root) tends to have higher"
  },
  {
    "id": 3117,
    "content": "Single-Precision Floating-Point Division __fdividef(x, y) (see Intrinsic Functions ) provides faster single-precision floating-point division than the division operator Single-Precision Floating-Point Reciprocal Square Root To preserve IEEE-754 semantics the compiler can optimize 1 0/sqrtf() into rsqrtf() only when both reciprocal and square root are approximate, (i"
  },
  {
    "id": 3120,
    "content": "Single-Precision Floating-Point Square Root Single-precision floating-point square root is implemented as a reciprocal square root followed by a reciprocal instead of a reciprocal square root followed by a multiplication so that it gives correct results for 0 and infinity"
  },
  {
    "id": 3121,
    "content": "Sine and Cosine sinf(x) , cosf(x) , tanf(x) , sincosf(x) , and corresponding double-precision instructions are much more expensive and even more so if the argument x is large in magnitude"
  },
  {
    "id": 3122,
    "content": "More precisely, the argument reduction code (see Mathematical Functions for implementation) comprises two code paths referred to as the fast path and the slow path, respectively The fast path is used for arguments sufficiently small in magnitude and essentially consists of a few multiply-add operations The slow path is used for arguments large in magnitude and consists of lengthy computations"
  },
  {
    "id": 3123,
    "content": "required to achieve correct results over the entire argument range At present, the argument reduction code for the trigonometric functions selects the fast path for arguments whose magnitude is less than 105615 0f for the single-precision functions, and less than 2147483648 0 for the double-precision functions As the slow path requires more registers than the fast path, an attempt has been made to"
  },
  {
    "id": 3124,
    "content": "reduce register pressure in the slow path by storing some intermediate variables in local memory, which may affect performance because of local memory high latency and bandwidth (see Device Memory Accesses ) At present, 28 bytes of local memory are used by single-precision functions, and 44 bytes are used by double-precision functions Due to the lengthy computations and use of local memory in the"
  },
  {
    "id": 3125,
    "content": "slow path, the throughput of these trigonometric functions is lower by one order of magnitude when the slow path reduction is required as opposed to the fast path reduction"
  },
  {
    "id": 3126,
    "content": "Integer Arithmetic Integer division and modulo operation are costly as they compile to up to 20 instructions"
  },
  {
    "id": 3127,
    "content": "They can be replaced with bitwise operations in some cases: If n is a power of 2, ( i/n ) is equivalent to (i>>log2(n)) and (i%n) is equivalent to ( i&(n-1) ); the compiler will perform these conversions if n is literal"
  },
  {
    "id": 3129,
    "content": "Half Precision Arithmetic In order to achieve good performance for 16-bit precision floating-point add, multiply or multiply-add, it is recommended that the half2 datatype is used for half precision and __nv_bfloat162 be used for __nv_bfloat16 precision"
  },
  {
    "id": 3130,
    "content": "Vector intrinsics (for example, __hadd2 , __hsub2 , __hmul2 , __hfma2 ) can then be used to do two operations in a single instruction"
  },
  {
    "id": 3131,
    "content": "Using half2 or __nv_bfloat162 in place of two calls using half or __nv_bfloat16 may also help performance of other intrinsics, such as warp shuffles The intrinsic __halves2half2 is provided to convert two half precision values to the half2 datatype The intrinsic __halves2bfloat162 is provided to convert two __nv_bfloat precision values to the __nv_bfloat162 datatype"
  },
  {
    "id": 3132,
    "content": "Type Conversion Sometimes, the compiler must insert conversion instructions, introducing additional execution cycles"
  },
  {
    "id": 3133,
    "content": "This is the case for: Functions operating on variables of type char or short whose operands generally need to be converted to int , Double-precision floating-point constants (i"
  },
  {
    "id": 3135,
    "content": ", those constants defined without any type suffix) used as input to single-precision floating-point computations (as mandated by C/C++ standards) This last case can be avoided by using single-precision floating-point constants, defined with an f suffix such as 3"
  },
  {
    "id": 3142,
    "content": "Control Flow Instructions  Any flow control instruction ( if , switch , do , for , while ) can significantly impact the effective instruction throughput by causing threads of the same warp to diverge (i"
  },
  {
    "id": 3144,
    "content": ", to follow different execution paths) If this happens, the different executions paths have to be serialized, increasing the total number of instructions executed for this warp"
  },
  {
    "id": 3145,
    "content": "To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture A trivial example is when the controlling condition only depends on ( threadIdx / warpSize ) where"
  },
  {
    "id": 3146,
    "content": "warpSize is the warp size In this case, no warp diverges since the controlling condition is perfectly aligned with the warps"
  },
  {
    "id": 3147,
    "content": "Sometimes, the compiler may unroll loops or it may optimize out short if or switch blocks by using branch predication instead, as detailed below The programmer can also control loop unrolling using the #pragma unroll directive (see #pragma unroll ) When using branch predication none of the instructions whose execution depends on the controlling condition gets skipped Instead, each of them is"
  },
  {
    "id": 3148,
    "content": "associated with a per-thread condition code or predicate that is set to true or false based on the controlling condition and although each of these instructions gets scheduled for execution, only the instructions with a true predicate are actually executed Instructions with a false predicate do not write results, and also do not evaluate addresses or read operands"
  },
  {
    "id": 3152,
    "content": "Synchronization Instruction  Throughput for __syncthreads() is 32 operations per clock cycle for devices of compute capability 6 0, 16 operations per clock cycle for devices of compute capability 7"
  },
  {
    "id": 3158,
    "content": "Note that __syncthreads() can impact performance by forcing the multiprocessor to idle as detailed in Device Memory Accesses"
  },
  {
    "id": 3161,
    "content": "Minimize Memory Thrashing  Applications that constantly allocate and free memory too often may find that the allocation calls tend to get slower over time up to a limit"
  },
  {
    "id": 3162,
    "content": "This is typically expected due to the nature of releasing memory back to the operating system for its own use"
  },
  {
    "id": 3163,
    "content": "For best performance in this regard, we recommend the following: Try to size your allocation to the problem at hand"
  },
  {
    "id": 3164,
    "content": "Don’t try to allocate all available memory with cudaMalloc / cudaMallocHost / cuMemCreate , as this forces memory to be resident immediately and prevents other applications from being able to use that memory"
  },
  {
    "id": 3165,
    "content": "This can put more pressure on operating system schedulers, or just prevent other applications using the same GPU from running entirely"
  },
  {
    "id": 3166,
    "content": "Try to allocate memory in appropriately sized allocations early in the application and allocations only when the application does not have any use for it"
  },
  {
    "id": 3167,
    "content": "Reduce the number of cudaMalloc + cudaFree calls in the application, especially in performance-critical regions"
  },
  {
    "id": 3168,
    "content": "If an application cannot allocate enough device memory, consider falling back on other memory types such as cudaMallocHost or cudaMallocManaged , which may not be as performant, but will enable the application to make progress For platforms that support the feature, cudaMallocManaged allows for oversubscription, and with the correct cudaMemAdvise policies enabled, will allow the application to"
  },
  {
    "id": 3170,
    "content": "cudaMallocManaged also won’t force an allocation to be resident until it is needed or prefetched, reducing the overall pressure on the operating system schedulers and better enabling multi-tenet use cases"
  },
  {
    "id": 3171,
    "content": "3 128 for __nv_bfloat16 4 8 for GeForce GPUs, except for Titan GPUs 5 2 for compute capability 7 5 GPUs 6 32 for extended-precision 7 32 for GeForce GPUs, except for Titan GPUs 8 16 for compute capabilities 7 5 GPUs 9 8 for GeForce GPUs, except for Titan GPUs 10 2 for compute capabilities 7 5 GPUs 6 CUDA-Enabled GPUs  https: developer"
  },
  {
    "id": 3174,
    "content": "The compute capability, number of multiprocessors, clock frequency, total amount of device memory, and other properties can be queried using the runtime (see reference manual)"
  },
  {
    "id": 3178,
    "content": "Function Execution Space Specifiers  Function execution space specifiers denote whether a function executes on the host or on the device and whether it is callable from the host or from the device"
  },
  {
    "id": 3183,
    "content": "Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 5"
  },
  {
    "id": 3185,
    "content": "Any call to a __global__ function must specify its execution configuration as described in Execution Configuration A call to a __global__ function is asynchronous, meaning it returns before the device has completed its execution"
  },
  {
    "id": 3189,
    "content": "__device__  The __device__ execution space specifier declares a function that is: Executed on the device, Callable from the device only The __global__ and __device__ execution space specifiers cannot be used together"
  },
  {
    "id": 3193,
    "content": "__host__  The __host__ execution space specifier declares a function that is: Executed on the host, Callable from the host only It is equivalent to declare a function with only the __host__ execution space specifier or to declare it without any of the __host__ , __device__ , or __global__ execution space specifier; in either case the function is compiled for the host only The __device__ and"
  },
  {
    "id": 3194,
    "content": "__host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device: __host__ __device__ func () { #if __CUDA_ARCH__ >= 800 Device code path for compute capability 8 x #elif __CUDA_ARCH__ >= 700 Device"
  },
  {
    "id": 3195,
    "content": "code path for compute capability 7 x #elif __CUDA_ARCH__ >= 600 Device code path for compute capability 6 x #elif __CUDA_ARCH__ >= 500 Device code path for compute capability 5 x #elif defined(__CUDA_ARCH__) Host code path #endif } 7"
  },
  {
    "id": 3198,
    "content": "Undefined behavior  A ‘cross-execution space’ call has undefined behavior when: __CUDA_ARCH__ is defined, a call from within a __global__ , __device__ or __host__ __device__ function to a __host__ function __CUDA_ARCH__ is undefined, a call from within a __host__ function to a __device__ function"
  },
  {
    "id": 3202,
    "content": "__noinline__ and __forceinline__  The compiler inlines any __device__ function when deemed appropriate The __noinline__ function qualifier can be used as a hint for the compiler not to inline the function if possible The __forceinline__ function qualifier can be used to force the compiler to inline the function The __noinline__ and __forceinline__ function qualifiers cannot be used together, and"
  },
  {
    "id": 3207,
    "content": "__inline_hint__  The __inline_hint__ qualifier enables more aggressive inlining in the compiler Neither the __noinline__ nor the __forceinline__ function qualifier can be used with the __inline_hint__ function qualifier"
  },
  {
    "id": 3210,
    "content": "Variable Memory Space Specifiers  Variable memory space specifiers denote the memory location on the device of a variable An automatic variable declared in device code without any of the __device__ , __shared__ and __constant__ memory space specifiers described in this section generally resides in a register"
  },
  {
    "id": 3211,
    "content": "However in some cases the compiler might choose to place it in local memory, which can have adverse performance consequences as detailed in Device Memory Accesses"
  },
  {
    "id": 3215,
    "content": "__device__  The __device__ memory space specifier declares a variable that resides on the device At most one of the other memory space specifiers defined in the next three sections may be used together with __device__ to further denote which memory space the variable belongs to If none of them is present, the variable: Resides in global memory space, Has the lifetime of the CUDA context in which"
  },
  {
    "id": 3216,
    "content": "it is created, Has a distinct object per device, Is accessible from all the threads within the grid and from the host through the runtime library (cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol() )"
  },
  {
    "id": 3220,
    "content": "__constant__  The __constant__ memory space specifier, optionally used together with __device__ , declares a variable that: Resides in constant memory space, Has the lifetime of the CUDA context in which it is created, Has a distinct object per device, Is accessible from all the threads within the grid and from the host through the runtime library ( cudaGetSymbolAddress() / cudaGetSymbolSize() /"
  },
  {
    "id": 3221,
    "content": "cudaMemcpyToSymbol() / cudaMemcpyFromSymbol() ) The behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point of this grid’s lifetime is undefined"
  },
  {
    "id": 3225,
    "content": "__shared__  The __shared__ memory space specifier, optionally used together with __device__ , declares a variable that: Resides in the shared memory space of a thread block, Has the lifetime of the block, Has a distinct object per block, Is only accessible from all the threads within the block, Does not have a constant address When declaring a variable in shared memory as an external array such"
  },
  {
    "id": 3226,
    "content": "as extern __shared__ float shared []; the size of the array is determined at launch time (see Execution Configuration ) All variables declared in this fashion, start at the same address in memory, so that the layout of the variables in the array must be explicitly managed through offsets For example, if one wants the equivalent of short array0 [ 128 ]; float array1 [ 64 ]; int array2 [ 256 ]; in"
  },
  {
    "id": 3227,
    "content": "dynamically allocated shared memory, one could declare and initialize the arrays the following way: extern __shared__ float array []; __device__ void func () __device__ or __global__ function { short * array0 = ( short * ) array ; float * array1 = ( float * ) & array0 [ 128 ]; int * array2 = ( int * ) & array1 [ 64 ]; } Note that pointers need to be aligned to the type they point to, so the"
  },
  {
    "id": 3228,
    "content": "following code, for example, does not work since array1 is not aligned to 4 bytes extern __shared__ float array []; __device__ void func () __device__ or __global__ function { short * array0 = ( short * ) array ; float * array1 = ( float * ) & array0 [ 127 ]; } Alignment requirements for the built-in vector types are listed in Table 5"
  },
  {
    "id": 3232,
    "content": "__grid_constant__  The __grid_constant__ annotation for compute architectures greater or equal to 7"
  },
  {
    "id": 3233,
    "content": "0 annotates a const -qualified __global__ function parameter of non-reference type that: Has the lifetime of the grid, Is private to the grid, i"
  },
  {
    "id": 3235,
    "content": ", the object is not accessible to host threads and threads from other grids, including sub-grids, Has a distinct object per grid, i"
  },
  {
    "id": 3239,
    "content": ", modifying a __grid_constant__ object or any of its sub-objects is undefined behavior , including mutable members"
  },
  {
    "id": 3240,
    "content": "Requirements: Kernel parameters annotated with __grid_constant__ must have const -qualified non-reference types A function template specialization must match the primary template declaration with respect to any __grid_constant__ parameters A function template instantiation directive must match the primary template declaration with respect to any __grid_constant__ parameters"
  },
  {
    "id": 3241,
    "content": "If the address of a __global__ function parameter is taken, the compiler will ordinarily make a copy of the kernel parameter in thread local memory and use the address of the copy, to partially support C++ semantics, which allow each thread to modify its own local copy of function parameters Annotating a __global__ function parameter with __grid_constant__ ensures that the compiler will not"
  },
  {
    "id": 3242,
    "content": "create a copy of the kernel parameter in thread local memory, but will instead use the generic address of the parameter itself __device__ void unknown_function ( S const & ); __global__ void kernel ( const __grid_constant__ S s ) { s x ; Undefined Behavior: tried to modify read-only memory Compiler will _not_ create a per-thread thread local copy of \"s\": unknown_function ( s ); } 7"
  },
  {
    "id": 3245,
    "content": "__managed__  The __managed__ memory space specifier, optionally used together with __device__ , declares a variable that: Can be referenced from both device and host code, for example, its address can be taken or it can be read or written directly from a device or host function See __managed__ Memory Space Specifier for more details"
  },
  {
    "id": 3250,
    "content": "Restricted pointers were introduced in C99 to alleviate the aliasing problem that exists in C-type languages, and which inhibits all kind of optimization from code re-ordering to common sub-expression elimination"
  },
  {
    "id": 3251,
    "content": "Here is an example subject to the aliasing issue, where use of restricted pointer can help the compiler to reduce the number of instructions: void foo ( const float * a , const float * b , float * c ) { c [ 0 ] = a [ 0 ] * b [ 0 ]; c [ 1 ] = a [ 0 ] * b [ 0 ]; c [ 2 ] = a [ 0 ] * b [ 0 ] * a [ 1 ]; c [ 3 ] = a [ 0 ] * a [ 1 ]; c [ 4 ] = a [ 0 ] * b [ 0 ]; c [ 5 ] = b [ 0 ];"
  },
  {
    "id": 3252,
    "content": "} In C-type languages, the pointers a , b , and c may be aliased, so any write through c could modify elements of a or b"
  },
  {
    "id": 3253,
    "content": "This means that to guarantee functional correctness, the compiler cannot load a[0] and b[0] into registers, multiply them, and store the result to both c[0] and c[1] , because the results would differ from the abstract execution model if, say, a[0] is really the same location as c[0] Likewise, the compiler cannot just reorder the computation of c[4] into the proximity of the computation of c[0]"
  },
  {
    "id": 3254,
    "content": "and c[1] because the preceding write to c[3] could change the inputs to the computation of c[4] By making a , b , and c restricted pointers, the programmer asserts to the compiler that the pointers are in fact not aliased, which in this case means writes through c would never overwrite elements of a or b"
  },
  {
    "id": 3255,
    "content": "This changes the function prototype as follows: void foo ( const float * __restrict__ a , const float * __restrict__ b , float * __restrict__ c ); Note that all pointer arguments need to be made restricted for the compiler optimizer to derive any benefit With the __restrict__ keywords added, the compiler can now reorder and do common sub-expression elimination at will, while retaining"
  },
  {
    "id": 3256,
    "content": "functionality identical with the abstract execution model: void foo ( const float * __restrict__ a , const float * __restrict__ b , float * __restrict__ c ) { float t0 = a [ 0 ]; float t1 = b [ 0 ]; float t2 = t0 * t1 ; float t3 = a [ 1 ]; c [ 0 ] = t2 ; c [ 1 ] = t2 ; c [ 4 ] = t2 ; c [ 2 ] = t2 * t3 ; c [ 3 ] = t0 * t3 ; c [ 5 ] = t1 ;"
  },
  {
    "id": 3258,
    "content": "This is balanced by an increase in register pressure due to “cached” loads and common sub-expressions Since register pressure is a critical issue in many CUDA codes, use of restricted pointers can have negative performance impact on CUDA code, due to reduced occupancy"
  },
  {
    "id": 3264,
    "content": "char, short, int, long, longlong, float, double  These are vector types derived from the basic integer and floating-point types"
  },
  {
    "id": 3265,
    "content": "They are structures and the 1st, 2nd, 3rd, and 4th components are accessible through the fields x , y , z , and w , respectively"
  },
  {
    "id": 3266,
    "content": "They all come with a constructor function of the form make_ ; for example, int2 make_int2 ( int x , int y ); which creates a vector of type int2 with value (x, y)"
  },
  {
    "id": 3267,
    "content": "Table 5 Alignment Requirements  Type Alignment char1, uchar1 1 char2, uchar2 2 char3, uchar3 1 char4, uchar4 4 short1, ushort1 2 short2, ushort2 4 short3, ushort3 2 short4, ushort4 8 int1, uint1 4 int2, uint2 8 int3, uint3 4 int4, uint4 16 long1, ulong1 4 if sizeof(long) is equal to sizeof(int) 8, otherwise long2, ulong2 8 if sizeof(long) is equal to sizeof(int), 16, otherwise long3, ulong3 4 if"
  },
  {
    "id": 3268,
    "content": "sizeof(long) is equal to sizeof(int), 8, otherwise long4, ulong4 16 longlong1, ulonglong1 8 longlong2, ulonglong2 16 longlong3, ulonglong3 8 longlong4, ulonglong4 16 float1 4 float2 8 float3 4 float4 16 double1 8 double2 16 double3 8 double4 16 7"
  },
  {
    "id": 3271,
    "content": "dim3  This type is an integer vector type based on uint3 that is used to specify dimensions When defining a variable of type dim3 , any component left unspecified is initialized to 1"
  },
  {
    "id": 3274,
    "content": "Built-in Variables  Built-in variables specify the grid and block dimensions and the block and thread indices"
  },
  {
    "id": 3283,
    "content": "blockIdx  This variable is of type uint3 (see char, short, int, long, longlong, float, double ) and contains the block index within the grid"
  },
  {
    "id": 3291,
    "content": "threadIdx  This variable is of type uint3 (see char, short, int, long, longlong, float, double ) and contains the thread index within the block"
  },
  {
    "id": 3295,
    "content": "warpSize  This variable is of type int and contains the warp size in threads (see SIMT Architecture for the definition of a warp)"
  },
  {
    "id": 3298,
    "content": "Memory Fence Functions  The CUDA programming model assumes a device with a weakly-ordered memory model, that is the order in which a CUDA thread writes data to shared memory, global memory, page-locked host memory, or the memory of a peer device is not necessarily the order in which the data is observed being written by another CUDA or host thread It is undefined behavior for two threads to read"
  },
  {
    "id": 3300,
    "content": "In the following example, thread 1 executes writeXY() , while thread 2 executes readXY() __device__ int X = 1 , Y = 2 ; __device__ void writeXY () { X = 10 ; Y = 20 ; } __device__ void readXY () { int B = Y ; int A = X ; } The two threads read and write from the same memory locations X and Y simultaneously"
  },
  {
    "id": 3301,
    "content": "Memory fence functions can be used to enforce a sequentially-consistent ordering on memory accesses The memory fence functions differ in the scope in which the orderings are enforced but they are independent of the accessed memory space (shared memory, global memory, page-locked host memory, and the memory of a peer device) void __threadfence_block (); is equivalent to"
  },
  {
    "id": 3302,
    "content": "cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_block) and ensures that: All writes to all memory made by the calling thread before the call to __threadfence_block() are observed by all threads in the block of the calling thread as occurring before all writes to all memory made by the calling thread after the call to __threadfence_block() ; All reads from all memory made"
  },
  {
    "id": 3303,
    "content": "by the calling thread before the call to __threadfence_block() are ordered before all reads from all memory made by the calling thread after the call to __threadfence_block() void __threadfence (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_device) and ensures that no writes to all memory made by the calling thread after the call to __threadfence()"
  },
  {
    "id": 3304,
    "content": "are observed by any thread in the device as occurring before any write to all memory made by the calling thread before the call to __threadfence() void __threadfence_system (); is equivalent to cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope_system) and ensures that all writes to all memory made by the calling thread before the call to __threadfence_system() are observed"
  },
  {
    "id": 3305,
    "content": "by all threads in the device, host threads, and all threads in peer devices as occurring before all writes to all memory made by the calling thread after the call to __threadfence_system() In the previous code sample, we can insert fences in the codes as follows: __device__ int X = 1 , Y = 2 ; __device__ void writeXY () { X = 10 ; __threadfence (); Y = 20 ; } __device__ void readXY () { int B = Y"
  },
  {
    "id": 3306,
    "content": "; __threadfence (); int A = X ; } For this code, the following outcomes can be observed: A equal to 1 and B equal to 2, A equal to 10 and B equal to 2, A equal to 10 and B equal to 20"
  },
  {
    "id": 3308,
    "content": "If thread 1 and 2 do not belong to the same block, __threadfence() must be used if they are CUDA threads from the same device and __threadfence_system() must be used if they are CUDA threads from two different devices"
  },
  {
    "id": 3309,
    "content": "A common use case is when threads consume some data produced by other threads as illustrated by the following code sample of a kernel that computes the sum of an array of N numbers in one call"
  },
  {
    "id": 3310,
    "content": "When all blocks are done, the last block done reads each of these partial sums from global memory and sums them to obtain the final result In order to determine which block is finished last, each block atomically increments a counter to signal that it is done with computing and storing its partial sum (see Atomic Functions about atomic functions) If no fence is placed between storing the partial"
  },
  {
    "id": 3311,
    "content": "sum and incrementing the counter, the counter might increment before the partial sum is stored and therefore, might reach gridDim x-1 and let the last block start reading partial sums before they have been actually updated in memory Memory fence functions only affect the ordering of memory operations by a thread; they do not, by themselves, ensure that these memory operations are visible to other"
  },
  {
    "id": 3312,
    "content": "threads (like __syncthreads() does for threads within a block (see Synchronization Functions )) In the code sample below, the visibility of memory operations on the result variable is ensured by declaring it as volatile (see Volatile Qualifier ) __device__ unsigned int count = 0 ; __shared__ bool isLastBlockDone ; __global__ void sum ( const float * array , unsigned int N , volatile float * result"
  },
  {
    "id": 3313,
    "content": ") { Each block sums a subset of the input array The compiler will use a store operation that bypasses the L1 cache since the \"result\" variable is declared as volatile This ensures that the threads of the last block will read the correct partial sums computed by all other blocks x ] = partialSum ; Thread 0 makes sure that the incrementing of the \"count\" variable is only performed after the partial"
  },
  {
    "id": 3314,
    "content": "sum has been written to global memory x - 1 )); } Synchronize to make sure that each thread reads the correct value of isLastBlockDone __syncthreads (); if ( isLastBlockDone ) { The last block sums the partial sums stored in result[0 x == 0 ) { Thread 0 of last block stores the total sum to global memory and resets the count variable, so that the next kernel call works properly Synchronization"
  },
  {
    "id": 3315,
    "content": "Functions  void __syncthreads (); waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to __syncthreads() are visible to all threads in the block __syncthreads() is used to coordinate communication between the threads of the same block When some threads within a block access the same addresses in shared or global"
  },
  {
    "id": 3316,
    "content": "memory, there are potential read-after-write, write-after-read, or write-after-write hazards for some of these memory accesses __syncthreads() is allowed in conditional code but only if the conditional evaluates identically across the entire thread block, otherwise the code execution is likely to hang or produce unintended side effects"
  },
  {
    "id": 3319,
    "content": "int __syncthreads_count ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero int __syncthreads_and ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and"
  },
  {
    "id": 3320,
    "content": "returns non-zero if and only if predicate evaluates to non-zero for all of them int __syncthreads_or ( int predicate ); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them"
  },
  {
    "id": 3321,
    "content": "void __syncwarp ( unsigned mask = 0xffffffff ); will cause the executing thread to wait until all warp lanes named in mask have executed a __syncwarp() (with the same mask) before resuming execution Each calling thread must have its own bit set in the mask and all non-exited threads named in mask must execute a corresponding __syncwarp() with the same mask, or the result is undefined Executing"
  },
  {
    "id": 3322,
    "content": "__syncwarp() guarantees memory ordering among threads participating in the barrier Thus, threads within a warp that wish to communicate via memory can store to memory, execute __syncwarp() , and then safely read values stored by other threads in the warp"
  },
  {
    "id": 3324,
    "content": "target sm_6x or below, all threads in mask must execute the same __syncwarp() in convergence, and the union of all values in mask must be equal to the active mask"
  },
  {
    "id": 3328,
    "content": "Mathematical Functions  The reference manual lists all C/C++ standard library mathematical functions that are supported in device code and all intrinsic functions that are only supported in device code Mathematical Functions provides accuracy information for some of these functions when relevant"
  },
  {
    "id": 3331,
    "content": "Texture Functions  Texture objects are described in Texture Object API Texture fetching is described in Texture Fetching tex1Dfetch()  template T tex1Dfetch ( cudaTextureObject_t texObj , int x ); fetches from the region of linear memory specified by the one-dimensional texture object texObj using integer texture coordinate x tex1Dfetch() only works with non-normalized coordinates, so only the"
  },
  {
    "id": 3338,
    "content": "tex1D()  template T tex1D ( cudaTextureObject_t texObj , float x ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x"
  },
  {
    "id": 3343,
    "content": "tex1DLod()  template T tex1DLod ( cudaTextureObject_t texObj , float x , float level ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x at the level-of-detail level"
  },
  {
    "id": 3348,
    "content": "tex1DGrad()  template T tex1DGrad ( cudaTextureObject_t texObj , float x , float dx , float dy ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x The level-of-detail is derived from the X-gradient dx and Y-gradient dy"
  },
  {
    "id": 3353,
    "content": "tex2D()  template T tex2D ( cudaTextureObject_t texObj , float x , float y ); fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y)"
  },
  {
    "id": 3358,
    "content": "tex2D() for sparse CUDA arrays  template T tex2D ( cudaTextureObject_t texObj , float x , float y , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y)"
  },
  {
    "id": 3364,
    "content": "tex2Dgather()  template T tex2Dgather ( cudaTextureObject_t texObj , float x , float y , int comp = 0 ); fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather"
  },
  {
    "id": 3369,
    "content": "tex2Dgather() for sparse CUDA arrays  template T tex2Dgather ( cudaTextureObject_t texObj , float x , float y , bool * isResident , int comp = 0 ); fetches from the CUDA array specified by the 2D texture object texObj using texture coordinates x and y and the comp parameter as described in Texture Gather"
  },
  {
    "id": 3374,
    "content": "tex2DGrad()  template T tex2DGrad ( cudaTextureObject_t texObj , float x , float y , float2 dx , float2 dy ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) The level-of-detail is derived from the dx and dy gradients"
  },
  {
    "id": 3379,
    "content": "tex2DGrad() for sparse CUDA arrays  template T tex2DGrad ( cudaTextureObject_t texObj , float x , float y , float2 dx , float2 dy , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y)"
  },
  {
    "id": 3384,
    "content": "tex2DLod()  template tex2DLod ( cudaTextureObject_t texObj , float x , float y , float level ); fetches from the CUDA array or the region of linear memory specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level"
  },
  {
    "id": 3389,
    "content": "tex2DLod() for sparse CUDA arrays  template tex2DLod ( cudaTextureObject_t texObj , float x , float y , float level , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) at level-of-detail level"
  },
  {
    "id": 3394,
    "content": "tex3D()  template T tex3D ( cudaTextureObject_t texObj , float x , float y , float z ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z)"
  },
  {
    "id": 3399,
    "content": "tex3D() for sparse CUDA arrays  template T tex3D ( cudaTextureObject_t texObj , float x , float y , float z , bool * isResident ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z)"
  },
  {
    "id": 3404,
    "content": "tex3DLod()  template T tex3DLod ( cudaTextureObject_t texObj , float x , float y , float z , float level ); fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level"
  },
  {
    "id": 3409,
    "content": "tex3DLod() for sparse CUDA arrays  template T tex3DLod ( cudaTextureObject_t texObj , float x , float y , float z , float level , bool * isResident ); fetches from the CUDA array or the region of linear memory specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at level-of-detail level"
  },
  {
    "id": 3414,
    "content": "tex3DGrad()  template T tex3DGrad ( cudaTextureObject_t texObj , float x , float y , float z , float4 dx , float4 dy ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy"
  },
  {
    "id": 3419,
    "content": "tex3DGrad() for sparse CUDA arrays  template T tex3DGrad ( cudaTextureObject_t texObj , float x , float y , float z , float4 dx , float4 dy , bool * isResident ); fetches from the CUDA array specified by the three-dimensional texture object texObj using texture coordinate (x,y,z) at a level-of-detail derived from the X and Y gradients dx and dy"
  },
  {
    "id": 3424,
    "content": "tex1DLayered()  template T tex1DLayered ( cudaTextureObject_t texObj , float x , int layer ); fetches from the CUDA array specified by the one-dimensional texture object texObj using texture coordinate x and index layer , as described in Layered Textures 7"
  },
  {
    "id": 3428,
    "content": "tex1DLayeredLod()  template T tex1DLayeredLod ( cudaTextureObject_t texObj , float x , int layer , float level ); fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and level-of-detail level"
  },
  {
    "id": 3433,
    "content": "tex1DLayeredGrad()  template T tex1DLayeredGrad ( cudaTextureObject_t texObj , float x , int layer , float dx , float dy ); fetches from the CUDA array specified by the one-dimensional layered texture at layer layer using texture coordinate x and a level-of-detail derived from the dx and dy gradients"
  },
  {
    "id": 3438,
    "content": "tex2DLayered()  template T tex2DLayered ( cudaTextureObject_t texObj , float x , float y , int layer ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer , as described in Layered Textures"
  },
  {
    "id": 3443,
    "content": "tex2DLayered() for sparse CUDA arrays  template T tex2DLayered ( cudaTextureObject_t texObj , float x , float y , int layer , bool * isResident ); fetches from the CUDA array specified by the two-dimensional texture object texObj using texture coordinate (x,y) and index layer , as described in Layered Textures"
  },
  {
    "id": 3448,
    "content": "tex2DLayeredLod()  template T tex2DLayeredLod ( cudaTextureObject_t texObj , float x , float y , int layer , float level ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y)"
  },
  {
    "id": 3453,
    "content": "tex2DLayeredLod() for sparse CUDA arrays  template T tex2DLayeredLod ( cudaTextureObject_t texObj , float x , float y , int layer , float level , bool * isResident ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y)"
  },
  {
    "id": 3458,
    "content": "tex2DLayeredGrad()  template T tex2DLayeredGrad ( cudaTextureObject_t texObj , float x , float y , int layer , float2 dx , float2 dy ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients"
  },
  {
    "id": 3463,
    "content": "tex2DLayeredGrad() for sparse CUDA arrays  template T tex2DLayeredGrad ( cudaTextureObject_t texObj , float x , float y , int layer , float2 dx , float2 dy , bool * isResident ); fetches from the CUDA array specified by the two-dimensional layered texture at layer layer using texture coordinate (x,y) and a level-of-detail derived from the dx and dy gradients"
  },
  {
    "id": 3468,
    "content": "texCubemap()  template T texCubemap ( cudaTextureObject_t texObj , float x , float y , float z ); fetches the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) , as described in Cubemap Textures"
  },
  {
    "id": 3473,
    "content": "texCubemapGrad()  template T texCubemapGrad ( cudaTextureObject_t texObj , float x , float , y , float z , float4 dx , float4 dy ); fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures The level-of-detail used is derived from the dx and dy gradients"
  },
  {
    "id": 3478,
    "content": "texCubemapLod()  template T texCubemapLod ( cudaTextureObject_t texObj , float x , float , y , float z , float level ); fetches from the CUDA array specified by the cubemap texture object texObj using texture coordinate (x,y,z) as described in Cubemap Textures"
  },
  {
    "id": 3484,
    "content": "texCubemapLayered()  template T texCubemapLayered ( cudaTextureObject_t texObj , float x , float y , float z , int layer ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinates (x,y,z) , and index layer , as described in Cubemap Layered Textures"
  },
  {
    "id": 3489,
    "content": "texCubemapLayeredGrad()  template T texCubemapLayeredGrad ( cudaTextureObject_t texObj , float x , float y , float z , int layer , float4 dx , float4 dy ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer , as described in Cubemap Layered Textures , at level-of-detail derived from the dx and dy gradients"
  },
  {
    "id": 3494,
    "content": "texCubemapLayeredLod()  template T texCubemapLayeredLod ( cudaTextureObject_t texObj , float x , float y , float z , int layer , float level ); fetches from the CUDA array specified by the cubemap layered texture object texObj using texture coordinate (x,y,z) and index layer , as described in Cubemap Layered Textures , at level-of-detail level level"
  },
  {
    "id": 3499,
    "content": "Surface objects are described in described in Surface Object API In the sections below, boundaryMode specifies the boundary mode, that is how out-of-range surface coordinates are handled; it is equal to either cudaBoundaryModeClamp , in which case out-of-range coordinates are clamped to the valid range, or cudaBoundaryModeZero , in which case out-of-range reads return zero and out-of-range writes"
  },
  {
    "id": 3500,
    "content": "are ignored, or cudaBoundaryModeTrap , in which case out-of-range accesses cause the kernel execution to fail"
  },
  {
    "id": 3508,
    "content": "surf1Dread()  template T surf1Dread ( cudaSurfaceObject_t surfObj , int x , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the one-dimensional surface object surfObj using byte coordinate x"
  },
  {
    "id": 3513,
    "content": "surf1Dwrite  template void surf1Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the one-dimensional surface object surfObj at byte coordinate x"
  },
  {
    "id": 3518,
    "content": "surf2Dread()  template T surf2Dread ( cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); template void surf2Dread ( T * data , cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the two-dimensional surface object surfObj using byte coordinates x and y"
  },
  {
    "id": 3523,
    "content": "surf2Dwrite()  template void surf2Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the two-dimensional surface object surfObj at byte coordinate x and y"
  },
  {
    "id": 3528,
    "content": "surf3Dread()  template T surf3Dread ( cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); template void surf3Dread ( T * data , cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the three-dimensional surface object surfObj using byte coordinates x, y, and z"
  },
  {
    "id": 3533,
    "content": "surf3Dwrite()  template void surf3Dwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int z , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the three-dimensional object surfObj at byte coordinate x, y, and z"
  },
  {
    "id": 3538,
    "content": "surf1DLayeredread()  template T surf1DLayeredread ( cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); template void surf1DLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the one-dimensional layered surface object surfObj using byte coordinate x and index layer"
  },
  {
    "id": 3543,
    "content": "surf1DLayeredwrite()  template void surf1DLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int layer , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the two-dimensional layered surface object surfObj at byte coordinate x and index layer"
  },
  {
    "id": 3548,
    "content": "surf2DLayeredread()  template T surf2DLayeredread ( cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); template void surf2DLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the two-dimensional layered surface object surfObj using byte coordinate x"
  },
  {
    "id": 3554,
    "content": "surf2DLayeredwrite()  template void surf2DLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layer , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the one-dimensional layered surface object surfObj at byte coordinate x and y, and index layer"
  },
  {
    "id": 3559,
    "content": "surfCubemapread()  template T surfCubemapread ( cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); template void surfCubemapread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the cubemap surface object surfObj using byte coordinate x and y, and face index"
  },
  {
    "id": 3565,
    "content": "surfCubemapwrite()  template void surfCubemapwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int face , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the cubemap object surfObj at byte coordinate x and y, and face index face"
  },
  {
    "id": 3570,
    "content": "surfCubemapLayeredread()  template T surfCubemapLayeredread ( cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); template void surfCubemapLayeredread ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); reads the CUDA array specified by the cubemap layered surface object surfObj using"
  },
  {
    "id": 3576,
    "content": "surfCubemapLayeredwrite()  template void surfCubemapLayeredwrite ( T data , cudaSurfaceObject_t surfObj , int x , int y , int layerFace , boundaryMode = cudaBoundaryModeTrap ); writes value data to the CUDA array specified by the cubemap layered object surfObj at byte coordinate x and y , and index layerFace"
  },
  {
    "id": 3579,
    "content": "Read-Only Data Cache Load Function  The read-only data cache load function is only supported by devices of compute capability 5"
  },
  {
    "id": 3581,
    "content": "T __ldg ( const T * address ); returns the data of type T located at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or"
  },
  {
    "id": 3588,
    "content": "Load Functions Using Cache Hints  These load functions are only supported by devices of compute capability 5"
  },
  {
    "id": 3590,
    "content": "T __ldcg ( const T * address ); T __ldca ( const T * address ); T __ldcs ( const T * address ); T __ldlu ( const T * address ); T __ldcv ( const T * address ); returns the data of type T located at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2 , short4 ,"
  },
  {
    "id": 3591,
    "content": "int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2"
  },
  {
    "id": 3592,
    "content": "Store Functions Using Cache Hints  These store functions are only supported by devices of compute capability 5"
  },
  {
    "id": 3594,
    "content": "void __stwb ( T * address , T value ); void __stcg ( T * address , T value ); void __stcs ( T * address , T value ); void __stwt ( T * address , T value ); stores the value argument of type T to the location at address address , where T is char , signed char , short , int , long , long long unsigned char , unsigned short , unsigned int , unsigned long , unsigned long long , char2 , char4 , short2"
  },
  {
    "id": 3595,
    "content": ", short4 , int2 , int4 , longlong2 uchar2 , uchar4 , ushort2 , ushort4 , uint2 , uint4 , ulonglong2 float , float2 , float4 , double , or double2 Time Function  clock_t clock (); long long int clock64 (); when executed in device code, returns the value of a per-multiprocessor counter that is incremented every clock cycle Sampling this counter at the beginning and at the end of a kernel, taking"
  },
  {
    "id": 3596,
    "content": "the difference of the two samples, and recording the result per thread provides a measure for each thread of the number of clock cycles taken by the device to completely execute the thread, but not of the number of clock cycles the device actually spent executing thread instructions"
  },
  {
    "id": 3600,
    "content": "Atomic Functions  An atomic function performs a read-modify-write atomic operation on one 32-bit, 64-bit, or 128-bit word residing in global or shared memory In the case of float2 or float4 , the read-modify-write operation is performed on each element of the vector residing in global memory"
  },
  {
    "id": 3601,
    "content": "For example, atomicAdd() reads a word at some address in global or shared memory, adds a number to it, and writes the result back to the same address"
  },
  {
    "id": 3602,
    "content": "The atomic functions described in this section have ordering cuda::memory_order_relaxed and are only atomic at a particular scope : Atomic APIs with _system suffix (example: atomicAdd_system ) are atomic at scope cuda::thread_scope_system if they meet particular conditions Atomic APIs without a suffix (example: atomicAdd ) are atomic at scope cuda::thread_scope_device Atomic APIs with _block"
  },
  {
    "id": 3604,
    "content": "In the following example both the CPU and the GPU atomically update an integer value at address addr : __global__ void mykernel ( int * addr ) { atomicAdd_system ( addr , 10 ); only available on devices with compute capability 6 x } void foo () { int * addr ; cudaMallocManaged ( & addr , 4 ); * addr = 0 ; mykernel >> ( addr ); __sync_fetch_and_add ( addr , 10 ); CPU atomic operation } Note that"
  },
  {
    "id": 3605,
    "content": "any atomic operation can be implemented based on atomicCAS() (Compare And Swap) For example, atomicAdd() for double-precision floating-point numbers is not available on devices with compute capability lower than 6"
  },
  {
    "id": 3606,
    "content": "0 but it can be implemented as follows: #if __CUDA_ARCH__ T atomicExch ( T * address , T val ); reads the 128-bit word old located at the address address in global or shared memory and stores val back to memory at the same address"
  },
  {
    "id": 3607,
    "content": "The type T must meet the following requirements: sizeof ( T ) == 16 alignof ( T ) >= 16 std :: is_trivially_copyable :: value == true   for C++03 and older std :: is_default_constructible :: value == true So, T must be 128-bit and properly aligned, be trivially copyable, and on C++03 or older, it must also be default constructible"
  },
  {
    "id": 3614,
    "content": "atomicMin()  int atomicMin ( int * address , int val ); unsigned int atomicMin ( unsigned int * address , unsigned int val ); unsigned long long int atomicMin ( unsigned long long int * address , unsigned long long int val ); long long int atomicMin ( long long int * address , long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory,"
  },
  {
    "id": 3615,
    "content": "computes the minimum of old and val , and stores the result back to memory at the same address The 64-bit version of atomicMin() is only supported by devices of compute capability 5"
  },
  {
    "id": 3621,
    "content": "atomicMax()  int atomicMax ( int * address , int val ); unsigned int atomicMax ( unsigned int * address , unsigned int val ); unsigned long long int atomicMax ( unsigned long long int * address , unsigned long long int val ); long long int atomicMax ( long long int * address , long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory,"
  },
  {
    "id": 3622,
    "content": "computes the maximum of old and val , and stores the result back to memory at the same address The 64-bit version of atomicMax() is only supported by devices of compute capability 5"
  },
  {
    "id": 3628,
    "content": "atomicInc()  unsigned int atomicInc ( unsigned int * address , unsigned int val ); reads the 32-bit word old located at the address address in global or shared memory, computes ((old >= val) 0 : (old+1)) , and stores the result back to memory at the same address"
  },
  {
    "id": 3633,
    "content": "atomicDec()  unsigned int atomicDec ( unsigned int * address , unsigned int val ); reads the 32-bit word old located at the address address in global or shared memory, computes (((old == 0) || (old > val)) val : (old-1) ), and stores the result back to memory at the same address"
  },
  {
    "id": 3638,
    "content": "atomicCAS()  int atomicCAS ( int * address , int compare , int val ); unsigned int atomicCAS ( unsigned int * address , unsigned int compare , unsigned int val ); unsigned long long int atomicCAS ( unsigned long long int * address , unsigned long long int compare , unsigned long long int val ); unsigned short int atomicCAS ( unsigned short int * address , unsigned short int compare , unsigned"
  },
  {
    "id": 3639,
    "content": "short int val ); reads the 16-bit, 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old == compare val : old) , and stores the result back to memory at the same address template T atomicCAS ( T * address , T compare , T val ); reads the 128-bit word old located at the address address in global or shared memory, computes (old == compare The 128-bit"
  },
  {
    "id": 3649,
    "content": "atomicAnd()  int atomicAnd ( int * address , int val ); unsigned int atomicAnd ( unsigned int * address , unsigned int val ); unsigned long long int atomicAnd ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old & val ), and stores the result back to memory at the same"
  },
  {
    "id": 3656,
    "content": "atomicOr()  int atomicOr ( int * address , int val ); unsigned int atomicOr ( unsigned int * address , unsigned int val ); unsigned long long int atomicOr ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old | val) , and stores the result back to memory at the same address"
  },
  {
    "id": 3663,
    "content": "atomicXor()  int atomicXor ( int * address , int val ); unsigned int atomicXor ( unsigned int * address , unsigned int val ); unsigned long long int atomicXor ( unsigned long long int * address , unsigned long long int val ); reads the 32-bit or 64-bit word old located at the address address in global or shared memory, computes (old ^ val) , and stores the result back to memory at the same"
  },
  {
    "id": 3668,
    "content": "Address Space Predicate Functions  The functions described in this section have unspecified behavior if the argument is a null pointer"
  },
  {
    "id": 3672,
    "content": "__isGlobal()  __device__ unsigned int __isGlobal ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in global memory space, otherwise returns 0"
  },
  {
    "id": 3676,
    "content": "__isShared()  __device__ unsigned int __isShared ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in shared memory space, otherwise returns 0"
  },
  {
    "id": 3680,
    "content": "__isConstant()  __device__ unsigned int __isConstant ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in constant memory space, otherwise returns 0"
  },
  {
    "id": 3684,
    "content": "__isGridConstant()  __device__ unsigned int __isGridConstant ( const void * ptr ); Returns 1 if ptr contains the generic address of a kernel parameter annotated with __grid_constant__ , otherwise returns 0"
  },
  {
    "id": 3690,
    "content": "__isLocal()  __device__ unsigned int __isLocal ( const void * ptr ); Returns 1 if ptr contains the generic address of an object in local memory space, otherwise returns 0 __cvta_generic_to_global()  __device__ size_t __cvta_generic_to_global ( const void * ptr ); Returns the result of executing the PTX cvta"
  },
  {
    "id": 3696,
    "content": "__cvta_generic_to_shared()  __device__ size_t __cvta_generic_to_shared ( const void * ptr ); Returns the result of executing the PTX cvta"
  },
  {
    "id": 3702,
    "content": "__cvta_generic_to_constant()  __device__ size_t __cvta_generic_to_constant ( const void * ptr ); Returns the result of executing the PTX cvta"
  },
  {
    "id": 3708,
    "content": "__cvta_generic_to_local()  __device__ size_t __cvta_generic_to_local ( const void * ptr ); Returns the result of executing the PTX cvta"
  },
  {
    "id": 3714,
    "content": "__cvta_global_to_generic()  __device__ void * __cvta_global_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta global instruction on the value provided by rawbits"
  },
  {
    "id": 3718,
    "content": "__cvta_shared_to_generic()  __device__ void * __cvta_shared_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta shared instruction on the value provided by rawbits"
  },
  {
    "id": 3722,
    "content": "__cvta_constant_to_generic()  __device__ void * __cvta_constant_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta const instruction on the value provided by rawbits"
  },
  {
    "id": 3726,
    "content": "__cvta_local_to_generic()  __device__ void * __cvta_local_to_generic ( size_t rawbits ); Returns the generic pointer obtained by executing the PTX cvta local instruction on the value provided by rawbits"
  },
  {
    "id": 3727,
    "content": "Description  The alloca() function allocates size bytes of memory in the stack frame of the caller The returned value is a pointer to allocated memory, the beginning of the memory is 16 bytes aligned when the function is invoked from device code Using alloca() may cause the stack to overflow, user needs to adjust stack size accordingly"
  },
  {
    "id": 3733,
    "content": "Example  __device__ void foo ( unsigned int num ) { int4 * ptr = ( int4 * ) alloca ( num * sizeof ( int4 ));   use of ptr"
  },
  {
    "id": 3736,
    "content": "Compiler Optimization Hint Functions  The functions described in this section can be used to provide additional information to the compiler optimizer"
  },
  {
    "id": 3740,
    "content": "__builtin_assume_aligned()  void * __builtin_assume_aligned ( const void * exp , size_t align ) Allows the compiler to assume that the argument pointer is aligned to at least align bytes, and returns the argument pointer Example: void * res = __builtin_assume_aligned ( ptr , 32 ); compiler can assume 'res' is at least 32-byte aligned Three parameter version: void * __builtin_assume_aligned ("
  },
  {
    "id": 3741,
    "content": "const void * exp , size_t align , offset ) Allows the compiler to assume that (char *)exp - offset is aligned to at least align bytes, and returns the argument pointer Example: void * res = __builtin_assume_aligned ( ptr , 32 , 8 ); compiler can assume '(char *)res - 8' is at least 32-byte aligned"
  },
  {
    "id": 3745,
    "content": "__builtin_assume()  void __builtin_assume ( bool exp ) Allows the compiler to assume that the Boolean argument is true Example: __device__ int get ( int * ptr , int idx ) { __builtin_assume ( idx __global__ void bcast ( int arg ) { int laneId = threadIdx x & 0x1f ; int value ; if ( laneId == 0 ) Note unused variable for value = arg ; all threads except lane 0 value = __shfl_sync ( 0xffffffff ,"
  },
  {
    "id": 3746,
    "content": "value , 0 ); Synchronize all threads in warp, and get \"value\" from lane 0 if ( value = arg ) printf ( \"Thread %d failed Inclusive plus-scan across sub-partitions of 8 threads  #include __global__ void scan4 () { int laneId = threadIdx x & 0x1f ; Seed sample starting value (inverse of lane ID) int value = 31 - laneId ; Loop to accumulate scan within my partition"
  },
  {
    "id": 3747,
    "content": "Scan requires log2(n) == 3 steps for 8 threads   It works by an accumulated sum up the warp   by 1, 2, 4, 8 etc"
  },
  {
    "id": 3748,
    "content": "for ( int i = 1 ; i = i ) value += n ; } printf ( \"Thread %d final value = %d   \" , threadIdx x , value ); } int main () { scan4 >> (); cudaDeviceSynchronize (); return 0 ; } 7"
  },
  {
    "id": 3752,
    "content": "Reduction across a warp  #include __global__ void warpReduce () { int laneId = threadIdx x & 0x1f ; Seed starting value as inverse lane ID int value = 31 - laneId ; Use XOR mode to perform butterfly reduction for ( int i = 16 ; i >= 1 ; i /= 2 ) value += __shfl_xor_sync ( 0xffffffff , value , i , 32 ); \"value\" now contains the sum across all threads printf ( \"Thread %d final value = %d \" ,"
  },
  {
    "id": 3755,
    "content": "Description  __nanosleep(ns) suspends the thread for a sleep duration of approximately ns nanoseconds"
  },
  {
    "id": 3762,
    "content": "__device__ void mutex_lock ( unsigned int * mutex ) { unsigned int ns = 8 ; while ( atomicCAS ( mutex , 0 , 1 ) == 1 ) { __nanosleep ( ns ); if ( ns class fragment ; void load_matrix_sync ( fragment & a , const T * mptr , unsigned ldm ); void load_matrix_sync ( fragment & a , const T * mptr , unsigned ldm , layout_t layout ); void store_matrix_sync ( T * mptr , const fragment & a , unsigned ldm ,"
  },
  {
    "id": 3763,
    "content": "layout_t layout ); void fill_fragment ( fragment & a , const T & v ); void mma_sync ( fragment & d , const fragment & a , const fragment & b , const fragment & c , bool satf = false ); fragment An overloaded class containing a section of a matrix distributed across all threads in the warp The mapping of matrix elements into fragment internal storage is unspecified and subject to change in future"
  },
  {
    "id": 3764,
    "content": "architectures The first template parameter specifies how the fragment will participate in the matrix operation Acceptable values for Use are: matrix_a when the fragment is used as the first multiplicand, A , matrix_b when the fragment is used as the second multiplicand, B , or accumulator when the fragment is used as the source or destination accumulators ( C or D , respectively) The m , n and k"
  },
  {
    "id": 3765,
    "content": "sizes describe the shape of the warp-wide matrix tiles participating in the multiply-accumulate operation For matrix_a the tile takes dimension m x k ; for matrix_b the dimension is k x n , and accumulator tiles are m x n"
  },
  {
    "id": 3766,
    "content": "The data type, T , may be double , float , __half , __nv_bfloat16 , char , or unsigned char for multiplicands and double , float , int , or __half for accumulators"
  },
  {
    "id": 3767,
    "content": "As documented in Element Types and Matrix Sizes , limited combinations of accumulator and multiplicand types are supported"
  },
  {
    "id": 3768,
    "content": "row_major or col_major indicate that elements within a matrix row or column are contiguous in memory, respectively A row or column layout is specified only when the accumulator is loaded or stored as described below"
  },
  {
    "id": 3769,
    "content": "load_matrix_sync Waits until all warp lanes have arrived at load_matrix_sync and then loads the matrix fragment a from memory"
  },
  {
    "id": 3770,
    "content": "mptr must be a 256-bit aligned pointer pointing to the first element of the matrix in memory ldm describes the stride in elements between consecutive rows (for row major layout) or columns (for column major layout) and must be a multiple of 8 for __half element type or multiple of 4 for float element type If the fragment is an accumulator , the layout argument must be specified as either"
  },
  {
    "id": 3771,
    "content": "mem_row_major or mem_col_major For matrix_a and matrix_b fragments, the layout is inferred from the fragment’s layout parameter The values of mptr , ldm , layout and all template parameters for a must be the same for all threads in the warp"
  },
  {
    "id": 3772,
    "content": "store_matrix_sync Waits until all warp lanes have arrived at store_matrix_sync and then stores the matrix fragment a to memory"
  },
  {
    "id": 3774,
    "content": "Because the mapping of matrix elements to each fragment is unspecified, this function is ordinarily called by all threads in the warp with a common value for v"
  },
  {
    "id": 3775,
    "content": "mma_sync Waits until all warp lanes have arrived at mma_sync, and then performs the warp-synchronous matrix multiply-accumulate operation D=A*B+C The value of satf and template parameters for each matrix fragment must be the same for all threads in the warp If satf (saturate to finite value) mode is true , the following additional numerical properties apply for the destination accumulator: If an"
  },
  {
    "id": 3776,
    "content": "element result is +Infinity, the corresponding accumulator will contain +MAX_NORM If an element result is -Infinity, the corresponding accumulator will contain -MAX_NORM If an element result is NaN, the corresponding accumulator will contain +0 Because the map of matrix elements into each thread’s fragment is unspecified, individual matrix elements must be accessed from memory (shared or global)"
  },
  {
    "id": 3777,
    "content": "after calling store_matrix_sync In the special case where all threads in the warp will apply an element-wise operation uniformly to all fragment elements, direct element access can be implemented using the following fragment class members enum fragment :: num_elements ; T fragment :: x [ num_elements ]; As an example, the following code scales an accumulator matrix tile by half"
  },
  {
    "id": 3781,
    "content": "In order to use this floating point format with WMMA operations, the input matrices must be manually converted to tf32 precision While the input and output arguments to the intrinsic are of float type, the output will be tf32 numerically This new precision is intended to be used with Tensor Cores only, and if mixed with other float type operations, the precision and range of the result will be"
  },
  {
    "id": 3782,
    "content": "undefined Once an input matrix ( matrix_a or matrix_b ) is converted to tf32 precision, the combination of a fragment with precision::tf32 precision, and a data type of float to load_matrix_sync will take advantage of this new capability The elements of the fragment are represented as float , hence the mapping from element_type to storage_element_type is: precision :: tf32 -> float 7"
  },
  {
    "id": 3785,
    "content": "Double Precision  Tensor Cores support double-precision floating point operations on devices with compute capability 8"
  },
  {
    "id": 3792,
    "content": "Sub-byte Operations  Sub-byte WMMA operations provide a way to access the low-precision capabilities of Tensor Cores"
  },
  {
    "id": 3793,
    "content": "the data structures and APIs for them are subject to change and may not be compatible with future releases"
  },
  {
    "id": 3794,
    "content": "This functionality is available via the nvcuda::wmma::experimental namespace: namespace experimental { namespace precision { struct u4 ; 4-bit unsigned struct s4 ; 4-bit signed struct b1 ; 1-bit } enum bmmaBitOp { bmmaBitOpXOR = 1 , compute_75 minimum bmmaBitOpAND = 2 compute_80 minimum }; enum bmmaAccumulateOp { bmmaAccumulateOpPOPC = 1 }; } For 4 bit precision, the APIs available remain the"
  },
  {
    "id": 3795,
    "content": "same, but you must specify experimental::precision::u4 or experimental::precision::s4 as the fragment data type"
  },
  {
    "id": 3796,
    "content": "Since the elements of the fragment are packed together, num_storage_elements will be smaller than num_elements for that fragment The num_elements variable for a sub-byte fragment, hence returns the number of elements of sub-byte type element_type This is true for single bit precision as well, in which case, the mapping from element_type to storage_element_type is as follows: experimental ::"
  },
  {
    "id": 3797,
    "content": "precision :: u4 -> unsigned ( 8 elements in 1 storage element ) experimental :: precision :: s4 -> int ( 8 elements in 1 storage element ) experimental :: precision :: b1 -> unsigned ( 32 elements in 1 storage element ) T -> T all other types The allowed layouts for sub-byte fragments is always row_major for matrix_a and col_major for matrix_b For sub-byte operations the value of ldm in"
  },
  {
    "id": 3798,
    "content": "load_matrix_sync should be a multiple of 32 for element type experimental::precision::u4 and experimental::precision::s4 or a multiple of 128 for element type experimental::precision::b1 (i"
  },
  {
    "id": 3801,
    "content": "Note Support for the following variants for MMA instructions is deprecated and will be removed in sm_90: experimental::precision::u4 experimental::precision::s4 experimental::precision::b1 with bmmaBitOp set to bmmaBitOpXOR bmma_sync Waits until all warp lanes have executed bmma_sync, and then performs the warp-synchronous bit matrix multiply-accumulate operation D = (A op B) + C , where op"
  },
  {
    "id": 3803,
    "content": "The available operations are: bmmaBitOpXOR , a 128-bit XOR of a row in matrix_a with the 128-bit column of matrix_b bmmaBitOpAND , a 128-bit AND of a row in matrix_a with the 128-bit column of matrix_b , available on devices with compute capability 8"
  },
  {
    "id": 3809,
    "content": "Restrictions  The special format required by tensor cores may be different for each major and minor device architecture"
  },
  {
    "id": 3810,
    "content": "This is further complicated by threads holding only a fragment (opaque architecture-specific ABI data structure) of the overall matrix, with the developer not allowed to make assumptions on how the individual parameters are mapped to the registers participating in the matrix multiply-accumulate"
  },
  {
    "id": 3811,
    "content": "Since fragments are architecture-specific, it is unsafe to pass them from function A to function B if the functions have been compiled for different link-compatible architectures and linked together into the same device executable"
  },
  {
    "id": 3812,
    "content": "In this case, the size and layout of the fragment will be specific to one architecture and using WMMA APIs in the other will lead to incorrect results or potentially, corruption An example of two link-compatible architectures, where the layout of the fragment differs, is sm_70 and sm_75 cu : void bar ( wmma :: fragment * mat_a ) { operate on mat_a } sm_70 fragment layout $ > nvcc - dc - arch ="
  },
  {
    "id": 3814,
    "content": "o This undefined behavior might also be undetectable at compilation time and by tools at runtime, so extra care is needed to make sure the layout of the fragments is consistent"
  },
  {
    "id": 3815,
    "content": "This linking hazard is most likely to appear when linking with a legacy library that is both built for a different link-compatible architecture and expecting to be passed a WMMA fragment"
  },
  {
    "id": 3816,
    "content": "Note that in the case of weak linkages (for example, a CUDA C++ inline function), the linker may choose any available function definition which may result in implicit passes between compilation units"
  },
  {
    "id": 3817,
    "content": "To avoid these sorts of problems, the matrix should always be stored out to memory for transit through external interfaces (e"
  },
  {
    "id": 3821,
    "content": "Note that since sm_70 can run on sm_75, the above example sm_75 code can be changed to sm_70 and correctly work on sm_75 However, it is recommended to have sm_75 native code in your application when linking with other sm_75 separately compiled binaries"
  },
  {
    "id": 3827,
    "content": "#include using namespace nvcuda ; __global__ void wmma_ker ( half * a , half * b , float * c ) { Declare the fragments wmma :: fragment a_frag ; wmma :: fragment b_frag ; wmma :: fragment c_frag ; Initialize the output to zero wmma :: fill_fragment ( c_frag , 0 0f ); Load the inputs wmma :: load_matrix_sync ( a_frag , a , 16 ); wmma :: load_matrix_sync ( b_frag , b , 16 ); Perform the matrix"
  },
  {
    "id": 3828,
    "content": "multiplication wmma :: mma_sync ( c_frag , a_frag , b_frag , c_frag ); Store the output wmma :: store_matrix_sync ( c , c_frag , 16 , wmma :: mem_row_major ); } 7"
  },
  {
    "id": 3830,
    "content": "Asynchronous Barrier  The NVIDIA C++ standard library introduces a GPU implementation of std::barrier Along with the implementation of std::barrier the library provides extensions that allow users to specify the scope of barrier objects"
  },
  {
    "id": 3832,
    "content": "0 or higher provide hardware acceleration for barrier operations and integration of these barriers with the memcpy_async feature"
  },
  {
    "id": 3840,
    "content": "Simple Synchronization Pattern  Without the arrive/wait barrier, synchronization is achieved using __syncthreads() (to synchronize all threads in a block) or group"
  },
  {
    "id": 3842,
    "content": "#include __global__ void simple_sync ( int iteration_count ) { auto block = cooperative_groups :: this_thread_block (); for ( int i = 0 ; i #include __device__ void compute ( float * data , int curr_iteration ); __global__ void split_arrive_wait ( int iteration_count , float * data ) { using barrier = cuda :: barrier ; __shared__ barrier bar ; auto block = cooperative_groups :: this_thread_block"
  },
  {
    "id": 3843,
    "content": "(); if ( block sync (); for ( int curr_iter = 0 ; curr_iter #include __global__ void init_barrier () { __shared__ cuda :: barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block sync (); } Before any thread can participate in cuda::barrier , the barrier must be initialized using init() with an expected arrival count , block"
  },
  {
    "id": 3845,
    "content": "This poses a bootstrapping challenge in that threads must synchronize before participating in the cuda::barrier , but threads are creating a cuda::barrier in order to synchronize In this example, threads that will participate are part of a cooperative group and use block"
  },
  {
    "id": 3846,
    "content": "sync() to bootstrap initialization In this example a whole thread block is participating in initialization, hence __syncthreads() could also be used"
  },
  {
    "id": 3849,
    "content": ", the number of times bar arrive() will be called by participating threads before a participating thread is unblocked from its call to bar"
  },
  {
    "id": 3851,
    "content": "In the prior example the cuda::barrier is initialized with the number of threads in the thread block i"
  },
  {
    "id": 3854,
    "content": "size() , and all threads within the thread block participate in the barrier A cuda::barrier is flexible in specifying how threads participate (split arrive/wait) and which threads participate"
  },
  {
    "id": 3856,
    "content": "sync() from cooperative groups or __syncthreads() is applicable to whole-thread-block and __syncwarp(mask) is a specified subset of a warp If the intention of the user is to synchronize a full thread block or a full warp we recommend using __syncthreads() and __syncwarp(mask) respectively for performance reasons"
  },
  {
    "id": 3860,
    "content": "A Barrier’s Phase: Arrival, Countdown, Completion, and Reset  A cuda::barrier counts down from the expected arrival count to zero as participating threads call bar"
  },
  {
    "id": 3863,
    "content": "arrive() causes the countdown to reach zero, the countdown is automatically and atomically reset The reset assigns the countdown to the expected arrival count, and moves the cuda::barrier to the next phase"
  },
  {
    "id": 3869,
    "content": ", while the phase associated with the token matches the phase of the cuda::barrier If the phase is advanced (because the countdown reaches zero) before the call to bar wait(std::move(token)) then the thread does not block; if the phase is advanced while the thread is blocked in bar wait(std::move(token)) , the thread is unblocked"
  },
  {
    "id": 3870,
    "content": "It is essential to know when a reset could or could not occur, especially in non-trivial arrive/wait synchronization patterns"
  },
  {
    "id": 3871,
    "content": "A thread’s calls to token=bar arrive() and bar wait(std::move(token)) must be sequenced such that token=bar arrive() occurs during the cuda::barrier ’s current phase, and bar wait(std::move(token)) occurs during the same or next phase After barrier initialization, if a thread’s call to bar arrive() causes the countdown to reach zero then a call to bar wait(std::move(token)) must happen before the"
  },
  {
    "id": 3872,
    "content": "barrier can be reused for a subsequent call to bar arrive() bar wait() must only be called using a token object of the current phase or the immediately preceding phase For simple arrive/wait synchronization patterns, compliance with these usage rules is straightforward"
  },
  {
    "id": 3876,
    "content": "Spatial Partitioning (also known as Warp Specialization)  A thread block can be spatially partitioned such that warps are specialized to perform independent computations Spatial partitioning is used in a producer or consumer pattern, where one subset of threads produces data that is concurrently consumed by the other (disjoint) subset of threads A producer/consumer spatial partitioning pattern"
  },
  {
    "id": 3877,
    "content": "requires two one sided synchronizations to manage a data buffer between the producer and consumer Producer Consumer wait for buffer to be ready to be filled signal buffer is ready to be filled produce data and fill the buffer signal buffer is filled wait for buffer to be filled consume data in filled buffer Producer threads wait for consumer threads to signal that the buffer is ready to be filled;"
  },
  {
    "id": 3878,
    "content": "however, consumer threads do not wait for this signal Consumer threads wait for producer threads to signal that the buffer is filled; however, producer threads do not wait for this signal For full producer/consumer concurrency this pattern has (at least) double buffering where each buffer requires two cuda::barrier s #include #include using barrier = cuda :: barrier ; __device__ void producer ("
  },
  {
    "id": 3879,
    "content": "barrier ready [], barrier filled [], float * buffer , float * in , int N , int buffer_len ) { for ( int i = 0 ; i #include __device__ bool condition_check (); __global__ void early_exit_kernel ( int N ) { using barrier = cuda :: barrier ; __shared__ barrier bar ; auto block = cooperative_groups :: this_thread_block (); if ( block"
  },
  {
    "id": 3880,
    "content": "sync (); for ( int i = 0 ; i is executed once per phase, after the last thread arrives and before any thread is unblocked from the wait Memory operations performed by the threads that arrived at the barrier during the phase are visible to the thread executing the CompletionFunction , and all memory operations performed within the CompletionFunction are visible to all threads waiting at the"
  },
  {
    "id": 3882,
    "content": "#include #include #include namespace cg = cooperative_groups ; __device__ int divergent_compute ( int * , int ); __device__ int independent_computation ( int * , int ); __global__ void psum ( int * data , int n , int * acc ) { auto block = cg :: this_thread_block (); constexpr int BlockSize = 128 ; __shared__ int smem [ BlockSize ]; assert ( BlockSize == block size ()); assert ( n % 128 == 0 );"
  },
  {
    "id": 3883,
    "content": "auto completion_fn = [ & ] { int sum = 0 ; for ( int i = 0 ; i ; __shared__ std :: aligned_storage bar_storage ; Initialize barrier: barrier_t * bar = ( barrier_t * ) & bar_storage ; if ( block size (), completion_fn }; equivalent to: init(bar, block size(), completion_fn); } block"
  },
  {
    "id": 3884,
    "content": "sync ();   Main loop for ( int i = 0 ; i arrive ();   We can do independent computation here bar -> wait ( std :: move ( t ));   shared-memory is safe to re-use in the next iteration   since all threads are done with it, including the one   that did the reduction } } 7"
  },
  {
    "id": 3887,
    "content": "Memory Barrier Primitives Interface  Memory barrier primitives are C-like interfaces to cuda::barrier functionality These primitives are available through including the header"
  },
  {
    "id": 3892,
    "content": "Data Types  typedef /* implementation defined */ __mbarrier_t ; typedef /* implementation defined */ __mbarrier_token_t ; 7"
  },
  {
    "id": 3896,
    "content": "Memory Barrier Primitives API  uint32_t __mbarrier_maximum_count (); void __mbarrier_init ( __mbarrier_t * bar , uint32_t expected_count ); bar must be a pointer to __shared__ memory expected_count __device__ void compute ( int * global_out , int const * shared_in ) { Computes using all values of current batch from shared memory } __global__ void without_memcpy_async ( int * global_out , int"
  },
  {
    "id": 3897,
    "content": "const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid size ()); Exposition: input size fits batch_sz * grid_size extern __shared__ int shared []; block size() * sizeof(int) bytes size_t local_idx = block thread_rank (); for ( size_t batch = 0 ; batch #include"
  },
  {
    "id": 3898,
    "content": "__device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_memcpy_async ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid size ()); Exposition: input size fits batch_sz * grid_size extern __shared__"
  },
  {
    "id": 3899,
    "content": "int shared []; block size() * sizeof(int) bytes for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_barrier ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz *"
  },
  {
    "id": 3900,
    "content": "grid size ()); Assume input size fits batch_sz * grid_size extern __shared__ int shared []; block size() * sizeof(int) bytes Create a synchronization object (C++20 barrier) __shared__ cuda :: barrier barrier ; if ( block sync (); for ( size_t batch = 0 ; batch (size_t size) Shape can be used to supply a proof that both pointers passed to memcpy_async are aligned to an Align alignment boundary and"
  },
  {
    "id": 3901,
    "content": "that size is a multiple of Align , by passing it as an argument where the memcpy_async APIs expect a Shape : cuda :: memcpy_async ( group , dst , src , cuda :: aligned_size_t ( N * block size ()), pipeline ); If the proof is incorrect, the behavior is undefined"
  },
  {
    "id": 3909,
    "content": "If the pointer types passed to memcpy_async do not point to TriviallyCopyable types, the copy constructor of each output element needs to be invoked, and these instructions cannot be used to accelerate memcpy_async"
  },
  {
    "id": 3914,
    "content": "Warp Entanglement - Commit  The sequence of memcpy_async batches is shared across the warp The commit operation is coalesced such that the sequence is incremented once for all converged threads that invoke the commit operation If the warp is fully converged, the sequence is incremented by one; if the warp is fully diverged, the sequence is incremented by 32 PB = {BP0, BP1, BP2, …, BPL} Let TB be"
  },
  {
    "id": 3915,
    "content": "a thread’s perceived sequence of batches, as if the sequence were only incremented by this thread’s invocation of the commit operation TB = {BT0, BT1, BT2, …, BTL} The pipeline::producer_commit() return value is from the thread’s perceived batch sequence An index in a thread’s perceived sequence always aligns to an equal or larger index in the actual warp-shared sequence The sequences are equal"
  },
  {
    "id": 3916,
    "content": "only when all commit operations are invoked from converged threads BTn ≡ BPm where n () or pipeline::consumer_wait() to wait for batches in the perceived sequence TB to complete Note that pipeline::consumer_wait() is equivalent to pipeline_consumer_wait_prior() , where N = PL The pipeline_consumer_wait_prior() function waits for batches in the actual sequence at least up to and including PL-N"
  },
  {
    "id": 3917,
    "content": "Since TL #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_single_stage ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid size ()); Assume input size fits batch_sz * grid_size"
  },
  {
    "id": 3918,
    "content": "constexpr size_t stages_count = 1 ; Pipeline with one stage One batch must fit in shared memory: extern __shared__ int shared []; block size() * sizeof(int) bytes Allocate shared storage for a single stage cuda::pipeline: __shared__ cuda :: pipeline_shared_state shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state ); Each thread processes `batch_sz` elements Compute offset"
  },
  {
    "id": 3919,
    "content": "of the batch `batch` of this thread block in global memory: auto block_batch = [ & ]( size_t batch ) -> int { return block size () * batch ; }; for ( size_t batch = 0 ; batch #include __device__ void compute ( int * global_out , int const * shared_in ); __global__ void with_staging ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups ::"
  },
  {
    "id": 3920,
    "content": "this_grid (); auto block = cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid size ()); Assume input size fits batch_sz * grid_size constexpr size_t stages_count = 2 ; Pipeline with two stages Two batches must fit in shared memory: extern __shared__ int shared []; stages_count * block size() * sizeof(int) bytes size_t shared_offset [ stages_count ] = { 0 , block size ()"
  },
  {
    "id": 3921,
    "content": "}; Offsets to each batch Allocate shared storage for a two-stage cuda::pipeline: __shared__ cuda :: pipeline_shared_state shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state ); Each thread processes `batch_sz` elements size () * batch ; }; Initialize first pipeline stage by submitting a `memcpy_async` to fetch a whole batch for the block: if ( batch_sz == 0 ) return ;"
  },
  {
    "id": 3922,
    "content": "pipeline producer_acquire (); cuda :: memcpy_async ( block , shared + shared_offset [ 0 ], global_in + block_batch ( 0 ), sizeof ( int ) * block producer_commit (); Pipelined copy/compute: for ( size_t batch = 1 ; batch encapsulates the finite resources that allow a pipeline to process up to count concurrent stages If all resources are in use, pipeline producer_acquire() blocks producer threads"
  },
  {
    "id": 3923,
    "content": "until the resources of the next pipeline stage are released by consumer threads This example can be written in a more concise manner by merging the prolog and epilog of the loop with the loop itself as follows: template __global__ void with_staging_unified ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid = cooperative_groups :: this_grid (); auto block ="
  },
  {
    "id": 3924,
    "content": "cooperative_groups :: this_thread_block (); assert ( size == batch_sz * grid size ()); Assume input size fits batch_sz * grid_size extern __shared__ int shared []; stages_count * block size() * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s shared_state ; auto pipeline = cuda :: make_pipeline ( block , & shared_state ); auto block_batch = [ & ]( size_t batch ) -> int"
  },
  {
    "id": 3925,
    "content": "{ return block size () * batch ; }; compute_batch: next batch to process fetch_batch: next batch to fetch from global memory for ( size_t compute_batch = 0 , fetch_batch = 0 ; compute_batch primitive used above is very flexible, and supports two features that our examples above are not using: any arbitrary subset of threads in the block can participate in the pipeline , and from the threads that"
  },
  {
    "id": 3926,
    "content": "participate, any subsets can be producers, consumers, or both In the following example, threads with an “even” thread rank are producers, while other threads are consumers: __device__ void compute ( int * global_out , int shared_in ); template __global__ void with_specialized_staging_unified ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid ="
  },
  {
    "id": 3927,
    "content": "cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); In this example, threads with \"even\" thread rank are producers, while threads with \"odd\" thread rank are consumers: const cuda :: pipeline_role thread_role = block"
  },
  {
    "id": 3929,
    "content": "cuda :: pipeline_role :: producer : cuda :: pipeline_role :: consumer ;   Each thread block only has half of its threads as producers: auto producer_threads = block"
  },
  {
    "id": 3931,
    "content": "thread_rank () / 2 ; auto elements_per_batch = size / batch_sz ; auto elements_per_batch_per_block = elements_per_batch / grid"
  },
  {
    "id": 3932,
    "content": "x ; extern __shared__ int shared []; stages_count * elements_per_batch_per_block * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s shared_state ; cuda :: pipeline pipeline = cuda :: make_pipeline ( block , & shared_state , thread_role ); Each thread block processes `batch_sz` batches Compute offset of the batch `batch` of this thread block in global memory: auto"
  },
  {
    "id": 3933,
    "content": "block_batch = [ & ]( size_t batch ) -> int { return elements_per_batch * batch + elements_per_batch_per_block * blockIdx x ; }; for ( size_t compute_batch = 0 , fetch_batch = 0 ; compute_batch by using a pipeline combined with __syncthreads() : template __global__ void with_staging_scope_thread ( int * global_out , int const * global_in , size_t size , size_t batch_sz ) { auto grid ="
  },
  {
    "id": 3934,
    "content": "cooperative_groups :: this_grid (); auto block = cooperative_groups :: this_thread_block (); auto thread = cooperative_groups :: this_thread (); assert ( size == batch_sz * grid size ()); Assume input size fits batch_sz * grid_size extern __shared__ int shared []; stages_count * block size() * sizeof(int) bytes size_t shared_offset [ stages_count ]; for ( int s = 0 ; s pipeline = cuda ::"
  },
  {
    "id": 3936,
    "content": "For a C-like interface, when compiling without ISO C++ 2011 compatibility, see Pipeline Primitives Interface"
  },
  {
    "id": 3940,
    "content": "Pipeline Primitives Interface  Pipeline primitives are a C-like interface for memcpy_async functionality"
  },
  {
    "id": 3946,
    "content": "memcpy_async Primitive  void __pipeline_memcpy_async ( void * __restrict__ dst_shared , const void * __restrict__ src_global , size_t size_and_align , size_t zfill = 0 ); Request that the following operation be submitted for asynchronous evaluation: size_t i = 0 ; for (; i #include using barrier = cuda :: barrier ; namespace ptx = cuda :: ptx ; static constexpr size_t buf_len = 1024 ; __global__"
  },
  {
    "id": 3947,
    "content": "void add_one_kernel ( int * data , size_t offset ) { Shared memory buffer a) Initialize shared memory barrier with the number of threads participating in the barrier #pragma nv_diag_suppress static_var_with_dynamic_init __shared__ barrier bar ; if ( threadIdx"
  },
  {
    "id": 3948,
    "content": "cuda::memcpy_async arrives on the barrier and communicates   how many bytes are expected to come in (the transaction count) cuda :: memcpy_async ( smem_data , data + offset , cuda :: aligned_size_t ( sizeof ( smem_data )), bar ); }   3b"
  },
  {
    "id": 3949,
    "content": "Shared memory barriers are described in more detail in Asynchronous Data Copies using cuda::barrier To make the initialized barrier visible to subsequent bulk-asynchronous copies, the fence"
  },
  {
    "id": 3953,
    "content": "This instruction ensures that subsequent bulk-asynchronous copy operations operate on the initialized barrier The bulk-asynchronous copy instruction directs the hardware to copy a large chunk of data into shared memory, and to update the transaction count of the shared memory barrier after completing the read"
  },
  {
    "id": 3954,
    "content": "In general, issuing as few bulk copies with as big a size as possible results in the best performance"
  },
  {
    "id": 3955,
    "content": "Because the copy can be performed asynchronously by the hardware, it is not necessary to split the copy into smaller chunks The thread that initiates the bulk-asynchronous copy operation arrives at the barrier using mbarrier"
  },
  {
    "id": 3957,
    "content": "This tells the barrier that the thread has arrived and also how many bytes (tx / transactions) are expected to arrive"
  },
  {
    "id": 3958,
    "content": "If multiple threads update the transaction count, the expected transaction will be the sum of the updates"
  },
  {
    "id": 3959,
    "content": "Once the barrier has flipped, the bytes are safe to read from shared memory, both by the threads as well as by subsequent bulk-asynchronous copies"
  },
  {
    "id": 3960,
    "content": "It can either return true, indicating that the wait is over, or return false, which may mean that the wait timed out"
  },
  {
    "id": 3965,
    "content": "This orders the writes to shared memory before subsequent reads from bulk-asynchronous copy operations, which read through the async proxy So each thread first orders the writes to objects in shared memory in the async proxy via the fence proxy async shared::cta , and these operations by all threads are ordered before the async operation performed in thread 0 using __syncthreads() Afterwards, the"
  },
  {
    "id": 3966,
    "content": "thread can wait for all operations in this group to have completed reading from shared memory (as in the code above) or to have completed writing to global memory, making the writes visible to the initiating thread Note that the bulk-asynchronous and non-bulk asynchronous copy instructions have different async-groups: there exist both cp async wait_group and cp async bulk wait_group instructions"
  },
  {
    "id": 3967,
    "content": "The bulk-asynchronous instructions have specific alignment requirements on their source and destination addresses Table 7 Alignment requirements for one-dimensional bulk-asynchronous operations in Compute Capability 9"
  },
  {
    "id": 3974,
    "content": "Using TMA to transfer multi-dimensional arrays  The primary difference between the one-dimensional and multi-dimensional case is that a tensor map must be created on the host and passed to the CUDA kernel This section describes how to create a tensor map using the CUDA driver API, how to pass it to device, and how to use it on device This API can be accessed by linking to the driver directly ("
  },
  {
    "id": 3976,
    "content": "Among them are the base pointer to an array in global memory, the size of the array (in number of elements), the stride from one row to the next (in bytes), the size of the shared memory buffer (in number of elements) The code below creates a tensor map to describe a two-dimensional row-major array of size GMEM_HEIGHT x GMEM_WIDTH constexpr uint32_t rank = 2 ; uint64_t size [ rank ] = {"
  },
  {
    "id": 3977,
    "content": "GMEM_WIDTH , GMEM_HEIGHT }; The stride is the number of bytes to traverse from the first element of one row to the next uint64_t stride [ rank - 1 ] = { GMEM_WIDTH * sizeof ( int )}; The box_size is the size of the shared memory buffer that is used as the destination of a TMA transfer uint32_t box_size [ rank ] = { SMEM_WIDTH , SMEM_HEIGHT }; The distance between elements in units of"
  },
  {
    "id": 3978,
    "content": "sizeof(element) A stride of 2 can be used to load only the real component of a complex-valued tensor, for instance uint32_t elem_stride [ rank ] = { 1 , 1 }; Get a function pointer to the cuTensorMapEncodeTiled driver API auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled (); Create the tensor descriptor CUresult res = cuTensorMapEncodeTiled ( & tensor_map , CUtensorMap *tensorMap,"
  },
  {
    "id": 3979,
    "content": "CUtensorMapDataType :: CU_TENSOR_MAP_DATA_TYPE_INT32 , rank , cuuint32_t tensorRank, tensor_ptr , void *globalAddress, size , const cuuint64_t *globalDim, stride , const cuuint64_t *globalStrides, box_size , const cuuint32_t *boxDim, elem_stride , const cuuint32_t *elementStrides, Interleave patterns can be used to accelerate loading of values that are less than 4 bytes long"
  },
  {
    "id": 3980,
    "content": "CUtensorMapInterleave :: CU_TENSOR_MAP_INTERLEAVE_NONE ,   Swizzling can be used to avoid shared memory bank conflicts"
  },
  {
    "id": 3981,
    "content": "CUtensorMapSwizzle :: CU_TENSOR_MAP_SWIZZLE_NONE ,   L2 Promotion can be used to widen the effect of a cache-policy to a wider   set of L2 cache lines"
  },
  {
    "id": 3982,
    "content": "CUtensorMapL2promotion :: CU_TENSOR_MAP_L2_PROMOTION_NONE ,   Any element that is outside of bounds will be set to zero by the TMA transfer"
  },
  {
    "id": 3983,
    "content": "This can be achieved by using constant memory or by passing the tensor map as a const __grid_constant__ parameter to a kernel When passing the tensor map as a parameter, some versions of the GCC C++ compiler issue the warning “the ABI for passing parameters with 64-byte alignment has changed in GCC 4"
  },
  {
    "id": 3985,
    "content": "__global__ void kernel ( const __grid_constant__ CUtensorMap tensor_map ) { Use tensor_map here ] kernel >> ( map ); } As an alternative to the __grid_constant__ kernel parameter, a global constant variable can be used __constant__ CUtensorMap global_tensor_map ; __global__ void kernel () { Use global_tensor_map here ] cudaMemcpyToSymbol ( global_tensor_map , & local_tensor_map , sizeof ("
  },
  {
    "id": 3986,
    "content": "CUtensorMap )); kernel >> (); } The following example copies the tensor map to global device memory Using a pointer to a tensor map in global device memory is undefined behavior and will lead to silent and difficult to track down bugs __device__ CUtensorMap global_tensor_map ; __global__ void kernel ( CUtensorMap * tensor_map ) { Do *not* use tensor_map here Using a global memory pointer is"
  },
  {
    "id": 3987,
    "content": "undefined behavior and can fail silently and unreliably ] cudaMemcpy ( global_tensor_map , & local_tensor_map , sizeof ( CUtensorMap )); kernel >> ( global_tensor_map ); } Use #include CUtensormap #include using barrier = cuda :: barrier ; namespace cde = cuda :: device :: experimental ; __global__ void kernel ( const __grid_constant__ CUtensorMap tensor_map , int x , int y ) { The destination"
  },
  {
    "id": 3988,
    "content": "shared memory buffer of a bulk tensor operation should be 128 byte aligned __shared__ alignas ( 128 ) int smem_buffer [ SMEM_HEIGHT ][ SMEM_WIDTH ]; Initialize shared memory barrier with the number of threads participating in the barrier cde :: fence_proxy_async_shared_cta (); } Syncthreads so initialized barrier is visible to all threads cde :: cp_async_bulk_tensor_2d_global_to_shared ( &"
  },
  {
    "id": 3989,
    "content": "smem_buffer , & tensor_map , x , y , bar ); Arrive on the barrier and tell how many bytes are expected to come in token = cuda :: device :: barrier_arrive_tx ( bar , 1 , sizeof ( smem_buffer )); } else { Other threads just arrive cde :: fence_proxy_async_shared_cta (); __syncthreads (); After syncthreads, writes by all threads are visible to TMA engine x == 0 ) { cde ::"
  },
  {
    "id": 3990,
    "content": "cp_async_bulk_tensor_2d_shared_to_global ( & tensor_map , x , y , & smem_buffer ); Wait for TMA transfer to have finished reading shared memory cde :: cp_async_bulk_commit_group (); Wait for the group to have completed reading from shared memory If further computations were to take place in the kernel, this allows the memory location of the shared memory barrier to be reused When part of the tile"
  },
  {
    "id": 3991,
    "content": "that is being read from global to shared memory is out of bounds, the shared memory that corresponds to the out of bounds area is zero-filled When writing from shared to global memory, parts of the tile may be out of bounds, but the top left corner cannot have any negative indices"
  },
  {
    "id": 3992,
    "content": "Due to alignment requirements, a 4 x 3 row-major matrix of integers must have strides of 4 and 16 bytes as well Each row is padded with 4 extra bytes to ensure that the start of the next row is aligned to 16 bytes For more information regarding alignment, refer to Table Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9"
  },
  {
    "id": 3994,
    "content": "Table 8 Alignment requirements for multi-dimensional bulk tensor asynchronous copy operations in Compute Capability 9"
  },
  {
    "id": 4001,
    "content": "Multi-dimensional TMA PTX wrappers  Below, the PTX instructions are ordered by their use in the example code above"
  },
  {
    "id": 4004,
    "content": "bulk tensor instructions initiate a bulk tensor asynchronous copy between global and shared memory The wrappers below read from global to shared memory and write from shared to global memory"
  },
  {
    "id": 4005,
    "content": "Profiler Counter Function  Each multiprocessor has a set of sixteen hardware counters that an application can increment with a single instruction by calling the __prof_trigger() function void __prof_trigger ( int counter ); increments by one per warp the per-multiprocessor hardware counter of index counter The value of counters 0, 1, …, 7 can be obtained via nvprof by nvprof --events"
  },
  {
    "id": 4006,
    "content": "prof_trigger_0x where x is 0, 1, …, 7 All counters are reset before each kernel launch (note that when collecting counters, kernel launches are synchronous as mentioned in Concurrent Execution between Host and Device )"
  },
  {
    "id": 4012,
    "content": "If the program is run within a debugger, this triggers a breakpoint and the debugger can be used to inspect the current state of the device"
  },
  {
    "id": 4013,
    "content": "Otherwise, each thread for which expression is equal to zero prints a message to stderr after synchronization with the host via cudaDeviceSynchronize() , cudaStreamSynchronize() , or cudaEventSynchronize()"
  },
  {
    "id": 4019,
    "content": "No more commands can be sent to this device until cudaDeviceReset() is called to reinitialize the device"
  },
  {
    "id": 4021,
    "content": "cu #include __global__ void testAssert ( void ) { int is_one = 1 ; int should_be_one = 0 ; This will have no effect assert ( is_one ); This will halt kernel execution assert ( should_be_one ); } int main ( int argc , char * argv []) { testAssert >> (); cudaDeviceSynchronize (); return 0 ; } will output: test cu:19: void testAssert(): block: [0,0,0], thread: [0,0,0] Assertion `should_be_one`"
  },
  {
    "id": 4024,
    "content": "They can be disabled at compile time by defining the NDEBUG preprocessor macro before including assert"
  },
  {
    "id": 4026,
    "content": "Note that expression should not be an expression with side effects (something like (++i > 0) , for example), otherwise disabling the assertion will affect the functionality of the code"
  },
  {
    "id": 4029,
    "content": "Trap function  A trap operation can be initiated by calling the __trap() function from any device thread void __trap (); The execution of the kernel is aborted and an interrupt is raised in the host program"
  },
  {
    "id": 4032,
    "content": "Breakpoint Function  Execution of a kernel function can be suspended by calling the __brkpt() function from any device thread"
  },
  {
    "id": 4037,
    "content": "The in-kernel printf() function behaves in a similar way to the standard C-library printf() function, and the user is referred to the host system’s manual pages for a complete description of printf() behavior"
  },
  {
    "id": 4038,
    "content": "In essence, the string passed in as format is output to a stream on the host, with substitutions made from the argument list wherever a format specifier is encountered"
  },
  {
    "id": 4039,
    "content": "The printf() command is executed as any other device-side function: per-thread, and in the context of the calling thread From a multi-threaded kernel, this means that a straightforward call to printf() will be executed by every thread, using that thread’s data as specified Multiple versions of the output string will then appear at the host stream, once for each thread which encountered the"
  },
  {
    "id": 4040,
    "content": "printf() It is up to the programmer to limit the output to a single thread if only a single output string is desired (see Examples for an illustrative example) Unlike the C-standard printf() , which returns the number of characters printed, CUDA’s printf() returns the number of arguments parsed"
  },
  {
    "id": 4045,
    "content": "Format Specifiers  As for standard printf() , format specifiers take the form: %[flags][width][ precision][size]type The following fields are supported (see widely-available documentation for a complete description of all behaviors): Flags: '#' ' ' '0' '+' '-' Width: '*' '0-9' Precision: '0-9' Size: 'h' 'l' 'll' Type: \"%cdiouxXpeEfgGaAs\" Note that CUDA’s printf() will accept any combination of"
  },
  {
    "id": 4047,
    "content": "In other words, “ %hd ” will be accepted and printf will expect a double-precision variable in the corresponding location in the argument list"
  },
  {
    "id": 4052,
    "content": "This means that the format string must be understood by the host-system’s compiler and C library Every effort has been made to ensure that the format specifiers supported by CUDA’s printf function form a universal subset from the most common host compilers, but exact behavior will be host-OS-dependent As described in Format Specifiers , printf() will accept all combinations of valid flags and"
  },
  {
    "id": 4053,
    "content": "types This is because it cannot determine what will and will not be valid on the host system where the final output is formatted The effect of this is that output may be undefined if the program emits a format string which contains invalid combinations"
  },
  {
    "id": 4054,
    "content": "Owing to the differing size of the long type on 64-bit Windows platforms (four bytes on 64-bit Windows platforms, eight bytes on other 64-bit platforms), a kernel which is compiled on a non-Windows 64-bit machine but then run on a win64 machine will see corrupted output for all format strings which include “ %ld ”"
  },
  {
    "id": 4056,
    "content": "The output buffer for printf() is set to a fixed size before kernel launch (see Associated Host-Side API ) It is circular and if more output is produced during kernel execution than can fit in the buffer, older output is overwritten"
  },
  {
    "id": 4057,
    "content": "It is flushed only when one of these actions is performed: Kernel launch via >> or cuLaunchKernel() (at the start of the launch, and if the CUDA_LAUNCH_BLOCKING environment variable is set to 1, at the end of the launch as well), Synchronization via cudaDeviceSynchronize() , cuCtxSynchronize() , cudaStreamSynchronize() , cuStreamSynchronize() , cudaEventSynchronize() , or cuEventSynchronize() ,"
  },
  {
    "id": 4058,
    "content": "Memory copies via any blocking version of cudaMemcpy*() or cuMemcpy*() , Module loading/unloading via cuModuleLoad() or cuModuleUnload() , Context destruction via cudaDeviceReset() or cuCtxDestroy()"
  },
  {
    "id": 4061,
    "content": "Internally printf() uses a shared data structure and so it is possible that calling printf() might change the order of execution of threads In particular, a thread which calls printf() might take a longer execution path than one which does not call printf() , and that path length is dependent upon the parameters of the printf() Note, however, that CUDA makes no guarantees of thread execution"
  },
  {
    "id": 4062,
    "content": "order except at explicit __syncthreads() barriers, so it is impossible to tell whether execution order has been modified by printf() or by other scheduling behavior in the hardware"
  },
  {
    "id": 4066,
    "content": "Associated Host-Side API  The following API functions get and set the size of the buffer used to transfer the printf() arguments and internal metadata to the host (default is 1 megabyte): cudaDeviceGetLimit(size_t* size,cudaLimitPrintfFifoSize) cudaDeviceSetLimit(cudaLimitPrintfFifoSize, size_t size) 7"
  },
  {
    "id": 4069,
    "content": "Examples  The following code sample: #include __global__ void helloCUDA ( float f ) { printf ( \"Hello thread %d, f=%f \" , threadIdx x , f ); } int main () { helloCUDA >> ( 1 2345f ); cudaDeviceSynchronize (); return 0 ; } will output: Hello thread 2, f=1 2345 Hello thread 1, f=1 2345 Hello thread 4, f=1 2345 Hello thread 0, f=1 2345 Hello thread 3, f=1 2345 Notice how each thread encounters the"
  },
  {
    "id": 4077,
    "content": "The following code sample: #include __global__ void helloCUDA ( float f ) { if ( threadIdx x , f ) ; } int main () { helloCUDA >> ( 1"
  },
  {
    "id": 4079,
    "content": "2345 Self-evidently, the if() statement limits which threads will call printf , so that only a single line of output is seen"
  },
  {
    "id": 4082,
    "content": "Dynamic Global Memory Allocation and Operations  Dynamic global memory allocation and operations are only supported by devices of compute capability 2"
  },
  {
    "id": 4084,
    "content": "__host__ __device__ void * malloc ( size_t size ); __device__ void * __nv_aligned_device_malloc ( size_t size , size_t align ); __host__ __device__ void free ( void * ptr ); allocate and free memory dynamically from a fixed-size heap in global memory __host__ __device__ void * memcpy ( void * dest , const void * src , size_t size ); copy size bytes from the memory location pointed by src to the"
  },
  {
    "id": 4085,
    "content": "memory location pointed by dest __host__ __device__ void * memset ( void * ptr , int value , size_t size ); set size bytes of memory block pointed by ptr to value (interpreted as an unsigned char) The CUDA in-kernel malloc() function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the request The CUDA"
  },
  {
    "id": 4086,
    "content": "in-kernel __nv_aligned_device_malloc() function allocates at least size bytes from the device heap and returns a pointer to the allocated memory or NULL if insufficient memory exists to fulfill the requested size or alignment The CUDA in-kernel free() function deallocates the memory pointed to by ptr , which must have been returned by a previous call to malloc() or __nv_aligned_device_malloc() The"
  },
  {
    "id": 4087,
    "content": "memory allocated by a given CUDA thread via malloc() or __nv_aligned_device_malloc() remains allocated for the lifetime of the CUDA context, or until it is explicitly released by a call to free() Any CUDA thread may free memory allocated by another thread, but care should be taken to ensure that the same pointer is not freed more than once"
  },
  {
    "id": 4091,
    "content": "Heap Memory Allocation  The device memory heap has a fixed size that must be specified before any program using malloc() , __nv_aligned_device_malloc() or free() is loaded into the context A default heap of eight megabytes is allocated if any program uses malloc() or __nv_aligned_device_malloc() without explicitly specifying the heap size The following API functions get and set the heap size:"
  },
  {
    "id": 4092,
    "content": "cudaDeviceGetLimit(size_t* size, cudaLimitMallocHeapSize) cudaDeviceSetLimit(cudaLimitMallocHeapSize, size_t size) The heap size granted will be at least size bytes The actual memory allocation for the heap occurs when a module is loaded into the context, either explicitly via the CUDA driver API (see Module ), or implicitly via the CUDA runtime API (see CUDA Runtime ) If the memory allocation"
  },
  {
    "id": 4093,
    "content": "fails, the module load will generate a CUDA_ERROR_SHARED_OBJECT_INIT_FAILED error Heap size cannot be changed once a module load has occurred and it does not resize dynamically according to need Memory reserved for the device heap is in addition to memory allocated through host-side CUDA API calls such as cudaMalloc()"
  },
  {
    "id": 4097,
    "content": "Interoperability with Host Memory API  Memory allocated via device malloc() or __nv_aligned_device_malloc() cannot be freed using the runtime (i"
  },
  {
    "id": 4102,
    "content": ", by calling any of the memory allocation functions from Device Memory ) cannot be freed via free() In addition, memory allocated by a call to malloc() or __nv_aligned_device_malloc() in device code cannot be used in any runtime or driver API calls (i"
  },
  {
    "id": 4104,
    "content": "Per Thread Allocation  The following code sample: #include #include __global__ void mallocTest () { size_t size = 123 ; char * ptr = ( char * ) malloc ( size ); memset ( ptr , 0 , size ); printf ( \"Thread %d got pointer: %p \" , threadIdx cudaDeviceSetLimit ( cudaLimitMallocHeapSize , 128 * 1024 * 1024 ); mallocTest >> (); cudaDeviceSynchronize (); return 0 ; } will output: Thread 0 got pointer :"
  },
  {
    "id": 4105,
    "content": "00057020 Thread 1 got pointer : 000570 8 c Thread 2 got pointer : 000570f 8 Thread 3 got pointer : 00057164 Thread 4 got pointer : 000571 d0 Notice how each thread encounters the malloc() and memset() commands and so receives and initializes its own allocation Per Thread Block Allocation  #include __global__ void mallocTest () { __shared__ int * data ; The first thread in the block does the"
  },
  {
    "id": 4106,
    "content": "allocation and then shares the pointer with all other threads through shared memory, so that access can easily be coalesced x * 64 ; data = ( int * ) malloc ( size ); } __syncthreads (); Check for failure if ( data == NULL ) return ; Threads index into the memory, ensuring coalescence int * ptr = data ; for ( int i = 0 ; i >> (); cudaDeviceSynchronize (); return 0 ; } 7"
  },
  {
    "id": 4110,
    "content": "Allocation Persisting Between Kernel Launches  #include #include #define NUM_BLOCKS 20 __device__ int * dataptr [ NUM_BLOCKS ]; Per-block pointer __global__ void allocmem () { Only the first thread in the block does the allocation since we want only one allocation per block x ] = 0 ; } Simple example: store thread ID into each element __global__ void usemem () { int * ptr = dataptr [ blockIdx x"
  },
  {
    "id": 4111,
    "content": "; } Print the content of the buffer before freeing it __global__ void freemem () { int * ptr = dataptr [ blockIdx x ]); Only free from one thread x == 0 ) free ( ptr ); } int main () { cudaDeviceSetLimit ( cudaLimitMallocHeapSize , 128 * 1024 * 1024 ); Allocate memory allocmem >> (); Use memory usemem >> (); usemem >> (); usemem >> (); Free memory freemem >> (); cudaDeviceSynchronize (); return 0"
  },
  {
    "id": 4114,
    "content": "Execution Configuration  Any call to a __global__ function must specify the execution configuration for that call The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams) As an example, a function declared as __global__ void Func ( float *"
  },
  {
    "id": 4115,
    "content": "parameter ); must be called like this: Func >> ( parameter ); The arguments to the execution configuration are evaluated before the actual function arguments The function call will fail if Dg or Db are greater than the maximum sizes allowed for the device as specified in Compute Capabilities , or if Ns is greater than the maximum amount of shared memory available on the device, minus the amount of"
  },
  {
    "id": 4118,
    "content": "0 and above allows users to specify compile time thread block cluster dimensions, so that the kernel can use the cluster hierarchy in CUDA The example below shows compile time cluster size of 2 in X dimension and 1 in Y and Z dimension __global__ void __cluster_dims__ ( 2 , 1 , 1 ) Func ( float * parameter ); Thread block cluster dimensions can also be specified at runtime and kernel with the"
  },
  {
    "id": 4120,
    "content": "The API takes a configuration arugument of type cudaLaunchConfig_t , kernel function pointer and kernel arguments __global__ void Func ( float * parameter ); Kernel invocation with runtime cluster size { cudaLaunchConfig_t config = { 0 }; The grid dimension is not affected by cluster launch, and is still enumerated using number of blocks numAttrs = 1 ; float * parameter ; cudaLaunchKernelEx ( &"
  },
  {
    "id": 4123,
    "content": "Launch Bounds  As discussed in detail in Multiprocessor Level , the fewer registers a kernel uses, the more threads and thread blocks are likely to reside on a multiprocessor, which can improve performance"
  },
  {
    "id": 4124,
    "content": "Therefore, the compiler uses heuristics to minimize register usage while keeping register spilling (see Device Memory Accesses ) and instruction count to a minimum"
  },
  {
    "id": 4125,
    "content": "An application can optionally aid these heuristics by providing additional information to the compiler in the form of launch bounds that are specified using the __launch_bounds__() qualifier in the definition of a __global__ function: __global__ void __launch_bounds__ ( maxThreadsPerBlock , minBlocksPerMultiprocessor , maxBlocksPerCluster ) MyKernel ("
  },
  {
    "id": 4127,
    "content": "} maxThreadsPerBlock specifies the maximum number of threads per block with which the application will ever launch MyKernel() ; it compiles to the"
  },
  {
    "id": 4129,
    "content": "minBlocksPerMultiprocessor is optional and specifies the desired minimum number of resident blocks per multiprocessor; it compiles to the"
  },
  {
    "id": 4131,
    "content": "maxBlocksPerCluster is optional and specifies the desired maximum number thread blocks per cluster with which the application will ever launch MyKernel() ; it compiles to the"
  },
  {
    "id": 4133,
    "content": "If launch bounds are specified, the compiler first derives from them the upper limit L on the number of registers the kernel should use to ensure that minBlocksPerMultiprocessor blocks (or a single block if minBlocksPerMultiprocessor is not specified) of maxThreadsPerBlock threads can reside on the multiprocessor (see Hardware Multithreading for the relationship between the number of registers"
  },
  {
    "id": 4134,
    "content": "used by a kernel and the number of registers allocated per block) A kernel will fail to launch if it is executed with more threads per block than its launch bound maxThreadsPerBlock A kernel will fail to launch if it is executed with more thread blocks per cluster than its launch bound maxBlocksPerCluster Per thread resources required by a CUDA kernel might limit the maximum block size in an"
  },
  {
    "id": 4135,
    "content": "unwanted way In order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an SM, developers should include the single argument __launch_bounds__(maxThreadsPerBlock) which specifies the largest block size that the kernel will be launched with Providing the two argument version of"
  },
  {
    "id": 4136,
    "content": "__launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor) can improve performance in some cases The right value for minBlocksPerMultiprocessor should be determined using a detailed per kernel analysis Optimal launch bounds for a given kernel will usually differ across major architecture revisions"
  },
  {
    "id": 4137,
    "content": "The sample code below shows how this is typically handled in device code using the __CUDA_ARCH__ macro introduced in Application Compatibility #define THREADS_PER_BLOCK 256 #if __CUDA_ARCH__ >= 200 #define MY_KERNEL_MAX_THREADS (2 * THREADS_PER_BLOCK) #define MY_KERNEL_MIN_BLOCKS 3 #else #define MY_KERNEL_MAX_THREADS THREADS_PER_BLOCK #define MY_KERNEL_MIN_BLOCKS 2 #endif Device code __global__"
  },
  {
    "id": 4140,
    "content": "} In the common case where MyKernel is invoked with the maximum number of threads per block (specified as the first parameter of __launch_bounds__() ), it is tempting to use MY_KERNEL_MAX_THREADS as the number of threads per block in the execution configuration: Host code MyKernel >> ( ); This will not work however since __CUDA_ARCH__ is undefined in host code as mentioned in Application"
  },
  {
    "id": 4141,
    "content": "Compatibility , so MyKernel will launch with 256 threads per block even when __CUDA_ARCH__ is greater or equal to 200 Instead the number of threads per block should be determined: Either at compile time using a macro that does not depend on __CUDA_ARCH__ , for example Host code MyKernel >> ("
  },
  {
    "id": 4142,
    "content": "); Or at runtime based on the compute capability   Host code cudaGetDeviceProperties ( & deviceProp , device ); int threadsPerBlock = ( deviceProp"
  },
  {
    "id": 4145,
    "content": "); Register usage is reported by the --ptxas-options=-v compiler option The number of resident blocks can be derived from the occupancy reported by the CUDA profiler (see Device Memory Accesses for a definition of occupancy)"
  },
  {
    "id": 4147,
    "content": "Register usage can also be controlled for all __global__ functions in a file using the maxrregcount compiler option The value of maxrregcount is ignored for functions with launch bounds"
  },
  {
    "id": 4150,
    "content": "Maximum Number of Registers per Thread  To provide a mechanism for low-level performance tuning, CUDA C++ provides the __maxnreg()__ function qualifier to pass performance tuning information to the backend optimizing compiler The __maxnreg__() qualifier specifies the maximum number of registers to be allocated to a single thread in a thread block In the definition of a __global__ function:"
  },
  {
    "id": 4153,
    "content": "} maxNumberRegistersPerThread specifies the maximum number of registers to be allocated to a single thread in a thread block of the kernel MyKernel() ; it compiles to the"
  },
  {
    "id": 4159,
    "content": "The pragma will be ignored if the ICE evaluates to a non-positive integer or to an integer greater than the maximum value representable by the int data type Examples: struct S1_t { static const int value = 4 ; }; template __device__ void foo ( int * p1 , int * p2 ) {   no argument specified, loop will be completely unrolled #pragma unroll for ( int i = 0 ; i ( p1 , p2 ); } 7"
  },
  {
    "id": 4161,
    "content": "SIMD Video Instructions  PTX ISA version 3 0 includes SIMD (Single Instruction, Multiple Data) video instructions which operate on pairs of 16-bit values and quads of 8-bit values The SIMD video instructions are: vadd2, vadd4 vsub2, vsub4 vavrg2, vavrg4 vabsdiff2, vabsdiff4 vmin2, vmin4 vmax2, vmax4 vset2, vset4 PTX instructions, such as the SIMD video instructions, can be included in CUDA"
  },
  {
    "id": 4162,
    "content": "programs by way of the assembler, asm() , statement The basic syntax of an asm() statement is: asm ( \"template-string\" : \"constraint\" ( output ) : \"constraint\" ( input ) \")); An example of using the vabsdiff4 PTX instruction is: asm ( \"vabsdiff4"
  },
  {
    "id": 4164,
    "content": "add\" \" %0, %1, %2, %3;\" : \"=r\" ( result ) : \"r\" ( A ), \"r\" ( B ), \"r\" ( C )); This uses the vabsdiff4 instruction to compute an integer quad byte SIMD sum of absolute differences The absolute difference value is computed for each byte of the unsigned integers A and B in SIMD fashion"
  },
  {
    "id": 4165,
    "content": "Refer to the document “Using Inline PTX Assembly in CUDA” for details on using the assembly statement in your code Refer to the PTX ISA documentation (“Parallel Thread Execution ISA Version 3 0” for example) for details on the PTX instructions for the version of PTX that you are using"
  },
  {
    "id": 4168,
    "content": "Diagnostic Pragmas  The following pragmas may be used to control the error severity used when a given diagnostic message is issued"
  },
  {
    "id": 4169,
    "content": "#pragma nv_diag_suppress #pragma nv_diag_warning #pragma nv_diag_error #pragma nv_diag_default #pragma nv_diag_once Uses of these pragmas have the following form: #pragma nv_diag_xxx error_number, error_number"
  },
  {
    "id": 4170,
    "content": "Any diagnostic may be overridden to be an error, but only warnings may have their severity suppressed or be restored to a warning after being promoted to an error The nv_diag_default pragma is used to return the severity of a diagnostic to the one that was in effect before any pragmas were issued (i"
  },
  {
    "id": 4173,
    "content": "The following example suppresses the \"declared but never referenced\" warning on the declaration of foo : #pragma nv_diag_suppress 177 void foo () { int i = 0 ; } #pragma nv_diag_default 177 void bar () { int i = 0 ; } The following pragmas may be used to save and restore the current diagnostic pragma state: #pragma nv_diagnostic push #pragma nv_diagnostic pop Examples: #pragma nv_diagnostic push"
  },
  {
    "id": 4174,
    "content": "#pragma nv_diag_suppress 177 void foo () { int i = 0 ; } #pragma nv_diagnostic pop void bar () { int i = 0 ; } Note that the pragmas only affect the nvcc CUDA frontend compiler; they have no effect on the host compiler"
  },
  {
    "id": 4176,
    "content": "0, if the pragmas are inside the device code, warning unrecognized #pragma in device code will be emitted, otherwise they will be passed to the host compiler"
  },
  {
    "id": 4177,
    "content": "11 When the enclosing __host__ function is a template, nvcc may currently fail to issue a diagnostic message in some cases; this behavior may change in the future"
  },
  {
    "id": 4178,
    "content": "12 The intent is to prevent the host compiler from encountering the call to the function if the host compiler does not support it"
  },
  {
    "id": 4183,
    "content": "Introduction  Cooperative Groups is an extension to the CUDA programming model, introduced in CUDA 9, for organizing groups of communicating threads Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions Historically, the CUDA programming model has provided a single, simple"
  },
  {
    "id": 4184,
    "content": "construct for synchronizing cooperating threads: a barrier across all threads of a thread block, as implemented with the __syncthreads() intrinsic function However, programmers would like to define and synchronize groups of threads at other granularities to enable greater performance, design flexibility, and software reuse in the form of “collective” group-wide function interfaces In an effort to"
  },
  {
    "id": 4185,
    "content": "express broader patterns of parallel interaction, many performance-oriented programmers have resorted to writing their own ad hoc and unsafe primitives for synchronizing threads within a single warp, or across sets of thread blocks running on a single GPU"
  },
  {
    "id": 4186,
    "content": "Whilst the performance improvements achieved have often been valuable, this has resulted in an ever-growing collection of brittle code that is expensive to write, tune, and maintain over time and across GPU generations"
  },
  {
    "id": 4187,
    "content": "Cooperative Groups addresses this by providing a safe and future-proof mechanism to enable performant code"
  },
  {
    "id": 4205,
    "content": "0  The following experimental APIs are now moved to the main namespace: asynchronous reduce and scan update added in CUDA 11 7 thread_block_tile larger than 32 added in CUDA 11"
  },
  {
    "id": 4206,
    "content": "1 It is no longer required to provide memory using the block_tile_memory object in order to create these large tiles on Compute Capability 8"
  },
  {
    "id": 4210,
    "content": "Programming Model Concept  The Cooperative Groups programming model describes synchronization patterns both within and across CUDA thread blocks"
  },
  {
    "id": 4211,
    "content": "It provides both the means for applications to define their own groups of threads, and the interfaces to synchronize them"
  },
  {
    "id": 4212,
    "content": "It also provides new launch APIs that enforce certain restrictions and therefore can guarantee the synchronization will work"
  },
  {
    "id": 4213,
    "content": "These primitives enable new patterns of cooperative parallelism within CUDA, including producer-consumer parallelism, opportunistic parallelism, and global synchronization across the entire Grid"
  },
  {
    "id": 4214,
    "content": "The Cooperative Groups programming model consists of the following elements: Data types for representing groups of cooperating threads; Operations to obtain implicit groups defined by the CUDA launch API (e"
  },
  {
    "id": 4216,
    "content": ", thread blocks); Collectives for partitioning existing groups into new groups; Collective Algorithms for data movement and manipulation (e"
  },
  {
    "id": 4218,
    "content": "memcpy_async, reduce, scan); An operation to synchronize all threads within the group; Operations to inspect the group properties; Collectives that expose low-level, group-specific and often HW accelerated, operations"
  },
  {
    "id": 4219,
    "content": "The main concept in Cooperative Groups is that of objects naming the set of threads that are part of it This expression of groups as first-class program objects improves software composition, since collective functions can receive an explicit object representing the group of participating threads"
  },
  {
    "id": 4220,
    "content": "This object also makes programmer intent explicit, which eliminates unsound architectural assumptions that result in brittle code, undesirable restrictions upon compiler optimizations, and better compatibility with new GPU generations"
  },
  {
    "id": 4221,
    "content": "To write efficient code, its best to use specialized groups (going generic loses a lot of compile time optimizations), and pass these group objects by reference to functions that intend to use these threads in some cooperative fashion"
  },
  {
    "id": 4222,
    "content": "Previously, there were hidden constraints on the implementation when writing this code: __device__ int sum ( int * x , int n ) { Entire thread block must call sum sum ( x , n ); } All threads in the thread block must arrive at the __syncthreads() barrier, however, this constraint is hidden from the developer who might want to use sum(…) With Cooperative Groups, a better way of writing this would"
  },
  {
    "id": 4223,
    "content": "be: __device__ int sum ( const thread_block & g , int * x , int n ) { Entire thread block must call sum thread_block tb = this_thread_block (); sum ( tb , x , n );"
  },
  {
    "id": 4229,
    "content": "Regardless of how your kernel is written, it always has a set number of threads, blocks and block dimensions, a single grid and grid dimensions In addition, if the multi-device cooperative launch API is used, it can have multiple grids (single grid per device)"
  },
  {
    "id": 4230,
    "content": "These groups provide a starting point for decomposition into finer grained groups which are typically HW accelerated and are more specialized for the problem the developer is solving"
  },
  {
    "id": 4231,
    "content": "Creating a handle for an implicit group is a collective operation—all threads in the group must participate If the group was created in a conditional branch that not all threads reach, this can lead to deadlocks or data corruption For this reason, it is recommended that you create a handle for the implicit group upfront (as early as possible, before any branching has occurred) and use that handle"
  },
  {
    "id": 4232,
    "content": "throughout the kernel Group handles must be initialized at declaration time (there is no default constructor) for the same reason and copy-constructing them is discouraged"
  },
  {
    "id": 4237,
    "content": "Thread Block Group  Any CUDA programmer is already familiar with a certain group of threads: the thread block"
  },
  {
    "id": 4238,
    "content": "The Cooperative Groups extension introduces a new datatype, thread_block , to explicitly represent this concept within the kernel class thread_block; Constructed via: thread_block g = this_thread_block (); Public Member Functions: static void sync() : Synchronize the threads named in the group, equivalent to g"
  },
  {
    "id": 4239,
    "content": "barrier_wait(g barrier_arrive()) thread_block::arrival_token barrier_arrive() : Arrive on the thread_block barrier, returns a token that needs to be passed into barrier_wait() More details here void barrier_wait(thread_block::arrival_token&& t) : Wait on the thread_block barrier, takes arrival token returned from barrier_arrive() as a rvalue reference"
  },
  {
    "id": 4240,
    "content": "thread_rank () == 0 ) {   load from global into shared for all threads to work with x = ( * globalInput ); }   After loading data into shared memory, you want to synchronize   if all threads in your thread block need to see it g"
  },
  {
    "id": 4241,
    "content": "sync ();   equivalent to __syncthreads(); } Note: that all threads in the group must participate in collective operations, or the behavior is undefined"
  },
  {
    "id": 4242,
    "content": "Related: The thread_block datatype is derived from the more generic thread_group datatype, which can be used to represent a wider class of groups"
  },
  {
    "id": 4248,
    "content": "class cluster_group; Constructed via: cluster_group g = this_cluster (); Public Member Functions: static void sync() : Synchronize the threads named in the group, equivalent to g"
  },
  {
    "id": 4249,
    "content": "barrier_wait(g barrier_arrive()) static cluster_group::arrival_token barrier_arrive() : Arrive on the cluster barrier, returns a token that needs to be passed into barrier_wait() More details here static void barrier_wait(cluster_group::arrival_token&& t) : Wait on the cluster barrier, takes arrival token returned from barrier_arrive() as a rvalue reference"
  },
  {
    "id": 4250,
    "content": "APIs other than sync() are available at all times, but to be able to synchronize across the grid, you need to use the cooperative launch API"
  },
  {
    "id": 4251,
    "content": "class grid_group; Constructed via: grid_group g = this_grid (); Public Member Functions: bool is_valid() const : Returns whether the grid_group can synchronize void sync() const : Synchronize the threads named in the group, equivalent to g"
  },
  {
    "id": 4252,
    "content": "barrier_wait(g barrier_arrive()) grid_group::arrival_token barrier_arrive() : Arrive on the grid barrier, returns a token that needs to be passed into barrier_wait() More details here void barrier_wait(grid_group::arrival_token&& t) : Wait on the grid barrier, takes arrival token returned from barrier_arrive() as a rvalue reference"
  },
  {
    "id": 4253,
    "content": "Multi Grid Group  This group object represents all the threads launched across all devices of a multi-device cooperative launch Unlike the grid group, all the APIs require that you have used the appropriate launch API"
  },
  {
    "id": 4254,
    "content": "Thread Block Tile  A templated version of a tiled group, where a template parameter is used to specify the size of the tile - with this known at compile time there is the potential for more optimal execution"
  },
  {
    "id": 4255,
    "content": "template class thread_block_tile ; Constructed via: template _CG_QUALIFIER thread_block_tile tiled_partition ( const ParentT & g ) Size must be a power of 2 and less than or equal to 1024"
  },
  {
    "id": 4256,
    "content": "Notes section describes extra steps needed to create tiles of size larger than 32 on hardware with Compute Capability 7"
  },
  {
    "id": 4258,
    "content": "It is automatically inferred, but a value of void will store this information in the group handle rather than in the type Public Member Functions: void sync() const : Synchronize the threads named in the group unsigned long long num_threads() const : Total number of threads in the group unsigned long long thread_rank() const : Rank of the calling thread within [0, num_threads) unsigned long long"
  },
  {
    "id": 4259,
    "content": "meta_group_size() const : Returns the number of groups created when the parent group was partitioned unsigned long long meta_group_rank() const : Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size) T shfl(T var, unsigned int src_rank) const : Refer to Warp Shuffle Functions , Note: For sizes larger than 32 all threads in the group have to"
  },
  {
    "id": 4260,
    "content": "specify the same src_rank, otherwise the behavior is undefined T shfl_up(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32 T shfl_down(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower or equal to 32 T shfl_xor(T var, int delta) const : Refer to Warp Shuffle Functions , available only for sizes lower"
  },
  {
    "id": 4261,
    "content": "or equal to 32 T any(int predicate) const : Refer to Warp Vote Functions T all(int predicate) const : Refer to Warp Vote Functions T ballot(int predicate) const : Refer to Warp Vote Functions , available only for sizes lower or equal to 32 unsigned int match_any(T val) const : Refer to Warp Match Functions , available only for sizes lower or equal to 32 unsigned int match_all(T val, int &pred)"
  },
  {
    "id": 4262,
    "content": "const : Refer to Warp Match Functions , available only for sizes lower or equal to 32 Legacy member functions (aliases): unsigned long long size() const : Total number of threads in the group (alias of num_threads() ) Notes: thread_block_tile templated data structure is being used here, the size of the group is passed to the tiled_partition call as a template parameter rather than an argument"
  },
  {
    "id": 4263,
    "content": "shfl, shfl_up, shfl_down, and shfl_xor functions accept objects of any type when compiled with C++11 or later"
  },
  {
    "id": 4264,
    "content": "This means it’s possible to shuffle non-integral types as long as they satisfy the below constraints: Qualifies as trivially copyable i"
  },
  {
    "id": 4266,
    "content": ", is_trivially_copyable::value == true sizeof(T) struct block_tile_memory ; MaxBlockSize Specifies the maximal number of threads in the current thread block This parameter can be used to minimize the shared memory usage of block_tile_memory in kernels launched only with smaller thread counts This block_tile_memory needs be then passed into cooperative_groups::this_thread_block , allowing the"
  },
  {
    "id": 4267,
    "content": "resulting thread_block to be partitioned into tiles of sizes larger than 32 Overload of this_thread_block accepting block_tile_memory argument is a collective operation and has to be called with all threads in the thread_block block_tile_memory can be used on hardware with Compute Capability 8"
  },
  {
    "id": 4268,
    "content": "0 or higher in order to be able to write one source targeting multiple different Compute Capabilities"
  },
  {
    "id": 4270,
    "content": "Examples: / The following code will create two sets of tiled groups, of size 32 and 4 respectively: / The latter has the provenance encoded in the type, while the first stores it in the handle thread_block block = this_thread_block (); thread_block_tile tile32 = tiled_partition ( block ); thread_block_tile tile4 = tiled_partition ( block ); / The following code will create tiles of size 128 on"
  },
  {
    "id": 4273,
    "content": ") {   reserve shared memory for thread_block_tile usage,   specify that block size will be at most 256 threads"
  },
  {
    "id": 4274,
    "content": "__shared__ block_tile_memory shared ; thread_block thb = this_thread_block ( shared );   Create tiles with 128 threads auto tile = tiled_partition ( thb );"
  },
  {
    "id": 4280,
    "content": "Warp-Synchronous Code Pattern  Developers might have had warp-synchronous codes that they previously made implicit assumptions about the warp size and would code around that number"
  },
  {
    "id": 4282,
    "content": ") {   obtain default \"current thread block\" group thread_block my_block = this_thread_block ();   subdivide into 32-thread, tiled subgroups   Tiled subgroups evenly partition a parent group into   adjacent sets of threads - in this case each one warp in size auto my_tile = tiled_partition ( my_block );   This operation will be performed by only the   first 32-thread tile of each block if ( my_tile"
  },
  {
    "id": 4283,
    "content": "Single thread group  Group representing the current thread can be obtained from this_thread function: thread_block_tile this_thread (); The following memcpy_async API uses a thread_group , to copy an int element from source to destination: #include #include cooperative_groups :: memcpy_async ( cooperative_groups :: this_thread (), dest , src , sizeof ( int )); More detailed examples of using"
  },
  {
    "id": 4284,
    "content": "this_thread to perform asynchronous copies can be found in the Single-Stage Asynchronous Data Copies using cuda::pipeline and Multi-Stage Asynchronous Data Copies using cuda::pipeline sections"
  },
  {
    "id": 4289,
    "content": "Coalesced Groups  In CUDA’s SIMT architecture, at the hardware level the multiprocessor executes threads in groups of 32 called warps"
  },
  {
    "id": 4290,
    "content": "If there exists a data-dependent conditional branch in the application code such that threads within a warp diverge, then the warp serially executes each branch disabling threads not on that path"
  },
  {
    "id": 4291,
    "content": "Cooperative Groups has functionality to discover, and create, a group containing all coalesced threads"
  },
  {
    "id": 4292,
    "content": "It returns the set of active threads at that point in time, and makes no guarantee about which threads are returned (as long as they are active) or that they will stay coalesced throughout execution (they will be brought back together for the execution of a collective but can diverge again afterwards) class coalesced_group; Constructed via: coalesced_group active = coalesced_threads (); Public"
  },
  {
    "id": 4293,
    "content": "Member Functions: void sync() const : Synchronize the threads named in the group unsigned long long num_threads() const : Total number of threads in the group unsigned long long thread_rank() const : Rank of the calling thread within [0, num_threads) unsigned long long meta_group_size() const : Returns the number of groups created when the parent group was partitioned unsigned long long"
  },
  {
    "id": 4294,
    "content": "meta_group_rank() const : Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size) T shfl(T var, unsigned int src_rank) const : Refer to Warp Shuffle Functions T shfl_up(T var, int delta) const : Refer to Warp Shuffle Functions T shfl_down(T var, int delta) const : Refer to Warp Shuffle Functions T any(int predicate) const : Refer to Warp Vote"
  },
  {
    "id": 4295,
    "content": "Functions T all(int predicate) const : Refer to Warp Vote Functions T ballot(int predicate) const : Refer to Warp Vote Functions unsigned int match_any(T val) const : Refer to Warp Match Functions unsigned int match_all(T val, int &pred) const : Refer to Warp Match Functions Legacy member functions (aliases): unsigned long long size() const : Total number of threads in the group (alias of"
  },
  {
    "id": 4296,
    "content": "num_threads() ) Notes: shfl, shfl_up, and shfl_down functions accept objects of any type when compiled with C++11 or later This means it’s possible to shuffle non-integral types as long as they satisfy the below constraints: Qualifies as trivially copyable i"
  },
  {
    "id": 4298,
    "content": "is_trivially_copyable::value == true sizeof(T) thread_block_tile tiled_partition ( const ParentT & g ); thread_group tiled_partition ( const thread_group & parent , unsigned int tilesz ); The tiled_partition method is a collective operation that partitions the parent group into a one-dimensional, row-major, tiling of subgroups A total of ((size(parent)/tilesz) subgroups will be created, therefore"
  },
  {
    "id": 4299,
    "content": "the parent group size must be evenly divisible by the Size The implementation may cause the calling thread to wait until all the members of the parent group have invoked the operation before resuming execution Functionality is limited to native hardware sizes, 1/2/4/8/16/32 and the cg::size(parent) must be greater than the Size parameter The templated version of tiled_partition supports"
  },
  {
    "id": 4303,
    "content": "0 minimum, C++11 for sizes larger than 32 Example: / The following code will create a 32-thread tile thread_block block = this_thread_block (); thread_block_tile tile32 = tiled_partition ( block ); We can partition each of these groups into even smaller groups, each of size 4 threads: auto tile4 = tiled_partition ( tile32 ); or using a general group thread_group tile4 = tiled_partition(tile32,"
  },
  {
    "id": 4304,
    "content": "4); If, for instance, if we were to then include the following line of code: if ( tile4 thread_rank () == 0 ) printf ( \"Hello from tile4 rank 0 \" ); then the statement would be printed by every fourth thread in the block: the threads of rank 0 in each tile4 group, which correspond to those threads with ranks 0,4,8,12,etc in the block group"
  },
  {
    "id": 4308,
    "content": "labeled_partition  template coalesced_group labeled_partition ( const coalesced_group & g , Label label ); template coalesced_group labeled_partition ( const thread_block_tile & g , Label label ); The labeled_partition method is a collective operation that partitions the parent group into one-dimensional subgroups within which the threads are coalesced The implementation will evaluate a"
  },
  {
    "id": 4309,
    "content": "condition label and assign threads that have the same value for label into the same group binary_partition  coalesced_group binary_partition ( const coalesced_group & g , bool pred ); template coalesced_group binary_partition ( const thread_block_tile & g , bool pred ); The binary_partition() method is a collective operation that partitions the parent group into one-dimensional subgroups within"
  },
  {
    "id": 4310,
    "content": "which the threads are coalesced The implementation will evaluate a predicate and assign threads that have the same value into the same group"
  },
  {
    "id": 4312,
    "content": "0 minimum, C++11 Example: / This example divides a 32-sized tile into a group with odd / numbers and a group with even numbers _global__ void oddEven ( int * inputArr ) { auto block = cg :: this_thread_block (); auto tile32 = cg :: tiled_partition ( block ); inputArr contains random integers int elem = inputArr [ block thread_rank ()]; after this, tile32 is split into 2 groups, a subtile where"
  },
  {
    "id": 4313,
    "content": "elem&1 is true and one where its false auto subtile = cg :: binary_partition ( tile32 , ( elem & 1 )); } 8"
  },
  {
    "id": 4315,
    "content": "Group Collectives  Cooperative Groups library provides a set of collective operations that can be performed by a group of threads These operations require participation of all threads in the specified group in order to complete the operation All threads in the group need to pass the same values for corresponding arguments to each collective call, unless different values are explicitly allowed in"
  },
  {
    "id": 4317,
    "content": "barrier_arrive and barrier_wait  T :: arrival_token T::barrier_arrive (); void T::barrier_wait ( T :: arrival_token && ); barrier_arrive and barrier_wait member functions provide a synchronization API similar to cuda::barrier (read more)"
  },
  {
    "id": 4318,
    "content": "Cooperative Groups automatically initializes the group barrier, but arrive and wait operations have an additional restriction resulting from collective nature of those operations: All threads in the group must arrive and wait at the barrier once per phase When barrier_arrive is called with a group, result of calling any collective operation or another barrier arrival with that group is undefined"
  },
  {
    "id": 4319,
    "content": "until completion of the barrier phase is observed with barrier_wait call Threads blocked on barrier_wait might be released from the synchronization before other threads call barrier_wait , but only after all threads in the group called barrier_arrive Group type T can be any of the implicit groups This allows threads to do independent work after they arrive and before they wait for the"
  },
  {
    "id": 4320,
    "content": "synchronization to resolve, allowing to hide some of the synchronization latency barrier_arrive returns an arrival_token object that must be passed into the corresponding barrier_wait"
  },
  {
    "id": 4321,
    "content": "Example of barrier_arrive and barrier_wait used to synchronize initization of shared memory across the cluster: #include using namespace cooperative_groups ; void __device__ init_shared_data ( const thread_block & block , int * data ); void __device__ local_processing ( const thread_block & block ); void __device__ process_shared_data ( const thread_block & block , int * data ); __global__ void"
  },
  {
    "id": 4322,
    "content": "cluster_kernel () { extern __shared__ int array []; auto cluster = this_cluster (); auto block = this_thread_block (); Use this thread block to initialize some shared state init_shared_data ( block , & array [ 0 ]); auto token = cluster barrier_arrive (); Let other blocks know this block is running and data was initialized Do some local processing to hide the synchronization latency"
  },
  {
    "id": 4323,
    "content": "local_processing ( block ); Map data in shared memory from the next block in the cluster int * dsmem = cluster num_blocks ()); Make sure all other blocks in the cluster are running and initialized shared data before accessing dsmem cluster barrier_wait ( std :: move ( token )); Consume data in distributed shared memory process_shared_data ( block , dsmem ); cluster"
  },
  {
    "id": 4324,
    "content": "sync  static void T::sync (); template void sync ( T & group ); sync synchronizes the threads named in the group Group type T can be any of the existing group types, as all of them support synchronization Its available as a member function in every group type or as a free function taking a group as parameter If the group is a grid_group or a multi_grid_group the kernel must have been launched"
  },
  {
    "id": 4326,
    "content": "memcpy_async  memcpy_async is a group-wide collective memcpy that utilizes hardware accelerated support for non-blocking memory transactions from global to shared memory Given a set of threads named in the group, memcpy_async will move specified amount of bytes or elements of the input type through a single pipeline stage Additionally for achieving best performance when using the memcpy_async"
  },
  {
    "id": 4327,
    "content": "API, an alignment of 16 bytes for both shared memory and global memory is required It is important to note that while this is a memcpy in the general case, it is only asynchronous if the source is global memory and the destination is shared memory and both can be addressed with 16, 8, or 4 byte alignments"
  },
  {
    "id": 4328,
    "content": "Asynchronously copied data should only be read following a call to wait or wait_prior which signals that the corresponding stage has completed moving data to shared memory"
  },
  {
    "id": 4330,
    "content": "In order to efficiently overlap data transfer and execution, its important to be able to kick off an N+1 memcpy_async request while waiting on and operating on request N To do so, use memcpy_async and wait on it using the collective stage-based wait_prior API"
  },
  {
    "id": 4331,
    "content": "Usage 1 template void memcpy_async ( const TyGroup & group , TyElem * __restrict__ _dst , const TyElem * __restrict__ _src , const TyShape & shape ); Performs a copy of ``shape`` bytes Usage 2 template void memcpy_async ( const TyGroup & group , TyElem * __restrict__ dst , const TyDstLayout & dstLayout , const TyElem * __restrict__ src , const TySrcLayout & srcLayout ); Performs a copy of"
  },
  {
    "id": 4334,
    "content": "1 with both src and dst input layouts, expects the layout to be provided in elements rather than bytes"
  },
  {
    "id": 4335,
    "content": "If cuda::aligned_size_t type is used as the layout, the number of elements specified times sizeof(TyElem) must be a multiple of N and it is recommended to use std::byte or char as the element type If specified shape or layout of the copy is of type cuda::aligned_size_t , alignment will be guaranteed to be at least min(16, N) In that case both dst and src pointers need to be aligned to N bytes and"
  },
  {
    "id": 4340,
    "content": "Example:  / This example streams elementsPerThreadBlock worth of data from global memory  / into a limited sized shared memory (elementsInShared) block to operate on"
  },
  {
    "id": 4341,
    "content": "#include #include namespace cg = cooperative_groups ; __global__ void kernel ( int * global_data ) { cg :: thread_block tb = cg :: this_thread_block (); const size_t elementsPerThreadBlock = 16 * 1024 ; const size_t elementsInShared = 128 ; __shared__ int local_smem [ elementsInShared ]; size_t copy_count ; size_t index = 0 ; while ( index void wait ( TyGroup & group ); template void wait_prior ("
  },
  {
    "id": 4342,
    "content": "TyGroup & group ); wait and wait_prior collectives allow to wait for memcpy_async copies to complete"
  },
  {
    "id": 4343,
    "content": "wait_prior allows that the latest NumStages are still not done and waits for all the previous requests So with N total copies requested, it waits until the first N-NumStages are done and the last NumStages might still be in progress"
  },
  {
    "id": 4347,
    "content": "Example: / This example streams elementsPerThreadBlock worth of data from global memory / into a limited sized shared memory (elementsInShared) block to operate on in / multiple (two) stages #include #include namespace cg = cooperative_groups ; __global__ void kernel ( int * global_data ) { cg :: thread_block tb = cg :: this_thread_block (); const size_t elementsPerThreadBlock = 16 * 1024 + 64 ;"
  },
  {
    "id": 4348,
    "content": "const size_t elementsInShared = 128 ; __align__ ( 16 ) __shared__ int local_smem [ 2 ][ elementsInShared ]; int stage = 0 ; First kick off an extra request size_t copy_count = elementsInShared ; size_t index = copy_count ; cg :: memcpy_async ( tb , local_smem [ stage ], elementsInShared , global_data , elementsPerThreadBlock - index ); while ( index ( tb ); Its now available and we can work with"
  },
  {
    "id": 4351,
    "content": "copy_count = min ( elementsInShared , elementsPerThreadBlock - index ); index += copy_count ;   A cg::sync(tb) might be needed here depending on whether   the work done with local_smem[stage] can release threads to race ahead or not   Wrap to the next stage stage ^= 1 ; } cg :: wait ( tb );   The last local_smem[stage] can be handled here } 8"
  },
  {
    "id": 4354,
    "content": "reduce  template auto reduce ( const TyGroup & group , TyArg && val , TyOp && op ) -> decltype ( op ( val , val )); reduce performs a reduction operation on the data provided by each thread named in the group passed in"
  },
  {
    "id": 4355,
    "content": "This takes advantage of hardware acceleration (on compute 80 and higher devices) for the arithmetic add, min, or max operations and the logical AND, OR, or XOR, as well as providing a software fallback on older generation hardware"
  },
  {
    "id": 4358,
    "content": "Reduce also supports lambdas and other function objects that can be invoked using operator() Asynchronous reduce template void reduce_update_async ( const TyGroup & group , TyAtomic & atomic , TyArg && val , TyOp && op ); template void reduce_store_async ( const TyGroup & group , TyAtomic & atomic , TyArg && val , TyOp && op ); template void reduce_store_async ( const TyGroup & group , TyArg *"
  },
  {
    "id": 4359,
    "content": "ptr , TyArg && val , TyOp && op ); *_async variants of the API are asynchronously calculating the result to either store to or update a specified destination by one of the participating threads, instead of returning it by each thread"
  },
  {
    "id": 4360,
    "content": "To observe the effect of these asynchronous calls, calling group of threads or a larger group containing them need to be synchronized"
  },
  {
    "id": 4361,
    "content": "In case of the atomic store or update variant, atomic argument can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library This variant of the API is available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library Result of the reduction is used to atomically update the atomic according to the specified op , eg Scope of the"
  },
  {
    "id": 4362,
    "content": "atomic must include all the threads in the group and if multiple groups are using the same atomic concurrently, scope must include all threads in all groups using it In case of the pointer store variant, result of the reduction will be weakly stored into the dst pointer"
  },
  {
    "id": 4365,
    "content": "Example of approximate standard deviation for integer vector: #include #include namespace cg = cooperative_groups ; / Calculate approximate standard deviation of integers in vec __device__ int std_dev ( const cg :: thread_block_tile & tile , int * vec , int length ) { int thread_sum = 0 ; calculate average first for ( int i = tile thread_rank (); i allows cg::reduce() to know it can use hardware"
  },
  {
    "id": 4366,
    "content": "acceleration for addition int avg = cg :: reduce ( tile , thread_sum , cg :: plus ()) / length ; int thread_diffs_sum = 0 ; for ( int i = tile thread_rank (); i ( cg :: reduce ( tile , thread_diffs_sum , cg :: plus ())) / length ; return static_cast ( sqrtf ( diff_sum )); } Example of block wide reduction: #include #include namespace cg = cooperative_groups ; / The following example accepts input"
  },
  {
    "id": 4367,
    "content": "in *A and outputs a result into *sum / It spreads the data equally within the block __device__ void block_reduce ( const int * A , int count , cuda :: atomic & total_sum ) { auto block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( block ); int thread_sum = 0 ; Stride loop over all values, each thread accumulates its part of the array thread_rank (); i allows cg::reduce() to"
  },
  {
    "id": 4368,
    "content": "know it can use hardware acceleration for addition cg :: reduce_update_async ( tile , total_sum , thread_sum , cg :: plus ()); synchronize the block, to ensure all async reductions are ready block Reduce Operators  Below are the prototypes of function objects for some of the basic operations that can be done with reduce namespace cooperative_groups { template struct cg :: plus ; template struct"
  },
  {
    "id": 4369,
    "content": "cg :: less ; template struct cg :: greater ; template struct cg :: bit_and ; template struct cg :: bit_xor ; template struct cg :: bit_or ; } Reduce is limited to the information available to the implementation at compile time"
  },
  {
    "id": 4372,
    "content": "These objects appear similar to those presented in the C++ STL, with the exception of less/greater The reason for any difference from the STL is that these function objects are designed to actually mirror the operation of the hardware intrinsics"
  },
  {
    "id": 4373,
    "content": "Functional description: cg::plus: Accepts two values and returns the sum of both using operator+ cg::less: Accepts two values and returns the lesser using operator is specialized within cg::reduce and calls __reduce_add_sync("
  },
  {
    "id": 4375,
    "content": "0+ cg :: reduce ( tile , ( int ) val , cg :: plus ()); cg::plus fails to match with an accelerator and instead performs a standard shuffle based reduction cg :: reduce ( tile , ( float ) val , cg :: plus ()); While individual components of a vector are supported, reduce will not use hardware intrinsics for the following It will also be necessary to define a corresponding operator for vector and"
  },
  {
    "id": 4376,
    "content": "any custom types that may be used int4 vec = { }; cg :: reduce ( tile , vec , cg :: plus ()) Finally lambdas and other function objects cannot be inspected for dispatch and will instead perform shuffle based reductions using the provided function object cg :: reduce ( tile , ( int ) val , []( int l , int r ) -> int { return l + r ;}); } 8"
  },
  {
    "id": 4380,
    "content": "inclusive_scan and exclusive_scan  template auto inclusive_scan ( const TyGroup & group , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal inclusive_scan ( const TyGroup & group , TyVal && val ); template auto exclusive_scan ( const TyGroup & group , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal exclusive_scan ( const TyGroup & group ,"
  },
  {
    "id": 4381,
    "content": "TyVal && val ); inclusive_scan and exclusive_scan performs a scan operation on the data provided by each thread named in the group passed in Result for each thread is a reduction of data from threads with lower thread_rank than that thread in case of exclusive_scan inclusive_scan and exclusive_scan also supports lambdas and other function objects that can be invoked using operator() Scan update"
  },
  {
    "id": 4382,
    "content": "template auto inclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal inclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val ); template auto exclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val , TyFn && op ) -> decltype ( op ( val , val )); template TyVal"
  },
  {
    "id": 4383,
    "content": "exclusive_scan_update ( const TyGroup & group , TyAtomic & atomic , TyVal && val ); *_scan_update collectives take an additional argument atomic that can be either of cuda::atomic or cuda::atomic_ref available in CUDA C++ Standard Library"
  },
  {
    "id": 4384,
    "content": "These variants of the API are available only on platforms and devices, where these types are supported by the CUDA C++ Standard Library These variants will perform an update to the atomic according to op with value of the sum of input values of all threads in the group Previous value of the atomic will be combined with the result of scan by each thread and returned Following pseudocode"
  },
  {
    "id": 4385,
    "content": "illustrates how the update variant of scan works: /* inclusive_scan_update behaves as the following block, except both reduce and inclusive_scan is calculated simultaneously auto total = reduce(group, val, op); TyVal old; if (group"
  },
  {
    "id": 4386,
    "content": "thread_rank() == selected_thread) { atomicaly { old = atomic load(); atomic store(op(old, total)); } } old = group shfl(old, selected_thread); return op(inclusive_scan(group, val, op), old); */ Codegen Requirements: Compute Capability 5"
  },
  {
    "id": 4388,
    "content": "Example: #include #include #include namespace cg = cooperative_groups ; __global__ void kernel () { auto thread_block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( thread_block ); unsigned int val = cg :: inclusive_scan ( tile , tile thread_rank (), val ); } /* prints for each group: 0: 0 1: 1 2: 3 3: 6 4: 10 5: 15 6: 21 7: 28 */ Example of stream compaction using"
  },
  {
    "id": 4389,
    "content": "exclusive_scan: #include #include namespace cg = cooperative_groups ; put data from input into output only if it passes test_fn predicate template __device__ int stream_compaction ( Group & g , Data * input , int count , TyFn && test_fn , Data * output ) { int per_thread = count / g thread_rank () * per_thread , count ); int my_count = min ( per_thread , count - thread_start ); get all passing"
  },
  {
    "id": 4390,
    "content": "items from my part of the input into a contagious part of the array and count them int i = thread_start ; while ( i #include namespace cg = cooperative_groups ; Buffer partitioning is static to make the example easier to follow, but any arbitrary dynamic allocation scheme can be implemented by replacing this function __device__ int calculate_buffer_space_needed ( cg :: thread_block_tile & tile ) {"
  },
  {
    "id": 4391,
    "content": "return tile thread_rank () % 2 + 1 ; } __device__ int my_thread_data ( int i ) { return i ; } __global__ void kernel () { __shared__ extern int buffer []; __shared__ cuda :: atomic buffer_used ; auto block = cg :: this_thread_block (); auto tile = cg :: tiled_partition ( block ); buffer_used = 0 ; block sync (); each thread calculates buffer size it needs int buf_needed ="
  },
  {
    "id": 4392,
    "content": "calculate_buffer_space_needed ( tile ); scan over the needs of each thread, result for each thread is an offset of that thread’s part of the buffer buffer_used is atomically updated with the sum of all thread's inputs, to correctly offset other tile’s allocations int buf_offset = cg :: exclusive_scan_update ( tile , buffer_used , buf_needed ); each thread fills its own part of the buffer with"
  },
  {
    "id": 4393,
    "content": "thread specific data for ( int i = 0 ; i void invoke_one ( const Group & group , Fn && fn , Args && args ) -> decltype ( fn ( args )); invoke_one selects a single arbitrary thread from the calling group and uses that thread to call the supplied invocable fn with the supplied arguments args In case of invoke_one_broadcast the result of the call is also distributed to all threads in the group and"
  },
  {
    "id": 4394,
    "content": "returned from this collective Calling group can be synchronized with the selected thread before and/or after it calls the supplied invocable It means that communication within the calling group is not allowed inside the supplied invocable body, otherwise forward progress is not guaranteed Communication with threads outside of the calling group is allowed in the body of the supplied invocable"
  },
  {
    "id": 4396,
    "content": "0 or higher hardware acceleration might be used to select the thread when called with explicit group types group : All group types are valid for invoke_one , coalesced_group and thread_block_tile are valid for invoke_one_broadcast"
  },
  {
    "id": 4397,
    "content": "args : Parameter pack of types matching types of parameters of the supplied invocable fn In case of invoke_one_broadcast the return type of the supplied invocable fn must satisfy the below requirements: Qualifies as trivially copyable i"
  },
  {
    "id": 4399,
    "content": "is_trivially_copyable::value == true sizeof(T) #include namespace cg = cooperative_groups ; template __device__ unsigned int atomicAddOneRelaxed ( cuda :: atomic & atomic ) { auto g = cg :: coalesced_threads (); auto prev = cg :: invoke_one_broadcast ( g , [ & ] () { return atomic"
  },
  {
    "id": 4400,
    "content": "Grid Synchronization  Prior to the introduction of Cooperative Groups, the CUDA programming model only allowed synchronization between thread blocks at a kernel completion boundary"
  },
  {
    "id": 4401,
    "content": "The kernel boundary carries with it an implicit invalidation of state, and with it, potential performance implications"
  },
  {
    "id": 4402,
    "content": "For example, in certain use cases, applications have a large number of small kernels, with each kernel representing a stage in a processing pipeline The presence of these kernels is required by the current CUDA programming model to ensure that the thread blocks operating on one pipeline stage have produced data before the thread block operating on the next pipeline stage is ready to consume it In"
  },
  {
    "id": 4403,
    "content": "such cases, the ability to provide global inter thread block synchronization would allow the application to be restructured to have persistent thread blocks, which are able to synchronize on the device when a given stage is complete To synchronize across the grid, from within a kernel, you would simply use the grid sync() function: grid_group grid = this_grid (); grid"
  },
  {
    "id": 4404,
    "content": "sync (); And when launching the kernel it is necessary to use, instead of the >> execution configuration syntax, the cudaLaunchCooperativeKernel CUDA runtime launch API or the CUDA driver equivalent"
  },
  {
    "id": 4405,
    "content": "Example: To guarantee co-residency of the thread blocks on the GPU, the number of blocks launched needs to be carefully considered"
  },
  {
    "id": 4406,
    "content": "For example, as many blocks as there are SMs can be launched as follows: int dev = 0 ; cudaDeviceProp deviceProp ; cudaGetDeviceProperties ( & deviceProp , dev ); initialize, then launch cudaLaunchCooperativeKernel (( void * ) my_kernel , deviceProp multiProcessorCount , numThreads , args ); Alternatively, you can maximize the exposed parallelism by calculating how many blocks can fit"
  },
  {
    "id": 4407,
    "content": "simultaneously per-SM using the occupancy calculator as follows: / This will launch a grid that can maximally fill the GPU, on the default stream with kernel arguments int numBlocksPerSm = 0 ; Number of threads my_kernel will be launched with int numThreads = 128 ; cudaDeviceProp deviceProp ; cudaGetDeviceProperties ( & deviceProp , dev ); cudaOccupancyMaxActiveBlocksPerMultiprocessor ( &"
  },
  {
    "id": 4408,
    "content": "numBlocksPerSm , my_kernel , numThreads , 0 ); launch void * kernelArgs [] = { /* add kernel args */ }; dim3 dimBlock ( numThreads , 1 , 1 ); dim3 dimGrid ( deviceProp multiProcessorCount * numBlocksPerSm , 1 , 1 ); cudaLaunchCooperativeKernel (( void * ) my_kernel , dimGrid , dimBlock , kernelArgs ); It is good practice to first ensure the device supports cooperative launches by querying the"
  },
  {
    "id": 4409,
    "content": "device attribute cudaDevAttrCooperativeLaunch : int dev = 0 ; int supportsCoopLaunch = 0 ; cudaDeviceGetAttribute ( & supportsCoopLaunch , cudaDevAttrCooperativeLaunch , dev ); which will set supportsCoopLaunch to 1 if the property is supported on device 0"
  },
  {
    "id": 4410,
    "content": "In addition, you need to be running on either of these: The Linux platform without MPS The Linux platform with MPS and on a device with compute capability 7 0 or higher The latest Windows platform 8"
  },
  {
    "id": 4412,
    "content": "Multi-Device Synchronization  In order to enable synchronization across multiple devices with Cooperative Groups, use of the cudaLaunchCooperativeKernelMultiDevice CUDA API is required This, a significant departure from existing CUDA APIs, will allow a single host thread to launch a kernel across multiple devices"
  },
  {
    "id": 4413,
    "content": "In addition to the constraints and guarantees made by cudaLaunchCooperativeKernel , this API has additional semantics: This API will ensure that a launch is atomic, i"
  },
  {
    "id": 4415,
    "content": "if the API call succeeds, then the provided number of thread blocks will launch on all specified devices"
  },
  {
    "id": 4417,
    "content": "All devices being targeted by this launch must be of the same compute capability - major and minor versions The block size, grid size and amount of shared memory per grid must be the same across all devices"
  },
  {
    "id": 4418,
    "content": "Note that this means the maximum number of blocks that can be launched per device will be limited by the device with the least number of SMs Any user defined __device__ , __constant__ or __managed__ device global variables present in the module that owns the CUfunction being launched are independently instantiated on every device"
  },
  {
    "id": 4422,
    "content": "Optimal performance in multi-device synchronization is achieved by enabling peer access via cuCtxEnablePeerAccess or cudaDeviceEnablePeerAccess for all participating devices"
  },
  {
    "id": 4423,
    "content": "The launch parameters should be defined using an array of structs (one per device), and launched with cudaLaunchCooperativeKernelMultiDevice Example: cudaDeviceProp deviceProp ; cudaGetDeviceCount ( & numGpus ); Per device launch parameters cudaLaunchParams * launchParams = ( cudaLaunchParams * ) malloc ( sizeof ( cudaLaunchParams ) * numGpus ); cudaStream_t * streams = ( cudaStream_t * ) malloc"
  },
  {
    "id": 4424,
    "content": "( sizeof ( cudaStream_t ) * numGpus ); The kernel arguments are copied over during launch Its also possible to have individual copies of kernel arguments per device, but the signature and name of the function/kernel must be the same"
  },
  {
    "id": 4425,
    "content": "void * kernelArgs [] = { /* Add kernel arguments */ }; for ( int i = 0 ; i >> ( data ); tail_launch >> ( data ); } } void host_launch ( int * data ) { parent_launch >> ( data ); } 9"
  },
  {
    "id": 4430,
    "content": "Zero Copy Memory  Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above A kernel may not allocate or free zero-copy memory, but may use pointers to zero-copy passed in from the host program"
  },
  {
    "id": 4436,
    "content": "Constant Memory  Constants may not be modified from the device They may only be modified from the host, but the behavior of modifying a constant from the host while there is a concurrent grid that access that constant at any point during its lifetime is undefined"
  },
  {
    "id": 4442,
    "content": "Shared and Local Memory  Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child"
  },
  {
    "id": 4443,
    "content": "Behavior is undefined when an object in one of these locations is referenced outside of the scope within which it belongs, and may cause an error"
  },
  {
    "id": 4444,
    "content": "The NVIDIA compiler will attempt to warn if it can detect that a pointer to local or shared memory is being passed as an argument to a kernel launch At runtime, the programmer may use the __isGlobal() intrinsic to determine whether a pointer references global memory and so may safely be passed to a child launch"
  },
  {
    "id": 4445,
    "content": "Note that calls to cudaMemcpy*Async() or cudaMemset*Async() may invoke new child kernels on the device in order to preserve stream semantics"
  },
  {
    "id": 4452,
    "content": "Local Memory  Local memory is private storage for an executing thread, and is not visible outside of that thread It is illegal to pass a pointer to local memory as a launch argument when launching a child kernel For example the following is illegal, with undefined behavior if x_array is accessed by child_launch : int x_array [ 10 ]; Creates x_array in parent's local memory child_launch >> ("
  },
  {
    "id": 4453,
    "content": "x_array ); It is sometimes difficult for a programmer to be aware of when a variable is placed into local memory by the compiler As a general rule, all storage passed to a child kernel should be allocated explicitly from the global-memory heap, either with cudaMalloc() , new() or by declaring __device__ storage at global scope For example: Correct - \"value\" is global storage __device__ int value ;"
  },
  {
    "id": 4454,
    "content": "__device__ void x () { value = 5 ; child >> ( & value ); } Invalid - \"value\" is local storage __device__ void y () { int value = 5 ; child >> ( & value ); } 9"
  },
  {
    "id": 4459,
    "content": "Texture Memory  Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses Coherence for texture memory is enforced at the invocation of a child grid and when a child grid completes This means that writes to memory prior to a child kernel launch are reflected in texture memory accesses of the child Similarly to Global Memory above, writes to"
  },
  {
    "id": 4460,
    "content": "memory by a child are never guaranteed to be reflected in the texture memory accesses by a parent The only way to access the modifications made by the threads in the child grid before the parent grid exits is via a kernel launched into the cudaStreamTailLaunch stream Concurrent accesses by parent and child may result in inconsistent data"
  },
  {
    "id": 4466,
    "content": "CUDA C++ Reference  This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism The language interface and API available to CUDA kernels using CUDA C++ for Dynamic Parallelism, referred to as the Device Runtime , is substantially like that of the CUDA Runtime API available on the host Where possible the syntax and semantics of the CUDA"
  },
  {
    "id": 4467,
    "content": "Runtime API have been retained in order to facilitate ease of code reuse for routines that may run in either the host or device environments"
  },
  {
    "id": 4468,
    "content": "This enables each thread to make unique, dynamic decisions regarding what kernel or operation to execute next There are no synchronization requirements between threads within a block to execute any of the provided device runtime APIs, which enables the device runtime API functions to be called in arbitrarily divergent kernel code without deadlock"
  },
  {
    "id": 4473,
    "content": "Device-Side Kernel Launch  Kernels may be launched from the device using the standard CUDA >> syntax: kernel_name >> ([ kernel arguments ]); Dg is of type dim3 and specifies the dimensions and size of the grid Db is of type dim3 and specifies the dimensions and size of each thread block Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per"
  },
  {
    "id": 4481,
    "content": "Launches are Asynchronous  Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread"
  },
  {
    "id": 4482,
    "content": "That is to say, the >> launch command will return immediately and the launching thread will continue to execute until it hits an implicit launch-synchronization point (such as at a kernel launched into the cudaStreamTailLaunch stream) The child grid launch is posted to the device and will execute independently of the parent thread The child grid may begin execution at any time after launch, but"
  },
  {
    "id": 4483,
    "content": "is not guaranteed to begin execution until the launching thread reaches an implicit launch-synchronization point"
  },
  {
    "id": 4489,
    "content": "Launch Environment Configuration  All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig() , and device limits returned from cudaDeviceGetLimit() ) will be inherited from the parent"
  },
  {
    "id": 4490,
    "content": "For host-launched kernels, per-kernel configurations set from the host will take precedence over the global setting"
  },
  {
    "id": 4496,
    "content": "Streams  Both named and unnamed (NULL) streams are available from the device runtime Named streams may be used by any thread within a grid, but stream handles may not be passed to other child/parent kernels Similar to host-side launch, work launched into separate streams may run concurrently, but actual concurrency is not guaranteed Programs that depend upon concurrency between child kernels are"
  },
  {
    "id": 4498,
    "content": "The host-side NULL stream’s cross-stream barrier semantic is not supported on the device (see below for details) In order to retain semantic compatibility with the host runtime, all device streams must be created using the cudaStreamCreateWithFlags() API, passing the cudaStreamNonBlocking flag The cudaStreamCreate() call is a host-runtime- only API and will fail to compile for the device As"
  },
  {
    "id": 4499,
    "content": "cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, a kernel launched into the cudaStreamTailLaunch stream should be used instead when the application needs to know that stream-launched child kernels have completed"
  },
  {
    "id": 4505,
    "content": "The Implicit (NULL) Stream  Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details) The device runtime offers a single implicit, unnamed stream shared between all threads in a thread block, but as all named streams must be created with the cudaStreamNonBlocking flag, work launched into the NULL stream"
  },
  {
    "id": 4506,
    "content": "will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks)"
  },
  {
    "id": 4512,
    "content": "The Fire-and-Forget Stream  The fire-and-forget named stream ( cudaStreamFireAndForget ) allows the user to launch fire-and-forget work with less boilerplate and without stream tracking overhead It is functionally identical to, but faster than, creating a new stream per launch, and launching into that stream Fire-and-forget launches are immediately scheduled for launch without any dependency on"
  },
  {
    "id": 4513,
    "content": "the completion of previously launched grids No other grid launches can depend on the completion of a fire-and-forget launch, except through the implicit synchronization at the end of the parent grid So a tail launch or the next grid in parent grid’s stream won’t launch before a parent grid’s fire-and-forget work has completed The fire-and-forget stream is not supported when compiled with"
  },
  {
    "id": 4514,
    "content": "CUDA_FORCE_CDP1_IF_SUPPORTED defined Fire-and-forget stream usage requires compilation to be in 64-bit mode"
  },
  {
    "id": 4520,
    "content": "The Tail Launch Stream  The tail launch named stream ( cudaStreamTailLaunch ) allows a grid to schedule a new grid for launch after its completion It should be possible to to use a tail launch to achieve the same functionality as a cudaDeviceSynchronize() in most cases All non-tail launch work launched by a grid is implicitly synchronized before the tail stream is kicked off A parent grid’s tail"
  },
  {
    "id": 4521,
    "content": "launch does not launch until the parent grid and all work launched by the parent grid to ordinary streams or per-thread or fire-and-forget streams have completed If two grids are launched to the same grid’s tail launch stream, the later grid does not launch until the earlier grid and all its descendent work has completed ); } Grids launched into the tail launch stream will not launch until the"
  },
  {
    "id": 4522,
    "content": "completion of all work by the parent grid, including all other grids (and their descendants) launched by the parent in all non-tail launched streams, including work executed or launched after the tail launch ) } The next grid in the parent grid’s stream will not be launched before a parent grid’s tail launch work has completed In other words, the tail launch stream behaves as if it were inserted"
  },
  {
    "id": 4523,
    "content": "between its parent grid and the next grid in its parent grid’s stream In this example, C1 and C2 will launch concurrently after P's completion __global__ void T ( The tail launch stream is not supported when compiled with CUDA_FORCE_CDP1_IF_SUPPORTED defined Tail launch stream usage requires compilation to be in 64-bit mode"
  },
  {
    "id": 4529,
    "content": "This means that cudaStreamWaitEvent() is supported, but cudaEventSynchronize() , cudaEventElapsedTime() , and cudaEventQuery() are not As cudaEventElapsedTime() is not supported, cudaEvents must be created via cudaEventCreateWithFlags() , passing the cudaEventDisableTiming flag"
  },
  {
    "id": 4530,
    "content": "As with named streams, event objects may be shared between all threads within the grid which created them but are local to that grid and may not be passed to other kernels Event handles are not guaranteed to be unique between grids, so using an event handle within a grid that did not create it will result in undefined behavior"
  },
  {
    "id": 4535,
    "content": "Synchronization  It is up to the program to perform sufficient inter-thread synchronization, for example via a CUDA Event, if the calling thread is intended to synchronize with child grids invoked from other threads As it is not possible to explicitly synchronize child work from a parent thread, there is no way to guarantee that changes occuring in child grids are visible to threads within the"
  },
  {
    "id": 4541,
    "content": "Device Management  Only the device on which a kernel is running will be controllable from that kernel This means that device APIs such as cudaSetDevice() are not supported by the device runtime The active device as seen from the GPU (returned from cudaGetDevice() ) will have the same device number as seen from the host system The cudaDeviceGetAttribute() call may request information about"
  },
  {
    "id": 4542,
    "content": "another device as this API allows specification of a device ID as a parameter of the call Note that the catch-all cudaGetDeviceProperties() API is not offered by the device runtime - properties must be queried individually"
  },
  {
    "id": 4552,
    "content": "Device and Constant Memory  Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime All kernels may read or write device variables, whether the kernel was initially launched by the host or device runtime Equivalently, all kernels will have the same view of __constant__ s as declared at the module scope"
  },
  {
    "id": 4558,
    "content": "Textures and Surfaces  CUDA supports dynamically created texture and surface objects 14 , where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host The device runtime does not allow creation or destruction of texture or surface objects from within device code, but texture and surface objects created from the host may be used and"
  },
  {
    "id": 4559,
    "content": "passed around freely on the device Regardless of where they are created, dynamically created texture objects are always valid and may be passed to child kernels from a parent Note The device runtime does not support legacy module-scope (i"
  },
  {
    "id": 4561,
    "content": ", Fermi-style) textures and surfaces within a kernel launched from the device Module-scope (legacy) textures may be created from the host and used in device code as for any kernel, but may only be used by a top-level kernel (i"
  },
  {
    "id": 4569,
    "content": "Shared Memory Variable Declarations  In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernel’s caller via a launch configuration argument"
  },
  {
    "id": 4570,
    "content": "__global__ void permute ( int n , int * data ) { extern __shared__ int smem []; if ( n >> ( n / 2 , data ); permute >> ( n / 2 , data + n / 2 ); } } void host_launch ( int * data ) { permute >> ( 256 , data ); } 9"
  },
  {
    "id": 4577,
    "content": ", those marked __device__ ) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernel’s visible address space"
  },
  {
    "id": 4578,
    "content": "This also applies to __constant__ symbols, although in this case the pointer will reference read-only data Given that device-side symbols can be referenced directly, those CUDA runtime APIs which reference symbols (e"
  },
  {
    "id": 4580,
    "content": ", cudaMemcpyToSymbol() or cudaGetSymbolAddress() ) are redundant and hence not supported by the device runtime"
  },
  {
    "id": 4581,
    "content": "Note this implies that constant data cannot be altered from within a running kernel, even ahead of a child kernel launch, as references to __constant__ space are read-only"
  },
  {
    "id": 4586,
    "content": "API Errors and Launch Failures  As usual for the CUDA runtime, any function may return an error code The last error code returned is recorded and may be retrieved via the cudaGetLastError() call Errors are recorded per-thread, so that each thread can identify the most recent error that it has generated"
  },
  {
    "id": 4587,
    "content": "Similar to a host-side launch, device-side launches may fail for many reasons (invalid arguments, etc)"
  },
  {
    "id": 4588,
    "content": "The user must call cudaGetLastError() to determine if a launch generated an error, however lack of an error after launch does not imply the child kernel completed successfully"
  },
  {
    "id": 4597,
    "content": "Launch Setup APIs  Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs It is permitted for a CUDA application to call these APIs itself, with the same requirements as for PTX"
  },
  {
    "id": 4598,
    "content": "In both cases, the user is then responsible for correctly populating all necessary data structures in the correct format according to specification"
  },
  {
    "id": 4600,
    "content": "This is so that users targeting PTX will be able to enact a launch, and so that the compiler front-end can translate >> into these calls"
  },
  {
    "id": 4601,
    "content": "Table 9 New Device-only Launch Implementation Functions  Runtime API Launch Functions Description of Difference From Host Runtime Behaviour (behavior is identical if no description) cudaGetParameterBuffer Generated automatically from >> The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows: extern device cudaError_t cudaGetParameterBuffer"
  },
  {
    "id": 4602,
    "content": "( void ** params ); extern __device__ cudaError_t cudaLaunchDevice ( void * kernel , void * params , dim3 gridDim , dim3 blockDim , unsigned int sharedMemSize = 0 , cudaStream_t stream = 0 ); 9"
  },
  {
    "id": 4606,
    "content": "API Reference  The portions of the CUDA Runtime API supported in the device runtime are detailed here Host and device runtime APIs have identical syntax; semantics are the same except where indicated The following table provides an overview of the API relative to the version available from the host"
  },
  {
    "id": 4607,
    "content": "Device-side Launch from PTX  This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language"
  },
  {
    "id": 4613,
    "content": "Kernel Launch APIs  Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer() cudaLaunchDevice() launches the specified kernel with the parameter buffer that is obtained by calling cudaGetParameterBuffer() and filled with the parameters to the launched kernel The parameter buffer can be NULL, i"
  },
  {
    "id": 4621,
    "content": "cudaLaunchDevice  At the PTX level, cudaLaunchDevice() needs to be declared in one of the two forms shown below before it is used b64 stream ) ; The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api"
  },
  {
    "id": 4623,
    "content": "The function is defined in the cudadevrt system library, which must be linked with a program in order to use device-side kernel launch functionality"
  },
  {
    "id": 4624,
    "content": "CUDA-level declaration of cudaLaunchDevice() extern \"C\" __device__ cudaError_t cudaLaunchDevice ( void * func , void * parameterBuffer , dim3 gridDimension , dim3 blockDimension , unsigned int sharedMemSize , cudaStream_t stream ); The first parameter is a pointer to the kernel to be is launched, and the second parameter is the parameter buffer that holds the actual parameters to the launched"
  },
  {
    "id": 4628,
    "content": ", as grid dimension, block dimension, shared memory size, and the stream associated with the launch (please refer to Execution Configuration for the detailed description of launch configuration"
  },
  {
    "id": 4634,
    "content": "cudaGetParameterBuffer  cudaGetParameterBuffer() needs to be declared at the PTX level before it’s used The PTX-level declaration must be in one of the two forms given below, depending on address size:   PTX-level Declaration of cudaGetParameterBuffer() when"
  },
  {
    "id": 4636,
    "content": "b64 size ) ; The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration: CUDA-level Declaration of cudaGetParameterBuffer() extern \"C\" __device__ void * cudaGetParameterBuffer ( size_t alignment , size_t size ); The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement"
  },
  {
    "id": 4637,
    "content": "in bytes In the current implementation, the parameter buffer returned by cudaGetParameterBuffer() is always guaranteed to be 64- byte aligned, and the alignment requirement parameter is ignored However, it is recommended to pass the correct alignment requirement value - which is the largest alignment of any parameter to be placed in the parameter buffer - to cudaGetParameterBuffer() to ensure"
  },
  {
    "id": 4643,
    "content": "Parameter Buffer Layout  Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned That is, each parameter must be placed at the n th byte in the parameter buffer, where n is the smallest multiple of the parameter size that is greater than the offset of the last byte taken by the preceding parameter"
  },
  {
    "id": 4644,
    "content": "For a more detailed description of PTX code generated by the CUDA compiler, please refer to the PTX-3"
  },
  {
    "id": 4653,
    "content": "Including Device Runtime API in CUDA Code  Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation"
  },
  {
    "id": 4660,
    "content": "Compiling and Linking  When compiling and linking CUDA programs using dynamic parallelism with nvcc , the program will automatically link against the static device runtime library libcudadevrt The device runtime is offered as a static library ( cudadevrt lib on Windows, libcudadevrt a under Linux), against which a GPU application that uses the device runtime must be linked A device runtime"
  },
  {
    "id": 4661,
    "content": "program may be compiled and linked in a single step, if all required source files can be specified from the command line: $ nvcc -arch=sm_75 -rdc=true hello_world"
  },
  {
    "id": 4663,
    "content": "cu source files first to object files, and then link these together in a two-stage process: $ nvcc -arch=sm_75 -dc hello_world cu -o hello_world o $ nvcc -arch=sm_75 -rdc=true hello_world"
  },
  {
    "id": 4664,
    "content": "o -o hello -lcudadevrt Please see the Using Separate Compilation section of The CUDA Driver Compiler NVCC guide for more details"
  },
  {
    "id": 4670,
    "content": "API level device management, kernel launching, device memcpy, stream management, and event management are exposed from the device runtime"
  },
  {
    "id": 4671,
    "content": "Programming for the device runtime should be familiar to someone who already has experience with CUDA"
  },
  {
    "id": 4672,
    "content": "Device runtime syntax and semantics are largely the same as that of the host API, with any exceptions detailed earlier in this document"
  },
  {
    "id": 4673,
    "content": "The following example shows a simple Hello World program incorporating dynamic parallelism: #include __global__ void childKernel () { printf ( \"Hello \" ); } __global__ void tailKernel () { printf ( \"World \" ); } __global__ void parentKernel () { launch child childKernel >> (); if ( cudaSuccess = cudaGetLastError ()) { return ; } launch tail into cudaStreamTailLaunch stream implicitly"
  },
  {
    "id": 4674,
    "content": "synchronizes: waits for child to complete tailKernel >> (); } int main ( int argc , char * argv []) { launch parent parentKernel >> (); if ( cudaSuccess = cudaGetLastError ()) { return 1 ; } wait for parent to complete if ( cudaSuccess"
  },
  {
    "id": 4675,
    "content": "= cudaDeviceSynchronize ()) { return 2 ; } return 0 ; } This program may be built in a single step from the command line as follows: $ nvcc -arch=sm_75 -rdc=true hello_world"
  },
  {
    "id": 4679,
    "content": "Dynamic-parallelism-enabled Kernel Overhead  System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own This overhead arises from the device runtime’s execution tracking and management software and may result in decreased performance This overhead is, in general,"
  },
  {
    "id": 4684,
    "content": "Implementation Restrictions and Limitations  Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime"
  },
  {
    "id": 4685,
    "content": "Memory Footprint  The device runtime system software reserves memory for various management purposes, in particular a reservation for tracking pending grid launches Configuration controls are available to reduce the size of this reservation in exchange for certain launch limitations See Configuration Options , below, for details"
  },
  {
    "id": 4691,
    "content": "Pending Kernel Launches  When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes"
  },
  {
    "id": 4692,
    "content": "The size of the fixed-size launch pool is configurable by calling cudaDeviceSetLimit() from the host and specifying cudaLimitDevRuntimePendingLaunchCount"
  },
  {
    "id": 4698,
    "content": "Configuration Options  Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program"
  },
  {
    "id": 4699,
    "content": "Limits must be set before any kernel is launched, and may not be changed while the GPU is actively running programs The following named limits may be set: Limit Behavior cudaLimitDevRuntimePendingLaunchCount Controls the amount of memory set aside for buffering kernel launches and events which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources"
  },
  {
    "id": 4700,
    "content": "When the buffer is full, an attempt to allocate a launch slot during a device side kernel launch will fail and return cudaErrorLaunchOutOfResources , while an attempt to allocate an event slot will fail and return cudaErrorMemoryAllocation"
  },
  {
    "id": 4701,
    "content": "Applications may increase the number of launch and/or event slots by setting cudaLimitDevRuntimePendingLaunchCount"
  },
  {
    "id": 4702,
    "content": "The CUDA driver automatically increases the per-thread stack size for each kernel launch as needed To set the per-thread stack size to a different value, cudaDeviceSetLimit() can be called to set this limit"
  },
  {
    "id": 4703,
    "content": "The stack will be immediately resized, and if necessary, the device will block until all preceding requested tasks are complete"
  },
  {
    "id": 4710,
    "content": "Memory Allocation and Lifetime  cudaMalloc() and cudaFree() have distinct semantics between the host and device environments When invoked from the host, cudaMalloc() allocates a new region from unused device memory When invoked from the device runtime these functions map to device-side malloc() and free() This implies that within the device environment the total allocatable memory is limited to"
  },
  {
    "id": 4711,
    "content": "the device malloc() heap size, which may be smaller than the available unused device memory Also, it is an error to invoke cudaFree() from the host program on a pointer which was allocated by cudaMalloc() on the device or vice-versa cudaMalloc() on Host cudaMalloc() on Device cudaFree() on Host Supported Not Supported cudaFree() on Device Not Supported Supported Allocation limit Free device memory"
  },
  {
    "id": 4717,
    "content": "The device runtime may reschedule thread blocks onto different SMs in order to more efficiently manage resources"
  },
  {
    "id": 4718,
    "content": "As such, it is unsafe to rely upon %smid or %warpid remaining unchanged across the lifetime of a thread or thread block"
  },
  {
    "id": 4724,
    "content": "ECC Errors  No notification of ECC errors is available to code within a CUDA kernel Any ECC errors which arise during execution of a nested program will either generate an exception or continue execution (depending upon error and configuration)"
  },
  {
    "id": 4727,
    "content": "CDP2 vs CDP1  This section summarises the differences between, and the compatibility and interoperability of, the new (CDP2) and legacy (CDP1) CUDA Dynamic Parallelism interfaces It also shows how to opt-out of the CDP2 interface on devices of compute capability less than 9"
  },
  {
    "id": 4732,
    "content": "Differences Between CDP1 and CDP2  Explicit device-side synchronization is no longer possible with CDP2 or on devices of compute capability 9"
  },
  {
    "id": 4733,
    "content": "0 or higher Attempting to query or set cudaLimitDevRuntimeSyncDepth (or CU_LIMIT_DEV_RUNTIME_SYNC_DEPTH ) with CDP2 or on devices of compute capability 9 0 or higher results in cudaErrorUnsupportedLimit"
  },
  {
    "id": 4735,
    "content": "cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of launch slots"
  },
  {
    "id": 4736,
    "content": "For CDP2, there is a limit to the total number of events existing at once (note that events are destroyed only after a launch completes), equal to twice the pending launch count"
  },
  {
    "id": 4737,
    "content": "cudaLimitDevRuntimePendingLaunchCount must be set to be large enough to avoid running out of event slots"
  },
  {
    "id": 4738,
    "content": "Streams are tracked per grid with CDP2 or on devices of compute capability 9 0 or higher, not per thread block CDP2 introduces the tail launch ( cudaStreamTailLaunch ) and fire-and-forget ( cudaStreamFireAndForget ) named streams CDP2 is supported only under 64-bit compilation mode"
  },
  {
    "id": 4742,
    "content": "Compatibility and Interoperability  CDP2 is the default Functions can be compiled with -DCUDA_FORCE_CDP1_IF_SUPPORTED to opt-out of using CDP2 on devices of compute capability less than 9"
  },
  {
    "id": 4744,
    "content": "Function compiler with CUDA 12 0 and newer (default) Function compiled with pre-CUDA 12 0 or with CUDA 12 0 and newer with -DCUDA_FORCE_CDP1_IF_SUPPORTED specified Compilation Compile error if device code references cudaDeviceSynchronize Compile error if device code references cudaDeviceSynchronize and code is compiled for sm_90 or newer"
  },
  {
    "id": 4745,
    "content": "Compute capability >> ( data ); cudaDeviceSynchronize (); } __syncthreads (); } void host_launch ( int * data ) { parent_launch >> ( data ); } 9"
  },
  {
    "id": 4751,
    "content": "Zero-copy system memory has identical coherence and consistency guarantees to global memory, and follows the semantics detailed above"
  },
  {
    "id": 4759,
    "content": "Constants are immutable and may not be modified from the device, even between parent and child launches"
  },
  {
    "id": 4761,
    "content": "Constant memory is inherited automatically by all child kernels from their respective parents Taking the address of a constant memory object from within a kernel thread has the same semantics as for all CUDA programs, and passing that pointer from parent to child or from a child to parent is naturally supported"
  },
  {
    "id": 4768,
    "content": "Shared and Local Memory (CDP1)  See Shared and Local Memory , above, for CDP2 version of document Shared and Local memory is private to a thread block or thread, respectively, and is not visible or coherent between parent and child"
  },
  {
    "id": 4775,
    "content": "Local Memory (CDP1)  See Local Memory , above, for CDP2 version of document Local memory is private storage for an executing thread, and is not visible outside of that thread"
  },
  {
    "id": 4776,
    "content": "For example:   Correct - \"value\" is global storage __device__ int value ; __device__ void x () { value = 5 ; child >> ( & value ); }   Invalid - \"value\" is local storage __device__ void y () { int value = 5 ; child >> ( & value ); } 9"
  },
  {
    "id": 4782,
    "content": "Writes to the global memory region over which a texture is mapped are incoherent with respect to texture accesses Similarly, writes to memory by a child will be reflected in the texture memory accesses by a parent, but only after the parent synchronizes on the child’s completion"
  },
  {
    "id": 4794,
    "content": "This section describes changes and additions to the CUDA C++ language extensions for supporting Dynamic Parallelism"
  },
  {
    "id": 4800,
    "content": "Device-Side Kernel Launch (CDP1)  See Device-Side Kernel Launch , above, for CDP2 version of document"
  },
  {
    "id": 4801,
    "content": "Kernels may be launched from the device using the standard CUDA >> syntax: kernel_name >> ([ kernel arguments ]); Dg is of type dim3 and specifies the dimensions and size of the grid Db is of type dim3 and specifies the dimensions and size of each thread block Ns is of type size_t and specifies the number of bytes of shared memory that is dynamically allocated per thread block for this call and"
  },
  {
    "id": 4802,
    "content": "addition to statically allocated memory The stream must have been allocated in the same thread block where the call is being made"
  },
  {
    "id": 4810,
    "content": "Launches are Asynchronous (CDP1)  See Launches are Asynchronous , above, for CDP2 version of document Identical to host-side launches, all device-side kernel launches are asynchronous with respect to the launching thread"
  },
  {
    "id": 4811,
    "content": "That is to say, the >> launch command will return immediately and the launching thread will continue to execute until it hits an explicit launch-synchronization point such as cudaDeviceSynchronize() The grid launch is posted to the device and will execute independently of the parent thread The child grid may begin execution at any time after launch, but is not guaranteed to begin execution until"
  },
  {
    "id": 4819,
    "content": "Launch Environment Configuration (CDP1)  See Launch Environment Configuration , above, for CDP2 version of document"
  },
  {
    "id": 4820,
    "content": "All global device configuration settings (for example, shared memory and L1 cache size as returned from cudaDeviceGetCacheConfig() , and device limits returned from cudaDeviceGetLimit() ) will be inherited from the parent"
  },
  {
    "id": 4827,
    "content": "Named streams may be used by any thread within a thread-block, but stream handles may not be passed to other blocks or child/parent kernels In other words, a stream should be treated as private to the block in which it is created Stream handles are not guaranteed to be unique between blocks, so using a stream handle within a block that did not allocate it will result in undefined behavior As"
  },
  {
    "id": 4828,
    "content": "cudaStreamSynchronize() and cudaStreamQuery() are unsupported by the device runtime, cudaDeviceSynchronize() should be used instead when the application needs to know that stream-launched child kernels have completed"
  },
  {
    "id": 4835,
    "content": "The Implicit (NULL) Stream (CDP1)  See The Implicit (NULL) Stream , above, for CDP2 version of document Within a host program, the unnamed (NULL) stream has additional barrier synchronization semantics with other streams (see Default Stream for details) The device runtime offers a single implicit, unnamed stream shared between all threads in a block, but as all named streams must be created with"
  },
  {
    "id": 4836,
    "content": "the cudaStreamNonBlocking flag, work launched into the NULL stream will not insert an implicit dependency on pending work in any other streams (including NULL streams of other thread blocks)"
  },
  {
    "id": 4843,
    "content": "As for all device runtime objects, event objects may be shared between all threads within the thread-block which created them but are local to that block and may not be passed to other kernels, or between blocks within the same kernel Event handles are not guaranteed to be unique between blocks, so using an event handle within a block that did not create it will result in undefined behavior"
  },
  {
    "id": 4850,
    "content": "The cudaDeviceSynchronize() function will synchronize on all work launched by any thread in the thread-block up to the point where cudaDeviceSynchronize() was called Note that cudaDeviceSynchronize() may be called from within divergent code (see Block Wide Synchronization (CDP1) ) It is up to the program to perform sufficient additional inter-thread synchronization, for example via a call to"
  },
  {
    "id": 4851,
    "content": "__syncthreads() , if the calling thread is intended to synchronize with child grids invoked from other threads"
  },
  {
    "id": 4858,
    "content": "Block Wide Synchronization (CDP1)  See CUDA Dynamic Parallelism , above, for CDP2 version of document"
  },
  {
    "id": 4859,
    "content": "In particular, without explicit synchronization via a __syncthreads() directive the calling thread can make no assumptions about what work has been launched by any thread other than itself For example if multiple threads within a block are each launching work and synchronization is desired for all this work at once (perhaps because of event-based dependencies), it is up to the program to"
  },
  {
    "id": 4860,
    "content": "guarantee that this work is submitted by all threads before calling cudaDeviceSynchronize() Because the implementation is permitted to synchronize on launches from any thread in the block, it is quite possible that simultaneous calls to cudaDeviceSynchronize() by multiple threads will drain all work in the first call and then have no effect for the later calls"
  },
  {
    "id": 4880,
    "content": "Device and Constant Memory (CDP1)  See Device and Constant Memory , above, for CDP2 version of document Memory declared at file scope with __device__ or __constant__ memory space specifiers behaves identically when using the device runtime"
  },
  {
    "id": 4888,
    "content": "CUDA supports dynamically created texture and surface objects 14 , where a texture object may be created on the host, passed to a kernel, used by that kernel, and then destroyed from the host"
  },
  {
    "id": 4895,
    "content": "Shared Memory Variable Declarations (CDP1)  See Shared Memory Variable Declarations , above, for CDP2 version of document In CUDA C++ shared memory can be declared either as a statically sized file-scope or function-scoped variable, or as an extern variable with the size determined at runtime by the kernel’s caller via a launch configuration argument"
  },
  {
    "id": 4896,
    "content": "__global__ void permute ( int n , int * data ) { extern __shared__ int smem []; if ( n >> ( n / 2 , data ); permute >> ( n / 2 , data + n / 2 ); } } void host_launch ( int * data ) { permute >> ( 256 , data ); } 9"
  },
  {
    "id": 4904,
    "content": ", those marked __device__ ) may be referenced from within a kernel simply via the & operator, as all global-scope device variables are in the kernel’s visible address space"
  },
  {
    "id": 4910,
    "content": "API Errors and Launch Failures (CDP1)  See API Errors and Launch Failures , above, for CDP2 version of document"
  },
  {
    "id": 4913,
    "content": ", access to an invalid address, an error in a child grid will be returned to the host instead of being returned by the parent’s call to cudaDeviceSynchronize()"
  },
  {
    "id": 4920,
    "content": "Launch Setup APIs (CDP1)  See Launch Setup APIs , above, for CDP2 version of document Kernel launch is a system-level mechanism exposed through the device runtime library, and as such is available directly from PTX via the underlying cudaGetParameterBuffer() and cudaLaunchDevice() APIs Table 11 New Device-only Launch Implementation Functions  Runtime API Launch Functions Description of"
  },
  {
    "id": 4921,
    "content": "Difference From Host Runtime Behaviour (behavior is identical if no description) cudaGetParameterBuffer Generated automatically from >> The APIs for these launch functions are different to those of the CUDA Runtime API, and are defined as follows: extern device cudaError_t cudaGetParameterBuffer ( void ** params ); extern __device__ cudaError_t cudaLaunchDevice ( void * kernel , void * params ,"
  },
  {
    "id": 4927,
    "content": "The table below provides an overview of the API relative to the version available from the host Table 12 Supported API Functions  Runtime API Functions Details cudaDeviceSynchronize Synchronizes on work launched from thread’s own block only"
  },
  {
    "id": 4930,
    "content": "Device-side Launch from PTX (CDP1)  See Device-side Launch from PTX , above, for CDP2 version of document"
  },
  {
    "id": 4931,
    "content": "This section is for the programming language and compiler implementers who target Parallel Thread Execution (PTX) and plan to support Dynamic Parallelism in their language"
  },
  {
    "id": 4937,
    "content": "Kernel Launch APIs (CDP1)  See Kernel Launch APIs , above, for CDP2 version of document Device-side kernel launches can be implemented using the following two APIs accessible from PTX: cudaLaunchDevice() and cudaGetParameterBuffer()"
  },
  {
    "id": 4944,
    "content": "cudaLaunchDevice (CDP1)  See cudaLaunchDevice , above, for CDP2 version of document At the PTX level, cudaLaunchDevice() needs to be declared in one of the two forms shown below before it is used b32 stream ) ; The CUDA-level declaration below is mapped to one of the aforementioned PTX-level declarations and is found in the system header file cuda_device_runtime_api"
  },
  {
    "id": 4953,
    "content": "cudaGetParameterBuffer (CDP1)  See cudaGetParameterBuffer , above, for CDP2 version of document The PTX-level declaration must be in one of the two forms given below, depending on address size:   PTX-level Declaration of cudaGetParameterBuffer() when"
  },
  {
    "id": 4957,
    "content": "b32 size ) ; The following CUDA-level declaration of cudaGetParameterBuffer() is mapped to the aforementioned PTX-level declaration: CUDA-level Declaration of cudaGetParameterBuffer() extern \"C\" __device__ void * cudaGetParameterBuffer ( size_t alignment , size_t size ); The first parameter specifies the alignment requirement of the parameter buffer and the second parameter the size requirement"
  },
  {
    "id": 4964,
    "content": "Parameter Buffer Layout (CDP1)  See Parameter Buffer Layout , above, for CDP2 version of document Parameter reordering in the parameter buffer is prohibited, and each individual parameter placed in the parameter buffer is required to be aligned"
  },
  {
    "id": 4969,
    "content": "Toolkit Support for Dynamic Parallelism (CDP1)  See Toolkit Support for Dynamic Parallelism , above, for CDP2 version of document"
  },
  {
    "id": 4975,
    "content": "Including Device Runtime API in CUDA Code (CDP1)  See Including Device Runtime API in CUDA Code , above, for CDP2 version of document Similar to the host-side runtime API, prototypes for the CUDA device runtime API are included automatically during program compilation"
  },
  {
    "id": 4981,
    "content": "Compiling and Linking (CDP1)  See Compiling and Linking , above, for CDP2 version of document When compiling and linking CUDA programs using dynamic parallelism with nvcc , the program will automatically link against the static device runtime library libcudadevrt"
  },
  {
    "id": 4986,
    "content": "The following example shows a simple Hello World program incorporating dynamic parallelism: #include __global__ void childKernel () { printf ( \"Hello \" ); } __global__ void parentKernel () { launch child childKernel >> (); if ( cudaSuccess = cudaGetLastError ()) { return ; } wait for child to complete if ( cudaSuccess = cudaDeviceSynchronize ()) { return ; } printf ( \"World \" ); } int main ( int"
  },
  {
    "id": 4987,
    "content": "argc , char * argv []) { launch parent parentKernel >> (); if ( cudaSuccess = cudaGetLastError ()) { return 1 ; } wait for parent to complete if ( cudaSuccess = cudaDeviceSynchronize ()) { return 2 ; } return 0 ; } This program may be built in a single step from the command line as follows: $ nvcc -arch=sm_75 -rdc=true hello_world cu -o hello -lcudadevrt 9"
  },
  {
    "id": 4998,
    "content": "Warning Explicit synchronization with child kernels from a parent block (such as using cudaDeviceSynchronize() in device code) is deprecated in CUDA 11"
  },
  {
    "id": 5000,
    "content": "Synchronization by one thread may impact the performance of other threads in the same Thread Block , even when those other threads do not call cudaDeviceSynchronize() themselves In general the implicit synchronization of child kernels done when a thread block ends is more efficient compared to calling cudaDeviceSynchronize() explicitly It is therefore recommended to only call"
  },
  {
    "id": 5001,
    "content": "cudaDeviceSynchronize() if it is needed to synchronize with a child kernel before a thread block ends"
  },
  {
    "id": 5007,
    "content": "Dynamic-parallelism-enabled Kernel Overhead (CDP1)  See Dynamic-parallelism-enabled Kernel Overhead , above, for CDP2 version of document System software which is active when controlling dynamic launches may impose an overhead on any kernel which is running at the time, whether or not it invokes kernel launches of its own"
  },
  {
    "id": 5008,
    "content": "This overhead arises from the device runtime’s execution tracking and management software and may result in decreased performance for example, library calls when made from the device compared to from the host side"
  },
  {
    "id": 5013,
    "content": "Implementation Restrictions and Limitations (CDP1)  See Implementation Restrictions and Limitations , above, for CDP2 version of document Dynamic Parallelism guarantees all semantics described in this document, however, certain hardware and software resources are implementation-dependent and limit the scale, performance and other properties of a program which uses the device runtime"
  },
  {
    "id": 5027,
    "content": "The device runtime system software reserves memory for various management purposes, in particular one reservation which is used for saving parent-grid state during synchronization, and a second reservation for tracking pending grid launches"
  },
  {
    "id": 5028,
    "content": "Configuration controls are available to reduce the size of these reservations in exchange for certain launch limitations"
  },
  {
    "id": 5029,
    "content": "The majority of reserved memory is allocated as backing-store for parent kernel state, for use when synchronizing on a child launch Conservatively, this memory must support storing of state for the maximum number of live threads possible on the device This means that each parent generation at which cudaDeviceSynchronize() is callable may require up to 860MB of device memory, depending on the"
  },
  {
    "id": 5037,
    "content": "Nesting and Synchronization Depth (CDP1)  See CUDA Dynamic Parallelism , above, for CDP2 version of document"
  },
  {
    "id": 5038,
    "content": "Using the device runtime, one kernel may launch another kernel, and that kernel may launch another, and so on"
  },
  {
    "id": 5039,
    "content": "Each subordinate launch is considered a new nesting level , and the total number of levels is the nesting depth of the program The synchronization depth is defined as the deepest level at which the program will explicitly synchronize on a child launch Typically this is one less than the nesting depth of the program, but if the program does not need to call cudaDeviceSynchronize() at all levels"
  },
  {
    "id": 5040,
    "content": "then the synchronization depth might be substantially different to the nesting depth The overall maximum nesting depth is limited to 24, but practically speaking the real limit will be the amount of memory required by the system for each new level (see Memory Footprint (CDP1) above)"
  },
  {
    "id": 5042,
    "content": "This maximum synchronization depth (and hence reserved storage) may be controlled by calling cudaDeviceSetLimit() and specifying cudaLimitDevRuntimeSyncDepth"
  },
  {
    "id": 5043,
    "content": "The number of levels to be supported must be configured before the top-level kernel is launched from the host, in order to guarantee successful execution of a nested program"
  },
  {
    "id": 5044,
    "content": "Calling cudaDeviceSynchronize() at a depth greater than the specified maximum synchronization depth will return an error"
  },
  {
    "id": 5045,
    "content": "An optimization is permitted where the system detects that it need not reserve space for the parent’s state in cases where the parent kernel never calls cudaDeviceSynchronize() In this case, because explicit parent/child synchronization never occurs, the memory footprint required for a program will be much less than the conservative maximum"
  },
  {
    "id": 5046,
    "content": "Such a program could specify a shallower maximum synchronization depth to avoid over-allocation of backing store"
  },
  {
    "id": 5054,
    "content": "When a kernel is launched, all associated configuration and parameter data is tracked until the kernel completes"
  },
  {
    "id": 5055,
    "content": "The launch pool is divided into a fixed-size pool and a virtualized pool with lower performance The device runtime system software will try to track launch data in the fixed-size pool first The virtualized pool will be used to track new launches when the fixed-size pool is full"
  },
  {
    "id": 5063,
    "content": "Resource allocation for the device runtime system software is controlled via the cudaDeviceSetLimit() API from the host program"
  },
  {
    "id": 5064,
    "content": "The following named limits may be set: Limit Behavior cudaLimitDevRuntimeSyncDepth Sets the maximum depth at which cudaDeviceSynchronize() may be called"
  },
  {
    "id": 5065,
    "content": "Launches may be performed deeper than this, but explicit synchronization deeper than this limit will return the cudaErrorLaunchMaxDepthExceeded"
  },
  {
    "id": 5066,
    "content": "cudaLimitDevRuntimePendingLaunchCount Controls the amount of memory set aside for buffering kernel launches which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources"
  },
  {
    "id": 5067,
    "content": "When the buffer is full, the device runtime system software will attempt to track new pending launches in a lower performance virtualized buffer when all available heap space is consumed, launches will not occur, and the thread’s last error will be set to cudaErrorLaunchPendingCountExceeded The default pending launch count is 2048 launches"
  },
  {
    "id": 5074,
    "content": "Memory Allocation and Lifetime (CDP1)  See Memory Allocation and Lifetime , above, for CDP2 version of document"
  },
  {
    "id": 5075,
    "content": "cudaMalloc() and cudaFree() have distinct semantics between the host and device environments cudaMalloc() on Host cudaMalloc() on Device cudaFree() on Host Supported Not Supported cudaFree() on Device Not Supported Supported Allocation limit Free device memory cudaLimitMallocHeapSize 9"
  },
  {
    "id": 5089,
    "content": "14 ( 1 , 2 , 3 ) Dynamically created texture and surface objects are an addition to the CUDA memory model introduced with CUDA 5"
  },
  {
    "id": 5091,
    "content": "Introduction  The Virtual Memory Management APIs provide a way for the application to directly manage the unified virtual address space that CUDA provides to map physical memory to virtual addresses accessible by the GPU"
  },
  {
    "id": 5093,
    "content": "2, these APIs additionally provide a new way to interop with other processes and graphics APIs like OpenGL and Vulkan, as well as provide newer memory attributes that a user can tune to fit their applications"
  },
  {
    "id": 5094,
    "content": "Historically, memory allocation calls (such as cudaMalloc() ) in the CUDA programming model have returned a memory address that points to the GPU memory In order to increase an allocation’s size, the user had to explicitly allocate a larger buffer, copy data from the initial allocation, free it and then continue to keep track of the newer allocation’s address"
  },
  {
    "id": 5096,
    "content": "Essentially, users had a malloc-like interface for allocating GPU memory, but did not have a corresponding realloc to complement it"
  },
  {
    "id": 5097,
    "content": "The Virtual Memory Management APIs decouple the idea of an address and memory and allow the application to handle them separately The APIs allow applications to map and unmap memory from a virtual address range as they see fit"
  },
  {
    "id": 5098,
    "content": "In the case of enabling peer device access to memory allocations by using cudaEnablePeerAccess , all past and future user allocations are mapped to the target peer device This lead to users unwittingly paying runtime cost of mapping all cudaMalloc allocations to peer devices However, in most situations applications communicate by sharing only a few allocations with another device and not all"
  },
  {
    "id": 5099,
    "content": "allocations are required to be mapped to all the devices With Virtual Memory Management, applications can specifically choose certain allocations to be accessible from target devices The CUDA Virtual Memory Management APIs expose fine grained control to the user for managing the GPU memory in applications"
  },
  {
    "id": 5100,
    "content": "It provides APIs that let users: Place memory allocated on different devices into a contiguous VA range"
  },
  {
    "id": 5101,
    "content": "In order to allocate memory, the Virtual Memory Management programming model exposes the following functionality: Allocating physical memory"
  },
  {
    "id": 5105,
    "content": "Query for Support  Before attempting to use Virtual Memory Management APIs, applications must ensure that the devices they want to use support CUDA Virtual Memory Management The following code sample shows querying for Virtual Memory Management support: int deviceSupportsVmm ; CUresult result = cuDeviceGetAttribute ( & deviceSupportsVmm , CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED ,"
  },
  {
    "id": 5108,
    "content": "Allocating Physical Memory  The first step in memory allocation using Virtual Memory Management APIs is to create a physical memory chunk that will provide a backing for the allocation The function argument CUmemGenericAllocationHandle describes the properties of the memory to allocate such as the location of the allocation, if the allocation is going to be shared to another process (or other"
  },
  {
    "id": 5109,
    "content": "Graphics APIs), or the physical attributes of the memory to be allocated Users must ensure the requested allocation’s size must be aligned to appropriate granularity Information regarding an allocation’s granularity requirements can be queried using cuMemGetAllocationGranularity The following code snippet shows allocating physical memory with cuMemCreate : CUmemGenericAllocationHandle"
  },
  {
    "id": 5110,
    "content": "allocatePhysicalMemory ( int device , size_t size ) { CUmemAllocationProp prop = {}; prop id = device ; cuMemGetAllocationGranularity ( & granularity , & prop , CU_MEM_ALLOC_GRANULARITY_MINIMUM ); Ensure size matches granularity requirements for the allocation size_t padded_size = ROUND_UP ( size , granularity ); Allocate physical memory CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( &"
  },
  {
    "id": 5111,
    "content": "allocHandle , padded_size , & prop , 0 ); return allocHandle ; } The memory allocated by cuMemCreate is referenced by the CUmemGenericAllocationHandle it returns This is a departure from the cudaMalloc-style of allocation, which returns a pointer to the GPU memory, which was directly accessible by CUDA kernel executing on the device The memory allocated cannot be used for any operations other than"
  },
  {
    "id": 5112,
    "content": "querying properties using cuMemGetAllocationPropertiesFromHandle In order to make this memory accessible, applications must map this memory into a VA range reserved by cuMemAddressReserve and provide suitable access rights to it Applications must free the allocated memory using the cuMemRelease API"
  },
  {
    "id": 5116,
    "content": "Shareable Memory Allocations  With cuMemCreate users now have the facility to indicate to CUDA, at allocation time, that they have earmarked a particular allocation for Inter process communication and graphics interop purposes"
  },
  {
    "id": 5117,
    "content": "Applications can do this by setting CUmemAllocationProp::requestedHandleTypes to a platform-specific field On Windows, when CUmemAllocationProp::requestedHandleTypes is set to CU_MEM_HANDLE_TYPE_WIN32 applications must also specify an LPSECURITYATTRIBUTES attribute in CUmemAllocationProp::win32HandleMetaData"
  },
  {
    "id": 5118,
    "content": "This security attribute defines the scope of which exported allocations may be transferred to other processes"
  },
  {
    "id": 5119,
    "content": "The CUDA Virtual Memory Management API functions do not support the legacy interprocess communication functions with their memory Instead, they expose a new mechanism for interprocess communication that uses OS-specific handles Applications can obtain these OS-specific handles corresponding to the allocations by using cuMemExportToShareableHandle The handles thus obtained can be transferred by"
  },
  {
    "id": 5120,
    "content": "using the usual OS native mechanisms for inter process communication The recipient process should import the allocation by using cuMemImportFromShareableHandle"
  },
  {
    "id": 5121,
    "content": "Users must ensure they query for support of the requested handle type before attempting to export memory allocated with cuMemCreate The following code snippet illustrates query for handle type support in a platform-specific way"
  },
  {
    "id": 5122,
    "content": "int deviceSupportsIpcHandle ; #if defined(__linux__) cuDeviceGetAttribute ( & deviceSupportsIpcHandle , CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED , device )); #else cuDeviceGetAttribute ( & deviceSupportsIpcHandle , CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED , device )); #endif Users should set the CUmemAllocationProp::requestedHandleTypes appropriately as shown"
  },
  {
    "id": 5124,
    "content": "#endif The memMapIpcDrv sample can be used as an example for using IPC with Virtual Memory Management allocations"
  },
  {
    "id": 5129,
    "content": "2, applications had no user-controlled way of allocating any special type of memory that certain devices may support"
  },
  {
    "id": 5130,
    "content": "With cuMemCreate , applications can additionally specify memory type requirements using the CUmemAllocationProp::allocFlags to opt into any specific memory features Applications must also ensure that the requested memory type is supported on the device of allocation"
  },
  {
    "id": 5135,
    "content": "Compressible Memory  Compressible memory can be used to accelerate accesses to data with unstructured sparsity and other compressible data patterns"
  },
  {
    "id": 5136,
    "content": "Compression can save DRAM bandwidth, L2 read bandwidth and L2 capacity depending on the data being operated on"
  },
  {
    "id": 5137,
    "content": "Applications that want to allocate compressible memory on devices that support Compute Data Compression can do so by setting CUmemAllocationProp::allocFlags::compressionType to CU_MEM_ALLOCATION_COMP_GENERIC Users must query if device supports Compute Data Compression by using CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED The following code snippet illustrates querying compressible memory"
  },
  {
    "id": 5138,
    "content": "support cuDeviceGetAttribute int compressionSupported = 0 ; cuDeviceGetAttribute ( & compressionSupported , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , device ); On devices that support Compute Data Compression, users must opt in at allocation time as shown below: prop compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; Due to various reasons such as limited HW resources, the allocation may"
  },
  {
    "id": 5139,
    "content": "not have compression attributes, the user is expected to query back the properties of the allocated memory using cuMemGetAllocationPropertiesFromHandle and check for compression attribute"
  },
  {
    "id": 5140,
    "content": "CUmemAllocationPropPrivate allocationProp = {}; cuMemGetAllocationPropertiesFromHandle ( & allocationProp , allocationHandle ); if ( allocationProp"
  },
  {
    "id": 5141,
    "content": "compressionType == CU_MEM_ALLOCATION_COMP_GENERIC ) {   Obtained compressible memory allocation } 10"
  },
  {
    "id": 5143,
    "content": "Reserving a Virtual Address Range  Since with Virtual Memory Management the notions of address and memory are distinct, applications must carve out an address range that can hold the memory allocations made by cuMemCreate The address range reserved must be at least as large as the sum of the sizes of all the physical memory allocations the user plans to place in them Applications can reserve a"
  },
  {
    "id": 5144,
    "content": "virtual address range by passing appropriate parameters to cuMemAddressReserve The address range obtained will not have any device or host physical memory associated with it The reserved virtual address range can be mapped to memory chunks belonging to any device in the system, thus providing the application a continuous VA range backed and mapped by memory belonging to different devices"
  },
  {
    "id": 5146,
    "content": "These functions are conceptually similar to mmap/munmap (on Linux) or VirtualAlloc/VirtualFree (on Windows) functions"
  },
  {
    "id": 5147,
    "content": "The following code snippet illustrates the usage for the function: CUdeviceptr ptr ;   `ptr` holds the returned start of virtual address range reserved"
  },
  {
    "id": 5148,
    "content": "CUresult result = cuMemAddressReserve ( & ptr , size , 0 , 0 , 0 );   alignment = 0 for default alignment 10"
  },
  {
    "id": 5150,
    "content": "Virtual Aliasing Support  The Virtual Memory Management APIs provide a way to create multiple virtual memory mappings or “proxies” to the same allocation using multiple calls to cuMemMap with different virtual addresses, so-called virtual aliasing"
  },
  {
    "id": 5151,
    "content": "Unless otherwise noted in the PTX ISA, writes to one proxy of the allocation are considered inconsistent and incoherent with any other proxy of the same memory until the writing device operation (grid launch, memcpy, memset, and so on) completes Grids present on the GPU prior to a writing device operation but reading after the writing device operation completes are also considered to have"
  },
  {
    "id": 5152,
    "content": "inconsistent and incoherent proxies For example, the following snippet is considered undefined, assuming device pointers A and B are virtual aliases of the same memory allocation: __global__ void foo ( char * A , char * B ) { * A = 0x1 ; printf ( \"%d \" , * B ); Undefined behavior"
  },
  {
    "id": 5154,
    "content": "} The following is defined behavior, assuming these two kernels are ordered monotonically (by streams or events)"
  },
  {
    "id": 5155,
    "content": "__global__ void foo1 ( char * A ) { * A = 0x1 ; } __global__ void foo2 ( char * B ) { printf ( \"%d \" , * B ); *B == *A == 0x1 assuming foo2 waits for foo1 to complete before launching } cudaMemcpyAsync ( B , input , size , stream1 ); Aliases are allowed at operation boundaries foo1 >> ( A ); allowing foo1 to access A cudaEventRecord ( event , stream1 ); cudaStreamWaitEvent ( stream2 , event );"
  },
  {
    "id": 5156,
    "content": "foo2 >> ( B ); cudaStreamWaitEvent ( stream3 , event ); cudaMemcpyAsync ( output , B , size , stream3 ); Both launches of foo2 and cudaMemcpy (which both read) wait for foo1 (which writes) to complete before proceeding 10"
  },
  {
    "id": 5158,
    "content": "Mapping Memory  The allocated physical memory and the carved out virtual address space from the previous two sections represent the memory and address distinction introduced by the Virtual Memory Management APIs For the allocated memory to be useable, the user must first place the memory in the address space The address range obtained from cuMemAddressReserve and the physical allocation obtained"
  },
  {
    "id": 5159,
    "content": "from cuMemCreate or cuMemImportFromShareableHandle must be associated with each other by using cuMemMap Users can associate allocations from multiple devices to reside in contiguous virtual address ranges as long as they have carved out enough address space In order to decouple the physical allocation and the address range, users must unmap the address of the mapping by using cuMemUnmap Users can"
  },
  {
    "id": 5160,
    "content": "map and unmap memory to the same address range as many times as they want, as long as they ensure that they don’t attempt to create mappings on VA range reservations that are already mapped The following code snippet illustrates the usage for the function: CUdeviceptr ptr ; `ptr`: address in the address range previously reserved by cuMemAddressReserve Controlling Access Rights  The Virtual Memory"
  },
  {
    "id": 5161,
    "content": "Management APIs enable applications to explicitly protect their VA ranges with access control mechanisms Mapping the allocation to a region of the address range using cuMemMap does not make the address accessible, and would result in a program crash if accessed by a CUDA kernel Users must specifically select access control using the cuMemSetAccess function, which allows or restricts access for"
  },
  {
    "id": 5162,
    "content": "specific devices to a mapped address range The following code snippet illustrates the usage for the function: void setAccessOnDevice ( int device , CUdeviceptr ptr , size_t size ) { CUmemAccessDesc accessDesc = {}; accessDesc flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; Make the address accessible cuMemSetAccess ( ptr , size , & accessDesc , 1 ); } The access control mechanism exposed with Virtual"
  },
  {
    "id": 5163,
    "content": "Memory Management allows users to be explicit about which allocations they want to share with other peer devices on the system"
  },
  {
    "id": 5164,
    "content": "As specified earlier, cudaEnablePeerAccess forces all prior and future cudaMalloc’d allocations to be mapped to the target peer device"
  },
  {
    "id": 5165,
    "content": "This can be convenient in many cases as user doesn’t have to worry about tracking the mapping state of every allocation to every device in the system"
  },
  {
    "id": 5166,
    "content": "But for users concerned with performance of their applications this approach has performance implications"
  },
  {
    "id": 5167,
    "content": "With access control at allocation granularity Virtual Memory Management exposes a mechanism to have peer mappings with minimal overhead The vectorAddMMAP sample can be used as an example for using the Virtual Memory Management APIs"
  },
  {
    "id": 5170,
    "content": "Introduction  Managing memory allocations using cudaMalloc and cudaFree causes GPU to synchronize across all executing CUDA streams"
  },
  {
    "id": 5171,
    "content": "The Stream Order Memory Allocator enables applications to order memory allocation and deallocation with other work launched into a CUDA stream such as kernel launches and asynchronous copies This improves application memory use by taking advantage of stream-ordering semantics to reuse memory allocations The allocator also allows applications to control the allocator’s memory caching behavior When"
  },
  {
    "id": 5172,
    "content": "set up with an appropriate release threshold, the caching behavior allows the allocator to avoid expensive calls into the OS when the application indicates it is willing to accept a bigger memory footprint For many applications, the Stream Ordered Memory Allocator reduces the need for custom memory management abstractions, and makes it easier to create high-performance custom memory management for"
  },
  {
    "id": 5173,
    "content": "applications that need it For applications and libraries that already have custom memory allocators, adopting the Stream Ordered Memory Allocator enables multiple libraries to share a common pool of memory managed by the driver, thus reducing excess memory consumption Additionally, the driver can perform optimizations based on its awareness of the allocator and other stream management APIs"
  },
  {
    "id": 5174,
    "content": "Finally, Nsight Compute and the Next-Gen CUDA debugger is aware of the allocator as part of their CUDA 11"
  },
  {
    "id": 5178,
    "content": "Query for Support  The user can determine whether or not a device supports the stream ordered memory allocator by calling cudaDeviceGetAttribute() with the device attribute cudaDevAttrMemoryPoolsSupported"
  },
  {
    "id": 5180,
    "content": "3, IPC memory pool support can be queried with the cudaDevAttrMemoryPoolSupportedHandleTypes device attribute"
  },
  {
    "id": 5181,
    "content": "Previous drivers will return cudaErrorInvalidValue as those drivers are unaware of the attribute enum"
  },
  {
    "id": 5182,
    "content": "int driverVersion = 0 ; int deviceSupportsMemoryPools = 0 ; int poolSupportedHandleTypes = 0 ; cudaDriverGetVersion ( & driverVersion ); if ( driverVersion >= 11020 ) { cudaDeviceGetAttribute ( & deviceSupportsMemoryPools , cudaDevAttrMemoryPoolsSupported , device ); } if ( deviceSupportsMemoryPools = 0 ) { `device` supports the Stream Ordered Memory Allocator } if ( driverVersion >= 11030 ) {"
  },
  {
    "id": 5183,
    "content": "cudaDeviceGetAttribute ( & poolSupportedHandleTypes , cudaDevAttrMemoryPoolSupportedHandleTypes , device ); } if ( poolSupportedHandleTypes & cudaMemHandleTypePosixFileDescriptor ) { Pools on the specified device can be created with posix file descriptor-based IPC } Performing the driver version check before the query avoids hitting a cudaErrorInvalidValue error on drivers where the attribute was"
  },
  {
    "id": 5188,
    "content": "API Fundamentals (cudaMallocAsync and cudaFreeAsync)  The APIs cudaMallocAsync and cudaFreeAsync form the core of the allocator"
  },
  {
    "id": 5189,
    "content": "Both APIs accept stream arguments to define when the allocation will become and stop being available for use"
  },
  {
    "id": 5190,
    "content": "The pointer value returned by cudaMallocAsync is determined synchronously and is available for constructing future work"
  },
  {
    "id": 5191,
    "content": "It is important to note that cudaMallocAsync ignores the current device/context when determining where the allocation will reside Instead, cudaMallocAsync determines the resident device based on the specified memory pool or the supplied stream"
  },
  {
    "id": 5193,
    "content": "void * ptr ; size_t size = 512 ; cudaMallocAsync ( & ptr , size , cudaStreamPerThread ); do work using the allocation kernel >> ( ptr , ); An asynchronous free can be specified without synchronizing the cpu and GPU cudaFreeAsync ( ptr , cudaStreamPerThread ); When using an allocation in a stream other than the allocating stream, the user must guarantee that the access will happen after the"
  },
  {
    "id": 5194,
    "content": "allocation operation, otherwise the behavior is undefined The user may make this guarantee either by synchronizing the allocating stream, or by using CUDA events to synchronize the producing and consuming streams The user must guarantee that the free operation happens after the allocation operation and any use of the allocation Also, any use of the allocation after the free operation starts"
  },
  {
    "id": 5195,
    "content": "results in undefined behavior Events and/or stream synchronizing operations should be used to guarantee any access to the allocation on other streams is complete before the freeing stream begins the free operation cudaMallocAsync ( & ptr , size , stream1 ); cudaEventRecord ( event1 , stream1 ); stream2 must wait for the allocation to be ready before accessing cudaStreamWaitEvent ( stream2 , event1"
  },
  {
    "id": 5196,
    "content": "); kernel >> ( ptr , ); cudaEventRecord ( event2 , stream2 ); stream3 must wait for stream2 to finish accessing the allocation before freeing the allocation cudaStreamWaitEvent ( stream3 , event2 ); cudaFreeAsync ( ptr , stream3 ); The user can free allocations allocated with cudaMalloc() with cudaFreeAsync() The user must make the same guarantees about accesses being complete before the free"
  },
  {
    "id": 5197,
    "content": "operation begins cudaMalloc ( & ptr , size ); kernel >> ( ptr , ); cudaFreeAsync ( ptr , stream ); The user can free memory allocated with cudaMallocAsync with cudaFree() When freeing such allocations through the cudaFree() API, the driver assumes that all accesses to the allocation are complete and performs no further synchronization The user can use cudaStreamQuery / cudaStreamSynchronize /"
  },
  {
    "id": 5198,
    "content": "cudaEventQuery / cudaEventSynchronize / cudaDeviceSynchronize to guarantee that the appropriate asynchronous work is complete and that the GPU will not try to access the allocation cudaMallocAsync ( & ptr , size , stream ); kernel >> ( ptr , ); synchronize is needed to avoid prematurely freeing the memory cudaStreamSynchronize ( stream ); cudaFree ( ptr ); 11"
  },
  {
    "id": 5200,
    "content": "Memory Pools and the cudaMemPool_t  Memory pools encapsulate virtual address and physical memory resources that are allocated and managed according to the pools attributes and properties"
  },
  {
    "id": 5201,
    "content": "In the absence of a specified memory pool, cudaMallocAsync uses the current memory pool of the supplied stream’s device The current memory pool for a device may be set with cudaDeviceSetMempool and queried with cudaDeviceGetMempool By default (in the absence of a cudaDeviceSetMempool call), the current memory pool is the default memory pool of a device The API cudaMallocFromPoolAsync and c++"
  },
  {
    "id": 5202,
    "content": "overloads of cudaMallocAsync allow a user to specify the pool to be used for an allocation without setting it as the current pool"
  },
  {
    "id": 5204,
    "content": "So allocating without specifying a memory pool will always yield an allocation local to the stream’s device"
  },
  {
    "id": 5208,
    "content": "Default/Implicit Pools  The default memory pool of a device may be retrieved with the cudaDeviceGetDefaultMempool API Allocations from the default memory pool of a device are non-migratable device allocation located on that device The accessibility of the default memory pool may be modified with cudaMemPoolSetAccess and queried by cudaMemPoolGetAccess Since the default pools do not need to be"
  },
  {
    "id": 5209,
    "content": "explicitly created, they are sometimes referred to as implicit pools The default memory pool of a device does not support IPC"
  },
  {
    "id": 5213,
    "content": "This allows applications to request properties for their allocation beyond what is provided by the default/implict pools"
  },
  {
    "id": 5214,
    "content": "These include properties such as IPC capability, maximum pool size, allocations resident on a specific CPU NUMA node on supported platforms etc"
  },
  {
    "id": 5215,
    "content": "create a pool similar to the implicit pool on device 0 int device = 0 ; cudaMemPoolProps poolProps = { }; poolProps type = cudaMemLocationTypeDevice ; cudaMemPoolCreate ( & memPool , & poolProps )); The following code snippet illustrates an example of creating an IPC capable memory pool on a valid CPU NUMA node create a pool resident on a CPU NUMA node that is capable of IPC sharing (via a file"
  },
  {
    "id": 5216,
    "content": "descriptor) handleType = cudaMemHandleTypePosixFileDescriptor ; cudaMemPoolCreate ( & ipcMemPool , & poolProps )); 11"
  },
  {
    "id": 5218,
    "content": "Physical Page Caching Behavior  By default, the allocator tries to minimize the physical memory owned by a pool To minimize the OS calls to allocate and free physical memory, applications must configure a memory footprint for each pool"
  },
  {
    "id": 5219,
    "content": "Applications can do this with the release threshold attribute ( cudaMemPoolAttrReleaseThreshold ) The release threshold is the amount of memory in bytes a pool should hold onto before trying to release memory back to the OS When more than the release threshold bytes of memory are held by the memory pool, the allocator will try to release memory back to the OS on the next call to stream, event or"
  },
  {
    "id": 5220,
    "content": "device synchronize Setting the release threshold to UINT64_MAX will prevent the driver from attempting to shrink the pool after every synchronization Cuuint64_t setVal = UINT64_MAX ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReleaseThreshold , & setVal ); Applications that set cudaMemPoolAttrReleaseThreshold high enough to effectively disable memory pool shrinking may wish to explicitly"
  },
  {
    "id": 5221,
    "content": "shrink a memory pool’s memory footprint When trimming a memory pool’s footprint, the minBytesToKeep parameter allows an application to hold onto an amount of memory it expects to need in a subsequent phase of execution Cuuint64_t setVal = UINT64_MAX ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReleaseThreshold , & setVal ); application phase needing a lot of memory from the stream ordered"
  },
  {
    "id": 5223,
    "content": "); for ( j = 0 ; j reserved ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrReservedMemHigh , statistics -> reservedHigh ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrUsedMemCurrent , statistics -> used ); cudaMemPoolGetAttribute ( memPool , cudaMemPoolAttrUsedMemHigh , statistics -> usedHigh ); } resetting the watermarks will make them take on the current value void resetStatistics"
  },
  {
    "id": 5224,
    "content": "( cudaMemoryPool_t memPool ) { cuuint64_t value = 0 ; cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrReservedMemHigh , & value ); cudaMemPoolSetAttribute ( memPool , cudaMemPoolAttrUsedMemHigh , & value ); } 11"
  },
  {
    "id": 5226,
    "content": "Memory Reuse Policies  In order to service an allocation request, the driver attempts to reuse memory that was previously freed via cudaFreeAsync() before attempting to allocate more memory from the OS For example, memory freed in a stream can immediately be reused for a subsequent allocation request in the same stream Similarly, when a stream is synchronized with the CPU, the memory that was"
  },
  {
    "id": 5228,
    "content": "The pool attributes cudaMemPoolReuseFollowEventDependencies , cudaMemPoolReuseAllowOpportunistic , and cudaMemPoolReuseAllowInternalDependencies control these policies"
  },
  {
    "id": 5233,
    "content": "cudaMemPoolReuseFollowEventDependencies  Before allocating more physical GPU memory, the allocator examines dependency information established by CUDA events and tries to allocate from memory freed in another stream"
  },
  {
    "id": 5234,
    "content": "cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ); cudaFreeAsync ( ptr , originalStream ); cudaEventRecord ( event , originalStream ); waiting on the event that captures the free in another stream allows the allocator to reuse the memory to satisfy a new allocation request in the other stream when cudaMemPoolReuseFollowEventDependencies is enabled cudaStreamWaitEvent ("
  },
  {
    "id": 5238,
    "content": "cudaMemPoolReuseAllowOpportunistic  According to the cudaMemPoolReuseAllowOpportunistic policy, the allocator examines freed allocations to see if the free’s stream order semantic has been met (such as the stream has passed the point of execution indicated by the free)"
  },
  {
    "id": 5239,
    "content": "When this is disabled, the allocator will still reuse memory made available when a stream is synchronized with the CPU"
  },
  {
    "id": 5241,
    "content": "cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ); cudaFreeAsync ( ptr , originalStream );   after some time, the kernel finishes running wait ( 10 );   When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request   can be fulfilled with the prior allocation based on the progress of originalStream"
  },
  {
    "id": 5242,
    "content": "cudaMemPoolReuseAllowInternalDependencies  Failing to allocate and map more physical memory from the OS, the driver will look for memory whose availability depends on another stream’s pending progress If such memory is found, the driver will insert the required dependency into the allocating stream and reuse the memory"
  },
  {
    "id": 5243,
    "content": "cudaMallocAsync ( & ptr , size , originalStream ); kernel >> ( ptr , ); cudaFreeAsync ( ptr , originalStream ); When cudaMemPoolReuseAllowInternalDependencies is enabled and the driver fails to allocate more physical memory, the driver may effectively perform a cudaStreamWaitEvent in the allocating stream to make sure that future work in ‘otherStream’ happens after the work in the original stream"
  },
  {
    "id": 5245,
    "content": "Disabling Reuse Policies  While the controllable reuse policies improve memory reuse, users may want to disable them Allowing opportunistic reuse (such as cudaMemPoolReuseAllowOpportunistic ) introduces run to run variance in allocation patterns based on the interleaving of CPU and GPU execution"
  },
  {
    "id": 5246,
    "content": "Internal dependency insertion (such as cudaMemPoolReuseAllowInternalDependencies ) can serialize work in unexpected and potentially non-deterministic ways when the user would rather explicitly synchronize an event or stream on allocation failure"
  },
  {
    "id": 5249,
    "content": "Device Accessibility for Multi-GPU Support  Just like allocation accessibility controlled through the virtual memory management APIs, memory pool allocation accessibility does not follow cudaDeviceEnablePeerAccess or cuCtxEnablePeerAccess"
  },
  {
    "id": 5250,
    "content": "Instead, the API cudaMemPoolSetAccess modifies what devices can access allocations from a pool To enable access from other devices, the accessing device must be peer capable with the memory pool’s device; check with cudaDeviceCanAccessPeer If the peer capability is not checked, the set access may fail with cudaErrorInvalidDevice If no allocations had been made from the pool, the"
  },
  {
    "id": 5251,
    "content": "cudaMemPoolSetAccess call may succeed even when the devices are not peer capable; in this case, the next allocation from the pool will fail It is worth noting that cudaMemPoolSetAccess affects all allocations from the memory pool, not just future ones Also the accessibility reported by cudaMemPoolGetAccess applies to all allocations from the pool, not just future ones It is recommended that the"
  },
  {
    "id": 5252,
    "content": "accessibility settings of a pool for a given GPU not be changed frequently; once a pool is made accessible from a given GPU, it should remain accessible from that GPU for the lifetime of the pool"
  },
  {
    "id": 5253,
    "content": "snippet showing usage of cudaMemPoolSetAccess: cudaError_t setAccessOnDevice ( cudaMemPool_t memPool , int residentDevice , int accessingDevice ) { cudaMemAccessDesc accessDesc = {}; accessDesc flags = cudaMemAccessFlagsProtReadWrite ; int canAccess = 0 ; cudaError_t error = cudaDeviceCanAccessPeer ( & canAccess , accessingDevice , residentDevice ); if ( error = cudaSuccess ) { return error ; }"
  },
  {
    "id": 5254,
    "content": "else if ( canAccess == 0 ) { return cudaErrorPeerAccessUnsupported ; } Make the address accessible return cudaMemPoolSetAccess ( memPool , & accessDesc , 1 ); } 11 11"
  },
  {
    "id": 5255,
    "content": "IPC Memory Pools  IPC capable memory pools allow easy, efficient and secure sharing of GPU memory between processes CUDA’s IPC memory pools provide the same security benefits as CUDA’s virtual memory management APIs"
  },
  {
    "id": 5256,
    "content": "The processes first need to share access to the pool, then share specific allocations from that pool"
  },
  {
    "id": 5257,
    "content": "The second phase coordinates what virtual addresses are used in each process and when mappings need to be valid in the importing process"
  },
  {
    "id": 5260,
    "content": "Creating and Sharing IPC Memory Pools  Sharing access to a pool involves retrieving an OS native handle to the pool (with the cudaMemPoolExportToShareableHandle() API), transferring the handle to the importing process using the usual OS native IPC mechanisms, and creating an imported memory pool (with the cudaMemPoolImportFromShareableHandle() API) For cudaMemPoolExportToShareableHandle to"
  },
  {
    "id": 5261,
    "content": "succeed, the memory pool had to be created with the requested handle type specified in the pool properties structure Please reference samples for the appropriate IPC mechanisms to transfer the OS native handle between processes in exporting process create an exportable IPC capable pool on device 0 cudaMemPoolProps poolProps = { }; poolProps type = cudaMemLocationTypeDevice ; Setting handleTypes to"
  },
  {
    "id": 5262,
    "content": "a non zero value will make the pool exportable (IPC capable) poolProps handleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR ; cudaMemPoolCreate ( & memPool , & poolProps )); FD based handles are integer types int fdHandle = 0 ; Retrieve an OS native handle to the pool cudaMemPoolExportToShareableHandle ( & fdHandle , memPool , CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR , 0 ); The handle must be"
  },
  {
    "id": 5263,
    "content": "sent to the importing process with the appropriate OS specific APIs in importing process int fdHandle ; The handle needs to be retrieved from the exporting process with the appropriate OS specific APIs cudaMemPoolImportFromShareableHandle ( & importedMemPool , ( void * ) fdHandle , CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR , 0 ); 11 11"
  },
  {
    "id": 5265,
    "content": "Set Access in the Importing Process  Imported memory pools are initially only accessible from their resident device The importing process needs to enable access (with cudaMemPoolSetAccess ) from any GPU it plans to access the memory from If the imported memory pool belongs to a non-visible device in the importing process, the user must use the cudaMemPoolSetAccess API to enable access from the"
  },
  {
    "id": 5269,
    "content": "Creating and Sharing Allocations from an Exported Pool  Once the pool has been shared, allocations made with cudaMallocAsync() from the pool in the exporting process can be shared with other processes that have imported the pool Since the pool’s security policy is established and verified at the pool level, the OS does not need extra bookkeeping to provide security for specific pool allocations;"
  },
  {
    "id": 5270,
    "content": "In other words, the opaque cudaMemPoolPtrExportData required to import a pool allocation may be sent to the importing process using any mechanism While allocations may be exported and even imported without synchronizing with the allocating stream in any way, the importing process must follow the same rules as the exporting process when accessing the allocation Namely, access to the allocation must"
  },
  {
    "id": 5271,
    "content": "happen after the stream ordering of the allocation operation in the allocating stream The two following code snippets show cudaMemPoolExportPointer() and cudaMemPoolImportPointer() sharing the allocation with an IPC event used to guarantee that the allocation isn’t accessed in the importing process before the allocation is ready preparing an allocation in the exporting process"
  },
  {
    "id": 5272,
    "content": "cudaMemPoolPtrExportData exportData ; cudaEvent_t readyIpcEvent ; cudaIpcEventHandle_t readyIpcEventHandle ; ipc event for coordinating between processes cudaEventInterprocess flag makes the event an ipc event cudaEventDisableTiming is set for performance reasons cudaEventCreate ( & readyIpcEvent , cudaEventDisableTiming | cudaEventInterprocess ) allocate from the exporting mem pool"
  },
  {
    "id": 5273,
    "content": "cudaMallocAsync ( & ptr , size , exportMemPool , stream ); event for sharing when the allocation is ready cudaEventRecord ( readyIpcEvent , stream ); cudaMemPoolExportPointer ( & exportData , ptr ); cudaIpcGetEventHandle ( & readyIpcEventHandle , readyIpcEvent ); Share IPC event and pointer export data with the importing process using any mechanism Here we copy the data into shared memory shmem ->"
  },
  {
    "id": 5274,
    "content": "ptrData = exportData ; shmem -> readyIpcEventHandle = readyIpcEventHandle ; signal consumers data is ready Importing an allocation cudaMemPoolPtrExportData * importData = & shmem -> prtData ; cudaEvent_t readyIpcEvent ; cudaIpcEventHandle_t * readyIpcEventHandle = & shmem -> readyIpcEventHandle ; Need to retrieve the ipc event handle and the export data from the exporting process using any"
  },
  {
    "id": 5275,
    "content": "mechanism Here we are using shmem and just need synchronization to make sure the shared memory is filled in cudaIpcOpenEventHandle ( & readyIpcEvent , readyIpcEventHandle ); import the allocation cudaMemPoolImportPointer ( & ptr , importedMemPool , importData ); Wait for the prior stream operations in the allocating stream to complete before using the allocation in the importing process"
  },
  {
    "id": 5276,
    "content": "cudaStreamWaitEvent ( stream , readyIpcEvent ); kernel >> ( ptr , ); When freeing the allocation, the allocation needs to be freed in the importing process before it is freed in the exporting process The following code snippet demonstrates the use of CUDA IPC events to provide the required synchronization between the cudaFreeAsync operations in both processes Access to the allocation from the"
  },
  {
    "id": 5277,
    "content": "importing process is obviously restricted by the free operation in the importing process side It is worth noting that cudaFree can be used to free the allocation in both processes and that other stream synchronization APIs may be used instead of CUDA IPC events The free must happen in importing process before the exporting process kernel >> ( ptr , ); Last access in importing process cudaFreeAsync"
  },
  {
    "id": 5278,
    "content": "( ptr , stream ); Access not allowed in the importing process after the free cudaIpcEventRecord ( finishedIpcEvent , stream ); Exporting process The exporting process needs to coordinate its free with the stream order of the importing process’s free cudaStreamWaitEvent ( stream , finishedIpcEvent ); kernel >> ( ptrInExportingProcess , ); The free in the importing process doesn’t stop the exporting"
  },
  {
    "id": 5279,
    "content": "process from using the allocation IPC Export Pool Limitations  IPC pools currently do not support releasing physical blocks back to the OS"
  },
  {
    "id": 5280,
    "content": "As a result the cudaMemPoolTrimTo API acts as a no-op and the cudaMemPoolAttrReleaseThreshold effectively gets ignored"
  },
  {
    "id": 5284,
    "content": "IPC Import Pool Limitations  Allocating from an import pool is not allowed; specifically, import pools cannot be set current and cannot be used in the cudaMallocFromPoolAsync API"
  },
  {
    "id": 5285,
    "content": "The resource usage stat attribute queries only reflect the allocations imported into the process and the associated physical memory"
  },
  {
    "id": 5288,
    "content": "Synchronization API Actions  One of the optimizations that comes with the allocator being part of the CUDA driver is integration with the synchronize APIs When the user requests that the CUDA driver synchronize, the driver waits for asynchronous work to complete Before returning, the driver will determine what frees the synchronization guaranteed to be completed"
  },
  {
    "id": 5289,
    "content": "These allocations are made available for allocation regardless of specified stream or disabled allocation policies"
  },
  {
    "id": 5290,
    "content": "The driver also checks cudaMemPoolAttrReleaseThreshold here and releases any excess physical memory that it can"
  },
  {
    "id": 5296,
    "content": "cudaMemcpyAsync Current Context/Device Sensitivity  In the current CUDA driver, any async memcpy involving memory from cudaMallocAsync should be done using the specified stream’s context as the calling thread’s current context This is not necessary for cudaMemcpyPeerAsync , as the device primary contexts specified in the API are referenced instead of the current context"
  },
  {
    "id": 5300,
    "content": "cuPointerGetAttribute Query  Invoking cuPointerGetAttribute on an allocation after invoking cudaFreeAsync on it results in undefined behavior Specifically, it does not matter if an allocation is still accessible from a given stream: the behavior is still undefined"
  },
  {
    "id": 5304,
    "content": "cuGraphAddMemsetNode  cuGraphAddMemsetNode does not work with memory allocated via the stream ordered allocator"
  },
  {
    "id": 5309,
    "content": "Pointer Attributes  The cuPointerGetAttributes query works on stream ordered allocations Since stream ordered allocations are not context associated, querying CU_POINTER_ATTRIBUTE_CONTEXT will succeed but return NULL in *data"
  },
  {
    "id": 5310,
    "content": "The attribute CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL can be used to determine the location of the allocation: this can be useful when selecting a context for making p2h2p copies using cudaMemcpyPeerAsync"
  },
  {
    "id": 5312,
    "content": "3 and can be useful for debugging and for confirming which pool an allocation comes from before doing IPC"
  },
  {
    "id": 5315,
    "content": "Graph memory nodes have GPU ordered lifetime semantics, which dictate when memory is allowed to be accessed on the device These GPU ordered lifetime semantics enable driver-managed memory reuse, and match those of the stream ordered allocation APIs cudaMallocAsync and cudaFreeAsync , which may be captured when creating a graph Graph allocations have fixed addresses over the life of a graph"
  },
  {
    "id": 5316,
    "content": "including repeated instantiations and launches This allows the memory to be directly referenced by other operations within the graph without the need of a graph update, even when CUDA changes the backing physical memory Within a graph, allocations whose graph ordered lifetimes do not overlap may use the same underlying physical memory CUDA may reuse the same physical memory for allocations across"
  },
  {
    "id": 5317,
    "content": "multiple graphs, aliasing virtual address mappings according to the GPU ordered lifetime semantics For example when different graphs are launched into the same stream, CUDA may virtually alias the same physical memory to satisfy the needs of allocations which have single-graph lifetimes"
  },
  {
    "id": 5322,
    "content": "int driverVersion = 0 ; int deviceSupportsMemoryPools = 0 ; int deviceSupportsMemoryNodes = 0 ; cudaDriverGetVersion ( & driverVersion ); if ( driverVersion >= 11020 ) { avoid invalid value error in cudaDeviceGetAttribute cudaDeviceGetAttribute ( & deviceSupportsMemoryPools , cudaDevAttrMemoryPoolsSupported , device ); } deviceSupportsMemoryNodes = ( driverVersion >= 11040 ) && ("
  },
  {
    "id": 5324,
    "content": "= 0 ); Doing the attribute query inside the driver version check avoids an invalid value return code on 11 0 and 11"
  },
  {
    "id": 5326,
    "content": "Be aware that the compute sanitizer emits warnings when it detects CUDA returning error codes, and a version check before reading the attribute will avoid this"
  },
  {
    "id": 5331,
    "content": "API Fundamentals  Graph memory nodes are graph nodes representing either memory allocation or free actions"
  },
  {
    "id": 5332,
    "content": "While these virtual addresses are fixed for the lifetime of the allocation node, the allocation contents are not persistent past the freeing operation and may be overwritten by accesses referring to a different allocation A graph allocation’s lifetime, which differs from the node’s lifetime, begins when GPU execution reaches the allocating graph node and ends when one of the following occurs: GPU"
  },
  {
    "id": 5333,
    "content": "execution reaches the freeing graph node GPU execution reaches the freeing cudaFreeAsync() stream call immediately upon the freeing call to cudaFree() Note Graph destruction does not automatically free any live graph-allocated memory, even though it ends the lifetime of the allocation node The allocation must subsequently be freed in another graph, or using cudaFreeAsync() /cudaFree() Just like"
  },
  {
    "id": 5334,
    "content": "other graph nodes , graph memory nodes are ordered within a graph by dependency edges A program must guarantee that operations accessing graph memory: are ordered after the allocation node are ordered before the operation freeing the memory Graph allocation lifetimes begin and usually end according to GPU execution (as opposed to API invocation) GPU ordering is the order that work runs on the GPU"
  },
  {
    "id": 5335,
    "content": "as opposed to the order that the work is enqueued or described Graph Node APIs  Graph memory nodes may be explicitly created with the memory node creation APIs, cudaGraphAddMemAllocNode and cudaGraphAddMemFreeNode"
  },
  {
    "id": 5336,
    "content": "The address allocated by cudaGraphAddMemAllocNode is returned to the user in the dptr field of the passed CUDA_MEM_ALLOC_NODE_PARAMS structure"
  },
  {
    "id": 5337,
    "content": "All operations using graph allocations inside the allocating graph must be ordered after the allocating node Similarly, any free nodes must be ordered after all uses of the allocation within the graph Kernel nodes a , b , and c are ordered after the allocation node and before the free node such that the kernels can access the allocation Kernel node e is not ordered after the alloc node and"
  },
  {
    "id": 5338,
    "content": "therefore cannot safely access the memory Kernel node d is not ordered before the free node, therefore it cannot safely access the memory Since the dependency of node b on node a establishes an indirect dependency, the free node does not need to explicitly depend on node a"
  },
  {
    "id": 5339,
    "content": "dependencies[0] = b; dependencies[1] = c; cudaGraphAddMemFreeNode(&freeNode, graph, dependencies, 2, params"
  },
  {
    "id": 5340,
    "content": "dptr); free node does not depend on kernel node d, so it must not access the freed graph allocation cudaGraphAddKernelNode(&d, graph, &c, 1, &nodeParams); node e does not depend on the allocation node, so it must not access the allocation Stream Capture  Graph memory nodes can be created by capturing the corresponding stream ordered allocation and free calls cudaMallocAsync and cudaFreeAsync In"
  },
  {
    "id": 5341,
    "content": "this case, the virtual addresses returned by the captured allocation API can be used by other operations inside the graph Since the stream ordered dependencies will be captured into the graph, the ordering requirements of the stream ordered allocation APIs guarantee that the graph memory nodes will be properly ordered with respect to the captured stream operations (for correctly written stream"
  },
  {
    "id": 5342,
    "content": "code) Ignoring kernel nodes d and e , for clarity, the following code snippet shows how to use stream capture to create the graph from the previous figure: cudaMallocAsync(&dptr, size, stream1); kernel_A>>(dptr, ); Fork into stream2 cudaEventRecord(event1, stream1); cudaStreamWaitEvent(stream2, event1); kernel_B>>(dptr, ); event dependencies translated into graph dependencies, so the kernel node"
  },
  {
    "id": 5343,
    "content": "created by the capture of kernel C will depend on the allocation node created by capturing the cudaMallocAsync call kernel_C>>(dptr, ); Join stream2 back to origin stream (stream1) cudaEventRecord(event2, stream2); cudaStreamWaitEvent(stream1, event2); Free depends on all work accessing the memory cudaFreeAsync(dptr, stream1); End capture in the origin stream cudaStreamEndCapture(stream1, &graph);"
  },
  {
    "id": 5347,
    "content": "Accessing and Freeing Graph Memory Outside of the Allocating Graph  Graph allocations do not have to be freed by the allocating graph When a graph does not free an allocation, that allocation persists beyond the execution of the graph and can be accessed by subsequent CUDA operations These allocations may be accessed in another graph or directly using a stream operation as long as the accessing"
  },
  {
    "id": 5348,
    "content": "operation is ordered after the allocation through CUDA events and other stream ordering mechanisms An allocation may subsequently be freed by regular calls to cudaFree , cudaFreeAsync , or by the launch of another graph with a corresponding free node, or a subsequent launch of the allocating graph (if it was instantiated with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag) It is illegal to"
  },
  {
    "id": 5349,
    "content": "access memory after it has been freed - the free operation must be ordered after all operations accessing the memory using graph dependencies, CUDA events, and other stream ordering mechanisms Note Because graph allocations may share underlying physical memory with each other, the Virtual Aliasing Support rules relating to consistency and coherency must be considered Simply put, the free operation"
  },
  {
    "id": 5350,
    "content": "must be ordered after the full device operation (for example, compute kernel / memcpy) completes Specifically, out of band synchronization - for example a handshake through memory as part of a compute kernel that accesses the graph-allocated memory - is not sufficient for providing ordering guarantees between the memory writes to graph memory and the free operation of that graph memory The"
  },
  {
    "id": 5351,
    "content": "following code snippets demonstrate accessing graph allocations outside of the allocating graph with ordering properly established by: using a single stream, using events between streams, and using events baked into the allocating and freeing graph cudaEvent_t streamUseDoneEvent; event indicating when the stream operations are done with the allocation Contents of allocating graph with event record"
  },
  {
    "id": 5352,
    "content": "node cudaGraphAddMemAllocNode(&allocNode, allocGraph, NULL, 0, ¶ms); dptr = params dptr; note: this event record node depends on the alloc node cudaGraphAddEventRecordNode(&recordNode, allocGraph, &allocNode, 1, allocEvent); cudaGraphInstantiate(&allocGraphExec, allocGraph, NULL, NULL, 0); contents of consuming/freeing graph with event wait nodes cudaGraphAddEventWaitNode(&streamUseDoneEventNode,"
  },
  {
    "id": 5353,
    "content": "waitAndFreeGraph, NULL, 0, streamUseDoneEvent); cudaGraphAddEventWaitNode(&allocReadyEventNode, waitAndFreeGraph, NULL, 0, allocEvent); nodeParams->kernelParams[0] = params dptr; The allocReadyEventNode provides ordering with the alloc node for use in a consuming graph cudaGraphAddKernelNode(&kernelNode, waitAndFreeGraph, &allocReadyEventNode, 1, &nodeParams); The free node has to be ordered after"
  },
  {
    "id": 5354,
    "content": "both external and internal users dependencies[0] = kernelNode; dependencies[1] = streamUseDoneEventNode; cudaGraphAddMemFreeNode(&freeNode, waitAndFreeGraph, &dependencies, 2, dptr); cudaGraphInstantiate(&waitAndFreeGraphExec, waitAndFreeGraph, NULL, NULL, 0); cudaGraphLaunch(allocGraphExec, allocStream); establish the dependency of stream2 on the event node satisfies the ordering requirement"
  },
  {
    "id": 5355,
    "content": "cudaStreamWaitEvent(stream2, allocEvent); kernel>> (dptr, …); cudaStreamRecordEvent(streamUseDoneEvent, stream2); the event wait node in the waitAndFreeGraphExec establishes the dependency on the “readyForFreeEvent” that is needed to prevent the kernel running in stream two from accessing the allocation after the free node in execution order cudaGraphInstantiateFlagAutoFreeOnLaunch  Under normal"
  },
  {
    "id": 5356,
    "content": "circumstances, CUDA will prevent a graph from being relaunched if it has unfreed memory allocations because multiple allocations at the same address will leak memory Instantiating a graph with the cudaGraphInstantiateFlagAutoFreeOnLaunch flag allows the graph to be relaunched while it still has unfreed allocations In this case, the launch automatically inserts an asynchronous free of the unfreed"
  },
  {
    "id": 5357,
    "content": "allocations At each iteration, a producer graph creates several allocations, and, depending on runtime conditions, a varying set of consumers accesses those allocations This type of variable execution sequence means that consumers cannot free the allocations because a subsequent consumer may require access Auto free on launch means that the launch loop does not need to track the producer’s"
  },
  {
    "id": 5358,
    "content": "allocations - instead, that information remains isolated to the producer’s creation and destruction logic In general, auto free on launch simplifies an algorithm which would otherwise need to free all the allocations owned by a graph before each relaunch Note The cudaGraphInstantiateFlagAutoFreeOnLaunch flag does not change the behavior of graph destruction The application must explicitly free the"
  },
  {
    "id": 5360,
    "content": "The following code shows the use of cudaGraphInstantiateFlagAutoFreeOnLaunch to simplify a single-producer / multiple-consumer algorithm: Create producer graph which allocates memory and populates it with data cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); cudaMallocAsync ( & data1 , blocks * threads , cudaStreamPerThread ); cudaMallocAsync ( & data2 , blocks *"
  },
  {
    "id": 5361,
    "content": "threads , cudaStreamPerThread ); produce >> ( data1 , data2 ); cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & producer , graph , cudaGraphInstantiateFlagAutoFreeOnLaunch ); cudaGraphDestroy ( graph ); Create first consumer graph by capturing an asynchronous library call cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal );"
  },
  {
    "id": 5362,
    "content": "consumerFromLibrary ( data1 , cudaStreamPerThread ); cudaStreamEndCapture ( cudaStreamPerThread , & graph ); cudaGraphInstantiateWithFlags ( & consumer1 , graph , 0 ); regular instantiation cudaGraphDestroy ( graph ); Create second consumer graph cudaStreamBeginCapture ( cudaStreamPerThread , cudaStreamCaptureModeGlobal ); consume2 >> ( data2 ); cudaStreamEndCapture ( cudaStreamPerThread , & graph"
  },
  {
    "id": 5363,
    "content": "); cudaGraphInstantiateWithFlags ( & consumer2 , graph , 0 ); cudaGraphDestroy ( graph ); Launch in a loop bool launchConsumer2 = false ; do { cudaGraphLaunch ( producer , myStream ); cudaGraphLaunch ( consumer1 , myStream ); if ( launchConsumer2 ) { cudaGraphLaunch ( consumer2 , myStream ); } } while ( determineAction ( & launchConsumer2 )); cudaFreeAsync ( data1 , myStream ); cudaFreeAsync ("
  },
  {
    "id": 5364,
    "content": "data2 , myStream ); cudaGraphExecDestroy ( producer ); cudaGraphExecDestroy ( consumer1 ); cudaGraphExecDestroy ( consumer2 ); 12"
  },
  {
    "id": 5366,
    "content": "Optimized Memory Reuse  CUDA reuses memory in two ways: Virtual and physical memory reuse within a graph is based on virtual address assignment, like in the stream ordered allocator Physical memory reuse between graphs is done with virtual aliasing: different graphs can map the same physical memory to their unique virtual addresses"
  },
  {
    "id": 5370,
    "content": "Address Reuse within a Graph  CUDA may reuse memory within a graph by assigning the same virtual address ranges to different allocations whose lifetimes do not overlap Since virtual addresses may be reused, pointers to different allocations with disjoint lifetimes are not guaranteed to be unique The following figure shows adding a new allocation node (2) that can reuse the address freed by a"
  },
  {
    "id": 5371,
    "content": "dependent node (1) Figure 29 Adding New Alloc Node 2  The following figure shows adding a new alloc node (4) The new alloc node is not dependent on the free node (2) so cannot reuse the address from the associated alloc node (2) If the alloc node (2) used the address freed by free node (1), the new alloc node 3 would need a new address Physical Memory Management and Sharing  CUDA is responsible"
  },
  {
    "id": 5372,
    "content": "for mapping physical memory to the virtual address before the allocating node is reached in GPU order As an optimization for memory footprint and mapping overhead, multiple graphs may use the same physical memory for distinct allocations if they will not run simultaneously; however, physical pages cannot be reused if they are bound to more than one executing graph at the same time, or to a graph"
  },
  {
    "id": 5373,
    "content": "allocation which remains unfreed CUDA may update physical memory mappings at any time during graph instantiation, launch, or execution CUDA may also introduce synchronization between future graph launches in order to prevent live graph allocations from referring to the same physical memory As for any allocate-free-allocate pattern, if a program accesses a pointer outside of an allocation’s"
  },
  {
    "id": 5374,
    "content": "lifetime, the erroneous access may silently read or write live data owned by another allocation (even if the virtual address of the allocation is unique) Since the graphs in the same stream never run concurrently, CUDA can and should use the same physical memory to satisfy all the allocations Performance Considerations  When multiple graphs are launched into the same stream, CUDA attempts to"
  },
  {
    "id": 5375,
    "content": "allocate the same physical memory to them because the execution of these graphs cannot overlap Physical mappings for a graph are retained between launches as an optimization to avoid the cost of remapping If, at a later time, one of the graphs is launched such that its execution may overlap with the others (for example if it is launched into a different stream) then CUDA must perform some"
  },
  {
    "id": 5376,
    "content": "remapping because concurrent graphs require distinct memory to avoid data corruption In general, remapping of graph memory in CUDA is likely caused by these operations: Changing the stream into which a graph is launched A trim operation on the graph memory pool, which explicitly frees unused memory (discussed in Physical Memory Footprint ) Relaunching a graph while an unfreed allocation from"
  },
  {
    "id": 5377,
    "content": "another graph is mapped to the same memory will cause a remap of memory before relaunch Remapping must happen in execution order, but after any previous execution of that graph is complete (otherwise memory that is still in use could be unmapped)"
  },
  {
    "id": 5378,
    "content": "Due to this ordering dependency, as well as because mapping operations are OS calls, mapping operations can be relatively expensive"
  },
  {
    "id": 5379,
    "content": "Applications can avoid this cost by launching graphs containing allocation memory nodes consistently into the same stream"
  },
  {
    "id": 5383,
    "content": "First Launch / cudaGraphUpload  Physical memory cannot be allocated or mapped during graph instantiation because the stream in which the graph will execute is unknown Calling cudaGraphUpload can separate out the cost of allocation from the launch by performing all mappings for that graph immediately and associating the graph with the upload stream If the graph is then launched into the same"
  },
  {
    "id": 5384,
    "content": "stream, it will launch without any additional remapping Using different streams for graph upload and graph launch behaves similarly to switching streams, likely resulting in remap operations"
  },
  {
    "id": 5385,
    "content": "In addition, unrelated memory pool management is permitted to pull memory from an idle stream, which could negate the impact of the uploads"
  },
  {
    "id": 5388,
    "content": "Physical Memory Footprint  The pool-management behavior of asynchronous allocation means that destroying a graph which contains memory nodes (even if their allocations are free) will not immediately return physical memory to the OS for use by other processes To explicitly release memory back to the OS, an application should use the cudaDeviceGraphMemTrim API cudaDeviceGraphMemTrim will unmap and"
  },
  {
    "id": 5389,
    "content": "release any physical memory reserved by graph memory nodes that is not actively in use Allocations that have not been freed and graphs that are scheduled or running are considered to be actively using the physical memory and will not be impacted Use of the trim API will make physical memory available to other allocation APIs and other applications or processes, but will cause CUDA to reallocate"
  },
  {
    "id": 5390,
    "content": "and remap memory when the trimmed graphs are next launched CUDA allows applications to query their graph memory footprint through the cudaDeviceGetGraphMemAttribute API Querying the attribute cudaGraphMemAttrReservedMemCurrent returns the amount of physical memory reserved by the driver for graph allocations in the current process Querying cudaGraphMemAttrUsedMemCurrent returns the amount of"
  },
  {
    "id": 5391,
    "content": "physical memory currently mapped by at least one graph Either of these attributes can be used to track when new physical memory is acquired by CUDA for the sake of an allocating graph Both of these attributes are useful for examining how much memory is saved by the sharing mechanism"
  },
  {
    "id": 5394,
    "content": "Peer Access  Graph allocations can be configured for access from multiple GPUs, in which case CUDA will map the allocations onto the peer GPUs as required CUDA allows graph allocations requiring different mappings to reuse the same virtual address When this occurs, the address range is mapped onto all GPUs required by the different allocations This means an allocation may sometimes allow more"
  },
  {
    "id": 5395,
    "content": "peer access than was requested during its creation; however, relying on these extra mappings is still an error"
  },
  {
    "id": 5399,
    "content": "Peer Access with Graph Node APIs  The cudaGraphAddMemAllocNode API accepts mapping requests in the accessDescs array field of the node parameters structures"
  },
  {
    "id": 5401,
    "content": "location embedded structure specifies the resident device for the allocation Access from the allocating GPU is assumed to be needed, thus the application does not need to specify an entry for the resident device in the accessDescs array"
  },
  {
    "id": 5411,
    "content": "bytesize = size;   allocate an allocation resident on device 1 accessible from device 1 cudaGraphAddMemAllocNode(&allocNode, graph, NULL, 0, ¶ms); accessDescs[2];   boilerplate for the access descs (only ReadWrite and Device access supported by the add node api) accessDescs[0] flags = cudaMemAccessFlagsProtReadWrite; accessDescs[0]"
  },
  {
    "id": 5413,
    "content": "type = cudaMemLocationTypeDevice; accessDescs[1] flags = cudaMemAccessFlagsProtReadWrite; accessDescs[1]"
  },
  {
    "id": 5422,
    "content": "accessDescs = accessDescs;   allocate an allocation resident on device 1 accessible from devices 0, 1 and 2"
  },
  {
    "id": 5423,
    "content": "Peer Access with Stream Capture  For stream capture, the allocation node records the peer accessibility of the allocating pool at the time of the capture Altering the peer accessibility of the allocating pool after a cudaMallocFromPoolAsync call is captured does not affect the mappings that the graph will make for the allocation boilerplate for the access descs (only ReadWrite and Device access"
  },
  {
    "id": 5428,
    "content": "id = 1; let memPool be resident and accessible on device 0 cudaStreamBeginCapture(stream); cudaMallocAsync(&dptr1, size, memPool, stream); cudaStreamEndCapture(stream, &graph1); cudaMemPoolSetAccess(memPool, &accessDesc, 1); cudaStreamBeginCapture(stream); cudaMallocAsync(&dptr2, size, memPool, stream); cudaStreamEndCapture(stream, &graph2); The graph node allocating dptr1 would only have the"
  },
  {
    "id": 5429,
    "content": "device 0 accessibility even though memPool now has device 1 accessibility The graph node allocating dptr2 will have device 0 and device 1 accessibility, since that was the pool accessibility at the time of the cudaMallocAsync call"
  },
  {
    "id": 5431,
    "content": "Mathematical Functions  The reference manual lists, along with their description, all the functions of the C/C++ standard library mathematical functions that are supported in device code, as well as all intrinsic functions (that are only supported in device code)"
  },
  {
    "id": 5432,
    "content": "For further information on the definition of the Unit in the Last Place (ULP), please see Jean-Michel Muller’s paper On the definition of ulp(x) , RR-5504, LIP RR-2005-09, INRIA, LIP"
  },
  {
    "id": 5433,
    "content": "Mathematical functions supported in device code do not set the global errno variable, nor report any floating-point exceptions to indicate errors; thus, if error diagnostic mechanisms are required, the user should implement additional screening for inputs and outputs of the functions The user must not pass uninitialized parameters to the Mathematical functions as this may result in undefined"
  },
  {
    "id": 5437,
    "content": "Standard Functions  The functions from this section can be used in both host and device code This section specifies the error bounds of each function when executed on the device and also when executed on the host in the case where the host does not supply the function The error bounds are generated from extensive but not exhaustive tests, so they are not guaranteed bounds"
  },
  {
    "id": 5438,
    "content": "Single-Precision Floating-Point Functions Addition and multiplication are IEEE-compliant, so have a maximum error of 0"
  },
  {
    "id": 5440,
    "content": "The recommended way to round a single-precision floating-point operand to an integer, with the result being a single-precision floating-point number is rintf() , not roundf() The reason is that roundf() maps to a 4-instruction sequence on the device, whereas rintf() maps to a single instruction The maximum error is stated as the absolute value of the difference in ulps between the result returned"
  },
  {
    "id": 5441,
    "content": "by the CUDA library function and a correctly rounded single-precision result obtained according to the round-to-nearest ties-to-even rounding mode  Function Maximum ulp error x+y 0 (IEEE-754 round-to-nearest-even) x*y 0 (IEEE-754 round-to-nearest-even) x/y 0 for compute capability \\(\\ge 2\\) when compiled with -prec-div=true 2 (full range), otherwise 1/x 0 for compute capability \\(\\ge 2\\) when"
  },
  {
    "id": 5442,
    "content": "compiled with -prec-div=true 1 (full range), otherwise rsqrtf(x) 1/sqrtf(x) 2 (full range) Applies to 1/sqrtf(x) only when it is converted to rsqrtf(x) by the compiler sqrtf(x) 0 when compiled with -prec-sqrt=true Otherwise 1 for compute capability \\(\\ge 5 2\\) and 3 for older architectures cbrtf(x) 1 (full range) rcbrtf(x) 1 (full range) hypotf(x,y) 3 (full range) rhypotf(x,y) 2 (full range)"
  },
  {
    "id": 5443,
    "content": "norm3df(x,y,z) 3 (full range) rnorm3df(x,y,z) 2 (full range) norm4df(x,y,z,t) 3 (full range) rnorm4df(x,y,z,t) 2 (full range) normf(dim,arr) An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off rnormf(dim,arr) An error bound cannot be provided because a fast algorithm is used with accuracy loss due to round-off"
  },
  {
    "id": 5444,
    "content": "Among these functions are the less accurate, but faster versions of some of the functions of Standard Functions"
  },
  {
    "id": 5445,
    "content": "The compiler has an option ( -use_fast_math ) that forces each function in Table 15 to compile to its intrinsic counterpart"
  },
  {
    "id": 5446,
    "content": "In addition to reducing the accuracy of the affected functions, it may also cause some differences in special case handling A more robust approach is to selectively replace mathematical function calls by calls to intrinsic functions only where it is merited by the performance gains and where changed properties such as reduced accuracy and different special case handling can be tolerated"
  },
  {
    "id": 5447,
    "content": "Table 15 Functions Affected by -use_fast_math  Operator/Function Device Function x/y __fdividef(x,y) sinf(x) __sinf(x) cosf(x) __cosf(x) tanf(x) __tanf(x) sincosf(x,sptr,cptr) __sincosf(x,sptr,cptr) logf(x) __logf(x) log2f(x) __log2f(x) log10f(x) __log10f(x) expf(x) __expf(x) exp10f(x) __exp10f(x) powf(x,y) __powf(x,y) Single-Precision Floating-Point Functions __fadd_[rn,rz,ru,rd]() and"
  },
  {
    "id": 5448,
    "content": "__fmul_[rn,rz,ru,rd]() map to addition and multiplication operations that the compiler never merges into FMADs"
  },
  {
    "id": 5449,
    "content": "By contrast, additions and multiplications generated from the ‘*’ and ‘+’ operators will frequently be combined into FMADs"
  },
  {
    "id": 5450,
    "content": "Functions suffixed with _ru operate using the round up (to positive infinity) rounding mode Functions suffixed with _rd operate using the round down (to negative infinity) rounding mode"
  },
  {
    "id": 5451,
    "content": "The accuracy of floating-point division varies depending on whether the code is compiled with -prec-div=false or -prec-div=true When the code is compiled with -prec-div=false , both the regular division / operator and __fdividef(x,y) have the same accuracy, but for 2 126 2"
  },
  {
    "id": 5454,
    "content": "C++ Language Support  As described in Compilation with NVCC , CUDA source files compiled with nvcc can include a mix of host code and device code"
  },
  {
    "id": 5455,
    "content": "The CUDA front-end compiler aims to emulate the host compiler behavior with respect to C++ input code The input source code is processed according to the C++ ISO/IEC 14882:2003, C++ ISO/IEC 14882:2011, C++ ISO/IEC 14882:2014 or C++ ISO/IEC 14882:2017 specifications, and the CUDA front-end compiler aims to emulate any host compiler divergences from the ISO specification"
  },
  {
    "id": 5456,
    "content": "In addition, the supported language is extended with CUDA-specific constructs described in this document 13 , and is subject to the restrictions described below C++11 Language Features , C++14 Language Features and C++17 Language Features provide support matrices for the C++11, C++14, C++17 and C++20 features, respectively"
  },
  {
    "id": 5460,
    "content": "C++11 Language Features  The following table lists new language features that have been accepted into the C++11 standard"
  },
  {
    "id": 5461,
    "content": "The “Proposal” column provides a link to the ISO C++ committee proposal that describes the feature, while the “Available in nvcc (device code)” column indicates the first version of nvcc that contains an implementation of this feature (if it has been implemented) for device code"
  },
  {
    "id": 5462,
    "content": "C++14 Language Features  The following table lists new language features that have been accepted into the C++14 standard Table 19 C++14 Language Features  Language Feature C++14 Proposal Available in nvcc (device code) Tweak to certain C++ contextual conversions N3323 9"
  },
  {
    "id": 5480,
    "content": "Host Compiler Extensions  Host compiler specific language extensions are not supported in device code __int128 type is supported in device code when compiled in conjunction with a host compiler that supports it"
  },
  {
    "id": 5481,
    "content": "A constant expression of __float128 type may be processed by the compiler in a floating point representation with lower precision"
  },
  {
    "id": 5489,
    "content": "__CUDA_ARCH__  The type signature of the following entities shall not depend on whether __CUDA_ARCH__ is defined or not, or on a particular value of __CUDA_ARCH__ : __global__ functions and function templates __device__ and __constant__ variables textures and surfaces Example: #if defined(__CUDA_ARCH__) typedef int mytype ; #else typedef double mytype ; #endif __device__ mytype xxx ; error:"
  },
  {
    "id": 5490,
    "content": "xxx's type depends on __CUDA_ARCH__ __global__ void foo ( mytype in , error: foo's type depends on __CUDA_ARCH__ mytype * ptr ) { * ptr = in ; } If a __global__ function template is instantiated and launched from the host, then the function template must be instantiated with the same template arguments irrespective of whether __CUDA_ARCH__ is defined and regardless of the value of __CUDA_ARCH__"
  },
  {
    "id": 5491,
    "content": "Example: __device__ int result ; template __global__ void kern ( T in ) { result = in ; } __host__ __device__ void foo ( void ) { #if defined(__CUDA_ARCH__) kern >> ( 1 ); error: \"kern\" instantiation only when __CUDA_ARCH__ is undefined #endif } int main ( void ) { foo (); cudaDeviceSynchronize (); return 0 ; } In separate compilation mode, the presence or absence of a definition of a function or"
  },
  {
    "id": 5492,
    "content": "variable with external linkage shall not depend on whether __CUDA_ARCH__ is defined or on a particular value of __CUDA_ARCH__ 14 Example: #if defined(__CUDA_ARCH__) void foo ( void ) { } error: The definition of foo() is only present when __CUDA_ARCH__ is undefined #endif In separate compilation, __CUDA_ARCH__ must not be used in headers such that different objects could contain different behavior"
  },
  {
    "id": 5493,
    "content": "If a weak function or template function is defined in a header and its behavior depends on __CUDA_ARCH__ , then the instances of that function in the objects could conflict if the objects are compiled for different compute arch For example, if an a h contains: template __device__ T * getptr ( void ) { #if __CUDA_ARCH__ == 700 return NULL ; /* no address */ #else __shared__ T arr [ 256 ]; return"
  },
  {
    "id": 5497,
    "content": "cu expects a non-NULL address, and compile with: nvcc –arch=compute_70 –dc a cu nvcc –arch=compute_80 –dc b cu nvcc –arch=sm_80 a"
  },
  {
    "id": 5499,
    "content": "o At link time only one version of the getptr is used, so the behavior would depend on which version is chosen"
  },
  {
    "id": 5501,
    "content": "cu and b cu must be compiled for the same compute arch, or __CUDA_ARCH__ should not be used in the shared header function The compiler does not guarantee that a diagnostic will be generated for the unsupported uses of __CUDA_ARCH__ described above"
  },
  {
    "id": 5509,
    "content": "Device Memory Space Specifiers  The __device__ , __shared__ , __managed__ and __constant__ memory space specifiers are not allowed on: class , struct , and union data members, formal parameters, non-extern variable declarations within a function that executes on the host The __device__ , __constant__ and __managed__ memory space specifiers are not allowed on variable declarations that are"
  },
  {
    "id": 5510,
    "content": "neither extern nor static within a function that executes on the device A __device__ , __constant__ , __managed__ or __shared__ variable definition cannot have a class type with a non-empty constructor or a non-empty destructor A constructor for a class type is considered empty at a point in the translation unit, if it is either a trivial constructor or it satisfies all of the following"
  },
  {
    "id": 5511,
    "content": "conditions: The constructor function has been defined The constructor function has no parameters, the initializer list is empty and the function body is an empty compound statement"
  },
  {
    "id": 5512,
    "content": "Its class has no virtual functions, no virtual base classes and no non-static data member initializers For all the nonstatic data members of its class that are of class type (or array thereof), the default constructors can be considered empty A destructor for a class is considered empty at a point in the translation unit, if it is either a trivial destructor or it satisfies all of the following"
  },
  {
    "id": 5513,
    "content": "conditions: The destructor function has been defined For all the nonstatic data members of its class that are of class type (or array thereof), the destructor can be considered empty"
  },
  {
    "id": 5514,
    "content": "When compiling in the whole program compilation mode (see the nvcc user manual for a description of this mode), __device__ , __shared__ , __managed__ and __constant__ variables cannot be defined as external using the extern keyword The only exception is for dynamically allocated __shared__ variables as described in index html#__shared__ When compiling in the separate compilation mode (see the"
  },
  {
    "id": 5515,
    "content": "nvcc user manual for a description of this mode), __device__ , __shared__ , __managed__ and __constant__ variables can be defined as external using the extern keyword nvlink will generate an error when it cannot find a definition for an external variable (unless it is a dynamically allocated __shared__ variable)"
  },
  {
    "id": 5520,
    "content": "__managed__ Memory Space Specifier  Variables marked with the __managed__ memory space specifier (“managed” variables) have the following restrictions: The address of a managed variable is not a constant expression The address or value of a managed variable shall not be used when the CUDA runtime may not be in a valid state, including the following cases: In static/dynamic initialization or"
  },
  {
    "id": 5522,
    "content": "In code that executes after exit() has been called (for example, a function marked with gcc’s “ __attribute__((destructor)) ”) In code that executes when CUDA runtime may not be initialized (for example, a function marked with gcc’s “ __attribute__((constructor)) ”)"
  },
  {
    "id": 5523,
    "content": "A managed variable cannot be used as an unparenthesized id-expression argument to a decltype() expression Managed variables have the same coherence and consistency behavior as specified for dynamically allocated managed memory When a CUDA program containing managed variables is run on an execution platform with multiple GPUs, the variables are allocated only once, and not per GPU A managed"
  },
  {
    "id": 5524,
    "content": "variable declaration without the extern linkage is not allowed within a function that executes on the host A managed variable declaration without the extern or static linkage is not allowed within a function that executes on the device"
  },
  {
    "id": 5525,
    "content": "Volatile Qualifier  The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers or L1 cache) as long as it respects the memory ordering semantics of memory fence functions ( Memory Fence Functions ) and memory visibility semantics of synchronization functions ( Synchronization Functions ) These optimizations can be disabled"
  },
  {
    "id": 5526,
    "content": "using the volatile keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory read or write instruction"
  },
  {
    "id": 5530,
    "content": "Pointers  Dereferencing a pointer either to global or shared memory in code that is executed on the host, or to host memory in code that is executed on the device results in an undefined behavior, most often in a segmentation fault and application termination"
  },
  {
    "id": 5531,
    "content": "The address obtained by taking the address of a __device__ , __shared__ or __constant__ variable can only be used in device code The address of a __device__ or __constant__ variable obtained through cudaGetSymbolAddress() as described in Device Memory can only be used in host code"
  },
  {
    "id": 5539,
    "content": "Assignment Operator  __constant__ variables can only be assigned from the host code through runtime functions ( Device Memory ); they cannot be assigned from the device code"
  },
  {
    "id": 5545,
    "content": "Address Operator  It is not allowed to take the address of any of the built-in variables defined in Built-in Variables"
  },
  {
    "id": 5549,
    "content": "Run Time Type Information (RTTI)  The following RTTI-related features are supported in host code, but not in device code Exception Handling  Exception handling is only supported in host code, but not in device code Exception specification is not supported for __global__ functions"
  },
  {
    "id": 5553,
    "content": "Standard Library  Standard libraries are only supported in host code, but not in device code, unless specified otherwise"
  },
  {
    "id": 5557,
    "content": "Namespace Reservations  Unless an exception is otherwise noted, it is undefined behavior to add any declarations or definitions to cuda:: , nv:: , cooperative_groups:: or any namespace nested within Examples: namespace cuda { Bad: class declaration added to namespace cuda struct foo {}; Bad: function definition added to namespace cuda cudaStream_t make_stream (){ cudaStream_t s ;"
  },
  {
    "id": 5558,
    "content": "cudaStreamCreate ( & s ); return s ; } } namespace cuda namespace cuda { namespace utils { Bad: function definition added to namespace nested within cuda cudaStream_t make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } } namespace utils } namespace cuda namespace utils { namespace cuda { Okay: namespace cuda may be used nested within a non-reserved namespace cudaStream_t"
  },
  {
    "id": 5559,
    "content": "make_stream (){ cudaStream_t s ; cudaStreamCreate ( & s ); return s ; } } namespace cuda } namespace utils Bad: Equivalent to adding symbols to namespace cuda at global scope using namespace utils ; 14"
  },
  {
    "id": 5562,
    "content": "External Linkage  A call within some device code of a function declared with the extern qualifier is only allowed if the function is defined within the same compilation unit as the device code, i"
  },
  {
    "id": 5569,
    "content": "Implicitly-declared and explicitly-defaulted functions  Let F denote a function that is either implicitly-declared or is explicitly-defaulted on its first declaration The execution space specifiers ( __host__ , __device__ ) for F are the union of the execution space specifiers of all the functions that invoke it (note that a __global__ caller will be treated as a __device__ caller for this"
  },
  {
    "id": 5570,
    "content": "analysis) For example: class Base { int x ; public : __host__ __device__ Base ( void ) : x ( 10 ) {} }; class Derived : public Base { int y ; }; class Other : public Base { int z ; }; __device__ void foo ( void ) { Derived D1 ; Other D2 ; } __host__ void bar ( void ) { Other D3 ; } Here, the implicitly-declared constructor function “Derived::Derived” will be treated as a __device__ function, since"
  },
  {
    "id": 5571,
    "content": "it is invoked only from the __device__ function “foo” The implicitly-declared constructor function “Other::Other” will be treated as a __host__ __device__ function, since it is invoked both from a __device__ function “foo” and a __host__ function “bar” In addition, if F is a virtual destructor, then the execution spaces of each virtual destructor D overridden by F are added to the set of execution"
  },
  {
    "id": 5572,
    "content": "spaces for F , if D is either not implicitly defined or is explicitly defaulted on a declaration other than its first declaration For example: struct Base1 { virtual __host__ __device__ ~ Base1 () { } }; struct Derived1 : Base1 { }; implicitly-declared virtual destructor ~Derived1 has __host__ __device__ execution space specifiers struct Base2 { virtual __device__ ~ Base2 (); }; __device__ Base2"
  },
  {
    "id": 5573,
    "content": "::~ Base2 () = default ; struct Derived2 : Base2 { }; implicitly-declared virtual destructor ~Derived2 has __device__ execution space specifiers 14"
  },
  {
    "id": 5577,
    "content": "Function Parameters  __global__ function parameters are passed to the device via constant memory and are limited to 32,764 bytes starting with Volta, and 4 KB on older architectures"
  },
  {
    "id": 5578,
    "content": "In separate compilation mode, if a __device__ or __global__ function is ODR-used in a particular translation unit, then the parameter and return types of the function must be complete in that translation unit"
  },
  {
    "id": 5580,
    "content": "cu: struct S ; __device__ void foo ( S ); error: type 'S' is incomplete __device__ auto * ptr = foo ; int main () { } second cu: struct S { int x ; }; __device__ void foo ( S ) { } compiler invocation $nvcc -std=c++14 -rdc=true first cu second cu -o first nvlink error : Prototype doesn't match for '_Z3foo1S' in '/tmp/tmpxft_00005c8c_00000000-18_second o', first defined in"
  },
  {
    "id": 5587,
    "content": "__global__ Function Argument Processing  When a __global__ function is launched from device code, each argument must be trivially copyable and trivially destructible When a __global__ function is launched from host code, each argument type is allowed to be non-trivially copyable or non-trivially-destructible, but the processing for such types does not follow the standard C++ model, as described"
  },
  {
    "id": 5588,
    "content": "below The workflow diverges from standard C++ in two areas: Memcpy instead of copy constructor invocation When lowering a __global__ function launch from host code, the compiler generates stub functions that copy the parameters one or more times by value, before eventually using memcpy to copy the arguments to the __global__ function’s parameter memory on the device This occurs even if an argument"
  },
  {
    "id": 5589,
    "content": "was non-trivially-copyable, and therefore may break programs where the copy constructor has side effects Example: #include struct S { int x ; int * ptr ; __host__ __device__ S () { } __host__ __device__ S ( const S & ) { ptr = & x ; } }; __global__ void foo ( S in ) { this assert may fail, because the compiler generated code will memcpy the contents of \"in\" from host to kernel parameter memory, so"
  },
  {
    "id": 5590,
    "content": "the \"in ptr\" is not initialized to \"&in x\" because the copy constructor is skipped x ); } int main () { S tmp ; foo >> ( tmp ); cudaDeviceSynchronize (); } Example: #include __managed__ int counter ; struct S1 { S1 () { } S1 ( const S1 & ) { ++ counter ; } }; __global__ void foo ( S1 ) { /* this assertion may fail, because the compiler generates stub functions on the host for a kernel launch, and"
  },
  {
    "id": 5591,
    "content": "they may copy the argument by value more than once */ assert ( counter == 1 ); } int main () { S1 V ; foo >> ( V ); cudaDeviceSynchronize (); } Destructor may be invoked before the ``__global__`` function has finished Kernel launches are asynchronous with host execution As a result, if a __global__ function argument has a non-trivial destructor, the destructor may execute in host code even before"
  },
  {
    "id": 5592,
    "content": "the __global__ function has finished execution Example: struct S { int * ptr ; S () : ptr ( nullptr ) { } S ( const S & ) { cudaMallocManaged ( & ptr , sizeof ( int )); } ~ S () { cudaFree ( ptr ); } }; __global__ void foo ( S in ) { error: This store may write to memory that has already been freed (see below) ptr ) = 4 ; } int main () { S V ; /* The object 'V' is first copied by value to a"
  },
  {
    "id": 5593,
    "content": "compiler-generated * stub function that does the kernel launch, and the stub function * bitwise copies the contents of the argument to kernel parameter * memory * As a result, S::~S() will execute when the stub function returns, releasing allocated memory, even though the kernel may not have finished execution"
  },
  {
    "id": 5594,
    "content": "Toolkit and Driver Compatibility  Developers must use the 12 1 Toolkit and r530 driver or higher to compile, launch, and debug kernels that accept parameters larger than 4KB"
  },
  {
    "id": 5601,
    "content": "Link Compatibility across Toolkit Revisions  When linking device objects, if at least one device object contains a kernel with a parameter larger than 4KB, the developer must recompile all objects from their respective device sources with the 12"
  },
  {
    "id": 5608,
    "content": "Static Variables within Function  Variable memory space specifiers are allowed in the declaration of a static variable V within the immediate or nested block scope of a function F where: F is a __global__ or __device__ -only function If no explicit memory space specifier is present in the declaration of V , an implicit __device__ specifier is assumed during device compilation V has the same"
  },
  {
    "id": 5609,
    "content": "initialization restrictions as a variable with the same memory space specifiers declared in namespace scope for example a __device__ variable cannot have a ‘non-empty’ constructor (see Device Memory Space Specifiers ) #ifdef __CUDA_ARCH__ static __device__ int d1 ; OK, declaration is only visible during device compilation (__CUDA_ARCH__ is defined) #else static int d0 ; OK, declaration is only"
  },
  {
    "id": 5610,
    "content": "visible during host compilation (__CUDA_ARCH__ is not defined) #endif static __device__ int d2 ; error: __device__ variable inside a host function during host compilation i"
  },
  {
    "id": 5612,
    "content": "when __CUDA_ARCH__ is not defined static __shared__ int i2 ;   error: __shared__ variable inside   a host function during host compilation   i"
  },
  {
    "id": 5614,
    "content": "Function Pointers  The address of a __global__ function taken in host code cannot be used in device code (e"
  },
  {
    "id": 5616,
    "content": "Similarly, the address of a __global__ function taken in device code cannot be used in host code It is not allowed to take the address of a __device__ function in host code"
  },
  {
    "id": 5626,
    "content": "Friend Functions  A __global__ function or function template cannot be defined in a friend declaration Example: struct S1_t { friend __global__ void foo1 ( void ); OK: not a definition template friend __global__ void foo2 ( void ); OK: not a definition friend __global__ void foo3 ( void ) { } error: definition in friend declaration template friend __global__ void foo4 ( void ) { } error:"
  },
  {
    "id": 5636,
    "content": "Allocation and Deallocation Functions  A user-defined operator new , operator new[] , operator delete , or operator delete[] cannot be used to replace the corresponding __host__ or __device__ builtins provided by the compiler"
  },
  {
    "id": 5637,
    "content": "Data Members  Static data members are not supported except for those that are also const-qualified (see Const-qualified variables )"
  },
  {
    "id": 5647,
    "content": "Virtual Functions  When a function in a derived class overrides a virtual function in a base class, the execution space specifiers (i"
  },
  {
    "id": 5650,
    "content": "It is not allowed to pass as an argument to a __global__ function an object of a class with virtual functions If an object is created in host code, invoking a virtual function for that object in device code has undefined behavior If an object is created in device code, invoking a virtual function for that object in host code has undefined behavior Example: struct S1 { virtual __host__ __device__"
  },
  {
    "id": 5651,
    "content": "void foo () { } }; __managed__ S1 * ptr1 , * ptr2 ; __managed__ __align__ ( 16 ) char buf1 [ 128 ]; __global__ void kern () { ptr1 -> foo (); error: virtual function call on a object created in host code ptr2 = new ( buf1 ) S1 (); } int main ( void ) { void * buf ; cudaMallocManaged ( & buf , sizeof ( S1 ), cudaMemAttachGlobal ); ptr1 = new ( buf ) S1 (); kern >> (); cudaDeviceSynchronize (); ptr2"
  },
  {
    "id": 5657,
    "content": "Virtual Base Classes  It is not allowed to pass as an argument to a __global__ function an object of a class derived from virtual base classes"
  },
  {
    "id": 5662,
    "content": "Anonymous Unions  Member variables of a namespace scope anonymous union cannot be referenced in a __global__ or __device__ function"
  },
  {
    "id": 5667,
    "content": "Windows-Specific  The CUDA compiler follows the IA64 ABI for class layout, while the Microsoft host compiler does not"
  },
  {
    "id": 5668,
    "content": "Let T denote a pointer to member type, or a class type that satisfies any of the following conditions: T has virtual functions"
  },
  {
    "id": 5669,
    "content": "All direct and indirect base classes B of T are empty and the type of the first field F of T uses B in its definition, such that B is laid out at offset 0 in the definition of F"
  },
  {
    "id": 5670,
    "content": "The CUDA compiler may compute the class layout and size differently than the Microsoft host compiler for the type C"
  },
  {
    "id": 5671,
    "content": "As long as the type C is used exclusively in host or device code, the program should work correctly Passing an object of type C between host and device code has undefined behavior, for example, as an argument to a __global__ function or through cudaMemcpy*() calls Accessing an object of type C or any subobject in device code, or invoking a member function in device code, has undefined behavior if"
  },
  {
    "id": 5672,
    "content": "the object is created in host code Accessing an object of type C or any subobject in host code, or invoking a member function in host code, has undefined behavior if the object is created in device code 18"
  },
  {
    "id": 5676,
    "content": "Templates  A type or template cannot be used in the type, non-type or template template argument of a __global__ function template instantiation or a __device__/__constant__ variable instantiation if either: The type or template is defined within a __host__ or __host__ __device__ The type or template is a class member with private or protected access and its parent class is not defined within a"
  },
  {
    "id": 5677,
    "content": "__device__ or __global__ function Example: template __global__ void myKernel ( void ) { } class myClass { private : struct inner_t { }; public : static void launch ( void ) { error: inner_t is used in template argument but it is private myKernel >> (); } }; C++14 only template __device__ T d1 ; template __device__ T1 d2 ; void fn () { struct S1_t { }; error (C++14 only): S1_t is local to the"
  },
  {
    "id": 5678,
    "content": "function fn d1 = {}; auto lam1 = [] { }; error (C++14 only): a closure type cannot be used for instantiating a variable template d2 = 10 ; } 14"
  },
  {
    "id": 5685,
    "content": "Const-qualified variables  Let ‘V’ denote a namespace scope variable or a class static member variable that has const qualified type and does not have execution space annotations (for example, __device__, __constant__, __shared__ ) The value of V may be directly used in device code, if V has been initialized with a constant expression before the point of use, the type of V is not"
  },
  {
    "id": 5686,
    "content": "volatile-qualified, and it has one of the following types: built-in floating point type except when the Microsoft compiler is used as the host compiler, built-in integral type Example: const int xxx = 10 ; struct S1_t { static const int yyy = 20 ; }; extern const int zzz ; const float www = 5 0 ; __device__ void foo ( void ) { int local1 [ xxx ]; OK int local2 [ S1_t :: yyy ]; OK int val1 = xxx ;"
  },
  {
    "id": 5687,
    "content": "OK int val2 = S1_t :: yyy ; OK int val3 = zzz ; error: zzz not initialized with constant expression at the point of use const int & val3 = xxx ; error: reference to host variable const int * val4 = & xxx ; error: address of host variable const float val5 = www ; OK except when the Microsoft compiler is used as the host compiler Long Double  The use of long double type is not supported in device"
  },
  {
    "id": 5692,
    "content": "Deprecation Annotation  nvcc supports the use of deprecated attribute when using gcc , clang , xlC , icc or pgcc host compilers, and the use of deprecated declspec when using the cl"
  },
  {
    "id": 5695,
    "content": "The CUDA frontend compiler will generate a deprecation diagnostic for a reference to a deprecated entity from within the body of a __device__ , __global__ or __host__ __device__ function when __CUDA_ARCH__ is defined (i"
  },
  {
    "id": 5701,
    "content": "The CUDA frontend compiler does not support the #pragma gcc diagnostic or #pragma warning mechanisms supported by various host compilers Therefore, deprecation diagnostics generated by the CUDA frontend compiler are not affected by these pragmas, but diagnostics generated by the host compiler will be affected To suppress the warning for device-code, user can use NVIDIA specific pragma #pragma"
  },
  {
    "id": 5703,
    "content": "The nvcc flag -Wno-deprecated-declarations can be used to suppress all deprecation warnings, and the flag -Werror=deprecated-declarations can be used to turn deprecation warnings into errors"
  },
  {
    "id": 5707,
    "content": "Noreturn Annotation  nvcc supports the use of noreturn attribute when using gcc , clang , xlC , icc or pgcc host compilers, and the use of noreturn declspec when using the cl"
  },
  {
    "id": 5714,
    "content": "[[likely]] / [[unlikely]] Standard Attributes  These attributes are accepted in all configurations that support the C++ standard attribute syntax The attributes can be used to hint to the device compiler optimizer whether a statement is more or less likely to be executed compared to any alternative path that does not include the statement"
  },
  {
    "id": 5715,
    "content": "Example: __device__ int foo ( int x ) { if ( i __global__ void foo ( T in ) { }; template struct S1_t { }; void bar ( void ) { auto temp1 = [] { }; foo >> ( temp1 );   error: lambda closure type used in   template type argument foo >> ( S1_t ());   error: lambda closure type used in   template type argument } 14"
  },
  {
    "id": 5719,
    "content": "std::initializer_list  By default, the CUDA compiler will implicitly consider the member functions of std::initializer_list to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code The nvcc flag --no-host-device-initializer-list will disable this behavior; member functions of std::initializer_list will then be considered as __host__"
  },
  {
    "id": 5720,
    "content": "functions and will not be directly invokable from device code Example: #include __device__ int foo ( std :: initializer_list in ); __device__ void bar ( void ) { foo ({ 4 , 5 , 6 }); (a) initializer list containing only constant expressions int i = 4 ; foo ({ i , 5 , 6 }); (b) initializer list with at least one non-constant element"
  },
  {
    "id": 5726,
    "content": "Rvalue references  By default, the CUDA compiler will implicitly consider std::move and std::forward function templates to have __host__ __device__ execution space specifiers, and therefore they can be invoked directly from device code The nvcc flag --no-host-device-move-forward will disable this behavior; std::move and std::forward will then be considered as __host__ functions and will not be"
  },
  {
    "id": 5732,
    "content": "Constexpr functions and function templates  By default, a constexpr function cannot be called from a function with incompatible execution space 22 When this flag is specified, host code can invoke a __device__ constexpr function and device code can invoke a __host__ constexpr function nvcc will define the macro __CUDACC_RELAXED_CONSTEXPR__ when --expt-relaxed-constexpr has been specified Note"
  },
  {
    "id": 5733,
    "content": "that a function template instantiation may not be a constexpr function even if the corresponding template is marked with the keyword constexpr (C++11 Standard Section [dcl constexpr"
  },
  {
    "id": 5739,
    "content": "Constexpr variables  Let ‘V’ denote a namespace scope variable or a class static member variable that has been marked constexpr and that does not have execution space annotations (e"
  },
  {
    "id": 5742,
    "content": "If V is of scalar type 24 other than long double and the type is not volatile-qualified, the value of V can be directly used in device code In addition, if V is of a non-scalar type then scalar elements of V can be used inside a constexpr __device__ or __host__ __device__ function, if the call to the function is a constant expression 25 Example: constexpr int xxx = 10 ; constexpr int yyy = xxx +"
  },
  {
    "id": 5743,
    "content": "4 ; struct S1_t { static constexpr int qqq = 100 ; }; constexpr int host_arr [] = { 1 , 2 , 3 }; constexpr __device__ int get ( int idx ) { return host_arr [ idx ]; } __device__ int foo ( int idx ) { int v1 = xxx + yyy + S1_t :: qqq ; OK const int & v2 = xxx ; error: reference to host constexpr variable const int * v3 = & xxx ; error: address of host constexpr variable const int & v4 = S1_t :: qqq"
  },
  {
    "id": 5744,
    "content": "; error: reference to host constexpr variable const int * v5 = & S1_t :: qqq ; error: address of host constexpr variable v1 += get ( 2 ); OK: 'get(2)' is a constant expression v1 += get ( idx ); error: 'get(idx)' is not a constant expression v1 += host_arr [ 2 ]; error: 'host_arr' does not have scalar type"
  },
  {
    "id": 5745,
    "content": "Inline namespaces  For an input CUDA translation unit, the CUDA compiler may invoke the host compiler for compiling the host code within the translation unit In the code passed to the host compiler, the CUDA compiler will inject additional compiler generated code, if the input CUDA translation unit contained a definition of any of the following entities: __global__ function or function template"
  },
  {
    "id": 5746,
    "content": "instantiation __device__ , __constant__ variables with surface or texture type The compiler generated code contains a reference to the defined entity If the entity is defined within an inline namespace and another entity of the same name and type signature is defined in an enclosing namespace, this reference may be considered ambiguous by the host compiler and host compilation will fail This"
  },
  {
    "id": 5747,
    "content": "limitation can be avoided by using unique names for such entities defined within an inline namespace"
  },
  {
    "id": 5748,
    "content": "Example: __device__ int Gvar ; inline namespace N1 { __device__ int Gvar ; }   __global__ void foo ( void );   error __global__ void bar ( void ) { }   error template <> __global__ void foo ( void ) { }   error __device__ int x1b ;   error __constant__ int x2b ;   error __shared__ int x3b ;   error texture q2 ;   error surface s2 ;   error } }; 14"
  },
  {
    "id": 5757,
    "content": "__global__ functions and function templates  If the closure type associated with a lambda expression is used in a template argument of a __global__ function template instantiation, the lambda expression must either be defined in the immediate or nested block scope of a __device__ or __global__ function, or must be an extended lambda Example: template __global__ void kernel ( T in ) { }"
  },
  {
    "id": 5758,
    "content": "__device__ void foo_device ( void ) { All kernel instantiations in this function are valid, since the lambdas are defined inside a __device__ function kernel >> ( [] __device__ { } ); kernel >> ( [] __host__ __device__ { } ); kernel >> ( [] { } ); } auto lam1 = [] { }; auto lam2 = [] __host__ __device__ { }; void foo_host ( void ) { OK: instantiated with closure type of an extended __device__"
  },
  {
    "id": 5759,
    "content": "lambda kernel >> ( [] __device__ { } ); OK: instantiated with closure type of an extended __host__ __device__ lambda kernel >> ( [] __host__ __device__ { } ); error: unsupported: instantiated with closure type of a lambda that is not an extended lambda kernel >> ( [] { } ); error: unsupported: instantiated with closure type of a lambda that is not an extended lambda kernel >> ( lam1 ); error:"
  },
  {
    "id": 5760,
    "content": "unsupported: instantiated with closure type of a lambda that is not an extended lambda kernel >> ( lam2 ); } A __global__ function or function template cannot be declared as constexpr A __global__ function or function template cannot have a parameter of type std::initializer_list or va_list A variadic __global__ function template has the following restrictions: Only a single pack parameter is"
  },
  {
    "id": 5761,
    "content": "allowed Pack > __global__ void foo1 ( Wrapper ); error: pack parameter is not last in parameter list template class Wrapper > __global__ void foo2 ( Wrapper ); error: multiple parameter packs template class Wrapper1 , template class Wrapper2 > __global__ void foo3 ( Wrapper1 , Wrapper2 ); 14"
  },
  {
    "id": 5765,
    "content": "__managed__ and __shared__ variables  `__managed__ and __shared__ variables cannot be marked with the keyword constexpr"
  },
  {
    "id": 5770,
    "content": "Defaulted functions  Execution space specifiers on a function that is explicitly-defaulted on its first declaration are ignored by the CUDA compiler Instead, the CUDA compiler will infer the execution space specifiers as described in Implicitly-declared and explicitly-defaulted functions Execution space specifiers are not ignored if the function is explicitly-defaulted, but not on its first"
  },
  {
    "id": 5771,
    "content": "declaration Example: struct S1 { warning: __host__ annotation is ignored on a function that is explicitly-defaulted on its first declaration __host__ S1 () = default ; }; __device__ void foo1 () { note: __device__ execution space is derived for S1::S1 based on implicit call from within __device__ function foo1 S1 s1 ; } struct S2 { __host__ S2 (); }; note: S2::S2 is not defaulted on its first"
  },
  {
    "id": 5772,
    "content": "declaration, and its execution space is fixed to __host__ based on its first declaration S2 :: S2 () = default ; __device__ void foo2 () { error: call from __device__ function 'foo2' to __host__ function 'S2::S2' S2 s2 ; } 14"
  },
  {
    "id": 5775,
    "content": "C++14 Features  C++14 features enabled by default by the host compiler are also supported by nvcc Passing nvcc -std=c++14 flag turns on all C++14 features and also invokes the host preprocessor, compiler and linker with the corresponding C++14 dialect option 26 This section describes the restrictions on the supported C++14 features 14"
  },
  {
    "id": 5779,
    "content": "Functions with deduced return type  A __global__ function cannot have a deduced return type If a __device__ function has deduced return type, the CUDA frontend compiler will change the function declaration to have a void return type, before invoking the host compiler This may cause issues for introspecting the deduced return type of the __device__ function in host code Thus, the CUDA compiler"
  },
  {
    "id": 5780,
    "content": "will issue compile-time errors for referencing such deduced return type outside device function bodies, except if the reference is absent when __CUDA_ARCH__ is undefined Examples: __device__ auto fn1 ( int x ) { return x ; } __device__ decltype ( auto ) fn2 ( int x ) { return x ; } __device__ void device_fn1 () { OK int ( * p1 )( int ) = fn1 ; } error: referenced outside device function bodies"
  },
  {
    "id": 5781,
    "content": "decltype ( fn1 ( 10 )) g1 ; void host_fn1 () { error: referenced outside device function bodies int ( * p1 )( int ) = fn1 ; struct S_local_t { error: referenced outside device function bodies decltype ( fn2 ( 10 )) m1 ; S_local_t () : m1 ( 10 ) { } }; } error: referenced outside device function bodies template void host_fn2 () { } template struct S1_t { }; error: referenced outside device function"
  },
  {
    "id": 5786,
    "content": "Variable templates  A __device__/__constant__ variable template cannot have a const qualified type when using the Microsoft host compiler Examples: error: a __device__ variable template cannot have a const qualified type on Windows template __device__ const T d1 ( 2 ); int * const x = nullptr ; error: a __device__ variable template cannot have a const qualified type on Windows template"
  },
  {
    "id": 5787,
    "content": "__device__ T * const d2 ( x ); OK template __device__ const T * d3 ; __device__ void fn () { int t1 = d1 ; int * const t2 = d2 ; const int * t3 = d3 ; } 14"
  },
  {
    "id": 5790,
    "content": "C++17 Features  C++17 features enabled by default by the host compiler are also supported by nvcc Passing nvcc -std=c++17 flag turns on all C++17 features and also invokes the host preprocessor, compiler and linker with the corresponding C++17 dialect option 27 This section describes the restrictions on the supported C++17 features"
  },
  {
    "id": 5795,
    "content": "Inline Variable  A namespace scope inline variable declared with __device__ or __constant__ or __managed__ memory space specifier must have internal linkage, if the code is compiled with nvcc in whole program compilation mode Examples: inline __device__ int xxx ; error when compiled with nvcc in whole program compilation mode static inline __device__ int yyy ; ok: internal linkage namespace {"
  },
  {
    "id": 5796,
    "content": "inline __device__ int zzz ; ok: internal linkage } When using g++ host compiler, an inline variable declared with __managed__ memory space specifier may not be visible to the debugger"
  },
  {
    "id": 5805,
    "content": "C++20 Features  C++20 features enabled by default by the host compiler are also supported by nvcc Passing nvcc -std=c++20 flag turns on all C++20 features and also invokes the host preprocessor, compiler and linker with the corresponding C++20 dialect option 28 This section describes the restrictions on the supported C++20 features"
  },
  {
    "id": 5817,
    "content": "Uses of the co_await , co_yield and co_return keywords in the scope of a device function are diagnosed as error during device compilation"
  },
  {
    "id": 5822,
    "content": "Three-way comparison operator  The three-way comparison operator is supported in both host and device code, but some uses implicitly rely on functionality from the Standard Template Library provided by the host implementation Uses of those operators may require specifying the flag --expt-relaxed-constexpr to silence warnings and the functionality requires that the host implementation satisfies"
  },
  {
    "id": 5824,
    "content": "Instances of nvstd::function in device code cannot be initialized with the address of a __host__ function or with a functor whose operator() is a __host__ function nvstd::function instances cannot be passed from host code to device code (and vice versa) at run time nvstd::function cannot be used in the parameter type of a __global__ function, if the __global__ function is launched from host code"
  },
  {
    "id": 5825,
    "content": "Extended Lambdas  The nvcc flag '--extended-lambda' allows explicit execution space annotations in a lambda expression 29 The execution space annotations should be present after the ‘lambda-introducer’ and before the optional ‘lambda-declarator’ nvcc will define the macro __CUDACC_EXTENDED_LAMBDA__ when the '--extended-lambda' flag has been specified An ‘extended __device__ lambda’ is a lambda"
  },
  {
    "id": 5826,
    "content": "expression that is annotated explicitly with ‘ __device__ ’, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__ function An ‘extended __host__ __device__ lambda’ is a lambda expression that is annotated explicitly with both ‘ __host__ ’ and ‘ __device__ ’, and is defined within the immediate or nested block scope of a __host__ or __host__ __device__"
  },
  {
    "id": 5827,
    "content": "function An ‘extended lambda’ denotes either an extended __device__ lambda or an extended __host__ __device__ lambda Extended lambdas can be used in the type arguments of __global__ function template instantiation If the execution space annotations are not explicitly specified, they are computed based on the scopes enclosing the closure class associated with the lambda, as described in the section"
  },
  {
    "id": 5828,
    "content": "on C++11 support The execution space annotations are applied to all methods of the closure class associated with the lambda auto lam1 = [] { }; auto lam2 = [] __device__ { }; auto lam3 = [] __host__ __device__ { }; auto lam4 = [] __host__ { }; } lam1 and lam2 are not extended lambdas because they are not defined within a __host__ or __host__ __device__ function Extended Lambda Type Traits  The"
  },
  {
    "id": 5829,
    "content": "compiler provides type traits to detect closure types for extended lambdas at compile time: __nv_is_extended_device_lambda_closure_type(type) : If ‘type’ is the closure class created for an extended __device__ lambda, then the trait is true, otherwise it is false __nv_is_extended_device_lambda_with_preserved_return_type(type) : If ‘type’ is the closure class created for an extended __device__"
  },
  {
    "id": 5830,
    "content": "lambda and the lambda is defined with trailing return type (with restriction), then the trait is true, otherwise it is false If the trailing return type definition refers to any lambda parameter name, the return type is not preserved __nv_is_extended_host_device_lambda_closure_type(type) : If ‘type’ is the closure class created for an extended __host__ __device__ lambda, then the trait is true,"
  },
  {
    "id": 5831,
    "content": "otherwise it is false These traits can be used in all compilation modes, irrespective of whether lambdas or extended lambdas are enabled 30"
  },
  {
    "id": 5832,
    "content": "static_assert ( IS_D_LAMBDA ( decltype ( lam5 )), \"\" ); static_assert ( IS_DPRT_LAMBDA ( decltype ( lam5 )), \"\" ); static_assert ( IS_HD_LAMBDA ( decltype ( lam5 )), \"\" ); } 14"
  },
  {
    "id": 5835,
    "content": "Extended Lambda Restrictions  The CUDA compiler will replace an extended lambda expression with an instance of a placeholder type defined in namespace scope, before invoking the host compiler The template argument of the placeholder type requires taking the address of a function enclosing the original extended lambda expression This is required for the correct execution of any __global__"
  },
  {
    "id": 5836,
    "content": "function template whose template argument involves the closure type of an extended lambda By definition, the extended lambda is present within the immediate or nested block scope of a __host__ or __host__ __device__ function If this function is not the operator() of a lambda expression, then it is considered the enclosing function for the extended lambda Otherwise, the extended lambda is defined"
  },
  {
    "id": 5837,
    "content": "within the immediate or nested block scope of the operator() of one or more enclosing lambda expressions If the outermost such lambda expression is defined in the immediate or nested block scope of a function F , then F is the computed enclosing function, else the enclosing function does not exist Example: void foo ( void ) { enclosing function for lam1 is \"foo\" auto lam1 = [] __device__ { }; auto"
  },
  {
    "id": 5838,
    "content": "lam2 = [] { auto lam3 = [] { enclosing function for lam4 is \"foo\" auto lam4 = [] __host__ __device__ { }; }; }; } auto lam6 = [] { enclosing function for lam7 does not exist auto lam7 = [] __host__ __device__ { }; }; Here are the restrictions on extended lambdas: An extended lambda cannot be defined inside another extended lambda expression Example: void foo ( void ) { auto lam1 = [] __host__"
  },
  {
    "id": 5839,
    "content": "__device__ { error: extended lambda defined within another extended lambda auto lam2 = [] __host__ __device__ { }; }; } An extended lambda cannot be defined inside a generic lambda expression Example: void foo ( void ) { auto lam1 = [] ( auto ) { error: extended lambda defined within a generic lambda auto lam2 = [] __host__ __device__ { }; }; } If an extended lambda is defined within the immediate"
  },
  {
    "id": 5840,
    "content": "or nested block scope of one or more nested lambda expression, the outermost such lambda expression must be defined inside the immediate or nested block scope of a function Example: auto lam1 = [] { error: outer enclosing lambda is not defined within a non-lambda-operator() function auto lam2 = [] __host__ __device__ { }; }; The enclosing function for the extended lambda must be named and its"
  },
  {
    "id": 5841,
    "content": "address can be taken If the enclosing function is a class member, then the following conditions must be satisfied: All classes enclosing the member function must have a name All enclosing classes must not have private or protected access within their respective parent classes Example: void foo ( void ) { OK auto lam1 = [] __device__ { return 0 ; }; { OK auto lam2 = [] __device__ { return 0 ; }; OK"
  },
  {
    "id": 5842,
    "content": "auto lam3 = [] __device__ __host__ { return 0 ; }; } } struct S1_t { S1_t ( void ) { Error: cannot take address of enclosing function auto lam4 = [] __device__ { return 0 ; }; } }; class C0_t { void foo ( void ) { Error: enclosing function has private access in parent class auto temp1 = [] __device__ { return 10 ; }; } struct S2_t { void foo ( void ) { Error: enclosing class S2_t has private"
  },
  {
    "id": 5843,
    "content": "access in its parent class auto temp1 = [] __device__ { return 10 ; }; } }; }; It must be possible to take the address of the enclosing routine unambiguously, at the point where the extended lambda has been defined Example: template struct A { typedef void Bar ; void test (); }; template <> struct A { }; template void A :: test () { /* In code sent to host compiler, nvcc will inject an address"
  },
  {
    "id": 5844,
    "content": "expression here, of the form: (void (A ::*)(void))(&A::test)) However, the class typedef 'Bar' (to void) shadows the template argument 'Bar', causing the address expression in A::test to actually refer to: (void (A ::*)(void))(&A::test)) which doesn't take the address of the enclosing routine 'A::test' correctly Example: void foo ( void ) { struct S1_t { void bar ( void ) { Error: bar is member of"
  },
  {
    "id": 5845,
    "content": "a class that is local to a function auto lam4 = [] __host__ __device__ { return 0 ; }; } }; } The enclosing function for an extended lambda cannot have deduced return type auto lam1 = [] __host__ __device__ { return 0 ; }; } __host__ __device__ extended lambdas cannot be generic lambdas Example: void foo ( void ) { Error: __host__ __device__ extended lambdas cannot be generic lambdas auto lam1 ="
  },
  {
    "id": 5846,
    "content": "[] __host__ __device__ ( auto i ) { return i ; }; Error: __host__ __device__ extended lambdas cannot be generic lambdas i ) { return sizeof ( i ); }; } If the enclosing function is an instantiation of a function template or a member function template, and/or the function is a member of a class template, the template(s) must satisfy the following constraints: The template must have at most one"
  },
  {
    "id": 5847,
    "content": "variadic parameter, and it must be listed last in the template parameter list The template instantiation argument types cannot involve types that are either local to a function (except for closure types for extended lambdas), or are private or protected class members Example: template __global__ void kern ( T in ) { in (); } template struct foo {}; template class T , typename P2 > void bar1 ("
  },
  {
    "id": 5848,
    "content": "const T , const T ) { Error: enclosing function has multiple parameter packs auto lam1 = [] __device__ { return 10 ; }; } template class T , typename P1 , typename T2 > void bar2 ( const T , T2 ) { Error: for enclosing function, the parameter pack is not last in the template parameter list auto lam1 = [] __device__ { return 10 ; }; } template void bar3 ( void ) { Error: for enclosing function, the"
  },
  {
    "id": 5849,
    "content": "second template parameter is not named auto lam1 = [] __device__ { return 10 ; }; } int main () { foo f1 ; foo f2 ; bar1 ( f1 , f2 ); bar2 ( f1 , 10 ); bar3 (); } Example: template __global__ void kern ( T in ) { in (); } template void bar4 ( void ) { auto lam1 = [] __device__ { return 10 ; }; kern >> ( lam1 ); } struct C1_t { struct S1_t { }; friend int main ( void ); }; int main () { struct S1_t"
  },
  {
    "id": 5850,
    "content": "{ }; Error: enclosing function for device lambda in bar4 is instantiated with a type local to main bar4 (); Error: enclosing function for device lambda in bar4 is instantiated with a type that is a private member of a class bar4 (); } With Visual Studio host compilers, the enclosing function must have external linkage The restriction is present because this host compiler does not support using the"
  },
  {
    "id": 5851,
    "content": "address of non-extern linkage functions as template arguments, which is needed by the CUDA compiler transformations to support extended lambdas With Visual Studio host compilers, an extended lambda shall not be defined within the body of an ‘if-constexpr’ block An extended lambda has the following restrictions on captured variables: In the code sent to the host compiler, the variable may be passed"
  },
  {
    "id": 5852,
    "content": "by value to a sequence of helper functions before being used to direct-initialize the field of the class type used to represent the closure type for the extended lambda 31 A variable of array type cannot be captured if the number of array dimensions is greater than 7 For a variable of array type, in the code sent to the host compiler, the closure type’s array field is first default-initialized,"
  },
  {
    "id": 5853,
    "content": "and then each element of the array field is copy-assigned from the corresponding element of the captured array variable Therefore, the array element type must be default-constructible and copy-assignable in host code The type of the captured variable cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members"
  },
  {
    "id": 5854,
    "content": "For a __host__ __device__ extended lambda, the types used in the return or parameter types of the lambda expression’s operator() cannot involve types that are either local to a function (except for closure types of extended lambdas), or are private or protected class members Init-capture is supported for __device__ extended lambdas, except when the init-capture is of array type or of type"
  },
  {
    "id": 5855,
    "content": "std::initializer_list The constexpr and consteval specifier cannot be used in the declaration of an extended lambda A variable cannot be implicitly captured inside an if-constexpr block lexically nested inside an extended lambda, unless it has already been implicitly captured earlier outside the if-constexpr block or appears in the explicit capture list for the extended lambda (see example below)"
  },
  {
    "id": 5856,
    "content": "Example void foo ( void ) { OK: an init-capture is allowed for an extended __device__ lambda auto lam1 = [ x = 1 ] __device__ () { return x ; }; Error: an init-capture is not allowed for an extended __host__ __device__ lambda auto lam2 = [ x = 1 ] __host__ __device__ () { return x ; }; int a = 1 ; Error: an extended __device__ lambda cannot capture variables by reference auto lam3 = [ & a ]"
  },
  {
    "id": 5857,
    "content": "__device__ () { return a ; }; Error: by-reference capture is not allowed for an extended __device__ lambda auto lam4 = [ & x = a ] __device__ () { return x ; }; struct S1_t { }; S1_t s1 ; Error: a type local to a function cannot be used in the type of a captured variable auto lam6 = [ s1 ] __device__ () { }; Error: an init-capture cannot be of type std::initializer_list auto lam7 = [ x = { 11 }]"
  },
  {
    "id": 5858,
    "content": "__device__ () { }; std :: initializer_list b = { 11 , 22 , 33 }; Error: an init-capture cannot be of type std::initializer_list auto lam8 = [ x = b ] __device__ () { }; Error scenario (lam9) and supported scenarios (lam10, lam11) for capture within 'if-constexpr' block int yyy = 4 ; auto lam9 = [ = ] __device__ { int result = 0 ; if constexpr ( false ) { Error: An extended __device__ lambda cannot"
  },
  {
    "id": 5859,
    "content": "first-capture 'yyy' in constexpr-if context result += yyy ; } return result ; }; auto lam10 = [ yyy ] __device__ { int result = 0 ; if constexpr ( false ) { OK: 'yyy' already listed in explicit capture list for the extended lambda result += yyy ; } return result ; }; auto lam11 = [ = ] __device__ { int result = yyy ; if constexpr ( false ) { OK: 'yyy' already implicit captured outside the"
  },
  {
    "id": 5860,
    "content": "'if-constexpr' block result += yyy ; } return result ; }; } When parsing a function, the CUDA compiler assigns a counter value to each extended lambda within that function Hence, whether or not an extended lambda is defined within a function should not depend on a particular value of __CUDA_ARCH__ , or on __CUDA_ARCH__ being undefined Example template __global__ void kernel ( T in ) { in (); }"
  },
  {
    "id": 5861,
    "content": "__host__ __device__ void foo ( void ) { Error: the number and relative declaration order of extended lambdas depends on __CUDA_ARCH__ #if defined(__CUDA_ARCH__) auto lam1 = [] __device__ { return 0 ; }; auto lam1b = [] __host___ __device__ { return 10 ; }; #endif auto lam2 = [] __device__ { return 4 ; }; kernel >> ( lam2 ); } As described above, the CUDA compiler replaces a __device__ extended"
  },
  {
    "id": 5862,
    "content": "lambda defined in a host function with a placeholder type defined in namespace scope Unless the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true for the closure type of the extended lambda, the placeholder type does not define a operator() function equivalent to the original lambda declaration An attempt to determine the return type or parameter types of the"
  },
  {
    "id": 5863,
    "content": "operator() function of such a lambda may therefore work incorrectly in host code, as the code processed by the host compiler will be semantically different than the input code processed by the CUDA compiler However, it is OK to introspect the return type or parameter types of the operator() function within device code Note that this restriction does not apply to __host__ __device__ extended"
  },
  {
    "id": 5864,
    "content": "lambdas, or to __device__ extended lambdas for which the trait __nv_is_extended_device_lambda_with_preserved_return_type() returns true"
  },
  {
    "id": 5866,
    "content": "__nv_is_extended_device_lambda_with_preserved_return_type ( decltype ( lam4 )), \"\" ); } For an extended device lambda: - Introspecting the parameter type of operator() is only supported in device code - Introspecting the return type of operator() is supported only in device code, unless the trait function __nv_is_extended_device_lambda_with_preserved_return_type() returns true If the functor"
  },
  {
    "id": 5869,
    "content": ", as the argument of a __global__ function), then any expression in the body of the lambda expression that captures variables must be remain unchanged irrespective of whether the __CUDA_ARCH__ macro is defined, and whether the macro has a particular value This restriction arises because the lambda’s closure class layout depends on the order in which captured variables are encountered when the"
  },
  {
    "id": 5870,
    "content": "compiler processes the lambda expression; the program may execute incorrectly if the closure class layout differs in device and host compilation"
  },
  {
    "id": 5871,
    "content": "Example __device__ int result ; template __global__ void kernel ( T in ) { result = in (); } void foo ( void ) { int x1 = 1 ; auto lam1 = [ = ] __host__ __device__ { Error: \"x1\" is only captured when __CUDA_ARCH__ is defined #ifdef __CUDA_ARCH__ return x1 + 1 ; #else return 10 ; #endif }; kernel >> ( lam1 ); } As described previously, the CUDA compiler replaces an extended __device__ lambda"
  },
  {
    "id": 5872,
    "content": "expression with an instance of a placeholder type in the code sent to the host compiler This placeholder type does not define a pointer-to-function conversion operator in host code, however the conversion operator is provided in device code Example template __global__ void kern ( T in ) { int ( * fp )( double ) = in ; OK: conversion in device code is supported fp ( 0 ); auto lam1 = []( double ) {"
  },
  {
    "id": 5873,
    "content": "return 1 ; }; OK: conversion in device code is supported fp = lam1 ; fp ( 0 ); } void foo ( void ) { auto lam_d = [] __device__ ( double ) { return 1 ; }; auto lam_hd = [] __host__ __device__ ( double ) { return 1 ; }; kern >> ( lam_d ); kern >> ( lam_hd ); OK : conversion for __host__ __device__ lambda is supported in host code int ( * fp )( double ) = lam_hd ; Error: conversion for __device__"
  },
  {
    "id": 5874,
    "content": "lambda is not supported in host code int ( * fp2 )( double ) = lam_d ; } As described previously, the CUDA compiler replaces an extended __device__ or __host__ __device__ lambda expression with an instance of a placeholder type in the code sent to the host compiler As a result, some standard C++ type traits may return different results for the closure type of the extended lambda, in the CUDA"
  },
  {
    "id": 5876,
    "content": "The following type traits are affected: std::is_trivially_copyable , std::is_trivially_constructible , std::is_trivially_copy_constructible , std::is_trivially_move_constructible , std::is_trivially_destructible"
  },
  {
    "id": 5877,
    "content": "Care must be taken that the results of these type traits are not used in __global__ function template instantiation or in __device__ / __constant__ / __managed__ variable template instantiation"
  },
  {
    "id": 5878,
    "content": "Example template void __global__ foo () { printf ( \"hi\" ); } template void dolaunch () { ERROR: this kernel launch may fail, because CUDA frontend compiler and host compiler may disagree on the result of std::is_trivially_copyable() trait on the closure type of the extended lambda foo :: value >>> (); cudaDeviceSynchronize (); } int main () { int x = 0 ; auto lam1 = [ = ] __host__ __device__ () {"
  },
  {
    "id": 5879,
    "content": "return x ; }; dolaunch (); } The CUDA compiler will generate compiler diagnostics for a subset of cases described in 1-12; no diagnostic will be generated for cases 13-17, but the host compiler may fail to compile the generated code"
  },
  {
    "id": 5883,
    "content": "Notes on __host__ __device__ lambdas  Unlike __device__ lambdas, __host__ __device__ lambdas can be called from host code"
  },
  {
    "id": 5884,
    "content": "As described earlier, the CUDA compiler replaces an extended lambda expression defined in host code with an instance of a named placeholder type The placeholder type for an extended __host__ __device__ lambda invokes the original lambda’s operator() with an indirect function call 30 The presence of the indirect function call may cause an extended __host__ __device__ lambda to be less optimized by"
  },
  {
    "id": 5885,
    "content": "the host compiler than lambdas that are implicitly or explicitly __host__ only In the latter case, the host compiler can easily inline the body of the lambda into the calling context But in case of an extended __host__ __device__ lambda, the host compiler encounters the indirect function call and may not be able to easily inline the original __host__ __device__ lambda body"
  },
  {
    "id": 5889,
    "content": "*this Capture By Value  When a lambda is defined within a non-static class member function, and the body of the lambda refers to a class member variable, C++11/C++14 rules require that the this pointer of the class is captured by value, instead of the referenced member variable If the lambda is an extended __device__ or __host__ __device__ lambda defined in a host function, and the lambda is"
  },
  {
    "id": 5890,
    "content": "executed on the GPU, accessing the referenced member variable on the GPU will cause a run time error if the this pointer points to host memory"
  },
  {
    "id": 5891,
    "content": "Example: #include template __global__ void foo ( T in ) { printf ( \" value = %d\" , in ()); } struct S1_t { int xxx ; __host__ __device__ S1_t ( void ) : xxx ( 10 ) { }; void doit ( void ) { auto lam1 = [ = ] __device__ { reference to \"xxx\" causes the 'this' pointer (S1_t*) to be captured by value return xxx + 1 ; }; Kernel launch fails at run time because 'this->xxx' is not accessible from the"
  },
  {
    "id": 5893,
    "content": "In this mode, the compiler makes a copy of the object denoted by “*this” instead of capturing the pointer this by value The “*this” capture mode is described in more detail here: http: www"
  },
  {
    "id": 5897,
    "content": "The CUDA compiler supports the “*this” capture mode for lambdas defined within __device__ and __global__ functions and for extended __device__ lambdas defined in host code, when the --extended-lambda nvcc flag is used Here’s the above example modified to use “*this” capture mode: #include template __global__ void foo ( T in ) { printf ( \" value = %d\" , in ()); } struct S1_t { int xxx ; __host__"
  },
  {
    "id": 5898,
    "content": "__device__ S1_t ( void ) : xxx ( 10 ) { }; void doit ( void ) { note the \"*this\" capture specification auto lam1 = [ = , * this ] __device__ { reference to \"xxx\" causes the object denoted by '*this' to be captured by value, and the GPU code will access copy_of_star_this->xxx return xxx + 1 ; }; Kernel launch succeeds foo >> ( lam1 ); cudaDeviceSynchronize (); } }; int main ( void ) { S1_t s1 ; s1"
  },
  {
    "id": 5899,
    "content": "doit (); } “*this” capture mode is not allowed for unannotated lambdas defined in host code, or for extended __host__ __device__ lambdas Additional Notes  ADL Lookup : As described earlier, the CUDA compiler will replace an extended lambda expression with an instance of a placeholder type, before invoking the host compiler One template argument of the placeholder type uses the address of the"
  },
  {
    "id": 5900,
    "content": "function enclosing the original lambda expression This may cause additional namespaces to participate in argument dependent lookup (ADL), for any host function call whose argument types involve the closure type of the extended lambda expression Example: namespace N1 { struct S1_t { }; template void foo ( T ); }; namespace N2 { template int foo ( T ); template void doit ( T in ) { foo ( in ); } }"
  },
  {
    "id": 5901,
    "content": "void bar ( N1 :: S1_t in ) { /* extended __device__ lambda In the code sent to the host compiler, this is replaced with the placeholder type instantiation expression ' __nv_dl_wrapper_t > { }' As a result, the namespace 'N1' participates in ADL lookup of the call to \"foo\" in the body of N2::doit, causing ambiguity */ auto lam1 = [ = ] __device__ { }; N2 :: doit ( lam1 ); } In the example above,"
  },
  {
    "id": 5902,
    "content": "the CUDA compiler replaced the extended lambda with a placeholder type that involves the N1 namespace As a result, the namespace N1 participates in the ADL lookup for foo(in) in the body of N2::doit , and host compilation fails because multiple overload candidates N1::foo and N2::foo are found"
  },
  {
    "id": 5908,
    "content": "Data Aggregation Class  class PixelRGBA { public : __device__ PixelRGBA () : r_ ( 0 ), g_ ( 0 ), b_ ( 0 ), a_ ( 0 ) { } __device__ PixelRGBA ( unsigned char r , unsigned char g , unsigned char b , unsigned char a = 255 ) : r_ ( r ), g_ ( g ), b_ ( b ), a_ ( a ) { } private : unsigned char r_ , g_ , b_ , a_ ; friend PixelRGBA operator + ( const PixelRGBA & , const PixelRGBA & ); }; __device__"
  },
  {
    "id": 5910,
    "content": "Derived Class  __device__ void * operator new ( size_t bytes , MemoryPool & p ); __device__ void operator delete ( void * , MemoryPool & p ); class Shape { public : __device__ Shape ( void ) { } __device__ void putThis ( PrintBuffer * p ) const ; __device__ virtual void Draw ( PrintBuffer * p ) const { p -> put ( \"Shapeless\" ); } __device__ virtual ~ Shape () {} }; class Point : public Shape {"
  },
  {
    "id": 5911,
    "content": "public : __device__ Point () : x ( 0 ), y ( 0 ) {} __device__ Point ( int ix , int iy ) : x ( ix ), y ( iy ) { } __device__ void PutCoord ( PrintBuffer * p ) const ; __device__ void Draw ( PrintBuffer * p ) const ; __device__ ~ Point () {} private : int x , y ; }; __device__ Shape * GetPointObj ( MemoryPool & pool ) { Shape * shape = new ( pool ) Point ( rand ( -20 , 10 ), rand ( -100 , -20 ));"
  },
  {
    "id": 5915,
    "content": "Class Template  template class myValues { T values [ MAX_VALUES ]; public : __device__ myValues ( T clear ) { } }; template void __global__ useValues ( T * memoryBuffer ) { myValues myLocation ( 0 ); useValues >> ( buffer );"
  },
  {
    "id": 5920,
    "content": "return ( ); } template <> __device__ bool func ( T x )   Specialization { return true ; }   Explicit argument specification bool result = func ( 0 5 );   Implicit argument deduction int x = 1 ; bool result = func ( x ); 14"
  },
  {
    "id": 5923,
    "content": "Functor Class  class Add { public : __device__ float operator () ( float a , float b ) const { return a + b ; } }; class Sub { public : __device__ float operator () ( float a , float b ) const { return a - b ; } };   Device code template __global__ void VectorOperation ( const float * A , const float * B , float * C , unsigned int N , O op ) { unsigned int iElement = blockDim"
  },
  {
    "id": 5924,
    "content": "16 This does not apply to entities that may be defined in more than one translation unit, such as compiler generated template instantiations"
  },
  {
    "id": 5925,
    "content": "17 The intent is to allow variable memory space specifiers for static variables in a __host__ __device__ function during device compilation, but disallow it during host compilation 18 One way to debug suspected layout mismatch of a type C is to use printf to output the values of sizeof(C) and offsetof(C, field) in host and device code"
  },
  {
    "id": 5927,
    "content": "20 At present, the -std=c++11 flag is supported only for the following host compilers : gcc version >= 4"
  },
  {
    "id": 5931,
    "content": "const] 26 At present, the -std=c++14 flag is supported only for the following host compilers : gcc version >= 5"
  },
  {
    "id": 5932,
    "content": "1, clang version >= 3 7 and icc version >= 17 27 At present, the -std=c++17 flag is supported only for the following host compilers : gcc version >= 7 0, clang version >= 8 0, Visual Studio version >= 2017, pgi compiler version >= 19 0, icc compiler version >= 19 0 28 At present, the -std=c++20 flag is supported only for the following host compilers : gcc version >= 10 0, clang version >= 10 0,"
  },
  {
    "id": 5935,
    "content": "31 In contrast, the C++ standard specifies that the captured variable is used to direct-initialize the field of the closure type"
  },
  {
    "id": 5937,
    "content": "Texture Fetching  This section gives the formula used to compute the value returned by the texture functions of Texture Functions depending on the various attributes of the texture object (see Texture and Surface Memory ) The texture bound to the texture object is represented as an array T of N texels for a one-dimensional texture, N x M texels for a two-dimensional texture, N x M x L texels for"
  },
  {
    "id": 5938,
    "content": "a three-dimensional texture It is fetched using non-normalized texture coordinates x , y , and z , or the normalized texture coordinates x/N , y/M , and z/L as described in Texture Memory Texture Memory explained how out-of-range coordinates are remapped to the valid range based on the addressing mode"
  },
  {
    "id": 5941,
    "content": "Nearest-Point Sampling  In this filtering mode, the value returned by the texture fetch is tex(x)=T[i] for a one-dimensional texture, tex(x,y)=T[i,j] for a two-dimensional texture, tex(x,y,z)=T[i,j,k] for a three-dimensional texture, where i=floor(x) , j=floor(y) , and k=floor(z)"
  },
  {
    "id": 5942,
    "content": "Table Lookup  A table lookup TL(x) where x spans the interval [0,R] can be implemented as TL(x)=tex((N-1)/R)x+0 5) in order to ensure that TL(0)=T[0] and TL(R)=T[N-1] Figure 34 illustrates the use of texture filtering to implement a table lookup with R=4 or R=1 from a one-dimensional texture with N=4"
  },
  {
    "id": 5943,
    "content": "Compute Capabilities  The general specifications and features of a compute device depend on its compute capability (see Compute Capability ) Table 20 and Table 21 show the features and technical specifications associated with each compute capability that is currently supported Sections Compute Capability 5 x , Compute Capability 6 x , Compute Capability 7 x , Compute Capability 8 x and Compute"
  },
  {
    "id": 5952,
    "content": "Feature Availability  A compute feature is introduced with a compute architecture with the intention that the feature will be available on all subsequent architectures This is shown in Table 20 by the “yes” for availability of a feature on compute capabilities subsequent to its introduction Highly specialized compute features that are introduced with an architecture may not be guaranteed to be"
  },
  {
    "id": 5953,
    "content": "available on all subsequent compute capabilities These features target acceleration of specialized operations which are not intended for all classes of compute capabilities (denoted by the compute capability’s minor number) or are likely to significantly change on future generations (denoted by the compute capability’s major number) There are potentially two sets of compute features for a given"
  },
  {
    "id": 5954,
    "content": "compute capability: Compute Capability # # : The predominant set of compute features that are introduced with the intent to be available for subsequent compute architectures Compute Capability # #a : A small and highly specialized set of features that are introduced to accelerate specialized operations, which are not guaranteed to be available or might change significantly on subsequent compute"
  },
  {
    "id": 5955,
    "content": "architecture A feature which appears in device code must be available for the targeted compute capability For example: The compute_90 compilation target allows use of Compute Capability 9 0 features but does not allow use of Compute Capability 9 0a features"
  },
  {
    "id": 5956,
    "content": "Floating-Point Standard  All compute devices follow the IEEE 754-2008 standard for binary floating-point arithmetic with the following deviations: There is no dynamically configurable rounding mode; however, most of the operations support multiple IEEE rounding modes, exposed via device intrinsics There is no mechanism for detecting that a floating-point exception has occurred and all operations"
  },
  {
    "id": 5957,
    "content": "behave as if the IEEE-754 exceptions are always masked, and deliver the masked response as defined by IEEE-754 if there is an exceptional event"
  },
  {
    "id": 5958,
    "content": "For the same reason, while SNaN encodings are supported, they are not signaling and are handled as quiet"
  },
  {
    "id": 5959,
    "content": "The result of a single-precision floating-point operation involving one or more input NaNs is the quiet NaN of bit pattern 0x7fffffff Double-precision floating-point absolute value and negation are not compliant with IEEE-754 with respect to NaNs; these are passed through unchanged"
  },
  {
    "id": 5960,
    "content": "Code must be compiled with -ftz=false , -prec-div=true , and -prec-sqrt=true to ensure IEEE compliance (this is the default setting; see the nvcc user manual for description of these compilation flags)"
  },
  {
    "id": 5961,
    "content": "Regardless of the setting of the compiler flag -ftz , atomic single-precision floating-point adds on global memory always operate in flush-to-zero mode, i"
  },
  {
    "id": 5966,
    "content": "RN , atomic single-precision floating-point adds on shared memory always operate with denormal support, i"
  },
  {
    "id": 5971,
    "content": "In accordance to the IEEE-754R standard, if one of the input parameters to fminf() , fmin() , fmaxf() , or fmax() is NaN, but not the other, the result is the non-NaN parameter The conversion of a floating-point value to an integer value in the case where the floating-point value falls outside the range of the integer format is left undefined by IEEE-754 The behavior of integer division by zero"
  },
  {
    "id": 5973,
    "content": "For compute devices, there is no mechanism for detecting that such integer operation exceptions have occurred"
  },
  {
    "id": 5975,
    "content": "nvidia com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus includes more information on the floating point accuracy and compliance of NVIDIA GPUs"
  },
  {
    "id": 5982,
    "content": "Architecture  An SM consists of: 128 CUDA cores for arithmetic operations (see Arithmetic Instructions for throughputs of arithmetic operations), 32 special function units for single-precision floating-point transcendental functions, 4 warp schedulers"
  },
  {
    "id": 5983,
    "content": "When an SM is given warps to execute, it first distributes them among the four schedulers Then, at every instruction issue time, each scheduler issues one instruction for one of its assigned warps that is ready to execute, if any"
  },
  {
    "id": 5984,
    "content": "An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified L1/texture cache of 24 KB used to cache reads from global memory, 64 KB of shared memory for devices of compute capability 5 0 or 96 KB of shared memory for devices of compute capability 5"
  },
  {
    "id": 5986,
    "content": "The unified L1/texture cache is also used by the texture unit that implements the various addressing modes and data filtering mentioned in Texture and Surface Memory There is also an L2 cache shared by all SMs that is used to cache accesses to local or global memory, including temporary register spills Applications may query the L2 cache size by checking the l2CacheSize device property (see"
  },
  {
    "id": 5989,
    "content": ", whether reads are cached in both the unified L1/texture cache and L2 or in L2 only) can be partially configured on a per-access basis using modifiers to the load instruction"
  },
  {
    "id": 5994,
    "content": "Data that is read-only for the entire lifetime of the kernel can also be cached in the unified L1/texture cache described in the previous section by reading it using the __ldg() function (see Read-Only Data Cache Load Function ) When the compiler detects that the read-only condition is satisfied for some data, it will use __ldg() to read it The compiler might not always be able to detect that the"
  },
  {
    "id": 5995,
    "content": "read-only condition is satisfied for some data Marking pointers used for loading such data with both the const and __restrict__ qualifiers increases the likelihood that the compiler will detect the read-only condition Data that is not read-only for the entire lifetime of the kernel cannot be cached in the unified L1/texture cache for devices of compute capability 5"
  },
  {
    "id": 5998,
    "content": "2, it is, by default, not cached in the unified L1/texture cache, but caching may be enabled using the following mechanisms: Perform the read using inline assembly with the appropriate modifier as described in the PTX reference manual; Compile with the -Xptxas -dlcm=ca compilation flag, in which case all reads are cached, except reads that are performed using inline assembly with a modifier that"
  },
  {
    "id": 5999,
    "content": "disables caching; Compile with the -Xptxas -fscm=ca compilation flag, in which case all reads are cached, including reads that are performed using inline assembly regardless of the modifier used When caching is enabled using one of the three mechanisms listed above, devices of compute capability 5 2 will cache global memory reads in the unified L1/texture cache for all kernel launches except for"
  },
  {
    "id": 6005,
    "content": "Shared Memory  Shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks A shared memory request for a warp does not generate a bank conflict between two threads that access any address within the same 32-bit word (even though the two addresses fall in the same bank) In that case, for read accesses, the word is broadcast to the requesting threads and"
  },
  {
    "id": 6006,
    "content": "for write accesses, each address is written by only one of the threads (which thread performs the write is undefined) Figure 23 shows some examples of memory read accesses that involve the broadcast mechanism Middle Conflict-free access since threads 3, 4, 6, 7, and 9 access the same word within bank 5 Right Conflict-free broadcast access (threads access the same word within a bank)"
  },
  {
    "id": 6023,
    "content": "An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified L1/texture cache for reads from global memory of size 24 KB (6"
  },
  {
    "id": 6032,
    "content": "Global Memory  Global memory behaves the same way as in devices of compute capability 5 x (See Global Memory )"
  },
  {
    "id": 6036,
    "content": "Shared Memory  Shared memory behaves the same way as in devices of compute capability 5 x (See Shared Memory )"
  },
  {
    "id": 6037,
    "content": "Architecture  An SM consists of: 64 FP32 cores for single-precision arithmetic operations, 32 FP64 cores for double-precision arithmetic operations, 34 64 INT32 cores for integer math, 8 mixed-precision Tensor Cores for deep learning matrix arithmetic 16 special function units for single-precision floating-point transcendental functions, 4 warp schedulers"
  },
  {
    "id": 6038,
    "content": "An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 128 KB ( Volta ) or 96 KB ( Turing ) Shared memory is partitioned out of unified data cache, and can be configured to various sizes (See Shared Memory ) The remaining data cache"
  },
  {
    "id": 6039,
    "content": "serves as an L1 cache and is also used by the texture unit that implements the various addressing and data filtering modes mentioned in Texture and Surface Memory"
  },
  {
    "id": 6043,
    "content": "Independent Thread Scheduling  The Volta architecture introduces Independent Thread Scheduling among threads in a warp, enabling intra-warp synchronization patterns previously unavailable and simplifying code changes when porting CPU code"
  },
  {
    "id": 6044,
    "content": "However, this can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures"
  },
  {
    "id": 6045,
    "content": "For applications using warp intrinsics ( __shfl* , __any , __all , __ballot ), it is necessary that developers port their code to the new, safe, synchronizing counterpart, with the *_sync suffix The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic Since the intrinsics are available with CUDA 9 0+, (if"
  },
  {
    "id": 6046,
    "content": "necessary) code can be executed conditionally with the following preprocessor macro: #if defined(CUDART_VERSION) && CUDART_VERSION >= 9000 *_sync intrinsic #endif These intrinsics are available on all architectures, not just Volta or Turing , and in most cases a single code-base will suffice for all architectures Note, however, that for Pascal and earlier architectures, all threads in mask must"
  },
  {
    "id": 6047,
    "content": "execute the same warp intrinsic instruction in convergence, and the union of all values in mask must be equal to the warp’s active mask The following code pattern is valid on Volta , but not on Pascal or earlier architectures"
  },
  {
    "id": 6048,
    "content": "if ( tid % warpSize threshold ); if ( warpLane == 0 ) { output [ i / 32 ] = bitPack ; } } This code is invalid because CUDA does not guarantee that the warp will diverge ONLY at the loop condition"
  },
  {
    "id": 6049,
    "content": "When divergence happens for other reasons, conflicting results will be computed for the same 32-bit output element by different subsets of threads in the warp"
  },
  {
    "id": 6050,
    "content": "A correct code might use a non-divergent loop condition together with __ballot_sync() to safely enumerate the set of threads in the warp participating in the threshold calculation as follows"
  },
  {
    "id": 6051,
    "content": "for ( int i = warpLane ; i - warpLane threshold ); if ( warpLane == 0 ) { output [ i / 32 ] = bitPack ; } } } Discovery Pattern demonstrates a valid use case for __activemask()"
  },
  {
    "id": 6052,
    "content": "If applications have warp-synchronous codes, they will need to insert the new __syncwarp() warp-wide barrier synchronization instruction between any steps where data is exchanged between threads via global or shared memory"
  },
  {
    "id": 6053,
    "content": "Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid"
  },
  {
    "id": 6054,
    "content": "__shared__ float s_buff [ BLOCK_SIZE ]; s_buff [ tid ] = val ; __syncthreads ();   Inter-warp reduction for ( int i = BLOCK_SIZE / 2 ; i >= 32 ; i /= 2 ) { if ( tid >> ("
  },
  {
    "id": 6055,
    "content": "); In addition to an integer percentage, several convenience enums are provided as listed in the code comments above Where a chosen integer percentage does not map exactly to a supported capacity (SM 7"
  },
  {
    "id": 6056,
    "content": "0 devices support shared capacities of 0, 8, 16, 32, 64, or 96 KB), the next larger capacity is used For instance, in the example above, 50% of the 96 KB maximum is 48 KB, which is not a supported shared memory capacity"
  },
  {
    "id": 6058,
    "content": "x devices allow a single thread block to address the full capacity of shared memory: 96 KB on Volta , 64 KB on Turing Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, as such they must use dynamic shared memory (rather than statically sized arrays) and require an explicit opt-in using cudaFuncSetAttribute() as follows } Host code int maxbytes = 98304 ;"
  },
  {
    "id": 6059,
    "content": "96 KB cudaFuncSetAttribute ( MyKernel , cudaFuncAttributeMaxDynamicSharedMemorySize , maxbytes ); MyKernel >> ( ); Otherwise, shared memory behaves the same way as for devices of compute capability 5 x (See Shared Memory )"
  },
  {
    "id": 6066,
    "content": "An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 192 KB for devices of compute capability 8"
  },
  {
    "id": 6072,
    "content": "Shared memory is partitioned out of the unified data cache, and can be configured to various sizes (see Shared Memory section)"
  },
  {
    "id": 6076,
    "content": "Global Memory  Global memory behaves the same way as for devices of compute capability 5 x (See Global Memory )"
  },
  {
    "id": 6080,
    "content": "Shared Memory  Similar to the Volta architecture , the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis For the NVIDIA Ampere GPU architecture , the unified data cache has a size of 192 KB for devices of compute capability 8"
  },
  {
    "id": 6083,
    "content": "The shared memory capacity can be set to 0, 8, 16, 32, 64, 100, 132 or 164 KB for devices of compute capability 8"
  },
  {
    "id": 6090,
    "content": ", the preferred shared memory capacity, with the cudaFuncSetAttribute() cudaFuncSetAttribute ( kernel_name , cudaFuncAttributePreferredSharedMemoryCarveout , carveout ); The API can specify the carveout either as an integer percentage of the maximum supported shared memory capacity of 164 KB for devices of compute capability 8"
  },
  {
    "id": 6092,
    "content": "9 respectively, or as one of the following values: {cudaSharedmemCarveoutDefault , cudaSharedmemCarveoutMaxL1 , or cudaSharedmemCarveoutMaxShared"
  },
  {
    "id": 6096,
    "content": "Setting the cudaFuncAttributePreferredSharedMemoryCarveout is considered a hint by the driver; the driver may choose a different configuration, if needed"
  },
  {
    "id": 6099,
    "content": "7 allow a single thread block to address up to 163 KB of shared memory, while devices of compute capabilities 8"
  },
  {
    "id": 6101,
    "content": "9 allow up to 99 KB of shared memory Kernels relying on shared memory allocations over 48 KB per block are architecture-specific, and must use dynamic shared memory rather than statically sized shared memory arrays These kernels require an explicit opt-in by using cudaFuncSetAttribute() to set the cudaFuncAttributeMaxDynamicSharedMemorySize ; see Shared Memory for the Volta architecture Note that"
  },
  {
    "id": 6102,
    "content": "the maximum amount of shared memory per thread block is smaller than the maximum shared memory partition available per SM The 1 KB of shared memory not made available to a thread block is reserved for system use"
  },
  {
    "id": 6109,
    "content": "Architecture  A Streaming Multiprocessor (SM) consists of: 128 FP32 cores for single-precision arithmetic operations, 64 FP64 cores for double-precision arithmetic operations, 64 INT32 cores for integer math, 4 mixed-precision fourth-generation Tensor Cores supporting the new FP8 input type in either E4M3 or E5M2 for exponent (E) and mantissa (M), half-precision (fp16), __nv_bfloat16 , tf32 ,"
  },
  {
    "id": 6110,
    "content": "INT8 and double precision (fp64) matrix arithmetic (see Warp Matrix Functions for details) with sparsity support, 16 special function units for single-precision floating-point transcendental functions, 4 warp schedulers"
  },
  {
    "id": 6111,
    "content": "An SM has: a read-only constant cache that is shared by all functional units and speeds up reads from the constant memory space, which resides in device memory, a unified data cache and shared memory with a total size of 256 KB for devices of compute capability 9"
  },
  {
    "id": 6120,
    "content": "Shared Memory  Similar to the NVIDIA Ampere GPU architecture , the amount of the unified data cache reserved for shared memory is configurable on a per kernel basis For the NVIDIA H100 Tensor Core GPU architecture , the unified data cache has a size of 256 KB for devices of compute capability 9"
  },
  {
    "id": 6122,
    "content": "As with the NVIDIA Ampere GPU architecture , an application can configure its preferred shared memory capacity, i"
  },
  {
    "id": 6126,
    "content": "0 allow a single thread block to address up to 227 KB of shared memory 32 above 48 KB requires dynamic shared memory 33 2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7 5 34 2 FP64 cores for double-precision arithmetic operations for devices of compute capabilities 7"
  },
  {
    "id": 6130,
    "content": "Features Accelerating Specialized Computations  The NVIDIA Hopper GPU architecture includes features to accelerate matrix multiply-accumulate (MMA) computations with: asynchronous execution of MMA instructions MMA instructions acting on large matrices spanning a warp-group dynamic reassignment of register capacity among warp-groups to support even larger matrices, and operand matrices accessed"
  },
  {
    "id": 6131,
    "content": "directly from shared memory This feature set is only available within the CUDA compilation toolchain through inline PTX"
  },
  {
    "id": 6132,
    "content": "It is strongly recommended that applications utilize this complex feature set through CUDA-X libraries such as cuBLAS, cuDNN, or cuFFT It is strongly recommended that device kernels utilize this complex feature set through CUTLASS , a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) and related computations at all levels and scales within"
  },
  {
    "id": 6135,
    "content": "Driver API  This section assumes knowledge of the concepts described in CUDA Runtime The driver API is implemented in the cuda dynamic library ( cuda dll or cuda so ) which is copied on the system during the installation of the device driver"
  },
  {
    "id": 6136,
    "content": "It is a handle-based, imperative API: Most objects are referenced by opaque handles that may be specified to functions to manipulate the objects"
  },
  {
    "id": 6137,
    "content": "Table 22 Objects Available in the CUDA Driver API  Object Handle Description Device CUdevice CUDA-enabled device Context CUcontext Roughly equivalent to a CPU process Module CUmodule Roughly equivalent to a dynamic library Function CUfunction Kernel Heap memory CUdeviceptr Pointer to device memory CUDA array CUarray Opaque container for one-dimensional or two-dimensional data on the device,"
  },
  {
    "id": 6138,
    "content": "readable via texture or surface references Texture object CUtexref Object that describes how to interpret texture memory data Surface reference CUsurfref Object that describes how to read or write CUDA arrays Stream CUstream Object that describes a CUDA stream Event CUevent Object that describes a CUDA event The driver API must be initialized with cuInit() before any function from the driver API"
  },
  {
    "id": 6139,
    "content": "is called A CUDA context must then be created that is attached to a specific device and made current to the calling host thread as detailed in Context Within a CUDA context, kernels are explicitly loaded as PTX or binary objects by the host code as described in Module Any application that wants to run on future device architectures must load PTX , not binary code This is because binary code is"
  },
  {
    "id": 6140,
    "content": "architecture-specific and therefore incompatible with future architectures, whereas PTX code is compiled to binary code at load time by the device driver Here is the host code of the sample from Kernels written using the driver API: int main () { int N ="
  },
  {
    "id": 6141,
    "content": "; size_t size = N * sizeof ( float );   Allocate input vectors h_A and h_B in host memory float * h_A = ( float * ) malloc ( size ); float * h_B = ( float * ) malloc ( size );   Initialize input vectors"
  },
  {
    "id": 6142,
    "content": "Initialize cuInit ( 0 );   Get number of devices supporting CUDA int deviceCount = 0 ; cuDeviceGetCount ( & deviceCount ); if ( deviceCount == 0 ) { printf ( \"There is no device supporting CUDA"
  },
  {
    "id": 6146,
    "content": "Context  A CUDA context is analogous to a CPU process All resources and actions performed within the driver API are encapsulated inside a CUDA context, and the system automatically cleans up these resources when the context is destroyed"
  },
  {
    "id": 6147,
    "content": "Besides objects such as modules and texture or surface references, each context has its own distinct address space"
  },
  {
    "id": 6149,
    "content": "When a context is created with cuCtxCreate( ), it is made current to the calling host thread CUDA functions that operate in a context (most functions that do not involve device enumeration or context management) will return CUDA_ERROR_INVALID_CONTEXT if a valid context is not current to the thread The context is then “floating” and may be pushed as the current context for any host thread A"
  },
  {
    "id": 6150,
    "content": "context is destroyed when the usage count goes to 0 when calling cuCtxDetach() or cuCtxDestroy() The driver API is interoperable with the runtime and it is possible to access the primary context (see Initialization ) managed by the runtime from the driver API via cuDevicePrimaryCtxRetain() Usage count facilitates interoperability between third party authored code operating in the same context For"
  },
  {
    "id": 6151,
    "content": "example, if three libraries are loaded to use the same context, each library would call cuCtxAttach() to increment the usage count and cuCtxDetach() to decrement the usage count when the library is done using the context For most libraries, it is expected that the application will have created a context before loading or initializing the library; that way, the application can create the context"
  },
  {
    "id": 6152,
    "content": "using its own heuristics, and the library simply operates on the context handed to it Libraries that wish to create their own contexts - unbeknownst to their API clients who may or may not have created contexts of their own - would use cuCtxPushCurrent() and cuCtxPopCurrent() as illustrated in the following figure"
  },
  {
    "id": 6153,
    "content": "Module  Modules are dynamically loadable packages of device code and data, akin to DLLs in Windows, that are output by nvcc (see Compilation with NVCC ) The names for all symbols, including functions, global variables, and texture or surface references, are maintained at module scope so that modules written by independent third parties may interoperate in the same CUDA context"
  },
  {
    "id": 6154,
    "content": "Linker Output:   %s   \" , walltime , info_log ); cuModuleLoadData ( cuModule , cubin ); cuLinkDestroy ( linkState ); Full code can be found in the ptxjit CUDA sample"
  },
  {
    "id": 6157,
    "content": "Kernel Execution  cuLaunchKernel() launches a kernel with a given execution configuration Parameters are passed either as an array of pointers (next to last parameter of cuLaunchKernel() ) where the nth pointer corresponds to the nth parameter and points to a region of memory from which the parameter is copied, or as one of the extra options (last parameter of cuLaunchKernel() ) When parameters"
  },
  {
    "id": 6158,
    "content": "are passed as an extra option (the CU_LAUNCH_PARAM_BUFFER_POINTER option), they are passed as a pointer to a single buffer where parameters are assumed to be properly offset with respect to each other by matching the alignment requirement for each parameter type in device code Alignment requirements in device code for the built-in vector types are listed in Table 5 For all other basic types, the"
  },
  {
    "id": 6159,
    "content": "alignment requirement in device code matches the alignment requirement in host code and can therefore be obtained using __alignof()"
  },
  {
    "id": 6160,
    "content": "The only exception is when the host compiler aligns double and long long (and long on a 64-bit system) on a one-word boundary instead of a two-word boundary (for example, using gcc ’s compilation flag -mno-align-double ) since in device code these types are always aligned on a two-word boundary"
  },
  {
    "id": 6161,
    "content": "CUdeviceptr is an integer, but represents a pointer, so its alignment requirement is __alignof(void*)"
  },
  {
    "id": 6162,
    "content": "The following code sample uses a macro ( ALIGN_UP() ) to adjust the offset of each parameter to meet its alignment requirement and another macro ( ADD_TO_PARAM_BUFFER() ) to add each parameter to the parameter buffer passed to the CU_LAUNCH_PARAM_BUFFER_POINTER option"
  },
  {
    "id": 6163,
    "content": "The alignment requirement of a structure that contains built-in vector types, CUdeviceptr , or non-aligned double and long long , might therefore differ between device code and host code The following structure, for example, is not padded at all in host code, but it is padded in device code with 12 bytes after field f since the alignment requirement for field f4 is 16 Interoperability between"
  },
  {
    "id": 6164,
    "content": "Runtime and Driver APIs  An application can mix runtime API code with driver API code If a context is created and made current via the driver API, subsequent runtime calls will pick up this context instead of creating a new one If the runtime is initialized (implicitly as mentioned in CUDA Runtime ), cuCtxGetCurrent() can be used to retrieve the context created during initialization The"
  },
  {
    "id": 6165,
    "content": "implicitly created context from the runtime is called the primary context (see Initialization ) CUdeviceptr can be cast to regular pointers and vice-versa: CUdeviceptr devPtr ; float * d_data ; Allocation using driver API cuMemAlloc ( & devPtr , size ); d_data = ( float * ) devPtr ; Allocation using runtime API cudaMalloc ( & d_data , size ); devPtr = ( CUdeviceptr ) d_data ; In particular, this"
  },
  {
    "id": 6166,
    "content": "means that applications written using the driver API can invoke libraries written using the runtime API (such as cuFFT, cuBLAS, …)"
  },
  {
    "id": 6167,
    "content": "All functions from the device and version management sections of the reference manual can be used interchangeably"
  },
  {
    "id": 6173,
    "content": "Introduction  The Driver Entry Point Access APIs provide a way to retrieve the address of a CUDA driver function"
  },
  {
    "id": 6175,
    "content": "3, users can call into available CUDA driver APIs using function pointers obtained from these APIs These APIs provide functionality similar to their counterparts, dlsym on POSIX platforms and GetProcAddress on Windows The provided APIs will let users: Retrieve the address of a driver function using the CUDA Driver API For more details, see Retrieve per-thread default stream versions Access new"
  },
  {
    "id": 6180,
    "content": "Driver Function Typedefs  To help retrieve the CUDA Driver API entry points, the CUDA Toolkit provides access to headers containing the function pointer definitions for all CUDA driver APIs These headers are installed with the CUDA Toolkit and are made available in the toolkit’s include/ directory The table below summarizes the header files containing the typedefs for each CUDA API header file"
  },
  {
    "id": 6197,
    "content": "h The above headers do not define actual function pointers themselves; they define the typedefs for function pointers"
  },
  {
    "id": 6199,
    "content": "h has the below typedefs for the driver API cuMemAlloc : typedef CUresult ( CUDAAPI * PFN_cuMemAlloc_v3020 )( CUdeviceptr_v2 * dptr , size_t bytesize ); typedef CUresult ( CUDAAPI * PFN_cuMemAlloc_v2000 )( CUdeviceptr_v1 * dptr , unsigned int bytesize ); CUDA driver symbols have a version based naming scheme with a _v* extension in its name except for the first version When the signature or the"
  },
  {
    "id": 6200,
    "content": "semantics of a specific CUDA driver API changes, we increment the version number of the corresponding driver symbol In the case of the cuMemAlloc driver API, the first driver symbol name is cuMemAlloc and the next symbol name is cuMemAlloc_v2 The typedef for the first version which was introduced in CUDA 2"
  },
  {
    "id": 6203,
    "content": "2 (3020) is PFN_cuMemAlloc_v3020 The typedefs can be used to more easily define a function pointer of the appropriate type in code: PFN_cuMemAlloc_v3020 pfn_cuMemAlloc_v2 ; PFN_cuMemAlloc_v2000 pfn_cuMemAlloc_v1 ; The above method is preferable if users are interested in a specific version of the API"
  },
  {
    "id": 6204,
    "content": "Additionally, the headers have predefined macros for the latest version of all driver symbols that were available when the installed CUDA toolkit was released; these typedefs do not have a _v* suffix"
  },
  {
    "id": 6206,
    "content": "3 toolkit, cuMemAlloc_v2 was the latest version and so we can also define its function pointer as below: PFN_cuMemAlloc pfn_cuMemAlloc ; 17"
  },
  {
    "id": 6209,
    "content": "Driver Function Retrieval  Using the Driver Entry Point Access APIs and the appropriate typedef, we can get the function pointer to any CUDA driver API"
  },
  {
    "id": 6214,
    "content": "Using the driver API  The driver API requires CUDA version as an argument to get the ABI compatible version for the requested driver symbol"
  },
  {
    "id": 6215,
    "content": "For example, consider the versions of cuStreamBeginCapture and their corresponding typedefs from cudaTypedefs"
  },
  {
    "id": 6217,
    "content": "h CUresult CUDAAPI cuStreamBeginCapture ( CUstream hStream ); CUresult CUDAAPI cuStreamBeginCapture_v2 ( CUstream hStream , CUstreamCaptureMode mode ); cudaTypedefs h typedef CUresult ( CUDAAPI * PFN_cuStreamBeginCapture_v10000 )( CUstream hStream ); typedef CUresult ( CUDAAPI * PFN_cuStreamBeginCapture_v10010 )( CUstream hStream , CUstreamCaptureMode mode ); From the above typedefs in the code"
  },
  {
    "id": 6218,
    "content": "snippet, version suffixes _v10000 and _v10010 indicate that the above APIs were introduced in CUDA 10"
  },
  {
    "id": 6221,
    "content": "#include Declare the entry points for cuStreamBeginCapture PFN_cuStreamBeginCapture_v10000 pfn_cuStreamBeginCapture_v1 ; PFN_cuStreamBeginCapture_v10010 pfn_cuStreamBeginCapture_v2 ; Get the function pointer to the cuStreamBeginCapture driver symbol cuGetProcAddress ( \"cuStreamBeginCapture\" , & pfn_cuStreamBeginCapture_v1 , 10000 , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Get the function"
  },
  {
    "id": 6222,
    "content": "pointer to the cuStreamBeginCapture_v2 driver symbol cuGetProcAddress ( \"cuStreamBeginCapture\" , & pfn_cuStreamBeginCapture_v2 , 10010 , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Referring to the code snippet above, to retrieve the address to the _v1 version of the driver API cuStreamBeginCapture , the CUDA version argument should be exactly 10"
  },
  {
    "id": 6226,
    "content": "Specifying a higher CUDA version for retrieving a specific version of a driver API might not always be portable"
  },
  {
    "id": 6227,
    "content": "For example, using 11030 here would still return the _v2 symbol, but if a hypothetical _v3 version is released in CUDA 11 3, the cuGetProcAddress API would start returning the newer _v3 symbol instead when paired with a CUDA 11"
  },
  {
    "id": 6229,
    "content": "Since the ABI and function signatures of the _v2 and _v3 symbols might differ, calling the _v3 function using the _v10010 typedef intended for the _v2 symbol would exhibit undefined behavior"
  },
  {
    "id": 6230,
    "content": "To retrieve the latest version of a driver API for a given CUDA Toolkit, we can also specify CUDA_VERSION as the version argument and use the unversioned typedef to define the function pointer Since _v2 is the latest version of the driver API cuStreamBeginCapture in CUDA 11 3, the below code snippet shows a different method to retrieve it"
  },
  {
    "id": 6232,
    "content": "3 Toolkit #include   Declare the entry point PFN_cuStreamBeginCapture pfn_cuStreamBeginCapture_latest ;   Intialize the entry point"
  },
  {
    "id": 6233,
    "content": "Specifying CUDA_VERSION will give the function pointer to the   cuStreamBeginCapture_v2 symbol since it is latest version on CUDA 11"
  },
  {
    "id": 6235,
    "content": "cuGetProcAddress ( \"cuStreamBeginCapture\" , & pfn_cuStreamBeginCapture_latest , CUDA_VERSION , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Note that requesting a driver API with an invalid CUDA version will return an error CUDA_ERROR_NOT_FOUND"
  },
  {
    "id": 6242,
    "content": "Using the runtime API  The runtime API cudaGetDriverEntryPoint uses the CUDA runtime version to get the ABI compatible version for the requested driver symbol In the below code snippet, the minimum CUDA runtime version required would be CUDA 11"
  },
  {
    "id": 6244,
    "content": "#include   Declare the entry point PFN_cuMemAllocAsync pfn_cuMemAllocAsync ;   Intialize the entry point"
  },
  {
    "id": 6246,
    "content": "2 cudaGetDriverEntryPoint ( \"cuMemAllocAsync\" , & pfn_cuMemAllocAsync , cudaEnableDefault , & driverStatus );   Call the entry point if ( driverStatus == cudaDriverEntryPointSuccess && pfn_cuMemAllocAsync ) { pfn_cuMemAllocAsync ("
  },
  {
    "id": 6247,
    "content": "); } The runtime API cudaGetDriverEntryPointByVersion uses the user provided CUDA version to get the ABI compatible version for the requested driver symbol This allows more specific control over the requested ABI version"
  },
  {
    "id": 6252,
    "content": "Retrieve per-thread default stream versions  Some CUDA driver APIs can be configured to have default stream or per-thread default stream semantics Driver APIs having per-thread default stream semantics are suffixed with _ptsz or _ptds in their name For example, cuLaunchKernel has a per-thread default stream variant named cuLaunchKernel_ptsz With the Driver Entry Point Access APIs, users can"
  },
  {
    "id": 6253,
    "content": "request for the per-thread default stream version of the driver API cuLaunchKernel instead of the default stream version Configuring the CUDA driver APIs for default stream or per-thread default stream semantics affects the synchronization behavior The default stream or per-thread default stream versions of a driver API can be obtained by one of the following ways: Use the compilation flag"
  },
  {
    "id": 6254,
    "content": "--default-stream per-thread or define the macro CUDA_API_PER_THREAD_DEFAULT_STREAM to get per-thread default stream behavior Force default stream or per-thread default stream behavior using the flags CU_GET_PROC_ADDRESS_LEGACY_STREAM/cudaEnableLegacyStream or CU_GET_PROC_ADDRESS_PER_THREAD_DEFAULT_STREAM/cudaEnablePerThreadDefaultStream respectively"
  },
  {
    "id": 6259,
    "content": "Access new CUDA features  It is always recommended to install the latest CUDA toolkit to access new CUDA driver features, but if for some reason, a user does not want to update or does not have access to the latest toolkit, the API can be used to access new CUDA features with only an updated CUDA driver"
  },
  {
    "id": 6261,
    "content": "3 and wants to use a new driver API cuFoo available in the CUDA 12 0 driver The below code snippet illustrates this use-case: int main () {   Assuming we have CUDA 12 0 driver installed"
  },
  {
    "id": 6264,
    "content": "3 does not have the cuFoo typedef typedef CUresult ( CUDAAPI * PFN_cuFoo )( ); PFN_cuFoo pfn_cuFoo = NULL ; CUdriverProcAddressQueryResult driverStatus ; Get the address for cuFoo API using cuGetProcAddress Specify CUDA version as 12000 since cuFoo was introduced then or get the driver version dynamically using cuDriverGetVersion int driverVersion ; cuDriverGetVersion ( & driverVersion );"
  },
  {
    "id": 6265,
    "content": "CUresult status = cuGetProcAddress ( \"cuFoo\" , & pfn_cuFoo , driverVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( status == CUDA_SUCCESS && pfn_cuFoo ) { pfn_cuFoo ( ); } else { printf ( \"Cannot retrieve the address to cuFoo - driverStatus = %d"
  },
  {
    "id": 6266,
    "content": "Potential Implications with cuGetProcAddress  Below is a set of concrete and theoretical examples of potential issues with cuGetProcAddress and cudaGetDriverEntryPoint"
  },
  {
    "id": 6273,
    "content": "To preserve minor version compatibility, cuDeviceGetUuid will not be version bumped to cuDeviceGetUuid_v2 in cuda"
  },
  {
    "id": 6276,
    "content": "This means that calling it by obtaining a function pointer to it via cuGetProcAddress might have different behavior"
  },
  {
    "id": 6277,
    "content": "Example using the API directly: #include CUuuid uuid ; CUdevice dev ; CUresult status ; status = cuDeviceGet ( & dev , 0 );   Get device 0   handle status status = cuDeviceGetUuid ( & uuid , dev )   Get uuid of device 0 In this example, assume the user is compiling with CUDA 11"
  },
  {
    "id": 6279,
    "content": "Now an example of using cuGetProcAddress : #include CUuuid uuid ; CUdevice dev ; CUresult status ; CUdriverProcAddressQueryResult driverStatus ; status = cuDeviceGet ( & dev , 0 ); Get device 0 handle status PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid ; status = cuGetProcAddress ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuid , CUDA_VERSION , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ("
  },
  {
    "id": 6283,
    "content": "Calling the function pointer will then invoke the new _v2 function, not the same cuDeviceGetUuid as shown in the previous example"
  },
  {
    "id": 6288,
    "content": "Compile Time vs Runtime Version Usage in cuGetProcAddress  Let’s take the same issue and make one small tweak"
  },
  {
    "id": 6289,
    "content": "The last example used the compile time constant of CUDA_VERSION to determine which function pointer to obtain"
  },
  {
    "id": 6290,
    "content": "More complications arise if the user queries the driver version dynamically using cuDriverGetVersion or cudaDriverGetVersion to pass to cuGetProcAddress"
  },
  {
    "id": 6291,
    "content": "Example: #include CUuuid uuid ; CUdevice dev ; CUresult status ; int cudaVersion ; CUdriverProcAddressQueryResult driverStatus ; status = cuDeviceGet ( & dev , 0 ); Get device 0 handle status status = cuDriverGetVersion ( & cudaVersion ); handle status PFN_cuDeviceGetUuid pfn_cuDeviceGetUuid ; status = cuGetProcAddress ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuid , cudaVersion ,"
  },
  {
    "id": 6292,
    "content": "CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && pfn_cuDeviceGetUuid ) { pfn_cuDeviceGetUuid points to"
  },
  {
    "id": 6295,
    "content": "The user would debug, test, and deploy this application with the known behavior of getting cuDeviceGetUuid (not the _v2 version)"
  },
  {
    "id": 6296,
    "content": "Since CUDA has guaranteed ABI compatibility between minor versions, this same application is expected to run after the driver is upgraded to CUDA 11"
  },
  {
    "id": 6298,
    "content": "This will have undefined behavior though, because now the typedef for PFN_cuDeviceGetUuid will still be of the signature for the original version, but since cudaVersion would now be 11040 (CUDA 11 4), cuGetProcAddress would return the function pointer to the _v2 version, meaning calling it might have undefined behavior Note in this case the original (not the _v2 version) typedef looks like:"
  },
  {
    "id": 6299,
    "content": "typedef CUresult ( CUDAAPI * PFN_cuDeviceGetUuid_v9020 )( CUuuid * uuid , CUdevice_v1 dev ); But the _v2 version typedef looks like: typedef CUresult ( CUDAAPI * PFN_cuDeviceGetUuid_v11040 )( CUuuid * uuid , CUdevice_v1 dev ); So in this case, the API/ABI is going to be the same and the runtime API call will likely not cause issues–only the potential for unknown uuid return"
  },
  {
    "id": 6306,
    "content": "Now for instance let’s use a theoretical example that still has issues with compatibility across driver versions"
  },
  {
    "id": 6307,
    "content": "Example: CUresult cuFoo ( int bar ); Introduced in CUDA 11 4 CUresult cuFoo_v2 ( int bar ); Introduced in CUDA 11 5 CUresult cuFoo_v3 ( int bar , void * jazz ); Introduced in CUDA 11 6 typedef CUresult ( CUDAAPI * PFN_cuFoo_v11040 )( int bar ); typedef CUresult ( CUDAAPI * PFN_cuFoo_v11050 )( int bar ); typedef CUresult ( CUDAAPI * PFN_cuFoo_v11060 )( int bar , void * jazz ); Notice that the API"
  },
  {
    "id": 6311,
    "content": "5 is: #include #include CUresult status ; int cudaVersion ; CUdriverProcAddressQueryResult driverStatus ; status = cuDriverGetVersion ( & cudaVersion ); handle status PFN_cuFoo_v11040 pfn_cuFoo_v11040 ; PFN_cuFoo_v11050 pfn_cuFoo_v11050 ; if ( cudaVersion = CUDA 11 5 version we can use the second version status = cuGetProcAddress ( \"cuFoo\" , & pfn_cuFoo_v11050 , cudaVersion ,"
  },
  {
    "id": 6312,
    "content": "CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); Handle status and validating pfn_cuFoo_v11050 } In this example, without updates for the new typedef in CUDA 11"
  },
  {
    "id": 6313,
    "content": "6 and recompiling the application with those new typedefs and case handling, the application will get the cuFoo_v3 function pointer returned and any usage of that function would then cause undefined behavior"
  },
  {
    "id": 6314,
    "content": "The point of this example was to illustrate that even explicit version checks for cuGetProcAddress may not safely cover the minor version bumps within a CUDA major release"
  },
  {
    "id": 6319,
    "content": "Issues with Runtime API Usage  The above examples were focused on the issues with the Driver API usage for obtaining the function pointers to driver APIs Now we will discuss the potential issues with the Runtime API usage for cudaApiGetDriverEntryPoint"
  },
  {
    "id": 6320,
    "content": "#include #include #include CUresult status ; cudaError_t error ; int driverVersion , runtimeVersion ; CUdriverProcAddressQueryResult driverStatus ; Ask the runtime for the function PFN_cuDeviceGetUuid pfn_cuDeviceGetUuidRuntime ; error = cudaGetDriverEntryPoint ( \"cuDeviceGetUuid\" , & pfn_cuDeviceGetUuidRuntime , cudaEnableDefault , & driverStatus ); if ( cudaSuccess == error &&"
  },
  {
    "id": 6322,
    "content": "The function pointer in this example is even more complicated than the driver only examples above because there is no control over which version of the function to obtain; it will always get the API for the current CUDA Runtime version See the following table for more information: Static Runtime Version Linkage Driver Version Installed V11 3 V11 4 V11 3 v1 v1x V11 4 v1 v2 V11"
  },
  {
    "id": 6330,
    "content": "h) v1 => cuDeviceGetUuid v2 => cuDeviceGetUuid_v2 x => Implies the typedef function pointer won't match the returned function pointer"
  },
  {
    "id": 6332,
    "content": "4 runtime, would match the _v2 version, but the returned function pointer would be the original (non _v2) function"
  },
  {
    "id": 6333,
    "content": "The problem in the table comes in with a newer CUDA 11 4 Runtime and Toolkit and older driver (CUDA 11"
  },
  {
    "id": 6335,
    "content": "This combination would have the driver returning the pointer to the older function (non _v2), but the typedef used in the application would be for the new function pointer"
  },
  {
    "id": 6340,
    "content": "Issues with Runtime API and Dynamic Versioning  More complications arise when we consider different combinations of the CUDA version with which an application is compiled, CUDA runtime version, and CUDA driver version that an application dynamically links against"
  },
  {
    "id": 6341,
    "content": "Because of that, notice the number of cases where the typedef does not match the actual version returned/used"
  },
  {
    "id": 6346,
    "content": "Issues with Runtime API allowing CUDA Version  Unless specified otherwise, the CUDA runtime API cudaGetDriverEntryPointByVersion will have similar implications as the driver entry point cuGetProcAddress since it allows for the user to request a specific CUDA driver version"
  },
  {
    "id": 6351,
    "content": "Implications to API/ABI  In the above examples using cuDeviceGetUuid , the implications of the mismatched API are minimal, and may not be entirely noticeable to many users as the _v2 was added to support Multi-Instance GPU (MIG) mode"
  },
  {
    "id": 6353,
    "content": "More problematic is an API which changes its application signature (and hence ABI) such as cuCtxCreate"
  },
  {
    "id": 6358,
    "content": "So, in some of the cases above, where the typedef to the function pointer doesn’t match the returned function pointer, there is a chance for non-obvious ABI incompatibility which would lead to undefined behavior"
  },
  {
    "id": 6360,
    "content": "4 driver installed: PFN_cuCtxCreate cuUnknown ; CUdriverProcAddressQueryResult driverStatus ; status = cuGetProcAddress ( \"cuCtxCreate\" , ( void ** ) & cuUnknown , cudaVersion , CU_GET_PROC_ADDRESS_DEFAULT , & driverStatus ); if ( CUDA_SUCCESS == status && cuUnknown ) { status = cuUnknown ( & ctx , 0 , dev ); } Running this code where cudaVersion is set to anything >=11040 (indicating CUDA 11"
  },
  {
    "id": 6361,
    "content": "4) could have undefined behavior due to not having adequately supplied all the parameters required for the _v3 version of the cuCtxCreate_v3 API"
  },
  {
    "id": 6366,
    "content": "The second error type encodes in the CUdriverProcAddressQueryResult *symbolStatus and can be used to help distinguish potential issues with the driver not being able to find the symbol requested"
  },
  {
    "id": 6367,
    "content": "In the example, specifying cudaVersion as anything 11030 or less and when running against a CUDA driver >= CUDA 11"
  },
  {
    "id": 6369,
    "content": "The second case with the return code CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND indicates that the symbol was not found when searching in the CUDA driver"
  },
  {
    "id": 6370,
    "content": "This can be due to a few reasons such as unsupported CUDA function due to older driver as well as just having a typo"
  },
  {
    "id": 6371,
    "content": "In the latter, similar to the last example if the user had put symbol as CUDeviceGetExecAffinitySupport - notice the capital CU to start the string - cuGetProcAddress would not be able to find the API because the string doesn’t match"
  },
  {
    "id": 6372,
    "content": "In the former case an example might be the user developing an application against a CUDA driver supporting the new API, and deploying the application against an older CUDA driver Using the last example, if the developer developed against CUDA 11 4 or later but was deployed against a CUDA 11 3 driver, during their development they may have had a succesful cuGetProcAddress , but when deploying an"
  },
  {
    "id": 6374,
    "content": "3 driver the call would no longer work with the CU_GET_PROC_ADDRESS_SYMBOL_NOT_FOUND returned in driverStatus"
  },
  {
    "id": 6376,
    "content": "CUDA Environment Variables  The following table lists the CUDA environment variables Environment variables related to the Multi-Process Service are documented in the Multi-Process Service section of the GPU Deployment and Management guide Table 24 CUDA Environment Variables  Variable Values Description Device Enumeration and Properties CUDA_VISIBLE_DEVICES A comma-separated sequence of GPU"
  },
  {
    "id": 6377,
    "content": "identifiers MIG support: MIG- GPU identifiers are given as integer indices or as UUID strings GPU UUID strings should follow the same format as given by nvidia-smi , such as GPU-8932f937-d72c-4106-c12f-20bd9faed9f6"
  },
  {
    "id": 6378,
    "content": "However, for convenience, abbreviated forms are allowed; simply specify enough digits from the beginning of the GPU UUID to uniquely identify that GPU in the target system For example, CUDA_VISIBLE_DEVICES=GPU-8932f937 may be a valid way to refer to the above GPU UUID, assuming no other GPU in the system shares this prefix"
  },
  {
    "id": 6379,
    "content": "Only the devices whose index is present in the sequence are visible to CUDA applications and they are enumerated in the order of the sequence If one of the indices is invalid, only the devices whose index precedes the invalid index are visible to CUDA applications"
  },
  {
    "id": 6380,
    "content": "For example, setting CUDA_VISIBLE_DEVICES to 2,1 causes device 0 to be invisible and device 2 to be enumerated before device 1 Setting CUDA_VISIBLE_DEVICES to 0,2,-1,1 causes devices 0 and 2 to be visible and device 1 to be invisible"
  },
  {
    "id": 6381,
    "content": "MIG format starts with MIG keyword and GPU UUID should follow the same format as given by nvidia-smi"
  },
  {
    "id": 6382,
    "content": "CUDA_MANAGED_FORCE_DEVICE_ALLOC 0 or 1 (default is 0) Forces the driver to place all managed allocations in device memory"
  },
  {
    "id": 6383,
    "content": "CUDA_DEVICE_ORDER FASTEST_FIRST, PCI_BUS_ID, (default is FASTEST_FIRST) FASTEST_FIRST causes CUDA to enumerate the available devices in fastest to slowest order using a simple heuristic"
  },
  {
    "id": 6384,
    "content": "Compilation CUDA_CACHE_DISABLE 0 or 1 (default is 0) Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation"
  },
  {
    "id": 6385,
    "content": "CUDA_CACHE_PATH filepath Specifies the folder where the just-in-time compiler caches binary codes; the default values are: on Windows, %APPDATA%\\NVIDIA\\ComputeCache on Linux, ~/ nv/ComputeCache CUDA_CACHE_MAXSIZE integer (default is 1073741824 (1 GiB) for desktop/server platforms and 268435456 (256 MiB) for embedded platforms and the maximum is 4294967296 (4 GiB)) Specifies the size in bytes of"
  },
  {
    "id": 6386,
    "content": "the cache used by the just-in-time compiler Older binary codes are evicted from the cache to make room for newer binary codes if needed CUDA_FORCE_PTX_JIT 0 or 1 (default is 0) When set to 1, forces the device driver to ignore any binary code embedded in an application (see Application Compatibility ) and to just-in-time compile embedded PTX code instead This environment variable can be used to"
  },
  {
    "id": 6387,
    "content": "validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application forward compatibility with future architectures (see Just-in-Time Compilation ) CUDA_DISABLE_PTX_JIT 0 or 1 (default is 0) When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see"
  },
  {
    "id": 6388,
    "content": "Application Compatibility ) If a kernel does not have embedded binary code or the embedded binary was compiled for an incompatible architecture, then it will fail to load This environment variable can be used to validate that an application has the compatible SASS code generated for each kernel CUDA_FORCE_JIT 0 or 1 (default is 0) When set to 1, forces the device driver to ignore any binary code"
  },
  {
    "id": 6389,
    "content": "embedded in an application (see Application Compatibility ) and to just-in-time compile embedded PTX code instead CUDA_DISABLE_JIT 0 or 1 (default is 0) When set to 1, disables the just-in-time compilation of embedded PTX code and use the compatible binary code embedded in an application (see Application Compatibility ) Execution CUDA_LAUNCH_BLOCKING 0 or 1 (default is 0) Disables (when set to 1)"
  },
  {
    "id": 6391,
    "content": "CUDA_DEVICE_MAX_CONNECTIONS 1 to 32 (default is 8) Sets the number of compute and copy engine concurrent connections (work queues) from the host to each device of compute capability 3"
  },
  {
    "id": 6393,
    "content": "CUDA_AUTO_BOOST 0 or 1 Overrides the autoboost behavior set by the –auto-boost-default option of nvidia-smi If an application requests via this environment variable a behavior that is different from nvidia-smi’s, its request is honored if there is no other application currently running on the same GPU that successfully requested a different behavior, otherwise it is ignored"
  },
  {
    "id": 6397,
    "content": "cuda-gdb (on Linux platform) CUDA_DEVICE_WAITS_ON_EXCEPTION 0 or 1 (default is 0) When set to 1, a CUDA application will halt when a device exception occurs, allowing a debugger to be attached for further debugging MPS service (on Linux platform) CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT Percentage value (between 0 - 100, default is 0) Devices of compute capability 8"
  },
  {
    "id": 6399,
    "content": "When using CUDA MPS service, the set-aside size can only be controlled using this environment variable, before starting CUDA MPS control daemon"
  },
  {
    "id": 6403,
    "content": "Module loading CUDA_MODULE_LOADING DEFAULT, LAZY, EAGER (default is LAZY) Specifies the module loading mode for the application"
  },
  {
    "id": 6404,
    "content": "When set to EAGER, all kernels and data from a cubin, fatbin or a PTX file are fully loaded upon corresponding cuModuleLoad* and cuLibraryLoad* API call When set to LAZY, loading of specific kernels is delayed to the point a CUfunc handle is extracted with cuModuleGetFunction or cuKernelGetFunction API calls and data from the cubin is loaded at load of first kernel in the cubin or at first access"
  },
  {
    "id": 6405,
    "content": "of variables in the cubin CUDA_MODULE_DATA_LOADING DEFAULT, LAZY, EAGER (default is LAZY) Specifies the data loading mode for the application When set to EAGER, all data from a cubin, fatbin or a PTX file are fully loaded to memory upon corresponding cuLibraryLoad* Data loading behavior is inherited from CUDA_MODULE_LOADING if this environment variable is not set"
  },
  {
    "id": 6406,
    "content": "Pre-loading dependent libraries CUDA_FORCE_PRELOAD_LIBRARIES 0 or 1 (default is 0) When set to 1, forces the driver to preload the libraries required for NVVM and PTX just-in-time compilation during driver initialization This will increase the memory footprint and the time taken for CUDA driver initialization"
  },
  {
    "id": 6407,
    "content": "This environment variable needs to be set to avoid certain deadlock situations involving multiple CUDA threads"
  },
  {
    "id": 6408,
    "content": "CUDA Graphs CUDA_GRAPHS_USE_NODE_PRIORITY 0 or 1 Overrides the cudaGraphInstantiateFlagUseNodePriority flag on graph instantiation When set to 1, the flag will be set for all graphs and when set to 0, the flag will be cleared for all graphs"
  },
  {
    "id": 6415,
    "content": "This documentation on Unified Memory is divided into 3 parts: General description of unified memory Unified Memory on devices with full CUDA Unified Memory support Unified Memory on devices without full CUDA Unified Memory support 19"
  },
  {
    "id": 6417,
    "content": "Unified Memory Introduction  CUDA Unified Memory provides all processors with: a single unified memory pool, that is, a single pointer value enables all processors in the system (all CPUs, all GPUs, etc ) to access this memory with all of their native memory operations (pointer dereferenes, atomics, etc"
  },
  {
    "id": 6419,
    "content": "Unified Memory improves GPU programming in several ways: Producitivity : GPU programs may access Unified Memory from GPU and CPU threads concurrently without needing to create separate allocations ( cudaMalloc() ) and copy memory manually back and forth ( cudaMemcpy*() )"
  },
  {
    "id": 6420,
    "content": "Performance : Data access speed may be maximized by migrating data towards processors that access it most frequently"
  },
  {
    "id": 6424,
    "content": "With CUDA Unified Memory, data movement still takes place, and hints may improve performance These hints are not required for correctness or functionality, that is, programmers may focus on parallelizing their applications across GPUs and CPUs first, and worry about data-movement later in the development cycle as a performance optimzation"
  },
  {
    "id": 6425,
    "content": "Note that the physical location of data is invisible to a program and may be changed at any time, but accesses to the data’s virtual address will remain valid and coherent from any processor regardless of locality"
  },
  {
    "id": 6426,
    "content": "There are two main ways to obtain CUDA Unified Memory: System-Allocated Memory : memory allocated on the host with system APIs: stack variables, global-/file-scope variables, malloc() / mmap() (see System Allocator for in-depth examples), thread locals, etc CUDA APIs that explicitly allocate Unified Memory : memory allocated with, for example, cudaMallocManaged() , are available on more systems"
  },
  {
    "id": 6431,
    "content": "System Requirements for Unified Memory  The following table shows the different levels of support for CUDA Unified Memory, the device properties required to detect these levels of support and links to the documentation specific to each level of support: Table 25 Overview of levels of unified memory support  Unified Memory Support Level System device properties Further documentation Full CUDA"
  },
  {
    "id": 6432,
    "content": "Unified Memory: all memory has full support Set to 1: pageableMemoryAccess Systems with hardware acceleration also have the following properties set to 1: hostNativeAtomicSupported , pageableMemoryAccessUsesHostPageTables , directManagedMemAccessFromHost Unified Memory on devices with full CUDA Unified Memory support Only CUDA Managed Memory has full support Set to 1: concurrentManagedAccess Set"
  },
  {
    "id": 6433,
    "content": "to 0: pageableMemoryAccess Unified Memory on devices with only CUDA Managed Memory support CUDA Managed Memory without full support: unified addressing but no concurrent access Set to 1: managedMemory Set to 0: concurrentManagedAccess Unified Memory on Windows or devices with compute capability 5 x CUDA for Tegra Memory Management Unified Memory on Tegra No Unified Memory support Set to 0:"
  },
  {
    "id": 6434,
    "content": "managedMemory CUDA for Tegra Memory Management The behavior of an application that attempts to use Unified Memory on a system that does not support it is undefined The following properties enable CUDA applications to check the level of system support for Unified Memory, and to be portable between systems with different levels of support: pageableMemoryAccess : This property is set to 1 on systems"
  },
  {
    "id": 6435,
    "content": "with CUDA Unified Memory support where all threads may access System-Allocated Memory and CUDA Managed Memory"
  },
  {
    "id": 6436,
    "content": "These systems include NVIDIA Grace Hopper, IBM Power9 + Volta, and modern Linux systems with HMM enabled (see next bullet), among others Linux HMM requires Linux kernel version 6"
  },
  {
    "id": 6443,
    "content": "concurrentManagedAccess : This property is set to 1 on systems with full CUDA Managed Memory support When this property is set to 0, there is only partial support for Unified Memory in CUDA Managed Memory A program may query the level of GPU support for CUDA Unified Memory, by querying the attributes in Table Overview of levels of unified memory support above using cudaGetDeviceProperties()"
  },
  {
    "id": 6447,
    "content": "Programming Model  With CUDA Unified Memory, separate allocations between host and device, and explicit memory transfers between them, are no longer required Programs may allocate Unified Memory in the following ways: System-Allocation APIs : on systems with full CUDA Unified Memory support via any system allocation of the host process (C’s malloc() , C++’s new operator, POSIX’s mmap and so on)"
  },
  {
    "id": 6448,
    "content": "CUDA Managed Memory Allocation APIs : via the cudaMallocManaged() API which is syntactically similar to cudaMalloc() CUDA Managed Variables : variables declared with __managed__ , which are semantically similar to a __device__ variable"
  },
  {
    "id": 6449,
    "content": "Most examples in this chapter provide at least two versions, one using CUDA Managed Memory and one using System-Allocated Memory"
  },
  {
    "id": 6450,
    "content": "ret may be used without a separate host_ret allocation and no copy routine is required, greatly simplifying and reducing the size of the program"
  },
  {
    "id": 6451,
    "content": "Managed Memory : data allocation changed to use cudaMallocManaged() , which returns a pointer valid from both host and device code"
  },
  {
    "id": 6456,
    "content": "Allocation APIs for System-Allocated Memory  On systems with full CUDA Unified Memory support , all memory is unified memory This includes memory allocated with system allocation APIs, such as malloc() , mmap() , C++ new() operator, and also automatic variables on CPU thread stacks, thread locals, global variables, and so on System-Allocated Memory may be popullated on first touch, depending on"
  },
  {
    "id": 6457,
    "content": "the API and system settings used First touch means that: - The allocation APIs allocate virtual memory and return immediately, and - physical memory is populated when a thread accesses the memory for the first time Usually, the physical memory will be chosen “close” to the processor that thread is running on For example, - GPU thread accesses it first: physical GPU memory of GPU that thread runs"
  },
  {
    "id": 6458,
    "content": "on is chosen - CPU thread accesses it first: physical CPU memory in the memory NUMA node of the CPU core that thread runs on is chosen CUDA Unified Memory Hint and Prefetch APIs, cudaMemAdvise and cudaMemPreftchAsync , may be used on System-Allocated Memory __global__ void printme ( char * str ) { printf ( str ); } int main () { Allocate 100 bytes of memory, accessible to both Host and Device code"
  },
  {
    "id": 6459,
    "content": "char * s = ( char * ) malloc ( 100 ); Physical allocation placed in CPU memory because host accesses \"s\" first strncpy ( s , \"Hello Unified Memory \" , 99 ); Here we pass \"s\" to a kernel without explicitly copying printme >> ( s ); cudaDeviceSynchronize (); Free as for normal CUDA allocations cudaFree ( s ); return 0 ; } 19"
  },
  {
    "id": 6463,
    "content": "Allocation API for CUDA Managed Memory: cudaMallocManaged()  On systems with CUDA Managed Memory support, unified memory may be allocated using: __host__ cudaError_t cudaMallocManaged ( void ** devPtr , size_t size ); This API is syntactically identical to cudaMalloc() : it allocates size bytes of managed memory and sets devPtr to refer to the allocation On systems with full CUDA Managed Memory"
  },
  {
    "id": 6464,
    "content": "support , managed memory allocations may be accessed concurrently by all CPUs and GPUs in the system Replacing host calls to cudaMalloc() with cudaMallocManaged() , does not impact program semantics on these systems; device code is not able to call cudaMallocManaged() The following example shows the use of cudaMallocManaged() : __global__ void printme ( char * str ) { printf ( str ); } int main ()"
  },
  {
    "id": 6465,
    "content": "{ Allocate 100 bytes of memory, accessible to both Host and Device code char * s ; cudaMallocManaged ( & s , 100 ); Note direct Host-code use of \"s\" strncpy ( s , \"Hello Unified Memory \" , 99 ); Here we pass \"s\" to a kernel without explicitly copying printme >> ( s ); cudaDeviceSynchronize (); Free as for normal CUDA allocations cudaFree ( s ); return 0 ; } Note For systems that support CUDA"
  },
  {
    "id": 6475,
    "content": "Global-Scope Managed Variables Using __managed__  CUDA __managed__ variables behave as if they were allocated via cudaMallocManaged() (see Explicit Allocation Using cudaMallocManaged() )"
  },
  {
    "id": 6476,
    "content": "They simplify programs with global variables, making it particularly easy to exchange data between host and device without manual allocations or copying"
  },
  {
    "id": 6477,
    "content": "On systems with full CUDA Unified Memory support , file-scope or global-scope variables cannot be directly accessed by device code"
  },
  {
    "id": 6478,
    "content": "But a pointer to these variables may be passed to the kernel as an argument, see System Allocator for examples"
  },
  {
    "id": 6479,
    "content": "System Allocator __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } int main () { Requires System-Allocated Memory support int value ; write_value >> ( & value , 1 ); Synchronize required (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \" , value ); return 0 ; } Managed __global__ void write_value ( int * ptr , int v ) { * ptr = v ; } Requires"
  },
  {
    "id": 6480,
    "content": "CUDA Managed Memory support __managed__ int value ; int main () { write_value >> ( & value , 1 ); Synchronize required (before, cudaMemcpy was synchronizing) cudaDeviceSynchronize (); printf ( \"value = %d \" , value ); return 0 ; } Note the absence of explicit cudaMemcpy() commands and the fact that the return array ret is visible on both CPU and GPU"
  },
  {
    "id": 6481,
    "content": "CUDA __managed__ variable implies __device__ and is equivalent to __managed__ __device__ , which is also allowed Accessing __managed__ variables can trigger CUDA context creation if a context for the current device hasn’t already been created In the example above, accessing x before the kernel launch triggers context creation on device 0 C++ objects declared as __managed__ are subject to certain"
  },
  {
    "id": 6482,
    "content": "specific constraints, particularly where static initializers are concerned Note For devices with CUDA Managed Memory without full support , visibility of __managed__ variables for asynchronous operations executing in CUDA streams is discussed in the section on Managing Data Visibility and Concurrent CPU + GPU Access with Streams"
  },
  {
    "id": 6487,
    "content": "Difference between Unified Memory and Mapped Memory  The main difference between Unified Memory and CUDA Mapped Memory is that CUDA Mapped Memory does not guarantee that all kinds of memory accesses (for example atomics) are supported on all systems, while Unified Memory does The limited set of memory operations that are guaranteed to be portably supported by CUDA Mapped Memory is available on"
  },
  {
    "id": 6493,
    "content": "Pointer Attributes  CUDA Programs may check whether a pointer addresses a CUDA Managed Memory allocation by calling cudaPointerGetAttributes() and testing whether the pointer attribute value is cudaMemoryTypeManaged"
  },
  {
    "id": 6494,
    "content": "This API returns cudaMemoryTypeHost for system-allocated memory that has been registered with cudaHostRegister() and cudaMemoryTypeUnregistered for system-allocated memory that CUDA is unaware of Pointer attributes do not state where the memory resides, they state how the memory was allocated or registered"
  },
  {
    "id": 6495,
    "content": "The following example shows how to detect the type of pointer at runtime: char const * kind ( cudaPointerAttributes a , bool pma , bool cma ) { switch ( a"
  },
  {
    "id": 6496,
    "content": "Data Usage Hints  When multiple processors simultaneously access the same data, cudaMemAdvise may be used to hint how the data at [devPtr, devPtr + count) will be accessed: cudaError_t cudaMemAdvise ( const void * devPtr , size_t count , enum cudaMemoryAdvise advice , int device ); Where advice may take the following values: cudaMemAdviseSetReadMostly : This implies that the data is mostly going"
  },
  {
    "id": 6498,
    "content": "Example: void test_advise_managed ( cudaStream_t stream ) { char * dataPtr ; size_t dataSize = 64 * TPB ; 16 KiB Allocate memory using cudaMallocManaged (malloc may be used on systems with full CUDA Unified memory support) cudaMallocManaged ( & dataPtr , dataSize ); Set the advice on the memory region cudaMemAdvise ( dataPtr , dataSize , cudaMemAdviseSetReadMostly , myGpuId ); int outerLoopIter ="
  },
  {
    "id": 6499,
    "content": "0 ; while ( outerLoopIter >> (( const char * ) dataPtr , dataSize ); innerLoopIter ++ ; } outerLoopIter ++ ; } cudaFree ( dataPtr ); } cudaMemAdviseSetPreferredLocation : In general, any memory may be migrated at any time to any location, for example, when a given processor is running out of physical memory"
  },
  {
    "id": 6500,
    "content": "This hint tells the system that migrating this memory region away from its preferred location is undesired, by setting the preferred location for the data to be the physical memory belonging to device Passing in a value of cudaCpuDeviceId for device sets the preferred location as CPU memory Other hints, like cudaMemPrefetchAsync , may override this hint, leading the memory to be migrated away"
  },
  {
    "id": 6502,
    "content": "cudaMemAdviseSetAccessedBy : In some systems, it may be beneficial for performance to establish a mapping into memory before accessing the data from a given processor"
  },
  {
    "id": 6503,
    "content": "This hint tells the system that the data will be frequently accessed by device , enabling the system to assume that creating these mappings pays off This hint does not imply where the data should reside, but it can be combined with cudaMemAdviseSetPreferredLocation to specify that"
  },
  {
    "id": 6504,
    "content": "Each advice can be also unset by using one of the following values: cudaMemAdviseUnsetReadMostly , cudaMemAdviseUnsetPreferredLocation and cudaMemAdviseUnsetAccessedBy"
  },
  {
    "id": 6510,
    "content": "Querying Data Usage Attributes on Managed Memory  A program can query memory range attributes assigned through cudaMemAdvise or cudaMemPrefetchAsync on CUDA Managed Memory by using the following API: cudaMemRangeGetAttribute ( void * data , size_t dataSize , enum cudaMemRangeAttribute attribute , const void * devPtr , size_t count ); This function queries an attribute of the memory range"
  },
  {
    "id": 6511,
    "content": "starting at devPtr with a size of count bytes The memory range must refer to managed memory allocated via cudaMallocManaged or declared via __managed__ variables It is possible to query the following attributes: cudaMemRangeAttributeReadMostly : the result returned will be 1 if the entire memory range has the cudaMemAdviseSetReadMostly attribute set, or 0 otherwise"
  },
  {
    "id": 6512,
    "content": "cudaMemRangeAttributePreferredLocation : the result returned will be a GPU device id or cudaCpuDeviceId if the entire memory range has the corresponding processor as preferred location, otherwise cudaInvalidDeviceId will be returned An application can use this query API to make decision about staging data through CPU or GPU depending on the preferred location attribute of the managed pointer Note"
  },
  {
    "id": 6513,
    "content": "that the actual location of the memory range at the time of the query may be different from the preferred location cudaMemRangeAttributeAccessedBy : will return the list of devices that have that advise set for that memory range cudaMemRangeAttributeLastPrefetchLocation : will return the last location to which the memory range was prefetched explicitly using cudaMemPrefetchAsync Note that this"
  },
  {
    "id": 6514,
    "content": "simply returns the last location that the application requested to prefetch the memory range to It gives no indication as to whether the prefetch operation to that location has completed or even begun"
  },
  {
    "id": 6515,
    "content": "Additionally, multiple attributes can be queried by using corresponding cudaMemRangeGetAttributes function"
  },
  {
    "id": 6521,
    "content": "System-Allocated Memory: in-depth examples  Systems with full CUDA Unified Memory support allow the device to access any memory owned by the host process interacting with the device"
  },
  {
    "id": 6522,
    "content": "The next three tabs show various ways a file-scope or global-scope variable can be accessed from the device"
  },
  {
    "id": 6523,
    "content": "Note that for the extern variable, it could be declared and its memory owned and managed by a third-party library, which does not interact with CUDA at all"
  },
  {
    "id": 6524,
    "content": "Also note that stack variables as well as file-scope and global-scope variables can only be accessed through a pointer by the GPU"
  },
  {
    "id": 6525,
    "content": "In this specific example, this is convenient because the character array is already declared as a pointer: const char*"
  },
  {
    "id": 6526,
    "content": "However, consider the following example with a global-scope integer: this variable is declared at global scope int global_variable ; __global__ void kernel_uncompilable () { this causes a compilation error: global (__host__) variables must not be accessed from __device__ / __global__ code printf ( \"%d \" , global_variable ); } On systems with pageableMemoryAccess set to 1, we can access the"
  },
  {
    "id": 6527,
    "content": "address of a global variable The below kernel takes that address as an argument __global__ void kernel ( int * global_variable_addr ) { printf ( \"%d \" , * global_variable_addr ); } int main () { kernel >> ( & global_variable ); return 0 ; } In the example above, we need to ensure to pass a pointer to the global variable to the kernel instead of directly accessing the global variable in the kernel"
  },
  {
    "id": 6528,
    "content": "This is because global variables without the __managed__ specifier are declared as __host__ -only by default, thus most compilers won’t allow using these variables directly in device code as of now"
  },
  {
    "id": 6533,
    "content": "File-backed Unified Memory  Since systems with full CUDA Unified Memory support allow the device to access any memory owned by the host process, they can directly access file-backed memory Here, we show a modified version of the initial example shown in the previous section to use file-backed memory in order to print a string from the GPU, read directly from an input file In the following"
  },
  {
    "id": 6534,
    "content": "example, the memory is backed by a physical file, but the example applies to memory-backed files, too, as shown in the section on Inter-Process Communication with Unified Memory"
  },
  {
    "id": 6535,
    "content": "__global__ void kernel ( const char * type , const char * data ) { static const int n_char = 8 ; printf ( \"%s - first %d characters: '\" , type , n_char ); for ( int i = 0 ; i = 0 , \"Invalid file handle\" ); struct stat file_stat ; int status = fstat ( fd , & file_stat ); ASSERT ( status >= 0 , \"Invalid file stats\" ); char * mapped = ( char * ) mmap ( 0 , file_stat"
  },
  {
    "id": 6536,
    "content": "st_size , PROT_READ , MAP_PRIVATE , fd , 0 ); ASSERT ( mapped = MAP_FAILED , \"Cannot map file into memory\" ); kernel >> ( \"file-backed\" , mapped ); ASSERT ( cudaDeviceSynchronize () == cudaSuccess , \"CUDA failed with '%s'\" , cudaGetErrorString ( cudaGetLastError ())); ASSERT ( munmap ( mapped , file_stat st_size ) == 0 , \"Cannot unmap file\" ); ASSERT ( close ( fd ) == 0 , \"Cannot close file\" ); }"
  },
  {
    "id": 6537,
    "content": "Note that on systems without the hostNativeAtomicSupported property, including systems with Linux HMM enabled , atomic accesses to file-backed memory are not supported"
  },
  {
    "id": 6542,
    "content": "Inter-Process Communication (IPC) with Unified Memory  Note As of now, using IPC with Unified Memory can have significant performance implications Many applications prefer to manage one GPU per process, but still need to use Unified Memory, for example for over-subscription, and access it from multiple GPUs CUDA IPC (see Interprocess Communication ) does not support Managed Memory: handles to"
  },
  {
    "id": 6543,
    "content": "this type of memory may not be shared through any of the mechanisms discussed in this section On systems with full CUDA Unified Memory support , System-Allocated Memory is Inter-Process Communication (IPC) capable Once access to System-Allocated Memory has been shared with other processes, the same Unified Memory Programming Model applies, similar to File-backed Unified Memory See the following"
  },
  {
    "id": 6544,
    "content": "references for more information on various ways of creating IPC-capable System-Allocated Memory under Linux: mmap with MAP_SHARED POSIX IPC APIs Linux memfd_create Note that it is not possible to share memory between different hosts and their devices using this technique"
  },
  {
    "id": 6548,
    "content": "Performance Tuning  In order to achieve good performance with Unified Memory, it is important to: Understand how paging works on your system, and how to avoid unnecessary page faults As general advice, Unified Memory Performance Hints might provide improved performance, but using them incorrectly might degrade performance compared to the default behavior Also note that any hint has a performance"
  },
  {
    "id": 6549,
    "content": "cost associated with it on the host, thus useful hints must at the very least improve performance enough to overcome this cost"
  },
  {
    "id": 6554,
    "content": "Memory Paging and Page Sizes  Many of the sections for unified memory performance tuning assume prior knowledge on virtual addressing, memory pages and page sizes This section attempts to define all necessary terms and explain why paging matters for performance All currently supported systems for Unified Memory use a virtual address space: this means that memory addresses used by an application"
  },
  {
    "id": 6555,
    "content": "represent a virtual location which might be mapped to a physical location where the memory actually resides All currently supported processors, including both CPUs and GPUs, additionally use memory paging Because all systems use a virtual address space, there are two types of memory pages: Virtual pages: this represents a fixed-size contiguous chunk of virtual memory per process tracked by the"
  },
  {
    "id": 6556,
    "content": "operating system, which can be mapped into physical memory Note that the virtual page is linked to the mapping : for example, a single virtual address might be mapped into physical memory using different page sizes Physical pages: this represents a fixed-size contiguous chunk of memory the processor’s main Memory Management Unit (MMU) supports and into which a virtual page can be mapped Arm CPUs"
  },
  {
    "id": 6557,
    "content": "support multiple physical page sizes - 4KiB, 16KiB, 32KiB and 64KiB - depending on the exact CPU Finally, NVIDIA GPUs support multiple physical page sizes, but prefer 2MiB physical pages or larger The default page size of virtual pages usually corresponds to the physical page size, but an application may use different page sizes as long as they are supported by the operating system and the"
  },
  {
    "id": 6558,
    "content": "hardware Typically, supported virtual page sizes must be powers of 2 and multiples of the physical page size The logical entity tracking the mapping of virtual pages into physical pages will be referred to as a page table , and each mapping of a given virtual page with a given virtual size to physical pages is called a page table entry (PTE) All supported processors provide specific caches for the"
  },
  {
    "id": 6559,
    "content": "page table to speed up the translation of virtual addresses to physical addresses There are two important aspects for performance tuning of applications: the choice of virtual page size, whether the system offers a combined page table used by both CPUs and GPUs, or separate page tables for each CPU and GPU individually"
  },
  {
    "id": 6565,
    "content": "Choosing the right page size  In general, small page sizes lead to less (virtual) memory fragmentation but more TLB misses, whereas larger page sizes lead to more memory fragmentation but less TLB misses Additionally, memory migration is generally more expensive with larger page sizes compared to smaller page sizes, because we typically migrate full memory pages One important aspect for"
  },
  {
    "id": 6566,
    "content": "performance tuning is that TLB misses are generally significantly more expensive on the GPU compared to the CPU This means that if a GPU thread frequently accesses random locations of Unified Memory mapped using a small enough page size, it might be significantly slower compared to the same accesses to Unified Memory mapped using a large enough page size While a similar effect might occur for a"
  },
  {
    "id": 6567,
    "content": "CPU thread randomly accessing a large area of memory mapped using a small page size, the slowdown is less pronounced, meaning that the application might want to trade-off this slowdown with having less memory fragmentation Note that in general, applications should not tune their performance to the physical page size of a given processor, since physical page sizes are subject to change depending on"
  },
  {
    "id": 6574,
    "content": "CPU and GPU page tables: hardware coherency vs software coherency  Note In the remainder of the performance tuning documentation, we will refer to systems with a combined page table for both CPUs and GPUs as hardware coherent systems Systems with separate page tables for CPUs and GPUs are referred to as software coherent Hardware coherent systems such as NVIDIA Grace Hopper offer a logically"
  },
  {
    "id": 6575,
    "content": "combined page table for both CPUs and GPUs This is important because in order to access System-Allocated Memory from the GPU , the GPU uses whichever page table entry was created by the CPU for the requested memory If that page table entry uses the default CPU page size of 4KiB or 64KiB, accesses to large virtual memory areas will cause significant TLB misses, thus significant slowdowns See the"
  },
  {
    "id": 6576,
    "content": "section on configuring huge pages for examples on how to ensure System-Allocated Memory uses large enough page sizes to avoid this type of issue On the other hand, on systems where the CPUs and GPUs each have their own logical page table, different performance tuning aspects should be considered: in order to guarantee coherency , these systems usually use page faults in case a processor accesses a"
  },
  {
    "id": 6577,
    "content": "memory address mapped into the physical memory of a different processor Such a page fault means that: it needs to be ensured that the currently owning processor (where the physical page currently resides) cannot access this page anymore, either by deleting the page table entry or updating it it needs to be ensured that the processor requesting access can access this page, either by creating a new"
  },
  {
    "id": 6578,
    "content": "page table entry or updating and existing entry, such that it becomes valid/active the physical page backing this virtual page must be moved/migrated to the processor requesting access: this can be an expensive operation, and the amount of work is proportional to the page size Overall, hardware coherent systems provide significant performance benefits compared to software coherent systems in cases"
  },
  {
    "id": 6579,
    "content": "where frequent concurrent accesses to the same memory page are made by both CPU and GPU threads: less page-faults: these systems do not need to use page-faults for emulating coherency or migrating memory, less contention: these systems are coherent at cache-line granularity instead of page-size granularity, that is, when there is contention from multiple processors within a cache line, only the"
  },
  {
    "id": 6580,
    "content": "cache line is exchanged which is much smaller than the smallest page-size, and when the different processors access different cache-lines within a page, then there is no contention This impacts the performance of the following scenarios: Atomic updates to the same address concurrently from both CPUs and GPUs"
  },
  {
    "id": 6586,
    "content": "Direct Unified Memory Access from host  Some devices have hardware support for coherent reads, stores and atomic accesses from the host on GPU-resident unified memory Note that all hardware coherent systems have this attribute set for NVLink-connected devices On these systems, the host has direct access to GPU-resident memory without page faults and data migration (see Data Usage Hints for more"
  },
  {
    "id": 6587,
    "content": "details on memory usage hints) Note that with CUDA Managed Memory, the cudaMemAdviseSetAccessedBy hint with cudaCpuDeviceId is necessary to enable this direct access without page faults"
  },
  {
    "id": 6588,
    "content": "Consider an example code below: System Allocator __global__ void write ( int * ret , int a , int b ) { ret [ threadIdx x ; } void test_malloc () { int * ret = ( int * ) malloc ( 1000 * sizeof ( int )); for shared page table systems, the following hint is not necesary cudaMemAdvise ( ret , 1000 * sizeof ( int ), cudaMemAdviseSetAccessedBy , cudaCpuDeviceId ); write >> ( ret , 10 , 100 ); pages"
  },
  {
    "id": 6589,
    "content": "populated in GPU memory cudaDeviceSynchronize (); for ( int i = 0 ; i >> ( ret , 10 , 100 ); directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations cudaDeviceSynchronize (); directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations free ( ret ); } Managed __global__ void write ( int * ret , int a , int b ) { ret [ threadIdx x ; } void test_managed ()"
  },
  {
    "id": 6590,
    "content": "{ int * ret ; cudaMallocManaged ( & ret , 1000 * sizeof ( int )); cudaMemAdvise ( ret , 1000 * sizeof ( int ), cudaMemAdviseSetAccessedBy , cudaCpuDeviceId ); set direct access hint write >> ( ret , 10 , 100 ); pages populated in GPU memory cudaDeviceSynchronize (); for ( int i = 0 ; i >> ( ret , 10 , 100 ); directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations"
  },
  {
    "id": 6591,
    "content": "cudaDeviceSynchronize (); directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations cudaFree ( ret ); } After write kernel is completed, ret will be created and initialized in GPU memory This code will show different behavior depending on the system architecture and support of hardware coherency: On systems with directManagedMemAccessFromHost=1 : CPU accesses to the"
  },
  {
    "id": 6592,
    "content": "managed buffer will not trigger any migrations; the data will remain resident in GPU memory and any subsequent GPU kernels can continue to access it directly without inflicting faults or migrations On systems with directManagedMemAccessFromHost=0 : CPU accesses to the managed buffer will page fault and initiate data migration; any GPU kernel trying to access the same data first time will page"
  },
  {
    "id": 6598,
    "content": "Host Native Atomics  Some devices, including NVLink-connected devices in hardware coherent systems , support hardware-accelerated atomic accesses to CPU-resident memory This implies that atomic accesses to host memory do not have to be emulated with a page fault"
  },
  {
    "id": 6605,
    "content": "Unified memory on devices with only CUDA Managed Memory support  For devices with compute capability 6 x or higher but without pageable memory access , CUDA Managed Memory is fully supported and coherent The programming model and performance tuning of unified memory is largely similar to the model as described in Unified memory on devices with full CUDA Unified Memory support , with the notable"
  },
  {
    "id": 6607,
    "content": "Thus, the following list of sub-sections do not apply: System Allocator Hardware/Software Coherency 19"
  },
  {
    "id": 6610,
    "content": "Unified memory on Windows or devices with compute capability 5 x  Devices with compute capability lower than 6 0 or Windows platforms support CUDA Managed Memory v1"
  },
  {
    "id": 6612,
    "content": "The following sub-sections describe in more detail how to use and optimize Managed Memory on these platforms"
  },
  {
    "id": 6618,
    "content": "0 do not support fine-grained movement of the managed data to GPU on-demand Whenever a GPU kernel is launched all managed memory generally has to be transferred to GPU memory to avoid faulting on memory access"
  },
  {
    "id": 6620,
    "content": "x a new GPU page faulting mechanism is introduced that provides more seamless Unified Memory functionality Combined with the system-wide virtual address space, page faulting provides several benefits First, page faulting means that the CUDA system software doesn’t need to synchronize all managed memory allocations to the GPU before each kernel launch If a kernel running on the GPU accesses a page"
  },
  {
    "id": 6621,
    "content": "that is not resident in its memory, it faults, allowing the page to be automatically migrated to the GPU memory on-demand Alternatively, the page may be mapped into the GPU address space for access over the PCIe or NVLink interconnects (mapping on access can sometimes be faster than migration) Note that Unified Memory is system-wide: GPUs (and CPUs) can fault on and migrate memory pages either"
  },
  {
    "id": 6627,
    "content": "GPU Memory Oversubscription  Devices of compute capability lower than 6 0 cannot allocate more managed memory than the physical size of GPU memory"
  },
  {
    "id": 6633,
    "content": "0 managed allocations are automatically visible to all GPUs in a system via the peer-to-peer capabilities of the GPUs Managed memory allocations behave similar to unmanaged memory allocated using cudaMalloc() : the current active device is the home for the physical allocation but other GPUs in the system will access the memory at reduced bandwidth over the PCIe bus On Linux the managed memory is"
  },
  {
    "id": 6634,
    "content": "allocated in GPU memory as long as all GPUs that are actively being used by a program have the peer-to-peer support If at any time the application starts using a GPU that doesn’t have peer-to-peer support with any of the other GPUs that have managed allocations on them, then the driver will migrate all managed allocations to system memory On Windows, if peer mappings are not available (for"
  },
  {
    "id": 6635,
    "content": "example, between GPUs of different architectures), then the system will automatically fall back to using zero-copy memory, regardless of whether both GPUs are actually used by a program If only one GPU is actually going to be used, it is necessary to set the CUDA_VISIBLE_DEVICES environment variable before launching the program This constrains which GPUs are visible and allows managed memory to be"
  },
  {
    "id": 6637,
    "content": "Alternatively, on Windows users can also set CUDA_MANAGED_FORCE_DEVICE_ALLOC to a non-zero value to force the driver to always use device memory for physical storage When this environment variable is set to a non-zero value, all devices used in that process that support managed memory have to be peer-to-peer compatible with each other The error ::cudaErrorInvalidDevice will be returned if a"
  },
  {
    "id": 6638,
    "content": "device that supports managed memory is used and it is not peer-to-peer compatible with any of the other managed memory supporting devices that were previously used in that process, even if ::cudaDeviceReset has been called on those devices"
  },
  {
    "id": 6645,
    "content": "Coherency and Concurrency  Simultaneous access to managed memory on devices of compute capability lower than 6"
  },
  {
    "id": 6646,
    "content": "0 is not possible, because coherence could not be guaranteed if the CPU accessed a Unified Memory allocation while a GPU kernel was active"
  },
  {
    "id": 6653,
    "content": "x GPU architectures, the Unified Memory programming model puts constraints on data accesses while both the CPU and GPU are executing concurrently In effect, the GPU has exclusive access to all managed data while any kernel operation is executing, regardless of whether the specific kernel is actively using the data When managed data is used with cudaMemcpy*() or cudaMemset*() , the system may"
  },
  {
    "id": 6654,
    "content": "choose to access the source or destination from the host or the device, which will put constraints on concurrent CPU access to that data while the cudaMemcpy*() or cudaMemset*() is executing It is not permitted for the CPU to access any managed allocations or variables while the GPU is active for devices with concurrentManagedAccess property set to 0 On these systems concurrent CPU/GPU accesses,"
  },
  {
    "id": 6655,
    "content": "even to different managed memory allocations, will cause a segmentation fault because the page is considered inaccessible to the CPU __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { kernel >> (); y = 20 ; Error on GPUs not supporting concurrent access cudaDeviceSynchronize (); return 0 ; } In example above, the GPU program kernel is still active when the"
  },
  {
    "id": 6662,
    "content": "The program must explicitly synchronize with the GPU before accessing y : __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { kernel >> (); cudaDeviceSynchronize (); y = 20 ;   Success on GPUs not supporing concurrent access return 0 ; } As this example shows, on systems with pre-6"
  },
  {
    "id": 6663,
    "content": "x GPU architectures, a CPU thread may not access any managed data in between performing a kernel launch and a subsequent synchronization call, regardless of whether the GPU kernel actually touches that same data (or any managed data at all)"
  },
  {
    "id": 6664,
    "content": "The mere potential for concurrent CPU and GPU access is sufficient for a process-level exception to be raised"
  },
  {
    "id": 6665,
    "content": "Note that if memory is dynamically allocated with cudaMallocManaged() or cuMemAllocManaged() while the GPU is active, the behavior of the memory is unspecified until additional work is launched or the GPU is synchronized"
  },
  {
    "id": 6666,
    "content": "Attempting to access the memory on the CPU during this time may or may not cause a segmentation fault"
  },
  {
    "id": 6673,
    "content": "Explicit Synchronization and Logical GPU Activity  Note that explicit synchronization is required even if kernel runs quickly and finishes before the CPU touches y in the above example"
  },
  {
    "id": 6674,
    "content": "This aligns with the CUDA programming model, which specifies that a kernel can run at any time following a launch and is not guaranteed to have finished until the host issues a synchronization call"
  },
  {
    "id": 6675,
    "content": "This includes cudaDeviceSynchronize() ; cudaStreamSynchronize() and cudaStreamQuery() (provided it returns cudaSuccess and not cudaErrorNotReady ) where the specified stream is the only stream still executing on the GPU; cudaEventSynchronize() and cudaEventQuery() in cases where the specified event is not followed by any device work; as well as uses of cudaMemcpy() and cudaMemset() that are"
  },
  {
    "id": 6677,
    "content": "Dependencies created between streams will be followed to infer completion of other streams by synchronizing on a stream or event Dependencies can be created via cudaStreamWaitEvent() or implicitly when using the default (NULL) stream"
  },
  {
    "id": 6678,
    "content": "It is legal for the CPU to access managed data from within a stream callback, provided no other stream that could potentially be accessing managed data is active on the GPU In addition, a callback that is not followed by any device work can be used for synchronization: for example, by signaling a condition variable from inside the callback; otherwise, CPU access is valid only for the duration of"
  },
  {
    "id": 6679,
    "content": "the callback(s) There are several important points of note: It is always permitted for the CPU to access non-managed zero-copy data while the GPU is active The GPU is considered active when it is running any kernel, even if that kernel does not make use of managed data If a kernel might use data, then access is forbidden, unless device property concurrentManagedAccess is 1 There are no constraints"
  },
  {
    "id": 6680,
    "content": "on concurrent inter-GPU access of managed memory, other than those that apply to multi-GPU access of non-managed memory Note how the last point allows for races between GPU kernels, as is currently the case for non-managed GPU memory As mentioned previously, managed memory functions identically to non-managed memory from the perspective of the GPU The following code example illustrates these"
  },
  {
    "id": 6681,
    "content": "points: int main () { cudaStream_t stream1 , stream2 ; cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); int * non_managed , * managed , * also_managed ; cudaMallocHost ( & non_managed , 4 ); Non-managed, CPU-accessible memory cudaMallocManaged ( & managed , 4 ); cudaMallocManaged ( & also_managed , 4 ); Point 1: CPU can access non-managed data kernel >> ( managed ); * non_managed ="
  },
  {
    "id": 6682,
    "content": "1 ; Point 2: CPU cannot access any managed data while GPU is busy, unless concurrentManagedAccess = 1 Note we have not yet synchronized, so \"kernel\" is still active * also_managed = 2 ; Will issue segmentation fault Point 3: Concurrent GPU kernels can access the same data Managing Data Visibility and Concurrent CPU + GPU Access with Streams  Until now it was assumed that for SM architectures"
  },
  {
    "id": 6683,
    "content": "before 6 x: 1) any active kernel may use any managed memory, and 2) it was invalid to use managed memory from the CPU while a kernel is active Here we present a system for finer-grained control of managed memory designed to work on all devices supporting managed memory, including older architectures with concurrentManagedAccess equal to 0"
  },
  {
    "id": 6684,
    "content": "The CUDA programming model provides streams as a mechanism for programs to indicate dependence and independence among kernel launches"
  },
  {
    "id": 6685,
    "content": "Kernels launched into the same stream are guaranteed to execute consecutively, while kernels launched into different streams are permitted to execute concurrently"
  },
  {
    "id": 6686,
    "content": "Streams describe independence between work items and hence allow potentially greater efficiency through concurrency"
  },
  {
    "id": 6687,
    "content": "Unified Memory builds upon the stream-independence model by allowing a CUDA program to explicitly associate managed allocations with a CUDA stream"
  },
  {
    "id": 6688,
    "content": "In this way, the programmer indicates the use of data by kernels based on whether they are launched into a specified stream or not This enables opportunities for concurrency based on program-specific data access patterns"
  },
  {
    "id": 6689,
    "content": "The function to control this behavior is: cudaError_t cudaStreamAttachMemAsync ( cudaStream_t stream , void * ptr , size_t length = 0 , unsigned int flags = 0 ); The cudaStreamAttachMemAsync() function associates length bytes of memory starting from ptr with the specified stream"
  },
  {
    "id": 6691,
    "content": ") Because of this association, the Unified Memory system allows CPU access to this memory region so long as all operations in stream have completed, regardless of whether other streams are active In effect, this constrains exclusive ownership of the managed memory region by an active GPU to per-stream activity instead of whole-GPU activity Most importantly, if an allocation is not associated with"
  },
  {
    "id": 6693,
    "content": "This is the default visibility for a cudaMallocManaged() allocation or a __managed__ variable; hence, the simple-case rule that the CPU may not touch the data while any kernel is running By associating an allocation with a specific stream, the program makes a guarantee that only kernels launched into that stream will touch that data No error checking is performed by the Unified Memory system: it"
  },
  {
    "id": 6695,
    "content": "In addition to allowing greater concurrency, the use of cudaStreamAttachMemAsync() can (and typically does) enable data transfer optimizations within the Unified Memory system that may affect latencies and other overhead"
  },
  {
    "id": 6701,
    "content": "Stream Association Examples  Associating data with a stream allows fine-grained control over CPU + GPU concurrency, but what data is visible to which streams must be kept in mind when using devices of compute capability lower than 6"
  },
  {
    "id": 6703,
    "content": "Looking at the earlier synchronization example: __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { cudaStream_t stream1 ; cudaStreamCreate ( & stream1 ); cudaStreamAttachMemAsync ( stream1 , & y , 0 , cudaMemAttachHost ); cudaDeviceSynchronize ();   Wait for Host attachment to occur"
  },
  {
    "id": 6704,
    "content": "return 0 ; } Here we explicitly associate y with host accessibility, thus enabling access at all times from the CPU"
  },
  {
    "id": 6707,
    "content": "Note that associating a variable with a stream does not change the associating of any other variable For example, associating x with stream1 does not ensure that only x is accessed by kernels launched in stream1 , thus an error is caused by this code: __device__ __managed__ int x , y = 2 ; __global__ void kernel () { x = 10 ; } int main () { cudaStream_t stream1 ; cudaStreamCreate ( & stream1 );"
  },
  {
    "id": 6709,
    "content": "y = 20 ;   ERROR: “y” is still associated globally   with all streams by default return 0 ; } Note how the access to y will cause an error because, even though x has been associated with a stream, we have told the system nothing about who can see y"
  },
  {
    "id": 6710,
    "content": "The system therefore conservatively assumes that kernel might access it and prevents the CPU from doing so"
  },
  {
    "id": 6716,
    "content": "Stream Attach With Multithreaded Host Programs  The primary use for cudaStreamAttachMemAsync() is to enable independent task parallelism using CPU threads Typically in such a program, a CPU thread creates its own stream for all work that it generates because using CUDA’s NULL stream would cause dependencies between threads The default global visibility of managed data to any GPU stream can make"
  },
  {
    "id": 6717,
    "content": "it difficult to avoid interactions between CPU threads in a multi-threaded program Function cudaStreamAttachMemAsync() is therefore used to associate a thread’s managed allocations with that thread’s own stream, and the association is typically not changed for the life of the thread Such a program would simply add a single call to cudaStreamAttachMemAsync() to use unified memory for its data"
  },
  {
    "id": 6718,
    "content": "accesses: This function performs some task, in its own private stream cudaStream_t stream ; cudaStreamCreate ( & stream ); Allocate some managed data and associate with our stream Note the use of the host-attach flag to cudaMallocManaged(); we then associate the allocation with our stream so that our GPU kernel launches can access it int * data ; cudaMallocManaged (( void ** ) & data , length ,"
  },
  {
    "id": 6719,
    "content": "cudaMemAttachHost ); cudaStreamAttachMemAsync ( stream , data ); cudaStreamSynchronize ( stream ); Iterate on the data in some way, using both Host & Device for ( int i = 0 ; i >> ( in , data , length ); cudaStreamSynchronize ( stream ); host_process ( data , length ); CPU uses managed data convert >> ( out , data , length ); } cudaStreamSynchronize ( stream ); cudaStreamDestroy ( stream );"
  },
  {
    "id": 6720,
    "content": "cudaFree ( data ); } In this example, the allocation-stream association is established just once, and then data is used repeatedly by both the host and device The result is much simpler code than occurs with explicitly copying data between host and device, although the result is the same"
  },
  {
    "id": 6726,
    "content": "Advanced Topic: Modular Programs and Data Access Constraints  In the previous example cudaMallocManaged() specifies the cudaMemAttachHost flag, which creates an allocation that is initially invisible to device-side execution"
  },
  {
    "id": 6728,
    "content": ") This ensures that there is no accidental interaction with another thread’s execution in the interval between the data allocation and when the data is acquired for a specific stream Without this flag, a new allocation would be considered in-use on the GPU if a kernel launched by another thread happens to be running This might impact the thread’s ability to access the newly allocated data from"
  },
  {
    "id": 6729,
    "content": "the CPU (for example, within a base-class constructor) before it is able to explicitly attach it to a private stream"
  },
  {
    "id": 6730,
    "content": "To enable safe independence between threads, therefore, allocations should be made specifying this flag"
  },
  {
    "id": 6731,
    "content": "Note An alternative would be to place a process-wide barrier across all threads after the allocation has been attached to the stream This would ensure that all threads complete their data/stream associations before any kernels are launched, avoiding the hazard A second barrier would be needed before the stream is destroyed because stream destruction causes allocations to revert to their default"
  },
  {
    "id": 6733,
    "content": "The cudaMemAttachHost flag exists both to simplify this process, and because it is not always possible to insert global barriers where required"
  },
  {
    "id": 6739,
    "content": "Memcpy()/Memset() Behavior With Stream-associated Unified Memory  See Memcpy()/Memset() Behavior With Unified Memory for a general overview of cudaMemcpy* / cudaMemset* behavior on devices with concurrentManagedAccess set On devices where concurrentManagedAccess is not set, the following rules apply: If cudaMemcpyHostTo* is specified and the source data is unified memory, then it will be"
  },
  {
    "id": 6740,
    "content": "accessed from the host if it is coherently accessible from the host in the copy stream (1) ; otherwise it will be accessed from the device Similar rules apply to the destination when cudaMemcpy*ToHost is specified and the destination is unified memory If cudaMemcpyDeviceTo* is specified and the source data is unified memory, then it will be accessed from the device The source must be coherently"
  },
  {
    "id": 6741,
    "content": "accessible from the device in the copy stream (2) ; otherwise, an error is returned Similar rules apply to the destination when cudaMemcpy*ToDevice is specified and the destination is unified memory If cudaMemcpyDefault is specified, then unified memory will be accessed from the host either if it cannot be coherently accessed from the device in the copy stream (2) or if the preferred location for"
  },
  {
    "id": 6742,
    "content": "the data is cudaCpuDeviceId and it can be coherently accessed from the host in the copy stream (1) ; otherwise, it will be accessed from the device When using cudaMemset*() with unified memory, the data must be coherently accessible from the device in the stream being used for the cudaMemset() operation (2) ; otherwise, an error is returned When data is accessed from the device either by"
  },
  {
    "id": 6743,
    "content": "cudaMemcpy* or cudaMemset* , the stream of operation is considered to be active on the GPU During this time, any CPU access of data that is associated with that stream or data that has global visibility, will result in a segmentation fault if the GPU has a zero value for the device attribute concurrentManagedAccess The program must synchronize appropriately to ensure the operation has completed"
  },
  {
    "id": 6744,
    "content": "before accessing any associated data from the CPU Coherently accessible from the host in a given stream means that the memory neither has global visibility nor is it associated with the given stream Coherently accessible from the device in a given stream means that the memory either has global visibility or is associated with the given stream"
  },
  {
    "id": 6747,
    "content": " Lazy Loading delays loading of CUDA modules and kernels from program initalization closer to kernels execution"
  },
  {
    "id": 6748,
    "content": "If a program does not use every single kernel it has included, then some kernels will be loaded unneccesarily"
  },
  {
    "id": 6749,
    "content": "Most of the time, programs only use a small amount of kernels from libraries they include Thanks to Lazy Loading, programs are able to only load kernels they are actually going to use, saving time on initialization"
  },
  {
    "id": 6750,
    "content": "Firstly, CUDA Runtime will no longer load all modules during program initialization, with the exception of modules containing managed variables"
  },
  {
    "id": 6751,
    "content": "This optimization is only relevant to CUDA Runtime users, CUDA Driver users who use cuModuleLoad are unaffected The behavior for CUDA Driver users who use cuLibraryLoad to load module data into memory can be changed by setting the CUDA_MODULE_DATA_LOADING environment variable"
  },
  {
    "id": 6752,
    "content": "Secondly, loading a module ( cuModuleLoad*() family of functions) will not be loading kernels immediately, instead it will delay loading of a kernel until cuModuleGetFunction() is called There are certain exceptions here, some kernels have to be loaded during cuModuleLoad*() , such as kernels of which pointers are stored in global variables CUDA Runtime will only call cuModuleGetFunction() when a"
  },
  {
    "id": 6754,
    "content": "Both of these optimizations are designed to be invisible to the user, assuming CUDA Programming Model is followed Upgrades to both might be necessary to utilize the feature"
  },
  {
    "id": 6758,
    "content": "Driver  Lazy Loading requires R515+ user-mode library, but it supports Forward Compatibility, meaning it can run on top of older kernel mode drivers Without R515+ user-mode library, Lazy Loading is not available in any shape or form, even if toolkit version is 11"
  },
  {
    "id": 6765,
    "content": "If your application uses CUDA Runtime, then in order to see benefits from Lazy Loading your application must use 11 7+ CUDA Runtime As CUDA Runtime is usually linked statically into programs and libraries, this means that you have to recompile your program with CUDA 11 7+ toolkit and use CUDA 11 7+ libraries Otherwise you will not see the benefits of Lazy Loading, even if your driver version"
  },
  {
    "id": 6766,
    "content": "supports it If only some of your libraries are 11 7+, you will only see benefits of Lazy Loading in those libraries Other libraries will still load everything eagerly"
  },
  {
    "id": 6777,
    "content": "Triggering loading of kernels in lazy mode  Loading kernels and variables happens automatically, without any need for explicit loading"
  },
  {
    "id": 6778,
    "content": "Simply launching a kernel or referencing a variable or a kernel will automatically load relevant modules and kernels"
  },
  {
    "id": 6779,
    "content": "However, if for any reason you wish to load a kernel without executing it or modifying it in any way, we recommend the following"
  },
  {
    "id": 6783,
    "content": "CUDA Driver API  Loading of kernels happens during cuModuleGetFunction() call This call is necessary even without Lazy Loading, as it is the only way to obtain a kernel handle"
  },
  {
    "id": 6788,
    "content": "CUDA Runtime API  CUDA Runtime API manages module management automatically, so we recommend simply using cudaFuncGetAttributes() to reference the kernel"
  },
  {
    "id": 6792,
    "content": "Querying whether Lazy Loading is turned on  In order to check whether user enabled Lazy Loading, CUresult cuModuleGetLoadingMode ( CUmoduleLoadingMode* mode ) can be used"
  },
  {
    "id": 6793,
    "content": "#include \"cuda h\" #include \"assert h\" #include \"iostream\" int main () { CUmoduleLoadingMode mode ; assert ( CUDA_SUCCESS == cuInit ( 0 )); assert ( CUDA_SUCCESS == cuModuleGetLoadingMode ( & mode )); std :: cout << \"CUDA Module Loading Mode is \" << (( mode == CU_MODULE_LAZY_LOADING )"
  },
  {
    "id": 6796,
    "content": "Possible issues when adopting lazy loading  Lazy Loading is designed so that it should not require any modifications to applications to use it"
  },
  {
    "id": 6797,
    "content": "That said, there are some caveats, especially when applications are not fully compliant with CUDA Programming Model"
  },
  {
    "id": 6801,
    "content": "Concurrent execution  Loading kernels might require context synchronization Some programs incorrectly treat the possibility of concurrent execution of kernels as a guarantee In such cases, if program assumes that two kernels will be able to execute concurrently, and one of the kernels will not return without the other kernel executing, there is a possibility of a deadlock If this loading will"
  },
  {
    "id": 6802,
    "content": "require context synchronization, then we have a deadlock: kernel A is waiting for kernel B, but loading kernel B is stuck waiting for kernel A to finish to synchronize the context Such program is an anti-pattern, but if for any reason you want to keep it you can do the following: preload all kernels that you hope to execute concurrently prior to launching them run application with"
  },
  {
    "id": 6803,
    "content": "CUDA_MODULE_DATA_LOADING=EAGER to force loading data eagerly without forcing each function to load eagerly 20"
  },
  {
    "id": 6806,
    "content": "Allocators  Lazy Loading delays loading code from initialization phase of the program closer to execution phase"
  },
  {
    "id": 6807,
    "content": "to use it for its own allocator, then it might turn out that there will be no more memory left to load the kernels"
  },
  {
    "id": 6808,
    "content": "CUDA will need to allocate some memory to load each kernel, which usually happens at first launch time of each kernel If your application allocator greedily allocated everything, CUDA will fail to allocate memory Possible solutions: use cudaMallocAsync() instead of an allocator that allocates the entire VRAM on startup add some buffer to compensate for the delayed loading of kernels preload all"
  },
  {
    "id": 6812,
    "content": "Autotuning  Some applications launch several kernels implementing the same functionality to determine which one is the fastest"
  },
  {
    "id": 6813,
    "content": "While it is overall advisable to run at least one warmup iteration, it becomes especially important with Lazy Loading Possible solutions: do at least one warmup interaction prior to measurement preload the benchmarked kernel prior to launching it 21"
  },
  {
    "id": 6814,
    "content": "Extended GPU Memory  The Extended GPU Memory (EGM) feature, utilizing the high-bandwidth NVLink-C2C, facilitates efficient access to all system memory by GPUs, in a single-node system EGM applies to integrated CPU-GPU NVIDIA systems by allowing physical memory allocation that can be accessed from any GPU thread within the setup EGM ensures that all GPUs can access its resources at the speed of"
  },
  {
    "id": 6815,
    "content": "either GPU-GPU NVLink or NVLink-C2C With EGM, GPU threads gain the capability to access all available memory resources, including CPU attached memory and HBM3, over the NVSwitch fabric"
  },
  {
    "id": 6818,
    "content": "Preliminaries  Before diving into API changes for EGM functionalities, we are going to cover currently supported topologies, identifier assignment, prerequisites for virtual memory management, and CUDA types for EGM"
  },
  {
    "id": 6822,
    "content": "EGM Platforms: System topology  Currently, EGM can be enabled in three platforms: (1) Single-Node, Single-GPU : Consists of an Arm-based CPU, CPU attached memory, and a GPU (2) Single-Node, Multi-GPU : Consists of fully connected four single-node, single-GPU platforms Note Using cgroups to limit available devices will block routing over EGM and cause performance issues"
  },
  {
    "id": 6827,
    "content": " NUMA (Non-Uniform Memory Access) is a memory architecture used in multi-processor computer systems such that the memory is divided into multiple nodes In such a system, NUMA divides the system into nodes and assigns a unique identifier ( numaID ) to every node Note that, this identifier is different from the ordinal of a device and it is associated with the closest host node In addition to the"
  },
  {
    "id": 6828,
    "content": "existing methods, the user can obtain the identifier of the host node ( numaID ) by calling cuDeviceGetAttribute with CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID attribute type as follows: int numaId ; cuDeviceGetAttribute ( & numaId , CU_DEVICE_ATTRIBUTE_HOST_NUMA_ID , deviceOrdinal ); 21"
  },
  {
    "id": 6832,
    "content": "Currently, cuMemCreate and cudaMemPoolCreate allocators are supported with appropriate location type and NUMA identifiers"
  },
  {
    "id": 6836,
    "content": "Memory management extensions to current APIs  Currently, EGM memory can be mapped with Virtual Memory ( cuMemCreate ) or Stream Ordered Memory ( cudaMemPoolCreate ) allocators The user is responsible for allocating physical memory and mapping it to a virtual memory address space on all sockets"
  },
  {
    "id": 6837,
    "content": "Therefore we encourage the reader to see Chapter 3 Note We encourage readers to read CUDA Programming Guide’s Chapter 10 and Chapter 11 for a better understanding"
  },
  {
    "id": 6838,
    "content": "New CUDA property types have been added to APIs for allowing those approaches to understand allocation locations using NUMA-like node identifiers: CUDA Type Used with CU_MEM_LOCATION_TYPE_HOST_NUMA CUmemAllocationProp for cuMemCreate cudaMemLocationTypeHostNuma cudaMemPoolProps for cudaMemPoolCreate Note Please see CUDA Driver API and CUDA Runtime Data Types to find more about NUMA specific CUDA"
  },
  {
    "id": 6845,
    "content": "Single-Node, Single-GPU  Any of the existing CUDA host allocators as well as system allocated memory can be used to benefit from high-bandwidth C2C Note Refer to the tuning guide for more information about memory allocators and page sizes"
  },
  {
    "id": 6849,
    "content": "Single-Node, Multi-GPU  In a multi-GPU system, the user has to provide host information for the placement"
  },
  {
    "id": 6850,
    "content": "As we mentioned, a natural way to express that information would be by using NUMA node IDs and EGM follows this approach Therefore, using the cuDeviceGetAttribute function the user should be able to learn the closest NUMA node id"
  },
  {
    "id": 6851,
    "content": "Then the user can allocate and manage EGM memory using VMM (Virtual Memory Management) API or CUDA Memory Pool"
  },
  {
    "id": 6856,
    "content": "Using VMM APIs  The first step in memory allocation using Virtual Memory Management APIs is to create a physical memory chunk that will provide a backing for the allocation"
  },
  {
    "id": 6857,
    "content": "In EGM allocations the user has to explicitly provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaID as the location identifier"
  },
  {
    "id": 6858,
    "content": "The following code snippet shows allocating physical memory with cuMemCreate : CUmemAllocationProp prop {}; prop id = numaId ; size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 );"
  },
  {
    "id": 6859,
    "content": "After physical memory allocation, we have to reserve an address space and map it to a pointer These procedures do not have EGM-specific changes: CUdeviceptr dptr ; cuMemAddressReserve ( & dptr , padded_size , 0 , 0 , 0 ); cuMemMap ( dptr , padded_size , 0 , allocHandle , 0 ); Finally, the user has to explicitly protect mapped virtual address ranges"
  },
  {
    "id": 6860,
    "content": "Similar to the memory allocation, the user has to provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaId as the location identifier"
  },
  {
    "id": 6861,
    "content": "Following code snippet create an access descriptors for the host node and the GPU to give read and write access for the mapped memory to both of them: CUmemAccessDesc accessDesc [ 2 ]{{}}; accessDesc [ 0 ] flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; cuMemSetAccess ( dptr , size , accessDesc , 2 ); 21"
  },
  {
    "id": 6865,
    "content": "Using CUDA Memory Pool  To define EGM, the user can create a memory pool on a node and give access to peers"
  },
  {
    "id": 6866,
    "content": "In this case, the user has to explicitly define cudaMemLocationTypeHostNuma as the location type and numaId as the location identifier"
  },
  {
    "id": 6867,
    "content": "The following code snippet shows creating a memory pool cudaMemPoolCreate : cudaSetDevice ( homeDevice ); cudaMemPoolProps props {}; props id = numaId ; cudaMemPoolCreate ( & memPool , & props ); Additionally, for direct connect peer access, it is also possible to use the existing peer access API, cudaMemPoolSetAccess"
  },
  {
    "id": 6868,
    "content": "An example for an accessingDevice is shown in the following code snippet: cudaMemAccessDesc desc {}; desc id = accessingDevice ; cudaMemPoolSetAccess ( memPool , & desc , 1 ); When the memory pool is created, and accesses are given, the user can set created memory pool to the residentDevice and start allocating memory using cudaMallocAsync : cudaDeviceSetMemPool ( residentDevice , memPool );"
  },
  {
    "id": 6874,
    "content": "Multi-Node, Single-GPU  Beyond memory allocation, remote peer access does not have EGM-specific modification and it follows CUDA inter process (IPC) protocol"
  },
  {
    "id": 6875,
    "content": "The user should allocate memory using cuMemCreate and again the user has to explicitly provide CU_MEM_LOCATION_TYPE_HOST_NUMA as the location type and numaID as the location identifier"
  },
  {
    "id": 6876,
    "content": "The following code snippet shows allocating physical memory on Node A: CUmemAllocationProp prop {}; prop"
  },
  {
    "id": 6877,
    "content": "id = numaId ; size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); size_t page_size ="
  },
  {
    "id": 6878,
    "content": "; assert ( padded_size % page_size == 0 ); CUmemGenericAllocationHandle allocHandle ; cuMemCreate ( & allocHandle , padded_size , & prop , 0 ); After creating allocation handle using cuMemCreate the user can export that handle to the other node, Node B, calling cuMemExportToShareableHandle : cuMemExportToShareableHandle ( & fabricHandle , allocHandle , CU_MEM_HANDLE_TYPE_FABRIC , 0 ); At this"
  },
  {
    "id": 6879,
    "content": "point, fabricHandle should be sent to Node B via TCP/IP On Node B, the handle can be imported using cuMemImportFromShareableHandle and treated as any other fabric handle At this point, fabricHandle should be received from Node A via TCP/IP CUmemGenericAllocationHandle allocHandle ; cuMemImportFromShareableHandle ( & allocHandle , & fabricHandle , CU_MEM_HANDLE_TYPE_FABRIC ); When handle is"
  },
  {
    "id": 6880,
    "content": "imported at Node B, then the user can reserve an address space and map it locally in a regular fashion: size_t granularity = 0 ; cuMemGetAllocationGranularity ( & granularity , & prop , MEM_ALLOC_GRANULARITY_MINIMUM ); size_t padded_size = ROUND_UP ( size , granularity ); size_t page_size = ; assert ( padded_size % page_size == 0 ); CUdeviceptr dptr ; cuMemAddressReserve ( & dptr , padded_size , 0"
  },
  {
    "id": 6881,
    "content": ", 0 , 0 ); cuMemMap ( dptr , padded_size , 0 , allocHandle , 0 ); As the final step, the user should give appropriate accesses to each of the local GPUs at Node B An example code snippet that gives read and write access to eight local GPUs: Give all 8 local GPUS access to exported EGM memory located on Node A flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE ; } cuMemSetAccess ( dptr , size , accessDesc"
  },
  {
    "id": 6883,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 6884,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 6886,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 6887,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 6888,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 6889,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 6890,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 6891,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 6892,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 6893,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 6894,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 6895,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 6896,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 6903,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 6905,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 6906,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 6909,
    "content": "5 | PDF | Archive CUDA C++ Best Practices Guide The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs  This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIA ® CUDA ® GPUs"
  },
  {
    "id": 6910,
    "content": "It presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures"
  },
  {
    "id": 6911,
    "content": "While the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored"
  },
  {
    "id": 6913,
    "content": "This approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later  The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code This guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are"
  },
  {
    "id": 6916,
    "content": "com/cuda/ The following documents are especially important resources: CUDA Installation Guide CUDA C++ Programming Guide CUDA Toolkit Reference Manual In particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit (if not, please refer to the relevant CUDA Installation Guide for your platform) and that you have a basic"
  },
  {
    "id": 6917,
    "content": "familiarity with the CUDA C++ programming language and environment (if not, please refer to the CUDA C++ Programming Guide)"
  },
  {
    "id": 6918,
    "content": "Assess, Parallelize, Optimize, Deploy  This guide introduces the Assess, Parallelize, Optimize, Deploy(APOD) design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible"
  },
  {
    "id": 6919,
    "content": "APOD is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production"
  },
  {
    "id": 6923,
    "content": "Assess  For an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time"
  },
  {
    "id": 6924,
    "content": "Armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate GPU acceleration By understanding the end-user’s requirements and constraints and by applying Amdahl’s and Gustafson’s laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application"
  },
  {
    "id": 6928,
    "content": "Parallelize  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code"
  },
  {
    "id": 6929,
    "content": "Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as cuBLAS , cuFFT , or Thrust , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler"
  },
  {
    "id": 6930,
    "content": "On the other hand, some applications’ designs will require some amount of refactoring to expose their inherent parallelism As even CPU architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc ) aims to make the expression of this parallelism as"
  },
  {
    "id": 6931,
    "content": "simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput"
  },
  {
    "id": 6935,
    "content": "Optimize  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance"
  },
  {
    "id": 6936,
    "content": "Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not"
  },
  {
    "id": 6937,
    "content": "necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups"
  },
  {
    "id": 6938,
    "content": "Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences"
  },
  {
    "id": 6939,
    "content": "The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developer’s optimization efforts and provide references into the relevant portions of the optimization section of this guide"
  },
  {
    "id": 6943,
    "content": "Deploy  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation"
  },
  {
    "id": 6944,
    "content": "Recall that the initial assess step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production This is important for a number of reasons; for example, it allows the"
  },
  {
    "id": 6945,
    "content": "user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application"
  },
  {
    "id": 6948,
    "content": "Recommendations and Best Practices  Throughout this guide, specific recommendations are made regarding the design and implementation of CUDA C++ code These recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope"
  },
  {
    "id": 6949,
    "content": "Actions that present substantial improvements for most CUDA applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority Before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied"
  },
  {
    "id": 6950,
    "content": "This approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization"
  },
  {
    "id": 6951,
    "content": "The criteria of benefit and scope for establishing priority will vary depending on the nature of the program"
  },
  {
    "id": 6952,
    "content": "Regardless of this possibility, it is good practice to verify that no higher-priority recommendations have been overlooked before undertaking lower-priority items"
  },
  {
    "id": 6953,
    "content": "Production code should, however, systematically check the error code returned by each API call and check for failures in kernel launches by calling cudaGetLastError()"
  },
  {
    "id": 6956,
    "content": "Assessing Your Application  From supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance"
  },
  {
    "id": 6957,
    "content": "The core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network"
  },
  {
    "id": 6958,
    "content": "As a result, all modern processors require parallel code in order to achieve good utilization of their computational power"
  },
  {
    "id": 6959,
    "content": "While processors are evolving to expose more fine-grained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarse-grained parallel codes (for example, where the data is decomposed into regions processed in parallel, with sub-regions shared using MPI)"
  },
  {
    "id": 6960,
    "content": "In order to profit from any modern processor architecture, GPUs included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future"
  },
  {
    "id": 6962,
    "content": "Heterogeneous Computing  CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU devices"
  },
  {
    "id": 6963,
    "content": "While NVIDIA GPUs are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel"
  },
  {
    "id": 6965,
    "content": "However, the device is based on a distinctly different design from the host system, and it’s important to understand those differences and how they determine the performance of CUDA applications in order to use CUDA effectively"
  },
  {
    "id": 6968,
    "content": "Differences between Host and Device  The primary differences are in threading model and in separate physical memories: Threading resources Execution pipelines on host systems can support a limited number of concurrent threads"
  },
  {
    "id": 6969,
    "content": "For example, servers that have two 32 core processors can run only 64 threads concurrently (or small multiple of that if the CPUs support simultaneous multithreading)"
  },
  {
    "id": 6970,
    "content": "By comparison, the smallest executable unit of parallelism on a CUDA device comprises 32 threads (termed a warp of threads)"
  },
  {
    "id": 6971,
    "content": "Modern NVIDIA GPUs can support up to 2048 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C++ Programming Guide) On GPUs with 80 multiprocessors, this leads to more than 160,000 concurrently active threads"
  },
  {
    "id": 6972,
    "content": "The operating system must swap threads on and off CPU execution channels to provide multithreading capability"
  },
  {
    "id": 6973,
    "content": "In a typical system, thousands of threads are queued up for work (in warps of 32 threads each) Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads In short, CPU cores are designed to minimize latency for a small number of threads at a time each, whereas GPUs are designed to handle a large number of"
  },
  {
    "id": 6975,
    "content": "RAM The host system and the device each have their own distinct attached physical memories 1 As the host and device memories are separated, items in the host memory must occasionally be communicated between device memory and host memory as described in What Runs on a CUDA-Enabled Device"
  },
  {
    "id": 6976,
    "content": "These are the primary hardware differences between CPU hosts and GPU devices with respect to parallel programming Applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device"
  },
  {
    "id": 6979,
    "content": " The following issues should be considered when determining what parts of an application to run on the device: The device is ideally suited for computations that can be run on numerous data elements simultaneously in parallel"
  },
  {
    "id": 6980,
    "content": "This typically involves arithmetic on large data sets (such as matrices) where the same operation can be performed across thousands, if not millions, of elements at the same time This is a requirement for good performance on CUDA: the software must use a large number (generally thousands or tens of thousands) of concurrent threads"
  },
  {
    "id": 6981,
    "content": "The support for running numerous threads in parallel derives from CUDA’s use of a lightweight threading model described above"
  },
  {
    "id": 6983,
    "content": ") This cost has several ramifications: The complexity of operations should justify the cost of moving data to and from the device"
  },
  {
    "id": 6984,
    "content": "Code that transfers data for brief use by a small number of threads will see little or no performance benefit"
  },
  {
    "id": 6985,
    "content": "For example, transferring two matrices to the device to perform a matrix addition and then transferring the results back to the host will not realize much performance benefit"
  },
  {
    "id": 6986,
    "content": "For the preceding procedure, assuming matrices of size NxN, there are N 2 operations (additions) and 3N 2 elements transferred, so the ratio of operations to elements transferred is 1:3 or O(1) For example, a matrix multiplication of the same matrices requires N 3 operations (multiply-add), so the ratio of operations to elements transferred is O(N), in which case the larger the matrix the greater"
  },
  {
    "id": 6987,
    "content": "the performance benefit The types of operations are an additional factor, as additions have different complexity profiles than, for example, trigonometric functions It is important to include the overhead of transferring data to and from the device in determining whether operations should be performed on the host or on the device Because transfers should be minimized, programs that run multiple"
  },
  {
    "id": 6988,
    "content": "kernels on the same data should favor leaving the data on the device between kernel calls, rather than transferring intermediate results to the host and then sending them back to the device for subsequent calculations So, in the previous example, had the two matrices to be added already been on the device as a result of some previous calculation, or if the results of the addition would be used in"
  },
  {
    "id": 6989,
    "content": "some subsequent calculation, the matrix addition should be performed locally on the device This approach should be used even if one of the steps in a sequence of calculations could be performed faster on the host Even a relatively slow kernel may be advantageous if it avoids one or more transfers between host and device memory Data Transfer Between Host and Device provides further details,"
  },
  {
    "id": 6990,
    "content": "including the measurements of bandwidth between the host and the device versus within the device proper For best performance, there should be some coherence in memory access by adjacent threads running on the device"
  },
  {
    "id": 6991,
    "content": "Certain memory access patterns enable the hardware to coalesce groups of reads or writes of multiple data items into one operation"
  },
  {
    "id": 6992,
    "content": "Data that cannot be laid out so as to enable coalescing , or that doesn’t have enough locality to use the L1 or texture caches effectively, will tend to see lesser speedups when used in computations on GPUs"
  },
  {
    "id": 6993,
    "content": "In general, they should be avoided, because compared to peak capabilities any architecture processes these memory access patterns at a low efficiency However, compared to cache based architectures, like CPUs, latency hiding architectures, like GPUs, tend to cope better with completely random memory access patterns"
  },
  {
    "id": 6994,
    "content": "1 On Systems on a Chip with integrated GPUs, such as NVIDIA® Tegra®, host and device memory are physically the same, but there is still a logical distinction between host and device memory"
  },
  {
    "id": 6995,
    "content": "Profile  Many codes accomplish a significant portion of the work with a relatively small amount of code"
  },
  {
    "id": 6996,
    "content": "Using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization"
  },
  {
    "id": 7000,
    "content": "Creating the Profile  There are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time"
  },
  {
    "id": 7001,
    "content": "Note High Priority: To maximize developer productivity, profile the application to determine hotspots and bottlenecks"
  },
  {
    "id": 7002,
    "content": "The most important consideration with any profiling activity is to ensure that the workload is realistic - i"
  },
  {
    "id": 7004,
    "content": ", that information gained from the test and decisions based upon that information are relevant to real data"
  },
  {
    "id": 7005,
    "content": "Using unrealistic workloads can lead to sub-optimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions"
  },
  {
    "id": 7006,
    "content": "The following example is based on gprof , which is an open-source profiler for Linux platforms from the GNU Binutils collection"
  },
  {
    "id": 7055,
    "content": "Identifying Hotspots  In the example above, we can clearly see that the function genTimeStep() takes one-third of the total running time of the application"
  },
  {
    "id": 7057,
    "content": "It is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such as calcStats() and calcSummaryData()"
  },
  {
    "id": 7058,
    "content": "However, since APOD is a cyclical process, we might opt to parallelize these functions in a subsequent APOD pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes"
  },
  {
    "id": 7062,
    "content": "Understanding Scaling  The amount of performance benefit an application will realize by running on CUDA depends entirely on the extent to which it can be parallelized Code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device"
  },
  {
    "id": 7063,
    "content": "Note High Priority: To get the maximum benefit from CUDA, focus first on finding ways to parallelize sequential code"
  },
  {
    "id": 7064,
    "content": "By understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy"
  },
  {
    "id": 7065,
    "content": "Strong Scaling and Amdahl’s Law describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size Weak Scaling and Gustafson’s Law describes weak scaling, where the speedup is attained by growing the problem size In many applications, a combination of strong and weak scaling is desirable"
  },
  {
    "id": 7070,
    "content": "Strong Scaling and Amdahl’s Law  Strong scaling is a measure of how, for a fixed overall problem size, the time to solution decreases as more processors are added to a system An application that exhibits linear strong scaling has a speedup equal to the number of processors used Strong scaling is usually equated with Amdahl’s Law, which specifies the maximum speedup that can be expected by"
  },
  {
    "id": 7071,
    "content": "parallelizing portions of a serial program Essentially, it states that the maximum speedup S of a program is: \\(S = \\frac{1}{(1 - P) + \\frac{P}{N}}\\) Here P is the fraction of the total serial execution time taken by the portion of code that can be parallelized and N is the number of processors over which the parallel portion of the code runs The larger N is(that is, the greater the number of"
  },
  {
    "id": 7072,
    "content": "processors), the smaller the P/N fraction It can be simpler to view N as a very large number, which essentially transforms the equation into \\(S = 1/(1 - P)\\) Now, if 3/4 of the running time of a sequential program is parallelized, the maximum speedup over serial code is 1 / (1 - 3/4) = 4 In reality, most applications do not exhibit perfectly linear strong scaling, even if they do exhibit some"
  },
  {
    "id": 7073,
    "content": "degree of strong scaling For most purposes, the key point is that the larger the parallelizable portion P is, the greater the potential speedup Conversely, if P is a small number (meaning that the application is not substantially parallelizable), increasing the number of processors N does little to improve performance Therefore, to get the largest speedup for a fixed problem size, it is worthwhile"
  },
  {
    "id": 7079,
    "content": "Weak Scaling and Gustafson’s Law  Weak scaling is a measure of how the time to solution changes as more processors are added to a system with a fixed problem size per processor ; i"
  },
  {
    "id": 7081,
    "content": ", where the overall problem size increases as the number of processors is increased Weak scaling is often equated with Gustafson’s Law, which states that in practice, the problem size scales with the number of processors Because of this, the maximum speedup S of a program is: \\(S = N + (1 - P)(1 - N)\\) Here P is the fraction of the total serial execution time taken by the portion of code that can"
  },
  {
    "id": 7082,
    "content": "be parallelized and N is the number of processors over which the parallel portion of the code runs Another way of looking at Gustafson’s Law is that it is not the problem size that remains constant as we scale up the system but rather the execution time Note that Gustafson’s Law assumes that the ratio of serial to parallel execution remains constant, reflecting additional cost in setting up and"
  },
  {
    "id": 7088,
    "content": "Applying Strong and Weak Scaling  Understanding which type of scaling is most applicable to an application is an important part of estimating speedup For some applications the problem size will remain constant and hence only strong scaling is applicable"
  },
  {
    "id": 7089,
    "content": "An example would be modeling how two molecules interact with each other, where the molecule sizes are fixed"
  },
  {
    "id": 7090,
    "content": "Examples include modeling fluids or structures as meshes or grids and some Monte Carlo simulations, where increasing the problem size provides increased accuracy"
  },
  {
    "id": 7091,
    "content": "Having understood the application profile, the developer should understand how the problem size would change if the computational performance changes and then apply either Amdahl’s or Gustafson’s Law to determine an upper bound for the speedup"
  },
  {
    "id": 7093,
    "content": "Parallelizing Your Application  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code"
  },
  {
    "id": 7094,
    "content": "As even CPU architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc"
  },
  {
    "id": 7096,
    "content": "While the details of how to apply these strategies to a particular application is a complex and problem-specific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore CPUs or for use on CUDA GPUs"
  },
  {
    "id": 7099,
    "content": "Parallel Libraries  The most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf"
  },
  {
    "id": 7100,
    "content": "The CUDA Toolkit includes a number of such libraries that have been fine-tuned for NVIDIA CUDA GPUs, such as cuBLAS , cuFFT , and so on"
  },
  {
    "id": 7101,
    "content": "The key here is that libraries are most useful when they match well with the needs of the application"
  },
  {
    "id": 7102,
    "content": "Applications already using other BLAS libraries can often quite easily switch to cuBLAS , for example, whereas applications that do little to no linear algebra will have little use for cuBLAS"
  },
  {
    "id": 7103,
    "content": "The same goes for other CUDA Toolkit libraries: cuFFT has an interface similar to that of FFTW , etc"
  },
  {
    "id": 7104,
    "content": "Also of note is the Thrust library, which is a parallel C++ template library similar to the C++ Standard Template Library Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code"
  },
  {
    "id": 7105,
    "content": "By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically"
  },
  {
    "id": 7106,
    "content": "As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial"
  },
  {
    "id": 7109,
    "content": "Parallelizing Compilers  Another common approach to parallelization of sequential codes is to make use of parallelizing compilers"
  },
  {
    "id": 7110,
    "content": "Often this means the use of directives-based approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself By exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel"
  },
  {
    "id": 7111,
    "content": "architecture The OpenACC standard provides a set of compiler directives to specify loops and regions of code in standard C, C++ and Fortran that should be offloaded from a host CPU to an attached accelerator such as a CUDA GPU The details of managing the accelerator device are handled implicitly by an OpenACC-enabled compiler and runtime"
  },
  {
    "id": 7117,
    "content": "Coding to Expose Parallelism  For applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as CUDA C++ that integrate seamlessly with existing sequential code are essential Once we have located a hotspot in our application’s profile assessment and determined that custom code"
  },
  {
    "id": 7118,
    "content": "is the best approach, we can use CUDA C++ to expose the parallelism in that portion of our code as a CUDA kernel We can then launch this kernel onto the GPU and retrieve the results without requiring major rewrites to the rest of our application This approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the"
  },
  {
    "id": 7122,
    "content": ", applications where the time spent is spread out relatively evenly across a wide portion of the code base"
  },
  {
    "id": 7123,
    "content": "For the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, CPU and GPU alike, so it is well worth the effort should it become necessary"
  },
  {
    "id": 7125,
    "content": "Getting the Right Answer  Obtaining the right answer is clearly the principal goal of all computation"
  },
  {
    "id": 7126,
    "content": "On parallel systems, it is possible to run into difficulties not typically found in traditional serial-oriented programming"
  },
  {
    "id": 7127,
    "content": "These include threading issues, unexpected values due to the way floating-point values are computed, and challenges arising from differences in the way CPU and GPU processors operate"
  },
  {
    "id": 7128,
    "content": "This chapter examines issues that can affect the correctness of returned data and points to appropriate solutions"
  },
  {
    "id": 7134,
    "content": "Reference Comparison  A key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous known-good reference outputs from representative inputs can be compared to new results"
  },
  {
    "id": 7135,
    "content": "After each change is made, ensure that the results match using whatever criteria apply to the particular algorithm"
  },
  {
    "id": 7136,
    "content": "Some will expect bitwise identical results, which is not always possible, especially where floating-point arithmetic is concerned; see Numerical Accuracy and Precision regarding numerical accuracy"
  },
  {
    "id": 7137,
    "content": "For other algorithms, implementations may be considered correct if they match the reference within some small epsilon"
  },
  {
    "id": 7138,
    "content": "Note that the process used for validating numerical results can easily be extended to validate performance results as well"
  },
  {
    "id": 7139,
    "content": "We want to ensure that each change we make is correct and that it improves performance (and by how much)"
  },
  {
    "id": 7140,
    "content": "Checking these things frequently as an integral part of our cyclical APOD process will help ensure that we achieve the desired results as rapidly as possible"
  },
  {
    "id": 7144,
    "content": "Unit Testing  A useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level"
  },
  {
    "id": 7145,
    "content": "For example, we can write our CUDA kernels as a collection of many short __device__ functions rather than one large monolithic __global__ function; each device function can be tested independently before hooking them all together"
  },
  {
    "id": 7146,
    "content": "For example, many kernels have complex addressing logic for accessing memory in addition to their actual computation If we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts (Note that the CUDA compiler considers any device code that does not contribute to a write to global memory as dead code subject to"
  },
  {
    "id": 7147,
    "content": "elimination, so we must at least write something out to global memory as a result of our addressing logic in order to successfully apply this strategy"
  },
  {
    "id": 7148,
    "content": ") Going a step further, if most functions are defined as __host__ __device__ rather than just __device__ functions, then these functions can be tested on both the CPU and the GPU, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results If there are differences, then those differences will be seen early and can be"
  },
  {
    "id": 7149,
    "content": "understood in the context of a simple function As a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both CPU and GPU execution paths in our application: if the bulk of the work of our CUDA kernels is done in __host__ __device__ functions, we can easily call those functions from both the host code and the device code without duplication"
  },
  {
    "id": 7152,
    "content": "Debugging  CUDA-GDB is a port of the GNU Debugger that runs on Linux and Mac; see: https: developer"
  },
  {
    "id": 7155,
    "content": "The NVIDIA Nsight Visual Studio Edition for Microsoft Windows 7, Windows HPC Server 2008, Windows 8 1, and Windows 10 is available as a free plugin for Microsoft Visual Studio; see: https: developer"
  },
  {
    "id": 7163,
    "content": "Numerical Accuracy and Precision  Incorrect or unexpected results arise principally from issues of floating-point accuracy due to the way floating-point values are computed and stored Other peculiarities of floating-point arithmetic are presented in Features and Technical Specifications of the CUDA C++ Programming Guide as well as in a whitepaper and accompanying webinar on floating-point"
  },
  {
    "id": 7169,
    "content": "Double Precision  Devices of compute capability 1 3 and higher provide native support for double-precision floating-point values (that is, values 64 bits wide) Results obtained using double-precision arithmetic will frequently differ from the same operation performed via single-precision arithmetic due to the greater precision of the former and due to rounding issues Therefore, it is important"
  },
  {
    "id": 7170,
    "content": "to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact"
  },
  {
    "id": 7174,
    "content": "Floating Point Math Is not Associative  Each floating-point arithmetic operation involves a certain amount of rounding If A, B, and C are floating-point values, (A+B)+C is not guaranteed to equal A+(B+C) as it is in symbolic math"
  },
  {
    "id": 7175,
    "content": "When you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results"
  },
  {
    "id": 7176,
    "content": "This limitation is not specific to CUDA, but an inherent part of parallel computation on floating-point values"
  },
  {
    "id": 7180,
    "content": "IEEE 754 Compliance  All CUDA compute devices follow the IEEE 754 standard for binary floating-point representation, with some small exceptions These exceptions, which are detailed in Features and Technical Specifications of the CUDA C++ Programming Guide, can lead to results that differ from IEEE 754 values computed on the host system"
  },
  {
    "id": 7181,
    "content": "One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution"
  },
  {
    "id": 7186,
    "content": "x86 80-bit Computations  x86 processors can use an 80-bit double extended precision math when performing floating-point calculations The results of these calculations can frequently differ from pure 64-bit operations performed on the CUDA device To get a closer match between values, set the x86 host processor to use regular double or single precision (64 bits and 32 bits, respectively) This is"
  },
  {
    "id": 7189,
    "content": "Optimizing CUDA Applications  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance"
  },
  {
    "id": 7191,
    "content": "Performance Metrics  When attempting to optimize CUDA code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement This chapter discusses how to correctly measure performance using CPU timers and CUDA events It then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses"
  },
  {
    "id": 7199,
    "content": "Using CPU Timers  Any CPU timer can be used to measure the elapsed time of a CUDA call or kernel execution"
  },
  {
    "id": 7200,
    "content": "The details of various CPU timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide"
  },
  {
    "id": 7201,
    "content": "When using CPU timers, it is critical to remember that many CUDA API functions are asynchronous; that is, they return control back to the calling CPU thread prior to completing their work All kernel launches are asynchronous, as are memory-copy functions with the Async suffix on their names Therefore, to accurately measure the elapsed time for a particular call or sequence of CUDA calls, it is"
  },
  {
    "id": 7202,
    "content": "necessary to synchronize the CPU thread with the GPU by calling cudaDeviceSynchronize() immediately before starting and stopping the CPU timer cudaDeviceSynchronize() blocks the calling CPU thread until all CUDA calls previously issued by the thread are completed Although it is also possible to synchronize the CPU thread with a particular stream or event on the GPU, these synchronization functions"
  },
  {
    "id": 7203,
    "content": "are not suitable for timing code in streams other than the default stream cudaStreamSynchronize() blocks the CPU thread until all CUDA calls previously issued into the given stream have completed cudaEventSynchronize() blocks until a given event in a particular stream has been recorded by the GPU Because the driver may interleave execution of CUDA calls from other non-default streams, calls in"
  },
  {
    "id": 7204,
    "content": "other streams may be included in the timing Because the default stream, stream 0, exhibits serializing behavior for work on the device (an operation in the default stream can begin only after all preceding calls in any stream have completed; and no subsequent operation in any stream can begin until it finishes), these functions can be used reliably for timing in the default stream"
  },
  {
    "id": 7205,
    "content": "Be aware that CPU-to-GPU synchronization points such as those mentioned in this section imply a stall in the GPU’s processing pipeline and should thus be used sparingly to minimize their performance impact"
  },
  {
    "id": 7209,
    "content": "Using CUDA GPU Timers  The CUDA event API provides calls that create and destroy events, record events (including a timestamp), and convert timestamp differences into a floating-point value in milliseconds"
  },
  {
    "id": 7210,
    "content": "How to time code using CUDA events cudaEvent_t start , stop ; float time ; cudaEventCreate ( & start ); cudaEventCreate ( & stop ); cudaEventRecord ( start , 0 ); kernel >> ( d_odata , d_idata , size_x , size_y , NUM_REPS ); cudaEventRecord ( stop , 0 ); cudaEventSynchronize ( stop ); cudaEventElapsedTime ( & time , start , stop ); cudaEventDestroy ( start ); cudaEventDestroy ( stop ); Here"
  },
  {
    "id": 7213,
    "content": "The cudaEventElapsedTime() function returns the time elapsed between the recording of the start and stop events"
  },
  {
    "id": 7215,
    "content": "Like the other calls in this listing, their specific operation, parameters, and return values are described in the CUDA Toolkit Reference Manual"
  },
  {
    "id": 7216,
    "content": "Note that the timings are measured on the GPU clock, so the timing resolution is operating-system-independent"
  },
  {
    "id": 7219,
    "content": "Bandwidth  Bandwidth - the rate at which data can be transferred - is one of the most important gating factors for performance As described in Memory Optimizations of this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors To measure performance accurately, it is"
  },
  {
    "id": 7220,
    "content": "useful to calculate theoretical and effective bandwidth When the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it Note High Priority: Use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits"
  },
  {
    "id": 7224,
    "content": "Theoretical Bandwidth Calculation  Theoretical bandwidth can be calculated using hardware specifications available in the product literature"
  },
  {
    "id": 7225,
    "content": "For example, the NVIDIA Tesla V100 uses HBM2 (double data rate) RAM with a memory clock rate of 877 MHz and a 4096-bit-wide memory interface Using these data items, the peak theoretical memory bandwidth of the NVIDIA Tesla V100 is 898 GB/s: \\(\\left \\times (4096/8) \\times 2 ight) \\div 10^{9} = 898\\text{GB/s}\\) In this calculation, the memory clock rate is converted in to Hz, multiplied by the"
  },
  {
    "id": 7226,
    "content": "interface width (divided by 8, to convert bits to bytes) and multiplied by 2 due to the double data rate It is important to use the same divisor when calculating theoretical and effective bandwidth so that the comparison is valid"
  },
  {
    "id": 7227,
    "content": "Note On GPUs with GDDR memory with ECC enabled the available DRAM is reduced by 6 25% to allow for the storage of ECC bits Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled, though the exact impact of ECC on bandwidth can be higher and depends on the memory access pattern HBM2 memories, on the other"
  },
  {
    "id": 7232,
    "content": "Effective Bandwidth Calculation  Effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program To do so, use this equation: \\(\\text{Effective\\ bandwidth} = \\left( {\\left( B_{r} + B_{w} ight) \\div 10^{9}} ight) \\div \\text{time}\\) Here, the effective bandwidth is in units of GB/s, B r is the number of bytes read per kernel, B w is the"
  },
  {
    "id": 7233,
    "content": "number of bytes written per kernel, and time is given in seconds For example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used: \\(\\text{Effective\\ bandwidth} = \\left( {\\left( 2048^{2} \\times 4 \\times 2 ight) \\div 10^{9}} ight) \\div \\text{time}\\) The number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by"
  },
  {
    "id": 7234,
    "content": "2 (because of the read and write), divided by 10 9 (or 1,024 3 ) to obtain GB of memory transferred This number is divided by the time in seconds to obtain GB/s"
  },
  {
    "id": 7238,
    "content": "Throughput Reported by Visual Profiler  For devices with compute capability of 2 0 or greater, the Visual Profiler can be used to collect several different memory throughput measures The following throughput metrics can be displayed in the Details or Detail Graphs view: Requested Global Load Throughput Requested Global Store Throughput Global Load Throughput Global Store Throughput DRAM Read"
  },
  {
    "id": 7239,
    "content": "Throughput DRAM Write Throughput The Requested Global Load Throughput and Requested Global Store Throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown under Effective Bandwidth Calculation Because the minimum memory transaction size is larger than most word sizes, the actual memory"
  },
  {
    "id": 7240,
    "content": "throughput required for a kernel can include the transfer of data not used by the kernel For global memory accesses, this actual throughput is reported by the Global Load Throughput and Global Store Throughput values The actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good"
  },
  {
    "id": 7241,
    "content": "estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses (see Coalesced Access to Global Memory ) For global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the Global Memory Load Efficiency and Global Memory Store Efficiency metrics"
  },
  {
    "id": 7242,
    "content": "2 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory"
  },
  {
    "id": 7246,
    "content": "This chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively"
  },
  {
    "id": 7249,
    "content": "Data Transfer Between Host and Device  The peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s on the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory and device memory (16 GB/s on the PCIe x16 Gen3) Hence, for best overall application performance, it is important to minimize data transfer between the host and the"
  },
  {
    "id": 7250,
    "content": "device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU Note High Priority: Minimize data transfer between the host and the device, even if it means running some kernels on the device that do not show performance gains when compared with running them on the host CPU Intermediate data structures should be created in device"
  },
  {
    "id": 7251,
    "content": "memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory Also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then"
  },
  {
    "id": 7252,
    "content": "unpacking after the transfer Finally, higher bandwidth between the host and the device is achieved when using page-locked (or pinned ) memory, as discussed in the CUDA C++ Programming Guide and the Pinned Memory section of this document"
  },
  {
    "id": 7256,
    "content": "Pinned Memory  Page-locked or pinned memory transfers attain the highest bandwidth between the host and the device On PCIe x16 Gen3 cards, for example, pinned memory can attain roughly 12 GB/s transfer rates"
  },
  {
    "id": 7257,
    "content": "The bandwidthTest CUDA Sample shows how to use these functions as well as how to measure memory transfer performance"
  },
  {
    "id": 7258,
    "content": "For regions of system memory that have already been pre-allocated, cudaHostRegister() can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it"
  },
  {
    "id": 7259,
    "content": "Excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance"
  },
  {
    "id": 7260,
    "content": "Furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters"
  },
  {
    "id": 7264,
    "content": "Asynchronous and Overlapping Transfers with Computation  Data transfers between the host and the device using cudaMemcpy() are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete The cudaMemcpyAsync() function is a non-blocking variant of cudaMemcpy() in which control is returned immediately to the host thread In contrast with cudaMemcpy()"
  },
  {
    "id": 7265,
    "content": ", the asynchronous transfer version requires pinned host memory (see Pinned Memory ), and it contains an additional argument, a stream ID Operations in different streams can be interleaved and in some cases overlapped - a property that can be used to hide data transfers between the host and the device Asynchronous transfers enable overlap of data transfers with computation in two different ways On"
  },
  {
    "id": 7266,
    "content": "all CUDA-enabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations For example, Overlapping computation and data transfers demonstrates how host computation in the routine cpuFunction() is performed while data is transferred to the device and a kernel using the device is executed Overlapping computation and data transfers"
  },
  {
    "id": 7267,
    "content": "cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , 0 ); kernel >> ( a_d ); cpuFunction (); The last argument to the cudaMemcpyAsync() function is the stream ID, which in this case uses the default stream, stream 0 The kernel also uses the default stream, and it will not begin execution until the memory copy completes; therefore, no explicit synchronization is needed Because the memory"
  },
  {
    "id": 7268,
    "content": "copy and the kernel both return control to the host immediately, the host function cpuFunction() overlaps their execution In Overlapping computation and data transfers , the memory copy and kernel execution occur sequentially On devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device"
  },
  {
    "id": 7269,
    "content": "Whether a device has this capability is indicated by the asyncEngineCount field of the cudaDeviceProp structure (or listed in the output of the deviceQuery CUDA Sample)"
  },
  {
    "id": 7270,
    "content": "On devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, non-default streams (streams with non-zero stream IDs) Non-default streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the"
  },
  {
    "id": 7271,
    "content": "device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished Concurrent copy and execute cudaStreamCreate ( & stream1 ); cudaStreamCreate ( & stream2 ); cudaMemcpyAsync ( a_d , a_h , size , cudaMemcpyHostToDevice , stream1 ); kernel >> ( otherData_d ); In this code, two streams are created and used in the data transfer and kernel"
  },
  {
    "id": 7272,
    "content": "executions as specified in the last arguments of the cudaMemcpyAsync call and the kernel’s execution configuration Concurrent copy and execute demonstrates how to overlap kernel execution with asynchronous data transfer"
  },
  {
    "id": 7273,
    "content": "This technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives"
  },
  {
    "id": 7274,
    "content": "The first segment shows the reference sequential implementation, which transfers and operates on an array of N floats (where N is assumed to be evenly divisible by nThreads)"
  },
  {
    "id": 7275,
    "content": "Sequential copy and execute cudaMemcpy ( a_d , a_h , N * sizeof ( float ), dir ); kernel >> ( a_d ); Staged concurrent copy and execute shows how the transfer and kernel execution can be broken up into nStreams stages Staged concurrent copy and execute size = N * sizeof ( float ) / nStreams ; for ( i = 0 ; i >> ( a_d + offset ); } (In Staged concurrent copy and execute , it is assumed that N is"
  },
  {
    "id": 7277,
    "content": ") Because execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete"
  },
  {
    "id": 7278,
    "content": "GPUs with a single copy engine can perform one asynchronous data transfer and execute kernels whereas GPUs with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels"
  },
  {
    "id": 7279,
    "content": "The number of copy engines on a GPU is given by the asyncEngineCount field of the cudaDeviceProp structure, which is also listed in the output of the deviceQuery CUDA Sample"
  },
  {
    "id": 7280,
    "content": "(It should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous CUDA calls complete It will not allow any other CUDA call to begin until it has completed"
  },
  {
    "id": 7281,
    "content": ") A diagram depicting the timeline of execution for the two code segments is shown in Figure 1 , and nStreams is equal to 4 for Staged concurrent copy and execute in the bottom half of the figure Timeline comparison for copy and kernel execution  Top Sequential Bottom Concurrent For this example, it is assumed that the data transfer and kernel execution times are comparable In such cases, and"
  },
  {
    "id": 7282,
    "content": "when the execution time ( tE ) exceeds the transfer time ( tT ), a rough estimate for the overall time is tE + tT/nStreams for the staged version versus tE + tT for the sequential version If the transfer time exceeds the execution time, a rough estimate for the overall time is tT + tE/nStreams"
  },
  {
    "id": 7290,
    "content": ", GPUs with the integrated field of the CUDA device properties structure set to 1), mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated GPU and CPU memory are physically the same Because the data is not cached on the GPU, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should"
  },
  {
    "id": 7292,
    "content": "Zero copy can be used in place of streams because kernel-originated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams"
  },
  {
    "id": 7295,
    "content": "canMapHostMemory ) exit ( 0 ); cudaSetDeviceFlags ( cudaDeviceMapHost ); cudaHostAlloc ( & a_h , nBytes , cudaHostAllocMapped ); cudaHostGetDevicePointer ( & a_map , a_h , 0 ); kernel >> ( a_map ); In this code, the canMapHostMemory field of the structure returned by cudaGetDeviceProperties() is used to check that the device supports mapping host memory to the device’s address space Page-locked"
  },
  {
    "id": 7296,
    "content": "memory mapping is enabled by calling cudaSetDeviceFlags() with cudaDeviceMapHost Note that cudaSetDeviceFlags() must be called prior to setting a device or making a CUDA call that requires state (that is, essentially, before a context is created) Page-locked mapped host memory is allocated using cudaHostAlloc() , and the pointer to the mapped device address space is obtained via the function"
  },
  {
    "id": 7297,
    "content": "cudaHostGetDevicePointer() In the code in Zero-copy host code , kernel() can reference the mapped pinned host memory using the pointer a_map in exactly the same was as it would if a_map referred to a location in device memory Note Mapped pinned host memory allows you to overlap CPU-GPU memory transfers with computation while avoiding the use of CUDA streams But since any repeated access to such"
  },
  {
    "id": 7298,
    "content": "memory areas causes repeated CPU-GPU transfers, consider creating a second area in device memory to manually cache the previously read host memory data"
  },
  {
    "id": 7302,
    "content": "Unified Virtual Addressing  Devices of compute capability 2 0 and later support a special addressing mode called Unified Virtual Addressing (UVA) on 64-bit Linux and Windows With UVA, the host memory and the device memories of all installed supported devices share a single virtual address space Prior to UVA, an application had to keep track of which pointers referred to device memory (and for"
  },
  {
    "id": 7303,
    "content": "which device) and which referred to host memory as a separate bit of metadata (or as hard-coded information in the program) for each pointer Using UVA, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer using cudaPointerGetAttributes() Under UVA, pinned host memory allocated with cudaHostAlloc() will have identical"
  },
  {
    "id": 7304,
    "content": "host and device pointers, so it is not necessary to call cudaHostGetDevicePointer() for such allocations Host memory allocations pinned after-the-fact via cudaHostRegister() , however, will continue to have different device pointers than their host pointers, so cudaHostGetDevicePointer() remains necessary in that case UVA is also a necessary precondition for enabling peer-to-peer (P2P) transfer of"
  },
  {
    "id": 7305,
    "content": "data directly across the PCIe bus or NVLink for supported GPUs in supported configurations, bypassing host memory See the CUDA C++ Programming Guide for further explanations and software requirements for UVA and P2P"
  },
  {
    "id": 7308,
    "content": "Device Memory Spaces  CUDA devices use several memory spaces, which have different characteristics that reflect their distinct usages in CUDA applications These memory spaces include global, local, shared, texture, and registers, as shown in Figure 2 Memory spaces on a CUDA device  Of these different memory spaces, global memory is the most plentiful; see Features and Technical Specifications"
  },
  {
    "id": 7309,
    "content": "of the CUDA C++ Programming Guide for the amounts of memory available in each memory space at each compute capability level Global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file Salient Features of Device Memory  Memory Location on/off chip Cached Access Scope Lifetime Register On n/a R/W 1 thread Thread Local Off"
  },
  {
    "id": 7310,
    "content": "Yes†† R/W 1 thread Thread Shared On n/a R/W All threads in block Block Global Off † R/W All threads + host Host allocation Constant Off Yes R All threads + host Host allocation Texture Off Yes R All threads + host Host allocation † Cached in L1 and L2 by default on devices of compute capability 6 0 and 7 x; cached only in L2 by default on devices of lower compute capabilities, though some allow"
  },
  {
    "id": 7311,
    "content": "opt-in to caching in L1 as well via compilation flags †† Cached in L1 and L2 by default except on devices of compute capability 5 x; devices of compute capability 5 x cache locals only in L2 In the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array Texture references that are bound to CUDA arrays can be"
  },
  {
    "id": 7312,
    "content": "written to via surface-write operations by binding a surface to the same underlying CUDA array storage) Reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are read-only and are not invalidated when the associated global memory is modified"
  },
  {
    "id": 7316,
    "content": "Coalesced Access to Global Memory  A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions The access requirements for coalescing depend on the compute capability of the device and are documented in"
  },
  {
    "id": 7317,
    "content": "the CUDA C++ Programming Guide For devices of compute capability 6 0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp For certain devices of compute capability 5 2, L1-caching of accesses to global"
  },
  {
    "id": 7318,
    "content": "memory can be optionally enabled If L1-caching is enabled on these devices, the number of required transactions is equal to the number of required 128-byte aligned segments Note On devices of compute capability 6 0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not On devices with GDDR memory, accessing memory in"
  },
  {
    "id": 7319,
    "content": "a coalesced way is even more important when ECC is turned on Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory These examples assume compute capability 6 0 or higher and that accesses are for 4-byte words, unless otherwise noted"
  },
  {
    "id": 7324,
    "content": "A Simple Access Pattern  The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6"
  },
  {
    "id": 7328,
    "content": ", adjacent float values), four coalesced 32-byte transactions will service that memory access Coalesced access  This access pattern results in four 32-byte transactions, indicated by the red rectangles"
  },
  {
    "id": 7329,
    "content": "if several threads had accessed the same word or if some threads did not participate in the access), the full segment is fetched anyway Furthermore, if accesses by the threads of the warp had been permuted within or accross the four segments, still only four 32-byte transactions would have been performed by a device with compute capability 6"
  },
  {
    "id": 7335,
    "content": "A Sequential but Misaligned Access Pattern  If sequential threads in a warp access memory that is sequential but not aligned with a 32-byte segment, five 32-byte segments will be requested, as shown in Figure 4 Misaligned sequential addresses that fall within five 32-byte segments  Memory allocated through the CUDA Runtime API, such as via cudaMalloc() , is guaranteed to be aligned to at least"
  },
  {
    "id": 7340,
    "content": "(Consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example"
  },
  {
    "id": 7345,
    "content": "Effects of Misaligned Accesses  It is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one in A copy kernel that illustrates misaligned accesses A copy kernel that illustrates misaligned accesses __global__ void offsetCopy ( float * odata , float * idata , int offset ) { int xid = blockIdx x + offset ; odata [ xid ] = idata [ xid ];"
  },
  {
    "id": 7346,
    "content": "} In A copy kernel that illustrates misaligned accesses , data is copied from the input array idata to the output array, both of which exist in global memory The kernel is executed within a loop in host code that varies the parameter offset from 0 to 32"
  },
  {
    "id": 7347,
    "content": "Figure 4 corresponds to this misalignments) The effective bandwidth for the copy with various offsets on an NVIDIA Tesla V100 ( compute capability 7"
  },
  {
    "id": 7349,
    "content": "Performance of offsetCopy kernel  For the NVIDIA Tesla V100, global memory accesses with no offset or with offsets that are multiples of 8 words result in four 32-byte transactions Otherwise, five 32-byte segments are loaded per warp, and we would expect approximately 4/5 th of the memory throughput achieved with no offsets In this particular example, the offset memory throughput achieved is,"
  },
  {
    "id": 7350,
    "content": "however, approximately 9/10 th , because adjacent warps reuse the cache lines their neighbors fetched It would have been more so if adjacent warps had not exhibited such a high degree of reuse of the over-fetched cache lines"
  },
  {
    "id": 7355,
    "content": "Strided Accesses  As seen above, in the case of misaligned sequential accesses, caches help to alleviate the performance impact It may be different with non-unit-strided accesses, however, and this is a pattern that occurs frequently when dealing with multidimensional data or matrices For this reason, ensuring that as much as possible of the data in each cache line fetched is actually used is an"
  },
  {
    "id": 7356,
    "content": "important part of performance optimization of memory accesses on these devices To illustrate the effect of strided access on effective bandwidth, see the kernel strideCopy() in A kernel to illustrate non-unit stride data copy , which copies data with a stride of stride elements between threads from idata to odata A kernel to illustrate non-unit stride data copy __global__ void strideCopy ( float *"
  },
  {
    "id": 7357,
    "content": "odata , float * idata , int stride ) { int xid = ( blockIdx x ) * stride ; odata [ xid ] = idata [ xid ]; } Figure 6 illustrates such a situation; in this case, threads within a warp access words in memory with a stride of 2"
  },
  {
    "id": 7358,
    "content": "This action leads to a load of eight L2 cache segments per warp on the Tesla V100 (compute capability 7"
  },
  {
    "id": 7360,
    "content": "Adjacent threads accessing memory with a stride of 2  A stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp, as indicated in Figure 7 Performance of strideCopy kernel "
  },
  {
    "id": 7361,
    "content": "As illustrated in Figure 7 , non-unit-stride global memory accesses should be avoided whenever possible"
  },
  {
    "id": 7367,
    "content": "0, devices of compute capability 8 0 and above have the capability to influence persistence of data in the L2 cache Because L2 cache is on-chip, it potentially provides higher bandwidth and lower latency accesses to global memory For more details refer to the L2 Access Management section in the CUDA C++ Programming Guide"
  },
  {
    "id": 7372,
    "content": "L2 Cache Access Window  When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be persisting On the other hand, if the data is only accessed once, such data accesses can be considered to be streaming A portion of the L2 cache can be set aside for persistent accesses to a data region in global memory If this set-aside portion is not used"
  },
  {
    "id": 7373,
    "content": "by persistent accesses, then streaming or normal data accesses can use it The L2 cache set-aside size for persisting accesses may be adjusted, within limits: cudaGetDeviceProperties ( & prop , device_id ); cudaDeviceSetLimit ( cudaLimitPersistingL2CacheSize , prop persistingL2CacheMaxSize ); /* Set aside max possible size of L2 cache for persisting accesses */ Mapping of user data to L2 set-aside"
  },
  {
    "id": 7377,
    "content": "0 ; Hint for L2 cache hit ratio for persisting accesses in the num_bytes region stream_attribute hitProp = cudaAccessPropertyPersisting ; Type of access property on cache hit stream_attribute Set the attributes to a CUDA stream of type cudaStream_t cudaStreamSetAttribute ( stream , cudaStreamAttributeAccessPolicyWindow , & stream_attribute ); The access policy window requires a value for hitRatio"
  },
  {
    "id": 7378,
    "content": "and num_bytes Depending on the value of the num_bytes parameter and the size of L2 cache, one may need to tune the value of hitRatio to avoid thrashing of L2 cache lines"
  },
  {
    "id": 7383,
    "content": "Tuning the Access Window Hit-Ratio  The hitRatio parameter can be used to specify the fraction of accesses that receive the hitProp property For example, if the hitRatio value is 0"
  },
  {
    "id": 7384,
    "content": "6, 60% of the memory accesses in the global memory region [ptr ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property"
  },
  {
    "id": 7386,
    "content": "First, we set aside 30 MB of the L2 cache for persisting accesses using cudaDeviceSetLimit() , as discussed above Then, as shown in the figure below, we specify that the accesses to the first freqSize * sizeof(int) bytes of the memory region are persistent In our experiment, we vary the size of this persistent data region from 10 MB to 60 MB to model various scenarios where data fits in or"
  },
  {
    "id": 7387,
    "content": "exceeds the available L2 set-aside portion of 30 MB Accesses to the remaining data of the memory region (i"
  },
  {
    "id": 7389,
    "content": ", streaming data) are considered normal or streaming accesses and will thus use the remaining 10 MB of the non set-aside L2 portion (unless part of the L2 set-aside portion is unused) Mapping Persistent data accesses to set-aside L2 in sliding window experiment  Consider the following kernel code and access window parameters, as the implementation of the sliding window experiment"
  },
  {
    "id": 7390,
    "content": "__global__ void kernel ( int * data_persistent , int * data_streaming , int dataSize , int freqSize ) { int tid = blockIdx"
  },
  {
    "id": 7391,
    "content": "x ; /*Each CUDA thread accesses one element in the persistent data section and one element in the streaming data section"
  },
  {
    "id": 7392,
    "content": "Because the size of the persistent memory region (freqSize * sizeof(int) bytes) is much smaller than the size of the streaming memory region (dataSize * sizeof(int) bytes), data in the persistent region is accessed more frequently*/ data_persistent [ tid % freqSize ] = 2 * data_persistent [ tid % freqSize ]; data_streaming [ tid % dataSize ] = 2 * data_streaming [ tid % dataSize ]; }"
  },
  {
    "id": 7393,
    "content": "stream_attribute num_bytes = freqSize * sizeof ( int ); Number of bytes for persisting accesses in range 10-60 MB stream_attribute When the persistent data region fits well into the 30 MB set-aside portion of the L2 cache, a performance increase of as much as 50% is observed However, once the size of this persistent data region exceeds the size of the L2 set-aside cache portion, approximately 10%"
  },
  {
    "id": 7395,
    "content": "The performance of the sliding-window benchmark with fixed hit-ratio of 1 0  In order to optimize the performance, when the size of the persistent data is more than the size of the set-aside L2 cache portion, we tune the num_bytes and hitRatio parameters in the access window as below"
  },
  {
    "id": 7396,
    "content": "hitRatio = ( 20 * 1024 * 1024 ) / (( float ) freqSize * sizeof ( int )); Such that up to 20MB of data is resident We fix the num_bytes in the access window to 20 MB and tune the hitRatio such that a random 20 MB of the total persistent data is resident in the L2 set-aside cache portion The remaining portion of this persistent data will be accessed using the streaming property The results are"
  },
  {
    "id": 7397,
    "content": "shown in the chart below, where we see good performance regardless of whether the persistent data fits in the L2 set-aside or not"
  },
  {
    "id": 7398,
    "content": "Shared Memory  Because it is on-chip, shared memory has much higher bandwidth and lower latency than local and global memory - provided there are no bank conflicts between the threads, as detailed in the following section"
  },
  {
    "id": 7403,
    "content": "Shared Memory and Memory Banks  To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules ( banks ) that can be accessed simultaneously Therefore, any memory load or store of n addresses that spans n distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is n times as high as the bandwidth of a single"
  },
  {
    "id": 7404,
    "content": "bank However, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized The hardware splits a memory request that has bank conflicts into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests The one exception here is when multiple threads in a warp address the"
  },
  {
    "id": 7405,
    "content": "same shared memory location, resulting in a broadcast In this case, multiple broadcasts from different banks are coalesced into a single multicast from the requested shared memory locations to the threads To minimize bank conflicts, it is important to understand how memory addresses map to memory banks and how to optimally schedule memory requests"
  },
  {
    "id": 7407,
    "content": "x or newer, each bank has a bandwidth of 32 bits every clock cycle, and successive 32-bit words are assigned to successive banks The warp size is 32 threads and the number of banks is also 32, so bank conflicts can occur between any threads in the warp"
  },
  {
    "id": 7414,
    "content": "Shared Memory in Matrix Multiplication (C=AB)  Shared memory enables cooperation between threads in a block When multiple threads in a block use the same data from global memory, shared memory can be used to access the data from global memory only once Shared memory can also be used to avoid uncoalesced memory accesses by loading and storing data in a coalesced pattern from global memory and"
  },
  {
    "id": 7415,
    "content": "then reordering it in shared memory Aside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a warp in shared memory The use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for the case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN To keep the kernels simple, M and N are multiples of 32, since"
  },
  {
    "id": 7417,
    "content": "Therefore, in terms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see Figure 11"
  },
  {
    "id": 7418,
    "content": "A grid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different tile in C from a single tile of A and a single tile of B"
  },
  {
    "id": 7419,
    "content": "Block-column matrix (A) multiplied by block-row matrix (B) with resulting product matrix (C)  To do this, the simpleMultiply kernel ( Unoptimized matrix multiplication ) calculates the output elements of a tile of matrix C Unoptimized matrix multiplication __global__ void simpleMultiply ( float * a , float * b , float * c , int N ) { int row = blockIdx x ; float sum = 0"
  },
  {
    "id": 7420,
    "content": "0f ; for ( int i = 0 ; i __global__ void pipeline_kernel_sync ( T * global , uint64_t * clock , size_t copy_count ) { extern __shared__ char s []; T * shared = reinterpret_cast ( s ); uint64_t clock_start = clock64 (); for ( size_t i = 0 ; i ( clock ), clock_end - clock_start ); } template __global__ void pipeline_kernel_async ( T * global , uint64_t * clock , size_t copy_count ) { extern"
  },
  {
    "id": 7421,
    "content": "__shared__ char s []; T * shared = reinterpret_cast ( s ); uint64_t clock_start = clock64 (); pipeline pipe; for ( size_t i = 0 ; i ( clock ), clock_end - clock_start ); } The synchronous version for the kernel loads an element from global memory to an intermediate register and then stores the intermediate register value to shared memory"
  },
  {
    "id": 7422,
    "content": "In the asynchronous version of the kernel, instructions to load from global memory and store directly into shared memory are issued as soon as __pipeline_memcpy_async() function is called"
  },
  {
    "id": 7423,
    "content": "The __pipeline_wait_prior(0) will wait until all the instructions in the pipe object have been executed"
  },
  {
    "id": 7424,
    "content": "Not using intermediate registers can help reduce register pressure and can increase kernel occupancy"
  },
  {
    "id": 7425,
    "content": "Data copied from global memory to shared memory using asynchronous copy instructions can be cached in the L1 cache or the L1 cache can be optionally bypassed If individual CUDA threads are copying elements of 16 bytes, the L1 cache can be bypassed Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memory  We evaluate the performance of both kernels using elements of size 4B,"
  },
  {
    "id": 7429,
    "content": "We adjust the copy_count in the kernels such that each thread block copies from 512 bytes up to 48 MB"
  },
  {
    "id": 7430,
    "content": "Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memory  From the performance chart, the following observations can be made for this experiment Best performance with synchronous copy is achieved when the copy_count parameter is a multiple of 4 for all three element sizes The async-copy does not require the copy_count parameter to be a multiple of 4, to"
  },
  {
    "id": 7431,
    "content": "maximize performance through compiler optimizations Overall, best performance is achieved when using asynchronous copies with an element of size 8 or 16 bytes"
  },
  {
    "id": 7435,
    "content": "Local Memory  Local memory is so named because its scope is local to the thread, not because of its physical location"
  },
  {
    "id": 7436,
    "content": "This is done by the nvcc compiler when it determines that there is insufficient register space to hold the variable Automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically"
  },
  {
    "id": 7437,
    "content": "Inspection of the PTX assembly code (obtained by compiling with -ptx or -keep command-line options to nvcc ) reveals whether a variable has been placed in local memory during the first compilation phases"
  },
  {
    "id": 7440,
    "content": "If it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture"
  },
  {
    "id": 7441,
    "content": "There is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel (lmem) when run with the --ptxas-options=-v option"
  },
  {
    "id": 7445,
    "content": "Texture Memory  The read-only texture memory space is cached Therefore, a texture fetch costs one device memory read only on a cache miss; otherwise, it just costs one read from the texture cache The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance Texture memory is also designed for"
  },
  {
    "id": 7446,
    "content": "streaming fetches with a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency In certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory"
  },
  {
    "id": 7451,
    "content": "Additional Texture Capabilities  If textures are fetched using tex1D() , tex2D() , or tex3D() rather than tex1Dfetch() , the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in Table 4 Useful Features for tex1D(), tex2D(), and tex3D() Fetches  Feature Use Caveat Filtering Fast, low-precision interpolation between texels Valid"
  },
  {
    "id": 7452,
    "content": "only if the texture reference returns floating-point data Normalized texture coordinates Resolution-independent coding None Addressing modes Automatic handling of boundary cases 1 Can be used only with normalized texture coordinates 1 The automatic handling of boundary cases in the bottom row of Table 4 refers to how a texture coordinate is resolved when it falls outside the valid addressing range"
  },
  {
    "id": 7453,
    "content": "If x is the coordinate and N is the number of texels for a one-dimensional texture, then with clamp, x is replaced by 0 if x >> ( data_1 ); kernel2 >> ( data_2 ); 10"
  },
  {
    "id": 7455,
    "content": "Multiple contexts  CUDA work occurs within a process space for a particular GPU known as a context The context encapsulates kernel launches and memory allocations for that GPU as well as supporting constructs such as the page tables The context is explicit in the CUDA Driver API but is entirely implicit in the CUDA Runtime API, which creates and manages contexts automatically With the CUDA"
  },
  {
    "id": 7456,
    "content": "Driver API, a CUDA application process can potentially create more than one context for a given GPU If multiple CUDA application processes access the same GPU concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unless Multi-Process Service is in use While multiple contexts (and their associated resources such as global memory"
  },
  {
    "id": 7457,
    "content": "allocations) can be allocated concurrently on a given GPU, only one of these contexts can execute work at any given moment on that GPU; contexts sharing the same GPU are time-sliced Creating additional contexts incurs memory overhead for per-context data and time overhead for context switching Furthermore, the need for context switching can reduce utilization when work from several contexts could"
  },
  {
    "id": 7458,
    "content": "otherwise execute concurrently (see also Concurrent Kernel Execution ) Therefore, it is best to avoid multiple contexts per GPU within the same CUDA application To assist with this, the CUDA Driver API provides methods to access and manage a special context on each GPU called the primary context These are the same contexts used implicitly by the CUDA Runtime when there is not already a current"
  },
  {
    "id": 7460,
    "content": "When initializing the program/library CUcontext ctx ; cuDevicePrimaryCtxRetain ( & ctx , dev ); When the program/library launches work cuCtxPushCurrent ( ctx ); kernel >> ( ); cuCtxPopCurrent ( & ctx ); When the program/library is finished with the context cuDevicePrimaryCtxRelease ( dev ); Note NVIDIA-SMI can be used to configure a GPU for exclusive process mode , which limits the number of"
  },
  {
    "id": 7461,
    "content": "contexts per GPU to one This context can be current to as many threads as desired within the creating process, and cuDevicePrimaryCtxRetain will fail if a non-primary context that was created with the CUDA driver API already exists on the device"
  },
  {
    "id": 7463,
    "content": "Instruction Optimization  Awareness of how instructions are executed often permits low-level optimizations that can be useful, especially in code that is run frequently (the so-called hot spot in a program) Best practices suggest that this optimization be performed after all higher-level optimizations have been completed"
  },
  {
    "id": 7466,
    "content": "Arithmetic Instructions  Single-precision floats provide the best performance, and their use is highly encouraged"
  },
  {
    "id": 7471,
    "content": "Division Modulo Operations  Note Low Priority: Use shift operations to avoid expensive division and modulo calculations Integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible: If \\(n\\) is a power of 2, ( \\(i/n\\) ) is equivalent to ( \\(i \\gg {log2}(n)\\) ) and ( \\(i\\% n\\) ) is equivalent to ( \\(i\\&\\left( {n - 1}"
  },
  {
    "id": 7477,
    "content": "Loop Counters Signed vs Unsigned  Note Low Medium Priority: Use signed integers rather than unsigned integers as loop counters"
  },
  {
    "id": 7478,
    "content": "In the C language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results"
  },
  {
    "id": 7479,
    "content": "Therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic"
  },
  {
    "id": 7480,
    "content": "This is of particular note with loop counters: since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned"
  },
  {
    "id": 7483,
    "content": "Formulae for exponentiation by small fractions  Computation Formula x 1/9 r = rcbrt(rcbrt(x)) x -1/9 r = cbrt(rcbrt(x)) x 1/6 r = rcbrt(rsqrt(x)) x -1/6 r = rcbrt(sqrt(x)) x 1/4 r = rsqrt(rsqrt(x)) x -1/4 r = sqrt(rsqrt(x)) x 1/3 r = cbrt(x) x -1/3 r = rcbrt(x) x 1/2 r = sqrt(x) x -1/2 r = rsqrt(x) x 2/3 r = cbrt(x); r = r*r x -2/3 r = rcbrt(x); r = r*r x 3/4 r = sqrt(x); r = r*sqrt(r) x -3/4 r"
  },
  {
    "id": 7484,
    "content": "= rsqrt(x); r = r*sqrt(r) x 7/6 r = x*rcbrt(rsqrt(x)) x -7/6 r = (1/x) * rcbrt(sqrt(x)) x 5/4 r = x*rsqrt(rsqrt(x)) x -5/4 r = (1/x)*sqrt(rsqrt(x)) x 4/3 r = x*cbrt(x) x -4/3 r = (1/x)*rcbrt(x) x 3/2 r = x*sqrt(x) x -3/2 r = (1/x)*rsqrt(x) 11"
  },
  {
    "id": 7488,
    "content": "They can be distinguished by their names: some have names with prepended underscores, whereas others do not (e"
  },
  {
    "id": 7490,
    "content": ", __functionName() versus functionName() ) Functions following the __functionName() naming convention map directly to the hardware level Functions following functionName() naming convention are slower but have higher accuracy (e"
  },
  {
    "id": 7492,
    "content": ", sinf(x) and expf(x) ) The throughput of __sinf(x) , __cosf(x) , and __expf(x) is much greater than that of sinf(x) , cosf(x) , and expf(x)"
  },
  {
    "id": 7493,
    "content": "The latter become even more expensive (about an order of magnitude slower) if the magnitude of the argument x needs to be reduced Moreover, in such cases, the argument-reduction code uses local memory, which can affect performance even more because of the high latency of local memory"
  },
  {
    "id": 7494,
    "content": "Note also that whenever sine and cosine of the same argument are computed, the sincos family of instructions should be used to optimize performance: __sincosf() for single-precision fast math (see next paragraph) sincosf() for regular single-precision sincos() for double precision The -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call"
  },
  {
    "id": 7495,
    "content": "It also disables single-precision denormal support and lowers the precision of single-precision division in general"
  },
  {
    "id": 7496,
    "content": "This is an aggressive optimization that can both reduce numerical accuracy and alter special case handling"
  },
  {
    "id": 7497,
    "content": "A more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated"
  },
  {
    "id": 7498,
    "content": "Note Medium Priority: Prefer faster, more specialized math functions over slower, more general ones when possible"
  },
  {
    "id": 7501,
    "content": ", x2 or x3 ), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as pow()"
  },
  {
    "id": 7502,
    "content": "While compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage"
  },
  {
    "id": 7505,
    "content": ", where both x2 and x5 are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization"
  },
  {
    "id": 7506,
    "content": "For exponentiation using base 2 or 10, use the functions exp2() or expf2() and exp10() or expf10() rather than the functions pow() or powf() Both pow() and powf() are heavy-weight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and"
  },
  {
    "id": 7507,
    "content": "the exponent The functions exp2() , exp2f() , exp10() , and exp10f() , on the other hand, are similar to exp() and expf() in terms of performance, and can be as much as ten times faster than their pow() / powf() equivalents For exponentiation with an exponent of 1/3, use the cbrt() or cbrtf() function rather than the generic exponentiation functions pow() or powf() , as the former are"
  },
  {
    "id": 7509,
    "content": "As a particular example, to evaluate the sine function in degrees instead of radians, use sinpi(x/180"
  },
  {
    "id": 7511,
    "content": "Similarly, the single-precision functions sinpif() , cospif() , and sincospif() should replace calls to sinf() , cosf() , and sincosf() when the function argument is of the form π*"
  },
  {
    "id": 7512,
    "content": "(The performance advantage sinpi() has over sin() is due to simplified argument reduction; the accuracy advantage is because sinpi() multiplies by π only implicitly, effectively using an infinitely precise mathematical π rather than a single- or double-precision approximation thereof"
  },
  {
    "id": 7516,
    "content": "Precision-related Compiler Flags  By default, the nvcc compiler generates IEEE-compliant code, but it also provides options to generate code that somewhat less accurate but faster: -ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) Another, more aggressive, option is -use_fast_math , which coerces every"
  },
  {
    "id": 7521,
    "content": "Memory Instructions  Note High Priority: Minimize the use of global memory Memory instructions include any instruction that reads from or writes to shared, local, or global memory When accessing uncached local or global memory, there are hundreds of clock cycles of memory latency As an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a"
  },
  {
    "id": 7522,
    "content": "latency of hundreds of clock cycles to read data from global memory: __shared__ float shared [ 32 ]; __device__ float device [ 32 ]; shared [ threadIdx x ]; Much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete However, it is best to avoid"
  },
  {
    "id": 7526,
    "content": "Branching and Divergence  Note High Priority: Avoid different execution paths within the same warp Flow control instructions ( if , switch , do , for , while ) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths If this happens, the different execution paths must be executed separately; this increases"
  },
  {
    "id": 7527,
    "content": "the total number of instructions executed for this warp To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture of the CUDA C++ Programming Guide A trivial"
  },
  {
    "id": 7528,
    "content": "example is when the controlling condition depends only on ( threadIdx / WSIZE ) where WSIZE is the warp size In this case, no warp diverges because the controlling condition is perfectly aligned with the warps For branches including just a few instructions, warp divergence generally results in marginal performance losses"
  },
  {
    "id": 7529,
    "content": "Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions"
  },
  {
    "id": 7530,
    "content": "Threads with a false predicate do not write results, and also do not evaluate addresses or read operands"
  },
  {
    "id": 7531,
    "content": "Starting with the Volta architecture, Independent Thread Scheduling allows a warp to remain diverged outside of the data-dependent conditional block"
  },
  {
    "id": 7532,
    "content": "An explicit __syncwarp() can be used to guarantee that the warp has reconverged for subsequent instructions"
  },
  {
    "id": 7535,
    "content": "Branch Predication  Note Low Priority: Make it easy for the compiler to use branch predication in lieu of loops or control statements Sometimes, the compiler may unroll loops or optimize out if or switch statements by using branch predication instead"
  },
  {
    "id": 7536,
    "content": "The programmer can also control loop unrolling using #pragma unroll For more information on this pragma, refer to the CUDA C++ Programming Guide"
  },
  {
    "id": 7537,
    "content": "When using branch predication, none of the instructions whose execution depends on the controlling condition is skipped Instead, each such instruction is associated with a per-thread condition code or predicate that is set to true or false according to the controlling condition Although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually"
  },
  {
    "id": 7538,
    "content": "executed Instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands The compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold"
  },
  {
    "id": 7540,
    "content": "Deploying CUDA Applications  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation"
  },
  {
    "id": 7542,
    "content": "Understanding the Programming Environment  With each generation of NVIDIA processors, new features are added to the GPU that CUDA can leverage"
  },
  {
    "id": 7543,
    "content": "The first is the compute capability , and the second is the version number of the CUDA Runtime and CUDA Driver APIs"
  },
  {
    "id": 7546,
    "content": "CUDA Compute Capability  The compute capability describes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor"
  },
  {
    "id": 7547,
    "content": "Higher compute capability versions are supersets of lower (that is, earlier) versions, so they are backward compatible"
  },
  {
    "id": 7548,
    "content": "The compute capability of the GPU in the device can be queried programmatically as illustrated in the deviceQuery CUDA Sample"
  },
  {
    "id": 7549,
    "content": "This information is obtained by calling cudaGetDeviceProperties() and accessing the information in the structure it returns"
  },
  {
    "id": 7550,
    "content": "Sample CUDA configuration data reported by deviceQuery  The major and minor revision numbers of the compute capability are shown on the seventh line of Figure 16 More details about the compute capabilities of various GPUs are in CUDA-Enabled GPUs and Compute Capabilities of the CUDA C++ Programming Guide In particular, developers should note the number of multiprocessors on the device, the"
  },
  {
    "id": 7554,
    "content": "Additional Hardware Data  Certain hardware features are not described by the compute capability For example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all GPUs irrespective of the compute capability"
  },
  {
    "id": 7555,
    "content": "In such cases, call cudaGetDeviceProperties() to determine whether the device is capable of a certain feature"
  },
  {
    "id": 7556,
    "content": "For example, the asyncEngineCount field of the device property structure indicates whether overlapping kernel execution and data transfers is possible (and, if so, how many concurrent transfers are possible); likewise, the canMapHostMemory field indicates whether zero-copy data transfers can be performed"
  },
  {
    "id": 7559,
    "content": "Which Compute Capability Target  To target specific versions of NVIDIA hardware and CUDA software, use the -arch , -code , and -gencode options of nvcc Code that uses the warp shuffle operation, for example, must be compiled with -arch=sm_30 (or higher compute capability)"
  },
  {
    "id": 7560,
    "content": "See Building for Maximum Compatibility for further discussion of the flags used for building code for multiple generations of CUDA-capable device simultaneously"
  },
  {
    "id": 7563,
    "content": "CUDA Runtime  The host runtime component of the CUDA software environment can be used only by host functions"
  },
  {
    "id": 7564,
    "content": "It provides functions to handle the following: Device management Context management Memory management Code module management Execution control Texture reference management Interoperability with OpenGL and Direct3D As compared to the lower-level CUDA Driver API, the CUDA Runtime greatly eases device management by providing implicit initialization, context management, and device code module"
  },
  {
    "id": 7566,
    "content": "The C++ host code generated by nvcc utilizes the CUDA Runtime, so applications that link to this code will depend on the CUDA Runtime; similarly, any code that uses the cuBLAS , cuFFT , and other CUDA Toolkit libraries will also depend on the CUDA Runtime, which is used internally by these libraries The functions that make up the CUDA Runtime API are explained in the CUDA Toolkit Reference Manual"
  },
  {
    "id": 7567,
    "content": "The CUDA Runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched The implicit driver version checking, code initialization, CUDA context management, CUDA module management (cubin to function mapping), kernel configuration, and parameter passing are all performed by the CUDA Runtime For more information on the Runtime API, refer to CUDA"
  },
  {
    "id": 7570,
    "content": "CUDA Compatibility Developer’s Guide  CUDA Toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes"
  },
  {
    "id": 7571,
    "content": "CUDA compatibility allows users to update the latest CUDA Toolkit software (including the compiler, libraries, and tools) without requiring update to the entire driver stack The CUDA software environment consists of three parts: CUDA Toolkit (libraries, CUDA runtime and developer tools) - SDK for developers to build CUDA applications"
  },
  {
    "id": 7572,
    "content": "On Linux systems, the CUDA driver and kernel mode components are delivered together in the NVIDIA display driver package Components of CUDA  The CUDA compiler (nvcc), provides a way to handle CUDA and non-CUDA code (by splitting and steering compilation), along with the CUDA runtime, is part of the CUDA compiler toolchain The CUDA Runtime API provides developers with high-level C++ interface for"
  },
  {
    "id": 7573,
    "content": "simplified management of devices, kernel executions etc , While the CUDA driver API provides ( CUDA Driver API ) a low-level programming interface for applications to target NVIDIA hardware Built on top of these technologies are CUDA libraries, some of which are included in the CUDA Toolkit, while others such as cuDNN may be released independently of the CUDA Toolkit"
  },
  {
    "id": 7576,
    "content": "CUDA Toolkit Versioning  Starting with CUDA 11, the toolkit versions are based on an industry-standard semantic versioning scheme:"
  },
  {
    "id": 7580,
    "content": "X stands for the major version - APIs have changed and binary compatibility is broken Y stands for the minor version - Introduction of new APIs, deprecation of old APIs, and source compatibility might be broken but binary compatibility is maintained"
  },
  {
    "id": 7581,
    "content": "Compatibility of the CUDA platform is thus intended to address a few scenarios: NVIDIA driver upgrades to systems with GPUs running in production for enterprises or datacenters can be complex and may need advance planning"
  },
  {
    "id": 7582,
    "content": "Delays in rolling out new NVIDIA drivers could mean that users of such systems may not have access to new features available in CUDA releases Not requiring driver updates for new CUDA releases can mean that new versions of the software can be made available faster to users"
  },
  {
    "id": 7583,
    "content": "math libraries or deep learning frameworks) do not have a direct dependency on the CUDA runtime, compiler or driver In such cases, users or developers can still benefit from not having to upgrade the entire CUDA Toolkit or driver to use these libraries or frameworks"
  },
  {
    "id": 7584,
    "content": "Upgrading dependencies is error-prone and time consuming, and in some corner cases, can even change the semantics of a program"
  },
  {
    "id": 7585,
    "content": "Constantly recompiling with the latest CUDA Toolkit means forcing upgrades on the end-customers of an application product"
  },
  {
    "id": 7586,
    "content": "Package managers facilitate this process but unexpected issues can still arise and if a bug is found, it necessitates a repeat of the above upgrade process"
  },
  {
    "id": 7587,
    "content": "CUDA supports several compatibility choices: First introduced in CUDA 10, the CUDA Forward Compatible Upgrade is designed to allow users to get access to new CUDA features and run applications built with new CUDA releases on systems with older installations of the NVIDIA datacenter driver First introduced in CUDA 11 1, CUDA Enhanced Compatibility provides two benefits: By leveraging semantic"
  },
  {
    "id": 7588,
    "content": "versioning across components in the CUDA Toolkit, an application can be built for one CUDA minor release (for example 11"
  },
  {
    "id": 7591,
    "content": "The CUDA runtime has relaxed the minimum driver version check and thus no longer requires a driver upgrade when moving to a new minor release"
  },
  {
    "id": 7592,
    "content": "The CUDA driver ensures backward Binary Compatibility is maintained for compiled CUDA applications Applications compiled with CUDA toolkit versions as old as 3"
  },
  {
    "id": 7596,
    "content": "Source Compatibility  We define source compatibility as a set of guarantees provided by the library, where a well-formed application built against a specific version of the library (using the SDK) will continue to build and run without errors when a newer version of the SDK is installed Both the CUDA driver and the CUDA runtime are not source compatible across the different SDK releases"
  },
  {
    "id": 7597,
    "content": "Therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit"
  },
  {
    "id": 7598,
    "content": "Developers are notified through deprecation and documentation mechanisms of any current or upcoming changes"
  },
  {
    "id": 7599,
    "content": "This does not mean that application binaries compiled using an older toolkit will not be supported anymore"
  },
  {
    "id": 7600,
    "content": "Application binaries rely on CUDA Driver API interface and even though the CUDA Driver API itself may also have changed across toolkit versions, CUDA guarantees Binary Compatibility of the CUDA Driver API interface"
  },
  {
    "id": 7603,
    "content": "Binary Compatibility  We define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library"
  },
  {
    "id": 7604,
    "content": "The CUDA Driver API has a versioned C-style ABI, which guarantees that applications that were running against an older driver (for example CUDA 3 2) will still run and function correctly against a modern driver (for example one shipped with CUDA 11"
  },
  {
    "id": 7606,
    "content": "This means that even though an application source might need to be changed if it has to be recompiled against a newer CUDA Toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions The CUDA Driver API thus is binary-compatible (the OS loader can pick up a newer version and the"
  },
  {
    "id": 7607,
    "content": "application continues to work) but not source-compatible (rebuilding your application against a newer SDK might require source changes)"
  },
  {
    "id": 7608,
    "content": "CUDA Toolkit and Minimum Driver Versions  Before we proceed further on this topic, it’s important for developers to understand the concept of Minimum Driver Version and how that may affect them Each version of the CUDA Toolkit (and runtime) requires a minimum version of the NVIDIA driver Applications compiled against a CUDA Toolkit version will only run on systems with the specified minimum"
  },
  {
    "id": 7611,
    "content": "0, the minimum driver version for a toolkit was the same as the driver shipped with that version of the CUDA Toolkit"
  },
  {
    "id": 7613,
    "content": "0, it can only run on a system with an R450 or later driver If such an application is run on a system with the R418 driver installed, CUDA initialization will return an error as can be seen in the example below In this example, the deviceQuery sample is compiled with CUDA 11 1 and is run on a system with R418 In this scenario, CUDA initialization returns an error due to the minimum driver"
  },
  {
    "id": 7616,
    "content": "0 Off | 0 | | N/A 42C P0 28W / 70W | 0MiB / 15079MiB | 0% Default | +-+-+-+ +-+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=| | No running processes found | +-+ $ samples/bin/x86_64/linux/release/deviceQuery samples/bin/x86_64/linux/release/deviceQuery Starting"
  },
  {
    "id": 7617,
    "content": "CUDA Device Query (Runtime API) version (CUDART static linking) cudaGetDeviceCount returned 3 -> initialization error Result = FAIL Refer to the CUDA Toolkit Release Notes for details for the minimum driver version and the version of the driver shipped with the toolkit"
  },
  {
    "id": 7621,
    "content": "CUDA Binary (cubin) Compatibility  A slightly related but important topic is one of application binary compatibility across GPU architectures in CUDA"
  },
  {
    "id": 7622,
    "content": "CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device"
  },
  {
    "id": 7623,
    "content": "Kernels can be written using the CUDA instruction set architecture, called PTX, which is described in the PTX reference manual"
  },
  {
    "id": 7625,
    "content": "In both cases, kernels must be compiled into binary code by nvcc (called cubins) to execute on the device"
  },
  {
    "id": 7626,
    "content": "Binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions In other words, a cubin object generated for compute capability X y will only execute on devices of compute capability X"
  },
  {
    "id": 7628,
    "content": "To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability For portability, that is, to be able to execute code on future GPU architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled by the NVIDIA"
  },
  {
    "id": 7629,
    "content": "driver for these future devices More information on cubins, PTX and application compatibility can be found in the CUDA C++ Programming Guide"
  },
  {
    "id": 7632,
    "content": "CUDA Compatibility Across Minor Releases  By leveraging the semantic versioning, starting with CUDA 11, components in the CUDA Toolkit will remain binary compatible across the minor versions of the toolkit In order to maintain binary compatibility across minor versions, the CUDA runtime no longer bumps up the minimum driver version required for every minor release - this only happens when a"
  },
  {
    "id": 7634,
    "content": "One of the main reasons a new toolchain requires a new minimum driver is to handle the JIT compilation of PTX code and the JIT linking of binary code"
  },
  {
    "id": 7635,
    "content": "In this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the CUDA platform"
  },
  {
    "id": 7646,
    "content": "0 Off | 0 | | N / A 39 C P8 9 W / 70 W | 0 MiB / 15109 MiB | 0 % Default | | | | N / A | +-+-+-+ +-+ | Processes : | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=| | No running processes found | +-+ When our CUDA 11"
  },
  {
    "id": 7650,
    "content": "1 is statically linked) is run on the system, we see that it runs successfully even when the driver reports a 11 0 version - that is, without requiring the driver or other toolkit components to be updated on the system"
  },
  {
    "id": 7651,
    "content": "$ samples / bin / x86_64 / linux / release / deviceQuery samples / bin / x86_64 / linux / release / deviceQuery Starting"
  },
  {
    "id": 7652,
    "content": "CUDA Device Query ( Runtime API ) version ( CUDART static linking ) Detected 1 CUDA Capable device ( s ) Device 0 : \"Tesla T4\" CUDA Driver Version / Runtime Version 11 0 / 11 1 CUDA Capability Major / Minor version number : 7"
  },
  {
    "id": 7655,
    "content": "1 , NumDevs = 1 Result = PASS By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features"
  },
  {
    "id": 7661,
    "content": "Handling New CUDA Features and Driver APIs  A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies For example, cuMemMap APIs or any of APIs introduced prior to CUDA 11"
  },
  {
    "id": 7662,
    "content": "0, such as cudaDeviceSynchronize , do not require a driver upgrade To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully"
  },
  {
    "id": 7663,
    "content": "This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions"
  },
  {
    "id": 7665,
    "content": "When working with a feature exposed in a minor version of the toolkit, the feature might not be available at runtime if the application is running against an older CUDA driver"
  },
  {
    "id": 7666,
    "content": "Applications that do not check for CUDA API errors could at times run to completion without having noticed that the data calculated by the GPU is incomplete, invalid, or uninitialized"
  },
  {
    "id": 7667,
    "content": "Note The CUDA Toolkit Samples provide several helper functions for error checking with the various CUDA APIs; these helper functions are located in the samples/common/inc/helper_cuda h file in the CUDA Toolkit"
  },
  {
    "id": 7670,
    "content": "Building for Maximum Compatibility  Each generation of CUDA-capable device has an associated compute capability version that indicates the feature set supported by the device (see CUDA Compute Capability ) One or more compute capability versions can be specified to the nvcc compiler while building a file; compiling for the native compute capability for the target GPU(s) of the application is"
  },
  {
    "id": 7671,
    "content": "important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of GPU When an application is built for multiple compute capabilities simultaneously (using several instances of the -gencode flag to nvcc), the binaries for the specified compute capabilities are combined into the executable, and the CUDA"
  },
  {
    "id": 7672,
    "content": "Driver selects the most appropriate binary at runtime according to the compute capability of the present device If an appropriate native binary ( cubin ) is not available, but the intermediate PTX code (which targets an abstract virtual instruction set and is used for forward-compatibility) is available, then the kernel will be compiled Just In Time (JIT) (see Compiler JIT Cache Management Tools )"
  },
  {
    "id": 7673,
    "content": "from the PTX to the native cubin for the device Unlike the CUDA Driver, the CUDA Runtime guarantees neither forward nor backward binary compatibility across versions It is therefore best to redistribute the CUDA Runtime library with the application when using dynamic linking or else to statically link against the CUDA Runtime This will ensure that the executable will be able to run even if the"
  },
  {
    "id": 7674,
    "content": "user does not have the same CUDA Toolkit installed that the application was built against Note When statically linking to the CUDA Runtime, multiple versions of the runtime can peacably coexist in the same application process simultaneously; for example, if an application uses one version of the CUDA Runtime, and a plugin to that application is statically linked to a different version, that is"
  },
  {
    "id": 7675,
    "content": "perfectly acceptable, as long as the installed NVIDIA Driver is sufficient for both Statically-linked CUDA Runtime The easiest option is to statically link against the CUDA Runtime Static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the CUDA"
  },
  {
    "id": 7676,
    "content": "Runtime library Dynamically-linked CUDA Runtime If static linking against the CUDA Runtime is impractical for some reason, then a dynamically-linked version of the CUDA Runtime library is also available (This was the default and only option provided in CUDA versions 5"
  },
  {
    "id": 7678,
    "content": ") To use dynamic linking with the CUDA Runtime when using the nvcc from CUDA 5 5 or later to link the application, add the --cudart=shared flag to the link command line; otherwise the statically-linked CUDA Runtime library is used by default After the application is dynamically linked against the CUDA Runtime, this version of the runtime library should be bundled with the application"
  },
  {
    "id": 7679,
    "content": "It can be copied into the same directory as the application executable or into a subdirectory of that installation path"
  },
  {
    "id": 7680,
    "content": "Other CUDA Libraries Although the CUDA Runtime provides the option of static linking, some libraries included in the CUDA Toolkit are available only in dynamically-linked form As with the dynamically-linked version of the CUDA Runtime library , these libraries should be bundled with the application executable when distributing that application"
  },
  {
    "id": 7684,
    "content": "CUDA Toolkit Library Redistribution  The CUDA Toolkit’s End-User License Agreement (EULA) allows for redistribution of many of the CUDA libraries under certain terms and conditions This allows applications that depend on these libraries to redistribute the exact versions of the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a"
  },
  {
    "id": 7686,
    "content": "Note This does not apply to the NVIDIA Driver; the end user must still download and install an NVIDIA Driver appropriate to their GPU(s) and operating system"
  },
  {
    "id": 7691,
    "content": "Which Files to Redistribute  When redistributing the dynamically-linked versions of one or more CUDA libraries, it is important to identify the exact files that need to be redistributed"
  },
  {
    "id": 7692,
    "content": "The following examples use the cuBLAS library from CUDA Toolkit 5 5 as an illustration: Linux In a shared library on Linux, there is a string field called the SONAME that indicates the binary compatibility level of the library The SONAME of the library against which the application was built must match the filename of the library that is redistributed with the application For example, in the"
  },
  {
    "id": 7711,
    "content": "5 Because of this, even if -lcublas (with no version number specified) is used when linking the application, the SONAME found at link time implies that “ libcublas"
  },
  {
    "id": 7714,
    "content": "5 ” is the name of the file that the dynamic loader will look for when loading the application and therefore must be the name of the file (or a symlink to the same) that is redistributed with the application The ldd tool is useful for identifying the exact filenames of the libraries that the application expects to find at runtime as well as the path, if any, of the copy of that library that the"
  },
  {
    "id": 7715,
    "content": "dynamic loader would select when loading the application given the current library search path: $ ldd a"
  },
  {
    "id": 7722,
    "content": "5 Mac In a shared library on Mac OS X, there is a field called the install name that indicates the expected installation path and filename the library; the CUDA libraries also use this filename to indicate binary compatibility The value of this field is propagated into an application built against the library and is used to locate the library of the correct version at runtime For example, if the"
  },
  {
    "id": 7731,
    "content": "Furthermore, this file should be installed into the @rpath of the application; see Where to Install Redistributed CUDA Libraries"
  },
  {
    "id": 7737,
    "content": ") Windows The binary compatibility version of the CUDA libraries on Windows is indicated as part of the filename"
  },
  {
    "id": 7740,
    "content": "dll at runtime, so this is the file that should be redistributed with that application, even though cublas lib is the file that the application is linked against To verify the exact DLL filename that the application expects to find at runtime, use the dumpbin tool from the Visual Studio command prompt: $ dumpbin /IMPORTS a"
  },
  {
    "id": 7753,
    "content": "Where to Install Redistributed CUDA Libraries  Once the correct library files are identified for redistribution, they must be configured for installation into a location where the application will be able to find them"
  },
  {
    "id": 7754,
    "content": "On Windows, if the CUDA Runtime or other dynamically-linked CUDA Toolkit library is placed in the same directory as the executable, Windows will locate it automatically"
  },
  {
    "id": 7755,
    "content": "On Linux and Mac, the -rpath linker option should be used to instruct the executable to search its local path for these libraries before searching the system paths: Linux/Mac nvcc -I $(CUDA_HOME)/include -Xlinker \"-rpath '$ORIGIN'\" --cudart=shared -o myprogram myprogram"
  },
  {
    "id": 7757,
    "content": "exe -ccbin \"C:\\vs2008\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" --cudart=shared -o \"Release\\myprogram exe\" \"myprogram"
  },
  {
    "id": 7758,
    "content": "cu\" Note It may be necessary to adjust the value of -ccbin to reflect the location of your Visual Studio installation"
  },
  {
    "id": 7759,
    "content": "To specify an alternate path where the libraries will be distributed, use linker options similar to those below: Linux/Mac nvcc -I $(CUDA_HOME)/include -Xlinker \"-rpath '$ORIGIN/lib'\" --cudart=shared -o myprogram myprogram"
  },
  {
    "id": 7761,
    "content": "exe -ccbin \"C:\\vs2008\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT /DELAY\" --cudart=shared -o \"Release\\myprogram exe\" \"myprogram"
  },
  {
    "id": 7762,
    "content": "cu\" For Linux and Mac, the -rpath option is used as before For Windows, the /DELAY option is used; this requires that the application call SetDllDirectory() before the first call to any CUDA API function in order to specify the directory containing the CUDA DLLs Note For Windows 8, SetDefaultDLLDirectories() and AddDllDirectory() should be used instead of SetDllDirectory()"
  },
  {
    "id": 7766,
    "content": "Nvidia-SMI  The NVIDIA System Management Interface ( nvidia-smi ) is a command line utility that aids in the management and monitoring of NVIDIA GPU devices"
  },
  {
    "id": 7767,
    "content": "This utility allows administrators to query GPU device state and, with the appropriate privileges, permits administrators to modify GPU device state"
  },
  {
    "id": 7768,
    "content": "nvidia-smi is targeted at Tesla and certain Quadro GPUs, though limited support is also available on other NVIDIA GPUs nvidia-smi ships with NVIDIA GPU display drivers on Linux, and with 64-bit Windows Server 2008 R2 and Windows 7"
  },
  {
    "id": 7769,
    "content": "nvidia-smi can output queried information as XML or as human-readable plain text either to standard output or to a file"
  },
  {
    "id": 7770,
    "content": "Please note that new versions of nvidia-smi are not guaranteed to be backward-compatible with previous versions"
  },
  {
    "id": 7774,
    "content": "Queryable state  ECC error counts Both correctable single-bit and detectable double-bit errors are reported"
  },
  {
    "id": 7775,
    "content": "GPU utilization Current utilization rates are reported for both the compute resources of the GPU and the memory interface Active compute process The list of active processes running on the GPU is reported, along with the corresponding process name/ID and allocated GPU memory Clocks and performance state Max and current clock rates are reported for several important clock domains, as well as the"
  },
  {
    "id": 7776,
    "content": "current GPU performance state ( pstate ) Temperature and fan speed The current GPU core temperature is reported, along with fan speeds for products with active cooling"
  },
  {
    "id": 7777,
    "content": "Power management The current board power draw and power limits are reported for products that report these measurements"
  },
  {
    "id": 7778,
    "content": "Identification Various dynamic and static information is reported, including board serial numbers, PCI device IDs, VBIOS/Inforom version numbers and product names"
  },
  {
    "id": 7783,
    "content": "Compute mode Indicate whether compute processes can run on the GPU and whether they run exclusively or concurrently with other compute processes Persistence mode Indicate whether the NVIDIA driver stays loaded when no applications are connected to the GPU"
  },
  {
    "id": 7787,
    "content": "NVML  The NVIDIA Management Library (NVML) is a C-based interface that provides direct access to the queries and commands exposed via nvidia-smi intended as a platform for building 3rd-party system management applications The NVML API is shipped with the CUDA Toolkit (since version 8"
  },
  {
    "id": 7788,
    "content": "0) and is also available standalone on the NVIDIA developer website as part of the GPU Deployment Kit through a single header file accompanied by PDF documentation, stub libraries, and sample applications; see https: developer"
  },
  {
    "id": 7791,
    "content": "These bindings expose the same features as the C-based interface and also provide backwards compatibility All of these products ( nvidia-smi , NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality"
  },
  {
    "id": 7796,
    "content": "Cluster Management Tools  Managing your GPU cluster will help achieve maximum GPU utilization and help you and your users extract the best possible performance"
  },
  {
    "id": 7802,
    "content": "Compiler JIT Cache Management Tools  Any PTX device code loaded by an application at runtime is compiled further to binary code by the device driver"
  },
  {
    "id": 7803,
    "content": "Just-in-time compilation increases application load time but allows applications to benefit from latest compiler improvements It is also the only way for applications to run on devices that did not exist at the time the application was compiled"
  },
  {
    "id": 7804,
    "content": "When JIT compilation of PTX device code is used, the NVIDIA driver caches the resulting binary code on disk"
  },
  {
    "id": 7805,
    "content": "Some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables; see Just in Time Compilation of the CUDA C++ Programming Guide"
  },
  {
    "id": 7808,
    "content": "CUDA_VISIBLE_DEVICES  It is possible to rearrange the collection of installed CUDA devices that will be visible to and enumerated by a CUDA application prior to the start of that application by way of the CUDA_VISIBLE_DEVICES environment variable Devices to be made visible to the application should be included as a comma-separated list in terms of the system-wide list of enumerable devices For"
  },
  {
    "id": 7809,
    "content": "example, to use only devices 0 and 2 from the system-wide list of devices, set CUDA_VISIBLE_DEVICES=0,2 before launching the application The application will then enumerate these devices as device 0 and device 1, respectively"
  },
  {
    "id": 7811,
    "content": "Recommendations and Best Practices  This chapter contains a summary of the recommendations for optimization that are explained in this document"
  },
  {
    "id": 7814,
    "content": "Overall Performance Optimization Strategies  Performance optimization revolves around three basic strategies: Maximizing parallel execution Optimizing memory usage to achieve maximum memory bandwidth Optimizing instruction usage to achieve maximum instruction throughput Maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible Once"
  },
  {
    "id": 7815,
    "content": "the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible The application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device Optimizing memory usage starts with minimizing data"
  },
  {
    "id": 7816,
    "content": "transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers Kernel access to global memory also should be minimized by maximizing the use of shared memory on the device"
  },
  {
    "id": 7817,
    "content": "Sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed"
  },
  {
    "id": 7818,
    "content": "The effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory"
  },
  {
    "id": 7819,
    "content": "The next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns This optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles Shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts As for optimizing"
  },
  {
    "id": 7821,
    "content": "This suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision"
  },
  {
    "id": 7822,
    "content": "Finally, particular attention must be paid to control flow instructions due to the SIMT (single instruction multiple thread) nature of the device"
  },
  {
    "id": 7827,
    "content": "It supports a number of command-line parameters, of which the following are especially useful for optimization and related best practices: -maxrregcount=N specifies the maximum number of registers kernels can use at a per-file level (See also the __launch_bounds__ qualifier discussed in Execution Configuration of the CUDA C++ Programming Guide to control the number of registers used on a"
  },
  {
    "id": 7830,
    "content": "-ftz=true (denormalized numbers are flushed to zero) -prec-div=false (less precise division) -prec-sqrt=false (less precise square root) -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call"
  },
  {
    "id": 7833,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 7834,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 7836,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 7837,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 7838,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 7839,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 7840,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 7841,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 7842,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 7843,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 7844,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 7845,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 7846,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 7853,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 7855,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 7856,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 7861,
    "content": "Maxwell Compatibility v12 5 | PDF | Archive Maxwell Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Maxwell Architecture About this Document  This application note, Maxwell Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ®"
  },
  {
    "id": 7862,
    "content": "Maxwell Architecture This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Maxwell"
  },
  {
    "id": 7865,
    "content": "Application Compatibility on Maxwell  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number For example, cubin files that target compute capability 3 0"
  },
  {
    "id": 7866,
    "content": "are supported on all compute-capability 3 x (Kepler) devices but are not supported on compute-capability 5 x (Maxwell) devices For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels Note CUDA Runtime applications containing both cubin and PTX code for a"
  },
  {
    "id": 7867,
    "content": "given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility purposes Applications that already include PTX versions of their kernels should work as-is on Maxwell-based GPUs Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Maxwell-compatible PTX or cubins"
  },
  {
    "id": 7870,
    "content": "Verifying Maxwell Compatibility for Existing Applications  The first step is to check that Maxwell-compatible device code (at least PTX) is compiled in to the application"
  },
  {
    "id": 7871,
    "content": "The following sections show how to accomplish this for applications built with different CUDA Toolkit versions"
  },
  {
    "id": 7875,
    "content": "Applications Using CUDA Toolkit 5 5 or Earlier  CUDA applications built using CUDA Toolkit versions 2"
  },
  {
    "id": 7878,
    "content": "To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https: www"
  },
  {
    "id": 7881,
    "content": "When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code"
  },
  {
    "id": 7882,
    "content": "If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Maxwell compatibility Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing"
  },
  {
    "id": 7886,
    "content": "Applications Using CUDA Toolkit 6 0 or Later  CUDA applications built using CUDA Toolkit 6 0 or Later 1 are compatible with Maxwell as long as they are built to include kernels in either Maxwell-native cubin format (see Building Applications with Maxwell Support ) or PTX format (see Applications Using CUDA Toolkit 5"
  },
  {
    "id": 7890,
    "content": "Building Applications with Maxwell Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime"
  },
  {
    "id": 7891,
    "content": "will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it The method used to build your application with either native cubin or at least PTX support for Maxwell depend on the version of the CUDA Toolkit used The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as"
  },
  {
    "id": 7892,
    "content": "PTX All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application"
  },
  {
    "id": 7893,
    "content": "Especially when using large libraries, this JIT compilation can take a significant amount of time The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code"
  },
  {
    "id": 7898,
    "content": "Applications Using CUDA Toolkit 5 5 or Earlier  The compilers included in CUDA Toolkit 5 5 or earlier generate cubin files native to earlier NVIDIA architectures such as Fermi and Kepler, but they cannot generate cubin files native to the Maxwell architecture To allow support for Maxwell and future architectures when using version 5 5 or earlier of the CUDA Toolkit, the compiler must generate a"
  },
  {
    "id": 7902,
    "content": "The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version The code= clause specifies the back-end compilation target and can either be cubin or PTX or both Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Maxwell"
  },
  {
    "id": 7905,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_35,code=compute_35 --compile -o \"Release\\mykernel"
  },
  {
    "id": 7908,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_35,code=compute_35 -O2 -o mykernel"
  },
  {
    "id": 7910,
    "content": "cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a"
  },
  {
    "id": 7911,
    "content": "PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly"
  },
  {
    "id": 7917,
    "content": "0 of the CUDA Toolkit, nvcc can generate cubin files native to the first-generation Maxwell architecture (compute capability 5 0); CUDA Toolkit 6 5 and later further add native support for second-generation Maxwell devices (compute capability 5"
  },
  {
    "id": 7920,
    "content": "x or Later, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 7921,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 --compile -o \"Release\\mykernel"
  },
  {
    "id": 7924,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_20,code=sm_20 -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 -O2 -o mykernel"
  },
  {
    "id": 7927,
    "content": "Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures"
  },
  {
    "id": 7931,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 7932,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 7934,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 7935,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 7936,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 7937,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 7938,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 7939,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 7940,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 7941,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 7942,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 7943,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 7944,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 7951,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 7953,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 7954,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 7959,
    "content": "Pascal Compatibility v12 5 | PDF | Archive Pascal Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Pascal Architecture About this Document  This application note, Pascal Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Pascal"
  },
  {
    "id": 7960,
    "content": "Architecture This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Pascal"
  },
  {
    "id": 7963,
    "content": "Application Compatibility on Pascal  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number For example, cubin files that target compute capability 3 0"
  },
  {
    "id": 7964,
    "content": "are supported on all compute-capability 3 x (Kepler) devices but are not supported on compute-capability 5"
  },
  {
    "id": 7967,
    "content": "For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility"
  },
  {
    "id": 7968,
    "content": "purposes Applications that already include PTX versions of their kernels should work as-is on Pascal-based GPUs Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Pascal-compatible PTX or cubins"
  },
  {
    "id": 7971,
    "content": "Verifying Pascal Compatibility for Existing Applications  The first step is to check that Pascal-compatible device code (at least PTX) is compiled in to the application"
  },
  {
    "id": 7972,
    "content": "The following sections show how to accomplish this for applications built with different CUDA Toolkit versions"
  },
  {
    "id": 7976,
    "content": "Applications Using CUDA Toolkit 7 5 or Earlier  CUDA applications built using CUDA Toolkit versions 2"
  },
  {
    "id": 7979,
    "content": "To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https: www"
  },
  {
    "id": 7982,
    "content": "When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code"
  },
  {
    "id": 7983,
    "content": "If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Pascal compatibility Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing"
  },
  {
    "id": 7987,
    "content": "Applications Using CUDA Toolkit 8 0  CUDA applications built using CUDA Toolkit 8 0 are compatible with Pascal as long as they are built to include kernels in either Pascal-native cubin format (see Building Applications with Pascal Support ) or PTX format (see Applications Using CUDA Toolkit 7"
  },
  {
    "id": 7991,
    "content": "Building Applications with Pascal Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime"
  },
  {
    "id": 7992,
    "content": "will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it The method used to build your application with either native cubin or at least PTX support for Pascal depend on the version of the CUDA Toolkit used The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as"
  },
  {
    "id": 7993,
    "content": "PTX All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application"
  },
  {
    "id": 7994,
    "content": "Especially when using large libraries, this JIT compilation can take a significant amount of time The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code"
  },
  {
    "id": 7999,
    "content": "Applications Using CUDA Toolkit 7 5 or Earlier  The compilers included in CUDA Toolkit 7 5 or earlier generate cubin files native to earlier NVIDIA architectures such as Kepler and Maxwell, but they cannot generate cubin files native to the Pascal architecture To allow support for Pascal and future architectures when using version 7 5 or earlier of the CUDA Toolkit, the compiler must generate a"
  },
  {
    "id": 8003,
    "content": "The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version The code= clause specifies the back-end compilation target and can either be cubin or PTX or both Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Pascal compatibility"
  },
  {
    "id": 8005,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8008,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_52,code=compute_52 -O2 -o mykernel"
  },
  {
    "id": 8010,
    "content": "cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a"
  },
  {
    "id": 8011,
    "content": "PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly"
  },
  {
    "id": 8017,
    "content": "0 of the CUDA Toolkit, nvcc can generate cubin files native to the Pascal architectures (compute capability 6"
  },
  {
    "id": 8021,
    "content": "0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 8022,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8025,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_30,code=sm_30 -gencode=arch=compute_35,code=sm_35 -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel"
  },
  {
    "id": 8028,
    "content": "Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures"
  },
  {
    "id": 8032,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8033,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8035,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8036,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8037,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8038,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8039,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8040,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8041,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8042,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8043,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8044,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8045,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8052,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8054,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8055,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 8060,
    "content": "Volta Compatibility v12 5 | PDF | Archive Volta Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Volta Architecture About this Document  This application note, Volta Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Volta"
  },
  {
    "id": 8061,
    "content": "Architecture This document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Volta"
  },
  {
    "id": 8064,
    "content": "Application Compatibility on Volta  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number For example, cubin files that target compute capability 3 0"
  },
  {
    "id": 8065,
    "content": "are supported on all compute-capability 3 x (Kepler) devices but are not supported on compute-capability 5"
  },
  {
    "id": 8068,
    "content": "For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility"
  },
  {
    "id": 8069,
    "content": "purposes Applications that already include PTX versions of their kernels should work as-is on Volta-based GPUs Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Volta-compatible PTX or cubins"
  },
  {
    "id": 8072,
    "content": "Verifying Volta Compatibility for Existing Applications  The first step is to check that Volta-compatible device code (at least PTX) is compiled into the application"
  },
  {
    "id": 8073,
    "content": "The following sections show how to accomplish this for applications built with different CUDA Toolkit versions"
  },
  {
    "id": 8077,
    "content": "Applications Using CUDA Toolkit 8 0 or Earlier  CUDA applications built using CUDA Toolkit versions 2"
  },
  {
    "id": 8080,
    "content": "To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from http: www"
  },
  {
    "id": 8083,
    "content": "When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code"
  },
  {
    "id": 8084,
    "content": "If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Volta compatibility Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing"
  },
  {
    "id": 8088,
    "content": "Applications Using CUDA Toolkit 9 0  CUDA applications built using CUDA Toolkit 9 0 are compatible with Volta as long as they are built to include kernels in either Volta-native cubin format (see Building Applications with Volta Support ) or PTX format (see Applications Using CUDA Toolkit 8"
  },
  {
    "id": 8092,
    "content": "Building Applications with Volta Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime"
  },
  {
    "id": 8093,
    "content": "will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it The method used to build your application with either native cubin or at least PTX support for Volta depend on the version of the CUDA Toolkit used The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as PTX"
  },
  {
    "id": 8094,
    "content": "All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application"
  },
  {
    "id": 8095,
    "content": "Especially when using large libraries, this JIT compilation can take a significant amount of time The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code"
  },
  {
    "id": 8100,
    "content": "Applications Using CUDA Toolkit 8 0 or Earlier  The compilers included in CUDA Toolkit 8 0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to the Volta architecture To allow support for Volta and future architectures when using version 8 0 or earlier of the CUDA Toolkit, the compiler must generate a"
  },
  {
    "id": 8104,
    "content": "The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version The code= clause specifies the back-end compilation target and can either be cubin or PTX or both Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Volta compatibility"
  },
  {
    "id": 8106,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8109,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel"
  },
  {
    "id": 8111,
    "content": "cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a"
  },
  {
    "id": 8112,
    "content": "PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly"
  },
  {
    "id": 8118,
    "content": "0 of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7"
  },
  {
    "id": 8121,
    "content": "0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 8122,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8125,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -O2 -o mykernel"
  },
  {
    "id": 8128,
    "content": "Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures"
  },
  {
    "id": 8133,
    "content": "Independent Thread Scheduling Compatibility  The Volta architecture introduces Independent Thread Scheduling among threads in a warp"
  },
  {
    "id": 8134,
    "content": "If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures"
  },
  {
    "id": 8137,
    "content": "To aid migration Volta developers can opt-in to the Pascal scheduling model with the following combination of compiler options"
  },
  {
    "id": 8143,
    "content": "1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide"
  },
  {
    "id": 8147,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8148,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8150,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8151,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8152,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8153,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8154,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8155,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8156,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8157,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8158,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8159,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8160,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8167,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8169,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8170,
    "content": "1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization"
  },
  {
    "id": 8171,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 8176,
    "content": "Turing Compatibility v12 5 | PDF | Archive Turing Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Turing GPUs About this Document  This application note, Turing Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on GPUs based on the NVIDIA ® Turing Architecture This"
  },
  {
    "id": 8177,
    "content": "document provides guidance to developers who are already familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Turing"
  },
  {
    "id": 8180,
    "content": "Application Compatibility on Turing  The NVIDIA CUDA C++ compiler, nvcc , can be used to generate both architecture-specific cubin files and forward-compatible PTX versions of each kernel Each cubin file targets a specific compute-capability version and is forward-compatible only with GPU architectures of the same major version number For example, cubin files that target compute capability 3 0"
  },
  {
    "id": 8181,
    "content": "are supported on all compute-capability 3 x (Kepler) devices but are not supported on compute-capability 5"
  },
  {
    "id": 8184,
    "content": "For this reason, to ensure forward compatibility with GPU architectures introduced after the application has been released, it is recommended that all applications include PTX versions of their kernels Note CUDA Runtime applications containing both cubin and PTX code for a given architecture will automatically use the cubin by default, keeping the PTX path strictly for forward-compatibility"
  },
  {
    "id": 8185,
    "content": "purposes Applications that already include PTX versions of their kernels should work as-is on Turing-based GPUs Applications that only support specific GPU architectures via cubin files, however, will need to be updated to provide Turing-compatible PTX or cubins"
  },
  {
    "id": 8188,
    "content": "Compatibility between Volta and Turing  The Turing architecture is based on Volta’s Instruction Set Architecture ISA 7"
  },
  {
    "id": 8190,
    "content": "As a consequence, any binary that runs on Volta will be able to run on Turing (forward compatibility), but a Turing binary will not be able to run on Volta Please note that Volta kernels using more than 64KB of shared memory (via the explicit opt-in, see CUDA C++ Programming Guide ) will not be able to launch on Turing, as they would exceed Turing’s shared memory capacity Most applications"
  },
  {
    "id": 8191,
    "content": "compiled for Volta should run efficiently on Turing, except if the application uses heavily the Tensor Cores, or if recompiling would allow use of new Turing-specific instructions Recompiling explicitly for Turing is thus recommended"
  },
  {
    "id": 8194,
    "content": "Verifying Turing Compatibility for Existing Applications  The first step is to check that Turing-compatible device code (at least PTX) is compiled into the application"
  },
  {
    "id": 8195,
    "content": "The following sections show how to accomplish this for applications built with different CUDA Toolkit versions"
  },
  {
    "id": 8199,
    "content": "Applications Using CUDA Toolkit 8 0 or Earlier  CUDA applications built using CUDA Toolkit versions 2"
  },
  {
    "id": 8202,
    "content": "To test that PTX JIT is working for your application, you can do the following: Download and install the latest driver from https: www"
  },
  {
    "id": 8205,
    "content": "When starting a CUDA application for the first time with the above environment flag, the CUDA driver will JIT-compile the PTX for each CUDA kernel that is used into native cubin code"
  },
  {
    "id": 8206,
    "content": "If you set the environment variable above and then launch your program and it works properly, then you have successfully verified Turing compatibility Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable when you are done testing"
  },
  {
    "id": 8210,
    "content": "Applications Using CUDA Toolkit 9 x  CUDA applications built using CUDA Toolkit 9 x are compatible with Turing as long as they are built to include kernels in either Volta-native cubin format (see Compatibility between Volta and Turing ) or PTX format (see Applications Using CUDA Toolkit 8"
  },
  {
    "id": 8215,
    "content": "Applications Using CUDA Toolkit 10 0  CUDA applications built using CUDA Toolkit 10 0 are compatible with Turing as long as they are built to include kernels in Volta-native or Turing-native cubin format (see Compatibility between Volta and Turing ), or PTX format (see Applications Using CUDA Toolkit 8"
  },
  {
    "id": 8219,
    "content": "Building Applications with Turing Support  When a CUDA application launches a kernel, the CUDA Runtime determines the compute capability of each GPU in the system and uses this information to automatically find the best matching cubin or PTX version of the kernel that is available If a cubin file supporting the architecture of the target GPU is available, it is used; otherwise, the CUDA Runtime"
  },
  {
    "id": 8220,
    "content": "will load the PTX and JIT-compile that PTX to the GPU’s native cubin format before launching it The method used to build your application with either native cubin or at least PTX support for Turing depend on the version of the CUDA Toolkit used The main advantages of providing native cubins are as follows: It saves the end user the time it takes to JIT-compile kernels that are available only as"
  },
  {
    "id": 8221,
    "content": "PTX All kernels compiled into the application must have native binaries at load time or else they will be built just-in-time from PTX, including kernels from all libraries linked to the application, even if those kernels are never launched by the application"
  },
  {
    "id": 8222,
    "content": "Especially when using large libraries, this JIT compilation can take a significant amount of time The CUDA driver will cache the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a given user, but it is time best avoided whenever possible PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled code"
  },
  {
    "id": 8227,
    "content": "Applications Using CUDA Toolkit 8 0 or Earlier  The compilers included in CUDA Toolkit 8 0 or earlier generate cubin files native to earlier NVIDIA architectures such as Maxwell and Pascal, but they cannot generate cubin files native to Volta or Turing architecture To allow support for Volta, Turing and future architectures when using version 8 0 or earlier of the CUDA Toolkit, the compiler must"
  },
  {
    "id": 8231,
    "content": "The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version The code= clause specifies the back-end compilation target and can either be cubin or PTX or both Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one must be PTX to provide Turing compatibility"
  },
  {
    "id": 8233,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8236,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_61,code=compute_61 -O2 -o mykernel"
  },
  {
    "id": 8238,
    "content": "cu Alternatively, you may be familiar with the simplified nvcc command-line option -arch=sm_XX , which is a shorthand equivalent to the following more explicit -gencode= command-line options used above -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a"
  },
  {
    "id": 8239,
    "content": "PTX back-end target by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly"
  },
  {
    "id": 8245,
    "content": "x of the CUDA Toolkit, nvcc can generate cubin files native to the Volta architecture (compute capability 7"
  },
  {
    "id": 8248,
    "content": "x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 8249,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8252,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_50,code=sm_50 -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -O2 -o mykernel"
  },
  {
    "id": 8255,
    "content": "Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures"
  },
  {
    "id": 8260,
    "content": "Applications Using CUDA Toolkit 10 0  With version 10 0 of the CUDA Toolkit, nvcc can generate cubin files native to the Turing architecture (compute capability 7"
  },
  {
    "id": 8263,
    "content": "0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below"
  },
  {
    "id": 8264,
    "content": "If the developer made assumptions about warp-synchronicity, 1 this feature can alter the set of threads participating in the executed code compared to previous architectures"
  },
  {
    "id": 8267,
    "content": "To aid migration Volta and Turing developers can opt-in to the Pascal scheduling model with the following combination of compiler options"
  },
  {
    "id": 8273,
    "content": "1 Use CUDA C++ instead of CUDA C/C++ Updated references to the CUDA C++ Programming Guide and CUDA C++ Best Practices Guide"
  },
  {
    "id": 8277,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8278,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8280,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8281,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8282,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8283,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8284,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8285,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8286,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8287,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8288,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8289,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8290,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8297,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8299,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8300,
    "content": "1 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization"
  },
  {
    "id": 8301,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024, NVIDIA Corporation"
  },
  {
    "id": 8306,
    "content": "NVIDIA Ampere GPU Architecture Compatibility v12 5 | PDF | Archive NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for GPUs based on the NVIDIA Ampere GPU Architecture About this Document  This application note, NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that"
  },
  {
    "id": 8307,
    "content": "their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Ampere Architecture based GPUs This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ampere GPU architecture"
  },
  {
    "id": 8310,
    "content": "Application Compatibility on the NVIDIA Ampere GPU Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel"
  },
  {
    "id": 8311,
    "content": "A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability For example, a cubin generated for compute capability 7 0 is supported to run on a GPU with compute capability 7 5, however a cubin generated for compute capability 7 5 is not supported to run on a GPU with compute capability 7 0,"
  },
  {
    "id": 8312,
    "content": "and a cubin generated with compute capability 7 x is not supported to run on a GPU with compute capability 8"
  },
  {
    "id": 8314,
    "content": "At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX For example, PTX code generated for compute capability 7 x is supported to run on compute capability 7 x or any higher revision (major or minor), including compute"
  },
  {
    "id": 8317,
    "content": "Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility"
  },
  {
    "id": 8318,
    "content": "To read more about cubin and PTX compatibilities see Compilation with NVCC from the Programming Guide"
  },
  {
    "id": 8319,
    "content": "When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel If a cubin compatible with that GPU is present in the binary, the cubin is used as-is for execution Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the"
  },
  {
    "id": 8320,
    "content": "PTX and then the cubin is used for the execution Application binaries that include PTX version of kernels, should work as-is on the NVIDIA Ampere architecture based GPUs However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the NVIDIA Ampere architecture based GPUs To know more about building compatible applications read Building Applications"
  },
  {
    "id": 8324,
    "content": "Verifying Ampere Compatibility for Existing Applications  The first step towards making a CUDA application compatible with the NVIDIA Ampere GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX) The following sections explain how to accomplish this for an already built CUDA application"
  },
  {
    "id": 8328,
    "content": "Applications Built Using CUDA Toolkit 10 2 or Earlier  CUDA applications built using CUDA Toolkit versions 2 1 through 10 2 are compatible with NVIDIA Ampere architecture based GPUs as long as they are built to include PTX versions of their kernels"
  },
  {
    "id": 8329,
    "content": "This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https: www"
  },
  {
    "id": 8332,
    "content": "This means the application is not compatible with the NVIDIA Ampere GPU architecture and needs to be rebuilt for compatibility On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ampere GPU architecture Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done"
  },
  {
    "id": 8337,
    "content": "0 are compatible with the NVIDIA Ampere GPU architecture as long as they are built to include kernels in native cubin (compute capability 8"
  },
  {
    "id": 8341,
    "content": "Building Applications with the NVIDIA Ampere GPU Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ampere GPU architecture Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile"
  },
  {
    "id": 8342,
    "content": "kernels that are available only as PTX All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application"
  },
  {
    "id": 8343,
    "content": "Especially when using large libraries, this JIT compilation can take a significant amount of time The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be"
  },
  {
    "id": 8350,
    "content": "2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7"
  },
  {
    "id": 8353,
    "content": "x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 8354,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8357,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel"
  },
  {
    "id": 8359,
    "content": "cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used It is a shorthand equivalent to the following more explicit -gencode= command-line options used above -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target"
  },
  {
    "id": 8360,
    "content": "binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly"
  },
  {
    "id": 8362,
    "content": "0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9"
  },
  {
    "id": 8364,
    "content": "The final -gencode to generate PTX would also need to be update – for further information and examples see the documentation for the specific CUDA toolkit version The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version The code= clause specifies the back-end compilation target and can either be cubin or PTX or both"
  },
  {
    "id": 8365,
    "content": "Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility with future architectures"
  },
  {
    "id": 8369,
    "content": "Building Applications Using CUDA Toolkit 11 0  With versions 11 0 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ampere GPU architecture (compute capability 8"
  },
  {
    "id": 8372,
    "content": "0, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below"
  },
  {
    "id": 8373,
    "content": "If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures"
  },
  {
    "id": 8376,
    "content": "To aid migration to the NVIDIA Ampere GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options"
  },
  {
    "id": 8381,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8382,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8384,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8385,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8386,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8387,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8388,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8389,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8390,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8391,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8392,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8393,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8394,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8401,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8403,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8404,
    "content": "1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization"
  },
  {
    "id": 8405,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 8410,
    "content": "Hopper Architecture Compatibility v12 5 | PDF | Archive Hopper Compatibility Guide for CUDA Applications The guide to building CUDA applications for Hopper GPUs 1 About this Document  This application note, Hopper Architecture Compatibility Guide for CUDA Applications, is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run on the NVIDIA ® Hopper architecture based"
  },
  {
    "id": 8411,
    "content": "GPUs This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with Hopper architecture"
  },
  {
    "id": 8414,
    "content": "Application Compatibility on Hopper Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel"
  },
  {
    "id": 8415,
    "content": "A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability For example, a cubin generated for compute capability 8 0 is supported to run on a GPU with compute capability 8 6, however a cubin generated for compute capability 8 6 is not supported to run on a GPU with compute capability 8 0,"
  },
  {
    "id": 8416,
    "content": "and a cubin generated with compute capability 8 x is not supported to run on a GPU with compute capability 9"
  },
  {
    "id": 8418,
    "content": "At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX For example, PTX code generated for compute capability 8 x is supported to run on compute capability 8 x or any higher revision (major or minor), including compute"
  },
  {
    "id": 8421,
    "content": "Therefore although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility"
  },
  {
    "id": 8422,
    "content": "To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel If a cubin compatible with that GPU is present in the binary, the cubin"
  },
  {
    "id": 8423,
    "content": "is used as-is for execution Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution Application binaries that include PTX version of kernels, should work as-is on the Hopper GPUs However application binaries which do not include PTX (only include cubins), need to be rebuilt to run on the Hopper GPUs To know more about"
  },
  {
    "id": 8424,
    "content": "building compatible applications read Building Applications with Hopper Architecture Support Application binaries that include PTX version of kernels with architecture conditional features using sm_90a or compute_90a in order to take full advantage of Hopper GPU architecture, are not forward or backward compatible"
  },
  {
    "id": 8427,
    "content": "Verifying Hopper Compatibility for Existing Applications  The first step towards making a CUDA application compatible with Hopper architecture is to check if the application binary already contains compatible GPU code (at least the PTX) The following sections explain how to accomplish this for an already built CUDA application"
  },
  {
    "id": 8431,
    "content": "Applications Built Using CUDA Toolkit 11 7 or Earlier  CUDA applications built using CUDA Toolkit versions 2 1 through 11 7 are compatible with Hopper GPUs as long as they are built to include PTX versions of their kernels"
  },
  {
    "id": 8432,
    "content": "This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https: www"
  },
  {
    "id": 8435,
    "content": "This means the application is not Hopper architecture compatible and needs to be rebuilt for compatibility On the other hand, if the application works properly with this environment variable set, then the application is Hopper compatible Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done"
  },
  {
    "id": 8440,
    "content": "8 are compatible with Hopper architecture as long as they are built to include kernels in native cubin (compute capability 9"
  },
  {
    "id": 8444,
    "content": "Building Applications with Hopper Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the Hopper architecture Although it is enough to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile kernels that are available"
  },
  {
    "id": 8445,
    "content": "only as PTX All kernels which do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application 2"
  },
  {
    "id": 8446,
    "content": "Especially when using large libraries, this JIT compilation can take a significant amount of time The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be"
  },
  {
    "id": 8447,
    "content": "faster or of greater accuracy PTX code compiled to target architecture conditional features using sm_90a or compute_90a only runs on devices with compute capability 9"
  },
  {
    "id": 8454,
    "content": "7) of the CUDA Toolkit can generate cubins native to the NVIDIA Ampere GPU architectures (compute capability 8"
  },
  {
    "id": 8457,
    "content": "7 or earlier, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 8458,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8461,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -O2 -o mykernel"
  },
  {
    "id": 8463,
    "content": "cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used It is a shorthand equivalent to the following more explicit -gencode= command-line options used above -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target"
  },
  {
    "id": 8464,
    "content": "binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly"
  },
  {
    "id": 8466,
    "content": "0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10"
  },
  {
    "id": 8469,
    "content": "The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version The code= clause specifies the back-end compilation target and can either be cubin or PTX or both Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility"
  },
  {
    "id": 8474,
    "content": "Building Applications Using CUDA Toolkit 11 8  With versions 11 8 of the CUDA Toolkit, nvcc can generate cubin native to the Hopper architecture (compute capability 9"
  },
  {
    "id": 8477,
    "content": "8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below"
  },
  {
    "id": 8478,
    "content": "If the developer made assumptions about warp-synchronicity 3 , this feature can alter the set of threads participating in the executed code compared to previous architectures"
  },
  {
    "id": 8481,
    "content": "To aid migration to the Hopper architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options"
  },
  {
    "id": 8486,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8487,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8489,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8490,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8491,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8492,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8493,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8494,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8495,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8496,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8497,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8498,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8499,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8506,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8508,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8511,
    "content": "3 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization"
  },
  {
    "id": 8512,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 8517,
    "content": "NVIDIA Ada GPU Architecture Compatibility v12 5 | PDF | Archive NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications The guide to building CUDA applications for NVIDIA Ada GPUs About this Document  This application note, NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications , is intended to help developers ensure that their NVIDIA ® CUDA ® applications will run"
  },
  {
    "id": 8518,
    "content": "on the NVIDIA ® Ada Architecture based GPUs This document provides guidance to developers who are familiar with programming in CUDA C++ and want to make sure that their software applications are compatible with the NVIDIA Ada GPU architecture"
  },
  {
    "id": 8521,
    "content": "Application Compatibility on the NVIDIA Ada GPU Architecture  A CUDA application binary (with one or more GPU kernels) can contain the compiled GPU code in two forms, binary cubin objects and forward-compatible PTX assembly for each kernel"
  },
  {
    "id": 8522,
    "content": "A cubin generated for a certain compute capability is supported to run on any GPU with the same major revision and same or higher minor revision of compute capability For example, a cubin generated for compute capability 8 6 is supported to run on a GPU with compute capability 8 9; however, a cubin generated for compute capability 8 9 is not supported to run on a GPU with compute capability 8 6,"
  },
  {
    "id": 8523,
    "content": "and a cubin generated with compute capability 8 x is not supported to run on a GPU with compute capability 9"
  },
  {
    "id": 8525,
    "content": "At the application load time, PTX is compiled to cubin and the cubin is used for kernel execution Meaning PTX is supported to run on any GPU with compute capability higher than the compute capability assumed for generation of that PTX For example, PTX code generated for compute capability 8 x is supported to run on compute capability 8 x or any higher revision (major or minor), including compute"
  },
  {
    "id": 8528,
    "content": "Therefore, although it is optional, it is recommended that all applications should include PTX of the kernels to ensure forward-compatibility"
  },
  {
    "id": 8529,
    "content": "To read more about cubin and PTX compatibilities see Compilation with NVCC from the CUDA C++ Programming Guide When a CUDA application launches a kernel on a GPU, the CUDA Runtime determines the compute capability of the GPU in the system and uses this information to find the best matching cubin or PTX version of the kernel If a cubin compatible with that GPU is present in the binary, the cubin"
  },
  {
    "id": 8530,
    "content": "is used as-is for execution Otherwise, the CUDA Runtime first generates compatible cubin by JIT-compiling 1 the PTX and then the cubin is used for the execution Application binaries that include PTX version of kernels should work as-is on the NVIDIA Ada architecture based GPUs However, application binaries that do not include PTX (only include cubins) need to be rebuilt to run on the NVIDIA Ada"
  },
  {
    "id": 8531,
    "content": "architecture based GPUs To know more about building compatible applications, read Building Applications with the NVIDIA Ada GPU Architecture Support"
  },
  {
    "id": 8534,
    "content": "Compatibility between Ampere and Ada  The NVIDIA Ada architecture is based on Ampere’s Instruction Set Architecture ISA 8"
  },
  {
    "id": 8536,
    "content": "As a consequence, any binary that runs on Ampere will be able to run on Ada (forward compatibility), but an Ada binary will not be able to run on Ampere"
  },
  {
    "id": 8539,
    "content": "Verifying Ada Compatibility for Existing Applications  The first step towards making a CUDA application compatible with the NVIDIA Ada GPU architecture is to check if the application binary already contains compatible GPU code (at least the PTX) The following sections explain how to accomplish this for an already built CUDA application"
  },
  {
    "id": 8543,
    "content": "Applications Built Using CUDA Toolkit 10 2 or Earlier  CUDA applications built using CUDA Toolkit versions 2 1 through 10 2 are compatible with NVIDIA Ada architecture based GPUs as long as they are built to include PTX versions of their kernels"
  },
  {
    "id": 8544,
    "content": "This can be tested by forcing the PTX to JIT-compile at application load time with following the steps: Download and install the latest driver from https: www"
  },
  {
    "id": 8547,
    "content": "This means the application is not compatible with the NVIDIA Ada GPU architecture and needs to be rebuilt for compatibility On the other hand, if the application works properly with this environment variable set, then the application is compatible with the NVIDIA Ada GPU architecture Note Be sure to unset the CUDA_FORCE_PTX_JIT environment variable after testing is done"
  },
  {
    "id": 8551,
    "content": "Applications Built Using CUDA Toolkit 11 0 through 11 7  CUDA applications built using CUDA Toolkit 11 0 through 11 7 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native cubin (see Compatibility between Ampere and Ada ) or PTX format (see Applications Built Using CUDA Toolkit 10"
  },
  {
    "id": 8556,
    "content": "Applications Built Using CUDA Toolkit 11 8  CUDA applications built using CUDA Toolkit 11 8 are compatible with the NVIDIA Ada GPU architecture as long as they are built to include kernels in Ampere-native or Ada-native cubin (see Compatibility between Ampere and Ada ), or PTX format (see Applications Built Using CUDA Toolkit 10"
  },
  {
    "id": 8560,
    "content": "Building Applications with the NVIDIA Ada GPU Architecture Support  Depending on the version of the CUDA Toolkit used for building the application, it can be built to include PTX and/or native cubin for the NVIDIA Ada GPU architecture Although it is sufficient to just include PTX, including native cubin also has the following advantages: It saves the end user the time it takes to JIT-compile"
  },
  {
    "id": 8561,
    "content": "kernels that are available only as PTX All kernels that do not have native cubins are JIT-compiled from PTX, including kernels from all the libraries linked to the application, even if those kernels are never launched by the application"
  },
  {
    "id": 8562,
    "content": "Especially when using large libraries, this JIT compilation can take a significant amount of time The CUDA driver caches the cubins generated as a result of the PTX JIT, so this is mostly a one-time cost for a user, but it is time best avoided whenever possible PTX JIT-compiled kernels often cannot take advantage of architectural features of newer GPUs, meaning that native-compiled cubins may be"
  },
  {
    "id": 8569,
    "content": "2) of the CUDA Toolkit can generate cubins native to the Volta and Turing architectures (compute capability 7"
  },
  {
    "id": 8572,
    "content": "x, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 8573,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8576,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -O2 -o mykernel"
  },
  {
    "id": 8578,
    "content": "cu Alternatively, the simplified nvcc command-line option -arch=sm_XX can be used It is a shorthand equivalent to the following more explicit -gencode= command-line options used above -arch=sm_XX expands to the following: -gencode=arch=compute_XX,code=sm_XX -gencode=arch=compute_XX,code=compute_XX However, while the -arch=sm_XX command-line option does result in inclusion of a PTX back-end target"
  },
  {
    "id": 8579,
    "content": "binary by default, it can only specify a single target cubin architecture at a time, and it is not possible to use multiple -arch= options on the same nvcc command line, which is why the examples above use -gencode= explicitly"
  },
  {
    "id": 8581,
    "content": "0, one or more of the -gencode options will need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 9"
  },
  {
    "id": 8584,
    "content": "The arch= clause of the -gencode= command-line option to nvcc specifies the front-end compilation target and must always be a PTX version The code= clause specifies the back-end compilation target and can either be cubin or PTX, or both Only the back-end target version(s) specified by the code= clause will be retained in the resulting binary; at least one should be PTX to provide compatibility"
  },
  {
    "id": 8589,
    "content": "Building Applications Using CUDA Toolkit 11 0 through 11 7  The nvcc compiler included with versions 11 0 through 11 7 of the CUDA Toolkit can generate cubins native to the Ampere architecture (compute capability 8"
  },
  {
    "id": 8593,
    "content": "7, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below Windows nvcc"
  },
  {
    "id": 8594,
    "content": "exe -ccbin \"C:\\vs2010\\VC\\bin\" -Xcompiler \"/EHsc /W3 /nologo /O2 /Zi /MT\" -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 --compile -o \"Release\\mykernel"
  },
  {
    "id": 8597,
    "content": "cu\" Mac/Linux /usr/local/cuda/bin/nvcc -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -O2 -o mykernel"
  },
  {
    "id": 8601,
    "content": "0, one or more of the -gencode options need to be removed according to the architectures supported by the specific toolkit version (for example, CUDA toolkit 10"
  },
  {
    "id": 8607,
    "content": "Building Applications Using CUDA Toolkit 11 8  With version 11 8 of the CUDA Toolkit, nvcc can generate cubin native to the NVIDIA Ada GPU architecture (compute capability 8"
  },
  {
    "id": 8610,
    "content": "8, to ensure that nvcc will generate cubin files for all recent GPU architectures as well as a PTX version for forward compatibility with future GPU architectures, specify the appropriate -gencode= parameters on the nvcc command line as shown in the examples below"
  },
  {
    "id": 8611,
    "content": "If the developer made assumptions about warp-synchronicity 2 , this feature can alter the set of threads participating in the executed code compared to previous architectures"
  },
  {
    "id": 8614,
    "content": "To aid migration to the NVIDIA Ada GPU architecture, developers can opt-in to the Pascal scheduling model with the following combination of compiler options"
  },
  {
    "id": 8619,
    "content": "1 Just-in-time compilation 2 Warp-synchronous refers to an assumption that threads in the same warp are synchronized at every instruction and can, for example, communicate values without explicit synchronization"
  },
  {
    "id": 8623,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8624,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8626,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8627,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8628,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8629,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8630,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8631,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8632,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8633,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8634,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8635,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8636,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8643,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8645,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8646,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 8651,
    "content": "Maxwell Tuning Guide v12 5 | PDF | Archive Tuning CUDA Applications for Maxwell The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Maxwell Architecture NVIDIA Maxwell Compute Architecture  Maxwell is NVIDIA’s next-generation architecture for CUDA compute applications Maxwell retains and extends the same CUDA programming model as in previous NVIDIA architectures such"
  },
  {
    "id": 8652,
    "content": "as Fermi and Kepler, and applications that follow the best practices for those architectures should typically see speedups on the Maxwell architecture without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features 1 Maxwell introduces an all-new design for the Streaming Multiprocessor ( SM )"
  },
  {
    "id": 8653,
    "content": "that dramatically improves energy efficiency Although the Kepler SMX design was extremely efficient for its generation, through its development, NVIDIA’s GPU architects saw an opportunity for another big leap forward in architectural efficiency; the Maxwell SM is the realization of that vision Improvements to control logic partitioning, workload balancing, clock-gating granularity, compiler-based"
  },
  {
    "id": 8654,
    "content": "scheduling, number of instructions issued per clock cycle, and many other enhancements allow the Maxwell SM (also called SMM ) to far exceed Kepler SMX efficiency"
  },
  {
    "id": 8655,
    "content": "The first Maxwell-based GPU is codenamed GM107 and is designed for use in power-limited environments like notebooks and small form factor (SFF) PCs GM107 is described in a whitepaper entitled NVIDIA GeForce GTX 750 Ti: Featuring First-Generation Maxwell GPU Technology, Designed for Extreme Performance per Watt Second-generation Maxwell GPUs retain the power efficiency of the earlier generation"
  },
  {
    "id": 8656,
    "content": "while delivering significantly higher performance GM204 is described in a whitepaper entitled NVIDIA GeForce GTX 980: Featuring Maxwell, The Most Advanced GPU Ever Made"
  },
  {
    "id": 8657,
    "content": "Compute programming features of GM204 are similar to those of GM107, except where explicitly noted in this guide For details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide"
  },
  {
    "id": 8660,
    "content": "CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures Programmers must primarily focus on following those recommendations to achieve the best performance"
  },
  {
    "id": 8661,
    "content": "The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads"
  },
  {
    "id": 8665,
    "content": "Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Maxwell Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Maxwell"
  },
  {
    "id": 8666,
    "content": "SMM  The Maxwell Streaming Multiprocessor, SMM, is similar in many respects to the Kepler architecture’s SMX The key enhancements of SMM over SMX are geared toward improving efficiency without requiring significant increases in available parallelism per SM from the application"
  },
  {
    "id": 8673,
    "content": ", 64), and factors influencing warp occupancy remain similar or improved over SMX: The register file size (64k 32-bit registers) is the same as that of SMX"
  },
  {
    "id": 8674,
    "content": "As with Kepler, experimentation should be used to determine the optimum balance of register spilling vs"
  },
  {
    "id": 8675,
    "content": "This should result in an automatic occupancy improvement for kernels with small thread blocks of 64 or fewer threads (shared memory and register file resource requirements permitting)"
  },
  {
    "id": 8676,
    "content": "As such, developers can expect similar or improved occupancy on SMM without changes to their application At the same time, warp occupancy requirements (i"
  },
  {
    "id": 8678,
    "content": ", available parallelism) for maximum device utilization are similar to or less than those of SMX (see Instruction Latencies )"
  },
  {
    "id": 8683,
    "content": "Instruction Scheduling  The number of CUDA Cores per SM has been reduced to a power of two, however with Maxwell’s improved execution efficiency, performance per SM is usually within 10% of Kepler performance, and the improved area efficiency of SMM means CUDA Cores per GPU will be substantially higher vs SMM retains the same number of instruction issue slots per clock and reduces arithmetic"
  },
  {
    "id": 8685,
    "content": "Unlike SMX, however, all SMM core functional units are assigned to a particular scheduler, with no shared units"
  },
  {
    "id": 8686,
    "content": "Along with the selection of a power-of-two number of CUDA Cores per SM, which simplifies scheduling and reduces stall cycles, this partitioning of SM computational resources in SMM is a major component of the streamlined efficiency of SMM The power-of-two number of CUDA Cores per partition simplifies scheduling, as each of SMM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the"
  },
  {
    "id": 8687,
    "content": "warp width Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores"
  },
  {
    "id": 8692,
    "content": "Instruction Latencies  Another major improvement of SMM is that dependent math latencies have been significantly reduced; a consequence of this is a further reduction of stall cycles, as the available warp-level parallelism (i"
  },
  {
    "id": 8694,
    "content": ", occupancy) on SMM should be equal to or greater than that of SMX (see Occupancy ), while at the same time each math operation takes less time to complete, improving utilization and throughput"
  },
  {
    "id": 8699,
    "content": "Instruction Throughput  The most significant changes to peak instruction throughputs in SMM are as follows: The change in number of CUDA Cores per SM brings with it a corresponding change in peak single-precision floating point operations per clock per SM However, since the number of SMs is typically increased, the result is an increase in aggregate peak throughput; furthermore, the scheduling"
  },
  {
    "id": 8701,
    "content": "The throughput of many integer operations including multiply, logical operations and shift is improved"
  },
  {
    "id": 8703,
    "content": "Note As was already the recommended best practice, signed arithmetic should be preferred over unsigned arithmetic wherever possible for best throughput on SMM"
  },
  {
    "id": 8704,
    "content": "The C language standard places more restrictions on overflow behavior for unsigned math, limiting compiler optimization opportunities"
  },
  {
    "id": 8712,
    "content": "Unified L1/Texture Cache  Maxwell combines the functionality of the L1 and texture caches into a single unit"
  },
  {
    "id": 8713,
    "content": "As with Kepler, global loads in Maxwell are cached in L2 only, unless using the LDG read-only data cache mechanism introduced in Kepler In a manner similar to Kepler GK110B, GM204 retains this behavior by default but also allows applications to opt-in to caching of global loads in its unified L1/Texture cache The opt-in mechanism is the same as with GK110B: pass the -Xptxas -dlcm=ca flag to nvcc"
  },
  {
    "id": 8714,
    "content": "at compile time Local loads also are cached in L2 only, which could increase the cost of register spilling if L1 local load hit rates were high with Kepler"
  },
  {
    "id": 8715,
    "content": "The balance of occupancy versus spilling should therefore be reevaluated to ensure best performance Especially given the improvements to arithmetic latencies, code built for Maxwell may benefit from somewhat lower occupancy (due to increased registers per thread) in exchange for lower spilling"
  },
  {
    "id": 8716,
    "content": "The unified L1/texture cache acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp"
  },
  {
    "id": 8719,
    "content": "Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process"
  },
  {
    "id": 8720,
    "content": "If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed"
  },
  {
    "id": 8721,
    "content": "Shared Memory Capacity  With Fermi and Kepler, shared memory and the L1 cache shared the same on-chip storage Maxwell, by contrast, provides dedicated space to the shared memory of each SMM, since the functionality of the L1 and texture caches have been merged in SMM This increases the shared memory space available per SMM as compared to SMX: GM107 provides 64 KB shared memory per SMM, and GM204"
  },
  {
    "id": 8722,
    "content": "further increases this to 96 KB shared memory per SMM This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e"
  },
  {
    "id": 8724,
    "content": ", radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count"
  },
  {
    "id": 8725,
    "content": "Applications no longer need to select a preference of the L1/shared split for optimal performance For purposes of backward compatibility with Fermi and Kepler, applications may optionally continue to specify such a preference, but the preference will be ignored on Maxwell, with the full 64 KB per SMM always going to shared memory Note While the per-SM shared memory capacity is increased in SMM,"
  },
  {
    "id": 8726,
    "content": "the per-thread-block limit remains 48 KB For maximum flexibility on possible future GPUs, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block, which would for example allow at least two such thread blocks to fit per SMM"
  },
  {
    "id": 8731,
    "content": "Shared Memory Bandwidth  Kepler SMX introduced an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM over Fermi for shared memory accesses of 8 or 16 bytes However, applications could only benefit from this when storing these larger elements in shared memory (i"
  },
  {
    "id": 8733,
    "content": ", integers and fp32 values saw no benefit), and only when the developer explicitly opted into the 8-byte bank mode via the API"
  },
  {
    "id": 8734,
    "content": "To simplify this, Maxwell returns to the Fermi style of shared memory banking, where banks are always four bytes wide"
  },
  {
    "id": 8735,
    "content": "Aggregate shared memory bandwidth across the chip remains comparable to that of corresponding Kepler chips, given increased SM count"
  },
  {
    "id": 8736,
    "content": "In this way, all applications using shared memory can now benefit from the higher bandwidth, even when storing only four-byte items into shared memory and without specifying any particular preference via the API"
  },
  {
    "id": 8741,
    "content": "Fast Shared Memory Atomics  Kepler introduced a dramatically higher throughput for atomic operations to global memory as compared to Fermi However, atomic operations to shared memory remained essentially unchanged: both architectures implemented shared memory atomics using a lock/update/unlock pattern that could be expensive in the case of high contention for updates to particular locations in"
  },
  {
    "id": 8742,
    "content": "shared memory Maxwell improves upon this by implementing native shared memory atomic operations for 32-bit integers and native shared memory 32-bit and 64-bit compare-and-swap (CAS), which can be used to implement other atomic functions with reduced overhead compared to the Fermi and Kepler methods"
  },
  {
    "id": 8743,
    "content": "Note Refer to the CUDA C++ Programming Guide for an example implementation of an fp64 atomicAdd() using atomicCAS()"
  },
  {
    "id": 8747,
    "content": "Dynamic Parallelism  GK110 introduced a new architectural feature called Dynamic Parallelism, which allows the GPU to create additional work for itself"
  },
  {
    "id": 8750,
    "content": "SMM brings Dynamic Parallelism into the mainstream by supporting it across the product line, even in lower-power chips such as GM107"
  },
  {
    "id": 8751,
    "content": "This will benefit developers, as it means that applications will no longer need special-case algorithm implementations for high-end GPUs that differ from those usable in more power-constrained environments"
  },
  {
    "id": 8762,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8763,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8765,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8766,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8767,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8768,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8769,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8770,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8771,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8772,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8773,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8774,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8775,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8782,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8784,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8785,
    "content": "1 Throughout this guide, Fermi refers to devices of compute capability 2 x, Kepler refers to devices of compute capability 3 x, and Maxwell refers to devices of compute capability 5"
  },
  {
    "id": 8787,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 8792,
    "content": "Pascal Tuning Guide v12 5 | PDF | Archive Tuning CUDA Applications for Pascal The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Pascal Architecture NVIDIA Pascal Compute Architecture  Pascal retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell, and applications that follow the best practices for those"
  },
  {
    "id": 8793,
    "content": "architectures should typically see speedups on the Pascal architecture without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Pascal architectural features"
  },
  {
    "id": 8794,
    "content": "2 A detailed overview of the major improvements in GP100 and GP104 over earlier NVIDIA architectures are described in a pair of white papers entitled NVIDIA Tesla P100: The Most Advanced Datacenter Accelerator Ever Built for GP100 and NVIDIA GeForce GTX 1080: Gaming Perfected for GP104"
  },
  {
    "id": 8795,
    "content": "For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide Some of the Pascal features described in this guide are specific to either GP100 or GP104, as noted; if not specified, features apply to both Pascal variants"
  },
  {
    "id": 8798,
    "content": "CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures Programmers must primarily focus on following those recommendations to achieve the best performance"
  },
  {
    "id": 8799,
    "content": "The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads"
  },
  {
    "id": 8803,
    "content": "Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Pascal Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Pascal Streaming Multiprocessor  The Pascal Streaming Multiprocessor (SM) is in many respects similar to that of Maxwell"
  },
  {
    "id": 8804,
    "content": "Pascal further improves the already excellent power efficiency provided by the Maxwell architecture through both an improved 16-nm FinFET manufacturing process and various architectural modifications"
  },
  {
    "id": 8809,
    "content": "Instruction Scheduling  Like Maxwell, Pascal employs a power-of-two number of CUDA Cores per partition This simplifies scheduling, since each of the SM’s warp schedulers issue to a dedicated set of CUDA Cores equal to the warp width (32) Each warp scheduler still has the flexibility to dual-issue (such as issuing a math operation to a CUDA Core in the same cycle as a memory operation to a"
  },
  {
    "id": 8810,
    "content": "load/store unit), but single-issue is now sufficient to fully utilize all CUDA Cores Like Maxwell, each GP104 SM provides four warp schedulers managing a total of 128 single-precision (FP32) and four double-precision (FP64) cores"
  },
  {
    "id": 8812,
    "content": "The resulting 2:1 ratio of FP32 to FP64 cores aligns well with GP100’s new datapath configuration, allowing Pascal to process FP64 workloads more efficiently than Kepler GK210, the previous NVIDIA architecture to emphasize FP64 performance"
  },
  {
    "id": 8819,
    "content": ", 64), and other factors influencing warp occupancy remain similar as well: The register file size (64k 32-bit registers) is the same as that of Maxwell"
  },
  {
    "id": 8820,
    "content": "As with previous architectures, experimentation should be used to determine the optimum balance of register spilling vs"
  },
  {
    "id": 8821,
    "content": "But each GP100 SM contains fewer CUDA Cores, so the shared memory available per core actually increases on GP100"
  },
  {
    "id": 8822,
    "content": "The maximum shared memory per block remains limited at 48KB as with prior architectures (see Shared Memory Capacity )"
  },
  {
    "id": 8823,
    "content": "As such, developers can expect similar occupancy as on Maxwell without changes to their application As a result of scheduling improvements relative to Kepler, warp occupancy requirements (i"
  },
  {
    "id": 8833,
    "content": "FP16 Arithmetic Support  Pascal provides improved FP16 support for applications, like deep learning, that are tolerant of low floating-point precision"
  },
  {
    "id": 8834,
    "content": "As with Maxwell, FP16 storage can be used to reduce the required memory footprint and bandwidth compared to FP32 or FP64 storage Peak FP16 throughput is attained by using a paired operation to perform two FP16 instructions per core simultaneously To be eligible for the paired operation the operands must be stored in a half2 vector type GP100, designed with training deep neural networks in mind,"
  },
  {
    "id": 8835,
    "content": "provides FP16 throughput up to 2x that of FP32 arithmetic However, compensating for reduced FP16 throughput, GP104 provides additional high-throughput INT8 support not available in GP100"
  },
  {
    "id": 8840,
    "content": "INT8 Dot Product  GP104 provides specialized instructions for two-way and four-way integer dot products The __dp4a intrinsic computes a dot product of four 8-bit integers with accumulation into a 32-bit integer Similarly, __dp2a performs a two-element dot product between two 16-bit integers in one vector, and two 8-bit integers in another with accumulation into a 32-bit integer"
  },
  {
    "id": 8851,
    "content": "The effective width of the memory bus is then 4096 bits, a significant increase over the 384 bits in GM200"
  },
  {
    "id": 8852,
    "content": "Thus, the GP100 equipped Tesla P100 has a peak bandwidth of 732 GB/s with a modest 715 MHz memory clock In order to hide DRAM latencies at full HBM2 bandwidth, more memory accesses must be kept in flight compared to GPUs equipped with traditional GDDR5 Helpfully, the large complement of SMs in GP100 will typically boost the number of concurrent threads (and thus reads-in-flight) compared to"
  },
  {
    "id": 8854,
    "content": "Resource constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread"
  },
  {
    "id": 8855,
    "content": "The GP100 GPU’s register files, shared memories, L1 and L2 caches, and DRAM are all protected by Single-Error Correct Double-Error Detect (SECDED) ECC code When enabling ECC support on a Kepler GK210, the available DRAM would be reduced by 6 25% to allow for the storage of ECC bits Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to"
  },
  {
    "id": 8856,
    "content": "the same GPU with ECC disabled HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection"
  },
  {
    "id": 8861,
    "content": "Unified L1/Texture Cache  Like Maxwell, Pascal combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp"
  },
  {
    "id": 8862,
    "content": "In contrast, GP104 follows Maxwell in caching global loads in L2 only, unless using the LDG read-only data cache mechanism As with previous architectures, GP104 allows the developer to opt-in to caching all global loads in the unified L1/Texture cache by passing the -Xptxas -dlcm=ca flag to nvcc at compile time Kepler serviced loads at a granularity of 128B when L1 caching of global loads was"
  },
  {
    "id": 8863,
    "content": "enabled and 32B otherwise On Pascal the data access unit is 32B regardless of whether global loads are cached in L1 So it is no longer necessary to turn off L1 caching in order to reduce wasted global memory transactions associated with uncoalesced accesses"
  },
  {
    "id": 8864,
    "content": "The balance of occupancy versus spilling should therefore be re-evaluated to ensure best performance"
  },
  {
    "id": 8867,
    "content": "Developers who wish to have separately-tuned paths for various architecture generations can use these fields to simplify the path selection process"
  },
  {
    "id": 8868,
    "content": "If per-thread-block SM resource usage would result in zero occupancy with caching enabled, the CUDA driver will override the caching selection to allow the kernel launch to succeed"
  },
  {
    "id": 8873,
    "content": "Atomic Memory Operations  Like Maxwell, Pascal provides native shared memory atomic operations for 32-bit integer arithmetic, along with native 32 or 64-bit compare-and-swap (CAS)"
  },
  {
    "id": 8874,
    "content": "Developers coming from Kepler, where shared memory atomics were implemented in software using a lock/update/unlock sequence, should see a large performance improvement particularly for heavily contended shared-memory atomics"
  },
  {
    "id": 8875,
    "content": "The atomicAdd() function in CUDA has thus been generalized to support 32 and 64-bit integer and floating-point types The rounding mode for all floating-point atomic operations is round-to-nearest-even in Pascal For GP100 atomic operations may target the memories of peer GPUs connected through NVLink Pascal GPUs provide support system-wide atomic operations targeting migratable allocations 5 If"
  },
  {
    "id": 8876,
    "content": "system-wide atomic visibility is desired, operations targeting migratable memory must specify a system scope by using the atomic[Op]_system() intrinsics 6 atomicAdd() ) on migratable memory remains valid, but enforces atomic visibility only within the local GPU"
  },
  {
    "id": 8877,
    "content": "Note Given the potential for incorrect usage of atomic scopes, it is recommended that applications use compute-sanitizer to detect and eliminate errors"
  },
  {
    "id": 8878,
    "content": "As implemented for Pascal, system-wide atomics are intended to allow developers to experiment with enhanced memory models"
  },
  {
    "id": 8879,
    "content": "When an atomic targets a migratable address backed by a remote memory space, the local processor page-faults so that the kernel can migrate the appropriate memory page to local memory Since the page is now locally resident, subsequent atomics from the same processor will not result in additional page-faults However, atomic updates from different processors can incur frequent page-faults"
  },
  {
    "id": 8887,
    "content": "Shared Memory Capacity  For Kepler, shared memory and the L1 cache shared the same on-chip storage Maxwell and Pascal, by contrast, provide dedicated space to the shared memory of each SM, since the functionality of the L1 and texture caches have been merged This increases the shared memory space available per SM as compared to Kepler: GP100 offers 64 KB shared memory per SM, and GP104 provides"
  },
  {
    "id": 8888,
    "content": "96 KB per SM This presents several benefits to application developers: Algorithms with significant shared memory capacity requirements (e"
  },
  {
    "id": 8890,
    "content": ", radix sort) see an automatic 33% to 100% boost in capacity per SM on top of the aggregate boost from higher SM count"
  },
  {
    "id": 8892,
    "content": "For maximum flexibility, NVIDIA recommends that applications use at most 32 KB of shared memory in any one thread block"
  },
  {
    "id": 8893,
    "content": "This would, for example, allow at least two thread blocks to fit per GP100 SM, or 3 thread blocks per GP104 SM"
  },
  {
    "id": 8898,
    "content": "Shared Memory Bandwidth  Kepler provided an optional 8-byte shared memory banking mode, which had the potential to increase shared memory bandwidth per SM for shared memory accesses of 8 or 16 bytes However, applications could only benefit from this when storing these larger elements in shared memory (i"
  },
  {
    "id": 8900,
    "content": ", integers and fp32 values saw no benefit), and only when the developer explicitly opted in to the 8-byte bank mode via the API This allows all applications using shared memory to benefit from the higher bandwidth, without specifying any particular preference via the API"
  },
  {
    "id": 8908,
    "content": "NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory"
  },
  {
    "id": 8909,
    "content": "GP100 supports up to four NVLink connections with each connection carrying up to 40 GB/s of bi-directional bandwidth"
  },
  {
    "id": 8910,
    "content": "Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs"
  },
  {
    "id": 8911,
    "content": "The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs"
  },
  {
    "id": 8916,
    "content": "GPUDirect RDMA Bandwidth  GPUDirect RDMA allows third party devices such as network interface cards (NICs) to directly access GPU memory"
  },
  {
    "id": 8917,
    "content": "This eliminates unnecessary copy buffers, lowers CPU overhead, and significantly decreases the latency of MPI send/receive messages from/to GPU memory"
  },
  {
    "id": 8918,
    "content": "Pascal doubles the delivered RDMA bandwidth when reading data from the source GPU memory and writing to the target NIC memory over PCIe"
  },
  {
    "id": 8922,
    "content": "Compute Preemption  Compute Preemption is a new feature specific to GP100 Compute Preemption allows compute tasks running on the GPU to be interrupted at instruction-level granularity"
  },
  {
    "id": 8925,
    "content": "Compute preemption offers two key advantages for developers: Long-running kernels no longer need to be broken up into small timeslices to avoid an unresponsive graphical user interface or kernel timeouts when a GPU is used simultaneously for compute and graphics"
  },
  {
    "id": 8930,
    "content": "Unified Memory Improvements  Pascal offers new hardware capabilities to extend Unified Memory (UM) support"
  },
  {
    "id": 8931,
    "content": "An extended 49-bit virtual addressing space allows Pascal GPUs to address the full 48-bit virtual address space of modern CPUs as well as the memories of all GPUs in the system through a single virtual address space, not limited by the physical memory sizes of any one processor"
  },
  {
    "id": 8932,
    "content": "Page faulting allows applications to access the same managed memory allocations from both host and device without explicit synchronization It also removes the need for the CUDA runtime to pre-synchronize all managed memory allocations before each kernel launch Instead, when a kernel accesses a non-resident memory page, it faults, and the page can be migrated to the GPU memory on-demand, or mapped"
  },
  {
    "id": 8934,
    "content": "In cases where the UM heuristics prove suboptimal, further tuning is possible through a set of migration hints that can be added to the source code"
  },
  {
    "id": 8935,
    "content": "On supporting operating system platforms, any memory allocated with the default OS allocator (for example, malloc or new) can be accessed from both GPU and CPU code using the same pointer"
  },
  {
    "id": 8941,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 8942,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 8944,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 8945,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 8946,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 8947,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 8948,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 8949,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 8950,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 8951,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 8952,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 8953,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 8954,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 8961,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 8963,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 8964,
    "content": "1 Throughout this guide, Kepler refers to devices of compute capability 3 x, Maxwell refers to devices of compute capability 5 x, and Pascal refers to device of compute capability 6"
  },
  {
    "id": 8966,
    "content": "4 As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory"
  },
  {
    "id": 8967,
    "content": "5 Migratable, or Unified Memory (UM) , allocations are made with cudaMallocManaged() or, for systems with Heterogeneous Memory Management (HMM) support, malloc()"
  },
  {
    "id": 8968,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 8973,
    "content": "Volta Tuning Guide v12 5 | PDF | Archive Tuning CUDA Applications for Volta The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Volta Architecture NVIDIA Volta Compute Architecture  Volta is NVIDIA’s latest architecture for CUDA compute applications Volta retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Maxwell and"
  },
  {
    "id": 8974,
    "content": "Pascal, and applications that follow the best practices for those architectures should typically see speedups on the Volta architecture without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Volta architectural features"
  },
  {
    "id": 8975,
    "content": "A detailed overview of the major improvements in GV100 over earlier NVIDIA architectures is provided in a white paper entitled NVIDIA Tesla V100 GPU Architecture: The World’s Most Advanced Datacenter GPU"
  },
  {
    "id": 8976,
    "content": "For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide"
  },
  {
    "id": 8979,
    "content": "CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures Programmers must primarily focus on following those recommendations to achieve the best performance"
  },
  {
    "id": 8980,
    "content": "The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads"
  },
  {
    "id": 8984,
    "content": "Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Volta Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Volta"
  },
  {
    "id": 8985,
    "content": "Streaming Multiprocessor  The Volta Streaming Multiprocessor (SM) provides the following improvements over Pascal"
  },
  {
    "id": 8990,
    "content": "Instruction Scheduling  Each Volta SM includes 4 warp-scheduler units Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units"
  },
  {
    "id": 8991,
    "content": "Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle Dependent instruction issue latency for core FMA math operations are reduced to four clock cycles, compared to six cycles on Pascal"
  },
  {
    "id": 8992,
    "content": "As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp"
  },
  {
    "id": 8993,
    "content": "Many more warps are, of course, recommended to cover the much greater latency of memory transactions and control-flow operations"
  },
  {
    "id": 8999,
    "content": "Independent Thread Scheduling  The Volta architecture introduces Independent Thread Scheduling among threads in a warp"
  },
  {
    "id": 9000,
    "content": "This feature enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code"
  },
  {
    "id": 9001,
    "content": "However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures"
  },
  {
    "id": 9003,
    "content": "To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic Applications that assume reads and writes are implicitly visible"
  },
  {
    "id": 9004,
    "content": "to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid"
  },
  {
    "id": 9006,
    "content": "sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier"
  },
  {
    "id": 9014,
    "content": ", 64), and other factors influencing warp occupancy remain similar as well: The register file size is 64k 32-bit registers per SM Shared memory capacity per SM is 96KB, similar to GP104, and a 50% increase compared to GP100 Overall, developers can expect similar occupancy as on Pascal without changes to their application"
  },
  {
    "id": 9020,
    "content": "For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput"
  },
  {
    "id": 9024,
    "content": "Tensor Core Operations  Each Tensor Core performs the following operation: D = AxB + C, where A, B, C, and D are 4x4 matrices The matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be FP16 or FP32 matrices When accumulating in FP32, the FP16 multiply results in a full precision product that is then accumulated using FP32 addition with the other"
  },
  {
    "id": 9025,
    "content": "intermediate products for a 4x4x4 matrix multiply In practice, Tensor Cores are used to perform much larger 2D or higher dimensional matrix operations, built up from these smaller elements The API exposes specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently use Tensor Cores from a CUDA-C++ program At the CUDA level, the warp-level interface assumes"
  },
  {
    "id": 9035,
    "content": "High Bandwidth Memory  GV100 uses up to eight memory dies per HBM2 stack and four stacks, with a maximum of 32 GB of GPU memory A faster and more efficient HBM2 implementation delivers up to 900 GB/s of peak memory bandwidth, compared to 732 GB/s for GP100 This combination of a new generation HBM2 memory, and a new generation memory controller, in Volta provides 1 5x delivered memory bandwidth,"
  },
  {
    "id": 9036,
    "content": "compared to Pascal GP100—and a greater than 95% memory bandwidth efficiency running many workloads In order to hide the DRAM latencies at full HBM2 bandwidth more memory accesses must be kept in flight, compared to GPUs equipped with traditional GDDR5 This is accomplished by the large complement of SMs in GV100, which typically boost the number of concurrent threads, and thus the reads-in-flight,"
  },
  {
    "id": 9038,
    "content": "Resource-constrained kernels that are limited to low occupancy may benefit from increasing the number of concurrent memory accesses per thread"
  },
  {
    "id": 9043,
    "content": "Unified Shared Memory/L1/Texture Cache  In Volta the L1 cache, texture cache, and shared memory are backed by a combined 128 KB data cache As in previous architectures, the portion of the cache dedicated to shared memory (known as the carveout ) can be selected at runtime using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout A new feature, Volta enables a"
  },
  {
    "id": 9045,
    "content": "To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit"
  },
  {
    "id": 9046,
    "content": "Like Pascal, Volta combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp Volta increases the maximum capacity of the L1 cache to 128 KB, more than 7x larger than the GP100 L1 Another benefit of its union with"
  },
  {
    "id": 9047,
    "content": "shared memory, the Volta L1 improves in terms of both latency and bandwidth compared to Pascal The result is that for many applications Volta narrows the performance gap between explicitly managed shared memory and direct access to device memory Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best"
  },
  {
    "id": 9052,
    "content": "Cooperative Groups  The Volta architecture introduced Independent Thread Scheduling, which enables intra-warp synchronization patterns that were previously not possible"
  },
  {
    "id": 9053,
    "content": "This is an extension to the CUDA programming model for organizing groups of communicating threads Cooperative Groups allows developers to express the granularity at which threads are communicating, helping them to express richer, more efficient parallel decompositions"
  },
  {
    "id": 9057,
    "content": "Multi-Process Service  The Volta Multi-Process Service is significantly improved compared to previous architecutres, both in terms of performance and robustness"
  },
  {
    "id": 9058,
    "content": "Intermediary software schedulers, used for MPS with previous architectures, have been replaced by hardware accelerated units within the GPU"
  },
  {
    "id": 9059,
    "content": "MPS clients now submit tasks directly to the GPU work queues, significantly decreasing submission latency and increasing aggregate throughput"
  },
  {
    "id": 9060,
    "content": "Volta MPS also provides each client with an isolated address space, 3 and extends Unified Memory support for MPS applications Volta MPS also provides control for clients to restrict each client to a fraction of the GPU execution resources Developers can use this feature to reduce or eliminate head-of-line blocking where work from one MPS client overwhelms GPU execution resources and prevents"
  },
  {
    "id": 9065,
    "content": "NVLink Interconnect  NVLink is NVIDIA’s high-speed data interconnect NVLink can be used to significantly increase performance for both GPU-to-GPU communication and for GPU access to system memory"
  },
  {
    "id": 9066,
    "content": "GV100 supports up to six NVLink connections with each connection carrying up to 50 GB/s of bi-directional bandwidth"
  },
  {
    "id": 9067,
    "content": "Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs"
  },
  {
    "id": 9068,
    "content": "The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs"
  },
  {
    "id": 9077,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 9078,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 9080,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 9081,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 9082,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 9083,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 9084,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 9085,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 9086,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 9087,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 9088,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 9089,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 9090,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 9097,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 9099,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 9100,
    "content": "1 Throughout this guide, Maxwell refers to devices of compute capability 5 x, Pascal refers to device of compute capability 6 x, and Volta refers to devices of compute capability 7"
  },
  {
    "id": 9102,
    "content": "2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction"
  },
  {
    "id": 9104,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 9109,
    "content": "Turing Tuning Guide v12 5 | PDF | Archive Tuning CUDA Applications for Turing The programming guide to tuning CUDA Applications for GPUs based on the NVIDIA Turing Architecture NVIDIA Turing Compute Architecture  Turing is NVIDIA’s latest architecture for CUDA compute applications Turing retains and extends the same CUDA programming model provided by previous NVIDIA architectures such as Pascal"
  },
  {
    "id": 9110,
    "content": "and Volta, and applications that follow the best practices for those architectures should typically see speedups on the Turing architecture without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging Turing architectural features 1 For further details on the programming features discussed in this guide, please refer to the"
  },
  {
    "id": 9114,
    "content": "CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures Programmers must primarily focus on following those recommendations to achieve the best performance"
  },
  {
    "id": 9115,
    "content": "The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential code, Minimize data transfers between the host and the device, Adjust kernel launch configuration to maximize device utilization, Ensure global memory accesses are coalesced, Minimize redundant accesses to global memory whenever possible, Avoid long sequences of diverged execution by threads"
  },
  {
    "id": 9119,
    "content": "Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Turing Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with Turing"
  },
  {
    "id": 9120,
    "content": "Streaming Multiprocessor  The Turing Streaming Multiprocessor (SM) is based on the same major architecture (7"
  },
  {
    "id": 9126,
    "content": "Instruction Scheduling  Each Turing SM includes 4 warp-scheduler units Each scheduler handles a static set of warps and issues to a dedicated set of arithmetic instruction units"
  },
  {
    "id": 9127,
    "content": "Instructions are performed over two cycles, and the schedulers can issue independent instructions every cycle Dependent instruction issue latency for core FMA math operations is four clock cycles, like Volta, compared to six cycles on Pascal"
  },
  {
    "id": 9128,
    "content": "As a result, execution latencies of core math operations can be hidden by as few as 4 warps per SM, assuming 4-way instruction-level parallelism ILP per warp, or by 16 warps per SM without any instuction-level parallelism"
  },
  {
    "id": 9129,
    "content": "Like Volta, the Turing SM provides 64 FP32 cores, 64 INT32 cores and 8 improved mixed-precision Tensor Cores Turing has a lower double precision throughput than Volta with only 2 FP64 cores"
  },
  {
    "id": 9134,
    "content": "Independent Thread Scheduling  The Turing architecture features the same Independent Thread Scheduling introduced with Volta"
  },
  {
    "id": 9135,
    "content": "This enables intra-warp synchronization patterns previously unavailable and simplifies code changes when porting CPU code"
  },
  {
    "id": 9136,
    "content": "However, Independent Thread Scheduling can also lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity 2 of previous hardware architectures"
  },
  {
    "id": 9137,
    "content": "When porting existing codes to Volta or Turing, the following three code patterns need careful attention"
  },
  {
    "id": 9138,
    "content": "To avoid data corruption, applications using warp intrinsics ( __shfl* , __any , __all , and __ballot ) should transition to the new, safe, synchronizing counterparts, with the *_sync suffix The new warp intrinsics take in a mask of threads that explicitly define which lanes (threads of a warp) must participate in the warp intrinsic Applications that assume reads and writes are implicitly visible"
  },
  {
    "id": 9139,
    "content": "to other threads in the same warp need to insert the new __syncwarp() warp-wide barrier synchronization instruction between steps where data is exchanged between threads via global or shared memory Assumptions that code is executed in lockstep or that reads/writes from separate threads are visible across a warp without synchronization are invalid"
  },
  {
    "id": 9141,
    "content": "sync (and their derivatives) in such a way that a barrier will not be reached by some non-exited thread in the thread block must be modified to ensure that all non-exited threads reach the barrier"
  },
  {
    "id": 9147,
    "content": "Occupancy  The maximum number of concurrent warps per SM is 32 on Turing (versus 64 on Volta) Other factors influencing warp occupancy remain otherwise similar: The register file size is 64k 32-bit registers per SM Overall, developers can expect similar occupancy as on Pascal or Volta without changes to their application"
  },
  {
    "id": 9153,
    "content": "For example, each iteration of a pipelined loop could update addresses and load data for the next iteration while simultaneously processing the current iteration at full FP32 throughput"
  },
  {
    "id": 9157,
    "content": "Tensor Core Operations  Volta introduced Tensor Cores to accelerate matrix multiply operations on mixed precision floating point data The API provides specialized matrix load, matrix multiply and accumulate, and matrix store operations, where each warp processes a small matrix fragment, allowing to efficiently use Tensor Cores from a CUDA-C++ program In practice, Tensor Cores are used to perform"
  },
  {
    "id": 9158,
    "content": "much larger 2D or higher dimensional matrix operations, built up from these smaller matrix fragments The Tensor Cores support half precision matrix multiplication, where the matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be either FP16 or FP32 matrices When accumulating in FP32, the FP16 multiply results in a full precision product that is then"
  },
  {
    "id": 9159,
    "content": "accumulated using FP32 addition CUDA 10 supports several fragment sizes, 16x16x16, 32x8x16, and 8x32x16 to use the Tensor Cores on Volta or Turing with FP16 inputs Any binary compiled for Volta will run on Turing, but Volta binaries using Tensor Cores will only be able to reach half of Turing’s Tensor Core peak performance Recompiling the binary specifically for Turing would allow it to reach the"
  },
  {
    "id": 9160,
    "content": "peak performance Turing’s Tensor Core supports integer matrix multiply operations, which can operate on 8-bit, 4-bit and 1-bit integer inputs, with 32-bit integer accumulation When operating on 8-bit inputs, CUDA exposes fragment sizes of 16x16x16, 32x8x16, and 8x32x16 For sub-byte operations the fragment sizes available are 8x8x32 for 4-bit inputs, or 8x8x128 for 1-bit inputs"
  },
  {
    "id": 9169,
    "content": "Unified Shared Memory/L1/Texture Cache  Turing features a unified L1 / Shared Memory cache similar to the one introduced in Volta, but with a smaller size The portion of the cache dedicated to shared memory or L1 (known as the carveout ) can be changed at runtime, either automatically by the driver, or manually using the cudaFuncSetAttribute() with the attribute"
  },
  {
    "id": 9170,
    "content": "cudaFuncAttributePreferredSharedMemoryCarveout Turing supports two carveout configurations, either with 64 KB of shared memory and 32 KB of L1, or with 32 KB of shared memory and 64 KB of L1 To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit Like Pascal and Volta,"
  },
  {
    "id": 9171,
    "content": "Turing combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp The state-of-the-art L1 cache in Volta and Turing offers lower latency, higher bandwidth, and higher capacity compared to the earlier architectures"
  },
  {
    "id": 9172,
    "content": "The result is that for many applications Volta and Turing narrow the performance gap between explicitly managed shared memory and direct access to device memory"
  },
  {
    "id": 9173,
    "content": "Also, the cost of register spills is lowered compared to Pascal, and the balance of occupancy versus spilling should be re-evaluated to ensure best performance"
  },
  {
    "id": 9178,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 9179,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 9181,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 9182,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 9183,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 9184,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 9185,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 9186,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 9187,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 9188,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 9189,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 9190,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 9191,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 9198,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 9200,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 9201,
    "content": "1 Throughout this guide, Kepler refers to devices of compute capability 3 x, Maxwell refers to devices of compute capability 5 x, Pascal refers to devices of compute capability 6 x, Volta refers to devices of compute capability 7 0, and Turing refers to devices of compute capability 7"
  },
  {
    "id": 9203,
    "content": "2 The term warp-synchronous refers to code that implicitly assumes threads in the same warp are synchronized at every instruction"
  },
  {
    "id": 9204,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 9209,
    "content": "NVIDIA Ampere GPU Architecture Tuning Guide v12 5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ampere GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ampere GPU Architecture NVIDIA Ampere GPU Architecture  The NVIDIA Ampere GPU architecture is NVIDIA’s latest architecture for CUDA compute applications The NVIDIA Ampere GPU architecture"
  },
  {
    "id": 9210,
    "content": "retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as Turing and Volta, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA A100 GPU without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA"
  },
  {
    "id": 9211,
    "content": "Ampere GPU architecture’s features 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide"
  },
  {
    "id": 9214,
    "content": "CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures Programmers must primarily focus on following those recommendations to achieve the best performance The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential"
  },
  {
    "id": 9219,
    "content": "Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ampere GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ampere GPU Architecture Streaming Multiprocessor  The NVIDIA Ampere GPU architecture’s Streaming Multiprocessor (SM)"
  },
  {
    "id": 9229,
    "content": "Other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM The maximum number of thread blocks per SM is 32 for devices of compute capability 8"
  },
  {
    "id": 9237,
    "content": ", A100 GPUs) shared memory capacity per SM is 164 KB, a 71% increase compared to V100’s capacity of 96 KB"
  },
  {
    "id": 9247,
    "content": "Asynchronous Data Copy from Global Memory to Shared Memory  The NVIDIA Ampere GPU architecture adds hardware acceleration for copying data from global memory to shared memory These copy instructions are asynchronous, with respect to computation and allow users to explicitly control overlap of compute with data movement from global memory into the SM"
  },
  {
    "id": 9248,
    "content": "These instructions also avoid using extra registers for memory copies and can also bypass the L1 cache"
  },
  {
    "id": 9254,
    "content": "Hardware Acceleration for Split Arrive/Wait Barrier  The NVIDIA Ampere GPU architecture adds hardware acceleration for a split arrive/wait barrier in shared memory"
  },
  {
    "id": 9255,
    "content": "These barriers can be used to implement fine grained thread controls, producer-consumer computation pipeline and divergence code patterns in CUDA"
  },
  {
    "id": 9256,
    "content": "For more information on the Arrive/Wait Barriers refer to the Arrive/Wait Barrier section in the CUDA C++ Programming Guide"
  },
  {
    "id": 9261,
    "content": "Warp level support for Reduction Operations  The NVIDIA Ampere GPU architecture adds native support for warp wide reduction operations for 32-bit signed and unsigned integer operands The warp wide reduction operations support arithmetic add , min , and max operations on 32-bit signed and unsigned integers and bitwise and , or and xor operations on 32-bit unsigned integers For more details on the"
  },
  {
    "id": 9267,
    "content": "Improved Tensor Core Operations  The NVIDIA Ampere GPU architecture includes new Third Generation Tensor Cores that are more powerful than the Tensor Cores used in Volta and Turing SMs The new Tensor Cores use a larger base matrix size and add powerful new math modes including: Support for FP64 Tensor Core, using new DMMA instructions TF32 is a new 19-bit Tensor Core format that can be easily"
  },
  {
    "id": 9269,
    "content": "Support for bitwise AND along with bitwise XOR which was introduced in Turing, through BMMA instructions"
  },
  {
    "id": 9270,
    "content": "The following table presents the evolution of matrix instruction sizes and supported data types for Tensor Cores across different GPU architecture generations"
  },
  {
    "id": 9282,
    "content": "Increased Memory Capacity and High Bandwidth Memory  The NVIDIA A100 GPU increases the HBM2 memory capacity from 32 GB in V100 GPU to 40 GB in A100 GPU Along with the increased memory capacity, the bandwidth is increased by 72%, from 900 GB/s on Volta V100 to 1550 GB/s on A100"
  },
  {
    "id": 9287,
    "content": "Increased L2 capacity and L2 Residency Controls  The NVIDIA Ampere GPU architecture increases the capacity of the L2 cache to 40 MB in Tesla A100, which is 7x larger than Tesla V100 Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased The NVIDIA Ampere GPU architecture allows CUDA users to control the persistence of data in L2 cache For more information"
  },
  {
    "id": 9288,
    "content": "on the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide"
  },
  {
    "id": 9293,
    "content": "Unified Shared Memory/L1/Texture Cache  The NVIDIA A100 GPU based on compute capability 8 0 increases the maximum capacity of the combined L1 cache, texture cache and shared memory to 192 KB, 50% larger than the L1 cache in NVIDIA V100 GPU In the NVIDIA Ampere GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in"
  },
  {
    "id": 9294,
    "content": "previous architectures such as Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout The NVIDIA A100 GPU supports shared memory capacity of 0, 8, 16, 32, 64, 100, 132 or 164 KB per SM GPUs with compute capability 8 6 support shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM Hence, the A100 GPU enables a single thread block to address up to"
  },
  {
    "id": 9295,
    "content": "163 KB of shared memory and GPUs with compute capability 8 6 can address up to 99 KB of shared memory in a single thread block To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit Like Volta, the NVIDIA Ampere GPU architecture combines the functionality of the L1 and"
  },
  {
    "id": 9296,
    "content": "texture caches into a unified L1/Texture cache which acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp Another benefit of its union with shared memory, similar to Volta L1 is improvement in terms of both latency and bandwidth"
  },
  {
    "id": 9300,
    "content": "Third Generation NVLink  The third generation of NVIDIA’s high-speed NVLink interconnect is implemented in A100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features The third generation NVLink has the same bi-directional data rate of 50 GB/s per link,"
  },
  {
    "id": 9301,
    "content": "but uses half the number of signal pairs to achieve this bandwidth Therefore, the total number of links available is increased to twelve in A100, versus six in V100, yielding 600 GB/s bidirectional bandwidth versus 300 GB/s for V100 Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe The cudaDeviceEnablePeerAccess() API call remains necessary to"
  },
  {
    "id": 9303,
    "content": "The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs"
  },
  {
    "id": 9304,
    "content": "In the NVIDIA Ampere GPU architecture remote NVLINK accesses go through a Link TLB on the remote GPU Applications with remote random accesses may want to constrain the remotely accessed region to 64 GB for each peer GPU"
  },
  {
    "id": 9309,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 9310,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 9312,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 9313,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 9314,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 9315,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 9316,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 9317,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 9318,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 9319,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 9320,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 9321,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 9322,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 9329,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 9331,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 9332,
    "content": "1 Throughout this guide, Kepler refers to devices of compute capability 3 x, Maxwell refers to devices of compute capability 5 x, Pascal refers to device of compute capability 6 x, Volta refers to devices of compute capability 7 0, Turing refers to devices of compute capability 7 5, and NVIDIA Ampere GPU Architecture refers to devices of compute capability 8"
  },
  {
    "id": 9333,
    "content": "x Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 9338,
    "content": "NVIDIA Hopper Tuning Guide v12 5 | PDF | Archive Tuning CUDA Applications for Hopper GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the Hopper GPU Architecture NVIDIA Hopper GPU Architecture  The NVIDIA® Hopper GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications The NVIDIA Hopper GPU architecture retains and extends the same CUDA"
  },
  {
    "id": 9339,
    "content": "programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere GPU architecture and NVIDIA Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA H100 GPU without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Hopper"
  },
  {
    "id": 9340,
    "content": "GPU architecture’s features 1 For further details on the programming features discussed in this guide, refer to the CUDA C++ Programming Guide"
  },
  {
    "id": 9343,
    "content": "CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures Programmers must primarily focus on following those recommendations to achieve the best performance The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential"
  },
  {
    "id": 9348,
    "content": "Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the Hopper Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with NVIDIA Hopper"
  },
  {
    "id": 9349,
    "content": "Streaming Multiprocessor  The NVIDIA Hopper Streaming Multiprocessor (SM) provides the following improvements over Turing and NVIDIA Ampere GPU architectures"
  },
  {
    "id": 9354,
    "content": "Occupancy  The maximum number of concurrent warps per SM remains the same as in NVIDIA Ampere GPU architecture (that is, 64), and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM The maximum number of thread blocks per SM is 32 for devices of compute capability 9"
  },
  {
    "id": 9357,
    "content": "0 (H100 GPUs), shared memory capacity per SM is 228 KB, a 39% increase compared to A100’s capacity of 164 KB"
  },
  {
    "id": 9360,
    "content": "For applications using Thread Block Clusters, it is always recommended to compute the occupancy using cudaOccupancyMaxActiveClusters and launch cluster-based kernels accordingly"
  },
  {
    "id": 9361,
    "content": "Overall, developers can expect similar occupancy as on NVIDIA Ampere GPU architecture GPUs without changes to their application"
  },
  {
    "id": 9366,
    "content": "Tensor Memory Accelerator  The Hopper architecture builds on top of the asynchronous copies introduced by NVIDIA Ampere GPU architecture and provides a more sophisticated asynchronous copy engine: the Tensor Memory Accelerator (TMA)"
  },
  {
    "id": 9367,
    "content": "TMA allows applications to transfer 1D and up to 5D tensors between global memory and shared memory, in both directions, as well as between the shared memory regions of different SMs in the same cluster (refer to Thread Block Clusters ) Additionally, for writes from shared memory to global memory, it allows specifying element wise reduction operations such as add/min/max as well as bitwise and/or"
  },
  {
    "id": 9368,
    "content": "for most common data types This has several advantages: Avoids using registers for moving data between the different memory spaces Avoids using SM instructions for moving data: a single thread can issue large data movement instructions to the TMA unit The whole block can then continue working on other instructions while the data is in flight and only wait for the data to be consumed when actually"
  },
  {
    "id": 9369,
    "content": "necessary Enables users to write warp specialized codes, where specific warps specialize on data movement between the different memory spaces while other warps only work on local data within the SM"
  },
  {
    "id": 9370,
    "content": "This feature will be exposed through cuda::memcpy_async along with the cuda::barrier and cuda::pipeline for synchronizing data movement"
  },
  {
    "id": 9375,
    "content": "Thread Block Clusters  NVIDIA Hopper Architecture adds a new optional level of hierarchy, Thread Block Clusters, that allows for further possibilities when parallelizing applications A thread block can read from, write to, and perform atomics in shared memory of other thread blocks within its cluster As demonstrated in the CUDA C++ Programming Guide , there are applications that cannot fit"
  },
  {
    "id": 9376,
    "content": "required data within shared memory and must use global memory instead This can benefit applications that need to communicate data between SMs by utilizing the combined bandwidth of both distributed shared memory and L2 In order to achieve best performance for accesses to Distributed Shared Memory, access patterns to those described in the CUDA C++ Best Practices Guide for Global Memory should be"
  },
  {
    "id": 9377,
    "content": "used Specifically, accesses to Distributed Shared Memory should be coalesced and aligned to 32-byte segments, if possible Access patterns with non-unit stride should be avoided if possible, which can be achieved by using local shared memory, similar to what is shown in the CUDA C++ Best Practices Guide for Shared Memory"
  },
  {
    "id": 9378,
    "content": "The maximum portable cluster size supported is 8; however, NVIDIA Hopper H100 GPU allows for a nonportable cluster size of 16 by opting in Launching a kernel with a nonportable cluster size requires setting the cudaFuncAttributeNonPortableClusterSizeAllowed function attribute Using larger cluster sizes may reduce the maximum number of active blocks across the GPU (refer to Occupancy )"
  },
  {
    "id": 9383,
    "content": "Improved FP32 Throughput  Devices of compute capability 9 0 have 2x more FP32 operations per cycle per SM than devices of compute capability 8"
  },
  {
    "id": 9389,
    "content": "Dynamic Programming Instructions  The NVIDIA Hopper architecture adds support for new instructions to accelerate dynamic programming algorithms, such as the Smith-Waterman algorithm for sequence alignment in bioinformatics, and algorithms in graph theory, game theory, ML, and finance problems"
  },
  {
    "id": 9390,
    "content": "The new instructions permit computation of max and min values among three operands, max and min operations yielding predicates, combined add operation with max or min, operating on signed and unsigned 32-bit int and 16-bit short2 types, and half2"
  },
  {
    "id": 9391,
    "content": "All DPX instructions with 16-bit short types DPX instructions enable 128 operations per cycle per SM"
  },
  {
    "id": 9399,
    "content": "High-Bandwidth Memory HBM3 Subsystem  The NVIDIA H100 GPU has support for HBM3 and HBM2e memory, with capacity up to 80 GB GPUs HBM3 memory system supports up to 3 TB/s memory bandwidth, a 93% increase over the 1 55 TB/s on A100-40GB"
  },
  {
    "id": 9404,
    "content": "Increased L2 Capacity  The NVIDIA Hopper architecture increases the L2 cache capacity from 40 MB in the A100 GPU to 50 MB in the H100 GPU Along with the increased capacity, the bandwidth of the L2 cache to the SMs is also increased The NVIDIA Hopper architecture allows CUDA users to control the persistence of data in L2 cache similar to the NVIDIA Ampere GPU Architecture For more information on"
  },
  {
    "id": 9405,
    "content": "the persistence of data in L2 cache, refer to the section on managing L2 cache in the CUDA C++ Programming Guide"
  },
  {
    "id": 9410,
    "content": "Inline Compression  The NVIDIA Hopper architecture allows CUDA compute kernels to benefit from the new inline compression (ILC)"
  },
  {
    "id": 9411,
    "content": "This feature can be applied to individual memory allocation, and the compressor automatically chooses between several possible compression algorithms, or none if there is no suitable pattern In case compression can be used, this feature allows accessing global memory at significantly higher bandwidth than global memory bandwidth, since only compressed data needs to be transferred between global"
  },
  {
    "id": 9412,
    "content": "memory and SMs However, the feature does not allow for reducing memory footprint: since compression is automatic, even if compression is active, the memory region will use the same footprint as if there was no compression"
  },
  {
    "id": 9413,
    "content": "This is because underlying data may be changed by the user application and may not be compressible during the entire duration of the application"
  },
  {
    "id": 9414,
    "content": "See the CUDA C++ Programming Guide section on compressible memory : CUmemGenericAllocationHandle allocationHandle ; CUmemAllocationProp prop = {}; memset ( prop , 0 , sizeof ( CUmemAllocationProp )); prop -> type = CU_MEM_ALLOCATION_TYPE_PINNED ; prop -> location compressionType = CU_MEM_ALLOCATION_COMP_GENERIC ; cuMemCreate ( & allocationHandle , size , & prop , 0 ); One can check whether"
  },
  {
    "id": 9415,
    "content": "compressible memory is available on the given device with: cuDeviceGetAttribute ( & compressionAvailable , CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED , currentDevice ) Note that this example code does not handle errors and compiling this code requires linking against the CUDA library ( libcuda"
  },
  {
    "id": 9421,
    "content": "Unified Shared Memory/L1/Texture Cache  The NVIDIA H100 GPU based on compute capability 9 0 increases the maximum capacity of the combined L1 cache, texture cache, and shared memory to 256 KB, from 192 KB in NVIDIA Ampere Architecture, an increase of 33% In the NVIDIA Hopper GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout) can be selected at"
  },
  {
    "id": 9422,
    "content": "runtime as in previous architectures such as NVIDIA Ampere Architecture and NVIDIA Volta, using cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout The NVIDIA H100 GPU supports shared memory capacities of 0, 8, 16, 32, 64, 100, 132, 164, 196 and 228 KB per SM Hence, the H100 GPU enables a single thread block to address up to 227 KB of shared memory To maintain"
  },
  {
    "id": 9423,
    "content": "architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit Like the NVIDIA Ampere Architecture and NVIDIA Volta GPU architectures, the NVIDIA Hopper GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache which acts as a coalescing"
  },
  {
    "id": 9424,
    "content": "buffer for memory accesses, gathering up the data requested by the threads of a warp before delivery of that data to the warp Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth"
  },
  {
    "id": 9428,
    "content": "Fourth-Generation NVLink  The fourth generation of NVIDIA’s high-speed NVLink interconnect is implemented in H100 GPUs, which significantly enhances multi-GPU scalability, performance, and reliability with more links per GPU, much faster communication bandwidth, and improved error-detection and recovery features"
  },
  {
    "id": 9429,
    "content": "The total number of links available is increased to 18 in H100, compared to 12 in A100, yielding 900 GB/s bidirectional bandwidth compared to 600 GB/s for A100"
  },
  {
    "id": 9430,
    "content": "Transfers between NVLink-connected endpoints are automatically routed through NVLink, rather than PCIe The cudaDeviceEnablePeerAccess() API call remains necessary to enable direct transfers (over either PCIe or NVLink) between GPUs"
  },
  {
    "id": 9431,
    "content": "The cudaDeviceCanAccessPeer() can be used to determine if peer access is possible between any pair of GPUs"
  },
  {
    "id": 9434,
    "content": "0 Initial Public Release Added support for compute capability 9 0 1 Throughout this guide, NVIDIA Volta refers to devices of compute capability 7 0, NVIDIA Turing refers to devices of compute capability 7 5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8 x, and NVIDIA Hopper refers to devices of compute capability 9"
  },
  {
    "id": 9436,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 9437,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 9439,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 9440,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 9441,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 9442,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 9443,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 9444,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 9445,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 9446,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 9447,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 9448,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 9449,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 9456,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 9458,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 9459,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 9464,
    "content": "NVIDIA Ada GPU Architecture Tuning Guide v12 5 | PDF | Archive Tuning CUDA Applications for NVIDIA Ada GPU Architecture The programming guide for tuning CUDA Applications for GPUs based on the NVIDIA Ada GPU Architecture NVIDIA Ada GPU Architecture  The NVIDIA ® Ada GPU architecture is NVIDIA’s latest architecture for CUDA ® compute applications The NVIDIA Ada GPU architecture retains and"
  },
  {
    "id": 9465,
    "content": "extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA"
  },
  {
    "id": 9466,
    "content": "Ada GPU architecture’s features 1 For further details on the programming features discussed in this guide, please refer to the CUDA C++ Programming Guide"
  },
  {
    "id": 9469,
    "content": "CUDA Best Practices  The performance guidelines and best practices described in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide apply to all CUDA-capable GPU architectures Programmers must primarily focus on following those recommendations to achieve the best performance The high-priority recommendations from those guides are as follows: Find ways to parallelize sequential"
  },
  {
    "id": 9474,
    "content": "Application Compatibility  Before addressing specific performance tuning issues covered in this guide, refer to the NVIDIA Ada GPU Architecture Compatibility Guide for CUDA Applications to ensure that your application is compiled in a way that is compatible with the NVIDIA Ada GPU Architecture Streaming Multiprocessor  The NVIDIA Ada GPU architecture’s Streaming Multiprocessor (SM) provides the"
  },
  {
    "id": 9480,
    "content": "Occupancy  The maximum number of concurrent warps per SM is 48, remaining the same compared to compute capability 8 6 GPUs, and other factors influencing warp occupancy are: The register file size is 64K 32-bit registers per SM Overall, developers can expect similar occupancy as on compute capability 8"
  },
  {
    "id": 9486,
    "content": "Improved Tensor Core Operations  The NVIDIA Ada GPU architecture includes new Ada Fourth Generation Tensor Cores featuring the Hopper FP8 Transformer Engine"
  },
  {
    "id": 9491,
    "content": "Improved FP32 throughput  Devices of compute capability 8 9 have 2x more FP32 operations per cycle per SM than devices of compute capability 8"
  },
  {
    "id": 9504,
    "content": "Increased L2 capacity  The NVIDIA Ada GPU architecture increases the capacity of the L2 cache to 98304 KB in AD102, 16x larger than GA102 The NVIDIA Ada GPU architecture allows CUDA users to control the persistence of data in the L2 cache For more information on the persistence of data in the L2 cache, refer to the section on managing the L2 cache in the CUDA C++ Programming Guide"
  },
  {
    "id": 9509,
    "content": "Unified Shared Memory/L1/Texture Cache  NVIDIA Ada architecture features a unified L1 cache, texture cache, and shared memory similar to that of the NVIDIA Ampere architecture In the NVIDIA Ada GPU architecture, the portion of the L1 cache dedicated to shared memory (known as the carveout ) can be selected at runtime as in previous architectures, such as NVIDIA Ampere, using"
  },
  {
    "id": 9510,
    "content": "cudaFuncSetAttribute() with the attribute cudaFuncAttributePreferredSharedMemoryCarveout The NVIDIA Ada GPU architecture supports shared memory capacity of 0, 8, 16, 32, 64 or 100 KB per SM"
  },
  {
    "id": 9512,
    "content": "9 can address up to 99 KB of shared memory in a single thread block To maintain architectural compatibility, static shared memory allocations remain limited to 48 KB, and an explicit opt-in is also required to enable dynamic allocations above this limit"
  },
  {
    "id": 9513,
    "content": "Like the NVIDIA Ampere and NVIDIA Volta GPU architectures, the NVIDIA Ada GPU architecture combines the functionality of the L1 and texture caches into a unified L1/Texture cache that acts as a coalescing buffer for memory accesses, gathering up the data requested by the threads of a warp prior to delivery of that data to the warp"
  },
  {
    "id": 9514,
    "content": "Another benefit of its union with shared memory, similar to previous architectures, is improvement in terms of both latency and bandwidth"
  },
  {
    "id": 9517,
    "content": "0 Initial Public Release Added support for compute capability 8 9 1 Throughout this guide, Volta refers to devices of compute capability 7 0, Turing refers to devices of compute capability 7 5, NVIDIA Ampere GPU Architecture refers to devices of compute capability 8"
  },
  {
    "id": 9521,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 9522,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 9524,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 9525,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 9526,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 9527,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 9528,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 9529,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 9530,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 9531,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 9532,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 9533,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 9534,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 9541,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 9543,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 9544,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 9619,
    "content": "Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_ 10"
  },
  {
    "id": 9622,
    "content": "5 | PDF | Archive Parallel Thread Execution ISA Version 8 5 The programming guide to using PTX (Parallel Thread Execution) and ISA (Instruction Set Architecture) Introduction  This document describes PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA) PTX exposes the GPU as a data-parallel computing device"
  },
  {
    "id": 9625,
    "content": "Scalable Data-Parallel Computing using GPUs  Driven by the insatiable market demand for real-time, high-definition 3D graphics, the programmable GPU has evolved into a highly parallel, multithreaded, many-core processor with tremendous computational horsepower and very high memory bandwidth The GPU is especially well-suited to address problems that can be expressed as data-parallel computations"
  },
  {
    "id": 9626,
    "content": "- the same program is executed on many data elements in parallel - with high arithmetic intensity - the ratio of arithmetic operations to memory operations Because the same program is executed for each data element, there is a lower requirement for sophisticated flow control; and because it is executed on many data elements and has high arithmetic intensity, the memory access latency can be hidden"
  },
  {
    "id": 9627,
    "content": "with calculations instead of big data caches Many applications that process large data sets can use a data-parallel programming model to speed up the computations"
  },
  {
    "id": 9628,
    "content": "Similarly, image and media processing applications such as post-processing of rendered images, video encoding and decoding, image scaling, stereo vision, and pattern recognition can map image blocks and pixels to parallel processing threads In fact, many algorithms outside the field of image rendering and processing are accelerated by data-parallel processing, from general signal processing or"
  },
  {
    "id": 9630,
    "content": "The PTX-to-GPU translator and driver enable NVIDIA GPUs to be used as programmable parallel computers"
  },
  {
    "id": 9633,
    "content": "Goals of PTX  PTX provides a stable programming model and instruction set for general purpose parallel programming"
  },
  {
    "id": 9634,
    "content": "It is designed to be efficient on NVIDIA GPUs supporting the computation features defined by the NVIDIA Tesla architecture"
  },
  {
    "id": 9635,
    "content": "High level language compilers for languages such as CUDA and C/C++ generate PTX instructions, which are optimized for and translated to native target-architecture instructions"
  },
  {
    "id": 9636,
    "content": "The goals for PTX include the following: Provide a stable ISA that spans multiple GPU generations Provide a common source-level ISA for optimizing code generators and translators, which map PTX to specific target machines Provide a scalable programming model that spans GPU sizes from a single unit to many parallel units"
  },
  {
    "id": 9644,
    "content": "Document Structure  The information in this document is organized into the following Chapters: Programming Model outlines the programming model"
  },
  {
    "id": 9646,
    "content": "Abstracting the ABI describes the function and call syntax, calling convention, and PTX support for abstracting the Application Binary Interface (ABI)"
  },
  {
    "id": 9662,
    "content": "A Highly Multithreaded Coprocessor  The GPU is a compute device capable of executing a very large number of threads in parallel It operates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive portions of applications running on the host are off-loaded onto the device"
  },
  {
    "id": 9663,
    "content": "More precisely, a portion of an application that is executed many times, but independently on different data, can be isolated into a kernel function that is executed on the GPU as many different threads"
  },
  {
    "id": 9664,
    "content": "To that effect, such a function is compiled to the PTX instruction set and the resulting kernel is translated at install time to the target GPU instruction set"
  },
  {
    "id": 9667,
    "content": "Thread Hierarchy  The batch of threads that executes a kernel is organized as a grid A grid consists of either cooperative thread arrays or clusters of cooperative thread arrays as described in this section and illustrated in Figure 1 and Figure 2 Cooperative thread arrays (CTAs) implement CUDA thread blocks and clusters implement CUDA thread block clusters"
  },
  {
    "id": 9671,
    "content": "Cooperative Thread Arrays  The Parallel Thread Execution (PTX) programming model is explicitly parallel: a PTX program specifies the execution of a given thread of a parallel thread array A cooperative thread array , or CTA, is an array of threads that execute a kernel concurrently or in parallel To coordinate the communication of the threads within the CTA, one can specify synchronization"
  },
  {
    "id": 9672,
    "content": "points where threads wait until all threads in the CTA have arrived Programs use a data parallel decomposition to partition inputs, work, and results across the threads of the CTA Each CTA thread uses its thread identifier to determine its assigned role, assign specific input and output positions, compute addresses, and select work to perform"
  },
  {
    "id": 9674,
    "content": "z ) that specifies the thread’s position within a 1D, 2D, or 3D CTA Each thread identifier component ranges from zero up to the number of thread ids in that CTA dimension Each CTA has a 1D, 2D, or 3D shape specified by a three-element vector ntid (with elements ntid x , ntid y , and ntid"
  },
  {
    "id": 9676,
    "content": "Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called warps A warp is a maximal subset of threads from a single CTA, such that the threads execute the same instructions at the same time"
  },
  {
    "id": 9677,
    "content": "Some applications may be able to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate constant, WARP_SZ , which may be used in any instruction where an immediate operand is allowed"
  },
  {
    "id": 9681,
    "content": "Cluster of Cooperative Thread Arrays  Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate with each other via shared memory The executing CTA has to make sure that the shared memory of the peer CTA exists before communicating with it via shared memory and the peer CTA hasn’t exited before completing the shared memory operation Threads within the"
  },
  {
    "id": 9682,
    "content": "different CTAs in a cluster can synchronize and communicate with each other via shared memory Each CTA in the cluster also has a unique CTA identifier ( cluster_ctarank ) across all dimensions The total number of CTAs across all the dimensions in the cluster is specified by cluster_nctarank Threads may read and use these values through predefined, read-only special registers %cluster_ctaid ,"
  },
  {
    "id": 9683,
    "content": "%cluster_nctaid , %cluster_ctarank , %cluster_nctarank If the user specifies the cluster dimensions at launch time then it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster launch with default dimension 1x1x1 PTX provides read-only special register %is_explicit_cluster to differentiate between explicit and implicit cluster launch"
  },
  {
    "id": 9687,
    "content": "Grid of Clusters  There is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a cluster can contain However, clusters with CTAs that execute the same kernel can be batched together into a grid of clusters, so that the total number of threads that can be launched in a single kernel invocation is very large This comes at the expense of reduced thread communication"
  },
  {
    "id": 9688,
    "content": "and synchronization, because threads in different clusters cannot communicate and synchronize with each other"
  },
  {
    "id": 9689,
    "content": "Threads may read and use these values through predefined, read-only special registers %tid , %ntid , %clusterid , %nclusterid , and %gridid Thread may use and read these values through predefined, read-only special registers %ctaid and %nctaid"
  },
  {
    "id": 9690,
    "content": "Each kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs where cluster is optional level and is applicable only for target architectures sm_90 and higher Figure 1 shows a grid consisting of CTAs and Figure 2 shows a grid consisting of clusters Grids may be launched with dependencies between one another - a grid may be a dependent grid and/or a prerequisite"
  },
  {
    "id": 9691,
    "content": "grid To understand how grid dependencies may be defined, refer to the section on CUDA Graphs in the Cuda Programming Guide Figure 1 Grid with CTAs  Figure 2 Grid with clusters  A cluster is a set of cooperative thread arrays (CTAs) where a CTA is a set of concurrent threads that execute the same kernel program A grid is a set of clusters consisting of CTAs that execute independently"
  },
  {
    "id": 9694,
    "content": "Memory Hierarchy  PTX threads may access data from multiple state spaces during their execution as illustrated by Figure 3 where cluster level is introduced from target architecture sm_90 onwards"
  },
  {
    "id": 9695,
    "content": "Each thread block (CTA) has a shared memory visible to all threads of the block and to all active blocks in the cluster and with the same lifetime as the block"
  },
  {
    "id": 9696,
    "content": "There are additional state spaces accessible by all threads: the constant, param, texture, and surface state spaces The global, constant, param, texture, and surface state spaces are optimized for different memory usages For example, texture memory offers different addressing modes as well as data filtering for specific data formats Note that texture and surface memory is cached, and within the"
  },
  {
    "id": 9697,
    "content": "same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global or a surface write in the same kernel call returns undefined data In other words, a thread can safely read some texture or surface memory location only if this memory location has been updated by a"
  },
  {
    "id": 9698,
    "content": "previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call The global, constant, and texture state spaces are persistent across kernel launches by the same application Both the host and the device maintain their own local memory, referred to as host memory and device memory , respectively The device memory may be"
  },
  {
    "id": 9699,
    "content": "mapped and read or written by the host, or, for more efficient transfer, copied from the host memory through optimized API calls that utilize the device’s high-performance Direct Memory Access (DMA) engine"
  },
  {
    "id": 9700,
    "content": "A Set of SIMT Multiprocessors  The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) When a host program invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity"
  },
  {
    "id": 9701,
    "content": "A multiprocessor consists of multiple Scalar Processor (SP) cores, a multithreaded instruction unit, and on-chip shared memory"
  },
  {
    "id": 9702,
    "content": "The multiprocessor creates, manages, and executes concurrent threads in hardware with zero scheduling overhead Fast barrier synchronization together with lightweight thread creation and zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for example, a low granularity decomposition of problems by assigning one thread to each data element (such as a pixel"
  },
  {
    "id": 9704,
    "content": "To manage hundreds of threads running several different programs, the multiprocessor employs an architecture we call SIMT (single-instruction, multiple-thread) The multiprocessor maps each thread to one scalar processor core, and each scalar thread executes independently with its own instruction address and register state The multiprocessor SIMT unit creates, manages, schedules, and executes"
  },
  {
    "id": 9707,
    "content": ") Individual threads composing a SIMT warp start together at the same program address but are otherwise free to branch and execute independently When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that get scheduled by the SIMT unit The way a block is split into warps is always the same; each warp contains threads of consecutive, increasing thread IDs"
  },
  {
    "id": 9708,
    "content": "with the first warp containing thread 0 At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues the next instruction to the active threads of the warp A warp executes one common instruction at a time, so full efficiency is realized when all threads of a warp agree on their execution path If threads of a warp diverge via a data-dependent conditional branch,"
  },
  {
    "id": 9709,
    "content": "the warp serially executes each branch path taken, disabling threads that are not on that path, and when all paths complete, the threads converge back to the same execution path Branch divergence occurs only within a warp; different warps execute independently regardless of whether they are executing common or disjointed code paths SIMT architecture is akin to SIMD (Single Instruction, Multiple"
  },
  {
    "id": 9710,
    "content": "Data) vector organizations in that a single instruction controls multiple processing elements A key difference is that SIMD vector organizations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread In contrast with SIMD vector machines, SIMT enables programmers to write thread-level parallel code for independent, scalar"
  },
  {
    "id": 9711,
    "content": "threads, as well as data-parallel code for coordinated threads For the purposes of correctness, the programmer can essentially ignore the SIMT behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge In practice, this is analogous to the role of cache lines in traditional code: Cache line size can be safely"
  },
  {
    "id": 9712,
    "content": "ignored when designing for correctness but must be considered in the code structure when designing for peak performance"
  },
  {
    "id": 9713,
    "content": "Vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually"
  },
  {
    "id": 9714,
    "content": "How many blocks a multiprocessor can process at once depends on how many registers per thread and how much shared memory per block are required for a given kernel since the multiprocessor’s registers and shared memory are split among all the threads of the batch of blocks If there are not enough registers or shared memory available per multiprocessor to process at least one block, the kernel will"
  },
  {
    "id": 9719,
    "content": "Independent Thread Scheduling  On architectures prior to Volta, warps used a single program counter shared amongst all 32 threads in the warp together with an active mask specifying the active threads of the warp As a result, threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data, and algorithms requiring fine-grained sharing of"
  },
  {
    "id": 9720,
    "content": "data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from Starting with the Volta architecture, Independent Thread Scheduling allows full concurrency between threads, regardless of warp With Independent Thread Scheduling , the GPU maintains execution state per thread, including a program counter and call stack, and can yield execution at"
  },
  {
    "id": 9721,
    "content": "a per-thread granularity, either to make better use of execution resources or to allow one thread to wait for data to be produced by another A schedule optimizer determines how to group active threads from the same warp together into SIMT units This retains the high throughput of SIMT execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at"
  },
  {
    "id": 9722,
    "content": "sub-warp granularity Independent Thread Scheduling can lead to a rather different set of threads participating in the executed code than intended if the developer made assumptions about warp-synchronicity of previous hardware architectures In particular, any warp-synchronous code (such as synchronization-free, intra-warp reductions) should be revisited to ensure compatibility with Volta and beyond"
  },
  {
    "id": 9725,
    "content": "Syntax  PTX programs are a collection of text source modules (files) PTX source modules have an assembly-language style syntax with instruction operation codes and operands The ptxas optimizing backend compiler optimizes and assembles PTX source modules to produce corresponding binary object files"
  },
  {
    "id": 9729,
    "content": "All whitespace characters are equivalent; whitespace is ignored except for its use in separating tokens in the language"
  },
  {
    "id": 9730,
    "content": "The following are common preprocessor directives: #include , #define , #if , #ifdef , #else , #endif , #line , #file C: A Reference Manual by Harbison and Steele provides a good description of the C preprocessor"
  },
  {
    "id": 9732,
    "content": "version directive specifying the PTX language version, followed by a target directive specifying the target architecture assumed"
  },
  {
    "id": 9736,
    "content": "Comments  Comments in PTX follow C/C++ syntax, using non-nested /* and */ for comments that may span multiple lines, and using   to begin a comment that extends up to the next newline character, which terminates the current line Comments cannot occur within character constants, string literals, or within other comments Comments in PTX are treated as whitespace"
  },
  {
    "id": 9752,
    "content": "Directive Statements  Directive keywords begin with a dot, so no conflict is possible with user-defined identifiers"
  },
  {
    "id": 9753,
    "content": "The directives in PTX are listed in Table 1 and described in State Spaces, Types, and Variables and Directives Table 1 PTX Directives "
  },
  {
    "id": 9791,
    "content": "Instruction Statements  Instructions are formed from an instruction opcode followed by a comma-separated list of zero or more operands, and terminated with a semicolon"
  },
  {
    "id": 9793,
    "content": "The guard predicate follows the optional label and precedes the opcode, and is written as @p , where p is a predicate register"
  },
  {
    "id": 9794,
    "content": "Table 2 Reserved Instruction Keywords  abs discard min shf vadd activemask div mma shfl vadd2 add dp2a mov shl vadd4 addc dp4a movmatrix shr vavrg2 alloca elect mul sin vavrg4 and ex2 mul24 slct vmad applypriority exit multimem sqrt vmax atom fence nanosleep st vmax2 bar fma neg stackrestore vmax4 barrier fns not stacksave vmin bfe getctarank or stmatrix vmin2 bfi griddepcontrol pmevent sub"
  },
  {
    "id": 9795,
    "content": "vmin4 bfind isspacep popc subc vote bmsk istypep prefetch suld vset bra ld prefetchu suq vset2 brev ldmatrix prmt sured vset4 brkpt ldu rcp sust vshl brx lg2 red szext vshr call lop3 redux tanh vsub clz mad rem testp vsub2 cnot mad24 ret tex vsub4 copysign madc rsqrt tld4 wgmma cos mapa sad trap wmma cp match selp txq xor createpolicy max set vabsdiff cvt mbarrier setmaxnreg vabsdiff2 cvta membar"
  },
  {
    "id": 9798,
    "content": "Identifiers  User-defined identifiers follow extended C++ rules: they either start with a letter followed by zero or more letters, digits, underscore, or dollar characters; or they start with an underscore, dollar, or percentage character followed by one or more letters, digits, underscore, or dollar characters: followsym: [a-zA-Z0-9_$] identifier: [a-zA-Z]{followsym}* | {[_$%]{followsym}+ PTX"
  },
  {
    "id": 9799,
    "content": "does not specify a maximum length for identifiers and suggests that all implementations support a minimum length of at least 1024 characters"
  },
  {
    "id": 9800,
    "content": "Many high-level languages such as C and C++ follow similar rules for identifier names, except that the percentage sign is not allowed The percentage sign can be used to avoid name conflicts, e"
  },
  {
    "id": 9803,
    "content": "PTX predefines one constant and a small number of special registers that begin with the percentage sign, listed in Table 3"
  },
  {
    "id": 9805,
    "content": ", %pm7 %clock64 %lanemask_eq %nctaid %smid %ctaid %lanemask_le %ntid %tid %envreg %lanemask_lt %nsmid %warpid %gridid %lanemask_ge %nwarpid WARP_SZ 4"
  },
  {
    "id": 9807,
    "content": "For predicate-type data and instructions, integer constants are allowed and are interpreted as in C, i"
  },
  {
    "id": 9818,
    "content": "The signed/unsigned nature of an integer constant is needed to correctly evaluate constant expressions containing operations such as division and ordered comparisons, where the behavior of the operation depends on the operand types When used in an instruction or data initialization, each integer constant is converted to the appropriate size based on the data or instruction type at its use"
  },
  {
    "id": 9819,
    "content": "Integer literals may be followed immediately by the letter U to indicate that the literal is unsigned hexadecimal literal: 0[xX]{hexdigit}+U octal literal: 0{octal digit}+U binary literal: 0[bB]{bit}+U decimal literal {nonzero-digit}{digit}*U Integer literals are non-negative and have a type determined by their magnitude and optional type suffix as follows: literals are signed ("
  },
  {
    "id": 9820,
    "content": "s64 ) unless the value cannot be fully represented in s64 or the unsigned suffix is specified, in which case the literal is unsigned ("
  },
  {
    "id": 9822,
    "content": "The predefined integer constant WARP_SZ specifies the number of threads per warp for the target platform; to date, all target architectures have a WARP_SZ value of 32"
  },
  {
    "id": 9826,
    "content": "Floating-Point Constants  Floating-point constants are represented as 64-bit double-precision values, and all floating-point constant expressions are evaluated using 64-bit double precision arithmetic The only exception is the 32-bit hex notation for expressing an exact single-precision floating-point value; such values retain their exact 32-bit single-precision value and may not be used in"
  },
  {
    "id": 9827,
    "content": "constant expressions Each 64-bit floating-point constant is converted to the appropriate floating-point size based on the data or instruction type at its use Floating-point literals may be written with an optional decimal point and an optional signed exponent Unlike C and C++, there is no suffix letter to specify size; literals are always represented in 64-bit double-precision format PTX includes"
  },
  {
    "id": 9828,
    "content": "a second representation of floating-point constants for specifying the exact machine representation using a hexadecimal constant To specify IEEE 754 double-precision floating point values, the constant begins with 0d or 0D followed by 16 hex digits To specify IEEE 754 single-precision floating point values, the constant begins with 0f or 0F followed by 8 hex digits 0[fF]{hexdigit}{8}"
  },
  {
    "id": 9834,
    "content": "For predicate-type data initializers and instruction operands, integer constants are interpreted as in C, i"
  },
  {
    "id": 9840,
    "content": "Constant Expressions  In PTX, constant expressions are formed using operators as in C and are evaluated using rules similar to those in C, but simplified by restricting types and sizes, removing most casts, and defining full semantics to eliminate cases where expression evaluation in C is implementation dependent Constant expressions are formed from constant literals, unary plus and minus, basic"
  },
  {
    "id": 9841,
    "content": "arithmetic operators (addition, subtraction, multiplication, division), comparison operators, the conditional ternary operator ("
  },
  {
    "id": 9844,
    "content": "), bitwise complement ( ~ ), remainder ( % ), shift operators ( > ), bit-type operators ( & , | , and ^ ), and logical operators ( && , || ) Operator precedence is highest for unary operators and decreases with each line in the chart Operators on the same line have the same precedence and are evaluated right-to-left for unary operators and left-to-right for binary operators Table 4 Operator"
  },
  {
    "id": 9848,
    "content": "u64) casts right Binary */ % multiplication, division, remainder left +- addition, subtraction >> = ordered comparisons =="
  },
  {
    "id": 9853,
    "content": "Integer Constant Expression Evaluation  Integer constant expressions are evaluated at compile time according to a set of rules that determine the type (signed"
  },
  {
    "id": 9856,
    "content": "These rules are based on the rules in C, but they’ve been simplified to apply only to 64-bit integers, and behavior is fully defined in all cases (specifically, for remainder and shift operators)"
  },
  {
    "id": 9857,
    "content": "Literals are signed unless unsigned is needed to prevent overflow, or unless the literal uses a U suffix"
  },
  {
    "id": 9858,
    "content": "Unary bitwise complement ( ~ ) interprets the source operand as unsigned and produces an unsigned result This normalization is known as the usual arithmetic conversions and simply converts both operands to unsigned type if either operand is unsigned Addition, subtraction, multiplication, and division perform the usual arithmetic conversions and produce a result with the same type as the converted"
  },
  {
    "id": 9859,
    "content": "operands That is, the operands and result are unsigned if either source operand is unsigned, and is otherwise signed"
  },
  {
    "id": 9860,
    "content": "Note that this differs from C, which allows a negative divisor but defines the behavior to be implementation dependent"
  },
  {
    "id": 9861,
    "content": "Left and right shift interpret the second operand as unsigned and produce a result with the same type as the first operand Note that the behavior of right-shift is determined by the type of the first operand: right shift of a signed value is arithmetic and preserves the sign, and right shift of an unsigned value is logical and shifts in a zero bit AND ( & ), OR ( | ), and XOR ( ^ ) perform the"
  },
  {
    "id": 9862,
    "content": "usual arithmetic conversions and produce a result with the same type as the converted operands Ordered comparisons ( , >= ) perform the usual arithmetic conversions on source operands and produce a signed result Casting of expressions to signed or unsigned is supported using ("
  },
  {
    "id": 9866,
    "content": ": ) , the first operand must be an integer, and the second and third operands are either both integers or both floating-point The usual arithmetic conversions are performed on the second and third operands, and the result type is the same as the converted type"
  },
  {
    "id": 9870,
    "content": "Summary of Constant Expression Evaluation Rules  Table 5 contains a summary of the constant expression evaluation rules Table 5 Constant Expression Evaluation Rules  Kind Operator Operand Types Operand Interpretation Result Type Primary () any type same as source same as source constant literal n/a n/a"
  },
  {
    "id": 9876,
    "content": "f64 f64 f64 integer use usual conversions converted type = f64 f64 s64 integer use usual conversions s64 =="
  },
  {
    "id": 9886,
    "content": "Packed Data Types  Certain PTX instructions operate on two sets of inputs in parallel, and produce two outputs In this section we describe the packed data types supported in PTX"
  },
  {
    "id": 9891,
    "content": "Packed Floating Point Data Types  PTX supports the following four variants of packed floating point data types: f16x2 packed type containing two f16 floating point values"
  },
  {
    "id": 9894,
    "content": "e5m2x2 cannot be used as fundamental types - they are supported as instruction types on certain instructions"
  },
  {
    "id": 9909,
    "content": "Texture Sampler and Surface Types  PTX includes built-in opaque types for defining texture, sampler, and surface descriptor variables"
  },
  {
    "id": 9910,
    "content": "These types have named fields similar to structures, but all information about layout, field ordering, base address, and overall size is hidden to a PTX program, hence the term opaque The use of these opaque types is limited to: Variable definition within global (module) scope and in kernel entry parameter lists"
  },
  {
    "id": 9911,
    "content": "Static initialization of module-scope variables using comma-delimited static assignment expressions for the named members of the type"
  },
  {
    "id": 9912,
    "content": "Referencing textures, samplers, or surfaces via texture and surface load/store instructions ( tex , suld , sust , sured ) The resulting pointer may be stored to and loaded from memory, passed as a parameter to functions, and de-referenced by texture and surface load, store, and query instructions, but the pointer cannot otherwise be treated as an address, i"
  },
  {
    "id": 9914,
    "content": ", accessing the pointer with ld and st instructions, or performing pointer arithmetic will result in undefined results"
  },
  {
    "id": 9917,
    "content": ", to initialize a pointer to an opaque variable Note Indirect access to textures and surfaces using pointers to opaque variables is supported beginning with PTX ISA version 3"
  },
  {
    "id": 9921,
    "content": "In the independent mode , texture and sampler information each have their own handle, allowing them to be defined separately and combined at the site of usage in the program In independent mode, the fields of the"
  },
  {
    "id": 9924,
    "content": "Table 9 and Table 10 list the named members of each type for unified and independent texture modes These members and their values have precise mappings to methods and values defined in the texture HW class as well as exposed values via the API Table 9 Opaque Type Fields in Unified Texture Mode  Member texref values surfref values width in elements height in elements depth in elements"
  },
  {
    "id": 9925,
    "content": "channel_data_type enum type corresponding to source language API channel_order enum type corresponding to source language API normalized_coords 0 , 1 N/A filter_mode nearest , linear N/A addr_mode_0 , addr_mode_1 , addr_mode_2 wrap , mirror , clamp_ogl , clamp_to_edge , clamp_to_border N/A array_size as number of textures in a texture array as number of surfaces in a surface array"
  },
  {
    "id": 9926,
    "content": "num_mipmap_levels as number of levels in a mipmapped texture N/A num_samples as number of samples in a multi-sample texture N/A memory_layout N/A 1 for linear memory layout; 0 otherwise 5"
  },
  {
    "id": 9929,
    "content": "Texture and Surface Properties  Fields width , height , and depth specify the size of the texture or surface in number of elements in each dimension The channel_data_type and channel_order fields specify these properties of the texture or surface using enumeration types corresponding to the source language API For example, see Channel Data Type and Channel Order Fields for the OpenCL enumeration"
  },
  {
    "id": 9934,
    "content": "Sampler Properties  The normalized_coords field indicates whether the texture or surface uses normalized coordinates in the range [0"
  },
  {
    "id": 9937,
    "content": "If no value is specified, the default is set by the runtime system based on the source language The filter_mode field specifies how the values returned by texture reads are computed based on the input texture coordinates The addr_mode_{0,1,2} fields define the addressing mode in each dimension, which determine how out-of-range coordinates are handled"
  },
  {
    "id": 9938,
    "content": "One additional sampler property, force_unnormalized_coords , is available in independent texture mode The force_unnormalized_coords field is a property of samplerref variables that allows the sampler to override the texture header normalized_coords property When True , the texture header setting is overridden and unnormalized coordinates are used; when False , the texture header setting is used"
  },
  {
    "id": 9939,
    "content": "The force_unnormalized_coords property is used in compiling OpenCL; in OpenCL, the property of normalized coordinates is carried in sampler headers To compile OpenCL to PTX, texture headers are always initialized with normalized_coords set to True, and the OpenCL sampler-based normalized_coords flag maps (negated) to the PTX-level force_unnormalized_coords flag"
  },
  {
    "id": 9947,
    "content": "surfref my_surface_name; When declared at module scope, the types may be initialized using a list of static expressions assigning values to the named members"
  },
  {
    "id": 9955,
    "content": "Channel Data Type and Channel Order Fields  The channel_data_type and channel_order fields have enumeration types corresponding to the source language API"
  },
  {
    "id": 9959,
    "content": "0 Channel Data Type Definition  CL_SNORM_INT8 0x10D0 CL_SNORM_INT16 0x10D1 CL_UNORM_INT8 0x10D2 CL_UNORM_INT16 0x10D3 CL_UNORM_SHORT_565 0x10D4 CL_UNORM_SHORT_555 0x10D5 CL_UNORM_INT_101010 0x10D6 CL_SIGNED_INT8 0x10D7 CL_SIGNED_INT16 0x10D8 CL_SIGNED_INT32 0x10D9 CL_UNSIGNED_INT8 0x10DA CL_UNSIGNED_INT16 0x10DB CL_UNSIGNED_INT32 0x10DC CL_HALF_FLOAT 0x10DD CL_FLOAT 0x10DE Table 12 OpenCL 1"
  },
  {
    "id": 9960,
    "content": "0 Channel Order Definition  CL_R 0x10B0 CL_A 0x10B1 CL_RG 0x10B2 CL_RA 0x10B3 CL_RGB 0x10B4 CL_RGBA 0x10B5 CL_BGRA 0x10B6 CL_ARGB 0x10B7 CL_INTENSITY 0x10B8 CL_LUMINANCE 0x10B9 5"
  },
  {
    "id": 9963,
    "content": "In addition to fundamental types, PTX supports types for simple aggregate objects such as vectors and arrays"
  },
  {
    "id": 9967,
    "content": "Variable Declarations  All storage for data is specified with variable declarations A variable declaration names the space in which the variable resides, its type and size, its name, an optional array size, an optional initializer, and an optional fixed address for the variable"
  },
  {
    "id": 9986,
    "content": "Vectors of length 2 and 4 of any non-predicate fundamental type can be declared by prefixing the type with"
  },
  {
    "id": 10000,
    "content": "b8 v;   a length-4 vector of bytes By default, vector variables are aligned to a multiple of their overall size (vector length times base-type size), to enable vector load and store instructions which require addresses aligned to a multiple of the access size"
  },
  {
    "id": 10004,
    "content": "Array Declarations  Array declarations are provided to allow the programmer to reserve space To declare an array, the variable name is followed with dimensional declarations similar to fixed-size array declarations in C"
  },
  {
    "id": 10009,
    "content": "u8 mailbox[128]; The size of the array specifies how many elements should be reserved For the declaration of array kernel above, 19*19 = 361 halfwords are reserved, for a total of 722 bytes The size of the first array dimension is determined by the number of elements in the array initializer"
  },
  {
    "id": 10014,
    "content": "s32 offset[][2] = { {-1, 0}, {0, -1}, {1, 0}, {0, 1} }; Array index has eight elements, and array offset is a 4x2 array"
  },
  {
    "id": 10018,
    "content": "Initializers  Declared variables may specify an initial value using a syntax similar to C/C++, where the variable name is followed by an equals sign and the initial value or values for the variable"
  },
  {
    "id": 10019,
    "content": "A scalar takes a single value, while vectors and arrays take nested lists of values inside of curly braces (the nesting matches the dimensionality of the declaration)"
  },
  {
    "id": 10022,
    "content": ", the number of initializer elements may be less than the extent of the corresponding array dimension, with remaining array locations initialized to the default value for the specified array type"
  },
  {
    "id": 10041,
    "content": "global s32 x[3][2] = { {1,2}, {3,0}, {0,0} }; Currently, variable initialization is supported only for constant and global state spaces Variables in constant and global state spaces with no explicit initializer are initialized to zero by default Variable names appearing in initializers represent the address of the variable; this can be used to statically initialize a pointer to a variable"
  },
  {
    "id": 10042,
    "content": "Initializers may also contain var+offset expressions, where offset is a byte offset added to the address of var By default, the resulting address is the offset in the variable’s state space (as is the case when taking the address of a variable with a mov instruction) An operator, generic() , is provided to create a generic address for variables used in initializers"
  },
  {
    "id": 10044,
    "content": "1, an operator mask() is provided, where mask is an integer immediate The only allowed expressions in the mask() operator are integer constant expression and symbol expression representing address of variable The mask() operator extracts n consecutive bits from the expression used in initializers and inserts these bits at the lowest position of the initialized variable The number n and the"
  },
  {
    "id": 10050,
    "content": "Supported values for mask are: 0xFF, 0xFF00, 0XFF0000, 0xFF000000, 0xFF00000000, 0xFF0000000000, 0xFF000000000000, 0xFF00000000000000"
  },
  {
    "id": 10051,
    "content": "Legacy PTX code is treated as having an implicit generic() operator for each global variable used in an initializer PTX 3"
  },
  {
    "id": 10052,
    "content": "1 code should either include explicit generic() operators in initializers, use cvta global to form generic addresses at runtime, or load from the non-generic address using ld global"
  },
  {
    "id": 10053,
    "content": "Device function names appearing in initializers represent the address of the first instruction in the function; this can be used to initialize a table of function pointers to be used with indirect calls to initialize a table of kernel function pointers, to be used with CUDA Dynamic Parallelism to launch kernels from GPU"
  },
  {
    "id": 10079,
    "content": "Alignment  Byte alignment of storage for all addressable variables can be specified in the variable declaration Alignment is specified using an optional"
  },
  {
    "id": 10081,
    "content": "For arrays, alignment specifies the address alignment for the starting address of the entire array, not for individual elements The default alignment for scalar and array variables is to a multiple of the base-type size"
  },
  {
    "id": 10084,
    "content": "b8 bar[8] = {0,0,0,0,2,0,0,0}; Note that all PTX instructions that access memory require that the address be aligned to a multiple of the access size For example, the access size of ld"
  },
  {
    "id": 10090,
    "content": "Parameterized Variable Names  Since PTX supports virtual registers, it is quite common for a compiler frontend to generate a large number of register names"
  },
  {
    "id": 10091,
    "content": "Rather than require explicit declaration of every name, PTX supports a syntax for creating a set of variables having a common prefix string appended with integer suffixes"
  },
  {
    "id": 10097,
    "content": ", %r99 This shorthand syntax may be used with any of the fundamental types and with any state space, and may be preceded by an alignment specifier"
  },
  {
    "id": 10102,
    "content": "Variable Attributes  Variables may be declared with an optional attribute directive which allows specifying special attributes of variables Variable and Function Attribute Directive: attribute describes the attribute directive"
  },
  {
    "id": 10106,
    "content": "Variable and Function Attribute Directive: attribute  attribute Variable and function attributes Description Used to specify special attributes of a variable or a function"
  },
  {
    "id": 10107,
    "content": "managed managed attribute specifies that variable will be allocated at a location in unified virtual memory environment where host and other devices in the system can reference the variable directly unified unified attribute specifies that function has the same memory address on the host and on other devices in the system"
  },
  {
    "id": 10108,
    "content": "Integer constants uuid1 and uuid2 respectively specify upper and lower 64 bits of the unique identifier associated with the function or the variable"
  },
  {
    "id": 10132,
    "content": "Tensors  A tensor is a multi-dimensional matrix structure in the memory Tensor is defined by the following properties: Dimensionality Dimension sizes across each dimension Individual element types Tensor stride across each dimension PTX supports instructions which can operate on the tensor data PTX Tensor instructions include: Copying data between global and shared memories Reducing the"
  },
  {
    "id": 10136,
    "content": "PTX Tensor instructions treat the tensor data in the global memory as a multi-dimensional structure and treat the data in the shared memory as a linear data"
  },
  {
    "id": 10155,
    "content": "Tensor can have padding at the end in each of the dimensions to provide alignment for the data in the subsequent dimensions Tensor stride can be used to specify the amount of padding in each dimension"
  },
  {
    "id": 10159,
    "content": "Tensor Access Modes  Tensor data can be accessed in two modes: Tiled mode: In tiled mode, the source multi-dimensional tensor layout is preserved at the destination Im2col mode: In im2col mode, the elements in the Bounding Box of the source tensor are rearranged into columns at the destination"
  },
  {
    "id": 10164,
    "content": "Tiled Mode  This section talks about how Tensor and Tensor access work in tiled mode Bounding Box has the following access properties: Bounding Box dimension sizes Out of boundary access mode Traversal strides The tensor-coordinates, specified in the PTX tensor instructions, specify the starting offset of the bounding box Starting offset of the bounding box along with the rest of the bounding"
  },
  {
    "id": 10170,
    "content": "Traversal-Stride  While the Bounding Box is iterating the tensor across a dimension, the traversal stride specifies the exact number of elements to be skipped Figure 5 illustrates tensor, tensor size, tensor stride, Bounding Box size and traversal stride Out of Boundary Access  PTX Tensor operation can detect and handle the case when the Bounding Box crosses the tensor boundary in any dimension"
  },
  {
    "id": 10171,
    "content": "There are 2 modes: Zero fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to 0 OOB-NaN fill mode: Elements in the Bounding Box which fall outside of the tensor boundary are set to a special NaN called OOB-NaN"
  },
  {
    "id": 10172,
    "content": "In this mode, the tensor data is treated as a batch of images with the following properties: N : number of images in the batch D, H, W : size of a 3D image (depth, height and width) C: channels per image element The above properties are associated with 3D, 4D and 5D tensors as follows: Dimension N/D/H/W/C applicability 3D NWC 4D NHWC 5D NDHWC 5"
  },
  {
    "id": 10176,
    "content": "Boundaries along other dimensions are specified by Pixels-per-Column and Channels-per-Pixel parameters as described below The following properties describe how to access of the elements in im2col mode: Bounding-Box Lower-Corner Bounding-Box Upper-Corner Pixels-per-Column Channels-per-Pixel Bounding-box Lower-Corner and Bounding-box Upper-Corner specify the two opposite corners of the Bounding Box"
  },
  {
    "id": 10177,
    "content": "in the DHW space Bounding-box Lower-Corner specifies the corner with the smallest coordinate and Bounding-box Upper-Corner specifies the corner with the largest coordinate Bounding-box Upper- and Lower-Corners are 16-bit signed values whose limits varies across the dimensions and are as shown below: 3D 4D 5D Upper- / Lower- Corner sizes [-2 15 , 2 15 -1] [-2 7 , 2 7 -1] [-2 4 , 2 4 -1] Figure 7"
  },
  {
    "id": 10178,
    "content": "and Figure 8 show the Upper-Corners and Lower-Corners Figure 7 im2col mode bounding box example 1  Figure 8 im2col mode bounding box example 2  The Bounding-box Upper- and Lower- Corners specify only the boundaries and not the number of elements to be accessed The tensor coordinates, specified in the PTX tensor instructions, behaves differently in different dimensions: Across N and C dimensions:"
  },
  {
    "id": 10179,
    "content": "specify the starting offsets along the dimension, similar to the tiled mode Across DHW dimensions: specify the location of the convolution filter base in the tensor space The im2col offsets, specified in the PTX tensor instructions in im2col mode, are added to the filter base coordinates to determine the starting location in the tensor space from where the elements are accessed The size of the"
  },
  {
    "id": 10180,
    "content": "im2col offsets varies across the dimensions and their valid ranges are as shown below: 3D 4D 5D im2col offsets range [0, 2 16 -1] [0, 2 8 -1] [0, 2 5 -1] Following are some examples of the im2col mode accesses: Example 1 ( Figure 9 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8 Bounding - Box Lower"
  },
  {
    "id": 10181,
    "content": "- Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1 tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 0 , 0 ) Figure 9 im2col mode example 1  Example 2 ( Figure 10 ): Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 9 Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Pixels - per - Column = 64 channels - per - pixel = 8"
  },
  {
    "id": 10182,
    "content": "Bounding - Box Lower - Corner W = 0 Bounding - Box Lower - Corner H = 0 Bounding - Box Upper - Corner W = -2 Bounding - Box Upper - Corner H = -2 tensor coordinates = ( 7 , 7 , 4 , 0 ) im2col offsets : ( 2 , 2 ) Figure 10 im2col mode example 2  5"
  },
  {
    "id": 10186,
    "content": "Traversal Stride  The traversal stride, in im2col mode, does not impact the total number of elements (or pixels) being accessed unlike the tiled mode The number of elements traversed along the D, H and W dimensions is strided by the traversal stride for that dimension The following example with Figure 11 illustrates accesse with traversal-strides: Tensor Size [ 0 ] = 64 Tensor Size [ 1 ] = 8"
  },
  {
    "id": 10187,
    "content": "Tensor Size [ 2 ] = 14 Tensor Size [ 3 ] = 64 Traversal Stride = 2 Pixels - per - Column = 32 channels - per - pixel = 16 Bounding - Box Lower - Corner W = -1 Bounding - Box Lower - Corner H = -1 Bounding - Box Upper - Corner W = -1 Bounding - Box Upper - Corner H = -1 Tensor coordinates in the instruction = ( 7 , 7 , 5 , 0 ) Im2col offsets in the instruction : ( 1 , 1 ) Figure 11 im2col mode"
  },
  {
    "id": 10192,
    "content": "Out of Boundary Access  In im2col mode, when the number of requested pixels in NDHW space specified by Pixels-per-Column exceeds the number of available pixels in the image batch then out-of-bounds access is performed"
  },
  {
    "id": 10197,
    "content": "Interleave layout  Tensor can be interleaved and the following interleave layouts are supported: No interleave (NDHWC) 8 byte interleave (NC/8DHWC8) : C8 utilizes 16 bytes in memory assuming 2B per channel 16 byte interleave (NC/16HWC16) : C16 utilizes 32 bytes in memory assuming 4B per channel The C information is organized in slices where sequential C elements are grouped in 16 byte or 32 byte"
  },
  {
    "id": 10199,
    "content": "If the total number of channels is not a multiple of the number of channels per slice, then the last slice must be padded with zeros to make it complete 16B or 32B slice"
  },
  {
    "id": 10204,
    "content": "Swizzling Modes  The layout of the data in the shared memory can be different to that of global memory, for access performance reasons The following describes various swizzling modes: No swizzle mode: There is no swizzling in this mode and the destination data layout is exactly similar to the source data layout 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 … Pattern repeats … 32 byte swizzle mode: The"
  },
  {
    "id": 10205,
    "content": "following table, where each elements (numbered cell) is 16 byte and the starting address is 256 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 … Pattern repeats … An example of the 32 byte swizzle mode for NC/(32B)HWC(32B) tensor of 1x2x10x10xC16 dimension, with the innermost dimension holding slice of 16 channels with 2 byte/channel, is shown in"
  },
  {
    "id": 10206,
    "content": "Figure 12 Figure 12 32-byte swizzle mode example  Figure 13 shows the two fragments of the tensor : one for C/(32B) = 0 and another for C/(32B) = 1 Figure 13 32-byte swizzle mode fragments  Figure 14 shows the destination data layout with 32 byte swizzling Figure 14 32-byte swizzle mode destination data layout  64 byte swizzle mode: The following table, where each elements (numbered cell) is 16"
  },
  {
    "id": 10207,
    "content": "byte and the starting address is 512 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 … Pattern repeats … An example of the 64 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes / channel and 32 channels, is shown in Figure 15 Figure 16 64-byte swizzle mode source data layout  Figure 17 shows the"
  },
  {
    "id": 10208,
    "content": "destination data layout with 64 byte swizzling Figure 17 64-byte swizzle mode destination data layout  128 byte swizzle mode: The following table, where each elements (numbered cell) is 16 byte and the starting address is 1024 bytes aligned, shows the pattern of the destination data layout: 0 1 2 3 4 5 6 7 1 0 3 2 5 4 7 6 2 3 0 1 6 7 4 5 3 2 1 0 7 6 5 4 4 5 6 7 0 1 2 3 5 4 7 6 1 0 3 2 6 7 4 5 2 3"
  },
  {
    "id": 10209,
    "content": "0 1 … Pattern repeats … An example of the 128 byte swizzle mode for NHWC tensor of 1x10x10x64 dimension, with 2 bytes / channel and 64 channels, is shown in Figure 18 Figure 19 128-byte swizzle mode source data layout  Figure 20 shows the destination data layout with 128 byte swizzling Tensor-map  The tensor-map is a 128-byte opaque object either in"
  },
  {
    "id": 10211,
    "content": "global space which describes the tensor properties and the access properties of the tensor data described in previous sections"
  },
  {
    "id": 10212,
    "content": "Operand Type Information  All operands in instructions have a known type from their declarations Each operand type must be compatible with the type determined by the instruction template and instruction type Operands having type different from but compatible with the instruction type are silently cast to the instruction type"
  },
  {
    "id": 10215,
    "content": "Source Operands  The source operands are denoted in the instruction descriptions by the names a , b , and c PTX describes a load-store machine, so operands for ALU instructions must all be in variables declared in the"
  },
  {
    "id": 10217,
    "content": "The cvt (convert) instruction takes a variety of operand types and sizes, as its job is to convert from nearly any data type to any other data type (and size)"
  },
  {
    "id": 10218,
    "content": "Most instructions have an optional predicate guard that controls conditional execution, and a few instructions have additional predicate source operands Predicate operands are denoted by the names p , q , r , s"
  },
  {
    "id": 10221,
    "content": "Destination Operands  PTX instructions that produce a single result store the result in the field denoted by d (for destination) in the instruction descriptions The result operand is a scalar or vector variable in the register state space"
  },
  {
    "id": 10224,
    "content": "Using Addresses, Arrays, and Vectors  Using scalar variables as operands is straightforward The interesting capabilities begin with addresses, arrays, and vectors"
  },
  {
    "id": 10228,
    "content": "Addresses as Operands  All the memory instructions take an address operand that specifies the memory location being accessed"
  },
  {
    "id": 10229,
    "content": "[reg+immOff] a sum of register reg containing a byte address plus a constant integer byte offset (signed, 32-bit) [var+immOff] a sum of address of addressable variable var containing a byte address plus a constant integer byte offset (signed, 32-bit)"
  },
  {
    "id": 10230,
    "content": "For example, among other things, the access may proceed by silently masking off low-order address bits to achieve proper rounding, or the instruction may fault"
  },
  {
    "id": 10231,
    "content": "Addresses are zero-extended to the specified width as needed, and truncated if the register width exceeds the state space address width for the target architecture"
  },
  {
    "id": 10232,
    "content": "All addresses and address computations are byte-based; there is no support for C-style pointer arithmetic"
  },
  {
    "id": 10234,
    "content": "The syntax is similar to that used in many assembly languages, where scalar variables are simply named and addresses are de-referenced by enclosing the address expression in square brackets"
  },
  {
    "id": 10235,
    "content": "Address expressions include variable names, address registers, address register plus byte offset, and immediate address expressions which evaluate at compile-time to a constant address"
  },
  {
    "id": 10263,
    "content": "Generic Addressing  If a memory instruction does not specify a state space, the operation is performed using generic addressing The state spaces"
  },
  {
    "id": 10268,
    "content": "Each window is defined by a window base and a window size that is equal to the size of the corresponding state space A generic address maps to global memory unless it falls within the window for const , local , or shared memory Within each window, a generic address maps to an address in the underlying state space by subtracting the window base from the generic address"
  },
  {
    "id": 10272,
    "content": "Arrays as Operands  Arrays of all types can be declared, and the identifier becomes an address constant in the space where the array is declared Array elements can be accessed using an explicitly calculated byte address, or by indexing into the array using square-bracket notation"
  },
  {
    "id": 10273,
    "content": "The expression within square brackets is either a constant integer, a register variable, or a simple register with constant offset expression, where the offset is a constant expression that is either added or subtracted from a register variable"
  },
  {
    "id": 10282,
    "content": "Vectors as Operands  Vector operands are supported by a limited subset of instructions, which include mov , ld , st , atom , red and tex"
  },
  {
    "id": 10298,
    "content": "f32 {a,b,c,d}, V; Vector loads and stores can be used to implement wide loads and stores, which may improve memory performance"
  },
  {
    "id": 10299,
    "content": "The registers in the load/store operations can be a vector, or a brace-enclosed list of similarly typed scalars"
  },
  {
    "id": 10305,
    "content": "v2 u32 V2, [addr+8]; Elements in a brace-enclosed vector, say {Ra, Rb, Rc, Rd}, correspond to extracted elements as follows: Ra = V"
  },
  {
    "id": 10318,
    "content": "Function names can be used in mov instruction to get the address of the function into a register, for use in an indirect call"
  },
  {
    "id": 10320,
    "content": "1, the mov instruction may be used to take the address of kernel functions, to be passed to a system call that initiates a kernel launch from the GPU"
  },
  {
    "id": 10324,
    "content": "Type Conversion  All operands to all arithmetic, logic, and data movement instruction must be of the same type and size, except for operations where changing the size and/or type is part of the definition of the instruction Operands of different sizes or types must be converted prior to the operation"
  },
  {
    "id": 10328,
    "content": "Scalar Conversions  Table 13 shows what precision and format the cvt instruction uses given operands of differing types For example, if a cvt"
  },
  {
    "id": 10329,
    "content": "s32 u16 instruction is given a u16 source operand and s32 as a destination operand, the u16 is zero-extended to s32"
  },
  {
    "id": 10330,
    "content": "Conversions to floating-point that are beyond the range of floating-point numbers are represented with the maximum floating-point value (IEEE 754 Inf for f32 and f64 , and ~131,000 for f16 )"
  },
  {
    "id": 10331,
    "content": "1 If the destination register is wider than the destination format, the result is extended to the destination register width after chopping"
  },
  {
    "id": 10338,
    "content": "Rounding Modifiers  Conversion instructions may specify a rounding modifier In PTX, there are four integer rounding modifiers and four floating-point rounding modifiers Table 14 Floating-Point Rounding Modifiers  Modifier Description"
  },
  {
    "id": 10339,
    "content": "rn mantissa LSB rounds to nearest even rna mantissa LSB rounds to nearest, ties away from zero rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity Table 15 Integer Rounding Modifiers  Modifier Description rni round to nearest integer, choosing even integer if source is equidistant between two integers rzi round to"
  },
  {
    "id": 10340,
    "content": "nearest integer in the direction of zero rmi round to nearest integer in direction of negative infinity rpi round to nearest integer in direction of positive infinity 6"
  },
  {
    "id": 10342,
    "content": "The first is to have multiple threads of execution so that the hardware can issue a memory operation and then switch to other execution Another way to hide latency is to issue the load instructions as early as possible, as execution is not blocked until the desired result is used in a subsequent (in time) instruction"
  },
  {
    "id": 10343,
    "content": "Table 16 Cost Estimates for Accessing State-Spaces  Space Time Notes Register 0 Shared 0 Constant 0 Amortized cost is low, first access is high Local > 100 clocks Parameter 0 Immediate 0 Global > 100 clocks Texture > 100 clocks Surface > 100 clocks 7"
  },
  {
    "id": 10344,
    "content": "Abstracting the ABI  Rather than expose details of a particular calling convention, stack layout, and Application Binary Interface (ABI), PTX provides a slightly higher-level abstraction and supports multiple ABI implementations In this section, we describe the features of PTX needed to achieve this hiding of the ABI"
  },
  {
    "id": 10345,
    "content": "These include syntax for function definitions, function calls, parameter passing, support for variadic functions ( varargs ), and memory allocated on the stack ( alloca )"
  },
  {
    "id": 10346,
    "content": "Refer to PTX Writers Guide to Interoperability for details on generating PTX compliant with Application Binary Interface (ABI) for the CUDA ® architecture"
  },
  {
    "id": 10351,
    "content": "A function declaration specifies an optional list of return parameters, the function name, and an optional list of input parameters; together these specify the function’s interface, or prototype The simplest function has no parameters or return values, and is represented in PTX as follows:"
  },
  {
    "id": 10352,
    "content": "func foo { Here, execution of the call instruction transfers control to foo , implicitly saving the return address Execution of the ret instruction within foo transfers control to the instruction following the call"
  },
  {
    "id": 10353,
    "content": "Scalar and vector base-type input and return parameters may be represented simply as register variables At the call, arguments may be register variables or constants, and return values may be placed directly into register variables The arguments and return variables at the call must have type and size that match the callee’s corresponding formal parameters"
  },
  {
    "id": 10363,
    "content": "param state space byte arrays described next Objects such as C structures and unions are flattened into registers or byte arrays in PTX and are represented using param space memory For example, consider the following C structure, passed by value to a function: struct { double dbl; char c[4]; }; In PTX, this structure will be flattened into a byte array Since memory accesses are required to be"
  },
  {
    "id": 10364,
    "content": "aligned to a multiple of the access size, the structure in this example will be a 12 byte array with 8 byte alignment so that accesses to the"
  },
  {
    "id": 10410,
    "content": "param variable py is declared in the body of the calling function and used to set up the structure being passed to bar"
  },
  {
    "id": 10414,
    "content": "param state space is used to set values that will be passed to a called function and/or to receive return values from a called function"
  },
  {
    "id": 10420,
    "content": "param space formal parameters that are byte arrays, the argument must also be a param space byte array with matching type, size, and alignment"
  },
  {
    "id": 10422,
    "content": "param space formal parameters that are base-type scalar or vector variables, the corresponding argument may be either a param or reg space variable with matching type and size, or a constant that can be represented in the type of the formal parameter"
  },
  {
    "id": 10426,
    "content": "reg space variable of matching type and size, or a constant that can be represented in the type of the formal parameter In the case of reg space formal parameters, the register must be at least 32-bits in size"
  },
  {
    "id": 10428,
    "content": "param instructions used for passing arguments to function call must immediately precede the corresponding call instruction and ld param instruction used for collecting return value must immediately follow the call instruction without any control flow alteration"
  },
  {
    "id": 10430,
    "content": "param variable does not consume extra space in the caller’s frame beyond that needed by the ABI The param variable simply allows a mapping to be made at the call site between data that may be in multiple locations (e"
  },
  {
    "id": 10432,
    "content": ", structure being manipulated by caller is located in registers and memory) to something that can be passed as a parameter or return value to the callee"
  },
  {
    "id": 10434,
    "content": "reg state space can be used to receive and return base-type scalar and vector values, including sub-word size objects when compiling in non-ABI mode"
  },
  {
    "id": 10437,
    "content": "param state space for parameter passing has no impact on whether the parameter is ultimately passed in physical registers or on the stack The mapping of parameters to physical registers and stack locations depends on the ABI definition and the order, size, and alignment of parameters"
  },
  {
    "id": 10468,
    "content": "Variadic Functions  Note Support for variadic functions which was unimplemented has been removed from the spec"
  },
  {
    "id": 10470,
    "content": "0 supports passing unsized array parameter to a function which can be used to implement variadic functions"
  },
  {
    "id": 10471,
    "content": "Alloca  PTX provides alloca instruction for allocating storage at runtime on the per-thread local memory stack The allocated stack memory can be accessed with ld"
  },
  {
    "id": 10472,
    "content": "local and st local instructions using the pointer returned by alloca In order to facilitate deallocation of memory allocated with alloca , PTX provides two additional instructions: stacksave which allows reading the value of stack pointer in a local variable, and stackrestore which can restore the stack pointer with the saved value alloca , stacksave , and stackrestore instructions are described"
  },
  {
    "id": 10473,
    "content": "in Stack Manipulation Instructions Preview Feature: Stack manipulation instructions alloca , stacksave and stackrestore are preview features in PTX ISA version 7"
  },
  {
    "id": 10475,
    "content": "All details are subject to change with no guarantees of backward compatibility on future PTX ISA versions or SM architectures"
  },
  {
    "id": 10477,
    "content": "Memory Consistency Model  In multi-threaded executions, the side-effects of memory operations performed by each thread become visible to other threads in a partial and non-identical order This means that any two operations may appear to happen in no order, or in different orders, to different threads The axioms introduced by the memory consistency model specify exactly which contradictions are"
  },
  {
    "id": 10479,
    "content": "In the absence of any constraint, each read operation returns the value committed by some write operation to the same memory location, including the initial write to that memory location The memory consistency model effectively constrains the set of such candidate writes from which a read operation can return a value"
  },
  {
    "id": 10482,
    "content": "Scope and applicability of the model  The constraints specified under this model apply to PTX programs with any PTX ISA version number, running on sm_70 or later architectures The memory consistency model does not apply to texture (including ld"
  },
  {
    "id": 10488,
    "content": "Limitations on atomicity at system scope  When communicating with the host CPU, certain strong operations with system scope may not be performed atomically on some systems For more details on atomicity guarantees to host memory, see the CUDA Atomicity Requirements"
  },
  {
    "id": 10491,
    "content": "Memory operations  The fundamental storage unit in the PTX memory model is a byte, consisting of 8 bits"
  },
  {
    "id": 10492,
    "content": "Every byte in a PTX state space has a unique address relative to all threads that have access to the same state space The address operand contains a virtual address that gets converted to a physical address during memory access The physical address and the size of the data type together define a physical memory location, which is the range of bytes starting from the physical address and extending"
  },
  {
    "id": 10493,
    "content": "up to the size of the data type in bytes The memory consistency model specification uses the terms “address” or “memory address” to indicate a virtual address, and the term “memory location” to indicate a physical memory location Each PTX memory instruction also specifies the operation — either a read, a write or an atomic read-modify-write — to be performed on all the bytes in the corresponding"
  },
  {
    "id": 10498,
    "content": "Overlap  Two memory locations are said to overlap when the starting address of one location is within the range of bytes constituting the other location Two memory operations are said to overlap when they specify the same virtual address and the corresponding memory locations overlap The overlap is said to be complete when both memory locations are identical, and it is said to be partial"
  },
  {
    "id": 10503,
    "content": "Aliases  Two distinct virtual addresses are said to be aliases if they map to the same memory location"
  },
  {
    "id": 10507,
    "content": "Multimem Addresses  A multimem address is a virtual address which points to multiple distinct memory locations across devices That is, the behavior of accessing a multimem address in any other memory operation is undefined"
  },
  {
    "id": 10511,
    "content": "Memory Operations on Vector Data Types  The memory consistency model relates operations executed on memory locations with scalar data types, which have a maximum size and alignment of 64 bits Memory operations with a vector data type are modelled as a set of equivalent memory operations with a scalar data type, executed in an unspecified order on the elements in the vector"
  },
  {
    "id": 10515,
    "content": "Memory Operations on Packed Data Types  A packed data type consists of two values of the same scalar data type, as described in Packed Data Types A memory operation on a packed data type is modelled as a pair of equivalent memory operations on the scalar data type, executed in an unspecified order on each element of the packed data"
  },
  {
    "id": 10519,
    "content": "Initialization  Each byte in memory is initialized by a hypothetical write W0 executed before starting any thread in the program If the byte is included in a program variable, and that variable has an initial value, then W0 writes the corresponding initial value for that byte; else W0 is assumed to have written an unknown but constant value to the byte"
  },
  {
    "id": 10522,
    "content": "State spaces  The relations defined in the memory consistency model are independent of state spaces In particular, causality order closes over all memory operations across all the state spaces But the side-effect of a memory operation in one state space can be observed directly only by operations that also have access to the same state space This further constrains the synchronizing effect of a"
  },
  {
    "id": 10523,
    "content": "memory operation in addition to scope For example, the synchronizing effect of the PTX instruction ld"
  },
  {
    "id": 10529,
    "content": "cluster , since no thread outside the same cluster can execute an operation that accesses the same memory location"
  },
  {
    "id": 10532,
    "content": "Operation types  For simplicity, the rest of the document refers to the following operation types, instead of mentioning specific instructions that give rise to them Table 17 Operation Types  Operation Type Instruction/Operation atomic operation atom or red instruction read operation All variants of ld instruction and atom instruction (but not red instruction) write operation All variants of st"
  },
  {
    "id": 10533,
    "content": "instruction, and atomic operations if they result in a write strong operation A memory fence operation, or a memory operation with a"
  },
  {
    "id": 10540,
    "content": "synchronizing operation A barrier instruction, fence operation, release operation or acquire operation"
  },
  {
    "id": 10545,
    "content": "It is usually performed on a memory location which is mapped to the control registers of peer I/O devices"
  },
  {
    "id": 10546,
    "content": "It can also be used for communication between threads but has poor performance relative to non- mmio operations The semantic meaning of mmio operations cannot be defined precisely as it is defined by the underlying I/O device For formal specification of semantics of mmio operation from Memory Consistency Model perspective, it is equivalent to the semantics of a strong operation"
  },
  {
    "id": 10547,
    "content": "But it follows a few implementation-specific properties, if it meets the CUDA atomicity requirements at the specified scope: Writes are always performed and are never combined within the scope specified Reads are always performed, and are not forwarded, prefetched, combined, or allowed to hit any cache within the scope specified"
  },
  {
    "id": 10548,
    "content": "In such cases the amount of data loaded is implementation specific and varies between 32 and 128 bytes in size"
  },
  {
    "id": 10551,
    "content": "Scope  Each strong operation must specify a scope , which is the set of threads that may interact directly with that operation and establish any of the relations described in the memory consistency model"
  },
  {
    "id": 10553,
    "content": "cta The set of all threads executing in the same CTA as the current thread gpu The set of all threads in the current program executing on the same compute device as the current thread This also includes other kernel grids invoked by the host program on the same compute device sys The set of all threads in the current program, including all kernel grids invoked by the host program on all compute"
  },
  {
    "id": 10554,
    "content": "devices, and all threads constituting the host program itself Note that the warp is not a scope ; the CTA is the smallest collection of threads that qualifies as a scope in the memory consistency model"
  },
  {
    "id": 10557,
    "content": "Proxies  A memory proxy , or a proxy is an abstract label applied to a method of memory access When two memory operations use distinct methods of memory access, they are said to be different proxies Other operations such as textures and surfaces all use distinct methods of memory access, also distinct from the generic method Although virtual aliases use the generic method of memory access, since"
  },
  {
    "id": 10558,
    "content": "using distinct virtual addresses behaves as if using different proxies , they require a proxy fence to establish memory ordering"
  },
  {
    "id": 10561,
    "content": "Morally strong operations  Two operations are said to be morally strong relative to each other if they satisfy all of the following conditions: The operations are related in program order (i e, they are both executed by the same thread), or each operation is strong and specifies a scope that includes the thread executing the other operation Most (but not all) of the axioms in the memory"
  },
  {
    "id": 10566,
    "content": "Conflict and Data-races  Two overlapping memory operations are said to conflict when at least one of them is a write Two conflicting memory operations are said to be in a data-race if they are not related in causality order and they are not morally strong"
  },
  {
    "id": 10570,
    "content": "Limitations on Mixed-size Data-races  A data-race between operations that overlap completely is called a uniform-size data-race , while a data-race between operations that overlap partially is called a mixed-size data-race The axioms in the memory consistency model do not apply if a PTX program contains one or more mixed-size data-races But these axioms are sufficient to describe the behavior of"
  },
  {
    "id": 10571,
    "content": "a PTX program with only uniform-size data-races Atomicity of mixed-size RMW operations In any program with or without mixed-size data-races , the following property holds for every pair of overlapping atomic operations A1 and A2 such that each specifies a scope that includes the other: Either the read-modify-write operation specified by A1 is performed completely before A2 is initiated, or vice"
  },
  {
    "id": 10572,
    "content": "versa This property holds irrespective of whether the two operations A1 and A2 overlap partially or completely"
  },
  {
    "id": 10575,
    "content": "Release and Acquire Patterns  Some sequences of instructions give rise to patterns that participate in memory synchronization as described later"
  },
  {
    "id": 10576,
    "content": "The release pattern makes prior operations from the current thread 1 visible to some operations from other threads The acquire pattern makes some operations from other threads visible to later operations from the current thread A release pattern on a location M consists of one of the following: A release operation on M E"
  },
  {
    "id": 10588,
    "content": "relaxed [M]; Any memory synchronization established by a release pattern only affects operations occurring in program order before the first instruction in that pattern"
  },
  {
    "id": 10601,
    "content": "relaxed [M]; fence; Any memory synchronization established by an acquire pattern only affects operations occurring in program order after the last instruction in that pattern 1 For both release and acquire patterns, this effect is further extended to operations in other threads through the transitive nature of causality order"
  },
  {
    "id": 10604,
    "content": "Ordering of memory operations  The sequence of operations performed by each thread is captured as program order while memory synchronization across threads is captured as causality order The visibility of the side-effects of memory operations to other memory operations is captured as communication order The memory consistency model defines contradictions that are disallowed between communication"
  },
  {
    "id": 10609,
    "content": "Program Order  The program order relates all operations performed by a thread to the order in which a sequential processor will execute instructions in the corresponding PTX source It is a transitive relation that forms a total order over the operations performed by the thread, but does not relate operations from different threads"
  },
  {
    "id": 10618,
    "content": "mma_async ) perform operations that are asynchronous to the thread that executed the instruction These asynchronous operations are ordered after prior instructions in the same thread (except in the case of wgmma mma_async ), but they are not part of the program order for that thread"
  },
  {
    "id": 10621,
    "content": "async are ordered with respect to each other, but not to those of any other cp async instructions initiated by the same thread, nor any other instruction subsequently issued by the thread with the exception of cp async commit_group or cp async"
  },
  {
    "id": 10626,
    "content": "arrive instruction is ordered with respect to the memory operations performed by all prior cp async operations initiated by the same thread, but not to those of any other instruction issued by the thread The implicit mbarrier complete-tx operation that is part of all variants of cp async bulk and cp"
  },
  {
    "id": 10629,
    "content": "bulk instructions is ordered only with respect to the memory operations performed by the same asynchronous instruction, and in particular it does not transitively establish ordering with respect to prior instructions from the issuing thread"
  },
  {
    "id": 10633,
    "content": "Observation Order  Observation order relates a write W to a read R through an optional sequence of atomic read-modify-write operations A write W precedes a read R in observation order if: R and W are morally strong and R reads the value written by W, or For some atomic operation Z, W precedes Z and Z precedes R in observation order"
  },
  {
    "id": 10637,
    "content": "Fence-SC Order  The Fence-SC order is an acyclic partial order, determined at runtime, that relates every pair of morally strong fence sc operations"
  },
  {
    "id": 10641,
    "content": "Memory synchronization  Synchronizing operations performed by different threads synchronize with each other at runtime as described here"
  },
  {
    "id": 10642,
    "content": "A fence sc operation X synchronizes with a fence sc operation Y if X precedes Y in the Fence-SC order"
  },
  {
    "id": 10653,
    "content": "red operation executed on the same barrier A release pattern X synchronizes with an acquire pattern Y, if a write operation in X precedes a read operation in Y in observation order , and the first operation in X and the last operation in Y are morally strong Completion of a task enqueued in a CUDA stream synchronizes with the start of the following task in the same stream, if any For purposes of"
  },
  {
    "id": 10654,
    "content": "the above, recording or waiting on a CUDA event in a stream, or causing a cross-stream barrier to be inserted due to cudaStreamLegacy , enqueues tasks in the associated streams even if there are no direct side effects An event record task synchronizes with matching event wait tasks, and a barrier arrival task synchronizes with matching barrier wait tasks Completion of the last task queued to a"
  },
  {
    "id": 10655,
    "content": "stream, if any, synchronizes with return from cudaStreamSynchronize Completion of the most recently queued matching event record task, if any, synchronizes with return from cudaEventSynchronize"
  },
  {
    "id": 10656,
    "content": "Synchronizing a CUDA device or context behaves as if synchronizing all streams in the context, including ones that have been destroyed Returning cudaSuccess from an API to query a CUDA handle, such as a stream or event, behaves the same as return from the matching synchronization API"
  },
  {
    "id": 10657,
    "content": "In addition to establishing a synchronizes relation, the CUDA API synchronization mechanisms above also participate in proxy-preserved base causality order"
  },
  {
    "id": 10661,
    "content": "Causality Order  Causality order captures how memory operations become visible across threads through synchronizing operations The axiom “Causality” uses this order to constrain the set of write operations from which a read operation may read a value Relations in the causality order primarily consist of relations in Base causality order 1 , which is a transitive order, determined at runtime Base"
  },
  {
    "id": 10662,
    "content": "causality order An operation X precedes an operation Y in base causality order if: X precedes Y in program order , or X synchronizes with Y, or For some operation Z, X precedes Z in program order and Z precedes Y in base causality order , or X precedes Z in base causality order and Z precedes Y in program order , or X precedes Z in base causality order and Z precedes Y in base causality order"
  },
  {
    "id": 10663,
    "content": "Proxy-preserved base causality order A memory operation X precedes a memory operation Y in proxy-preserved base causality order if X precedes Y in base causality order , and: X and Y are performed to the same address, using the generic proxy , or X and Y are performed to the same address, using the same proxy , and by the same thread block, or X and Y are aliases and there is an alias proxy fence"
  },
  {
    "id": 10664,
    "content": "along the base causality path from X to Y Causality order Causality order combines base causality order with some non-transitive relations as follows: An operation X precedes an operation Y in causality order if: X precedes Y in proxy-preserved base causality order , or For some operation Z, X precedes Z in observation order, and Z precedes Y in proxy-preserved base causality order 1 The"
  },
  {
    "id": 10669,
    "content": "Coherence Order  There exists a partial transitive order that relates overlapping write operations, determined at runtime, called the coherence order 1 Two overlapping write operations are related in coherence order if they are morally strong or if they are related in causality order Two overlapping writes are unrelated in coherence order if they are in a data-race , which gives rise to the"
  },
  {
    "id": 10670,
    "content": "partial nature of coherence order 1 Coherence order cannot be observed directly since it consists entirely of write operations It may be observed indirectly by its use in constraining the set of candidate writes that a read operation may read from"
  },
  {
    "id": 10674,
    "content": "Communication Order  The communication order is a non-transitive order, determined at runtime, that relates write operations to other overlapping memory operations A write W precedes an overlapping read R in communication order if R returns the value of any byte that was written by W A write W precedes a write W’ in communication order if W precedes W’ in coherence order A read R precedes an"
  },
  {
    "id": 10675,
    "content": "overlapping write W in communication order if, for any byte accessed by both R and W, R returns the value written by a write W’ that precedes W in coherence order Communication order captures the visibility of memory operations — when a memory operation X1 precedes a memory operation X2 in communication order , X1 is said to be visible to X2"
  },
  {
    "id": 10681,
    "content": "Coherence  If a write W precedes an overlapping write W’ in causality order , then W must precede W’ in coherence order"
  },
  {
    "id": 10685,
    "content": "Fence-SC  Fence-SC order cannot contradict causality order For a pair of morally strong fence sc operations F1 and F2, if F1 precedes F2 in causality order , then F1 must precede F2 in Fence-SC order"
  },
  {
    "id": 10689,
    "content": "Atomicity  Single-Copy Atomicity Conflicting morally strong operations are performed with single-copy atomicity"
  },
  {
    "id": 10690,
    "content": "When a read R and a write W are morally strong , then the following two communications cannot both exist in the same execution, for the set of bytes accessed by both R and W: R reads any byte from W Atomicity of read-modify-write (RMW) operations When an atomic operation A and a write W overlap and are morally strong , then the following two communications cannot both exist in the same execution,"
  },
  {
    "id": 10691,
    "content": "for the set of bytes accessed by both A and W: A reads any byte from a write W’ that precedes W in coherence order u32 % r0 , [ x ]; FINAL STATE : x == 2 Atomicity is guaranteed when the operations are morally strong u32 % r0 , [ x ]; FINAL STATE : x == 1 OR x == 2 Atomicity is not guaranteed if the operations are not morally strong"
  },
  {
    "id": 10695,
    "content": "No Thin Air  Values may not appear “out of thin air”: an execution cannot speculatively produce a value in such a way that the speculation becomes self-satisfying through chains of instruction dependencies and inter-thread communication"
  },
  {
    "id": 10696,
    "content": "This matches both programmer intuition and hardware reality, but is necessary to state explicitly when performing formal analysis"
  },
  {
    "id": 10697,
    "content": "u32 [ x ], % r1 ; FINAL STATE : x == 0 AND y == 0 The litmus test known as “LB” (Load Buffering) checks such forbidden values that may arise out of thin air"
  },
  {
    "id": 10698,
    "content": "Two threads T1 and T2 each read from a first variable and copy the observed result into a second variable, with the first and second variable exchanged between the threads"
  },
  {
    "id": 10699,
    "content": "If A1 reads from B2 and A2 reads from B1, then values passing through the memory operations in this example form a cycle: A1->B1->A2->B2->A1"
  },
  {
    "id": 10700,
    "content": "If any of the memory operations in this example were to speculatively associate a different value with the corresponding memory location, then such a speculation would become self-fulfilling, and hence forbidden"
  },
  {
    "id": 10704,
    "content": "Sequential Consistency Per Location  Within any set of overlapping memory operations that are pairwise morally strong , communication order cannot contradict program order , i"
  },
  {
    "id": 10706,
    "content": ", a concatenation of program order between overlapping operations and morally strong relations in communication order cannot result in a cycle This ensures that each program slice of overlapping pairwise morally strong operations is strictly sequentially-consistent"
  },
  {
    "id": 10707,
    "content": "u32 % r1 , [ x ]; IF % r0 == 1 THEN % r1 == 1 The litmus test “CoRR” (Coherent Read-Read), demonstrates one consequence of this guarantee"
  },
  {
    "id": 10708,
    "content": "A thread T1 executes a write W1 on a location x, and a thread T2 executes two (or an infinite sequence of) reads R1 and R2 on the same location x If R2 observed the initial value of x instead, then this would form a sequence of morally-strong relations R2->W1->R1 in communication order that contradicts the program order R1->R2 in thread T2 Hence R2 cannot read the initial value of x in such an"
  },
  {
    "id": 10713,
    "content": "Causality  Relations in communication order cannot contradict causality order This constrains the set of candidate write operations that a read operation may read from: If a read R precedes an overlapping write W in causality order , then R cannot read from W If a write W precedes an overlapping read R in causality order , then for any byte accessed by both R and W, R cannot read from any write"
  },
  {
    "id": 10715,
    "content": "u32 % r1 , [ data ]; IF % r0 == 1 THEN % r1 == 1 The litmus test known as “MP” (Message Passing) represents the essence of typical synchronization algorithms"
  },
  {
    "id": 10717,
    "content": "Thread T1 first writes to a data variable and then to a flag variable while a second thread T2 first reads from the flag variable and then from the data variable"
  },
  {
    "id": 10718,
    "content": "The operations on the flag are morally strong and the memory operations in each thread are separated by a fence , and these fences are morally strong"
  },
  {
    "id": 10720,
    "content": "Then axiom causality guarantees that R2 cannot read from any write that precedes W1 in coherence order"
  },
  {
    "id": 10721,
    "content": "u32 % r1 , [ data_alias_2 ]; % r1 == 1 Virtual aliases require an alias proxy fence along the synchronization path"
  },
  {
    "id": 10722,
    "content": "Litmus Test: Store Buffering The litmus test known as “SB” (Store Buffering) demonstrates the sequential consistency enforced by the fence"
  },
  {
    "id": 10724,
    "content": "A thread T1 writes to a first variable, and then reads the value of a second variable, while a second thread T2 writes to the second variable and then reads the value of the first variable"
  },
  {
    "id": 10725,
    "content": "u32 % r1 , [ x ]; % r0 == 1 OR % r1 == 1 In any execution, either F1 precedes F2 in Fence-SC order, or vice versa Axiom causality ensures that R2 cannot read from any write that precedes W1 in coherence order If each fence sc in this example were replaced by a fence"
  },
  {
    "id": 10727,
    "content": "There may be an execution where the write from each thread remains unobserved from the other thread, i"
  },
  {
    "id": 10729,
    "content": ", an execution is possible, where both R1 and R2 return the initial value “0” for variables y and x respectively"
  },
  {
    "id": 10733,
    "content": "Format and Semantics of Instruction Descriptions  This section describes each PTX instruction In addition to the name and the format of the instruction, the semantics are described, followed by some examples that attempt to show several possible instantiations of the instruction"
  },
  {
    "id": 10736,
    "content": "PTX Instructions  PTX instructions generally have from zero to four operands, plus an optional guard predicate appearing after an @ symbol to the left of the opcode : @p opcode; @p opcode a; @p opcode d, a; @p opcode d, a, b; @p opcode d, a, b, c; For instructions that create a result value, the d operand is the destination operand, while a , b , and c are source operands"
  },
  {
    "id": 10737,
    "content": "Floating Point Comparisons  The ordered floating-point comparisons are eq , ne , lt , le , gt , and ge Table 20 Floating-Point Comparison Operators  Meaning Floating-Point Operator a == b &&"
  },
  {
    "id": 10740,
    "content": "isNaN(a) && isNaN(b) ne a b && isNaN(a) && isNaN(b) gt a >= b && isNaN(a) && isNaN(b) ge To aid comparison operations in the presence of NaN values, unordered floating-point comparisons are provided: equ , neu , ltu , leu , gtu , and geu If both operands are numeric values (not NaN ), then the comparison has the same result as its ordered counterpart Table 21 Floating-Point Comparison Operators"
  },
  {
    "id": 10741,
    "content": "Accepting NaN  Meaning Floating-Point Operator a == b || isNaN(a) || isNaN(b) equ a = b || isNaN(a) || isNaN(b) neu a b || isNaN(a) || isNaN(b) gtu a >= b || isNaN(a) || isNaN(b) geu To test for NaN values, two operators num ( numeric ) and nan ( isNaN ) are provided num returns True if both operands are numeric values (not NaN ), and nan returns True if either operand is NaN Table 22"
  },
  {
    "id": 10742,
    "content": "Floating-Point Comparison Operators Testing for NaN  Meaning Floating-Point Operator isNaN(a) && isNaN(b) num isNaN(a) || isNaN(b) nan 9"
  },
  {
    "id": 10745,
    "content": "Manipulating Predicates  Predicate values may be computed and manipulated using the following instructions: and , or , xor , not , and mov There is no direct conversion between predicates and integer values, and no direct way to load or store predicate register values However, setp can be used to generate a predicate from an integer, and the predicate-based select ( selp ) instruction can be"
  },
  {
    "id": 10746,
    "content": "used to generate an integer value based on the value of a predicate; for example: selp u32 %r1,1,0,%p; convert predicate to 32-bit value 9"
  },
  {
    "id": 10748,
    "content": "Type Information for Instructions and Operands  Typed instructions must have a type-size modifier For example, the add instruction requires type and size information to properly perform the addition operation (signed, unsigned, float, different sizes), and this information must be specified as a suffix to the opcode Example"
  },
  {
    "id": 10750,
    "content": "u16 d, a, b; add u16 d, a, b;   perform a 16-bit unsigned add Some instructions require multiple type-size modifiers, most notably the data conversion instruction cvt It requires separate type-size modifiers for the result and source, and these are placed in the same order as the operands"
  },
  {
    "id": 10756,
    "content": "u16 d, a; convert 16-bit unsigned to 32-bit float In general, an operand’s type must agree with the corresponding instruction-type modifier The rules for operand and instruction type conformance are as follows: Bit-size types agree with any type of the same size Signed and unsigned integer types agree provided they have the same size, and integer operands are silently cast to the instruction type"
  },
  {
    "id": 10757,
    "content": "if needed For example, an unsigned integer operand used in a signed integer instruction will be treated as a signed integer by the instruction Floating-point types agree only if they have the same size; i"
  },
  {
    "id": 10765,
    "content": "bX okay okay okay okay sX okay okay okay invalid uX okay okay okay invalid fX okay invalid invalid okay Note Some operands have their type and size defined independently from the instruction type-size"
  },
  {
    "id": 10767,
    "content": "u32 , while the remaining operands have their type and size determined by the instruction type Operand Size Exceeding Instruction-Type Size  For convenience, ld , st , and cvt instructions permit source and destination data operands to be wider than the instruction-type size, so that narrow values may be loaded, stored, and converted using regular-width registers For example, 8-bit or 16-bit"
  },
  {
    "id": 10768,
    "content": "values may be held directly in 32-bit or 64-bit registers when being loaded, stored, or converted to other types and sizes The operand type checking rules are relaxed for bit-size and integer (signed and unsigned) instruction types; floating-point instruction types still require that the operand type-size matches exactly, unless the operand is of bit-size type When a source operand has a size that"
  },
  {
    "id": 10769,
    "content": "exceeds the instruction-type size, the source data is truncated (chopped) to the appropriate number of bits specified by the instruction type-size Note that some combinations may still be invalid for a particular instruction; for example, the cvt instruction does not support bX instruction types, so those rows are invalid for cvt The data are truncated (“chopped”) to the instruction-type size and"
  },
  {
    "id": 10770,
    "content": "interpreted according to the instruction type Integer source registers may be used with any appropriately-sized bit-size or integer instruction type The data are truncated to the instruction-type size and interpreted according to the instruction type Floating-point source registers can only be used with bit-size or floating-point instruction types When a destination operand has a size that exceeds"
  },
  {
    "id": 10771,
    "content": "the instruction-type size, the destination data is zero- or sign-extended to the size of the destination register If the corresponding instruction type is signed integer, the data is sign-extended; otherwise, the data is zero-extended Destination register size must be of equal or greater size than the instruction-type size The data are sign-extended to the destination register width for signed"
  },
  {
    "id": 10772,
    "content": "integer instruction types, and are zero-extended to the destination register width otherwise Integer destination registers may be used with any appropriately-sized bit-size or integer instruction type The data are sign-extended to the destination register width for signed integer instruction types, and are zero-extended to the destination register width for bit-size an d unsigned integer"
  },
  {
    "id": 10773,
    "content": "instruction types Floating-point destination registers can only be used with bit-size or floating-point instruction types When used with a narrower bit-size instruction type, the data are zero-extended"
  },
  {
    "id": 10776,
    "content": "Divergence of Threads in Control Constructs  Threads in a CTA execute together, at least in appearance, until they come to a conditional control construct such as a conditional branch, conditional function call, or conditional return If all of the threads act in unison and follow a single control flow path, the threads are called uniform"
  },
  {
    "id": 10777,
    "content": "A CTA with divergent threads may have lower performance than a CTA with uniformly executing threads, so it is important to have divergent threads re-converge as soon as possible All control constructs are assumed to be divergent points unless the control-flow instruction is marked as uniform, using the"
  },
  {
    "id": 10779,
    "content": "For divergent control flow, the optimizing code generator automatically determines points of re-convergence Therefore, a compiler or code author targeting PTX can ignore the issue of divergent threads, but has the opportunity to improve performance by marking branch points as uniform when the compiler or author can guarantee that the branch point is non-divergent"
  },
  {
    "id": 10782,
    "content": "Semantics  The goal of the semantic description of an instruction is to describe the results in all cases in as simple language as possible"
  },
  {
    "id": 10787,
    "content": "Machine-Specific Semantics of 16-bit Code  A PTX program may execute on a GPU with either a 16-bit or a 32-bit data path When executing on a 32-bit data path, 16-bit registers in PTX are mapped to 32-bit physical registers, and 16-bit computations are promoted to 32-bit computations This can lead to computational differences between code run on a 16-bit machine versus the same code run on a"
  },
  {
    "id": 10788,
    "content": "32-bit machine, since the promoted computation may have bits in the high-order half-word of registers that are not present in 16-bit physical registers"
  },
  {
    "id": 10789,
    "content": "These extra precision bits can become visible at the application level, for example, by a right-shift instruction"
  },
  {
    "id": 10790,
    "content": "At the PTX language level, one solution would be to define semantics for 16-bit code that is consistent with execution on a 16-bit data path This approach introduces a performance penalty for 16-bit code executing on a 32-bit data path, since the translated code would require many additional masking instructions to suppress extra precision bits in the high-order half-word of 32-bit registers"
  },
  {
    "id": 10791,
    "content": "Rather than introduce a performance penalty for 16-bit code running on 32-bit GPUs, the semantics of 16-bit instructions in PTX is machine-specific A compiler or programmer may chose to enforce portable, machine-independent 16-bit semantics by adding explicit conversions to 16-bit values at appropriate points in the program to guarantee portability of the code However, for many"
  },
  {
    "id": 10792,
    "content": "performance-critical applications, this is not desirable, and for many applications the difference in execution is preferable to limiting performance"
  },
  {
    "id": 10800,
    "content": "Integer Arithmetic Instructions  Integer arithmetic instructions operate on the integer types in register and constant immediate forms The integer arithmetic instructions are: add sub mul mad mul24 mad24 sad div rem abs neg min max popc clz bfind fns brev bfe bfi bmsk szext dp4a dp2a 9"
  },
  {
    "id": 10820,
    "content": "s16x2 instruction types, forms input vectors by half word values from source operands Half-word operands are then added in parallel to produce"
  },
  {
    "id": 10822,
    "content": "s16x2 result in destination Semantics if (type == u16x2 || type == s16x2) { iA[0] = a[0:15]; iA[1] = a[16:31]; iB[0] = b[0:15]; iB[1] = b[16:31]; for (i = 0; i ;   for"
  },
  {
    "id": 10823,
    "content": "hi variant d = t;   for lo variant Notes The type of the operation represents the types of the a and b operands"
  },
  {
    "id": 10826,
    "content": "lo is specified, then d is the same size as a and b , and either the upper or lower half of the result is written to the destination register"
  },
  {
    "id": 10828,
    "content": "wide is specified, then d is twice as wide as a and b to receive the full result of the multiplication"
  },
  {
    "id": 10839,
    "content": "Integer Arithmetic Instructions: mad  mad Multiply two values, optionally extract the high or low half of the intermediate result, and add a third value Syntax mad"
  },
  {
    "id": 10855,
    "content": "s64 }; Description Multiplies two values, optionally extracts the high or low half of the intermediate result, and adds a third value"
  },
  {
    "id": 10858,
    "content": "hi variant d = t + c;   for lo variant Notes The type of the operation represents the types of the a and b operands"
  },
  {
    "id": 10861,
    "content": "lo is specified, then d and c are the same size as a and b , and either the upper or lower half of the result is written to the destination register"
  },
  {
    "id": 10863,
    "content": "wide is specified, then d and c are twice as wide as a and b to receive the result of the multiplication"
  },
  {
    "id": 10875,
    "content": "s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and return either the high or low 32-bits of the 48-bit result"
  },
  {
    "id": 10877,
    "content": "hi variant d = t;   for lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i"
  },
  {
    "id": 10887,
    "content": "Integer Arithmetic Instructions: mad24  mad24 Multiply two 24-bit integer values and add a third value Syntax mad24"
  },
  {
    "id": 10898,
    "content": "s32 }; Description Compute the product of two 24-bit integer values held in 32-bit source registers, and add a third, 32-bit value to either the high or low 32-bits of the 48-bit result"
  },
  {
    "id": 10900,
    "content": "hi variant d = t + c;   for lo variant Notes Integer multiplication yields a result that is twice the size of the input operands, i"
  },
  {
    "id": 10930,
    "content": "b64 }; Description Count the number of one bits in a and place the resulting population count in 32-bit destination register d"
  },
  {
    "id": 10938,
    "content": "b64 }; Description Count the number of leading zeros in a starting with the most-significant bit and place the result in 32-bit destination register d"
  },
  {
    "id": 10942,
    "content": "b32) { max = 32; mask = 0x80000000; } else { max = 64; mask = 0x8000000000000000; } while (d =0; i--) { if (a & (1 0)"
  },
  {
    "id": 10946,
    "content": "true : false; mask = too_large 0 : (~0) > sign_pos) & 1; } d = (a & ~mask) | (sign_bit mask | 0); PTX ISA Notes Introduced in PTX ISA version 7"
  },
  {
    "id": 10962,
    "content": "wrap }; Description Generates a 32-bit mask starting from the bit position specified in operand a , and of the width specified in operand b"
  },
  {
    "id": 10971,
    "content": "clamp) { if (a >= 32) { bit-position-overflow = true; mask0 = 0; } if (b >= 32) { bit-width-overflow = true; } } if (sum-overflow || bit-position-overflow || bit-width-overflow) { mask1 = 0; } else if (b1 == 0) { mask1 = ~0; } d = mask0 & ~mask1; Notes The bitmask width specified by operand b is limited to range 0"
  },
  {
    "id": 10991,
    "content": "s32 }; Description Four-way byte dot product which is accumulated in 32-bit result Operand a and b are 32-bit inputs which hold 4 byte inputs in packed form for dot product"
  },
  {
    "id": 10994,
    "content": "btype); for (i = 0; i + c;   for hi variant d = t + c;   for lo variant carry-out from addition is written to CC"
  },
  {
    "id": 10995,
    "content": "CF Notes Generally used in combination with madc and addc to implement extended-precision multi-word multiplication Extended-Precision Arithmetic Instructions: madc  madc Multiply two values, extract high or low half of result, and add a third value with carry-in and optional carry-out Syntax madc{"
  },
  {
    "id": 11003,
    "content": "s64 }; Description Multiplies two values, extracts either the high or low part of the result, and adds a third value along with carry-in Writes the result to the destination register and optionally writes the carry-out from the addition into the condition code register"
  },
  {
    "id": 11022,
    "content": "The floating-point instructions are: testp copysign add sub mul fma mad div abs neg min max rcp sqrt rsqrt sin cos lg2 ex2 tanh Instructions that support rounding modifiers are IEEE-754 compliant"
  },
  {
    "id": 11023,
    "content": "Single-precision instructions support subnormal inputs and results by default for sm_20 and subsequent targets, and flush subnormal inputs and results to sign-preserving zero for sm_1x targets"
  },
  {
    "id": 11025,
    "content": "ftz modifier on single-precision instructions provides backward compatibility with sm_1x targets by flushing subnormal inputs and results to sign-preserving zero regardless of the target architecture Single-precision add , sub , mul , and mad support saturation of results to the range [0"
  },
  {
    "id": 11034,
    "content": "f64 , which maps input NaN s to a canonical NaN ) Note that future implementations may support NaN payloads for single-precision instructions, so PTX programs should not rely on the specific single-precision NaN s being generated"
  },
  {
    "id": 11091,
    "content": "f64 }; Description testp tests common properties of floating-point numbers and returns a predicate value of 1 if True and 0 if False testp finite True if the input is not infinite or NaN testp infinite True if the input is positive or negative infinity testp number True if the input is not NaN testp notanumber True if the input is NaN testp normal True if the input is a normal number (not NaN ,"
  },
  {
    "id": 11092,
    "content": "not infinity) testp subnormal True if the input is a subnormal number (not NaN , not infinity) As a special case, positive and negative zero are considered normal numbers"
  },
  {
    "id": 11110,
    "content": "rn mantissa LSB rounds to nearest even rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is rn Note that an add instruction with an explicit rounding modifier is treated conservatively by the code optimizer An add instruction with no rounding modifier defaults to"
  },
  {
    "id": 11111,
    "content": "round-to-nearest-even and may be optimized aggressively by the code optimizer In particular, mul / add sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device Rounding modifiers have the following target requirements: rn , rz available for all targets rm , rp for add"
  },
  {
    "id": 11126,
    "content": "rn mantissa LSB rounds to nearest even rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is rn Note that a sub instruction with an explicit rounding modifier is treated conservatively by the code optimizer A sub instruction with no rounding modifier defaults to"
  },
  {
    "id": 11127,
    "content": "round-to-nearest-even and may be optimized aggressively by the code optimizer In particular, mul / sub sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device Rounding modifiers have the following target requirements: rn , rz available for all targets rm , rp for sub"
  },
  {
    "id": 11143,
    "content": "rn mantissa LSB rounds to nearest even rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity The default value of rounding modifier is rn Note that a mul instruction with an explicit rounding modifier is treated conservatively by the code optimizer A mul instruction with no rounding modifier defaults to"
  },
  {
    "id": 11144,
    "content": "round-to-nearest-even and may be optimized aggressively by the code optimizer In particular, mul/add and mul/sub sequences with no rounding modifiers may be optimized to use fused-multiply-add instructions on the target device Rounding modifiers have the following target requirements: rn , rz available for all targets rm , rp for mul"
  },
  {
    "id": 11157,
    "content": "rp }; Description Performs a fused multiply-add with no loss of precision in the intermediate product and addition"
  },
  {
    "id": 11159,
    "content": "f32 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision The resulting value is then rounded to single precision using the rounding mode specified by"
  },
  {
    "id": 11162,
    "content": "f64 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision The resulting value is then rounded to double precision using the rounding mode specified by"
  },
  {
    "id": 11165,
    "content": "rn mantissa LSB rounds to nearest even rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported"
  },
  {
    "id": 11183,
    "content": "rp }; Description Multiplies two values and adds a third, and then writes the resulting value into a destination register"
  },
  {
    "id": 11186,
    "content": "f32 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision"
  },
  {
    "id": 11188,
    "content": "f64 computes the product of a and b to infinite precision and then adds c to this product, again in infinite precision"
  },
  {
    "id": 11191,
    "content": "f32 computes the product of a and b at double precision, and then the mantissa is truncated to 23 bits, but the exponent is preserved Note that this is different from computing the product with mul , where the mantissa can be rounded and the exponent will be clamped"
  },
  {
    "id": 11204,
    "content": "In this case, mad f32 can produce slightly different numeric results and backward compatibility is not guaranteed in this case"
  },
  {
    "id": 11239,
    "content": "rp }; Description Divides a by b , stores result in d Semantics d = a / b; Notes Fast, approximate single-precision divides: div"
  },
  {
    "id": 11263,
    "content": "abs modifier is specified, the magnitude of destination operand d is the maximum of absolute values of both the input arguments"
  },
  {
    "id": 11265,
    "content": "xorsign modifier is specified, the sign bit of destination d is equal to the XOR of the sign bits of both the inputs"
  },
  {
    "id": 11268,
    "content": "xorsign must be specified together and xorsign considers the sign bit of both inputs before applying"
  },
  {
    "id": 11272,
    "content": "abs) { a = |a|; b = |b|; } } if (isNaN(a) && isNaN(b)) d = NaN; else if ( NaN && (isNaN(a) || isNaN(b))) d = NaN; else if (isNaN(a)) d = b; else if (isNaN(b)) d = a; else d = (a > b) a : b; if ("
  },
  {
    "id": 11273,
    "content": "xorsign && isNaN(d)) { setSignBit(d, xorsign); } Notes Subnormal numbers: sm_20+ By default, subnormal numbers are supported"
  },
  {
    "id": 11304,
    "content": "rn mantissa LSB rounds to nearest even rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported"
  },
  {
    "id": 11329,
    "content": "f64 d, a; Description Compute a fast, gross approximation to the reciprocal as follows: extract the most-significant 32 bits of f64 operand a in 1"
  },
  {
    "id": 11333,
    "content": ", ignore the least-significant 32 bits of a ), compute an approximate f64 reciprocal of this value using the most-significant 20 bits of the mantissa of operand a , place the resulting 32-bits in 1"
  },
  {
    "id": 11335,
    "content": "20 IEEE floating-point format in the most-significant 32-bits of destination d ,and zero the least significant 32 mantissa bits of"
  },
  {
    "id": 11344,
    "content": "Input a[63:32] Result d[63:32] -Inf -0 0 -subnormal -Inf -0 0 -Inf +0 0 +Inf +subnormal +Inf +Inf +0"
  },
  {
    "id": 11369,
    "content": "rn mantissa LSB rounds to nearest even rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity Subnormal numbers: sm_20+ By default, subnormal numbers are supported"
  },
  {
    "id": 11384,
    "content": "Floating Point Instructions: rsqrt  rsqrt Take the reciprocal of the square root of a value Syntax rsqrt"
  },
  {
    "id": 11389,
    "content": "f64 d, a; Description Compute 1/sqrt(a) and store the result in d Semantics d = 1/sqrt(a); Notes rsqrt"
  },
  {
    "id": 11391,
    "content": "Input Result -Inf NaN -normal NaN -subnormal -Inf -0 0 -Inf +0 0 +Inf +subnormal +Inf +Inf +0 0 NaN NaN The maximum absolute error for rsqrt"
  },
  {
    "id": 11414,
    "content": "f64 d, a; Description Compute a double-precision ( f64 ) approximation of the square root reciprocal of a value The least significant 32 bits of the double-precision ( f64 ) destination d are all zeros"
  },
  {
    "id": 11422,
    "content": "Input Result -Inf NaN -subnormal -Inf -0 0 -Inf +0 0 +Inf +subnormal +Inf +Inf +0 0 NaN NaN Input NaN s map to a canonical NaN with encoding 0x7fffffff00000000 Input Result -Inf NaN -subnormal -0"
  },
  {
    "id": 11445,
    "content": "Input Result -Inf NaN -subnormal -Inf -0 0 -Inf +0 0 -Inf +subnormal -Inf +Inf +Inf NaN NaN The maximum absolute error is 2 -22"
  },
  {
    "id": 11456,
    "content": "Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh"
  },
  {
    "id": 11462,
    "content": "Results of tanh for various corner-case inputs are as follows: Input Result -Inf -1 0 -subnormal Same as input -0"
  },
  {
    "id": 11466,
    "content": "0 +subnormal Same as input +Inf 1 0 NaN NaN The subnormal numbers are supported Note The subnormal inputs gets passed through to the output since the value of tanh(x) for small values of x is approximately the same as x"
  },
  {
    "id": 11470,
    "content": "The half precision floating-point instructions are: add sub mul fma neg abs min max tanh ex2 Half-precision add , sub , mul , and fma support saturation of results to the range [0"
  },
  {
    "id": 11494,
    "content": "bf16x2 instruction type, forms input vectors by half word values from source operands Half-word operands are then added in parallel to produce"
  },
  {
    "id": 11497,
    "content": "Semantics if (type == f16 || type == bf16) { d = a + b; } else if (type == f16x2 || type == bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; for (i = 0; i -0"
  },
  {
    "id": 11502,
    "content": "f16x2 b0,b1,b2;   SIMD fp16 min with NaN min NaN f16x2 b0,b1,b2; min bf16 h0, h1, h2;   SIMD bf16 min with NaN min NaN bf16x2 b0, b1, b2;   scalar bf16 min with xorsign abs min xorsign abs bf16 Rd, Ra, Rb 9"
  },
  {
    "id": 11527,
    "content": "bf16x2 instruction types, input vectors are formed with half-word values from source operands Half-word operands are then processed in parallel to store"
  },
  {
    "id": 11532,
    "content": "abs) { a = |a|; b = |b|; } } if (isNaN(a) && isNaN(b)) d = NaN; if ( NaN && (isNaN(a) || isNaN(b))) d = NaN; else if (isNaN(a)) d = b; else if (isNaN(b)) d = a; else d = (a > b) a : b; if ("
  },
  {
    "id": 11533,
    "content": "xorsign && isNaN(d)) { setSignBit(d, xorsign); } } else if (type == f16x2 || type == bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; for (i = 0; i fB[i]) fA[i] : fB[i]; if ( xorsign && isNaN(fA[i])) { setSignBit(d[i], xorsign); } } } Notes Subnormal numbers: By default, subnormal numbers are supported"
  },
  {
    "id": 11536,
    "content": "f16 h0,h1,h2; max f16x2 b0,b1,b2;   SIMD fp16 max with NaN max NaN f16x2 b0,b1,b2;   scalar f16 max with xorsign abs max xorsign abs f16 Rd, Ra, Rb; max bf16 h0, h1, h2;   scalar bf16 max and NaN max NaN bf16x2 b0, b1, b2;   SIMD bf16 max with xorsign abs max xorsign abs bf16x2 Rd, Ra, Rb; 9"
  },
  {
    "id": 11540,
    "content": "Half Precision Floating Point Instructions: tanh  tanh Find the hyperbolic tangent of a value (in radians) Syntax tanh"
  },
  {
    "id": 11549,
    "content": "bf16x2 instruction type, each of the half-word operands are operated in parallel and the results are packed appropriately into a"
  },
  {
    "id": 11639,
    "content": "Comparison and Selection Instructions  The comparison select instructions are: set setp selp slct As with single-precision floating-point instructions, the set , setp , and slct instructions support subnormal numbers for sm_20 and higher targets and flush single-precision subnormal inputs to sign-preserving zero for sm_1x targets"
  },
  {
    "id": 11641,
    "content": "ftz modifier provides backward compatibility with sm_1x targets by flushing subnormal inputs and results to sign-preserving zero regardless of the target architecture"
  },
  {
    "id": 11646,
    "content": "Comparison and Selection Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a predicate value by applying a Boolean operator Syntax set"
  },
  {
    "id": 11674,
    "content": "f64 }; Description Compares two numeric values and optionally combines the result with another predicate value by applying a Boolean operator"
  },
  {
    "id": 11676,
    "content": "0f is written for floating-point destination types, and 0xffffffff is written for integer destination types"
  },
  {
    "id": 11682,
    "content": "0f : 0x00000000; else d = BoolOp(t, c) 0xffffffff : 0x00000000; Integer Notes The signed and unsigned comparison operators are eq , ne , lt , le , gt , ge For unsigned values, the comparison operators lo , ls , hi , and hs for lower, lower-or-same, higher, and higher-or-same may be used instead of lt , le , gt , ge , respectively"
  },
  {
    "id": 11683,
    "content": "To aid comparison operations in the presence of NaN values, unordered versions are included: equ , neu , ltu , leu , gtu , geu"
  },
  {
    "id": 11684,
    "content": "If both operands are numeric values (not NaN ), then these comparisons have the same result as their ordered counterparts"
  },
  {
    "id": 11685,
    "content": "Comparison and Selection Instructions: setp  setp Compare two numeric values with a relational operator, and (optionally) combine this result with a predicate value by applying a Boolean operator Syntax setp"
  },
  {
    "id": 11707,
    "content": "f64 }; Description Compares two values and combines the result with another predicate value by applying a Boolean operator"
  },
  {
    "id": 11708,
    "content": "A related value computed using the complement of the compare result is written to the second destination operand"
  },
  {
    "id": 11711,
    "content": "Comparison and Selection Instructions: selp  selp Select between source operands, based on the value of the predicate source operand Syntax selp"
  },
  {
    "id": 11723,
    "content": "f64 }; Description Conditional selection Comparison and Selection Instructions: slct  slct Select one source operand, based on the sign of the third operand Syntax slct"
  },
  {
    "id": 11741,
    "content": "Operands d , a , and b are treated as a bitsize type of the same width as the first instruction type; operand c must match the second instruction type ("
  },
  {
    "id": 11758,
    "content": "Half Precision Comparison Instructions: set  set Compare two numeric values with a relational operator, and optionally combine this result with a predicate value by applying a Boolean operator"
  },
  {
    "id": 11759,
    "content": "Result of this computation is written in destination register in the following way: If result is True , 0xffffffff is written for destination types"
  },
  {
    "id": 11774,
    "content": "bf16x2) { fA[0] = a[0:15]; fA[1] = a[16:31]; fB[0] = b[0:15]; fB[1] = b[16:31]; t[0] = (fA[0] CmpOp fB[0]) 1 : 0; t[1] = (fA[1] CmpOp fB[1])"
  },
  {
    "id": 11778,
    "content": "r:   extract 32 lsbs u32 d = (b > n); } Notes Use funnel shift for multi-word shift operations and for rotate operations The shift amount is limited to the range 0"
  },
  {
    "id": 11779,
    "content": "32 in clamp mode and 0 31 in wrap mode, so shifting multi-word values by distances greater than 32 requires first moving 32-bit words, then using shf to shift the remaining 0"
  },
  {
    "id": 11782,
    "content": "r instructions applied to adjacent words, operating from least-significant word towards most-significant word"
  },
  {
    "id": 11784,
    "content": "l instructions applied to adjacent words, operating from most-significant word towards least-significant word"
  },
  {
    "id": 11785,
    "content": "Use funnel shift to perform 32-bit left or right rotate by supplying the same value for source arguments a and b"
  },
  {
    "id": 11801,
    "content": "b32 r1,r0,r0,n;   rotate right by n; n > b; Notes Shift amounts greater than the register width N are clamped to N"
  },
  {
    "id": 11803,
    "content": "Data Movement and Conversion Instructions  These instructions copy data from place to place, and from state space to state space, possibly converting it from one format to another The isspacep instruction is provided to query whether a generic address falls within a particular state space window The cvta instruction converts addresses between generic and const , global , local , or shared state"
  },
  {
    "id": 11836,
    "content": "Cache Operators  PTX ISA version 2 0 introduced optional cache operators on load and store instructions"
  },
  {
    "id": 11837,
    "content": "The use of a cache operator on an ld or st instruction does not change the memory consistency behavior of the program Table 27 Cache Operators for Memory Load Instructions  Operator Meaning"
  },
  {
    "id": 11841,
    "content": "Global data is coherent at the L2 level, but multiple L1 caches are not coherent for global data If one thread stores to global memory via one L1 cache, and a second thread loads that address via a second L1 cache with ld ca , the second thread may get stale L1 cache data, rather than the data stored by the first thread The driver must invalidate global L1 cache lines between dependent grids of"
  },
  {
    "id": 11842,
    "content": "parallel threads Stores by the first grid program are then correctly fetched by the second grid program issuing default ld ca loads cached in L1 Use ld cg to cache loads only globally, bypassing the L1 cache, and cache only in the L2 cache The ld cs load cached streaming operation allocates global lines with evict-first policy in L1 and L2 to limit cache pollution by temporary streaming data that"
  },
  {
    "id": 11844,
    "content": "lu when restoring spilled registers and popping function stack frames to avoid needless write-backs of lines that will not be used again"
  },
  {
    "id": 11848,
    "content": "cv load operation applied to a global System Memory address invalidates (discards) a matching L2 line and re-fetches the line on each new load"
  },
  {
    "id": 11851,
    "content": "The default store instruction cache operation is st wb , which writes back cache lines of coherent cache levels with normal eviction policy If one thread stores to global memory, bypassing its L1 cache, and a second thread in a different SM later loads from that address via a different L1 cache with ld ca , the second thread may get a hit on stale L1 cache data, rather than get the data from L2"
  },
  {
    "id": 11852,
    "content": "or memory stored by the first thread The driver must invalidate global L1 cache lines between dependent grids of thread arrays Stores by the first grid program are then correctly missed in L1 and fetched by the second grid program issuing default ld ca loads Use st cg to cache global store data only globally, bypassing the L1 cache, and cache only in the L2 cache The st cs store cached-streaming"
  },
  {
    "id": 11853,
    "content": "operation allocates cache lines with evict-first policy to limit cache pollution by streaming output data The st wt store write-through operation applied to a global System Memory address writes through the L2 cache"
  },
  {
    "id": 11858,
    "content": "Cache Eviction Priority Hints  PTX ISA version 7 4 adds optional cache eviction priority hints on load and store instructions"
  },
  {
    "id": 11861,
    "content": "Table 29 Cache Eviction Priority Hints for Memory Load and Store Instructions  Cache Eviction Priority Meaning evict_normal Cache data with normal eviction priority evict_first Data cached with this priority will be first in the eviction priority order and will likely be evicted when cache eviction is required evict_last Data cached with this priority will be last in the eviction priority order"
  },
  {
    "id": 11862,
    "content": "and will likely be evicted only after other data with evict_normal or evict_first eviction priotity is already evicted no_allocate Do not allocate data to cache"
  },
  {
    "id": 11867,
    "content": "Data Movement and Conversion Instructions: mov  mov Set a register variable with the value of a register variable or an immediate value Syntax mov type d, a; mov type d, sreg; mov type d, avar; get address of variable mov type d, avar+imm; get address of variable with offset mov u32 d, fname; get address of device function mov u64 d, fname; get address of device function mov u32 d, kernel; get"
  },
  {
    "id": 11880,
    "content": "f64 }; Description Write register d with the value of a Operand a may be a register, special register, variable with optional offset in an addressable memory space, or function name"
  },
  {
    "id": 11887,
    "content": ", the address of the variable in its state space) into the destination register The generic address of a variable in const , global , local , or shared state space may be generated by first taking the address within the state space with mov and then converting it to a generic address using the cvta instruction; alternately, the generic address of a variable declared in const , global , local , or"
  },
  {
    "id": 11888,
    "content": "shared state space may be taken directly using the cvta instruction Note that if the address of a device function parameter is moved to a register, the parameter will be copied onto the stack and the address will be in the local state space Semantics d = a; d = sreg; d = &avar; address is non-generic; i"
  },
  {
    "id": 11890,
    "content": ", within the variable's declared state space d = &avar+imm; Notes Although only predicate and bit-size types are required, we include the arithmetic types for the programmer’s convenience: their use enhances program readability and allows additional type checking"
  },
  {
    "id": 11894,
    "content": "Kernel function addresses should only be used in the context of CUDA Dynamic Parallelism system calls"
  },
  {
    "id": 11895,
    "content": "Examples mov f32 d,a; mov u16 u,v; mov f32 k,0 1; mov u32 ptr, A;   move address of A into ptr mov u32 ptr, A[5];   move address of A[5] into ptr mov u32 ptr, A+20;   move address with offset into ptr mov u32 addr, myFunc;   get address of device function 'myFunc' mov u64 kptr, main;   get address of entry function 'main' 9"
  },
  {
    "id": 11899,
    "content": "Data Movement and Conversion Instructions: mov  mov Move vector-to-scalar (pack) or scalar-to-vector (unpack) Syntax mov"
  },
  {
    "id": 11904,
    "content": "b128 }; Description Write scalar register d with the packed value of vector register a , or write vector register d with the unpacked values from scalar register a When destination operand d is a vector register, the sink symbol '_' may be used for one or more elements provided that at least one element is a scalar register For bit-size types, mov may be used to pack vector elements into a scalar"
  },
  {
    "id": 11905,
    "content": "register or unpack sub-fields of a scalar register into a vector Both the overall size of the vector and the size of the scalar must match the size of the instruction type Semantics pack two 8-bit elements into"
  },
  {
    "id": 11909,
    "content": "down: j = lane + bval; pval = (j > 0) & 0xf; ctl[1] = (c >> 4) & 0xf; ctl[2] = (c >> 8) & 0xf; ctl[3] = (c >> 12) & 0xf; } else { ctl[0] = ctl[1] = ctl[2] = ctl[3] = (c >> 0) & 0x3; } tmp[07:00] = ReadByte( mode, ctl[0], tmp64 ); tmp[15:08] = ReadByte( mode, ctl[1], tmp64 ); tmp[23:16] = ReadByte( mode, ctl[2], tmp64 ); tmp[31:24] = ReadByte( mode, ctl[3], tmp64 ); PTX ISA Notes Introduced in PTX"
  },
  {
    "id": 11912,
    "content": "Data Movement and Conversion Instructions: ld  ld Load a register variable from an addressable state space variable"
  },
  {
    "id": 11915,
    "content": "Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands If no sub-qualifier is specified with"
  },
  {
    "id": 11916,
    "content": "param state space, then : ::func is assumed when access is inside a device function Otherwise, when accessing device function parameters or any other param variables from entry function ::func is assumed by default"
  },
  {
    "id": 11918,
    "content": "param::entry instruction, operand a must be a kernel parameter address, otherwise behavior is undefined"
  },
  {
    "id": 11920,
    "content": "param::func instruction, operand a must be a device function parameter address, otherwise behavior is undefined Instruction ld param{::func} used for reading value returned from device function call cannot be predicated See Parameter State Space and Function Declarations and Definitions for descriptions of the proper use of ld param"
  },
  {
    "id": 11927,
    "content": "The effects of this instruction become visible to other threads only when synchronization is established by other means"
  },
  {
    "id": 11929,
    "content": "volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location"
  },
  {
    "id": 11942,
    "content": "unified must be specified on operand a if a is the address of a variable declared with unified attribute as described in Variable and Function Attribute Directive: attribute"
  },
  {
    "id": 11945,
    "content": "The level::prefetch_size qualifier is a hint to fetch additional data of the specified size into the respective cache level The sub-qualifier prefetch_size can be set to either of 64B , 128B , 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes respectively The qualifier level::prefetch_size may only be used with"
  },
  {
    "id": 11946,
    "content": "global state space and with generic addressing where the address points to global state space If the generic address does not fall within the address window of the global memory, then the prefetching behavior is undefined"
  },
  {
    "id": 11949,
    "content": "The 64-bit operand cache-policy specifies the cache eviction policy that may be used during the memory access"
  },
  {
    "id": 11954,
    "content": "It is treated as a performance hint only, and does not change the memory consistency behavior of the program 1 This synchronization is further extended to other threads through the transitive nature of causality order , as described in the memory consistency model"
  },
  {
    "id": 11955,
    "content": "Semantics d = a;   named variable a d = *(&a+immOff)   variable-plus-offset d = *a;   register d = *(a+immOff);   register-plus-offset d = *(immAddr);   immediate address Notes Destination d must be in the"
  },
  {
    "id": 11957,
    "content": "The value loaded is sign-extended to the destination register width for signed integers, and is zero-extended to the destination register width for unsigned and bit-size types"
  },
  {
    "id": 11970,
    "content": "level::eviction_priority , level::prefetch_size and level::cache_hint qualifiers introduced in PTX ISA version 7"
  },
  {
    "id": 11982,
    "content": "global nc Load a register variable from global state space via non-coherent cache Note On some architectures, the texture cache is larger, has higher bandwidth, and longer latency than the global memory cache For applications with sufficient parallelism to cover the longer latency, ld global nc should offer better performance than ld global on such architectures"
  },
  {
    "id": 11983,
    "content": "Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The qualifier"
  },
  {
    "id": 12013,
    "content": "Data Movement and Conversion Instructions: ldu  ldu Load read-only data from an address that is common across threads in the warp Syntax ldu{"
  },
  {
    "id": 12038,
    "content": "f64 }; Description Load read-only data into register variable d from the location specified by the source address operand a in the global state space, where the address is guaranteed to be the same across all threads in the warp Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands Semantics d = a; named variable a d = *(&a+immOff)"
  },
  {
    "id": 12039,
    "content": "variable-plus-offset d = *a; register d = *(a+immOff); register-plus-offset d = *(immAddr); immediate address Notes Destination d must be in the reg state space"
  },
  {
    "id": 12059,
    "content": "Data Movement and Conversion Instructions: st  st Store data to an addressable state space variable"
  },
  {
    "id": 12060,
    "content": "Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands If"
  },
  {
    "id": 12064,
    "content": "See Parameter State Space and Function Declarations and Definitions for descriptions of the proper use of st"
  },
  {
    "id": 12073,
    "content": "volatile operation is always performed and it will not be reordered with respect to other volatile operations to the same memory location"
  },
  {
    "id": 12085,
    "content": "Semantics d = a;   named variable d *(&a+immOffset) = b;   variable-plus-offset *a = b;   register *(a+immOffset) = b;   register-plus-offset *(immAddr) = b;   immediate address Notes Operand b must be in the"
  },
  {
    "id": 12153,
    "content": "Data Movement and Conversion Instructions: st async  st async Asynchronous store operation on shared memory Syntax st async{"
  },
  {
    "id": 12175,
    "content": "async is a non-blocking instruction which initiates an asynchronous store operation that stores the value specified by source operand b to the destination memory location specified by operand a"
  },
  {
    "id": 12177,
    "content": "completion_mechanism specifies that upon completion of the asynchronous operation, complete-tx operation, with completeCount argument equal to amount of data stored in bytes, will be performed on the mbarrier object specified by the operand mbar"
  },
  {
    "id": 12178,
    "content": "Operand a represents destination address and must be a register or of the form register + immOff as described in Addresses as Operands The shared memory addresses of destination operand a and the mbarrier object mbar , must meet all of the following conditions: They belong to the same CTA They are different to the CTA of the executing thread but must be within the same cluster"
  },
  {
    "id": 12183,
    "content": "The store operation in st async is treated as a weak memory operation and the complete_tx operation on the mbarrier has"
  },
  {
    "id": 12194,
    "content": "Data Movement and Conversion Instructions: multimem ld_reduce, multimem st, multimem red  The multimem * operations operate on multimem addresses and accesses all of the multiple memory locations which the multimem address points to Accessing a multimem address with ld , st or any other memory operations results in undefined behavior multimem ld_reduce, multimem st, multimem red Perform memory"
  },
  {
    "id": 12195,
    "content": "operations on the multimem address Instruction multimem st performs a store operation of the input operand b to all the memory locations pointed to by the multimem address a Instruction multimem red performs a reduction operation on all the memory locations pointed to by the multimem address a , with operand b Instruction multimem ld_reduce performs reduction on the values loaded from all the"
  },
  {
    "id": 12196,
    "content": "memory locations that the multimem address points to In contrast, the multimem red perform reduction on all the memory locations that the multimem address points to"
  },
  {
    "id": 12197,
    "content": "Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands"
  },
  {
    "id": 12247,
    "content": "scope qualifier specifies the set of threads that can directly observe the memory synchronizing effect of this operation, as described in Memory Consistency Model"
  },
  {
    "id": 12291,
    "content": "Data Movement and Conversion Instructions: prefetch, prefetchu  prefetch, prefetchu Prefetch line containing a generic address at a specified level of memory hierarchy, in specified state space Syntax prefetch{"
  },
  {
    "id": 12295,
    "content": "level::eviction_priority [a];   prefetch to data cache prefetchu L1 [a];   prefetch to uniform cache prefetch{"
  },
  {
    "id": 12308,
    "content": "param }; Description The prefetch instruction brings the cache line containing the specified address in global or local memory state space into the specified cache level"
  },
  {
    "id": 12310,
    "content": "tensormap qualifier is specified then the prefetch instruction brings the cache line containing the specified address in the"
  },
  {
    "id": 12316,
    "content": "Optionally, the eviction priority to be applied on the prefetched cache line can be specified by the modifier"
  },
  {
    "id": 12318,
    "content": "Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The prefetchu instruction brings the cache line containing the specified generic address into the specified uniform cache level A prefetch into the uniform cache requires a generic address, and no operation occurs if the address maps to a const , local , or shared memory location Examples"
  },
  {
    "id": 12330,
    "content": "Data Movement and Conversion Instructions: applypriority  applypriority Apply the cache eviction priority to the specified address in the specified cache level Syntax applypriority{"
  },
  {
    "id": 12333,
    "content": "L2::evict_normal }; Description The applypriority instruction applies the cache eviction priority specified by the"
  },
  {
    "id": 12334,
    "content": "level::eviction_priority qualifier to the address range [a a+size) in the specified cache level If the specified address does not fall within the address window of"
  },
  {
    "id": 12336,
    "content": "The operand size is an integer constant that specifies the amount of data, in bytes, in the specified cache level on which the priority is to be applied If the data pointed to by address a is not already present in the specified cache level, then the data will be prefetched before applying the specified priority Data Movement and Conversion Instructions: discard  discard Invalidate the data in"
  },
  {
    "id": 12341,
    "content": "a + (size - 1)] in the cache level specified by the level qualifier without writing back the data in the cache to the memory The operand size is an integer constant that specifies the amount of data, in bytes, in the cache level specified by the level qualifier to be discarded Data Movement and Conversion Instructions: createpolicy  createpolicy Create a cache eviction policy for the specified"
  },
  {
    "id": 12356,
    "content": "L2::evict_unchanged }; Description The createpolicy instruction creates a cache eviction policy for the specified cache level in an opaque 64-bit register specified by the destination operand cache-policy The cache eviction policy specifies how cache eviction priorities are applied to global memory addresses used in memory operations with"
  },
  {
    "id": 12358,
    "content": "There are two types of cache eviction policies: Range-based policy The cache eviction policy created using createpolicy range specifies the cache eviction behaviors for the following three address ranges: [a When a range-based cache eviction policy is used in a memory operation with level::cache_hint qualifier, the eviction priorities are applied as follows: If the memory address falls in the"
  },
  {
    "id": 12362,
    "content": "L2::secondary_priority is applied If the memory address does not fall in either of the above ranges, then the applied eviction priority is unspecified The 32-bit operand total-size specifies the combined size, in bytes, of the address range including primary and secondary ranges"
  },
  {
    "id": 12363,
    "content": "Fraction-based policy A memory operation with level::cache_hint qualifier can use the fraction-based cache eviction policy to request the cache eviction priority specified by L2:primary_priority to be applied to a fraction of cache accesses specified by the 32-bit floating point operand fraction The remainder of the cache accesses get the eviction priority specified by"
  },
  {
    "id": 12365,
    "content": "This implies that in a memory operation that uses a fraction-based cache policy, the memory access has a probability specified by the operand fraction of getting the cache eviction priority specified by"
  },
  {
    "id": 12367,
    "content": "The access property created using the CUDA APIs can be converted into cache eviction policy by the instruction createpolicy"
  },
  {
    "id": 12381,
    "content": "Data Movement and Conversion Instructions: isspacep  isspacep Query whether a generic address falls within a specified state space window Syntax isspacep"
  },
  {
    "id": 12388,
    "content": "param{::entry} }; Description Write predicate register p with 1 if generic address a falls within the specified state space window and with 0 otherwise"
  },
  {
    "id": 12390,
    "content": "param{::entry} returns 1 if the generic address falls within the window of Kernel Function Parameters , otherwise returns 0"
  },
  {
    "id": 12394,
    "content": "Note ispacep shared::cluster will return 1 for every shared memory address that is accessible to the threads in the cluster, whereas ispacep shared::cta will return 1 only if the address is of a variable declared in the executing CTA"
  },
  {
    "id": 12395,
    "content": "Examples isspacep const iscnst, cptr; isspacep global isglbl, gptr; isspacep local islcl, lptr; isspacep shared isshrd, sptr; isspacep param::entry isparam, pptr; isspacep shared::cta isshrdcta, sptr; isspacep shared::cluster ishrdany sptr; 9"
  },
  {
    "id": 12416,
    "content": "size p, var+imm;   generic address of var+offset   convert generic address to const, global, local, or shared address cvta"
  },
  {
    "id": 12438,
    "content": "When converting a generic address into a const , Kernel Function Parameters ( param ), global , local , or shared address, the resulting address is undefined in cases where the generic address does not fall within the address window of the specified state space"
  },
  {
    "id": 12440,
    "content": "shared state space, the address must belong to the space specified by ::cta or ::cluster sub-qualifier, otherwise the behavior is undefined"
  },
  {
    "id": 12441,
    "content": "Note: The current implementation does not allow generic pointers to const space variables in programs that contain pointers to constant buffers passed as kernel parameters"
  },
  {
    "id": 12462,
    "content": "Data Movement and Conversion Instructions: cvt  cvt Convert a value from one type to another Syntax cvt{"
  },
  {
    "id": 12551,
    "content": "bf16 type and the converted values are packed in the destination register d , such that the value converted from input a is stored in the upper half of d and the value converted from input b is stored in the lower half of d For"
  },
  {
    "id": 12560,
    "content": "e5m2x2 , each input is converted to the specified format, and the converted values are packed in the destination operand d such that the value converted from input a is stored in the upper 8 bits of d and the value converted from input b is stored in the lower 8 bits of d"
  },
  {
    "id": 12565,
    "content": "f16 input from operand a is converted to the specified format The converted values are packed in the destination operand d such that the value converted from the upper 16 bits of input a is stored in the upper 8 bits of d and the value converted from the lower 16 bits of input a is stored in the lower 8 bits of d The converted values are packed in the destination operand d such that the value"
  },
  {
    "id": 12566,
    "content": "converted from the upper 8 bits of a is stored in the upper 16 bits of d and the value converted from the lower 8 bits of a is stored in the lower 16 bits of d"
  },
  {
    "id": 12567,
    "content": "Rounding modifier is mandatory in all of the following cases: float-to-float conversions, when destination type is smaller than source type All float-to-int conversions All int-to-float conversions All conversions involving"
  },
  {
    "id": 12572,
    "content": "tf32 instruction types satfinite modifier is only supported for conversions involving the following types:"
  },
  {
    "id": 12578,
    "content": "bf16x2 */) { d[31:16] = convert(a); d[15:0] = convert(b); } else { d = convert(a); } Integer Notes Integer rounding is required for float-to-integer conversions, and for same-size float-to-float conversions where the value is rounded to an integer Integer rounding modifiers: rni round to nearest integer, choosing even integer if source is equidistant between two integers rzi round to nearest"
  },
  {
    "id": 12579,
    "content": "integer in the direction of zero rmi round to nearest integer in direction of negative infinity rpi round to nearest integer in direction of positive infinity In float-to-integer conversion, NaN inputs are converted to 0"
  },
  {
    "id": 12585,
    "content": "f32 f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero"
  },
  {
    "id": 12596,
    "content": "f32 f32 float-to-float conversions with integer rounding, subnormal inputs are flushed to sign-preserving zero"
  },
  {
    "id": 12598,
    "content": "4 and earlier, the cvt instruction did not flush single-precision subnormal inputs or results to zero if the destination type size was 64-bits"
  },
  {
    "id": 12602,
    "content": "The saturation modifier is allowed only in cases where the destination type’s value range is not a superset of the source type’s value range; i"
  },
  {
    "id": 12605,
    "content": "sat modifier is illegal in cases where saturation is not possible based on the source and destination types"
  },
  {
    "id": 12609,
    "content": "Floating Point Notes Floating-point rounding is required for float-to-float conversions that result in loss of precision, and for integer-to-float conversions Floating-point rounding modifiers:"
  },
  {
    "id": 12610,
    "content": "rn mantissa LSB rounds to nearest even rna mantissa LSB rounds to nearest, ties away from zero rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity A floating-point value may be rounded to an integral value using the integer rounding modifiers (see Integer Notes)"
  },
  {
    "id": 12614,
    "content": "4 and earlier, the cvt instruction did not flush single-precision subnormal inputs or results to zero if either source or destination type was"
  },
  {
    "id": 12617,
    "content": "4 or earlier, single-precision subnormal inputs and results are flushed to sign-preserving zero only for cvt"
  },
  {
    "id": 12640,
    "content": "tf32 destination formats, if the input value is NaN , then the result is NaN in the specified destination format If the absolute value of input (ignoring sign) is greater than MAX_NORM of the specified destination format, then the result is sign-preserved MAX_NORM of the destination format"
  },
  {
    "id": 12641,
    "content": "Notes A source register wider than the specified type may be used, except when the source operand has"
  },
  {
    "id": 12644,
    "content": "See Operand Size Exceeding Instruction-Type Size for a description of these relaxed type-checking rules"
  },
  {
    "id": 12645,
    "content": "A destination register wider than the specified type may be used, except when the destination operand has"
  },
  {
    "id": 12649,
    "content": "The result of conversion is sign-extended to the destination register width for signed integers, and is zero-extended to the destination register width for unsigned, bit-size, and floating-point types"
  },
  {
    "id": 12661,
    "content": "Data Movement and Conversion Instructions: cvt pack  cvt pack Convert two integer values from one integer type to another and pack the results Syntax cvt pack"
  },
  {
    "id": 12685,
    "content": "b32 } Description Convert two 32-bit integers a and b into specified type and pack the results into d Source operands a and b are integers of type abType and the source operand c is an integer of type"
  },
  {
    "id": 12688,
    "content": "convertType with saturation and the results after conversion are packed into lower bits of d Data Movement and Conversion Instructions: cp"
  },
  {
    "id": 12695,
    "content": "The size of the source and the destination array must be the same and is specified by the operand size Each data element in the destination array is reduced inline with the corresponding data element in the source array with the reduction operation specified by the modifier"
  },
  {
    "id": 12697,
    "content": "The type of each data element in the source and the destination array is specified by the modifier type The source address operand srcMem is located in the state space specified by src and the destination address operand dstMem is located in the state specified by the"
  },
  {
    "id": 12699,
    "content": "The 32-bit operand size specifies the amount of memory to be copied from the source location and used in the reduction operation, in terms of number of bytes The memory range [dstMem, dstMem + size - 1] must not overflow the destination memory space and the memory range [srcMem, srcMem + size - 1] must not overflow the source memory space"
  },
  {
    "id": 12770,
    "content": "completion_mechanism specifies the completion mechanism that is supported on the instruction variant The completion mechanisms that are supported for different variants are summarized in the following table: Completion mechanism"
  },
  {
    "id": 12784,
    "content": "The complete-tx operation, with completeCount argument equal to amount of data copied in bytes, will be performed on the mbarrier object specified by the operand mbar"
  },
  {
    "id": 12870,
    "content": "prefetch is a non-blocking instruction which may initiate an asynchronous prefetch of data from the location specified by source address operand srcMem , in"
  },
  {
    "id": 12896,
    "content": "The operand dstMem specifies the location in the dst state space into which the tensor data has to be copied and srcMem specifies the location in the src state space from which the tensor data has to be copied The operand tensorMap is the generic address of the opaque tensor-map object which resides either in param space or const space or global space The operand tensorMap specifies the"
  },
  {
    "id": 12897,
    "content": "properties of the tensor copy operation, as described in Tensor-map The vector operand tensorCoords specifies the starting coordinates in the tensor data in the global memory from or to which the copy operation has to be performed The number of tensor coordinates in the vector argument tensorCoords should be equal to the dimension specified by the modifier"
  },
  {
    "id": 12900,
    "content": "completion_mechanism specifies the completion mechanism that is supported on the instruction variant The completion mechanisms that are supported for different variants are summarized in the following table: Completion mechanism"
  },
  {
    "id": 12922,
    "content": "im2col mode, some dimensions of the source tensors are unrolled in a single dimensional column at the destination"
  },
  {
    "id": 12929,
    "content": "multicast::cluster allows copying of data from global memory to shared memory of multiple CTAs in the cluster Operand ctaMask specifies the destination CTAs in the cluster such that each bit position in the 16-bit ctaMask operand corresponds to the %ctaid of the destination CTA The source data is multicast to the same offset as dstMem in the shared memory of each destination CTA The mbarrier"
  },
  {
    "id": 12938,
    "content": "multicast::cluster qualifier is optimized for target architecture sm_90a and may have substantially reduced performance on other targets and hence multicast::cluster is advised to be used with target sm_90a"
  },
  {
    "id": 12968,
    "content": "mbarrier::complete_tx::bytes [sMem2], [tensorMap2, {tc0, tc1, tc2, tc3, tc4}], [mbar2], {i2cOffW, i2cOffH, i2cOffD}; @p cp"
  },
  {
    "id": 13037,
    "content": "tensor is a non-blocking instruction which initiates an asynchronous reduction operation of tensor data in the dst state space with tensor data in the src state space The operand srcMem specifies the location of the tensor data in the src state space using which the reduction operation has to be performed Each element of the tensor data in the dst state space is reduced inline with the"
  },
  {
    "id": 13038,
    "content": "corresponding element from the tensor data in the src state space The type of each tensor data element in the source and the destination tensor is specified in Tensor-map The vector operand tensorCoords specifies the starting coordinates of the tensor data in the global memory on which the reduce operation is to be performed"
  },
  {
    "id": 13064,
    "content": "completion_mechanism specifies the completion mechanism that is supported on the instruction variant"
  },
  {
    "id": 13072,
    "content": "im2col_no_offs mode, some dimensions of the source tensors are unrolled in a single dimensional column at the destination"
  },
  {
    "id": 13129,
    "content": "prefetch tensor Provides a hint to the system to initiate the asynchronous prefetch of tensor data to the cache"
  },
  {
    "id": 13154,
    "content": "prefetch tensor is a non-blocking instruction which may initiate an asynchronous prefetch of tensor data from the location in"
  },
  {
    "id": 13226,
    "content": "tensor} instructions satisfying the following conditions into the new bulk async-group : The prior cp{"
  },
  {
    "id": 13232,
    "content": "tensor} instructions use bulk_group based completion mechanism, and They are initiated by the executing thread but not committed to any bulk async-group"
  },
  {
    "id": 13250,
    "content": "bulk wait_group instruction will cause the executing thread to wait until only N or fewer of the most recent bulk async-groups are pending and all the prior bulk async-groups committed by the executing threads are complete For example, when N is 0, the executing thread waits on all the prior bulk async-groups to complete"
  },
  {
    "id": 13253,
    "content": "bulk wait_group instruction will cause the executing thread to wait till all the bulk async operations in the specified bulk async-group have completed all of the following: Reading from the source locations"
  },
  {
    "id": 13255,
    "content": "read modifier indicates that the waiting has to be done until all the bulk async operations in the specified bulk async-group have completed reading from their source locations"
  },
  {
    "id": 13256,
    "content": "Data Movement and Conversion Instructions: tensormap replace  tensormap replace Modifies the field of a tensor-map object Syntax tensormap replace"
  },
  {
    "id": 13295,
    "content": "replace instruction replaces the field, specified by field qualifier, of the tensor-map object at the location specified by the address operand addr with a new value Qualifier mode specifies the mode of the tensor-map object located at the address operand addr The immediate integer operand ord specifies the ordinal of the field across the rank of the tensor which needs to be replaced in the"
  },
  {
    "id": 13296,
    "content": "tensor-map object For field rank , the operand new_val must be ones less than the desired tensor rank as this field uses zero-based numbering"
  },
  {
    "id": 13298,
    "content": "field3 is specified, the operand new_val must be an immediate and the Table 30 shows the mapping of the operand new_val across various fields Table 30 Tensormap new_val validity  new_val field3"
  },
  {
    "id": 13303,
    "content": "u8 No interleave No swizzling Zero fill 1 u16 16B interleave 32B swizzling OOB-NaN fill 2 u32 32B interleave 64B swizzling x 3 s32 x 128B swizzling x 4"
  },
  {
    "id": 13317,
    "content": "replace is treated as a weak memory operation, on the entire 1024-bit opaque tensor-map object, in the Memory Consistency Model"
  },
  {
    "id": 13327,
    "content": "Texture Instructions  This section describes PTX instructions for accessing textures and samplers PTX supports the following operations on texture and sampler descriptors: Static initialization of texture and sampler descriptors Ability to query fields within texture and sampler descriptors"
  },
  {
    "id": 13332,
    "content": "Texturing Modes  For working with textures and samplers, PTX has two modes of operation The advantage of unified mode is that it allows 256 samplers per kernel (128 for architectures prior to sm_3x ), with the restriction that they correspond 1-to-1 with the 256 possible textures per kernel (128 for architectures prior to sm_3x ) The advantage of independent mode is that textures and samplers"
  },
  {
    "id": 13333,
    "content": "can be mixed and matched, but the number of samplers is greatly restricted to 32 per kernel (16 for architectures prior to sm_3x ) Table 31 summarizes the number of textures, samplers and surfaces available in different texturing modes Table 31 Texture, sampler and surface limits  Texturing mode Resource sm_1x , sm_2x sm_3x+ Unified mode Textures 128 256 Samplers 128 256 Surfaces 8 16 Independent"
  },
  {
    "id": 13354,
    "content": "Mipmaps  A mipmap is a sequence of textures, each of which is a progressively lower resolution representation of the same image The height and width of each image, or level of detail (LOD), in the mipmap is a power of two smaller than the previous level"
  },
  {
    "id": 13356,
    "content": "For example, a high-resolution mipmap image is used for objects that are close to the user; lower-resolution images are used as the object appears farther away"
  },
  {
    "id": 13357,
    "content": "Mipmap filtering modes are provided when switching between two levels of detail (LODs) in order to avoid abrupt changes in visual fidelity"
  },
  {
    "id": 13358,
    "content": "Example: If the texture has a basic size of 256 by 256 pixels, then the associated mipmap set may contain a series of eight images, each one-fourth the total area of the previous one: 128×128 pixels, 64×64, 32×32, 16×16, 8×8, 4×4, 2×2, 1×1 (a single pixel) If, for example, a scene is rendering this texture in a space of 40×40 pixels, then either a scaled up version of the 32×32 (without trilinear"
  },
  {
    "id": 13359,
    "content": "interpolation) or an interpolation of the 64×64 and the 32×32 mipmaps (with trilinear interpolation) would be used"
  },
  {
    "id": 13360,
    "content": "The total number of LODs in a complete mipmap pyramid is calculated through the following equation: numLODs = 1 + floor(log2(max(w, h, d))) The finest LOD is called the base level and is the 0th level Each successively smaller mipmap level has half the {width, height, depth} of the previous level, but if this half value is a fractional value, it’s rounded down to the next largest integer"
  },
  {
    "id": 13361,
    "content": "Essentially, the size of a mipmap level can be specified as: max(1, floor(w_b / 2^i)) x max(1, floor(h_b / 2^i)) x max(1, floor(d_b / 2^i)) where i is the ith level beyond the 0th level (the base level) PTX support for mipmaps The PTX tex instruction supports three modes for specifying the LOD: base , level , and grad ient"
  },
  {
    "id": 13364,
    "content": ", {ds/dx, dt/dx} and {ds/dy, dt/dy} for a 2d texture), which the tex instruction uses to compute the LOD"
  },
  {
    "id": 13365,
    "content": "The instruction loads data from the texture named by operand a at coordinates given by operand c into destination d"
  },
  {
    "id": 13366,
    "content": "Operand c is a scalar or singleton tuple for 1d textures; is a two-element vector for 2d textures; and is a four-element vector for 3d textures, where the fourth element is ignored"
  },
  {
    "id": 13367,
    "content": "The optional destination predicate p is set to True if data from texture at specified coordinates is resident in memory, False otherwise Memory residency of Texture Data at specified coordinates is dependent on execution environment setup using Driver API calls, prior to kernel launch"
  },
  {
    "id": 13368,
    "content": "Refer to Driver API documentation for more details including any system/implementation specific behavior"
  },
  {
    "id": 13369,
    "content": "Operand e is a singleton tuple for 1d textures; is a two element vector 2d textures; and is four-element vector for 3d textures, where the fourth element is ignored"
  },
  {
    "id": 13372,
    "content": "A texture base address is assumed to be aligned to a 16 byte boundary, and the address given by the coordinate vector must be naturally aligned to a multiple of the access size If an address is not properly aligned, the resulting behavior is undefined; i"
  },
  {
    "id": 13374,
    "content": ", the access may proceed by silently masking off low-order address bits to achieve proper rounding, or the instruction may fault"
  },
  {
    "id": 13375,
    "content": "The instruction first selects a texture from the texture array named by operand a using the index given by the first element of the array coordinate vector c The instruction then loads data from the selected texture at coordinates given by the remaining elements of operand c into destination d Operand c is a bit-size type vector or tuple containing an index into the array of textures followed by"
  },
  {
    "id": 13379,
    "content": "The first element is interpreted as an unsigned integer index ( u32 ) into the texture array, and the second element is interpreted as a 1d texture coordinate of type"
  },
  {
    "id": 13381,
    "content": "The first element is interpreted as an unsigned integer index ( u32 ) into the texture array, and the next two elements are interpreted as 2d texture coordinates of type"
  },
  {
    "id": 13383,
    "content": "Operand e is a singleton tuple for 1d texture arrays; and is a two element vector 2d texture arrays When using depth compare operand, the coordinates in texture coordinate vector c have"
  },
  {
    "id": 13385,
    "content": "The texture array index is a 32-bit unsigned integer, and texture coordinate elements are 32-bit signed integer or floating point values"
  },
  {
    "id": 13386,
    "content": "The instruction loads data from the cubemap texture named by operand a at coordinates given by operand c into destination d"
  },
  {
    "id": 13387,
    "content": "Cubemap textures are special two-dimensional layered textures consisting of six layers that represent the faces of a cube"
  },
  {
    "id": 13390,
    "content": "f32 , and comprises three floating-point coordinates ( s , t , r ) and a fourth padding argument which is ignored"
  },
  {
    "id": 13391,
    "content": "The ( s , t , r ) coordinates can be thought of as a direction vector emanating from the center of the cube Of the three coordinates ( s , t , r ), the coordinate of the largest magnitude (the major axis) selects the cube face Then, the other two coordinates (the minor axes) are divided by the absolute value of the major axis to produce a new ( s , t ) coordinate pair to lookup into the selected"
  },
  {
    "id": 13395,
    "content": "The instruction first selects a cubemap texture from the cubemap array named by operand a using the index given by the first element of the array coordinate vector c The instruction then loads data from the selected cubemap texture at coordinates given by the remaining elements of operand c into destination d Cubemap array textures consist of an array of cubemaps, i"
  },
  {
    "id": 13398,
    "content": "The first element is interpreted as an unsigned integer index ( u32 ) into the cubemap array, and the remaining three elements are interpreted as floating-point cubemap coordinates ( s , t , r ), used to lookup in the selected cubemap as described above"
  },
  {
    "id": 13401,
    "content": "The instruction loads data from the texture named by operand a from sample number given by first element of the operand c , at coordinates given by remaining elements of operand c into destination d The first element in operand c is interpreted as unsigned integer sample number ( u32 ), and the next two elements are interpreted as signed integer ( s32 ) 2d texture coordinates The instruction"
  },
  {
    "id": 13402,
    "content": "first selects a multi-sample texture from the multi-sample texture array named by operand a using the index given by the first element of the array coordinate vector c The instruction then loads data from the selected multi-sample texture from sample number given by second element of the operand c , at coordinates given by remaining elements of operand c into destination d When accessing a"
  },
  {
    "id": 13406,
    "content": "The first element in operand c is interpreted as unsigned integer sampler number, the second element is interpreted as unsigned integer index ( u32 ) into the multi-sample texture array and the next two elements are interpreted as signed integer ("
  },
  {
    "id": 13408,
    "content": "level (lod explicit) Requires an additional 32-bit scalar argument, lod , which contains the LOD to fetch from grad (lod gradient) Requires two"
  },
  {
    "id": 13410,
    "content": "The vectors are singletons for 1d and a1d textures; are two-element vectors for 2d and a2d textures; and are four-element vectors for 3d, cube and acube textures, where the fourth element is ignored for 3d and cube geometries"
  },
  {
    "id": 13411,
    "content": "Indirect texture access Beginning with PTX ISA version 3 1, indirect texture access is supported in unified mode for target architecture sm_20 or higher In indirect access, operand a is a"
  },
  {
    "id": 13420,
    "content": "Texture Instructions: tld4  tld4 Perform a texture fetch of the 4-texel bilerp footprint Syntax tld4"
  },
  {
    "id": 13444,
    "content": "f32 }; Description Texture fetch of the 4-texel bilerp footprint using a texture coordinate vector The instruction loads the bilerp footprint from the texture named by operand a at coordinates given by operand c into vector destination d The four texel samples are placed into destination vector d in counter-clockwise order starting at lower left"
  },
  {
    "id": 13446,
    "content": "2d For 2D textures, operand c specifies coordinates as a two-element, 32-bit floating-point vector The first element in operand c is interpreted as an unsigned integer index ( u32 ) into the texture array, and the next two elements are interpreted as 32-bit floating point coordinates of 2d texture"
  },
  {
    "id": 13448,
    "content": "cube For cubemap textures, operand c specifies four-element vector which comprises three floating-point coordinates (s, t, r) and a fourth padding argument which is ignored The (s, t, r) coordinates can be thought of as a direction vector emanating from the center of the cube Of the three coordinates (s, t, r), the coordinate of the largest magnitude (the major axis) selects the cube face Then,"
  },
  {
    "id": 13449,
    "content": "the other two coordinates (the minor axes) are divided by the absolute value of the major axis to produce a new (s, t) coordinate pair to lookup into the selected cube face"
  },
  {
    "id": 13450,
    "content": "The first element in operand c is interpreted as an unsigned integer index ( u32 ) into the cubemap texture array, and the remaining three elements are interpreted as floating-point cubemap coordinates (s, t, r), used to lookup in the selected cubemap"
  },
  {
    "id": 13466,
    "content": "f32 {r1,r2,r3,r4}, [tex_a,{f1,f2}], {r5, r6};   Example of unified mode texturing using compare tld4"
  },
  {
    "id": 13503,
    "content": "addr_mode_0, addr_mode_1, addr_mode_2 }; Description Query an attribute of a texture or sampler Query Returns"
  },
  {
    "id": 13507,
    "content": "channel_data_type Unsigned integer corresponding to source language’s channel data type enumeration If the source language combines channel data type and channel order into a single enumeration type, that value is returned for both channel_data_type and channel_order queries channel_order Unsigned integer corresponding to source language’s channel order enumeration"
  },
  {
    "id": 13515,
    "content": "array_size For a texture array, number of textures in array, 0 otherwise num_mipmap_levels For a mipmapped texture, number of levels of details (LOD), 0 otherwise"
  },
  {
    "id": 13516,
    "content": "In unified mode, sampler attributes are also accessed via a texref argument, and in independent mode sampler attributes are accessed via a separate samplerref argument"
  },
  {
    "id": 13518,
    "content": "level requires an additional 32bit integer argument, lod , which specifies LOD and queries requested attribute for the specified LOD"
  },
  {
    "id": 13536,
    "content": "Texture Instructions: istypep  istypep Query whether a register points to an opaque variable of a specified type Syntax istypep"
  },
  {
    "id": 13542,
    "content": "surfref }; Description Write predicate register p with 1 if register a points to an opaque variable of the specified type, and with 0 otherwise"
  },
  {
    "id": 13547,
    "content": "PTX supports the following operations on surface descriptors: Static initialization of surface descriptors"
  },
  {
    "id": 13579,
    "content": "The instruction loads data from the surface named by operand a at coordinates given by operand b into destination d"
  },
  {
    "id": 13580,
    "content": "Operand b is a scalar or singleton tuple for 1d surfaces; is a two-element vector for 2d surfaces; and is a four-element vector for 3d surfaces, where the fourth element is ignored"
  },
  {
    "id": 13581,
    "content": "The lowest dimension coordinate represents a byte offset into the surface and is not scaled, and the size of the data transfer matches the size of destination operand d The instruction first selects a surface layer from the surface array named by operand a using the index given by the first element of the array coordinate vector b The instruction then loads data from the selected surface at"
  },
  {
    "id": 13582,
    "content": "coordinates given by the remaining elements of operand b into destination d Operand b is a bit-size type vector or tuple containing an index into the array of surfaces followed by coordinates within the selected surface, as follows: For 1d surface arrays, operand b has type"
  },
  {
    "id": 13585,
    "content": "The first element is interpreted as an unsigned integer index ( u32 ) into the surface array, and the second element is interpreted as a 1d surface coordinate of type"
  },
  {
    "id": 13587,
    "content": "The first element is interpreted as an unsigned integer index ( u32 ) into the surface array, and the next two elements are interpreted as 2d surface coordinates of type"
  },
  {
    "id": 13589,
    "content": "A surface base address is assumed to be aligned to a 16 byte boundary, and the address given by the coordinate vector must be naturally aligned to a multiple of the access size The"
  },
  {
    "id": 13590,
    "content": "clamp field specifies how to handle out-of-bounds addresses: trap causes an execution trap on out-of-bounds addresses"
  },
  {
    "id": 13591,
    "content": "clamp loads data at the nearest surface location (sized appropriately) zero loads zero for out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3 1, indirect surface access is supported for target architecture sm_20 or higher In indirect access, operand a is a"
  },
  {
    "id": 13637,
    "content": "The instruction stores data from operand c to the surface named by operand a at coordinates given by operand b"
  },
  {
    "id": 13639,
    "content": "The source vector elements are interpreted left-to-right as R , G , B , and A surface components Surface sample components that do not occur in the source vector will be written with an unpredictable value The source data interpretation is based on the surface sample format as follows: If the surface format contains UNORM , SNORM , or FLOAT data, then f32 is assumed; if the surface format"
  },
  {
    "id": 13640,
    "content": "contains UINT data, then u32 is assumed; if the surface format contains SINT data, then s32 is assumed {a1d,a2d} Surface layer selection, followed by an unformatted store to the selected surface The instruction then stores the data in operand c to the selected surface at coordinates given by the remaining elements of operand b The"
  },
  {
    "id": 13641,
    "content": "clamp field specifies how to handle out-of-bounds addresses: trap causes an execution trap on out-of-bounds addresses clamp stores data at the nearest surface location (sized appropriately) zero drops stores to out-of-bounds addresses Indirect surface access Beginning with PTX ISA version 3 1, indirect surface access is supported for target architecture sm_20 or higher"
  },
  {
    "id": 13701,
    "content": "zero }; Description Reduction to surface memory using a surface coordinate vector The instruction performs a reduction operation with data from operand c to the surface named by operand a at coordinates given by operand b"
  },
  {
    "id": 13712,
    "content": "s32 based on the surface sample format as follows: if the surface format contains UINT data, then u32 is assumed; if the surface format contains SINT data, then s32 is assumed"
  },
  {
    "id": 13714,
    "content": "b64 , if the surface format contains UINT data, then u64 is assumed; if the surface format contains SINT data, then s64 is assumed"
  },
  {
    "id": 13749,
    "content": "memory_layout }; Description Query an attribute of a surface memory_layout 1 for surface with linear memory layout; 0 otherwise Indirect surface access Beginning with PTX ISA version 3 1, indirect surface access is supported for target architecture sm_20 or higher"
  },
  {
    "id": 13750,
    "content": "Control Flow Instructions  The following PTX instructions and syntax are for controlling execution in a PTX program: {} @ bra call ret exit 9"
  },
  {
    "id": 13754,
    "content": "Syntax { instructionList } Description The curly braces create a group of instructions, used primarily for defining a function body The curly braces also provide a mechanism for determining the scope of a variable: any variable declared within a scope is not available outside the scope Syntax @{"
  },
  {
    "id": 13755,
    "content": "}p instruction; Description Execute an instruction or instruction block for threads that have the guard predicate True"
  },
  {
    "id": 13765,
    "content": "Syntax @p bra{ uni} tgt;   tgt is a label bra{ uni} tgt;   unconditional branch Description Continue execution at the target all active threads in a warp that are currently executing this instruction have identical values for the guard predicate and branch target"
  },
  {
    "id": 13772,
    "content": "Control Flow Instructions: brx idx  brx idx Branch to a label indexed from a list of potential branch targets Syntax @p brx idx{ uni} index, tlist; brx idx{ uni} index, tlist; Description Index into a list of possible destination labels, and continue execution from the chosen label"
  },
  {
    "id": 13773,
    "content": "all active threads in a warp that are currently executing this instruction have identical values for the guard predicate and the index argument"
  },
  {
    "id": 13783,
    "content": "Parallel Synchronization and Communication Instructions: red  red Reduction operations on global and shared memory"
  },
  {
    "id": 13819,
    "content": "level::cache_hint = { L2::cache_hint } Description Performs a reduction operation with operand b and the value in location a , and stores the result of the specified operation at location a , overwriting the original value"
  },
  {
    "id": 13827,
    "content": "For red with vector type, operand b is brace-enclosed vector expressions, size of which is equal to the size of vector qualifier"
  },
  {
    "id": 13831,
    "content": "scope qualifier specifies the set of threads that can directly observe the memory synchronizing effect of this operation, as described in the Memory Consistency Model"
  },
  {
    "id": 13832,
    "content": "For red with vector type, the supported combinations of vector qualifier, types and reduction operations supported on these combinations are depicted in following table: Vector qualifier Types"
  },
  {
    "id": 13855,
    "content": "max Not supported Not Supported Two atomic operations { atom or red } are performed atomically with respect to each other only if each operation specifies a scope that includes the other When this condition is not met, each operation observes the other operation being performed as if it were split into a read followed by a dependent write"
  },
  {
    "id": 13856,
    "content": "red instruction on packed type or vector type, accesses adjacent scalar elements in memory In such case, the atomicity is guaranteed separately for each of the individual scalar elements; the entire red is not guaranteed to be atomic as a single access For sm_6x and earlier architectures, red operations on shared state space do not guarantee atomicity with respect to normal store instructions to"
  },
  {
    "id": 13857,
    "content": "the same address It is the programmer’s responsibility to guarantee correctness of programs that use shared memory reduction instructions, e"
  },
  {
    "id": 13859,
    "content": ", by inserting barriers between normal stores and reduction operations to a common address, or by using atom exch to store to locations accessed by other reduction operations"
  },
  {
    "id": 13860,
    "content": "Supported addressing modes for operand a and alignment requirements are described in Addresses as Operands The bit-size operations are"
  },
  {
    "id": 13886,
    "content": "Parallel Synchronization and Communication Instructions: red async  red async Asynchronous reduction operation on shared memory"
  },
  {
    "id": 13887,
    "content": "The shared memory addresses of destination operand a and the mbarrier object mbar , must meet all of the following conditions: They Belong to the same CTA"
  },
  {
    "id": 13889,
    "content": "shared::cluster , if the addresses specified do not fall within the address window of shared::cluster state space, then the behaviour is undefined"
  },
  {
    "id": 13890,
    "content": "The reduce operation in red async is treated as a relaxed memory operation and the complete_tx operation on the mbarrier has"
  },
  {
    "id": 13903,
    "content": "Parallel Synchronization and Communication Instructions: vote (deprecated)  vote (deprecated) Vote across thread group Syntax vote"
  },
  {
    "id": 13922,
    "content": "Description Performs a reduction of the source predicate across all active threads in a warp The reduction modes are: all True if source predicate is True for all active threads in warp"
  },
  {
    "id": 13924,
    "content": "b32 simply copies the predicate from each thread in a warp into the corresponding bit position of destination register d , where the bit position corresponds to the thread’s lane id"
  },
  {
    "id": 13928,
    "content": "Release Notes Note that vote applies to threads in a single warp, not across an entire CTA Examples vote"
  },
  {
    "id": 13937,
    "content": "Parallel Synchronization and Communication Instructions: vote sync  vote sync Vote across thread group Syntax vote sync"
  },
  {
    "id": 13948,
    "content": "uni }; Description vote sync will cause executing thread to wait until all non-exited threads corresponding to membermask have executed vote sync with the same qualifiers and same membermask value before resuming execution Operand membermask specifies a 32-bit integer which is a mask indicating threads participating in this instruction where the bit position corresponds to thread’s laneid In the"
  },
  {
    "id": 13949,
    "content": "mode form, vote sync performs a reduction of the source predicate across all non-exited threads in membermask The destination operand d is a predicate register and its value is the same across all threads in membermask The reduction modes are: all True if source predicate is True for all non-exited threads in membermask uni True if source predicate has the same value in all non-exited threads in"
  },
  {
    "id": 13952,
    "content": "b32 simply copies the predicate from each thread in membermask into the corresponding bit position of destination register d , where the bit position corresponds to the thread’s lane id A thread not specified in membermask will contribute a 0 for its entry in vote"
  },
  {
    "id": 13957,
    "content": "target sm_6x or below, all threads in membermask must execute the same vote sync instruction in convergence, and only threads belonging to some membermask can be active when the vote sync instruction is executed Examples vote sync all pred p,q,0xffffffff; vote sync"
  },
  {
    "id": 13962,
    "content": "Parallel Synchronization and Communication Instructions: match sync  match sync Broadcast and compare a value across threads in warp Syntax match"
  },
  {
    "id": 13970,
    "content": "b64 }; Description match sync will cause executing thread to wait until all non-exited threads from membermask have executed match sync with the same qualifiers and same membermask value before resuming execution Operand membermask specifies a 32-bit integer which is a mask indicating threads participating in this instruction where the bit position corresponds to thread’s laneid match sync"
  },
  {
    "id": 13971,
    "content": "performs broadcast and compare of operand a across all non-exited threads in membermask and sets destination d and optional predicate p based on mode"
  },
  {
    "id": 13973,
    "content": "all d is set to mask corresponding to non-exited threads in membermask if all non-exited threads in membermask have same value of operand a ; otherwise d is set to 0 Optionally predicate p is set to true if all non-exited threads in membermask have same value of operand a ; otherwise p is set to false any d is set to mask of non-exited threads in membermask that have same value of operand a"
  },
  {
    "id": 13978,
    "content": "Parallel Synchronization and Communication Instructions: activemask  activemask Queries the active threads within a warp Syntax activemask b32 d; Description activemask queries predicated-on active threads from the executing warp and sets the destination d with 32-bit integer mask where bit position in the mask corresponds to the thread’s laneid An active thread will contribute 1 for its entry"
  },
  {
    "id": 13979,
    "content": "in the result and exited or inactive or predicated-off thread will contribute 0 for its entry in the result Parallel Synchronization and Communication Instructions: redux sync  redux sync Perform reduction operation on the data from each predicated active thread in the thread group Syntax redux sync"
  },
  {
    "id": 13995,
    "content": "xor} Description redux sync will cause the executing thread to wait until all non-exited threads corresponding to membermask have executed redux sync with the same qualifiers and same membermask value before resuming execution redux sync performs a reduction operation op of the 32 bit source register src across all non-exited threads in the membermask Reduction operation can be one of the bitwise"
  },
  {
    "id": 14018,
    "content": "Parallel Synchronization and Communication Instructions: griddepcontrol  griddepcontrol Control execution of dependent grids Syntax griddepcontrol"
  },
  {
    "id": 14021,
    "content": "wait } Description The griddepcontrol instruction allows the dependent grids and prerequisite grids as defined by the runtime, to control execution in the following way: launch_dependents modifier signals that specific dependents the runtime system designated to react to this instruction can be scheduled as soon as all other CTAs in the grid issue the same instruction or have completed There is"
  },
  {
    "id": 14022,
    "content": "no guarantee that the dependent will launch before the completion of the current grid Repeated invocations of this instruction by threads in the current CTA will have no additional side effects past that of the first invocation wait modifier causes the executing thread to wait until all prerequisite grids in flight have completed and all the memory operations from the prerequisite grids are"
  },
  {
    "id": 14023,
    "content": "performed and made visible to the current grid Note If the prerequisite grid is using griddepcontrol launch_dependents , then the dependent grid must use griddepcontrol wait to ensure correct functional execution"
  },
  {
    "id": 14024,
    "content": "Parallel Synchronization and Communication Instructions: elect sync  elect sync Elect a leader thread from a set of threads Syntax elect sync d|p, membermask; Description elect sync elects one predicated active leader thread from among a set of threads specified by membermask The predicate destination p is set to True for the leader thread, and False for all other threads Operand membermask"
  },
  {
    "id": 14027,
    "content": "sync qualifier indicates that elect causes the executing thread to wait until all threads in the membermask execute the elect instruction before resuming execution"
  },
  {
    "id": 14028,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier  mbarrier is a barrier created in shared memory that supports : Synchronizing any subset of threads within a CTA One-way synchronization of threads across CTAs of a cluster As noted in mbarrier support with shared memory , threads can perform only arrive operations but not *_wait on an mbarrier located in shared::cluster space"
  },
  {
    "id": 14029,
    "content": "Waiting for completion of asynchronous memory operations initiated by a thread and making them visible to other threads"
  },
  {
    "id": 14030,
    "content": "An mbarrier object is an opaque object in memory which can be initialized and invalidated using : mbarrier init mbarrier inval Operations supported on mbarrier object s are : mbarrier expect_tx mbarrier complete_tx mbarrier arrive mbarrier arrive_drop mbarrier test_wait mbarrier try_wait mbarrier"
  },
  {
    "id": 14033,
    "content": "mbarrier arrive Performing any mbarrier operation except mbarrier init on an uninitialized mbarrier object results in undefined behavior"
  },
  {
    "id": 14035,
    "content": "cta} / barrier{ cta} instructions which can access a limited number of barriers per CTA, mbarrier objects are used defined and are only limited by the total shared memory size available"
  },
  {
    "id": 14036,
    "content": "mbarrier operations enable threads to perform useful work after the arrival at the mbarrier and before waiting for the mbarrier to complete"
  },
  {
    "id": 14042,
    "content": "Size and alignment of mbarrier object  An mbarrier object is an opaque object with the following type and alignment requirements : Type Alignment (bytes) Memory space"
  },
  {
    "id": 14049,
    "content": "Contents of the mbarrier object  An opaque mbarrier object keeps track of the following information : Current phase of the mbarrier object Count of pending arrivals for the current phase of the mbarrier object Count of expected arrivals for the next phase of the mbarrier object Count of pending asynchronous memory operations (or transactions) tracked by the current phase of the mbarrier object"
  },
  {
    "id": 14050,
    "content": "An mbarrier object progresses through a sequence of phases where each phase is defined by threads performing an expected number of arrive-on operations The valid range of each of the counts is as shown below: Count name Minimum value Maximum value Expected arrival count 1 2 20 - 1 Pending arrival count 0 2 20 - 1 tx-count -(2 20 - 1) 2 20 - 1 9"
  },
  {
    "id": 14061,
    "content": "Phase of the mbarrier object  The phase of an mbarrier object is the number of times the mbarrier object has been used to synchronize threads and cp"
  },
  {
    "id": 14062,
    "content": "async operations In each phase {0, 1, 2, …}, threads perform in program order : arrive-on operations to complete the current phase and test_wait / try_wait operations to check for the completion of the current phase An mbarrier object is automatically reinitialized upon completion of the current phase for immediate use in the next phase For each phase of the mbarrier object, at least one"
  },
  {
    "id": 14063,
    "content": "test_wait or try_wait operation must be performed which returns True for waitComplete before an arrive-on operation in the subsequent phase"
  },
  {
    "id": 14069,
    "content": "Tracking asynchronous operations by the mbarrier object  Starting with the Hopper architecture ( sm_9x ), mbarrier object supports a new count, called tx-count , which is used for tracking the completion of asynchronous memory operations or transactions tx-count tracks the number of asynchronous transactions, in units specified by the asynchronous memory operation, that are outstanding and yet"
  },
  {
    "id": 14070,
    "content": "to be complete The tx-count of an mbarrier object must be set to the total amount of asynchronous memory operations, in units as specified by the asynchronous operations, to be tracked by the current phase Upon completion of each of the asynchronous operations, the complete-tx operation will be performed on the mbarrier object and thus progress the mbarrier towards the completion of the current"
  },
  {
    "id": 14078,
    "content": "expect-tx operation  The expect-tx operation, with an expectCount argument, increases the tx-count of an mbarrier object by the value specified by expectCount This makes the current phase of the mbarrier object to expect and track the completion of additional asynchronous transactions"
  },
  {
    "id": 14085,
    "content": "complete-tx operation  The complete-tx operation, with an completeCount argument, on an mbarrier object consists of the following: mbarrier signaling Signals the completion of asynchronous transactions that were tracked by the current phase mbarrier potentially completing the current phase If the current phase has been completed then the mbarrier transitions to the next phase Refer to Phase"
  },
  {
    "id": 14086,
    "content": "Completion of the mbarrier object for details on phase completion requirements and phase transition process"
  },
  {
    "id": 14092,
    "content": "Phase Completion of the mbarrier object  The requirements for completion of the current phase are described below Upon completion of the current phase, the phase transitions to the subsequent phase as described below Current phase completion requirements An mbarrier object completes the current phase when all of the following conditions are met: The count of the pending arrivals has reached zero"
  },
  {
    "id": 14093,
    "content": "Phase transition When an mbarrier object completes the current phase, the following actions are performed atomically: The mbarrier object transitions to the next phase The pending arrival count is reinitialized to the expected arrival count"
  },
  {
    "id": 14099,
    "content": "Arrive-on operation on mbarrier object  An arrive-on operation, with an optional count argument, on an mbarrier object consists of the following 2 steps : mbarrier signalling: Signals the arrival of the executing thread OR completion of the cp async instruction which signals the arrive-on operation initiated by the executing thread on the mbarrier object mbarrier potentially completing the"
  },
  {
    "id": 14100,
    "content": "current phase: If the current phase has been completed then the mbarrier transitions to the next phase"
  },
  {
    "id": 14106,
    "content": "mbarrier support with shared memory  The following table summarizes the support of various mbarrier operations on mbarrier objects located at different shared memory locations: mbarrier operations shared::cta shared::cluster mbarrier arrive Supported Supported, cannot return result mbarrier expect_tx Supported Supported mbarrier complete_tx Supported Supported Other mbarrier operations Supported"
  },
  {
    "id": 14112,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier init  mbarrier init Initialize the mbarrier object Syntax mbarrier init{"
  },
  {
    "id": 14114,
    "content": "b64 [addr], count; Description mbarrier init initializes the mbarrier object at the location specified by the address operand addr with the unsigned 32-bit integer count The value of operand count must be in the range as specified in Contents of the mbarrier object If the address specified by addr does not fall within the address window of"
  },
  {
    "id": 14134,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier inval  mbarrier inval Invalidates the mbarrier object Syntax mbarrier inval{"
  },
  {
    "id": 14136,
    "content": "b64 [addr]; Description mbarrier inval invalidates the mbarrier object at the location specified by the address operand addr An mbarrier object must be invalidated before using its memory location for any other purpose Performing any mbarrier operation except mbarrier init on an invalidated mbarrier object results in undefined behaviour"
  },
  {
    "id": 14178,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier expect_tx  mbarrier expect_tx Perfoms expect-tx operation on the mbarrier object Syntax mbarrier expect_tx{"
  },
  {
    "id": 14190,
    "content": "expect_tx performs an expect-tx operation on the mbarrier object at the location specified by the address operand addr The 32-bit unsigned integer operand txCount specifies the expectCount argument to the expect-tx operation If the address specified by addr does not fall within the address window of"
  },
  {
    "id": 14210,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier complete_tx  mbarrier complete_tx Perfoms complete-tx operation on the mbarrier object Syntax mbarrier complete_tx{"
  },
  {
    "id": 14222,
    "content": "complete_tx performs a complete-tx operation on the mbarrier object at the location specified by the address operand addr The 32-bit unsigned integer operand txCount specifies the completeCount argument to the complete-tx operation mbarrier complete_tx does not involve any asynchronous memory operations and only simulates the completion of an asynchronous memory operation and its side effect of"
  },
  {
    "id": 14234,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier arrive  mbarrier arrive Performs arrive-on operation on the mbarrier object Syntax mbarrier arrive{"
  },
  {
    "id": 14266,
    "content": "arrive performs an arrive-on operation on the mbarrier object at the location specified by the address operand addr The 32-bit unsigned integer operand count specifies the count argument to the arrive-on operation"
  },
  {
    "id": 14270,
    "content": "arrive and expect_tx are specified, then the count argument of the arrive-on operation is assumed to be 1"
  },
  {
    "id": 14273,
    "content": "noComplete qualifier must not cause the mbarrier to complete its current phase, otherwise the behavior is undefined The value of the operand count must be in the range as specified in Contents of the mbarrier object Note: for sm_8x , when the argument count is specified, the modifier noComplete is required mbarrier arrive operation on an mbarrier object located in shared::cta returns an opaque"
  },
  {
    "id": 14274,
    "content": "64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the destination operand state mbarrier arrive operation on an mbarrier object located in"
  },
  {
    "id": 14277,
    "content": "scope qualifier indicates the set of threads that directly observe the memory synchronizing effect of this operation, as described in the Memory Consistency Model"
  },
  {
    "id": 14329,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier arrive_drop  mbarrier arrive_drop Decrements the expected count of the mbarrier object and performs arrive-on operation Syntax mbarrier arrive_drop{"
  },
  {
    "id": 14361,
    "content": "arrive_drop on the mbarrier object at the location specified by the address operand addr performs the following steps: Decrements the expected arrival count of the mbarrier object by the value specified by the 32-bit integer operand count The decrement done in the expected arrivals count of the mbarrier object will be for all the subsequent phases of the mbarrier object"
  },
  {
    "id": 14363,
    "content": "arrive and expect_tx are specified, then the count argument of the arrive-on operation is assumed to be 1"
  },
  {
    "id": 14365,
    "content": "arrive_drop operation forms the release pattern as described in the Memory Consistency Model and synchronizes with the acquire patterns"
  },
  {
    "id": 14372,
    "content": "A thread that wants to either exit or opt out of participating in the arrive-on operation can use mbarrier"
  },
  {
    "id": 14373,
    "content": "arrive_drop to drop itself from the mbarrier mbarrier arrive_drop operation on an mbarrier object located in shared::cta returns an opaque 64-bit register capturing the phase of the mbarrier object prior to the arrive-on operation in the destination operand state mbarrier arrive_drop operation on an mbarrier object located in"
  },
  {
    "id": 14428,
    "content": "b64 [addr]; Description Causes an arrive-on operation to be triggered by the system on the mbarrier object upon the completion of all prior cp"
  },
  {
    "id": 14431,
    "content": "noinc modifier is not specified, the pending count of the mbarrier object is incremented by 1 prior to the asynchronous arrive-on operation This results in a zero-net change for the pending count from the asynchronous arrive-on operation during the current phase The pending count of the mbarrier object after the increment should not exceed the limit as mentioned in Contents of the mbarrier object"
  },
  {
    "id": 14433,
    "content": "noinc modifier is specified, the increment to the pending count of the mbarrier object is not performed Hence the decrement of the pending count done by the asynchronous arrive-on operation must be accounted for in the initialization of the mbarrier object"
  },
  {
    "id": 14473,
    "content": "noinc requires mbarrier initalization to have accounted for arrive-on from cp async cp async mbarrier arrive noinc"
  },
  {
    "id": 14518,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier test_wait/mbarrier try_wait  mbarrier test_wait/mbarrier try_wait Checks whether the mbarrier object has completed the phase Syntax mbarrier test_wait{"
  },
  {
    "id": 14544,
    "content": "cluster } Description The test_wait and try_wait operations test for the completion of the current or the immediately preceding phase of an mbarrier object at the location specified by the operand addr mbarrier test_wait is a non-blocking instruction which tests for the completion of the phase mbarrier try_wait is a potentially blocking instruction which tests for the completion of the phase"
  },
  {
    "id": 14545,
    "content": "Suspended thread resumes execution when the specified phase completes OR before the phase completes following a system-dependent time limit The optional 32-bit unsigned integer operand suspendTimeHint specifies the time limit, in nanoseconds, that may be used for the time limit instead of the system-dependent limit mbarrier test_wait and mbarrier try_wait test for completion of the phase :"
  },
  {
    "id": 14546,
    "content": "Specified by the operand state , which was returned by an mbarrier arrive instruction on the same mbarrier object during the current or the immediately preceding phase Or Indicated by the operand phaseParity , which is the integer parity of either the current phase or the immediately preceding phase of the mbarrier object The parity variant of the instructions test for the completion of the phase"
  },
  {
    "id": 14547,
    "content": "indicated by the operand phaseParity , which is the integer parity of either the current phase or the immediately preceding phase of the mbarrier object Note: the use of the parity variants of the instructions requires tracking the phase of an mbarrier object throughout its lifetime The test_wait and try_wait operations are valid only for : the current incomplete phase, for which waitComplete"
  },
  {
    "id": 14548,
    "content": "returns False When mbarrier test_wait and mbarrier try_wait operations return True , they form the acquire pattern as described in the Memory Consistency Model The optional scope qualifier indicates the set of threads that the mbarrier test_wait and mbarrier try_wait instructions can directly synchronize The following ordering of memory operations hold for the executing thread when mbarrier"
  },
  {
    "id": 14549,
    "content": "test_wait or mbarrier try_wait returns True : All memory accesses (except async operations ) requested prior, in program order, to mbarrier arrive during the completed phase by the participating threads of the CTA are performed and are visible to the executing thread"
  },
  {
    "id": 14552,
    "content": "arrive during the completed phase by the participating threads of the CTA are performed and made visible to the executing thread"
  },
  {
    "id": 14555,
    "content": "bulk asynchronous operations using the same mbarrier object requested prior, in program order, to mbarrier"
  },
  {
    "id": 14556,
    "content": "arrive during the completed phase by the participating threads of the CTA are performed and made visible to the executing thread"
  },
  {
    "id": 14557,
    "content": "All memory accesses requested after the mbarrier test_wait or mbarrier try_wait , in program order, are not performed and not visible to memory accesses performed prior to mbarrier arrive , in program order, by other threads participating in the mbarrier There is no ordering and visibility guarantee for memory accesses requested by the thread after mbarrier arrive and prior to mbarrier test_wait"
  },
  {
    "id": 14655,
    "content": "u64 addr, shMem; mapa u64 remAddr, addr, 0;   CTA0’s shMem instance   One thread from CTA0 executing the below initialization operation @p0 mbarrier"
  },
  {
    "id": 14658,
    "content": "b64 [shMem], N;   N = no of cluster threads barrier cluster arrive; barrier cluster wait;   Entire cluster executing the below arrive operation mbarrier arrive"
  },
  {
    "id": 14673,
    "content": "Parallel Synchronization and Communication Instructions: mbarrier pending_count  mbarrier pending_count Query the pending arrival count from the opaque mbarrier state Syntax mbarrier pending_count b64 count, state; Description The pending count can be queried from the opaque mbarrier state using mbarrier pending_count The state operand is a 64-bit register that must be the result of a prior"
  },
  {
    "id": 14679,
    "content": "The destination register count is a 32-bit unsigned integer representing the pending count of the mbarrier object prior to the arrive-on operation from which the state register was obtained"
  },
  {
    "id": 14696,
    "content": "Parallel Synchronization and Communication Instructions: tensormap cp_fenceproxy  tensormap cp_fenceproxy A fused copy and fence operation Syntax tensormap cp_fenceproxy"
  },
  {
    "id": 14715,
    "content": "cp_fence instructions perform the following operations in order : Copies data of size specified by the size argument, in bytes, from the location specified by the address operand src in shared memory to the location specified by the address operand dst in the global memory, in the generic proxy Establishes a uni-directional proxy release pattern on the ordering from the copy operation to the"
  },
  {
    "id": 14716,
    "content": "subsequent access performed in the tensormap proxy on the address dst The operands src and dst specify non-generic addresses in shared::cta and global state space respectively"
  },
  {
    "id": 14718,
    "content": "scope qualifier specifies the set of threads that can directly observe the proxy synchronizing effect of this operation, as described in Memory Consistency Model"
  },
  {
    "id": 14720,
    "content": "sync qualifier indicates that tensormap cp_fenceproxy causes the executing thread to wait until all threads in the warp execute the same tensormap cp_fenceproxy instruction before resuming execution"
  },
  {
    "id": 14725,
    "content": "cp_fenceproxy instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined"
  },
  {
    "id": 14774,
    "content": "Warp Level Matrix Multiply-Accumulate Instructions  The matrix multiply and accumulate operation has the following form: D = A * B + C where D and C are called accumulators and may refer to the same matrix PTX provides two ways to perform matrix multiply-and-accumulate computation: Using wmma instructions: This warp-level computation is performed collectively by all threads in the warp as"
  },
  {
    "id": 14776,
    "content": "load operation When the operation completes, the destination registers in each thread hold a fragment of the loaded matrix Perform the matrix multiply and accumulate operation using the wmma mma operation on the loaded matrices When the operation completes, the destination registers in each thread hold a fragment of the result matrix returned by the wmma mma operation Alternately, result matrix D"
  },
  {
    "id": 14777,
    "content": "can also be used as argument C for a subsequent wmma mma operation The wmma load and wmma store instructions implicitly handle the organization of matrix elements when loading the input matrices from memory for the wmma mma operation and when storing the result back to memory Using mma instruction: Similar to wmma , mma also requires computation to be performed collectively by all threads in the"
  },
  {
    "id": 14778,
    "content": "warp however distribution of matrix elements across different threads in warp needs to be done explicitly before invoking the mma operation The sparse variant can be used when A is a structured sparse matrix as described in Sparse matrix storage"
  },
  {
    "id": 14783,
    "content": "Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices A, B and C The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while C and D are MxN matrices Matrix Data-types  The matrix multiply and accumulate operation is supported separately on integer,"
  },
  {
    "id": 14784,
    "content": "floating-point, sub-byte integer and single bit data-types For floating-point matrix multiply and accumulate operation, different matrix operands may have different precision, as described later Data-type Multiplicands (A or B) Accumulators (C or D) Integer"
  },
  {
    "id": 14805,
    "content": "Matrix multiply-accumulate operation using wmma instructions  This section describes warp level wmma load, wmma mma and wmma"
  },
  {
    "id": 14812,
    "content": "Matrix Fragments for WMMA  Each thread in the warp holds a fragment of the matrix The distribution of fragments loaded by the threads in a warp is unspecified and is target architecture dependent, and hence the identity of the fragment within the matrix is also unspecified and is target architecture dependent The fragment returned by a wmma operation can be used as an operand for another wmma"
  },
  {
    "id": 14813,
    "content": "operation if the shape, layout and element type of the underlying matrix matches Since fragment layout is architecture dependent, using the fragment returned by a wmma operation in one function as an operand for a wmma operation in a different function may not work as expected if the two functions are linked together but were compiled for different link-compatible SM architectures Note passing"
  },
  {
    "id": 14814,
    "content": "wmma fragment to a function having weak linkage is unsafe since at link time references to such function may get resolved to a function in different compilation module Integer fragments Multiplicands (A or B): Data-type Shape Matrix Fragment"
  },
  {
    "id": 14821,
    "content": "m8n32k16 A A vector expression containing a single b32 register containing four elements from the matrix B A vector expression of four b32 registers, with each register containing four elements from the matrix m32n8k16 A A vector expression of four b32 registers, with each register containing four elements from the matrix B A vector expression containing single b32 register, with each containing"
  },
  {
    "id": 14836,
    "content": "m8n32k16 A A vector expression containing a two b32 registers, with containing two elements from the matrix B A vector expression of eight b32 registers, with each register containing two elements from the matrix m32n8k16 A A vector expression of eight b32 registers, with each register containing two elements from the matrix B A vector expression containing two b32 registers, with each containing"
  },
  {
    "id": 14859,
    "content": "m8n8k32 A vector expression containing a single b32 register, containing eight elements from the matrix"
  },
  {
    "id": 14861,
    "content": "m8n8k128 A vector expression containing a single b32 register, containing 32 elements from the matrix"
  },
  {
    "id": 14866,
    "content": "Manipulating fragment contents The contents of a matrix fragment can be manipulated by reading and writing to individual registers in the fragment, provided the following conditions are satisfied: All matrix element in the fragment are operated on uniformly across threads, using the same parameters"
  },
  {
    "id": 14867,
    "content": "For example, if each register corresponding to a given matrix is multiplied by a uniform constant value, then the resulting matrix is simply the scaled version of the original matrix"
  },
  {
    "id": 14877,
    "content": "Matrix Storage for WMMA  Each matrix can be stored in memory with a row-major or column-major layout In a row-major format, consecutive elements of each row are stored in contiguous memory locations, and the row is called the leading dimension of the matrix In a column-major format, consecutive elements of each column are stored in contiguous memory locations and the column is called the leading"
  },
  {
    "id": 14878,
    "content": "dimension of the matrix Consecutive instances of the leading dimension (rows or columns) need not be stored contiguously in memory The wmma load and wmma store operations accept an optional argument stride that specifies the offset from the beginning of each row (or column) to the next, in terms of matrix elements (and not bytes) For example, the matrix being accessed by a wmma operation may be a"
  },
  {
    "id": 14879,
    "content": "submatrix from a larger matrix stored in memory This allows the programmer to compose a multiply-and-accumulate operation on matrices that are larger than the shapes supported by the wmma operation Address Alignment: The starting address of each instance of the leading dimension (row or column) must be aligned with the size of the corresponding fragment in bytes Note that the starting address is"
  },
  {
    "id": 14880,
    "content": "determined by the base pointer and the optional stride Consider the following instruction as an example: wmma load"
  },
  {
    "id": 14889,
    "content": "f16 elements, not bytes) For each row of this matrix to be aligned at fragment size the following must be true: p is a multiple of 32 Default value for stride: The default value of the stride is the size of the leading dimension of the matrix For example, for an MxK matrix, the stride is K for a row-major layout and M for a column-major layout"
  },
  {
    "id": 14890,
    "content": "In particular, the default strides for the supported matrix shapes are as follows: Shape A (row) A (column) B (row) B (column) Accumulator (row) Accumulator (column) 16x16x16 16 16 16 16 16 16 8x32x16 16 8 32 16 32 8 32x8x16 16 32 8 16 8 32 8x8x32 32 8 8 32 8 8 8x8x128 128 8 8 128 8 8 16x16x8 8 16 16 8 16 16 8x8x4 4 8 8 4 8 8 9"
  },
  {
    "id": 14904,
    "content": "c indicate whether matrix A, B or C is being loaded respectively for the wmma computation The destination operand r is a brace-enclosed vector expression that can hold the fragment returned by the load operation, as described in Matrix Fragments for WMMA The shape qualifier indicates the dimensions of all the matrix arguments involved in the intended wmma computation The layout qualifier"
  },
  {
    "id": 14906,
    "content": "stride is an optional 32-bit integer operand that provides an offset in terms of matrix elements between the start of consecutive instances of the leading dimension (rows or columns) The default value of stride is described in Matrix Storage for WMMA and must be specified if the actual value is larger than the default For example, if the matrix is a sub-matrix of a larger matrix, then the value"
  },
  {
    "id": 14907,
    "content": "of stride is the leading dimension of the larger matrix The required alignment for address p and stride is described in the Matrix Storage for WMMA"
  },
  {
    "id": 14909,
    "content": "sync qualifier indicates that wmma load causes the executing thread to wait until all threads in the warp execute the same wmma"
  },
  {
    "id": 14915,
    "content": "load instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined The behavior of wmma load is undefined if all threads do not use the same qualifiers and the same values of p and stride , or if any thread in the warp has exited Double precision and alternate floating point precision wmma introduced in PTX ISA"
  },
  {
    "id": 14919,
    "content": "aligned is required from PTX ISA version 6 3 onwards, and considered implicit in PTX ISA versions less than 6"
  },
  {
    "id": 15003,
    "content": "The source operand r is a brace-enclosed vector expression that matches the shape of the fragment expected by the store operation, as described in Matrix Fragments for WMMA"
  },
  {
    "id": 15008,
    "content": "sync qualifier indicates that wmma store causes the executing thread to wait until all threads in the warp execute the same wmma"
  },
  {
    "id": 15014,
    "content": "store instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined The behavior of wmma store is undefined if all threads do not use the same qualifiers and the same values of p and stride , or if any thread in the warp has exited Examples   Storing f32 elements computed by a wmma"
  },
  {
    "id": 15062,
    "content": "The register arguments a , b , c and d hold unspecified fragments of the corresponding matrices as described in Matrix Fragments for WMMA The qualifiers"
  },
  {
    "id": 15086,
    "content": "For single-bit wmma , multiplication is replaced by a sequence of logical operations; specifically, wmma"
  },
  {
    "id": 15088,
    "content": "popc and wmma and popc computes the XOR, AND respectively of a 128-bit row of A with a 128-bit column of B, then counts the number of set bits in the result ( popc )"
  },
  {
    "id": 15105,
    "content": "satfinite indicates that the final values in the destination register are saturated as follows: The output is clamped to the minimum or maximum 32-bit signed integer value"
  },
  {
    "id": 15106,
    "content": "Precision and rounding for f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision"
  },
  {
    "id": 15116,
    "content": "tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision"
  },
  {
    "id": 15119,
    "content": "rn ): rn mantissa LSB rounds to nearest even rz mantissa LSB rounds towards zero rm mantissa LSB rounds towards negative infinity rp mantissa LSB rounds towards positive infinity The mandatory"
  },
  {
    "id": 15120,
    "content": "sync qualifier indicates that wmma mma causes the executing thread to wait until all threads in the warp execute the same wmma"
  },
  {
    "id": 15126,
    "content": "mma instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined The behavior of wmma mma is undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited"
  },
  {
    "id": 15131,
    "content": "Matrix multiply-accumulate operation using mma instruction  This section describes warp-level mma , ldmatrix , stmatrix , and movmatrix instructions and the organization of various matrices involved in these instructions"
  },
  {
    "id": 15143,
    "content": "MMA Computation Threads participating in MMA computation MMA computation 1 Threads with %laneid 0-3 (low group) and 16-19 (high group) MMA computation 2 Threads with %laneid 4-7 (low group) and 20-23 (high group) MMA computation 3 Threads with %laneid 8-11 (low group) and 24-27 (high group) MMA computation 4 Threads with %laneid 12-15 (low group) and 28-31 (high group) For each of the individual"
  },
  {
    "id": 15144,
    "content": "MMA computation shown above, each of the required thread holds a fragment of the matrix for performing mma operation as follows: Multiplicand A: atype Fragment Elements (low to high)"
  },
  {
    "id": 15145,
    "content": "f16 A vector expression containing two f16x2 registers, with each register containing two f16 elements from the matrix A"
  },
  {
    "id": 15146,
    "content": "a0, a1, a2, a3 The layout of the fragments held by different threads is shown below: Fragment layout for Row Major matrix A is shown in Figure 21 Figure 21 MMA m8n8k4 fragment layout for row-major matrix A with f16 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 if % laneid > 2 col = % laneid % 4 Multiplicand B: btype Fragment Elements (low to high)"
  },
  {
    "id": 15147,
    "content": "f64 A vector expression containing a single f64 register, containing a single f64 element from the matrix B"
  },
  {
    "id": 15149,
    "content": "m8n8k4 fragment layout for matrix B with f64 type  The row and column of a matrix fragment can be computed as: row = % laneid % 4 col = % laneid >> 2 Accumulators (C or D):"
  },
  {
    "id": 15152,
    "content": "f64 A vector expression containing of two f64 registers containing two f64 elements from the matrix C"
  },
  {
    "id": 15155,
    "content": "f64 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 } 9"
  },
  {
    "id": 15160,
    "content": "Matrix Fragments for mma m8n8k16  A warp executing mma m8n8k16 will compute an MMA operation of shape m8n8k16 Elements of the matrix are distributed across the threads in a warp so each thread of the warp holds a fragment of the matrix"
  },
  {
    "id": 15167,
    "content": "a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 31 Figure 31 MMA"
  },
  {
    "id": 15170,
    "content": "s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 4 ) + i for ai where i = { 0 ,"
  },
  {
    "id": 15177,
    "content": "b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 32 Figure 32 MMA"
  },
  {
    "id": 15180,
    "content": "s8 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,"
  },
  {
    "id": 15187,
    "content": "s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9"
  },
  {
    "id": 15192,
    "content": "Matrix Fragments for mma m8n8k32  A warp executing mma m8n8k32 will compute an MMA operation of shape m8n8k32"
  },
  {
    "id": 15199,
    "content": "a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 34 Figure 34 MMA"
  },
  {
    "id": 15202,
    "content": "s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 8 ) + i for ai where i = { 0 ,"
  },
  {
    "id": 15209,
    "content": "b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 35 Figure 35 MMA"
  },
  {
    "id": 15212,
    "content": "s4 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + i for bi where i = { 0 ,"
  },
  {
    "id": 15219,
    "content": "s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9"
  },
  {
    "id": 15224,
    "content": "Matrix Fragments for mma m8n8k128  A warp executing mma m8n8k128 will compute an MMA operation of shape m8n8k128"
  },
  {
    "id": 15227,
    "content": "b1 A vector expression containing a single b32 register, containing thirty two b1 elements from the matrix A"
  },
  {
    "id": 15229,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 32 ) + i for ai where i = { 0 ,"
  },
  {
    "id": 15232,
    "content": "b1 A vector expression containing a single b32 register, containing thirty two b1 elements from the matrix B b0, b1, …, b30, b31 The layout of the fragments held by different threads is shown in Figure 38"
  },
  {
    "id": 15233,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,"
  },
  {
    "id": 15237,
    "content": "s32 A vector expression containing two s32 registers, containing two s32 elements from the matrix C (or D)"
  },
  {
    "id": 15240,
    "content": "s32 type  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID col = ( threadID_in_group * 2 ) + i for ci where i = { 0 , 1 } 9"
  },
  {
    "id": 15245,
    "content": "Matrix Fragments for mma m16n8k4  A warp executing mma m16n8k4 will compute an MMA operation of shape m16n8k4"
  },
  {
    "id": 15249,
    "content": "tf32 A vector expression containing two b32 registers, containing two tf32 elements from the matrix A"
  },
  {
    "id": 15250,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group"
  },
  {
    "id": 15254,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = threadID_in_group Multiplicand B:"
  },
  {
    "id": 15257,
    "content": "tf32 A vector expression of a single b32 register, containing a single tf32 element from the matrix B"
  },
  {
    "id": 15258,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID"
  },
  {
    "id": 15262,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group col = groupID Accumulators (C or D):"
  },
  {
    "id": 15266,
    "content": "f32 A vector expression containing four f32 registers, containing four f32 elements from the matrix C (or D)"
  },
  {
    "id": 15267,
    "content": "c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 44  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15272,
    "content": "f64 A vector expression containing four f64 registers, containing four f64 elements from the matrix C (or D)"
  },
  {
    "id": 15273,
    "content": "c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 45  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15279,
    "content": "Matrix Fragments for mma m16n8k8  A warp executing mma m16n8k8 will compute an MMA operation of shape m16n8k8"
  },
  {
    "id": 15288,
    "content": "a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 46  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = threadID_in_group * 2 + ( i & 0x1 ) for ai where i = { 0 ,"
  },
  {
    "id": 15292,
    "content": "tf32 A vector expression containing four b32 registers, containing four tf32 elements from the matrix A"
  },
  {
    "id": 15293,
    "content": "a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 47  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3"
  },
  {
    "id": 15296,
    "content": "f64 A vector expression containing four f64 registers, containing four f64 elements from the matrix A"
  },
  {
    "id": 15297,
    "content": "a0, a1, a2, a3 The layout of the fragments held by different threads is shown in Figure 48  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = threadID_in_group for a0 and a1 threadID_in_group + 4 for a2 and a3 Multiplicand B:"
  },
  {
    "id": 15305,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + i for bi where i = { 0 , 1 } col = groupID"
  },
  {
    "id": 15308,
    "content": "tf32 A vector expression containing two b32 registers, containing two tf32 elements from the matrix B"
  },
  {
    "id": 15309,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID"
  },
  {
    "id": 15313,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group for b0 threadID_in_group + 4 for b1 col = groupID Accumulators (C or D):"
  },
  {
    "id": 15319,
    "content": "f16 A vector expression containing two f16x2 registers, with each register containing two f16 elements from the matrix C (or D)"
  },
  {
    "id": 15324,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15330,
    "content": "c0, c1, c2, c3 The layout of the fragments held by different threads is shown in Figure 53  The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for c0 and c1 groupID + 8 for c2 and c3 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15336,
    "content": "Matrix Fragments for mma m16n8k16 with floating point type  A warp executing mma m16n8k16 floating point types will compute an MMA operation of shape m16n8k16"
  },
  {
    "id": 15345,
    "content": "a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 54"
  },
  {
    "id": 15346,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 4"
  },
  {
    "id": 15349,
    "content": "f64 A vector expression containing eight f64 registers, with each register containing one f64 element from the matrix A"
  },
  {
    "id": 15350,
    "content": "a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 55"
  },
  {
    "id": 15351,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i % 2 = 0 groupID + 8 Otherwise col = ( i * 2 ) + threadID_in_group for ai where i % 2 = 0 ( i * 2 ) - 2 + ( threadID_in_group Otherwise Multiplicand B:"
  },
  {
    "id": 15360,
    "content": " where the row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 2 ) + ( i & 0x1 ) for bi where i = 2 col = groupID"
  },
  {
    "id": 15363,
    "content": "f64 A vector expression containing four f64 registers, with each register containing one f64 element from the matrix B"
  },
  {
    "id": 15365,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = threadID_in_group + ( i * 4 ) for bi where i > 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15381,
    "content": "a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 59"
  },
  {
    "id": 15382,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 4 col = ( threadID_in_group * 4 ) + ( i & 0x3 ) for ai where i = { 0 ,"
  },
  {
    "id": 15390,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + i for bi where i = { 0 ,"
  },
  {
    "id": 15394,
    "content": "s32 A vector expression containing four s32 registers, containing four s32 elements from the matrix C (or D)"
  },
  {
    "id": 15396,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15402,
    "content": "Matrix Fragments for mma m16n8k32  A warp executing mma m16n8k32 will compute an MMA operation of shape m16n8k32"
  },
  {
    "id": 15411,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 8 col = ( threadID_in_group * 8 ) + ( i & 0x7 ) for ai where i = { 0 ,"
  },
  {
    "id": 15427,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 8 Multiplicand B:"
  },
  {
    "id": 15435,
    "content": "b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 64"
  },
  {
    "id": 15436,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = { 0 , , 7 } col = groupID"
  },
  {
    "id": 15449,
    "content": "b0, b1, b2, b3, b4, b5, b6, b7 The layout of the fragments held by different threads is shown in Figure 65 and Figure 66"
  },
  {
    "id": 15450,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 4 ) + ( i & 0x3 ) for bi where i = 4 col = groupID Accumulators (C or D):"
  },
  {
    "id": 15453,
    "content": "s32 A vector expression containing four s32 registers, containing four s32 elements from the matrix C (or D)"
  },
  {
    "id": 15455,
    "content": "f32 A vector expression containing four f32 registers, containing four f32 elements from the matrix C (or D)"
  },
  {
    "id": 15457,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15463,
    "content": "Matrix Fragments for mma m16n8k64  A warp executing mma m16n8k64 will compute an MMA operation of shape m16n8k64"
  },
  {
    "id": 15471,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 16 Multiplicand B:"
  },
  {
    "id": 15477,
    "content": "b0, b1, …, b14, b15 The layout of the fragments held by different threads is shown in Figure 69 and Figure 70"
  },
  {
    "id": 15478,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 8 ) + ( i & 0x7 ) for bi where i = 8 col = groupID Accumulators (C or D):"
  },
  {
    "id": 15481,
    "content": "s32 A vector expression containing four s32 registers, containing four s32 elements from the matrix C (or D)"
  },
  {
    "id": 15483,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 ,"
  },
  {
    "id": 15489,
    "content": "Matrix Fragments for mma m16n8k128  A warp executing mma m16n8k128 will compute an MMA operation of shape m16n8k128"
  },
  {
    "id": 15492,
    "content": "b1 A vector expression containing two b32 registers, with each register containing thirty two b1 elements from the matrix A"
  },
  {
    "id": 15494,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where i = 32 col = ( threadID_in_group * 32 ) + ( i & 0x1F ) for ai where i = { 0 ,"
  },
  {
    "id": 15497,
    "content": "b1 A vector expression containing a single b32 register containing thirty two b1 elements from the matrix B b0, b1, … , b30, b31 The layout of the fragments held by different threads is shown in Figure 73"
  },
  {
    "id": 15498,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + i for bi where i = { 0 ,"
  },
  {
    "id": 15502,
    "content": "s32 A vector expression containing four s32 registers, containing four s32 elements from the matrix C (or D)"
  },
  {
    "id": 15504,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9"
  },
  {
    "id": 15509,
    "content": "Matrix Fragments for mma m16n8k256  A warp executing mma m16n8k256 will compute an MMA operation of shape m16n8k256"
  },
  {
    "id": 15516,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 64 Multiplicand B:"
  },
  {
    "id": 15518,
    "content": "b1 A vector expression containing two b32 registers, with each register containing thirty two b1 elements from the matrix B b0, b1, …, b62, b63 The layout of the fragments held by different threads is shown in Figure 76 and Figure 77"
  },
  {
    "id": 15519,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = ( threadID_in_group * 32 ) + ( i & 0x1F ) for bi where i = 32 col = groupID Accumulators (C or D):"
  },
  {
    "id": 15522,
    "content": "s32 A vector expression containing four s32 registers, containing four s32 elements from the matrix C (or D)"
  },
  {
    "id": 15524,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ci where i = 2 col = ( threadID_in_group * 2 ) + ( i & 0x1 ) for ci where i = { 0 , 1 , 2 , 3 } 9"
  },
  {
    "id": 15534,
    "content": "For single-bit mma sync , multiplication is replaced by a sequence of logical operations; specifically, mma"
  },
  {
    "id": 15536,
    "content": "popc and mma and popc computes the XOR, AND respectively of a k-bit row of A with a k-bit column of B, then counts the number of set bits in the result ( popc ) Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp"
  },
  {
    "id": 15537,
    "content": "The registers in each thread hold a fragment of matrix as described in Matrix multiply-accumulate operation using mma instruction"
  },
  {
    "id": 15552,
    "content": "Precision and rounding : f16 floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision"
  },
  {
    "id": 15554,
    "content": "e5m2 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision"
  },
  {
    "id": 15556,
    "content": "tf32 floating point operations : Element-wise multiplication of matrix A and B is performed with specified precision f64 floating point operations : Precision of the element-wise multiplication and addition operation is identical to that of f64 precision fused multiply-add"
  },
  {
    "id": 15557,
    "content": "The satfinite qualifier indicates that on overflow, the accumulated value is limited to the range MIN_INT32 MAX_INT32 (where the bounds are defined as the minimum negative signed 32-bit integer and the maximum positive signed 32-bit integer respectively) The mandatory sync qualifier indicates that mma instruction causes the executing thread to wait until all threads in the warp execute the same"
  },
  {
    "id": 15558,
    "content": "mma instruction before resuming execution The mandatory aligned qualifier indicates that all threads in the warp must execute the same mma instruction In conditionally executed code, a mma instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined The behavior of mma instruction is undefined if all threads in the"
  },
  {
    "id": 15559,
    "content": "same warp do not use the same qualifiers, or if any thread in the warp has exited Notes Programs using double precision floating point mma instruction with shapes"
  },
  {
    "id": 15609,
    "content": "m8n8k4 is optimized for target architecture sm_70 and may have substantially reduced performance on other target architectures"
  },
  {
    "id": 15631,
    "content": "Warp-level matrix load instruction: ldmatrix  ldmatrix Collectively load one or more matrices from shared memory for mma instruction Syntax ldmatrix"
  },
  {
    "id": 15648,
    "content": "b16}; Description Collectively load one or more matrices across all threads in a warp from the location indicated by the address operand p , from"
  },
  {
    "id": 15649,
    "content": "shared state space into destination register r If no state space is provided, generic addressing is used, such that the address in p points into shared space If the generic address doesn’t fall in shared state space, then the behavior is undefined"
  },
  {
    "id": 15651,
    "content": "sync qualifier indicates that ldmatrix causes the executing thread to wait until all threads in the warp execute the same ldmatrix instruction before resuming execution"
  },
  {
    "id": 15653,
    "content": "aligned qualifier indicates that all threads in the warp must execute the same ldmatrix instruction In conditionally executed code, an ldmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined The behavior of ldmatrix is undefined if all threads do not use the same qualifiers, or if any thread in"
  },
  {
    "id": 15655,
    "content": "The destination operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit registers as per the value of"
  },
  {
    "id": 15657,
    "content": "The eight addresses required for each matrix are provided by eight threads, depending upon the value of"
  },
  {
    "id": 15659,
    "content": "Addresses addr0–addr7 correspond to the rows of the first matrix, addresses addr8–addr15 correspond to the rows of the second matrix, and so on"
  },
  {
    "id": 15661,
    "content": "x1 addr0–addr7 – – – x2 addr0–addr7 addr8–addr15 – – x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 Note For"
  },
  {
    "id": 15667,
    "content": "x2 , addresses contained in lower threads can be copied to higher threads to achieve the expected behavior"
  },
  {
    "id": 15668,
    "content": "Each thread in a warp loads fragments of a row, with thread 0 receiving the first fragment in its register r , and so on"
  },
  {
    "id": 15671,
    "content": "x2 , the elements of the second matrix are loaded in the next destination register in each thread as per the layout in above table"
  },
  {
    "id": 15674,
    "content": "x4 , elements of the third and fourth matrices are loaded in the subsequent destination registers in each thread"
  },
  {
    "id": 15711,
    "content": "Warp-level matrix store instruction: stmatrix  stmatrix Collectively store one or more matrices to shared memory Syntax stmatrix"
  },
  {
    "id": 15728,
    "content": "b16}; Description Collectively store one or more matrices across all threads in a warp to the location indicated by the address operand p , in"
  },
  {
    "id": 15731,
    "content": "sync qualifier indicates that stmatrix causes the executing thread to wait until all threads in the warp execute the same stmatrix instruction before resuming execution"
  },
  {
    "id": 15733,
    "content": "aligned qualifier indicates that all threads in the warp must execute the same stmatrix instruction In conditionally executed code, an stmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined The behavior of stmatrix is undefined if all threads do not use the same qualifiers, or if any thread in"
  },
  {
    "id": 15735,
    "content": "The source operand r is a brace-enclosed vector expression consisting of 1, 2, or 4 32-bit registers as per the value of"
  },
  {
    "id": 15737,
    "content": "x1 addr0–addr7 – – – x2 addr0–addr7 addr8–addr15 – – x4 addr0–addr7 addr8–addr15 addr16–addr23 addr24–addr31 When storing 8x8 matrices, a group of four consecutive threads stores 16 bytes"
  },
  {
    "id": 15738,
    "content": "Each thread in a warp stores fragments of a row, with thread 0 storing the first fragment from its register r , and so on"
  },
  {
    "id": 15741,
    "content": "x2 , the elements of the second matrix are storedd from the next source register in each thread as per the layout in above table"
  },
  {
    "id": 15744,
    "content": "x4 , elements of the third and fourth matrices are stored from the subsequent source registers in each thread"
  },
  {
    "id": 15781,
    "content": "Warp-level matrix transpose instruction: movmatrix  movmatrix Transpose a matrix in registers across the warp Syntax movmatrix"
  },
  {
    "id": 15790,
    "content": "b16}; Description Move a row-major matrix across all threads in a warp, reading elements from source a , and writing the transposed elements to destination d"
  },
  {
    "id": 15792,
    "content": "sync qualifier indicates that movmatrix causes the executing thread to wait until all threads in the warp execute the same movmatrix instruction before resuming execution"
  },
  {
    "id": 15794,
    "content": "aligned qualifier indicates that all threads in the warp must execute the same movmatrix instruction In conditionally executed code, a movmatrix instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise the behavior is undefined"
  },
  {
    "id": 15795,
    "content": "Operands a and d are 32-bit registers containing fragments of the input matrix and the resulting matrix respectively"
  },
  {
    "id": 15797,
    "content": "trans indicates that the resulting matrix in d is a transpose of the input matrix specified by a Each thread in a warp holds a fragment of a row of the input matrix, with thread 0 holding the first fragment in register a , and so on Figure 81 movmatrix source matrix fragment layout  Each thread in a warp holds a fragment of a column of the result matrix, with thread 0 holding the first fragment"
  },
  {
    "id": 15798,
    "content": "in register d , and so on A group of four threads holds an entire column of the result matrix as shown in Figure 82 Figure 82 movmatrix result matrix fragment layout  PTX ISA Notes Introduced in PTX ISA version 7"
  },
  {
    "id": 15800,
    "content": "Matrix multiply-accumulate operation using mma sp instruction with sparse matrix A  This section describes warp-level mma sp{::ordered_metadata} instruction with sparse matrix A This variant of the mma operation can be used when A is a structured sparse matrix with 50% zeros in each row distributed in a shape-specific granularity For an MxNxK sparse mma sp{::ordered_metadata} operation, the MxK"
  },
  {
    "id": 15801,
    "content": "matrix A is packed into MxK/2 elements For each K-wide row of matrix A, 50% elements are zeros and the remaining K/2 non-zero elements are packed in the operand representing matrix A The mapping of these K/2 elements to the corresponding K-wide row is provided explicitly as metadata"
  },
  {
    "id": 15807,
    "content": "Sparse matrix storage  Granularity of sparse matrix A is defined as the ratio of the number of non-zero elements in a sub-chunk of the matrix row to the total number of elements in that sub-chunk where the size of the sub-chunk is shape-specific"
  },
  {
    "id": 15808,
    "content": "Values 0b0000 , 0b0101 , 0b1010 , 0b1111 are invalid values for metadata and will result in undefined behavior"
  },
  {
    "id": 15809,
    "content": "In a group of four consecutive threads, one or more threads store the metadata for the whole group depending upon the matrix shape Figure 83 shows an example of a 16x16 matrix A represented in sparse format and sparsity selector indicating which thread in a group of four consecutive threads stores the metadata Figure 83 Sparse MMA storage example  Granularities for different matrix shapes and"
  },
  {
    "id": 15816,
    "content": "In other words, each chunk of four adjacent elements in a row of matrix A has two zeros and two non-zero elements Only the two non-zero elements are stored in the operand representing matrix A and their positions in the four-wide chunk in matrix A are indicated by two 2-bit indices in the metadata operand"
  },
  {
    "id": 15818,
    "content": "sp::ordered_metadata , 0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior"
  },
  {
    "id": 15819,
    "content": " The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k16 : One thread within a group of four consecutive threads contributes the metadata for the entire group m16n8k32 : A thread-pair within a group of four consecutive threads contributes the sparsity metadata Hence, the sparsity selector must be either 0 (threads T0, T1) or 1 (threads T2, T3); any other"
  },
  {
    "id": 15824,
    "content": "In other words, each chunk of two adjacent elements in a row of matrix A has one zero and one non-zero element Only the non-zero elements are stored in the operand for matrix A and their positions in a two-wide chunk in matrix A are indicated by the 4-bit index in the metadata 0b1110 and 0b0100 are the only meaningful index values; any other values result in an undefined behavior  The sparsity"
  },
  {
    "id": 15825,
    "content": "selector indicates the threads which contribute metadata as listed below: m16n8k8 : One thread within a group of four consecutive threads contributes the metadata for the entire group m16n8k16 : A thread-pair within a group of four consecutive threads contributes the sparsity metadata"
  },
  {
    "id": 15829,
    "content": "s8 elements, matrix A is structured sparse at a granularity of 2:4 In other words, each chunk of four adjacent elements in a row of matrix A have two zeroes and two non-zero elements Only the two non-zero elements are stored in sparse matrix and their positions in the four-wide chunk are indicated by two 2-bit indices in the metadata"
  },
  {
    "id": 15833,
    "content": "In other words, each chunk of eight adjacent elements in a row of matrix A has four zeroes and four non-zero values Further, the zero and non-zero values are clustered in sub-chunks of two elements each within the eight-wide chunk"
  },
  {
    "id": 15836,
    "content": ", each two-wide sub-chunk within the eight-wide chunk must be all zeroes or all non-zeros Only the four non-zero values are stored in sparse matrix and the positions of the two two-wide sub-chunks with non-zero values in the eight-wide chunk of a row of matrix A are indicated by two 2-bit indices in the metadata"
  },
  {
    "id": 15837,
    "content": " The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k32 with"
  },
  {
    "id": 15853,
    "content": "0b0100 , 0b1000 , 0b1001 , 0b1100 , 0b1101 , 0b1110 are the meaningful values of indices; any other values result in an undefined behavior"
  },
  {
    "id": 15854,
    "content": " The sparsity selector indicates the threads which contribute metadata as listed below: m16n8k64 : All threads within a group of four consecutive threads contribute the sparsity metadata"
  },
  {
    "id": 15860,
    "content": "Matrix fragments for multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of various matrices and the sparsity metadata The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and their association with the"
  },
  {
    "id": 15861,
    "content": "matrix data For matrix B, when the combination of matrix dimension and the supported data type is not already covered in Matrix multiply-accumulate operation using mma instruction , a pictorial representation of matrix fragments is provided For matrices C and D, since the matrix dimension - data type combination is the same for all supported shapes, and is already covered in Matrix"
  },
  {
    "id": 15862,
    "content": "multiply-accumulate operation using mma instruction , the pictorial representations of matrix fragments are not included in this section For the metadata operand, pictorial representations of the association between indices of the elements of matrix A and the contents of the metadata operand are included"
  },
  {
    "id": 15865,
    "content": "z] indicates that bits m through n (with m being higher) in the metadata operand of thread with %laneid=k contains the indices of the non-zero elements from the chunk [x][y]"
  },
  {
    "id": 15887,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a1 groupID + 8 for a2 and a3 col = [ firstcol lastcol ] As per the mapping of non-zero elements as described in Sparse matrix storage Where firstcol = threadID_in_group * 4 lastcol = firstcol + 3 Matrix fragments for multiplicand B and accumulators C and"
  },
  {
    "id": 15893,
    "content": "b32 register containing 16 2-bit vectors each storing the index of a non-zero element of a 4-wide chunk of matrix A as shown in Figure 90 Figure 90 Sparse MMA"
  },
  {
    "id": 15917,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 4 lastcol = firstcol + 3 Multiplicand B:"
  },
  {
    "id": 15923,
    "content": "b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 92  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma"
  },
  {
    "id": 15928,
    "content": "b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of two non-zero element from a 4-wide chunk of matrix A as shown in Figure 93 Figure 93 Sparse MMA"
  },
  {
    "id": 15946,
    "content": "tf32 A vector expression containing four b32 registers, with each register containing one non-zero tf32 element out of 2 consecutive elements from matrix A"
  },
  {
    "id": 15947,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 and a2 groupID + 8 for a1 and a3 col = [ firstcol lastcol ] As per the mapping of non-zero elements as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 for a0 and a1 ( threadID_in_group * 2 ) + 8 for a2 and a3 lastcol = firstcol + 1"
  },
  {
    "id": 15950,
    "content": "tf32 A vector expression containing four b32 registers, each containing four tf32 elements from matrix B"
  },
  {
    "id": 15951,
    "content": "b0, b1, b2, b3 The layout of the fragments held by different threads is shown in Figure 95  Matrix fragments for accumulators C and D are the same as in case of Matrix Fragments for mma"
  },
  {
    "id": 15954,
    "content": "b32 register containing 8 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A as shown in Figure 96 Figure 96 Sparse MMA"
  },
  {
    "id": 15971,
    "content": "tf32 A vector expression containing two b32 registers, each containing one non-zero tf32 element out of 2 consecutive elements from matrix A"
  },
  {
    "id": 15972,
    "content": " The row and column of a matrix fragment can be computed as: groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for a0 groupID + 8 for a1 col = [ firstcol lastcol ] As per the mapping of non-zero elements as described in Sparse matrix storage Where firstcol = threadID_in_group * 2 lastcol = firstcol + 1 Matrix fragments for multiplicand B and accumulators C and D are the same"
  },
  {
    "id": 15977,
    "content": "b32 register containing 8 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A as shown in Figure 98 Figure 98 Sparse MMA"
  },
  {
    "id": 16000,
    "content": " groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 > 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 8 lastcol = firstcol + 7 Multiplicand B:"
  },
  {
    "id": 16011,
    "content": "The layout of the fragments held by different threads is shown in Figure 103 , Figure 104 , Figure 105 and Figure 106 Figure 103 Sparse MMA"
  },
  {
    "id": 16038,
    "content": "b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of two non-zero elements from a 4-wide chunk of matrix A as shown in Figure 107 and Figure 108 Figure 107 Sparse MMA"
  },
  {
    "id": 16071,
    "content": " groupID = % laneid >> 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 > 2 threadID_in_group = % laneid % 4 row = groupID for ai where 0 = 16 lastcol = firstcol + 15 Multiplicand B:"
  },
  {
    "id": 16077,
    "content": "b0, b1, b2, b3, …, b31 The layout of the fragments held by different threads is shown in Figure 113 , Figure 114 , Figure 115 , Figure 116 Figure 113 Sparse MMA"
  },
  {
    "id": 16096,
    "content": "b32 register containing 16 2-bit vectors with each pair of 2-bit vectors storing the indices of four non-zero elements from a 8-wide chunk of matrix A as shown in Figure 117 and Figure 118"
  },
  {
    "id": 16102,
    "content": "Operands a and b represent two multiplicand matrices A and B, while c and d represent the accumulator and destination matrices, distributed across the threads in warp Matrix A is structured sparse as described in Sparse matrix storage Operands e and f represent sparsity metadata and sparsity selector respectively"
  },
  {
    "id": 16105,
    "content": "sp::ordered_metadata requires the indices in the sparsity metadata to be sorted in an increasing order starting from LSB, otherwise behavior is undefined"
  },
  {
    "id": 16106,
    "content": "The registers in each thread hold a fragment of matrix as described in Matrix fragments for multiply-accumulate operation with sparse matrix A"
  },
  {
    "id": 16112,
    "content": "f16 floating point operations : Element-wise multiplication of matrix A and B is performed with at least single precision"
  },
  {
    "id": 16116,
    "content": "sync qualifier indicates that mma sp/mma sp::ordered_metadata instruction causes the executing thread to wait until all threads in the warp execute the same mma sp/mma sp::ordered_metadata instruction before resuming execution"
  },
  {
    "id": 16118,
    "content": "aligned qualifier indicates that all threads in the warp must execute the same mma sp/mma sp::ordered_metadata instruction In conditionally executed code, a mma sp/mma sp::ordered_metadata instruction should only be used if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined The behavior of mma sp/mma sp::ordered_metadata instruction is"
  },
  {
    "id": 16119,
    "content": "undefined if all threads in the same warp do not use the same qualifiers, or if any thread in the warp has exited Notes mma sp instruction may have substantially reduced performance on some target architectures"
  },
  {
    "id": 16124,
    "content": "Asynchronous Warpgroup Level Matrix Multiply-Accumulate Instructions  The warpgroup level matrix multiply and accumulate operation has either of the following forms, where matrix D is called accumulator: D = A * B + D D = A * B , where the input from accumulator D is disabled The wgmma instructions perform warpgroup level matrix multiply-and-accumulate operation by having all threads in a"
  },
  {
    "id": 16125,
    "content": "warpgroup collectively perform the following actions: Load matrices A, B and D into registers or into shared memory Perform the following fence operations: wgmma fence operations to indicate that the register/shared-memory across the warpgroup have been written into fence"
  },
  {
    "id": 16129,
    "content": "Create a wgmma-group and commit all the prior outstanding wgmma mma_async operations into the group, by using wgmma"
  },
  {
    "id": 16137,
    "content": "Warpgroup  A warpgroup is a set of four contiguous warps such that the warp-rank of the first warp is a multiple of 4 warp-rank of a warp is defined as: (%tid x + %tid"
  },
  {
    "id": 16145,
    "content": "Matrix Shape  The matrix multiply and accumulate operations support a limited set of shapes for the operand matrices A, B and D The shapes of all three matrix operands are collectively described by the tuple MxNxK , where A is an MxK matrix, B is a KxN matrix, while D is a MxN matrix For integer matrix multiply and accumulate operation, both multiplicand matrices (A and B) must have elements of"
  },
  {
    "id": 16166,
    "content": "Async Proxy  The wgmma mma_async operations are performed in the asynchronous proxy (or async proxy) For the async proxy, fence proxy async should be used to synchronize memory between generic proxy and the async proxy The completion of a wgmma mma_async operation is followed by an implicit generic-async proxy fence So the result of the asynchronous operation is made visible to the generic proxy"
  },
  {
    "id": 16167,
    "content": "as soon as its completion is observed wgmma commit_group and wgmma wait_group operations must be used to wait for the completion of the wgmma mma_async instructions"
  },
  {
    "id": 16172,
    "content": "Asynchronous Warpgroup Level Matrix Multiply-Accumulate Operation using wgmma mma_async instruction  This section describes warpgroup level wgmma mma_async instruction and the organization of various matrices involved in this instruction"
  },
  {
    "id": 16178,
    "content": "Register Fragments and Shared Memory Matrix Layouts  The input matrix A of the warpgroup wide MMA operations can be either in registers or in the shared memory This section describes the layouts of register fragments and shared memory expected by the warpgroup MMA instructions When the matrices are in shared memory, their starting addresses must be aligned to 16 bytes"
  },
  {
    "id": 16185,
    "content": "Register Fragments  This section describes the organization of various matrices located in register operands of the wgmma"
  },
  {
    "id": 16198,
    "content": "m64nNk16 will compute an MMA operation of shape m64nNk16 where N is a valid n dimension as listed in Matrix Shape"
  },
  {
    "id": 16199,
    "content": "Elements of the matrix are distributed across the threads in a warpgroup so each thread of the warpgroup holds a fragment of the matrix"
  },
  {
    "id": 16206,
    "content": "a0, a1, a2, a3, a4, a5, a6, a7 The layout of the fragments held by different threads is shown in Figure 119"
  },
  {
    "id": 16209,
    "content": "f16 A vector expression containing N/4 number of f16x2 registers, with each register containing two f16 elements from matrix D"
  },
  {
    "id": 16210,
    "content": "d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2,"
  },
  {
    "id": 16224,
    "content": "m64nNk8 will compute an MMA operation of shape m64nNk8 where N is a valid n dimension as listed in Matrix Shape"
  },
  {
    "id": 16232,
    "content": "d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2,"
  },
  {
    "id": 16246,
    "content": "m64nNk32 will compute an MMA operation of shape m64nNk32 where N is a valid n dimension as listed in Matrix Shape"
  },
  {
    "id": 16260,
    "content": "d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N depends on"
  },
  {
    "id": 16265,
    "content": "f16 A vector expression containing N/4 number of f16x2 registers, with each register containing two f16 elements from matrix D"
  },
  {
    "id": 16279,
    "content": "m64nNk256 will compute an MMA operation of shape m64nNk256 where N is a valid n dimension as listed in Matrix Shape"
  },
  {
    "id": 16289,
    "content": "d0, d1, d2, d3, …, dX, dY, dZ, dW where X = N/2 - 4 Y = N/2 - 3 Z = N/2 - 2 W = N/2 - 1 N = 8*i where i = {1, 2, 3, 4} = 16*i where i = {3, 4,"
  },
  {
    "id": 16290,
    "content": ", 15, 16} The layout of the fragments held by different threads is shown in Figure 126 Figure 126 WGMMA"
  },
  {
    "id": 16298,
    "content": "Shared Memory Matrix Layout  Matrices in shared memory are organized into a number of smaller matrices called core matrices Matrix A is made up of 8x2 core matrices and Matrix B is made up of 2x(N/8) core matrices This section describes the layout of the core matrices for each shape"
  },
  {
    "id": 16308,
    "content": "m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of eight"
  },
  {
    "id": 16312,
    "content": "m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 128 Figure 128 WGMMA m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 129 Shared Memory Layout for wgmma"
  },
  {
    "id": 16314,
    "content": "m64nNk8  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of four"
  },
  {
    "id": 16317,
    "content": "m64nNk8 core matrices for A and B  Layout of core matrices of A is shown in Figure 131 Figure 131 WGMMA m64nNk8 core matrix layout for A  Layout of core matrices of B is shown in Figure 132 Shared Memory Layout for wgmma"
  },
  {
    "id": 16327,
    "content": "m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 134 Figure 134 WGMMA m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 135 Shared Memory Layout for wgmma"
  },
  {
    "id": 16329,
    "content": "m64nNk256  Core matrices of A and B are as follows: Core matrix Matrix description Matrix size A Each row is made up of 256"
  },
  {
    "id": 16332,
    "content": "m64nNk256 core matrices for A and B  Layout of core matrices of A is shown in Figure 137 Figure 137 WGMMA m64nNk256 core matrix layout for A  Layout of core matrices of B is shown in Figure 138 Strides  Leading dimension byte offset of matrix A or B is the distance, in bytes, between two adjacent core matrices in the K dimension Stride dimension byte offset of matrix A or B is the distance, in"
  },
  {
    "id": 16333,
    "content": "bytes, between two adjacent core matrices in the M or N dimension Figure 139 and Figure 140 show the leading dimension byte offset and the stride dimension byte offsets for A and B matrices Matrix A: Figure 139 WGMMA stride and leading dimension byte offset for matrix A  Matrix B: Figure 140 WGMMA stride and leading dimension byte offset for matrix B  Leading dimension byte offset and stride"
  },
  {
    "id": 16334,
    "content": "dimension byte offset must be specified in the matrix descriptor as described in Matrix Descriptor Format"
  },
  {
    "id": 16342,
    "content": "Swizzling Modes  The core matrices can be swizzled in the shared memory by specifying one of the following swizzling modes: No swizzling: All the elements of the entire core matrix are adjacent to each other and there is no swizzling Figure 141 illustrates this: Figure 141 WGMMA core matrices with no swizzling  32-Byte swizzling: A group of two adjacent core matrices are swizzled as shown in"
  },
  {
    "id": 16343,
    "content": "Figure 142 Figure 142 WGMMA core matrices with 32-byte swizzling  64-Byte swizzling: A group of four adjacent core matrices are swizzled as shown in Figure 143 Figure 143 WGMMA core matrices with 64-byte swizzling  128-Byte swizzling: A group of eight adjacent core matrices are swizzled as shown in Figure 144"
  },
  {
    "id": 16344,
    "content": "Matrix Descriptor Format  Matrix descriptor specifies the properties of the matrix in shared memory that is a multiplicand in the matrix multiply and accumulate operation It is a 64-bit value contained in a register with the following layout: Bit-field Size in bits Description 13–0 14 matrix-descriptor-encode(Matrix start address) 29–16 14 matrix-descriptor-encode(Leading dimension byte offset)"
  },
  {
    "id": 16345,
    "content": "45–32 14 matrix-descriptor-encode(Stride dimension byte offset) 51–49 3 Matrix base offset 63–62 2 Specifies the swizzling mode to be used: 0: No swizzle 1: 128-Byte swizzle 2: 64-Byte swizzle 3: 32-Byte swizzle where matrix-descriptor-encode(x) = (x & 0x3FFFF) >> 0x4 The value of base offset is 0 when the repeating pattern of the specified swizzling mode starts as per the below table: Swizzling"
  },
  {
    "id": 16346,
    "content": "mode Starting address of the repeating pattern 128-Byte swizzle 1024-Byte boundary 64-Byte swizzle 512-Byte boundary 32-Byte swizzle 256-Byte boundary Otherwise, the base offset must be a non-zero value, computed using the following formula: base offset = (pattern start addr >> 0x7) & 0x7 9"
  },
  {
    "id": 16352,
    "content": "wgmma fence instruction must be used to fence the register accesses of wgmma mma_async instruction from their prior accesses wgmma commit_group and wgmma wait_group operations must be used to wait for the completion of the asynchronous matrix multiply and accumulate operations before the results are accessed"
  },
  {
    "id": 16353,
    "content": "Register operand d represents the accumulator matrix as well as the destination matrix, distributed across the participating threads Register operand a represents the multiplicand matrix A in register distributed across the participating threads The 64-bit register operands a-desc and b-desc are the matrix descriptors which represent the multiplicand matrices A and B in shared memory respectively"
  },
  {
    "id": 16354,
    "content": "For certain floating point variants, the input matrices A and B can be transposed by specifying the value 1 for the immediate integer arguments imm-trans-a and imm-trans-b respectively"
  },
  {
    "id": 16360,
    "content": "mma_async operation, each element of the input matrices A and B can be negated by specifying the value -1 for operands imm-scale-a and imm-scale-b respectively"
  },
  {
    "id": 16366,
    "content": "btype must be the same for all floating point wgmma mma_async variants except for the FP8 floating point variants The sizes of individual data elements of matrices A and B in alternate floating point variants of the wgmma mma_async operation are as follows: Matrices A and B have 8-bit data elements when"
  },
  {
    "id": 16371,
    "content": "Precision and rounding: Floating point operations: Element-wise multiplication of matrix A and B is performed with at least single precision"
  },
  {
    "id": 16376,
    "content": "tf32 floating point operations: Element-wise multiplication of matrix A and B is performed with specified precision"
  },
  {
    "id": 16385,
    "content": "mma_async instruction causes the executing thread to wait until all threads in the warp execute the same wgmma mma_async instruction before resuming execution"
  },
  {
    "id": 16390,
    "content": "mma_async instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise behavior is undefined"
  },
  {
    "id": 16397,
    "content": "mma_async operation can be used when A is a structured sparse matrix with 50% zeros in each row distributed in a shape-specific granularity"
  },
  {
    "id": 16409,
    "content": "Matrix A and its corresponding input operand to the sparse wgmma is similar to the diagram shown in Figure 83 , with an appropriate matrix size Sparse wgmma"
  },
  {
    "id": 16415,
    "content": "Only the two non-zero elements are stored in matrix A and their positions in the four-wide chunk in Matrix A are indicated by two 2-bits indices in the metadata operand"
  },
  {
    "id": 16416,
    "content": " The sparsity selector indicates a thread-pair within a group of four consecutive threads which contributes the sparsity metadata"
  },
  {
    "id": 16420,
    "content": "tf32 type For tf32 type, for all supported 64xNx16 shapes, matrix A is structured sparse at a granularity of 1:2"
  },
  {
    "id": 16421,
    "content": "In other words, each chunk of two adjacent elements in a row of matrix A have one zero and one non-zero element Only the non-zero element is stored in operand for matrix A and the 4-bit index in the metadata indicates the position of the non-zero element in the two-wide chunk 0b1110 and 0b0100 are the only meaningful values of the index, the remaining values result in an undefined behavior"
  },
  {
    "id": 16429,
    "content": " All threads contribute the sparsity metadata and the sparsity selector must be 0; any other value results in an undefined behavior"
  },
  {
    "id": 16432,
    "content": "sp with integer type For the integer type, for all supported 64xNx64 shapes, matrix A is structured sparse at a granularity of 2:4"
  },
  {
    "id": 16433,
    "content": "Only the two non-zero elements are stored in matrix A and two 2-bit indices in the metadata indicate the position of these two non-zero elements in the four-wide chunk"
  },
  {
    "id": 16442,
    "content": "Matrix fragments for warpgroup-level multiply-accumulate operation with sparse matrix A  In this section we describe how the contents of thread registers are associated with fragments of A matrix and the sparsity metadata The following table shows the assignment of warps to rows of matrix A: Warp Sparsity information for rows of matrix A %warpid % 4 = 3 48-63 %warpid % 4 = 2 32-47 %warpid % 4 ="
  },
  {
    "id": 16443,
    "content": "1 16-31 %warpid % 4 = 0 0-15 The following conventions are used throughout this section: For matrix A, only the layout of a fragment is described in terms of register vector sizes and their association with the matrix data For matrix D, since the matrix dimension - data type combination is the same for all supported shapes, and is already covered in Matrix multiply-accumulate operation using wgmma"
  },
  {
    "id": 16455,
    "content": "m64nNk32 will compute an MMA operation of shape m64nNk32 where N is a valid n dimension as listed in Matrix shape"
  },
  {
    "id": 16464,
    "content": "bf16 elements out of 4 consecutive elements from matrix A Non-zero elements: a0, a1, a2, a3, a4, a5, a6, a7 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 149  Accumulator D: Matrix fragments for accumulator D are the same as in case of Matrix Fragments for wgmma"
  },
  {
    "id": 16471,
    "content": "b32 register containing 16 2-bit vectors each storing the index of a non-zero element of a 4-wide chunk of matrix A"
  },
  {
    "id": 16486,
    "content": "m64nNk16 will compute an MMA operation of shape m64nNk16 where N is a valid n dimension as listed in Matrix shape"
  },
  {
    "id": 16492,
    "content": "tf32 A vector expression containing four b32 registers, containing four non-zero tf32 elements out of eight consecutive elements from matrix A Non-zero elements: a0, a1, a2, a3 Mapping of the non-zero elements is as described in Sparse matrix storage The layout of the fragments held by different threads is shown in Figure 151  Accumulator D: Matrix fragments for accumulator D are the same as in"
  },
  {
    "id": 16500,
    "content": "b32 register containing eight 4-bit vectors each storing the index of a non-zero element of a 2-wide chunk of matrix A"
  },
  {
    "id": 16514,
    "content": "m64nNk64 will compute an MMA operation of shape m64nNk64 where N is a valid n dimension as listed in Matrix shape"
  },
  {
    "id": 16523,
    "content": "e5m2 elements out of eight consecutive elements from matrix A Non-zero elements: a0, a1, a2, … , a15 Mapping of the non-zero elements is as described in Sparse matrix storage"
  },
  {
    "id": 16538,
    "content": "b32 register containing 16 4-bit vectors each storing the indices of two non-zero elements of a 4-wide chunk of matrix A Figure 154 shows the mapping of the metadata bits to the elements of columns 0–31 of matrix A Figure 154 Sparse WGMMA m64nNk64 metadata layout for"
  },
  {
    "id": 16542,
    "content": "u8 type for columns 0–31  Figure 155 shows the mapping of the metadata bits to the elements of columns 32–63 of matrix A Figure 155 Sparse WGMMA"
  },
  {
    "id": 16562,
    "content": "m64nNk32  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of sixteen"
  },
  {
    "id": 16566,
    "content": "m64nNk32 core matrices for A and B  Layout of core matrices of A is shown in Figure 157 Figure 157 Sparse WGMMA m64nNk32 core matrix layout for A  Layout of core matrices of B is shown in Figure 158 Shared Memory Layout for wgmma"
  },
  {
    "id": 16569,
    "content": "m64nNk16  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of eight"
  },
  {
    "id": 16572,
    "content": "m64nNk16 core matrices for A and B  Layout of core matrices of A is shown in Figure 160 Figure 160 Sparse WGMMA m64nNk16 core matrix layout for A  Layout of core matrices of B is shown in Figure 161 Shared Memory Layout for wgmma"
  },
  {
    "id": 16575,
    "content": "m64nNk64  Core matrices of A and B are as follows: Core matrix Matrix Description Matrix size A Each row is made up of thirty-two"
  },
  {
    "id": 16579,
    "content": "m64nNk64 core matrices for A and B  Layout of core matrices of A is shown in Figure 163 Figure 163 Sparse WGMMA m64nNk64 core matrix layout for A  Layout of core matrices of B is shown in Figure 164"
  },
  {
    "id": 16580,
    "content": "The matrix A is stored in the packed format Mx(K/2) as described in Matrix multiply-accumulate operation using wgmma"
  },
  {
    "id": 16582,
    "content": "sp instruction with sparse matrix A Operands sp-meta and sp-sel represent sparsity metadata and sparsity selector respectively Operand sp-meta is a 32-bit integer and operand sp-sel is a 32-bit integer constant with values in the range 0"
  },
  {
    "id": 16584,
    "content": "The valid values of sp-meta and sp-sel for each shape is specified in Matrix multiply-accumulate operation using wgmma"
  },
  {
    "id": 16586,
    "content": "sp instruction with sparse matrix A and are summarized here : Matrix shape atype Valid values of sp-meta Valid values of sp-sel"
  },
  {
    "id": 16596,
    "content": "u8 0b00, 0b01, 0b10, 0b11 0 (all threads contribute) Matrices A and B are stored in row-major and column-major format respectively"
  },
  {
    "id": 16625,
    "content": "Asynchronous wgmma Proxy Operations  This section describes warpgroup level wgmma fence , wgmma commit_group and wgmma"
  },
  {
    "id": 16632,
    "content": "Asynchronous Multiply-and-Accumulate Instruction: wgmma fence  wgmma fence Enforce an ordering of register accesses between wgmma"
  },
  {
    "id": 16638,
    "content": "fence instruction establishes an ordering between prior accesses to any warpgroup registers and subsequent accesses to the same registers by a wgmma"
  },
  {
    "id": 16640,
    "content": "Only the accumulator register and the input registers containing the fragments of matrix A require this ordering"
  },
  {
    "id": 16641,
    "content": "The wgmma fence instruction must be issued by all warps of the warpgroup at the following locations: Before the first wgmma mma_async operation in a warpgroup Between a register access by a thread in the warpgroup and any wgmma mma_async instruction that accesses the same registers, either as accumulator or input register containing fragments of matrix A, except when these are accumulator"
  },
  {
    "id": 16642,
    "content": "register accesses across multiple wgmma mma_async instructions of the same shape An async proxy fence must be used to establish an ordering between prior writes to shared memory matrices and subsequent reads of the same matrices in a wgmma mma_async instruction"
  },
  {
    "id": 16645,
    "content": "fence instruction causes the executing thread to wait until all threads in the warp execute the same wgmma fence instruction before resuming execution"
  },
  {
    "id": 16650,
    "content": "fence instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined"
  },
  {
    "id": 16700,
    "content": "Asynchronous Multiply-and-Accumulate Instruction: wgmma commit_group  wgmma commit_group Commits all prior uncommitted wgmma mma_async operations into a wgmma-group Syntax wgmma commit_group"
  },
  {
    "id": 16702,
    "content": "aligned; Description wgmma commit_group instruction creates a new wgmma-group per warpgroup and batches all prior wgmma mma_async instructions initiated by the executing warp but not committed to any wgmma-group into the new wgmma-group If there are no uncommitted wgmma mma_async instructions then wgmma commit_group results in an empty wgmma-group An executing thread can wait for the completion"
  },
  {
    "id": 16707,
    "content": "commit_group instruction causes the executing thread to wait until all threads in the warp execute the same wgmma commit_group instruction before resuming execution"
  },
  {
    "id": 16712,
    "content": "commit_group instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined"
  },
  {
    "id": 16713,
    "content": "Asynchronous Multiply-and-Accumulate Instruction: wgmma wait_group  wgmma wait_group Signal the completion of a preceding warpgroup operation Syntax wgmma wait_group"
  },
  {
    "id": 16715,
    "content": "aligned N; Description wgmma wait_group instruction will cause the executing thread to wait until only N or fewer of the most recent wgmma-groups are pending and all the prior wgmma-groups committed by the executing threads are complete For example, when N is 0, the executing thread waits on all the prior wgmma-groups to complete Accessing the accumulator register or the input register containing"
  },
  {
    "id": 16716,
    "content": "the fragments of matrix A of a wgmma mma_async instruction without first performing a wgmma wait_group instruction that waits on a wgmma-group including that wgmma mma_async instruction is undefined behavior"
  },
  {
    "id": 16719,
    "content": "wait_group instruction causes the executing thread to wait until all threads in the warp execute the same wgmma wait_group instruction before resuming execution"
  },
  {
    "id": 16724,
    "content": "wait_group instruction should only be used if it is known that all threads in the warpgroup evaluate the condition identically, otherwise the behavior is undefined"
  },
  {
    "id": 16752,
    "content": "Stack Manipulation Instructions  The stack manipulation instructions can be used to dynamically allocate and deallocate memory on the stack frame of the current function Stack Manipulation Instructions: stacksave  stacksave Save the value of stack pointer into a register Syntax stacksave"
  },
  {
    "id": 16755,
    "content": "u64 }; Description Copies the current value of stack pointer into the destination register d Pointer returned by stacksave can be used in a subsequent stackrestore instruction to restore the stack pointer If d is modified prior to use in stackrestore instruction, it may corrupt data in the stack Stack Manipulation Instructions: stackrestore  stackrestore Update the stack pointer with a new value"
  },
  {
    "id": 16760,
    "content": "When stackrestore is used with operand a written by a prior stacksave instruction, it will effectively restore the state of stack as it was before stacksave was executed Note that if stackrestore is used with an arbitrary value of a , it may cause corruption of stack pointer This implies that the correct use of this feature requires that stackrestore type a is used after stacksave"
  },
  {
    "id": 16765,
    "content": "u64 }; Description The alloca instruction dynamically allocates memory on the stack frame of the current function and updates the stack pointer accordingly"
  },
  {
    "id": 16766,
    "content": "The returned pointer ptr points to local memory and can be used in the address operand of ld local and st local instructions"
  },
  {
    "id": 16767,
    "content": "If sufficient memory is unavailable for allocation on the stack, then execution of alloca may result in stack overflow"
  },
  {
    "id": 16768,
    "content": "In such cases, attempting to access the allocated memory with ptr will result in undefined program behavior"
  },
  {
    "id": 16769,
    "content": "The memory allocated by alloca is deallocated in the following ways: It is automatically deallocated when the function exits It can be explicitly deallocated using stacksave and stackrestore instructions: stacksave can be used to save the value of stack pointer before executing alloca , and stackrestore can be used after alloca to restore stack pointer to the original value which was previously"
  },
  {
    "id": 16770,
    "content": "saved with stacksave Note that accessing deallocated memory after executing stackrestore results in undefined behavior size is an unsigned value which specifies the amount of memory in number of bytes to be allocated on stack immAlign is a 32-bit value which specifies the alignment requirement in number of bytes for the memory allocated by alloca immAlign is an optional argument with default value"
  },
  {
    "id": 16771,
    "content": "being 8 which is the minimum guaranteed alignment Semantics alloca type ptr, size, immAlign: a = max(immAlign, frame_align); frame_align is the minimum guaranteed alignment Allocate size bytes of stack memory with alignment a and update the stack pointer Since ptr is the lowest address of the memory allocated by alloca, the memory can be accessed using ptr up to (ptr + size of allocated memory)"
  },
  {
    "id": 16774,
    "content": "u32 ra, stackptr, ptr, size; stacksave u32 stackptr;   Save the current stack pointer alloca ptr, size, 8;   Allocate stack memory st"
  },
  {
    "id": 16776,
    "content": "u32 [ptr], ra;   Use the allocated stack memory stackrestore u32 stackptr;   Deallocate memory by restoring the stack pointer 9"
  },
  {
    "id": 16779,
    "content": "However, the video instructions may be classified as either scalar or SIMD based on whether their core operation applies to one or multiple values"
  },
  {
    "id": 16780,
    "content": "The video instructions are: vadd , vadd2 , vadd4 vsub , vsub2 , vsub4 vmad vavrg2 , vavrg4 vabsdiff , vabsdiff2 , vabsdiff4 vmin , vmin2 , vmin4 vmax , vmax2 , vmax4 vshl vshr vset , vset2 , vset4 9"
  },
  {
    "id": 16784,
    "content": "Scalar Video Instructions  All scalar video instructions operate on 32-bit register operands The scalar video instructions are: vadd vsub vabsdiff vmin vmax vshl vshr vmad vset The scalar video instructions execute the following stages: Extract and sign- or zero-extend byte, half-word, or word values from its source operands, to produce signed 33-bit input values Optionally perform one of the"
  },
  {
    "id": 16785,
    "content": "following: apply a second operation to the intermediate result and a third operand, or truncate the intermediate result to a byte or half-word value and merge into a specified position in the third operand to produce the final result The general format of scalar video instructions is as follows: 32-bit scalar operation, with optional secondary operation vop"
  },
  {
    "id": 16826,
    "content": "s32 ) is specified in the instruction type; all combinations of dtype , atype , and btype are valid Using the atype/btype and asel/bsel specifiers, the input values are extracted and sign- or zero-extended internally to"
  },
  {
    "id": 16828,
    "content": "The intermediate result is optionally clamped to the range of the destination type (signed or unsigned), taking into account the subword destination size in the case of optional data merging"
  },
  {
    "id": 16836,
    "content": "h1: if ( sign ) return CLAMP( tmp, S16_MAX, S16_MIN ); else return CLAMP( tmp, U16_MAX, U16_MIN ); default: if ( sign ) return CLAMP( tmp, S32_MAX, S32_MIN ); else return CLAMP( tmp, U32_MAX, U32_MIN ); } } This intermediate result is then optionally combined with the third source operand using a secondary arithmetic operation or subword data merge, as shown in the following pseudocode"
  },
  {
    "id": 16837,
    "content": "s33 optSecOp(Modifier secop, s33 tmp, s33 c) { switch ( secop ) { add: return tmp + c; min: return MIN(tmp, c); max return MAX(tmp, c); default: return tmp; } } s33 optMerge( Modifier dsel, s33 tmp, s33 c ) { switch ( dsel ) { case h0: return ((tmp & 0xffff) | (0xffff0000 & c); case h1: return ((tmp & 0xffff) 32 ) tb = 32; if ( mode == wrap ) tb = tb & 0x1f; switch ( vop ){ case vshl: tmp = ta >"
  },
  {
    "id": 16838,
    "content": "tb; } saturate, taking into account destination type and merge operations tmp = optSaturate( tmp, sat, isSigned(dtype), dsel ); d = optSecondaryOp( op2, tmp, c ); optional secondary operation d = optMerge( dsel, tmp, c ); optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2"
  },
  {
    "id": 16882,
    "content": "shr15 }; Description Calculate (a*b) + c , with optional operand negates, plus one mode, and scaling"
  },
  {
    "id": 16883,
    "content": "Although PTX syntax allows separate negation of the a and b operands, internally this is represented as negation of the product (a*b)"
  },
  {
    "id": 16884,
    "content": "The intermediate result of (a*b) is unsigned if atype and btype are unsigned and the product (a*b) is not negated; otherwise, the intermediate result is signed The final result is optionally saturated to the appropriate 32-bit range based on the type (signed or unsigned) of the final result"
  },
  {
    "id": 16885,
    "content": "Semantics   extract byte/half-word/word and sign- or zero-extend   based on source operand type ta = partSelectSignExtend( a, atype, asel ); tb = partSelectSignExtend( b, btype, bsel ); signedFinal = isSigned(atype) || isSigned(btype) || (a"
  },
  {
    "id": 16886,
    "content": "negate ^ b negate) || c negate; tmp[127:0] = ta * tb; lsb = 0; if ( po ) { lsb = 1; } else if ( a negate ^ b negate ) { tmp = ~tmp; lsb = 1; } else if ( c negate ) { c = ~c; lsb = 1; } c128[127:0] = (signedFinal) sext32( c ) : zext ( c ); tmp = tmp + c128 + lsb; switch( scale ) { case shr7: result = (tmp >> 7) & 0xffffffffffffffff; case shr15: result = (tmp >> 15) & 0xffffffffffffffff; } if ( sat"
  },
  {
    "id": 16887,
    "content": ") { if (signedFinal) result = CLAMP(result, S32_MAX, S32_MIN); else result = CLAMP(result, U32_MAX, U32_MIN); } PTX ISA Notes Introduced in PTX ISA version 2"
  },
  {
    "id": 16941,
    "content": "max }; Description Compare input values using specified comparison, with optional secondary arithmetic operation or subword data merge"
  },
  {
    "id": 16942,
    "content": "The intermediate result of the comparison is always unsigned, and therefore destination d and operand c are also unsigned"
  },
  {
    "id": 16943,
    "content": "Semantics   extract byte/half-word/word and sign- or zero-extend   based on source operand type ta = partSelectSignExtend( a, atype, asel ); tb = partSelectSignExtend( b, btype, bsel ); tmp = compare( ta, tb, cmp )"
  },
  {
    "id": 16944,
    "content": "1 : 0; d = optSecondaryOp( op2, tmp, c );   optional secondary operation d = optMerge( dsel, tmp, c );   optional merge with c operand PTX ISA Notes Introduced in PTX ISA version 2"
  },
  {
    "id": 16946,
    "content": "SIMD Video Instructions  The SIMD video instructions operate on pairs of 16-bit values and quads of 8-bit values The SIMD video instructions are: vadd2 , vadd4 vsub2 , vsub4 vavrg2 , vavrg4 vabsdiff2 , vabsdiff4 vmin2 , vmin4 vmax2 , vmax4 vset2 , vset4 PTX includes SIMD video instructions for operation on pairs of 16-bit values and quads of 8-bit values The SIMD video instructions execute the"
  },
  {
    "id": 16947,
    "content": "following stages: Form input vectors by extracting and sign- or zero-extending byte or half-word values from the source operands, to form pairs of signed 17-bit values"
  },
  {
    "id": 16948,
    "content": "Optionally clamp the result to the appropriate signed or unsigned range, as determinted by the destination type"
  },
  {
    "id": 16949,
    "content": "Optionally perform one of the following: perform a second SIMD merge operation, or apply a scalar accumulate operation to reduce the intermediate SIMD results to a single scalar The general format of dual half-word SIMD video instructions is as follows:   2-way SIMD operation, with second SIMD merge or accumulate vop2"
  },
  {
    "id": 16969,
    "content": "hxy, where x,y are from { 0, 1, 2, 3 } }; The general format of quad byte SIMD video instructions is as follows:   4-way SIMD operation, with second SIMD merge or accumulate vop4"
  },
  {
    "id": 17009,
    "content": "SIMD Video Instructions: vadd2, vsub2, vavrg2, vabsdiff2, vmin2, vmax2  vadd2, vsub2 Integer dual half-word SIMD addition/subtraction Syntax   SIMD instruction with secondary SIMD merge operation vop2"
  },
  {
    "id": 17040,
    "content": "Elements of each dual half-word source to the operation are selected from any of the four half-words in the two source operands a and b using the asel and bsel modifiers"
  },
  {
    "id": 17041,
    "content": "The results are optionally clamped to the appropriate range determined by the destination type (signed or unsigned)"
  },
  {
    "id": 17042,
    "content": "For instructions with a secondary SIMD merge operation: For half-word positions indicated in mask, the selected half-word results are copied into destination d For all other positions, the corresponding half-word from source operand c is copied to d For instructions with a secondary accumulate operation: For half-word positions indicated in mask, the selected half-word results are added to"
  },
  {
    "id": 17050,
    "content": "z;   thread id components Description A predefined, read-only, per-thread special register initialized with the thread identifier within the CTA The %tid special register contains a 1D, 2D, or 3D vector to match the CTA shape; the %tid value in unused dimensions is 0 The number of threads in each dimension are specified by the predefined special register %ntid"
  },
  {
    "id": 17068,
    "content": "z; Description A predefined, read-only special register initialized with the number of clusters in each grid dimension The %nclusterid special register contains a 3D grid shape vector that holds the grid dimensions in terms of clusters"
  },
  {
    "id": 17088,
    "content": "z; Description A predefined, read-only special register initialized with the CTA identifier in a cluster in each dimension The %cluster_ctaid special register contains a 1D, 2D, or 3D vector, depending upon the shape of the cluster"
  },
  {
    "id": 17107,
    "content": "z; Description A predefined, read-only special register initialized with the number of CTAs in a cluster in each dimension The %cluster_nctaid special register contains a 3D grid shape vector that holds the cluster dimensions in terms of CTAs"
  },
  {
    "id": 17121,
    "content": "Special Registers: %cluster_ctarank  %cluster_ctarank CTA identifier in a cluster across all dimensions"
  },
  {
    "id": 17124,
    "content": "u32 %cluster_ctarank; Description A predefined, read-only special register initialized with the CTA rank within a cluster across all dimensions It is guaranteed that: 0 ; Description Special registers %pm0"
  },
  {
    "id": 17143,
    "content": "u64 %pm7_64; Description Special registers %pm0_64 %pm7_64 are unsigned 64-bit read-only performance monitor counters"
  },
  {
    "id": 17146,
    "content": "b32 %envreg; Description A set of 32 pre-defined read-only registers used to capture execution environment of PTX program outside of PTX virtual machine"
  },
  {
    "id": 17147,
    "content": "These registers are initialized by the driver prior to kernel launch and can contain cta-wide or grid-wide values"
  },
  {
    "id": 17148,
    "content": "Special Registers: %globaltimer, %globaltimer_lo, %globaltimer_hi  %globaltimer, %globaltimer_lo, %globaltimer_hi %globaltimer A predefined, 64-bit global nanosecond timer"
  },
  {
    "id": 17153,
    "content": "u32 %globaltimer_lo, %globaltimer_hi; Description Special registers intended for use by NVIDIA tools"
  },
  {
    "id": 17154,
    "content": "Special Registers: %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_  %reserved_smem_offset_begin, %reserved_smem_offset_end, %reserved_smem_offset_cap, %reserved_smem_offset_ %reserved_smem_offset_begin Start of the reserved shared memory region"
  },
  {
    "id": 17163,
    "content": "b32 %reserved_smem_offset_; Description These are predefined, read-only special registers containing information about the shared memory region which is reserved for the NVIDIA system software use This region of shared memory is not available to users, and accessing this region from user code results in undefined behavior"
  },
  {
    "id": 17166,
    "content": "b32 %reg_begin, %reg_end, %reg_cap, %reg_offset0, %reg_offset1; mov b32 %reg_begin, %reserved_smem_offset_begin; mov b32 %reg_end, %reserved_smem_offset_end; mov b32 %reg_cap, %reserved_smem_offset_cap; mov b32 %reg_offset0, %reserved_smem_offset_0; mov b32 %reg_offset1, %reserved_smem_offset_1; 10"
  },
  {
    "id": 17168,
    "content": "Special Registers: %total_smem_size  %total_smem_size Total size of shared memory used by a CTA of a kernel"
  },
  {
    "id": 17171,
    "content": "u32 %total_smem_size; Description A predefined, read-only special register initialized with total size of shared memory allocated (statically and dynamically, excluding the shared memory reserved for the NVIDIA system software use) for the CTA of a kernel at launch time Size is returned in multiples of shared memory allocation unit size supported by target architecture Allocation unit values are"
  },
  {
    "id": 17172,
    "content": "as follows: Target architecture Shared memory allocation unit size sm_2x 128 bytes sm_3x , sm_5x , sm_6x , sm_7x 256 bytes sm_8x , sm_9x 128 bytes PTX ISA Notes Introduced in PTX ISA version 4"
  },
  {
    "id": 17174,
    "content": "Special Registers: %aggr_smem_size  %aggr_smem_size Total size of shared memory used by a CTA of a kernel"
  },
  {
    "id": 17177,
    "content": "u32 %aggr_smem_size; Description A predefined, read-only special register initialized with total aggregated size of shared memory consisting of the size of user shared memory allocated (statically and dynamically) at launch time and the size of shared memory region which is reserved for the NVIDIA system software use Special Registers: %dynamic_smem_size  %dynamic_smem_size Size of shared memory"
  },
  {
    "id": 17181,
    "content": "u32 %dynamic_smem_size; Description Size of shared memory allocated dynamically at kernel launch A predefined, read-only special register initialized with size of shared memory allocated dynamically for the CTA of a kernel at launch time"
  },
  {
    "id": 17182,
    "content": "Special Registers: %current_graph_exec  %current_graph_exec An Identifier for currently executing CUDA device graph"
  },
  {
    "id": 17185,
    "content": "u64 %current_graph_exec; Description A predefined, read-only special register initialized with the identifier referring to the CUDA device graph being currently executed"
  },
  {
    "id": 17186,
    "content": "PTX Module Directives  The following directives declare the PTX ISA version of the code in the module, the target architecture for which the code was generated, and the size of addresses within the PTX module"
  },
  {
    "id": 17188,
    "content": "version major minor   major, minor are integers Description Specifies the PTX language version number The major number is incremented when there are incompatible changes to the PTX language, such as changes to the syntax or semantics The version major number is used by the PTX compiler to ensure correct execution of legacy PTX code"
  },
  {
    "id": 17189,
    "content": "Semantics Indicates that this module must be compiled with tools that support an equal or greater version number Each PTX module must begin with a"
  },
  {
    "id": 17192,
    "content": "target stringlist comma separated list of target specifiers string = { sm_90a, sm_90, sm_9x target architectures sm_80, sm_86, sm_87, sm_89, sm_8x target architectures sm_70, sm_72, sm_75, sm_7x target architectures sm_60, sm_61, sm_62, sm_6x target architectures sm_50, sm_52, sm_53, sm_5x target architectures sm_30, sm_32, sm_35, sm_37, sm_3x target architectures sm_20, sm_2x target"
  },
  {
    "id": 17193,
    "content": "architectures sm_10, sm_11, sm_12, sm_13, sm_1x target architectures texmode_unified, texmode_independent, texturing mode debug, platform option map_f64_to_f32 }; platform option Description Specifies the set of features in the target architecture for which the current PTX code was generated"
  },
  {
    "id": 17194,
    "content": "In general, generations of SM architectures follow an onion layer model, where each generation adds new features and retains all features of previous generations The onion layer model allows the PTX code generated for a given target to be run on later generation devices Target architectures with suffix “ a ”, such as sm_90a , include architecture-accelerated features that are supported on the"
  },
  {
    "id": 17195,
    "content": "specified architecture only, hence such targets do not follow the onion layer model Architecture-accelerated features can only be used with targets that support these features"
  },
  {
    "id": 17197,
    "content": "version directive, immediately followed by a target directive containing a target architecture and optional platform options"
  },
  {
    "id": 17199,
    "content": "target directive specifies a single target architecture, but subsequent target directives can be used to change the set of target features allowed during parsing"
  },
  {
    "id": 17201,
    "content": "target directives will compile and run only on devices that support all features of the highest-numbered architecture listed in the program PTX features are checked against the specified target architecture, and an error is generated if an unsupported feature is used The following table summarizes the features in PTX that vary according to target architecture"
  },
  {
    "id": 17202,
    "content": "sm_72 Adds support for integer multiplicand and accumulator matrices in wmma instructions sm_75 Adds support for sub-byte integer and single-bit multiplicant matrices in wmma instructions sm_53 Adds support for arithmetic, comparsion and texture instructions for"
  },
  {
    "id": 17206,
    "content": "target debug option declares that the PTX file contains DWARF debug information, and subsequent compilation of PTX will retain information needed for source-level debugging If the debug option is declared, an error message is generated if no DWARF information is found in the file"
  },
  {
    "id": 17207,
    "content": "map_f64_to_f32 indicates that all double-precision instructions map to single-precision regardless of the target architecture This enables high-level language compilers to compile programs containing type double to target device that do not support double-precision operations"
  },
  {
    "id": 17212,
    "content": "target sm_10   baseline target architecture target sm_13   supports double-precision target sm_20, texmode_independent target sm_90   baseline target architecture target sm_90a   PTX using arch accelerated features 11"
  },
  {
    "id": 17218,
    "content": "address_size address-size address-size = { 32, 64 }; Description Specifies the address size assumed throughout the module by the PTX code and the binary DWARF information in PTX In the presence of separate compilation all modules must specify (or default to) the same address size The"
  },
  {
    "id": 17219,
    "content": "address_size directive is optional, but it must immediately follow the target directive if present within a module"
  },
  {
    "id": 17221,
    "content": "address_size 32   addresses are 32 bit address_size 64   addresses are 64 bit   example of directive placement within a module"
  },
  {
    "id": 17229,
    "content": "Specifying Kernel Entry Points and Functions  The following directives specify kernel entry points and functions Kernel and Function Directives: entry  entry Kernel entry point and body, with optional parameters"
  },
  {
    "id": 17231,
    "content": "entry kernel-name ( param-list ) kernel-body entry kernel-name kernel-body Description Defines a kernel entry point name, parameters, and body for the kernel function"
  },
  {
    "id": 17240,
    "content": "These parameters can only be referenced by name within texture and surface load, store, and query instructions and cannot be accessed via ld"
  },
  {
    "id": 17242,
    "content": "At kernel launch, the kernel dimensions and properties are established and made available via special registers, e"
  },
  {
    "id": 17250,
    "content": "The maximum memory size supported by PTX for normal (non-opaque type) parameters is 32764 bytes The following table shows the allowed parameter size for a PTX ISA version: PTX ISA Version Maximum parameter size (In bytes) PTX ISA version 8 1 and above 32764 PTX ISA version 1 5 and above 4352 PTX ISA version 1 4 and above 256 The CUDA and OpenCL drivers support the following limits for parameter"
  },
  {
    "id": 17251,
    "content": "memory: Driver Parameter memory size CUDA 256 bytes for sm_1x , 4096 bytes for sm_2x and higher , 32764 bytes fo sm_70 and higher OpenCL 32764 bytes for sm_70 and higher, 4352 bytes on sm_6x and lower Target ISA Notes Supported on all target architectures"
  },
  {
    "id": 17290,
    "content": "attribute(attr-list)} (ret-param) fname (param-list) function-body Description Defines a function, including input and return parameters and optional function body An optional noreturn directive indicates that the function does not return to the caller function An optional attribute directive specifies additional information associated with the function See the description of Variable and"
  },
  {
    "id": 17291,
    "content": "Function Attribute Directive: attribute for allowed attributes Parameters in register state space may be referenced directly within instructions in the function body Parameters in param space are accessed using ld param{::func} and st param{::func} instructions in the body The last parameter in the parameter list may be a param array of type"
  },
  {
    "id": 17293,
    "content": "It is used to pass an arbitrary number of parameters to the function packed into a single array object"
  },
  {
    "id": 17294,
    "content": "When calling a function with such an unsized last argument, the last argument may be omitted from the call instruction if no parameter is passed through it The result of an access is undefined if no array was passed, or if the access was outside the bounds of the actual array being passed"
  },
  {
    "id": 17295,
    "content": "The implementation of parameter passing is left to the optimizing translator, which may use a combination of registers and stack locations to pass parameters"
  },
  {
    "id": 17302,
    "content": "0 and later with target sm_20 or higher support at most one return value Target ISA Notes Functions without unsized array parameter supported on all target architectures"
  },
  {
    "id": 17344,
    "content": "alias fAlias, fAliasee; Description alias is a module scope directive that defines identifier fAlias to be an alias to function specified by fAliasee Identifier fAliasee is a function symbol which must be defined in the same module as alias declaration Program can use either fAlias or fAlisee identifiers to reference function defined with fAliasee"
  },
  {
    "id": 17369,
    "content": "branchtargets list-of-labels ; Description Declares a list of potential branch targets for a subsequent brx idx , and associates the list with the label at the start of the line All control flow labels in the list must occur within the same function as the declaration The list of labels may use the compact, shorthand syntax for enumerating a range of labels having a common prefix, similar to the"
  },
  {
    "id": 17379,
    "content": "Syntax Label: calltargets list-of-functions ; Description Declares a list of potential call targets for a subsequent indirect call, and associates the list with the label at the start of the line All functions named in the list must be declared prior to the calltargets directive, and all functions must have the same type signature"
  },
  {
    "id": 17390,
    "content": "noreturn;   no input params,   return params label: callprototype (ret-param) _ ;   input, return parameters label: callprototype (ret-param) _ (param-list); Description Defines a prototype with no specific function name, and associates the prototype with a label The prototype may then be used in indirect call instructions where there is incomplete knowledge of the possible call targets"
  },
  {
    "id": 17391,
    "content": "Parameters may have either base types in the register or parameter state spaces, or array types in parameter state space"
  },
  {
    "id": 17415,
    "content": "Performance-Tuning Directives  To provide a mechanism for low-level performance tuning, PTX supports the following directives, which pass information to the backend optimizing compiler"
  },
  {
    "id": 17422,
    "content": "maxnreg directive specifies the maximum number of registers to be allocated to a single thread; the maxntid directive specifies the maximum number of threads in a thread block (CTA); the reqntid directive specifies the required number of threads in a thread block (CTA); and the minnctapersm directive specifies a minimum number of thread blocks to be scheduled on a single multiprocessor (SM)"
  },
  {
    "id": 17425,
    "content": ", registers) to increase total thread count and provide a greater opportunity to hide memory latency"
  },
  {
    "id": 17429,
    "content": "reqntid directive to trade-off registers-per-thread against multiprocessor utilization without needed to directly specify a maximum number of registers This may achieve better performance when compiling PTX for multiple devices having different numbers of registers per SM"
  },
  {
    "id": 17434,
    "content": "minnctapersm directives may be applied per-entry and must appear between an entry directive and its body"
  },
  {
    "id": 17435,
    "content": "The directives take precedence over any module-level constraints passed to the optimizing backend A warning message is generated if the directives’ constraints are inconsistent or cannot be met for the specified target device"
  },
  {
    "id": 17436,
    "content": "The directive passes a list of strings to the backend, and the strings have no semantics within the PTX virtual machine model"
  },
  {
    "id": 17440,
    "content": "pragma directives may appear at module (file) scope, at entry-scope, or as statements within a kernel or device function body"
  },
  {
    "id": 17447,
    "content": "maxnreg n Description Declare the maximum number of registers per thread in a CTA The actual number of registers used may be less; for example, the backend may be able to compile to fewer registers, or the maximum number of registers may be further constrained by"
  },
  {
    "id": 17453,
    "content": "maxntid nx maxntid nx, ny maxntid nx, ny, nz Description Declare the maximum number of threads in the thread block (CTA) This maximum is specified by giving the maximum extent of each dimension of the 1D, 2D, or 3D CTA Semantics The maximum number of threads in the thread block, computed as the product of the maximum extent specified for each dimension, is guaranteed not to be exceeded in any"
  },
  {
    "id": 17454,
    "content": "invocation of the kernel in which this directive appears Exceeding the maximum number of threads results in a runtime error or kernel launch failure Note that this directive guarantees that the total number of threads does not exceed the maximum, but does not guarantee that the limit in any particular dimension is not exceeded"
  },
  {
    "id": 17458,
    "content": "reqntid nx reqntid nx, ny reqntid nx, ny, nz Description Declare the number of threads in the thread block (CTA) by specifying the extent of each dimension of the 1D, 2D, or 3D CTA Semantics The size of each CTA dimension specified in any invocation of the kernel is required to be equal to that specified in this directive Specifying a different CTA dimension at launch will result in a runtime"
  },
  {
    "id": 17463,
    "content": "minnctapersm ncta Description Declare the minimum number of CTAs from the kernel’s grid to be mapped to a single multiprocessor (SM)"
  },
  {
    "id": 17488,
    "content": "maxnctapersm ncta Description Declare the maximum number of CTAs from the kernel’s grid that may be mapped to a single multiprocessor (SM)"
  },
  {
    "id": 17494,
    "content": "maxnctapersm to compute an upper-bound on per-thread register usage so that the specified number of CTAs can be mapped to a single multiprocessor However, if the number of registers used by the backend is sufficiently lower than this bound, additional CTAs may be mapped to a single multiprocessor"
  },
  {
    "id": 17507,
    "content": "noreturn directive indicates that the function does not return to caller function noreturn directive can only be specified on device functions and must appear between a func directive and its body If a function with noreturn directive returns to the caller function at runtime, then the behavior is undefined"
  },
  {
    "id": 17517,
    "content": "pragma list-of-strings ; Description Pass module-scoped, entry-scoped, or statement-level directives to the PTX backend compiler"
  },
  {
    "id": 17518,
    "content": "Semantics The interpretation of pragma directive strings is implementation-specific and has no impact on PTX semantics"
  },
  {
    "id": 17526,
    "content": "Debugging Directives  DWARF-format debug information is passed through PTX modules using the following directives: @@DWARF"
  },
  {
    "id": 17531,
    "content": "0 and replaces the @@DWARF syntax The @@DWARF syntax was deprecated in PTX ISA version 2 0 but is supported for legacy PTX ISA version 1"
  },
  {
    "id": 17539,
    "content": "Debugging Directives: @@dwarf  @@dwarf DWARF-format information Syntax @@DWARF dwarf-string dwarf-string may have one of the"
  },
  {
    "id": 17540,
    "content": "byte byte-list   comma-separated hexadecimal byte values 4byte int32-list   comma-separated hexadecimal integers in range [0"
  },
  {
    "id": 17551,
    "content": "debug_info @@DWARF 4byte 0x000006b5, 0x00000364, 0x61395a5f, 0x5f736f63 @@DWARF 4byte 0x6e69616d, 0x63613031, 0x6150736f, 0x736d6172 @@DWARF"
  },
  {
    "id": 17581,
    "content": "file file_index \"filename\" {, timestamp, file_size} Description Associates a source filename with an integer index file directive allows optionally specifying an unsigned number representing time of last modification and an unsigned integer representing size in bytes of source file"
  },
  {
    "id": 17596,
    "content": "loc file_index line_number column_position loc file_index line_number column_position,function_name label {+ immediate }, inlined_at file_index2 line_number2 column_position2 Description Declares the source file location (source file, line number, and column position) to be associated with lexically subsequent PTX instructions"
  },
  {
    "id": 17597,
    "content": "To indicate PTX instructions that are generated from a function that got inlined, additional attribute"
  },
  {
    "id": 17600,
    "content": "inlined_at attribute specifies source location at which the specified function is inlined file_index2 , line_number2 , and column_position2 specify the location at which function is inlined Source location specified as part of inlined_at directive must lexically precede as source location in loc directive"
  },
  {
    "id": 17602,
    "content": "debug_str section DWARF section debug_str contains ASCII null-terminated strings that specify the name of the function that is inlined"
  },
  {
    "id": 17603,
    "content": "Note that a PTX instruction may have a single associated source location, determined by the nearest lexically preceding loc directive, or no associated source location if there is no preceding loc directive"
  },
  {
    "id": 17605,
    "content": "loc 2 4237 0 L1:   line 4237, col 0 of file #2,   inherited from mov mov u32 %r1,%r2;   line 4237, col 0 of file #2 add u32 %r2,%r1,%r3;   line 4237, col 0 of file #2 L2:   line 4239, col 5 of file #2,   inherited from sub loc 2 4239 5 sub u32 %r2,%r1,%r3;   line 4239, col 5 of file #2 loc 1 21 3 loc 1 9 3, function_name info_string0, inlined_at 1 21 3 ld"
  },
  {
    "id": 17610,
    "content": "loc 1 27 3 loc 1 10 5, function_name info_string1, inlined_at 1 27 3 loc 1 15 3, function_name debug_str+16, inlined_at 1 10 5 setp"
  },
  {
    "id": 17615,
    "content": "b8 95   _ b8 90   z b8 51   3 b8 102   f b8 111   o b8 111   o b8 118   v b8 0 info_string1: b8 95   _ b8 90   z b8 51   3 b8 98   b b8 97   a b8 114   r b8 118   v b8 0 b8 95   _ b8 90   z b8 51   3 b8 99   c b8 97   a b8 114   r b8 118   v b8 0 } 11"
  },
  {
    "id": 17618,
    "content": "extern identifier Description Declares identifier to be defined external to the current module The module defining such identifier must define it as"
  },
  {
    "id": 17621,
    "content": "Extern declaration of symbol may appear multiple times and references to that get resolved against the single definition of that symbol"
  },
  {
    "id": 17622,
    "content": "Unlike C, where identifiers are globally visible unless declared static, PTX identifiers are visible only within the current module unless declared visible outside the current Weak symbols are similar to globally visible symbols, except during linking, weak symbols are only chosen after globally visible symbols during symbol resolution Unlike globally visible symbols, multiple object files may"
  },
  {
    "id": 17623,
    "content": "declare the same weak symbol, and references to a symbol get resolved against a weak symbol only if no global symbols have the same name"
  },
  {
    "id": 17625,
    "content": "common identifier Description Declares identifier to be globally visible but “common” However multiple object files may declare the same common symbol and they may have different types and sizes and references to a symbol get resolved against a common symbol with the largest size Only one object file can initialize a common symbol and that must have the largest size among all other definitions of"
  },
  {
    "id": 17631,
    "content": "reqnctapercluster directive specifies the number of CTAs in the cluster The explicitcluster directive specifies that the kernel should be launched with explicit cluster details The cluster dimension directives can be applied only on kernel functions"
  },
  {
    "id": 17638,
    "content": "reqnctapercluster nx reqnctapercluster nx, ny reqnctapercluster nx, ny, nz Description Set the number of thread blocks (CTAs) in the cluster by specifying the extent of each dimension of the 1D, 2D, or 3D cluster"
  },
  {
    "id": 17640,
    "content": "reqnctapercluster directive specified, runtime will use the specified values for configuring the launch if the same are not specified at launch time Semantics If cluster dimension is explicitly specified at launch time, it should be equal to the values specified in this directive Specifying a different cluster dimension at launch will result in a runtime error or kernel launch failure"
  },
  {
    "id": 17653,
    "content": "explicitcluster  explicitcluster Declare that Kernel must be launched with cluster dimensions explicitly specified"
  },
  {
    "id": 17655,
    "content": "explicitcluster Description Declares that this Kernel should be launched with cluster dimension explicitly specified"
  },
  {
    "id": 17657,
    "content": "explicitcluster directive must be launched with cluster dimension explicitly specified (either at launch time or via"
  },
  {
    "id": 17668,
    "content": "maxclusterrank n Description Declare the maximum number of thread blocks (CTAs) allowed to be part of the cluster Semantics Product of the number of CTAs in each cluster dimension specified in any invocation of the kernel is required to be less or equal to that specified in this directive The maxclusterrank directive cannot be used in conjunction with the reqnctapercluster directive"
  },
  {
    "id": 17673,
    "content": "Release Notes  This section describes the history of change in the PTX ISA and implementation The first section describes ISA and implementation changes in the current release of PTX ISA version 8 5, and the remaining sections provide a record of changes in previous releases of PTX ISA versions back to PTX ISA version 2"
  },
  {
    "id": 17675,
    "content": "Changes in PTX ISA Version 8 5  New Features PTX ISA version 8 5 introduces the following new features: Adds support for mma"
  },
  {
    "id": 17677,
    "content": "Semantic Changes and Clarifications Values 0b0000 , 0b0101 , 0b1010 , 0b1111 for sparsity metadata (operand e ) of instruction mma"
  },
  {
    "id": 17681,
    "content": "Changes in PTX ISA Version 8 4  New Features PTX ISA version 8 4 introduces the following new features: Extends ld , st and atom instructions with"
  },
  {
    "id": 17694,
    "content": "Changes in PTX ISA Version 8 3  New Features PTX ISA version 8 3 introduces the following new features: Adds support for pragma used_bytes_mask that is used to specify mask for used bytes for a load operation"
  },
  {
    "id": 17703,
    "content": "Changes in PTX ISA Version 8 2  New Features PTX ISA version 8 2 introduces the following new features: Adds support for"
  },
  {
    "id": 17718,
    "content": "tensor instructions is optimized for target architecture sm_90a and may have substantially reduced performance on other targets and hence multicast::cluster is advised to be used with sm_90a"
  },
  {
    "id": 17721,
    "content": "Changes in PTX ISA Version 8 1  New Features PTX ISA version 8 1 introduces the following new features: Adds support for st"
  },
  {
    "id": 17722,
    "content": "async and red async instructions for asynchronous store and asynchronous reduction operations respectively on shared memory"
  },
  {
    "id": 17728,
    "content": "Adds support for multimem ld_reduce , multimem st and multimem red instructions to perform memory operations on multimem addresses"
  },
  {
    "id": 17731,
    "content": "Changes in PTX ISA Version 8 0  New Features PTX ISA version 8 0 introduces the following new features: Adds support for target sm_90a that supports specialized accelerated features Adds support for asynchronous warpgroup-level matrix multiply-and-accumulate operation wgmma"
  },
  {
    "id": 17732,
    "content": "Extends the asynchronous copy operations with bulk operations that operate on large data, including tensor data"
  },
  {
    "id": 17742,
    "content": "Adds support for special register %current_graph_exec that identifies the currently executing CUDA device graph"
  },
  {
    "id": 17746,
    "content": "try_wait operations Adds support for transaction count operations on mbarrier objects, specified with"
  },
  {
    "id": 17751,
    "content": "Changes in PTX ISA Version 7 8  New Features PTX ISA version 7 8 introduces the following new features: Adds support for sm_89 target architecture Adds support for movmatrix instruction which transposes a matrix in registers across a warp"
  },
  {
    "id": 17757,
    "content": "Extends add , sub , mul , set , setp , cvt , tanh , ex2 , atom and red instructions with bf16 alternate floating point data format"
  },
  {
    "id": 17758,
    "content": "Adds support for griddepcontrol instruction as a communication mechanism to control the execution of dependent grids Adds support for new thread scope"
  },
  {
    "id": 17762,
    "content": "shared state space qualifier with ::cluster sub-qualifier for cluster-level visibility of shared memory Extends isspacep , cvta , ld , st , atom , and red instructions to accept ::cluster sub-qualifier with shared state space qualifier Adds support for mapa instruction to map a shared memory address to the corresponding address in a different CTA within the cluster Adds support for getctarank"
  },
  {
    "id": 17763,
    "content": "instruction to query the rank of the CTA that contains a given address Adds support for special registers related to cluster information: %is_explicit_cluster , %clusterid , %nclusterid , %cluster_ctaid , %cluster_nctaid , %cluster_ctarank , %cluster_nctarank Adds support for cluster dimension directives"
  },
  {
    "id": 17769,
    "content": "Changes in PTX ISA Version 7 7  New Features PTX ISA version 7 7 introduces the following new features: Extends isspacep and cvta instructions to include the"
  },
  {
    "id": 17773,
    "content": "Changes in PTX ISA Version 7 6  New Features PTX ISA version 7 6 introduces the following new features: Support for szext instruction which performs sign-extension or zero-extension on a specified value Support for bmsk instruction which creates a bitmask of the specified width starting at the specified bit position"
  },
  {
    "id": 17774,
    "content": "Support for special registers %reserved_smem_offset_begin , %reserved_smem_offset_end , %reserved_smem_offset_cap , %reserved_smem_offset"
  },
  {
    "id": 17777,
    "content": "Changes in PTX ISA Version 7 5  New Features PTX ISA version 7 5 introduces the following new features: Debug information enhancements to support label difference and negative values in the"
  },
  {
    "id": 17779,
    "content": "Extensions to the memory consistency model to introduce the following new concepts: A memory proxy as an abstract label for different methods of memory access"
  },
  {
    "id": 17781,
    "content": "proxy and membar proxy instructions to allow synchronization of memory accesses performed via virtual aliases"
  },
  {
    "id": 17784,
    "content": "Changes in PTX ISA Version 7 4  New Features PTX ISA version 7 4 introduces the following new features: Support for sm_87 target architecture Support for"
  },
  {
    "id": 17791,
    "content": "Support for createpolicy instruction which allows construction of different types of cache eviction policies Support for level::cache_hint qualifier which allows the use of cache eviction policies with ld , ld"
  },
  {
    "id": 17797,
    "content": "Changes in PTX ISA Version 7 3  New Features PTX ISA version 7 3 introduces the following new features: Extends mask() operator used in initializers to also support integer constant expression"
  },
  {
    "id": 17798,
    "content": "Adds support for stack manpulation instructions that allow manipulating stack using stacksave and stackrestore instructions and allocation of per-thread stack using alloca instruction Semantic Changes and Clarifications The unimplemented version of alloca from the older PTX ISA specification has been replaced with new stack manipulation instructions in PTX ISA version 7"
  },
  {
    "id": 17802,
    "content": "Changes in PTX ISA Version 7 2  New Features PTX ISA version 7 2 introduces the following new features: Enhances"
  },
  {
    "id": 17809,
    "content": "Changes in PTX ISA Version 7 1  New Features PTX ISA version 7 1 introduces the following new features: Support for sm_86 target architecture"
  },
  {
    "id": 17810,
    "content": "Adds a new operator, mask() , to extract a specific byte from variable’s address used in initializers"
  },
  {
    "id": 17811,
    "content": "Extends tex and tld4 instructions to return an optional predicate that indicates if data at specified coordinates is resident in memory Extends mma instruction to support"
  },
  {
    "id": 17817,
    "content": "Changes in PTX ISA Version 7 0  New Features PTX ISA version 7 0 introduces the following new features: Support for sm_80 target architecture"
  },
  {
    "id": 17818,
    "content": "Adds support for asynchronous copy instructions that allow copying of data asynchronously from one state space to another Adds support for mbarrier instructions that allow creation of mbarrier objects in memory and use of these objects to synchronize threads and asynchronous copy operations initiated by threads Adds support for redux"
  },
  {
    "id": 17850,
    "content": "Changes in PTX ISA Version 6 5  New Features PTX ISA version 6 5 introduces the following new features: Adds support for integer destination types for half precision comparison instruction set Adds support for cvt"
  },
  {
    "id": 17852,
    "content": "Adds support for ldmatrix instruction which loads one or more matrices from shared memory for mma instruction"
  },
  {
    "id": 17860,
    "content": "Changes in PTX ISA Version 6 4  New Features PTX ISA version 6 4 introduces the following new features: Adds support for"
  },
  {
    "id": 17882,
    "content": "Changes in PTX ISA Version 6 3  New Features PTX ISA version 6 3 introduces the following new features: Support for sm_75 target architecture Adds support for a new instruction nanosleep that suspends a thread for a specified duration"
  },
  {
    "id": 17891,
    "content": "aligned qualifier for all wmma instructions Specified the alignment required for the base address and stride parameters passed to wmma load and wmma"
  },
  {
    "id": 17893,
    "content": "Clarified that layout of fragment returned by wmma operation is architecture dependent and passing wmma fragments around functions compiled for different link compatible SM architectures may not work as expected Clarified that atomicity for {atom/red}"
  },
  {
    "id": 17894,
    "content": "f16x2} operations is guranteed separately for each of the two f16 elements but not guranteed to be atomic as single 32-bit access"
  },
  {
    "id": 17897,
    "content": "Changes in PTX ISA Version 6 2  New Features PTX ISA version 6 2 introduces the following new features: A new instruction activemask for querying active threads in a warp"
  },
  {
    "id": 17905,
    "content": "Semantic Changes and Clarifications Clarified that wmma instructions can be used in conditionally executed code only if it is known that all threads in the warp evaluate the condition identically, otherwise behavior is undefined"
  },
  {
    "id": 17906,
    "content": "In the memory consistency model, the definition of morally strong operations was updated to exclude fences from the requirement of complete overlap since fences do not access memory"
  },
  {
    "id": 17909,
    "content": "Changes in PTX ISA Version 6 1  New Features PTX ISA version 6 1 introduces the following new features: Support for sm_72 target architecture Support for new matrix shapes 32x8x16 and 8x32x16 in wmma instruction"
  },
  {
    "id": 17912,
    "content": "Changes in PTX ISA Version 6 0  New Features PTX ISA version 6 0 introduces the following new features: Support for sm_70 target architecture Specifies the memory consistency model for programs running on sm_70 and later architectures"
  },
  {
    "id": 17913,
    "content": "Various extensions to memory instructions to specify memory synchronization semantics and scopes at which such synchronization can be observed"
  },
  {
    "id": 17914,
    "content": "New instruction wmma for matrix operations which allows loading matrices from memory, performing multiply-and-accumulate on them and storing result in memory"
  },
  {
    "id": 17915,
    "content": "Extends vote and shfl instructions with sync modifier which waits for specified threads before executing the vote and shfl operation respectively"
  },
  {
    "id": 17922,
    "content": "Semantic Changes and Clarifications Semantics of bar instruction were updated to indicate that executing thread waits for other non-exited threads from it’s warp"
  },
  {
    "id": 17924,
    "content": "1 which was unimplemented has been removed from the spec Support for taking address of labels, using labels in initializers which was unimplemented has been removed from the spec Support for variadic functions which was unimplemented has been removed from the spec"
  },
  {
    "id": 17927,
    "content": "Changes in PTX ISA Version 5 0  New Features PTX ISA version 5 0 introduces the following new features: Support for sm_60 , sm_61 , sm_62 target architecture A new"
  },
  {
    "id": 17928,
    "content": "common directive to permit linking multiple object files containing declarations of the same symbol with different size"
  },
  {
    "id": 17929,
    "content": "Semantic Changes and Clarifications Semantics of cache modifiers on ld and st instructions were clarified to reflect cache operations are treated as performance hint only and do not change memory consistency behavior of the program Semantics of volatile operations on ld and st instructions were clarified to reflect how volatile operations are handled by optimizing compiler"
  },
  {
    "id": 17932,
    "content": "Changes in PTX ISA Version 4 3  New Features PTX ISA version 4 3 introduces the following new features: A new lop3 instruction which allows arbitrary logical operation on 3 inputs"
  },
  {
    "id": 17933,
    "content": "Extends tex and tld4 instructions to support optional operands for offset vector and depth compare Extends txq instruction to support querying texture fields from specific LOD"
  },
  {
    "id": 17936,
    "content": "Changes in PTX ISA Version 4 2  New Features PTX ISA version 4 2 introduces the following new features: Support for sm_53 target architecture Support for memory_layout field for surfaces and suq instruction support for querying this field"
  },
  {
    "id": 17937,
    "content": "Semantic Changes and Clarifications Semantics for parameter passing under ABI were updated to indicate ld"
  },
  {
    "id": 17941,
    "content": "f32 were updated to indicate subnormal inputs and results are flushed to sign-preserving zero for atomic operations on global memory; whereas atomic operations on shared memory preserve subnormal inputs and results and don’t flush them to zero"
  },
  {
    "id": 17944,
    "content": "Changes in PTX ISA Version 4 1  New Features PTX ISA version 4 1 introduces the following new features: Support for sm_37 and sm_52 target architectures Support for new fields array_size , num_mipmap_levels and num_samples for Textures, and the txq instruction support for querying these fields Support for new field array_size for Surfaces, and the suq instruction support for querying this field"
  },
  {
    "id": 17948,
    "content": "Changes in PTX ISA Version 4 0  New Features PTX ISA version 4 0 introduces the following new features: Support for sm_32 and sm_50 target architectures A new instruction, rsqrt"
  },
  {
    "id": 17952,
    "content": "Semantic Changes and Clarifications The vote instruction semantics were updated to clearly indicate that an inactive thread in a warp contributes a 0 for its entry when participating in vote"
  },
  {
    "id": 17957,
    "content": "Changes in PTX ISA Version 3 2  New Features PTX ISA version 3 2 introduces the following new features: The texture instruction supports reads from multi-sample and multisample array textures"
  },
  {
    "id": 17958,
    "content": "Semantic Changes and Clarifications The vavrg2 and vavrg4 instruction semantics were updated to indicate that instruction adds 1 only if Va[i] + Vb[i] is non-negative, and that the addition result is shifted by 1 (rather than being divided by 2)"
  },
  {
    "id": 17961,
    "content": "Changes in PTX ISA Version 3 1  New Features PTX ISA version 3 1 introduces the following new features: Support for sm_35 target architecture Support for CUDA Dynamic Parallelism, which enables a kernel to create and synchronize new work"
  },
  {
    "id": 17962,
    "content": "Extends atomic and reduction instructions to perform 64-bit {and, or, xor} operations, and 64-bit integer {min, max} operations"
  },
  {
    "id": 17963,
    "content": "Extends support for generic addressing to include the const state space, and adds a new operator, generic() , to form a generic address for"
  },
  {
    "id": 17969,
    "content": "1 redefines the default addressing for global variables in initializers, from generic addresses to offsets in the global state space"
  },
  {
    "id": 17974,
    "content": "Changes in PTX ISA Version 3 0  New Features PTX ISA version 3 0 introduces the following new features: Support for sm_30 target architectures"
  },
  {
    "id": 17977,
    "content": "Semantic Changes and Clarifications Special register %gridid has been extended from 32-bits to 64-bits"
  },
  {
    "id": 17982,
    "content": "The shfl instruction semantics were updated to clearly indicate that value of source operand a is unpredictable for inactive and predicated-off threads within the warp"
  },
  {
    "id": 17988,
    "content": "The texture array feature supports access to an array of 1D or 2D textures, where an integer indexes into the array of textures, and then one or two single-precision floating point coordinates are used to address within the selected 1D or 2D texture"
  },
  {
    "id": 17992,
    "content": "maxntid only guarantees that the total number of threads in a thread block does not exceed the maximum Previously, the semantics indicated that the maximum was enforced separately in each dimension, which is not the case"
  },
  {
    "id": 17993,
    "content": "Bit field extract and insert instructions BFE and BFI now indicate that the len and pos operands are restricted to the value range 0"
  },
  {
    "id": 18000,
    "content": "2 adds a new directive for specifying kernel parameter attributes; specifically, there is a new directives for specifying that a kernel parameter is a pointer, for specifying to which state space the parameter points, and for optionally specifying the alignment of the memory to which the parameter points"
  },
  {
    "id": 18001,
    "content": "This field is used in the independent texturing mode to override the normalized_coords field in the texture header This field is needed to support languages such as OpenCL, which represent the property of normalized/unnormalized coordinates in the sampler header rather than in the texture header"
  },
  {
    "id": 18006,
    "content": "2 adds a new tld4 instruction for loading a component ( r , g , b , or a ) from the four texels compising the bilinear interpolation footprint of a given texture location"
  },
  {
    "id": 18007,
    "content": "This instruction may be used to compute higher-precision bilerp results in software, or for performing higher-bandwidth texture loads"
  },
  {
    "id": 18010,
    "content": "Changes in PTX ISA Version 2 1  New Features The underlying, stack-based ABI is supported in PTX ISA version 2"
  },
  {
    "id": 18014,
    "content": "calltargets , have been added for specifying potential targets for indirect branches and indirect function calls"
  },
  {
    "id": 18016,
    "content": "callprototype directive has been added for declaring the type signatures for indirect function calls"
  },
  {
    "id": 18021,
    "content": "Textures and surfaces have new fields for channel data type and channel order, and the txq and suq instructions support queries for these fields"
  },
  {
    "id": 18031,
    "content": "Changes in PTX ISA Version 2 0  New Features Floating Point Extensions This section describes the floating-point changes in PTX ISA version 2"
  },
  {
    "id": 18033,
    "content": "The goal is to achieve IEEE 754 compliance wherever possible, while maximizing backward compatibility with legacy PTX ISA version 1"
  },
  {
    "id": 18036,
    "content": "x are as follows: Single-precision instructions support subnormal numbers by default for sm_20 targets Single-precision add , sub , and mul now support"
  },
  {
    "id": 18038,
    "content": "rp rounding modifiers for sm_20 targets A single-precision fused multiply-add (fma) instruction has been added, with support for IEEE 754 compliant rounding modifiers and support for subnormal numbers"
  },
  {
    "id": 18046,
    "content": "shared have been extended to handle 64-bit data types for sm_20 targets The bar instruction has been extended as follows: A bar arrive instruction has been added Instruction isspacep for querying whether a generic address falls within a specified state space window has been added Instruction cvta for converting global, local, and shared addresses to generic address and vice-versa has been added"
  },
  {
    "id": 18047,
    "content": "Other New Features Instructions ld , ldu , st , prefetch , prefetchu , isspacep , cvta , atom , and red now support generic addressing New special registers %nwarpid , %nsmid , %clock64 , %lanemask_{eq,le,lt,ge,gt} have been added Cache operations have been added to instructions ld , st , suld , and sust , e"
  },
  {
    "id": 18054,
    "content": "section , has been added to replace the @@DWARF syntax for passing DWARF-format debugging information through PTX"
  },
  {
    "id": 18059,
    "content": "4 and earlier, where single-precision subnormal inputs and results were not flushed to zero if either source or destination type size was 64-bits, has been fixed"
  },
  {
    "id": 18063,
    "content": "ftz is implied) instructions flush single-precision subnormal inputs and results to sign-preserving zero for all combinations of floating-point instruction types"
  },
  {
    "id": 18066,
    "content": "4 or earlier, single-precision subnormal inputs and results are flushed to sign-preserving zero only when neither source nor destination type size is 64-bits"
  },
  {
    "id": 18067,
    "content": "Components of special registers %tid , %ntid , %ctaid , and %nctaid have been extended from 16-bits to 32-bits"
  },
  {
    "id": 18068,
    "content": "The number of samplers available in independent texturing mode was incorrectly listed as thirty-two in PTX ISA version 1"
  },
  {
    "id": 18077,
    "content": "pragma \"nounroll\"; Description The \"nounroll\" pragma is a directive to disable loop unrolling in the optimizing backend compiler The \"nounroll\" pragma is allowed at module, entry-function, and statement levels, with the following meanings: module scope disables unrolling for all loops in module, including loops preceding the pragma statement-level pragma disables unrolling of the loop for which"
  },
  {
    "id": 18078,
    "content": "the current block is the loop header Note that in order to have the desired effect at statement level, the \"nounroll\" directive must appear before any instruction statements in the loop header basic block for the desired loop The loop header block is defined as the block that dominates all blocks in the loop body and is the target of the loop backedge Statement-level \"nounroll\" directives"
  },
  {
    "id": 18087,
    "content": "Pragma Strings: “used_bytes_mask”  “used_bytes_mask” Mask for indicating used bytes in data of ld operation"
  },
  {
    "id": 18089,
    "content": "pragma \"used_bytes_mask mask\"; Description The \"used_bytes_mask\" pragma is a directive that specifies used bytes in a load operation based on the mask provided \"used_bytes_mask\" pragma needs to be specified prior to a load instruction for which information about bytes used from the load operation is needed For a load instruction without this pragma, all bytes from the load operation are assumed"
  },
  {
    "id": 18090,
    "content": "to be used Operand mask is a 32-bit integer with set bits indicating the used bytes in data of load operation Semantics Each bit in mask operand corresponds to a byte data where each set bit represents the used byte For 4 bytes load with only lower 3 bytes used pragma \"used_bytes_mask 0x7\"; ld"
  },
  {
    "id": 18092,
    "content": "u32 %r0, [gbl];   Higher 1 byte from %r0 is unused   For vector load of 16 bytes with lower 12 bytes used"
  },
  {
    "id": 18103,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 18104,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 18106,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 18107,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 18108,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 18109,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 18110,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 18111,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 18112,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 18113,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 18114,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 18115,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 18116,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 18123,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 18125,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 18126,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 18132,
    "content": "5 | PDF | Archive PTX Writer’s Guide to Interoperability The guide to writing ABI-compliant PTX Introduction  This document defines the Application Binary Interface (ABI) for the CUDA ® architecture when generating PTX By following the ABI, external developers can generate compliant PTX code that can be linked with other code"
  },
  {
    "id": 18134,
    "content": "PTX is meant to be GPU-architecture independent, so that the same code can be reused for different GPU architectures"
  },
  {
    "id": 18136,
    "content": "Programs conforming to an ABI are expected to be executed on the appropriate architecture GPU, and can assume that instructions from that ISA are available"
  },
  {
    "id": 18140,
    "content": "Any PTX producer must use these sizes and alignments in order for its PTX to be compatible with PTX generated by other producers PTX has an"
  },
  {
    "id": 18143,
    "content": "b8 1 1 untyped byte b16 2 2 untyped halfword b32 4 4 untyped word b64 8 8 untyped doubleword s8 1 1 signed integral byte s16 2 2 signed integral halfword s32 4 4 signed integral word s64 8 8 signed integral doubleword u8 1 1 unsigned integral byte u16 2 2 unsigned integral halfword u32 4 4 unsigned integral word u64 8 8 unsigned integral doubleword"
  },
  {
    "id": 18146,
    "content": "Aggregates and Unions  Beyond the scalar types, PTX also supports native-vector types of these scalar types, with both its vector syntax and its byte-array syntax For scalar types with a size no greater than four bytes, vector types with 1, 2, 3, and 4 elements exist; for all other types, only 1 and 2 element vector types exist For a non-native-vector type, an entire aggregate or union is"
  },
  {
    "id": 18147,
    "content": "aligned on the same boundary as its most strictly aligned member For example, in OpenCL built-in vector data types have their alignment set to the size of the built-in data type in bytes For a native vector type – discussed at the start of this section – the alignment is defined as follows (For the definitions below, the native vector has n elements and has an element type t ) For a vector with an"
  },
  {
    "id": 18148,
    "content": "odd number of elements, its alignment is the same as its member: alignof(t) For a vector with an even number of elements, its alignment is set to number of elements times the alignment of its member: n*alignof(t) The size of an aggregate or union, if necessary, is increased to make it a multiple of the alignment of the aggregate or union"
  },
  {
    "id": 18152,
    "content": "Bit Fields  C structure and union definitions may have bit fields that define integral objects with a specified number of bits"
  },
  {
    "id": 18153,
    "content": "Bit Field Type Width w Range signed char 1 to 8 -2 w-1 to 2 w-1 - 1 unsigned char 1 to 8 0 to 2 w - 1 signed short 1 to 16 -2 w-1 to 2 w-1 - 1 unsigned short 1 to 16 0 to 2 w - 1 signed int 1 to 32 -2 w-1 to 2 w-1 - 1 unsigned int 1 to 32 0 to 2 w - 1 signed long long 1 to 64 -2 w-1 to 2 w-1 - 1 unsigned long long 1 to 64 0 to 2 w - 1 Current GPUs only support little-endian memory, so the below"
  },
  {
    "id": 18155,
    "content": "Bit fields obey the same size and alignment rules as other structure and union members, with the following modifications"
  },
  {
    "id": 18156,
    "content": "Bit fields are allocated in memory from right to left (least to more significant) for little endian Bit fields may share a storage unit with other structure and union members, including members that are not bit fields, as long as there is enough space within the storage unit Zero-length bit fields force the alignment of the following member of a structure to the next alignment boundary"
  },
  {
    "id": 18157,
    "content": "corresponding to the bit-field type An unnamed, zero-length bit field will not force the external alignment of the structure to that boundary If an unnamed, zero-length bit field has a stricter alignment than the external alignment, there is no guarantee that the stricter alignment will be maintained when the structure or union gets allocated to memory"
  },
  {
    "id": 18158,
    "content": "Figure 1 shows the byte offsets (upper corners) and the bit numbers (lower corners) that are used in the examples"
  },
  {
    "id": 18159,
    "content": "Bit Numbering  Bit-field Allocation  Boundary Alignment  Storage Unit Sharing  Union Allocation  Unnamed Bit Fields  2"
  },
  {
    "id": 18161,
    "content": "Texture, Sampler, and Surface Types  Texture, sampler and surface types are used to define references to texture and surface memory The CUDA architecture provides hardware and instructions to efficiently read data from texture or surface memory as opposed to global memory References to textures are bound through runtime functions to device read-only regions of memory, called a texture memory,"
  },
  {
    "id": 18162,
    "content": "before they can be used by a kernel At the PTX level objects that access texture or surface memory are referred to as opaque objects Textures are expressed by either a"
  },
  {
    "id": 18170,
    "content": "The attributes of opaque objects are implemented by allocating a descriptor in memory which is populated by the driver The internal format of the descriptor varies with each architecture and should not be relied on by the user The data and the attributes of an opaque object may be accessed directly if the texture or surface reference is known at compile time or indirectly If the reference is not"
  },
  {
    "id": 18173,
    "content": "The handle can be used to pass and return oqaque object references to and from functions as well as to reference external textures, samplers and surfaces"
  },
  {
    "id": 18175,
    "content": "Function Calling Sequence  This section describes the PTX-level function calling sequence, including register usage, stack-frame layout, and parameter passing The PTX-level function calling sequence describes what gets represented in PTX to enable function calls Most of the details associated with the function calling sequence are handled at the SASS level PTX versions earlier than 2 0 do not"
  },
  {
    "id": 18176,
    "content": "conform to the ABI defined in this document, and cannot perform ABI compatible function calls For the calling convention to work PTX version 2"
  },
  {
    "id": 18180,
    "content": "Registers  At the PTX level, the registers that are specified are virtual The PTX-to-SASS translation also converts parameters and return values to physical registers or stack locations"
  },
  {
    "id": 18183,
    "content": "Stack Frame  The PTX level has no concept of the software stack Manipulation of the stack is completely defined at the SASS level, and gets allocated during the PTX-to-SASS translation process"
  },
  {
    "id": 18186,
    "content": "Parameter Passing  At the PTX level, all parameters and return values present in a device function use the parameter state space ("
  },
  {
    "id": 18188,
    "content": "The below table contains the rules for handling parameters and return values that are defined at the source level For each source-level type, the corresponding PTX-level type that should be used is provided Source Type Size in Bits PTX Type Integral types 8 to 32 (A)"
  },
  {
    "id": 18192,
    "content": "s64 (if signed) Pointers (B) 32 u32 Pointers (B) 64 u64 Floating-point types (C) 32 f32 Floating-point types (C) 64"
  },
  {
    "id": 18194,
    "content": "align align b8 name [ size ] Where align is overall aggregate-or-union alignment in bytes (D), name is variable name associated with aggregate or union, and size is the aggregate-or-union size in bytes"
  },
  {
    "id": 18199,
    "content": "surfref) NOTES: Values shorter than 32-bits are sign extended or zero extended, depending on whether they are signed or unsigned types"
  },
  {
    "id": 18200,
    "content": "Unless the memory type is specified in the function declaration, all pointers passed at the PTX level must use a generic address"
  },
  {
    "id": 18201,
    "content": "The PTX built-in opaque types such as texture, sampler, and surface types are can be passed into functions as parameters and be returned by them through 64-bit handles The handle contains the necessary information to access the actual data from the texture or surface memory as well as the attributes of the object stored in its type descriptor See section Texture, Sampler, and Surface Types for"
  },
  {
    "id": 18205,
    "content": "A prototype must be provided in the PTX file, but the implementation of the function is provided by the driver"
  },
  {
    "id": 18206,
    "content": "param t2 valist ) The following are the definitions for the vprintf parameters and return value uni ( _ ), vprintf , ( param0 , param1 ); For this code, _fmt is the format string in global memory, and _valist_array is the valist of arguments param t2 size ) The following are the definitions for the malloc parameters and return value The malloc and free system calls are emitted as part of the"
  },
  {
    "id": 18209,
    "content": "In order to support assert, the PTX function call __assertfail is used whenever the assert expression produces a false value charSize : The size in bytes of the characters contained in the __assertfail parameter strings The __assertfail system call is emitted as part of the assert macro defined in “assert"
  },
  {
    "id": 18215,
    "content": "Generation of Debug Information  The responsibility for generating debug information is split between the PTX producer and the PTX-to-SASS backend The PTX producer is responsible for emitting binary DWARF into the PTX file, using the"
  },
  {
    "id": 18234,
    "content": "CUDA-Specific DWARF Definitions  In order to support debugging of multiple memory segments, address class codes are defined to reflect the memory space of variables The address-class values are emitted as the DW_AT_address_class attribute for all variable and parameter Debugging Information Entries"
  },
  {
    "id": 18235,
    "content": "Code Value Description ADDR_code_space 1 Code storage ADDR_reg_space 2 Register storage ADDR_sreg_space 3 Special register storage ADDR_const_space 4 Constant storage ADDR_global_space 5 Global storage ADDR_local_space 6 Local storage ADDR_param_space 7 Parameter storage ADDR_shared_space 8 Shared storage ADDR_surf_space 9 Surface storage ADDR_tex_space 10 Texture storage ADDR_tex_sampler_space"
  },
  {
    "id": 18237,
    "content": "Example  The following is example PTX with debug information for implementing the following program that makes a call: __device__ __noinline__ int foo ( int i , int j ) { return i + j ; } __global__ void test ( int * p ) { * p = foo ( 1 , 2 ); } The resulting PTX would be something like:"
  },
  {
    "id": 18239,
    "content": "b8 1 , 17 , 1 , 37 , 8 , 19 , 11 , 3 , 8 , 17 , 1 , 16 , 6 , 27 , 8 , 0 , 0 , 2 , 46 , 1 , 135 b8 64 , 8 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 63 , 12 , 17 , 1 , 18 , 1 , 64 , 10 , 0 , 0 b8 3 , 5 , 0 , 3 , 8 , 58 , 6 , 59 , 6 , 73 , 19 , 2 , 10 , 51 , 11 , 0 , 0 , 4 , 36 , 0 , 3 b8 8 , 62 , 11 , 11 , 6 , 0 , 0 , 5 , 59 , 0 , 3 , 8 , 0 , 0 , 6 , 15 , 0 , 73 , 19 , 51 , 11"
  },
  {
    "id": 18240,
    "content": "Exceptions and try/catch blocks RTTI STL library Global constructors and destructors Virtual functions and classes across host and device (i"
  },
  {
    "id": 18242,
    "content": ", vtables cannot be used across host and device) There are also a few C features that are not currently supported: stdio other than printf 8"
  },
  {
    "id": 18243,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 18244,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 18246,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 18247,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 18248,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 18249,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 18250,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 18251,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 18252,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 18253,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 18254,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 18255,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 18256,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 18263,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 18265,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 18266,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 18271,
    "content": "Using Inline PTX Assembly in CUDA v12 5 | PDF | Archive Inline PTX Assembly in CUDA The reference guide for inlining PTX (parallel thread execution) assembly statements into CUDA Using Inline PTX Assembly in CUDA  The NVIDIA ® CUDA ® programming environment provides a parallel thread execution (PTX) instruction set architecture (ISA) for using the GPU as a data-parallel computing device For more"
  },
  {
    "id": 18272,
    "content": "information on the PTX ISA, refer to the latest version of the PTX ISA reference document This application note describes how to inline PTX assembly language statements into CUDA code"
  },
  {
    "id": 18275,
    "content": "Assembler (ASM) Statements  Assembler statements, asm() , provide a way to insert arbitrary PTX code into your CUDA program A simple example is: asm ( \"membar gl;\" ); This inserts a PTX membar gl into your generated PTX code at the point of the asm() statement"
  },
  {
    "id": 18279,
    "content": "Parameters  An asm() statement becomes more complicated, and more useful, when we pass values in and out of the asm"
  },
  {
    "id": 18280,
    "content": "The basic syntax is as follows: asm ( \"template-string\" : \"constraint\" ( output ) : \"constraint\" ( input )); where you can have multiple input or output operands separated by commas A simple example is as follows: asm ( \"add s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); Each %n in the template string is an index into the following list of operands, in text order Since the output operands"
  },
  {
    "id": 18281,
    "content": "are always listed ahead of the input operands, they are assigned the smallest indices s32 i , j , k ; Note that the numbered references in the string can be in arbitrary order The following is equivalent to the above example: asm ( \"add s32 %0, %2, %1;\" : \"=r\" ( i ) : \"r\" ( k ), \"r\" ( j )); You can also repeat a reference, e"
  },
  {
    "id": 18287,
    "content": "s32 r1, %0;\" :: \"r\" ( i )); If you want the % in a ptx instruction, then you should escape it with double %% , e"
  },
  {
    "id": 18290,
    "content": "u32 %0, %%clock;\" : \"=r\" ( x )); The above was simplified to explain the ordering of the string % references"
  },
  {
    "id": 18291,
    "content": "In reality, the operand values are passed via whatever mechanism the constraint specifies The full list of constraints will be explained later, but the “r” constraint refers to a 32bit integer register"
  },
  {
    "id": 18293,
    "content": "s32 %0, %1, %2;\" : \"=r\" ( i ) : \"r\" ( j ), \"r\" ( k )); produces the following code sequence in the output generated by the compiler: ld s32 [ i ], r3 ; This is where the distinction between input and output operands becomes important The input operands are loaded into registers before the asm() statement, then the result register is stored to the output operand"
  },
  {
    "id": 18296,
    "content": ": asm ( \"add s32 %0, %0, %1;\" : \"+r\" ( i ) : \"r\" ( j )); Multiple instructions can be combined into a single asm() statement; basically, anything legal can be put into the asm string Multiple instructions can be split across multiple lines by making use of C/C++’s implicit string concatenation"
  },
  {
    "id": 18297,
    "content": "Both C++ style line end comments “ ” and classical C-style comments “/**/” can be interspersed with these strings"
  },
  {
    "id": 18298,
    "content": "To generate readable output in the PTX intermediate file it is best practice to terminate each instruction string except the last one with “nt”"
  },
  {
    "id": 18304,
    "content": "u32 %0, t1, %1;\"   y = t1 * x : \"=r\" ( y ) : \"r\" ( x )); return y ; } If an output operand is conditionally updated by the asm instructions, then the “+” modifier should be used"
  },
  {
    "id": 18315,
    "content": "f32 [ x ], f1 ; The constraint \"n\" may be used for immediate integer operands with a known value u32 r1 , r1 , 42 ; The constraint \"C\" can be used for operand of type ‘array of const char’, where the array contents are known at compile time"
  },
  {
    "id": 18316,
    "content": "It is intended to allow customization of PTX instruction modes based on compile time computation (see examples)"
  },
  {
    "id": 18317,
    "content": "Here is the specification for the \"C\" constraint: 'C' ( constant - expression ) The constant-expression is evaluated during compilation and shall generate the address of a variable V , where: V has static storage duration"
  },
  {
    "id": 18318,
    "content": "If V is a static class member, then V ’s initializing declaration is the declaration within the class"
  },
  {
    "id": 18319,
    "content": "During translation, the compiler will replace a reference to the operand within the Assembler Template with the contents of V ’s initializer, except for the last trailing zero"
  },
  {
    "id": 18321,
    "content": "PTX instructions types accepting 8-bit wide types permit operands to be wider than the instruction-type size"
  },
  {
    "id": 18322,
    "content": "Example: __device__ void copy_u8 ( char * in , char * out ) { int d ; asm ( \"ld u8 %0, [%1];\" : \"=r\" ( d ) : \"l\" ( in )); * out = d ; } generates: ld"
  },
  {
    "id": 18323,
    "content": "u8 [ rd2 ], r1 ; The behavior of using a constraint string that is not one of those specified above is undefined"
  },
  {
    "id": 18326,
    "content": "Pitfalls  Although asm() statements are very flexible and powerful, you may encounter some pitfalls—these are listed in this section"
  },
  {
    "id": 18330,
    "content": "Namespace Conflicts  If the cube function (described before) is called and inlined multiple times in the code, it generates an error about duplicate definitions of the temp register t1 To avoid this error you need to: not inline the cube function, or, nest the t1 use inside {} so that it has a separate scope for each invocation, e"
  },
  {
    "id": 18337,
    "content": "u32 %0, t1, %1;  \\t \"   y = t1 * x \"}\" : \"=r\" ( y ) : \"r\" ( x )); return y ; } Note that you can similarly use braces for local labels inside the asm() statement"
  },
  {
    "id": 18341,
    "content": "Memory Space Conflicts  Since asm() statements have no way of knowing what memory space a register is in, the user must make sure that the appropriate PTX instruction is used"
  },
  {
    "id": 18346,
    "content": "Incorrect Optimization  The compiler assumes that an asm() statement has no side effects except to change the output operands"
  },
  {
    "id": 18347,
    "content": "To ensure that the asm is not deleted or moved during generation of PTX, you should use the volatile keyword, e"
  },
  {
    "id": 18350,
    "content": "u32 %0, %%clock;\" : \"=r\" ( x )); Normally any memory that is written to will be specified as an out operand, but if there is a hidden side effect on user memory (for example, indirect access of a memory location via an operand), or if you want to stop any memory optimizations around the asm() statement performed during generation of PTX, you can add a “memory” clobbers specification after a 3rd"
  },
  {
    "id": 18353,
    "content": ": asm volatile ( \"mov u32 %0, %%clock;\" : \"=r\" ( x ) :: \"memory\" ); asm ( \"st u32 [%0], %1;\" :: \"l\" ( p ), \"r\" ( x ) : \"memory\" ); 1"
  },
  {
    "id": 18356,
    "content": "Incorrect PTX  The compiler front end does not parse the asm() statement template string and does not know what it means or even whether it is valid PTX input"
  },
  {
    "id": 18360,
    "content": "u32 %0, %n1;\" : \"=r\" ( n ) : \"r\" ( 1 )); the ‘n’ modifier in “%n1” is not supported and will be passed to ptxas , where it can cause undefined behavior"
  },
  {
    "id": 18365,
    "content": "Error Checking  The following are some of the error checks that the compiler will do on inlinePTXasm: Multiple constraint letters for a single asm operand are not allowed, e"
  },
  {
    "id": 18367,
    "content": ": asm ( \"add s32 %0, %1, %2;\" : \"=r\" ( i ) : \"rf\" ( j ), \"r\" ( k )); error: an asm operand may specify only one constraint letter in a __device__/__global__ function Only scalar variables are allowed as asm operands int4 i4 ; asm ( \"add s32 %0, %1, %2;\" : \"=r\" ( i4 ) : \"r\" ( j ), \"r\" ( k )); error: an asm operand must have scalar type The type and size implied by a PTX asm constraint must match"
  },
  {
    "id": 18368,
    "content": "that of the associated operand Example where size does not match: For ‘char’ type variable “ci”, asm ( \"add s32 %0,%1,%2;\" : \"=r\" ( ci ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(1) does not match type/size implied by constraint ‘r’ In order to use ‘char’ type variables “ci”, “cj”, and “ck” in the above asm statement, code segment similar to the following may be used, int temp = ci ;"
  },
  {
    "id": 18369,
    "content": "asm ( \"add s32 %0,%1,%2;\" : \"=r\" ( temp ) : \"r\" (( int ) cj ), \"r\" (( int ) ck )); ci = temp ; Another example where type does not match: For ‘float’ type variable “fi”, asm ( \"add s32 %0,%1,%2;\" : \"=r\" ( fi ) : \"r\" ( j ), \"r\" ( k )); error: asm operand type size(4) does not match type/size implied by constraint ‘r’ 2"
  },
  {
    "id": 18370,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 18371,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 18373,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 18374,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 18375,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 18376,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 18377,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 18378,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 18379,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 18380,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 18381,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 18382,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 18383,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 18390,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 18392,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 18393,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 18397,
    "content": "pageBottom();}\nNVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12"
  },
  {
    "id": 18402,
    "content": "Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight"
  },
  {
    "id": 18403,
    "content": "options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite"
  },
  {
    "id": 18404,
    "content": "pageBottom();\nNVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12"
  },
  {
    "id": 18409,
    "content": "Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight"
  },
  {
    "id": 18410,
    "content": "options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite"
  },
  {
    "id": 18412,
    "content": "Notices CUDA Math API Reference Manual » CUDA Math API Reference Manual v12 5 | PDF | Archive CUDA Math API Reference Manual  CUDA mathematical functions are always available in device code Host implementations of the common mathematical functions are mapped in a platform-specific way to standard math library functions, provided by the host compiler and respective host libm where available Some"
  },
  {
    "id": 18416,
    "content": "Note that many floating-point and integer functions names are overloaded for different argument types"
  },
  {
    "id": 18417,
    "content": "For example, the log() function has the following prototypes: double log ( double x ); float log ( float x ); float logf ( float x ); Note also that due to implementation constraints, certain math functions from std:: namespace may be callable in device code even via explicitly qualified std:: names"
  },
  {
    "id": 18418,
    "content": "However, such use is discouraged, since this capability is unsupported, unverified, undocumented, not portable, and may change without notice"
  },
  {
    "id": 18419,
    "content": "Notices Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 18421,
    "content": "5 | PDF | Archive cuBLAS The API Reference guide for cuBLAS, the CUDA Basic Linear Algebra Subroutine library Introduction  The cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA®CUDA™ runtime"
  },
  {
    "id": 18423,
    "content": "The cuBLAS Library exposes four sets of APIs: The cuBLAS API , which is simply called cuBLAS API in this document (starting with CUDA 6 0), The cuBLASXt API (starting with CUDA 6 0), and The cuBLASLt API (starting with CUDA 10 1) The cuBLASDx API (not shipped with the CUDA Toolkit) To use the cuBLAS API, the application must allocate the required matrices and vectors in the GPU memory space, fill"
  },
  {
    "id": 18424,
    "content": "them with data, call the sequence of desired cuBLAS functions, and then upload the results from the GPU memory space back to the host The cuBLAS API also provides helper functions for writing and retrieving data from the GPU To use the cuBLASXt API, the application may have the data on the Host or any of the devices involved in the computation, and the Library will take care of dispatching the"
  },
  {
    "id": 18425,
    "content": "operation to, and transferring the data to, one or multiple GPUs present in the system, depending on the user request The cuBLASLt is a lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API"
  },
  {
    "id": 18426,
    "content": "This library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability"
  },
  {
    "id": 18427,
    "content": "After a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs"
  },
  {
    "id": 18428,
    "content": "This is analogous to how cuFFT and FFTW first create a plan and reuse for same size and type FFTs with different input data"
  },
  {
    "id": 18431,
    "content": "Data Layout  For maximum compatibility with existing Fortran environments, the cuBLAS library uses column-major storage, and 1-based indexing"
  },
  {
    "id": 18432,
    "content": "Since C and C++ use row-major storage, applications written in these languages can not use the native array semantics for two-dimensional arrays"
  },
  {
    "id": 18433,
    "content": "Instead, macros or inline functions should be defined to implement matrices on top of one-dimensional arrays"
  },
  {
    "id": 18434,
    "content": "For Fortran code ported to C in mechanical fashion, one may chose to retain 1-based indexing to avoid the need to transform loops"
  },
  {
    "id": 18435,
    "content": "In this case, the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2F(i,j,ld) (j)-1)*(ld))+((i)-1)) Here, ld refers to the leading dimension of the matrix, which in the case of column-major storage is the number of rows of the allocated matrix (even if only a submatrix of it is being used) For natively written C and C++ code, one would"
  },
  {
    "id": 18436,
    "content": "most likely choose 0-based indexing, in which case the array index of a matrix element in row “i” and column “j” can be computed via the following macro #define IDX2C(i,j,ld) (((j)*(ld))+(i)) 1"
  },
  {
    "id": 18438,
    "content": "New and Legacy cuBLAS API  Starting with version 4 0, the cuBLAS Library provides a new API, in addition to the existing legacy API This section discusses why a new API is provided, the advantages of using it, and the differences with the existing legacy API It has the following features that the legacy cuBLAS API does not have: The handle to the cuBLAS library context is initialized using the"
  },
  {
    "id": 18439,
    "content": "function and is explicitly passed to every subsequent library function call This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs The scalars \\(\\alpha\\) and \\(\\beta\\) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host This change allows library functions to execute"
  },
  {
    "id": 18440,
    "content": "asynchronously using streams even when \\(\\alpha\\) and \\(\\beta\\) are generated by a previous kernel When a library routine returns a scalar result, it can be returned by reference on the host or the device, instead of only being allowed to be returned by value only on the host This change allows library routines to be called asynchronously when the scalar result is generated and returned by"
  },
  {
    "id": 18441,
    "content": "reference on the device resulting in maximum parallelism Note that cublasStatus was renamed cublasStatus_t to be more consistent with other types in the cuBLAS library"
  },
  {
    "id": 18443,
    "content": "The function cublasSetKernelStream() was renamed cublasSetStream() to be more consistent with the other CUDA libraries"
  },
  {
    "id": 18444,
    "content": "The legacy cuBLAS API, explained in more detail in Using the cuBLAS Legacy API , can be used by including the header file cublas"
  },
  {
    "id": 18446,
    "content": "Since the legacy API is identical to the previously released cuBLAS library API, existing applications will work out of the box and automatically use this legacy API without any source code changes The current and the legacy cuBLAS APIs cannot be used simultaneously in a single translation unit: including both cublas"
  },
  {
    "id": 18449,
    "content": "In general, new applications should not use the legacy cuBLAS API, and existing applications should convert to using the new API if it requires sophisticated and optimal stream parallelism, or if it calls cuBLAS routines concurrently from multiple threads For the rest of the document, the new cuBLAS Library API will simply be referred to as the cuBLAS Library API As mentioned earlier the"
  },
  {
    "id": 18453,
    "content": "In addition, applications using the cuBLAS library need to link against: The DSO cublas so for Linux, The DLL cublas dll for Windows, or The dynamic library cublas"
  },
  {
    "id": 18459,
    "content": "They show an application written in C using the cuBLAS library API with two indexing styles (Example 1"
  },
  {
    "id": 18460,
    "content": "In that case, the results are not guaranteed to be bit-wise reproducible because atomics are used for the computation"
  },
  {
    "id": 18464,
    "content": "Scalar Parameters  There are two categories of the functions that use scalar parameters : Functions that take alpha and/or beta parameters by reference on the host or the device as scaling factors, such as gemm Functions that return a scalar result on the host or the device such as amax() , amin , asum() , rotg() , rotmg() , dot() and nrm2() For the functions of the first category, when the"
  },
  {
    "id": 18465,
    "content": "pointer mode is set to CUBLAS_POINTER_MODE_HOST , the scalar parameters alpha and/or beta can be on the stack or allocated on the heap, shouldn’t be placed in managed memory Underneath, the CUDA kernels related to those functions will be launched with the value of alpha and/or beta Therefore if they were allocated on the heap, they can be freed just after the return of the call even though the"
  },
  {
    "id": 18466,
    "content": "kernel launch is asynchronous When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , alpha and/or beta must be accessible on the device and their values should not be modified until the kernel is done Note that since cudaFree() does an implicit cudaDeviceSynchronize() , cudaFree() can still be called on alpha and/or beta just after the call but it would defeat the purpose of using this"
  },
  {
    "id": 18467,
    "content": "pointer mode in that case For the functions of the second category, when the pointer mode is set to CUBLAS_POINTER_MODE_HOST , these functions block the CPU, until the GPU has completed its computation and the results have been copied back to the Host When the pointer mode is set to CUBLAS_POINTER_MODE_DEVICE , these functions return immediately In this case, similar to matrix and vector results,"
  },
  {
    "id": 18468,
    "content": "the scalar result is ready only when execution of the routine on the GPU has completed In either case, the pointer mode CUBLAS_POINTER_MODE_DEVICE allows the library functions to execute completely asynchronously from the Host even when alpha and/or beta are generated by a previous kernel"
  },
  {
    "id": 18469,
    "content": "For example, this situation can arise when iterative methods for solution of linear systems and eigenvalue problems are implemented using the cuBLAS library"
  },
  {
    "id": 18473,
    "content": "Parallelism with Streams  If the application uses the results computed by multiple independent tasks, CUDA™ streams can be used to overlap the computation performed in these tasks In order to achieve the overlap of computation between the tasks, the user should create CUDA™ streams using the function cudaStreamCreate() and set the stream to be used by each individual cuBLAS library routine by"
  },
  {
    "id": 18474,
    "content": "calling cublasSetStream() just before calling the actual cuBLAS routine Note that cublasSetStream() resets the user-provided workspace to the default workspace pool; see cublasSetWorkspace() Then, the computation performed in separate streams would be overlapped automatically when possible on the GPU This approach is especially useful when the computation performed by a single task is relatively"
  },
  {
    "id": 18475,
    "content": "small and is not enough to fill the GPU with work We recommend using the new cuBLAS API with scalar parameters and results passed by reference in the device memory to achieve maximum overlap of the computation when using streams A particular application of streams, batching of multiple small kernels, is described in the following section"
  },
  {
    "id": 18479,
    "content": "Batching Kernels  In this section, we explain how to use streams to batch the execution of small kernels For instance, suppose that we have an application where we need to make many small independent matrix-matrix multiplications with dense matrices It is clear that even with millions of small independent matrices we will not be able to achieve the same GFLOPS rate as with a one large matrix For"
  },
  {
    "id": 18480,
    "content": "example, a single \\(n \\times n\\) large matrix-matrix multiplication performs \\(n^{3}\\) operations for \\(n^{2}\\) input size, while 1024 \\(\\frac{n}{32} \\times \\frac{n}{32}\\) small matrix-matrix multiplications perform \\(1024\\left( \\frac{n}{32} ight)^{3} = \\frac{n^{3}}{32}\\) operations for the same input size However, it is also clear that we can achieve a significantly better performance with many"
  },
  {
    "id": 18481,
    "content": "small independent matrices compared with a single small matrix Hence, in order to batch the execution of independent kernels, we can run each of them in a separate stream In particular, in the above example we could create 1024 CUDA™ streams using the function cudaStreamCreate() , then preface each call to cublasgemm() with a call to cublasSetStream() with a different stream for each of the"
  },
  {
    "id": 18482,
    "content": "matrix-matrix multiplications (note that cublasSetStream() resets user-provided workspace to the default workspace pool, see cublasSetWorkspace() )"
  },
  {
    "id": 18484,
    "content": "Although the user can create many streams, in practice it is not possible to have more than 32 concurrent kernels executing at the same time"
  },
  {
    "id": 18488,
    "content": "Cache Configuration  On some devices, L1 cache and shared memory use the same hardware resources The cache configuration can be set directly with the CUDA Runtime function cudaDeviceSetCacheConfig The cache configuration can also be set specifically for some functions using the routine cudaFuncSetCacheConfig Please refer to the CUDA Runtime API documentation for details about the cache"
  },
  {
    "id": 18489,
    "content": "configuration settings Because switching from one configuration to another can affect kernels concurrency, the cuBLAS Library does not set any cache configuration preference and relies on the current setting However, some cuBLAS routines, especially Level-3 routines, rely heavily on shared memory Thus the cache preference setting might affect adversely their performance"
  },
  {
    "id": 18495,
    "content": "The static cuBLAS library and all other static math libraries depend on a common thread abstraction layer library called libculibos"
  },
  {
    "id": 18497,
    "content": "For example, on Linux, to compile a small application using cuBLAS, against the dynamic library, the following command can be used: nvcc myCublasApp c - lcublas - o myCublasApp Whereas to compile against the static cuBLAS library, the following command must be used: nvcc myCublasApp c - lcublas_static - lculibos - o myCublasApp It is also possible to use the native Host C++ compiler"
  },
  {
    "id": 18498,
    "content": "Depending on the Host operating system, some additional libraries like pthread or dl might be needed on the linking line"
  },
  {
    "id": 18499,
    "content": "c - lcublas_static - lculibos - lcudart_static - lpthread - ldl - I / include - L / lib64 - o myCublasApp Note that in the latter case, the library cuda is not needed"
  },
  {
    "id": 18500,
    "content": "In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available"
  },
  {
    "id": 18502,
    "content": "2, using the typed functions instead of the extension functions (cublas**Ex()) helps in reducing the binary size when linking to static cuBLAS Library"
  },
  {
    "id": 18506,
    "content": "GEMM Algorithms Numerical Behavior  Some GEMM algorithms split the computation along the dimension K to increase the GPU occupancy, especially when the dimension K is large compared to dimensions M and N When this type of algorithm is chosen by the cuBLAS heuristics or explicitly by the user, the results of each split is summed deterministically into the resulting matrix to get the final result"
  },
  {
    "id": 18507,
    "content": "For the routines cublasgemmEx and cublasGemmEx() , when the compute type is greater than the output type, the sum of the split chunks can potentially lead to some intermediate overflows thus producing a final resulting matrix with some overflows Those overflows might not have occurred if all the dot products had been accumulated in the compute type before being converted at the end in the output"
  },
  {
    "id": 18509,
    "content": "This computation side-effect can be easily exposed when the computeType is CUDA_R_32F and Atype, Btype and Ctype are in CUDA_R_16F"
  },
  {
    "id": 18510,
    "content": "This behavior can be controlled using the compute precision mode CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION with cublasSetMathMode() 2"
  },
  {
    "id": 18517,
    "content": "0, the library may automatically make use of Tensor Core capabilities wherever possible, unless they are explicitly disabled by selecting pedantic compute modes in cuBLAS (see cublasSetMathMode() , cublasMath_t ) It should be noted that the library will pick a Tensor Core enabled implementation wherever it determines that it would provide the best performance The best performance when using"
  },
  {
    "id": 18518,
    "content": "Tensor Cores can be achieved when the matrix dimensions and pointers meet certain memory alignment requirements Specifically, all of the following conditions must be satisfied to get the most performance out of Tensor Cores: ((op_A == CUBLAS_OP_N"
  },
  {
    "id": 18519,
    "content": "m : k) * AtypeSize) % 16 == 0 ((op_B == CUBLAS_OP_N k : n) * BtypeSize) % 16 == 0 (m * CtypeSize) % 16 == 0 (lda * AtypeSize) % 16 == 0 (ldb * BtypeSize) % 16 == 0 (ldc * CtypeSize) % 16 == 0 intptr_t(A) % 16 == 0 intptr_t(B) % 16 == 0 intptr_t(C) % 16 == 0 To conduct matrix multiplication with FP8 types (see 8-bit Floating Point Data Types (FP8) Usage ), you must ensure that your matrix"
  },
  {
    "id": 18521,
    "content": "Aside from FP8, there are no longer any restrictions on matrix dimensions and memory alignments to use Tensor Cores (starting with cuBLAS version 11"
  },
  {
    "id": 18527,
    "content": "CUDA Graphs Support  cuBLAS routines can be captured in CUDA Graph stream capture without restrictions in most situations"
  },
  {
    "id": 18528,
    "content": "cublasdot while pointer mode CUBLAS_POINTER_MODE_HOST is configured), as it enforces synchronization"
  },
  {
    "id": 18529,
    "content": "For input coefficients (such as alpha , beta ) behavior depends on the pointer mode setting: In the case of CUBLAS(LT)_POINTER_MODE_HOST , coefficient values are captured in the graph In the case of pointer modes with device pointers, coefficient value is accessed using the device pointer at the time of graph execution Note When captured in CUDA Graph stream capture, cuBLAS routines can create"
  },
  {
    "id": 18530,
    "content": "memory nodes through the use of stream-ordered allocation APIs, cudaMallocAsync and cudaFreeAsync However, as there is currently no support for memory nodes in child graphs or graphs launched from the device , attempts to capture cuBLAS routines in such scenarios may fail"
  },
  {
    "id": 18535,
    "content": "64-bit Integer Interface  cuBLAS version 12 introduced 64-bit integer capable functions Each 64-bit integer function is equivalent to a 32-bit integer function with the following changes: The function name has _64 suffix"
  },
  {
    "id": 18537,
    "content": "For documentation brevity, the 64-bit integer APIs are not explicitly listed, but only mentioned that they exist for the relevant functions"
  },
  {
    "id": 18543,
    "content": "cublasHandle_t  The cublasHandle_t type is a pointer type to an opaque structure holding the cuBLAS library context The cuBLAS library context must be initialized using cublasCreate() and the returned handle must be passed to all subsequent library function calls The context should be destroyed at the end using cublasDestroy()"
  },
  {
    "id": 18548,
    "content": "This is usually caused by the lack of a prior cublasCreate() call, an error in the CUDA Runtime API called by the cuBLAS routine, or an error in the hardware setup To correct: call cublasCreate() before the function call; and check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed To correct: prior to the function call, deallocate previously"
  },
  {
    "id": 18550,
    "content": "CUBLAS_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example)"
  },
  {
    "id": 18551,
    "content": "CUBLAS_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture; usually caused by compute capability lower than 5"
  },
  {
    "id": 18554,
    "content": "CUBLAS_STATUS_MAPPING_ERROR An access to GPU memory space failed, which is usually caused by a failure to bind a texture This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons"
  },
  {
    "id": 18555,
    "content": "To correct: check that the hardware, an appropriate version of the driver, and the cuBLAS library are correctly installed Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine’s completion CUBLAS_STATUS_LICENSE_ERROR The functionality requested requires some license and an error was detected when trying to check the current licensing"
  },
  {
    "id": 18556,
    "content": "This error can happen if the license is not present or is expired or if the environment variable NVIDIA_LICENSE_FILE is not set properly"
  },
  {
    "id": 18560,
    "content": "cublasOperation_t  The cublasOperation_t type indicates which operation needs to be performed with the dense matrix"
  },
  {
    "id": 18561,
    "content": "Its values correspond to Fortran characters ‘N’ or ‘n’ (non-transpose), ‘T’ or ‘t’ (transpose) and ‘C’ or ‘c’ (conjugate transpose) that are often used as parameters to legacy BLAS implementations CUBLAS_OP_C The conjugate transpose operation is selected"
  },
  {
    "id": 18565,
    "content": "cublasFillMode_t  The type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function Its values correspond to Fortran characters L or l (lower) and U or u (upper) that are often used as parameters to legacy BLAS implementations CUBLAS_FILL_MODE_FULL The full matrix is filled"
  },
  {
    "id": 18569,
    "content": "cublasDiagType_t  The type indicates whether the main diagonal of the dense matrix is unity and consequently should not be touched or modified by the function"
  },
  {
    "id": 18570,
    "content": "Its values correspond to Fortran characters ‘N’ or ‘n’ (non-unit) and ‘U’ or ‘u’ (unit) that are often used as parameters to legacy BLAS implementations CUBLAS_DIAG_UNIT The matrix diagonal has unit elements"
  },
  {
    "id": 18574,
    "content": "cublasSideMode_t  The type indicates whether the dense matrix is on the left or right side in the matrix equation solved by a particular function"
  },
  {
    "id": 18575,
    "content": "Its values correspond to Fortran characters ‘L’ or ‘l’ (left) and ‘R’ or ‘r’ (right) that are often used as parameters to legacy BLAS implementations"
  },
  {
    "id": 18580,
    "content": "cublasPointerMode_t  The cublasPointerMode_t type indicates whether the scalar values are passed by reference on the host or device"
  },
  {
    "id": 18581,
    "content": "It is important to point out that if several scalar values are present in the function call, all of them must conform to the same single pointer mode"
  },
  {
    "id": 18582,
    "content": "The pointer mode can be set and retrieved using cublasSetPointerMode() and cublasGetPointerMode() routines, respectively"
  },
  {
    "id": 18587,
    "content": "cublasAtomicsMode_t  The type indicates whether cuBLAS routines which has an alternate implementation using atomics can be used The atomics mode can be set and queried using cublasSetAtomicsMode() and cublasGetAtomicsMode() and routines, respectively CUBLAS_ATOMICS_ALLOWED The usage of atomics is allowed"
  },
  {
    "id": 18591,
    "content": "cublasGemmAlgo_t  cublasGemmAlgo_t type is an enumerant to specify the algorithm for matrix-matrix multiplication on GPU architectures up to sm_75 cuBLAS has the following algorithm options: Value Meaning CUBLAS_GEMM_DEFAULT Apply Heuristics to select the GEMM algorithm CUBLAS_GEMM_ALGO0 to CUBLAS_GEMM_ALGO23 Explicitly choose an Algorithm [0,23]"
  },
  {
    "id": 18592,
    "content": "CUBLAS_GEMM_DEFAULT_TENSOR_OP [DEPRECATED] This mode is deprecated and will be removed in a future release"
  },
  {
    "id": 18593,
    "content": "Apply Heuristics to select the GEMM algorithm, while allowing use of reduced precision CUBLAS_COMPUTE_32F_FAST_16F kernels (for backward compatibility)"
  },
  {
    "id": 18594,
    "content": "CUBLAS_GEMM_ALGO0_TENSOR_OP to CUBLAS_GEMM_ALGO15_TENSOR_OP [DEPRECATED] Those values are deprecated and will be removed in a future release"
  },
  {
    "id": 18599,
    "content": "cublasMath_t  cublasMath_t enumerate type is used in cublasSetMathMode() to choose compute precision modes as defined in the following table"
  },
  {
    "id": 18600,
    "content": "Since this setting does not directly control the use of Tensor Cores, the mode CUBLAS_TENSOR_OP_MATH is being deprecated, and will be removed in a future release"
  },
  {
    "id": 18601,
    "content": "Value Meaning CUBLAS_DEFAULT_MATH This is the default and highest-performance mode that uses compute and intermediate storage precisions with at least the same number of mantissa and exponent bits as requested"
  },
  {
    "id": 18602,
    "content": "CUBLAS_PEDANTIC_MATH This mode uses the prescribed precision and standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness studies, testing, and debugging"
  },
  {
    "id": 18604,
    "content": "CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION Forces any reductions during matrix multiplications to use the accumulator type (that is, compute type) and not the output type in case of mixed precision routines where output type precision is less than the compute type precision"
  },
  {
    "id": 18611,
    "content": "cublasComputeType_t  cublasComputeType_t enumerate type is used in cublasGemmEx() and cublasLtMatmul() (including all batched and strided batched variants) to choose compute precision modes as defined below"
  },
  {
    "id": 18612,
    "content": "Value Meaning CUBLAS_COMPUTE_16F This is the default and highest-performance mode for 16-bit half precision floating point and all compute and intermediate storage precisions with at least 16-bit half precision CUBLAS_COMPUTE_16F_PEDANTIC This mode uses 16-bit half precision floating point standardized arithmetic for all phases of calculations and is primarily intended for numerical robustness"
  },
  {
    "id": 18615,
    "content": "CUBLAS_COMPUTE_32F This is the default 32-bit single precision floating point and uses compute and intermediate storage precisions of at least 32-bits CUBLAS_COMPUTE_32F_PEDANTIC Uses 32-bit single precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M) CUBLAS_COMPUTE_32F_FAST_16F Allows the library"
  },
  {
    "id": 18616,
    "content": "to use Tensor Cores with automatic down-conversion and 16-bit half-precision compute for 32-bit input and output matrices CUBLAS_COMPUTE_32F_FAST_16BF Allows the library to use Tensor Cores with automatic down-convesion and bfloat16 compute for 32-bit input and output matrices CUBLAS_COMPUTE_32F_FAST_TF32 Allows the library to use Tensor Cores with TF32 compute for 32-bit input and output matrices"
  },
  {
    "id": 18617,
    "content": "CUBLAS_COMPUTE_64F This is the default 64-bit double precision floating point and uses compute and intermediate storage precisions of at least 64-bits CUBLAS_COMPUTE_64F_PEDANTIC Uses 64-bit double precision floatin point arithmetic for all phases of calculations and also disables algorithmic optimizations such as Gaussian complexity reduction (3M) CUBLAS_COMPUTE_32I This is the default 32-bit"
  },
  {
    "id": 18618,
    "content": "integer mode and uses compute and intermediate storage precisions of at least 32-bits CUBLAS_COMPUTE_32I_PEDANTIC Uses 32-bit integer arithmetic for all phases of calculations Note Setting the environment variable NVIDIA_TF32_OVERRIDE = 0 will override any defaults or programmatic configuration of NVIDIA libraries, and consequently, cuBLAS will not accelerate FP32 computations with TF32 tensor"
  },
  {
    "id": 18622,
    "content": "CUDA Datatypes Reference  The chapter describes types shared by multiple CUDA Libraries and defined in the header file library_types"
  },
  {
    "id": 18628,
    "content": "It is used when the data reference does not carry the type itself (e g void *) For example, it is used in the routine cublasSgemmEx() Value Meaning CUDA_R_16F the data type is a 16-bit real half precision floating-point CUDA_C_16F the data type is a 32-bit structure comprised of two half precision floating-points representing a complex number CUDA_R_16BF the data type is a 16-bit real bfloat16"
  },
  {
    "id": 18629,
    "content": "floating-point CUDA_C_16BF the data type is a 32-bit structure comprised of two bfloat16 floating-points representing a complex number CUDA_R_32F the data type is a 32-bit real single precision floating-point CUDA_C_32F the data type is a 64-bit structure comprised of two single precision floating-points representing a complex number CUDA_R_64F the data type is a 64-bit real double precision"
  },
  {
    "id": 18630,
    "content": "floating-point CUDA_C_64F the data type is a 128-bit structure comprised of two double precision floating-points representing a complex number CUDA_R_8I the data type is a 8-bit real signed integer CUDA_C_8I the data type is a 16-bit structure comprised of two 8-bit signed integers representing a complex number CUDA_R_8U the data type is a 8-bit real unsigned integer CUDA_C_8U the data type is a"
  },
  {
    "id": 18631,
    "content": "16-bit structure comprised of two 8-bit unsigned integers representing a complex number CUDA_R_32I the data type is a 32-bit real signed integer CUDA_C_32I the data type is a 64-bit structure comprised of two 32-bit signed integers representing a complex number CUDA_R_8F_E4M3 the data type is an 8-bit real floating point in E4M3 format CUDA_R_8F_E5M2 the data type is an 8-bit real floating point"
  },
  {
    "id": 18635,
    "content": "libraryPropertyType_t  The libraryPropertyType_t is used as a parameter to specify which property is requested when using the routine cublasGetProperty() Value Meaning MAJOR_VERSION enumerant to query the major version MINOR_VERSION enumerant to query the minor version PATCH_LEVEL number to identify the patch level 2"
  },
  {
    "id": 18637,
    "content": "cublasCreate()  cublasStatus_t cublasCreate ( cublasHandle_t * handle ) This function initializes the cuBLAS library and creates a handle to an opaque structure holding the cuBLAS library context It allocates hardware resources on the host and device and must be called prior to making any other cuBLAS library calls To use the library on multiple devices, one cuBLAS handle needs to be created for"
  },
  {
    "id": 18638,
    "content": "each device Furthermore, for a given device, multiple cuBLAS handles with different configurations can be created Because cublasCreate() allocates some internal resources and the release of those resources by calling cublasDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are called For multi-threaded applications that use"
  },
  {
    "id": 18639,
    "content": "the same device from different threads, the recommended programming model is to create one cuBLAS handle per thread and use that cuBLAS handle for the entire life of the thread"
  },
  {
    "id": 18640,
    "content": "Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded CUBLAS_STATUS_NOT_INITIALIZED the CUDA™ Runtime initialization failed CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_INVALID_VALUE handle == NULL 2"
  },
  {
    "id": 18643,
    "content": "cublasDestroy()  cublasStatus_t cublasDestroy ( cublasHandle_t handle ) This function releases hardware resources used by the cuBLAS library"
  },
  {
    "id": 18644,
    "content": "Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2"
  },
  {
    "id": 18647,
    "content": "cublasGetVersion()  cublasStatus_t cublasGetVersion ( cublasHandle_t handle , int * version ) This function returns the version number of the cuBLAS library Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the provided storage for library version number is not initialized (NULL) Note This function can be safely called with the handle set"
  },
  {
    "id": 18653,
    "content": "cublasGetProperty()  cublasStatus_t cublasGetProperty ( libraryPropertyType type , int * value ) This function returns the value of the requested property in memory pointed to by value Return Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully CUBLAS_STATUS_INVALID_VALUE Invalid type value If invalid type value or value == NULL 2"
  },
  {
    "id": 18656,
    "content": "cublasGetStatusName()  const char * cublasGetStatusName ( cublasStatus_t status ) This function returns the string representation of a given status Return Value Meaning NULL-terminated string The string representation of the status 2"
  },
  {
    "id": 18659,
    "content": "cublasGetStatusString()  const char * cublasGetStatusString ( cublasStatus_t status ) This function returns the description string for a given status"
  },
  {
    "id": 18660,
    "content": "cublasSetStream()  cublasStatus_t cublasSetStream ( cublasHandle_t handle , cudaStream_t streamId ) This function sets the cuBLAS library stream, which will be used to execute all subsequent calls to the cuBLAS library functions In particular, this routine can be used to change the stream between kernel launches and then to reset the cuBLAS library stream back to NULL Additionally this function"
  },
  {
    "id": 18661,
    "content": "unconditionally resets the cuBLAS library workspace back to the default workspace pool (see cublasSetWorkspace() ) Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2"
  },
  {
    "id": 18664,
    "content": "cublasSetWorkspace()  cublasStatus_t cublasSetWorkspace ( cublasHandle_t handle , void * workspace , size_t workspaceSizeInBytes ) This function sets the cuBLAS library workspace to a user-owned device buffer, which will be used to execute all subsequent calls to the cuBLAS library functions (on the currently set stream) If the cuBLAS library workspace is not set, all kernels will use the"
  },
  {
    "id": 18665,
    "content": "default workspace pool allocated during the cuBLAS context creation The workspace pointer has to be aligned to at least 256 bytes, otherwise CUBLAS_STATUS_INVALID_VALUE error is returned The cublasSetStream() function unconditionally resets the cuBLAS library workspace back to the default workspace pool Calling this function, including with workspaceSizeInBytes equal to 0, will prevent the cuBLAS"
  },
  {
    "id": 18667,
    "content": "Too small workspaceSizeInBytes may cause some routines to fail with CUBLAS_STATUS_ALLOC_FAILED error returned or cause large regressions in performance Workspace size equal to or larger than 16KiB is enough to prevent CUBLAS_STATUS_ALLOC_FAILED error, while a larger workspace can provide performance benefits for some routines"
  },
  {
    "id": 18668,
    "content": "Note If the stream set by cublasSetStream() is cudaStreamPerThread and there are multiple threads using the same cuBLAS library handle, then users must manually manage synchronization to avoid possible race conditions in the user provided workspace Alternatively, users may rely on the default workspace pool which safely guards against race conditions This is based on the cuBLAS default workspace"
  },
  {
    "id": 18669,
    "content": "pool size which is GPU architecture dependent GPU Architecture Recommended workspace size NVIDIA Hopper Architecture 32 MiB Other 4 MiB The possible error values returned by this function and their meanings are listed below Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the"
  },
  {
    "id": 18673,
    "content": "cublasGetStream()  cublasStatus_t cublasGetStream ( cublasHandle_t handle , cudaStream_t * streamId ) This function gets the cuBLAS library stream, which is being used to execute all calls to the cuBLAS library functions Return Value Meaning CUBLAS_STATUS_SUCCESS the stream was returned successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE"
  },
  {
    "id": 18677,
    "content": "cublasGetPointerMode()  cublasStatus_t cublasGetPointerMode ( cublasHandle_t handle , cublasPointerMode_t * mode ) This function obtains the pointer mode used by the cuBLAS library Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was obtained successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode == NULL 2"
  },
  {
    "id": 18680,
    "content": "cublasSetPointerMode()  cublasStatus_t cublasSetPointerMode ( cublasHandle_t handle , cublasPointerMode_t mode ) This function sets the pointer mode used by the cuBLAS library Return Value Meaning CUBLAS_STATUS_SUCCESS the pointer mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE mode is not CUBLAS_POINTER_MODE_HOST or"
  },
  {
    "id": 18684,
    "content": "cublasSetVector()  cublasStatus_t cublasSetVector ( int n , int elemSize , const void * x , int incx , void * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18685,
    "content": "This function copies n elements from a vector x in host memory space to a vector y in GPU memory space The storage spacing between consecutive elements is given by incx for the source vector x and by incy for the destination vector y Since column-major format for two-dimensional matrices is assumed, if a vector is part of a matrix, a vector increment equal to 1 accesses a (partial) column of that"
  },
  {
    "id": 18686,
    "content": "matrix Similarly, using an increment equal to the leading dimension of the matrix results in accesses to a (partial) row of that matrix Return Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE the parameters incx , incy , elemSizesymv and cublashemv have an alternate implementation that use atomics to cumulate results"
  },
  {
    "id": 18687,
    "content": "This implementation is generally significantly faster but can generate results that are not strictly identical from one run to the others Mathematically, those different results are not significant but when debugging those differences can be prejudicial"
  },
  {
    "id": 18688,
    "content": "This function allows or disallows the usage of atomics in the cuBLAS library for all routines which have an alternate implementation When not explicitly specified in the documentation of any cuBLAS routine, it means that this routine does not have an alternate implementation that use atomics When atomics mode is disabled, each cuBLAS routine should produce the same results from one run to the"
  },
  {
    "id": 18689,
    "content": "other when called with identical parameters on the same Hardware The default atomics mode of default initialized cublasHandle_t object is CUBLAS_ATOMICS_NOT_ALLOWED Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was set successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 2"
  },
  {
    "id": 18692,
    "content": "cublasGetAtomicsMode()  cublasStatus_t cublasGetAtomicsMode ( cublasHandle_t handle , cublasAtomicsMode_t * mode ) This function queries the atomic mode of a specific cuBLAS context"
  },
  {
    "id": 18693,
    "content": "Return Value Meaning CUBLAS_STATUS_SUCCESS the atomics mode was queried successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE the argument mode is a NULL pointer 2"
  },
  {
    "id": 18696,
    "content": "cublasSetMathMode()  cublasStatus_t cublasSetMathMode ( cublasHandle_t handle , cublasMath_t mode ) The cublasSetMathMode() function enables you to choose the compute precision modes as defined by cublasMath_t"
  },
  {
    "id": 18697,
    "content": "Users are allowed to set the compute precision mode as a logical combination of them (except the deprecated CUBLAS_TENSOR_OP_MATH )"
  },
  {
    "id": 18698,
    "content": "For example, cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION)"
  },
  {
    "id": 18699,
    "content": "For matrix and compute precisions allowed for cublasGemmEx() and cublasLtMatmul() APIs and their strided variants please refer to: cublasGemmEx() , cublasGemmBatchedEx() , cublasGemmStridedBatchedEx() , and cublasLtMatmul()"
  },
  {
    "id": 18704,
    "content": "cublasGetMathMode()  cublasStatus_t cublasGetMathMode ( cublasHandle_t handle , cublasMath_t * mode ) This function returns the math mode used by the library routines"
  },
  {
    "id": 18709,
    "content": "cublasSetSmCountTarget()  cublasStatus_t cublasSetSmCountTarget ( cublasHandle_t handle , int smCountTarget ) The cublasSetSmCountTarget() function allows overriding the number of multiprocessors available to the library during kernels execution"
  },
  {
    "id": 18710,
    "content": "This option can be used to improve the library performance when cuBLAS routines are known to run concurrently with other work on different CUDA streams"
  },
  {
    "id": 18711,
    "content": "a NVIDIA A100 GPU has 108 SM and there is a concurrent kenrel running with grid size of 8, one can use cublasSetSmCountTarget() with value 100 to override the library heuristics to optimize for running on 100 multiprocessors"
  },
  {
    "id": 18712,
    "content": "The input value should not exceed the device’s multiprocessor count, which can be obtained using cudaDeviceGetAttribute"
  },
  {
    "id": 18713,
    "content": "The user must ensure thread safety when modifying the library handle with this routine similar to when using cublasSetStream() , etc"
  },
  {
    "id": 18718,
    "content": "cublasGetSmCountTarget()  cublasStatus_t cublasGetSmCountTarget ( cublasHandle_t handle , int * smCountTarget ) This function obtains the value previously programmed to the library handle CUBLAS_STATUS_INVALID_VALUE smCountTarget is NULL"
  },
  {
    "id": 18722,
    "content": "cublasLoggerConfigure()  cublasStatus_t cublasLoggerConfigure ( int logIsOn , int logToStdOut , int logToStdErr , const char * logFileName ) This function configures logging during runtime"
  },
  {
    "id": 18723,
    "content": "Besides this type of configuration, it is possible to configure logging with special environment variables which will be checked by libcublas: CUBLAS_LOGINFO_DBG - Setup env"
  },
  {
    "id": 18724,
    "content": "By default is off, but is turned on by calling cublasSetLoggerCallback() to user defined callback function"
  },
  {
    "id": 18729,
    "content": "cublasGetLoggerCallback()  cublasStatus_t cublasGetLoggerCallback ( cublasLogCallback * userCallback ) This function retrieves function pointer to previously installed custom user defined callback function via cublasSetLoggerCallback() or zero otherwise"
  },
  {
    "id": 18730,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_INVALID_VALUE userCallback is NULL 2"
  },
  {
    "id": 18733,
    "content": "cublasSetLoggerCallback()  cublasStatus_t cublasSetLoggerCallback ( cublasLogCallback userCallback ) This function installs a custom user defined callback function via cublas C public API Parameters userCallback Input"
  },
  {
    "id": 18736,
    "content": "cuBLAS Level-1 Function Reference  In this chapter we describe the Level-1 Basic Linear Algebra Subprograms (BLAS1) functions that perform scalar and vector based operations"
  },
  {
    "id": 18737,
    "content": "We will use abbreviations for type and for the corresponding short type to make a more concise and clear presentation of the implemented functions"
  },
  {
    "id": 18738,
    "content": "Unless otherwise specified and have the following meanings: Meaning float ‘s’ or ‘S’ real single-precision double ‘d’ or ‘D’ real double-precision cuComplex ‘c’ or ‘C’ complex single-precision cuDoubleComplex ‘z’ or ‘Z’ complex double-precision When the parameters and returned values of the function differ, which sometimes happens for complex input, the can also have the following meanings Sc ,"
  },
  {
    "id": 18740,
    "content": "The abbreviation \\(\\mathbf{Re}(\\cdot)\\) and \\(\\mathbf{Im}(\\cdot)\\) will stand for the real and imaginary part of a number, respectively Since imaginary part of a real number does not exist, we will consider it to be zero and can usually simply discard it from the equation where it is being used In general throughout the documentation, the lower case Greek symbols \\(\\alpha\\) and \\(\\beta\\) will"
  },
  {
    "id": 18741,
    "content": "denote scalars, lower case English letters in bold type \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) will denote vectors and capital English letters \\(A\\) , \\(B\\) and \\(C\\) will denote matrices"
  },
  {
    "id": 18745,
    "content": "cublasIamax()  cublasStatus_t cublasIsamax ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamax ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamax ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result ) cublasStatus_t cublasIzamax ( cublasHandle_t handle , int n"
  },
  {
    "id": 18746,
    "content": ", const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18747,
    "content": "Hence, the result is the first \\(i\\) such that \\(\\left| \\mathbf{Im}\\left( {x\\lbrack j brack}  ight) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j brack}  ight)  ight|\\) is maximum for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{ incx}\\)"
  },
  {
    "id": 18749,
    "content": "result host or device output the resulting index, which is 0 if n,incxamin()  cublasStatus_t cublasIsamin ( cublasHandle_t handle , int n , const float * x , int incx , int * result ) cublasStatus_t cublasIdamin ( cublasHandle_t handle , int n , const double * x , int incx , int * result ) cublasStatus_t cublasIcamin ( cublasHandle_t handle , int n , const cuComplex * x , int incx , int * result"
  },
  {
    "id": 18750,
    "content": ") cublasStatus_t cublasIzamin ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , int * result ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18751,
    "content": "Hence, the result is the first \\(i\\) such that \\(\\left| \\mathbf{Im}\\left( {x\\lbrack j brack}  ight) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j brack}  ight)  ight|\\) is minimum for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{incx}\\) Notice that the last equation reflects 1-based indexing used for compatibility with Fortran"
  },
  {
    "id": 18752,
    "content": "result host or device output the resulting index, which is 0 if n,incxasum()  cublasStatus_t cublasSasum ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDasum ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScasum ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float *"
  },
  {
    "id": 18753,
    "content": "result ) cublasStatus_t cublasDzasum ( cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18754,
    "content": "\\sum_{i = 1}^{n} \\middle| \\mathbf{Im}\\left( {x\\lbrack j brack}  ight) \\middle| + \\middle| \\mathbf{Re}\\left( {x\\lbrack j brack}  ight)  ight|\\) where \\(j = 1 + \\left( {i - 1}  ight)*\\text{incx}\\)"
  },
  {
    "id": 18756,
    "content": "0 if n,incxaxpy()  cublasStatus_t cublasSaxpy ( cublasHandle_t handle , int n , const float * alpha , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDaxpy ( cublasHandle_t handle , int n , const double * alpha , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCaxpy ( cublasHandle_t handle , int n , const cuComplex * alpha , const cuComplex * x"
  },
  {
    "id": 18757,
    "content": ", int incx , cuComplex * y , int incy ) cublasStatus_t cublasZaxpy ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18758,
    "content": "This function multiplies the vector x by the scalar \\(\\alpha\\) and adds it to the vector y overwriting the latest vector with the result"
  },
  {
    "id": 18759,
    "content": "Hence, the performed operation is \\(\\mathbf{y}\\lbrack j brack = \\alpha \\times \\mathbf{x}\\lbrack k brack + \\mathbf{y}\\lbrack j brack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1}  ight)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{incy}\\)"
  },
  {
    "id": 18761,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: saxpy , daxpy , caxpy , zaxpy 2"
  },
  {
    "id": 18764,
    "content": "cublascopy()  cublasStatus_t cublasScopy ( cublasHandle_t handle , int n , const float * x , int incx , float * y , int incy ) cublasStatus_t cublasDcopy ( cublasHandle_t handle , int n , const double * x , int incx , double * y , int incy ) cublasStatus_t cublasCcopy ( cublasHandle_t handle , int n , const cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZcopy ("
  },
  {
    "id": 18765,
    "content": "cublasHandle_t handle , int n , const cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18766,
    "content": "Hence, the performed operation is \\(\\mathbf{y}\\lbrack j brack = \\mathbf{x}\\lbrack k brack\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1}  ight)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{incy}\\)"
  },
  {
    "id": 18767,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: scopy , dcopy , ccopy , zcopy 2"
  },
  {
    "id": 18770,
    "content": "Hence, the result is \\(\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack k brack \\times \\mathbf{y}\\lbrack j brack}  ight)\\) where \\(k = 1 + \\left( {i - 1}  ight)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{incy}\\)"
  },
  {
    "id": 18771,
    "content": "Notice that in the first equation the conjugate of the element of vector x should be used if the function name ends in character ‘c’ and that the last two equations reflect 1-based indexing used for compatibility with Fortran"
  },
  {
    "id": 18773,
    "content": "0 if nnrm2()  cublasStatus_t cublasSnrm2 ( cublasHandle_t handle , int n , const float * x , int incx , float * result ) cublasStatus_t cublasDnrm2 ( cublasHandle_t handle , int n , const double * x , int incx , double * result ) cublasStatus_t cublasScnrm2 ( cublasHandle_t handle , int n , const cuComplex * x , int incx , float * result ) cublasStatus_t cublasDznrm2 ( cublasHandle_t handle ,"
  },
  {
    "id": 18774,
    "content": "int n , const cuDoubleComplex * x , int incx , double * result ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18775,
    "content": "The code uses a multiphase model of accumulation to avoid intermediate underflow and overflow, with the result being equivalent to \\(\\sqrt{\\sum_{i = 1}^{n}\\left( {\\mathbf{x}\\lbrack j brack \\times \\mathbf{x}\\lbrack j brack}  ight)}\\) where \\(j = 1 + \\left( {i - 1}  ight)*\\text{incx}\\) in exact arithmetic"
  },
  {
    "id": 18778,
    "content": ", rotation in the x,y plane counter-clockwise by angle defined by cos(alpha)=c, sin(alpha)=s): \\(G = \\begin{pmatrix} c & s \\\\ {- s} & c \\\\ \\end{pmatrix}\\) to vectors x and y"
  },
  {
    "id": 18779,
    "content": "Hence, the result is \\(\\mathbf{x}\\lbrack k brack = c \\times \\mathbf{x}\\lbrack k brack + s \\times \\mathbf{y}\\lbrack j brack\\) and \\(\\mathbf{y}\\lbrack j brack = - s \\times \\mathbf{x}\\lbrack k brack + c \\times \\mathbf{y}\\lbrack j brack\\) where \\(k = 1 + \\left( {i - 1}  ight)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{incy}\\)"
  },
  {
    "id": 18780,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2"
  },
  {
    "id": 18783,
    "content": "cublasrotg()  cublasStatus_t cublasSrotg ( cublasHandle_t handle , float * a , float * b , float * c , float * s ) cublasStatus_t cublasDrotg ( cublasHandle_t handle , double * a , double * b , double * c , double * s ) cublasStatus_t cublasCrotg ( cublasHandle_t handle , cuComplex * a , cuComplex * b , float * c , cuComplex * s ) cublasStatus_t cublasZrotg ( cublasHandle_t handle ,"
  },
  {
    "id": 18784,
    "content": "cuDoubleComplex * a , cuDoubleComplex * b , double * c , cuDoubleComplex * s ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18785,
    "content": "This function constructs the Givens rotation matrix \\(G = \\begin{pmatrix} c & s \\\\ {- s} & c \\\\ \\end{pmatrix}\\) that zeros out the second entry of a \\(2 \\times 1\\) vector \\(\\left( {a,b} ight)^{T}\\) Then, for real numbers we can write \\(\\begin{pmatrix} c & s \\\\ {- s} & c \\\\ \\end{pmatrix}\\begin{pmatrix} a \\\\ b \\\\ \\end{pmatrix} = \\begin{pmatrix} r \\\\ 0 \\\\ \\end{pmatrix}\\) where \\(c^{2} + s^{2} = 1\\)"
  },
  {
    "id": 18786,
    "content": "and \\(r = a^{2} + b^{2}\\) The value of \\(z\\) is such that \\(c\\) and \\(s\\) may be recovered using the following rules: \\(\\left( {c,s} ight) = \\begin{cases} \\left( {\\sqrt{1 - z^{2}},z} ight) & {\\text{ if }\\left| z \\middle| 1 ight } \\\\ \\end{cases}\\) For complex numbers we can write \\(\\begin{pmatrix} c & s \\\\ {- \\bar{s}} & c \\\\ \\end{pmatrix}\\begin{pmatrix} a \\\\ b \\\\ \\end{pmatrix} = \\begin{pmatrix} r"
  },
  {
    "id": 18787,
    "content": "\\\\ 0 \\\\ \\end{pmatrix}\\) where \\(c^{2} + \\left( {\\bar{s} \\times s} ight) = 1\\) and \\(r = \\frac{a}{|a|} \\times \\parallel \\left( {a,b} ight)^{T} \\parallel_{2}\\) with \\(\\parallel \\left( {a,b} ight)^{T} \\parallel_{2} = \\sqrt{\\left| a|^{2} + \\middle| b|^{2} ight"
  },
  {
    "id": 18788,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotg , drotg , crotg , zrotg 2"
  },
  {
    "id": 18791,
    "content": "cublasrotm()  cublasStatus_t cublasSrotm ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy , const float * param ) cublasStatus_t cublasDrotm ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy , const double * param ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18792,
    "content": "This function applies the modified Givens transformation \\(H = \\begin{pmatrix} h_{11} & h_{12} \\\\ h_{21} & h_{22} \\\\ \\end{pmatrix}\\) to vectors x and y Hence, the result is \\(\\mathbf{x}\\lbrack k brack = h_{11} \\times \\mathbf{x}\\lbrack k brack + h_{12} \\times \\mathbf{y}\\lbrack j brack\\) and \\(\\mathbf{y}\\lbrack j brack = h_{21} \\times \\mathbf{x}\\lbrack k brack + h_{22} \\times \\mathbf{y}\\lbrack j"
  },
  {
    "id": 18793,
    "content": "brack\\) where \\(k = 1 + \\left( {i - 1} ight)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1} ight)*\\text{incy}\\)"
  },
  {
    "id": 18794,
    "content": "The elements , , and of matrix \\(H\\) are stored in param[1] , param[2] , param[3] and param[4] , respectively The flag=param[0] defines the following predefined values for the matrix \\(H\\) entries flag=-1 0 flag= 0 0 flag= 1 0 flag=-2"
  },
  {
    "id": 18795,
    "content": "0 \\(\\begin{pmatrix} h_{11} & h_{12} \\\\ h_{21} & h_{22} \\\\ \\end{pmatrix}\\) \\(\\begin{pmatrix} {1 0} & h_{12} \\\\ h_{21} & {1 0} \\\\ \\end{pmatrix}\\) \\(\\begin{pmatrix} h_{11} & {1"
  },
  {
    "id": 18804,
    "content": "0 implied by the flag are not stored in param param host or device input vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\(H\\)"
  },
  {
    "id": 18805,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotm , drotm 2"
  },
  {
    "id": 18808,
    "content": "cublasrotmg()  cublasStatus_t cublasSrotmg ( cublasHandle_t handle , float * d1 , float * d2 , float * x1 , const float * y1 , float * param ) cublasStatus_t cublasDrotmg ( cublasHandle_t handle , double * d1 , double * d2 , double * x1 , const double * y1 , double * param ) This function supports the 64-bit Integer Interface This function constructs the modified Givens transformation \\(H ="
  },
  {
    "id": 18809,
    "content": "\\begin{pmatrix} h_{11} & h_{12} \\\\ h_{21} & h_{22} \\\\ \\end{pmatrix}\\) that zeros out the second entry of a \\(2 \\times 1\\) vector \\(\\left( {\\sqrt{d1}*x1,\\sqrt{d2}*y1} ight)^{T}\\)"
  },
  {
    "id": 18810,
    "content": "param host or device output vector of 5 elements, where param[0] and param[1-4] contain the flag and matrix \\(H\\)"
  },
  {
    "id": 18811,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srotmg , drotmg 2"
  },
  {
    "id": 18814,
    "content": "cublasscal()  cublasStatus_t cublasSscal ( cublasHandle_t handle , int n , const float * alpha , float * x , int incx ) cublasStatus_t cublasDscal ( cublasHandle_t handle , int n , const double * alpha , double * x , int incx ) cublasStatus_t cublasCscal ( cublasHandle_t handle , int n , const cuComplex * alpha , cuComplex * x , int incx ) cublasStatus_t cublasCsscal ( cublasHandle_t handle ,"
  },
  {
    "id": 18815,
    "content": "int n , const float * alpha , cuComplex * x , int incx ) cublasStatus_t cublasZscal ( cublasHandle_t handle , int n , const cuDoubleComplex * alpha , cuDoubleComplex * x , int incx ) cublasStatus_t cublasZdscal ( cublasHandle_t handle , int n , const double * alpha , cuDoubleComplex * x , int incx ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18817,
    "content": "Hence, the performed operation is \\(\\mathbf{x}\\lbrack j brack = \\alpha \\times \\mathbf{x}\\lbrack j brack\\) for \\(i = 1,\\ldots,n\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{incx}\\)"
  },
  {
    "id": 18818,
    "content": ":class: table-no-stripes  Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sscal , dscal , csscal , cscal , zdscal , zscal 2"
  },
  {
    "id": 18821,
    "content": "cublasswap()  cublasStatus_t cublasSswap ( cublasHandle_t handle , int n , float * x , int incx , float * y , int incy ) cublasStatus_t cublasDswap ( cublasHandle_t handle , int n , double * x , int incx , double * y , int incy ) cublasStatus_t cublasCswap ( cublasHandle_t handle , int n , cuComplex * x , int incx , cuComplex * y , int incy ) cublasStatus_t cublasZswap ( cublasHandle_t handle ,"
  },
  {
    "id": 18822,
    "content": "int n , cuDoubleComplex * x , int incx , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18824,
    "content": "\\) for \\(i = 1,\\ldots,n\\) , \\(k = 1 + \\left( {i - 1}  ight)*\\text{incx}\\) and \\(j = 1 + \\left( {i - 1}  ight)*\\text{incy}\\)"
  },
  {
    "id": 18825,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: sswap , dswap , cswap , zswap 2"
  },
  {
    "id": 18827,
    "content": "This function performs the banded matrix-vector multiplication \\(\\mathbf{y} = \\alpha\\text{ op}(A)\\mathbf{x} + \\beta\\mathbf{y}\\) where \\(A\\) is a banded matrix with \\(kl\\) subdiagonals and \\(ku\\) superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars Also, for matrix \\(A\\) \\(\\text{ op}(A) = \\begin{cases} A & \\text{ if transa =="
  },
  {
    "id": 18828,
    "content": "$\\mathrm{CUBLAS\\_OP\\_N}$} \\\\ A^{T} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_T}$} \\\\ A^{H} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_C}$} \\\\ \\end{cases}\\) The banded matrix \\(A\\) is stored column by column, with the main diagonal stored in row \\(ku + 1\\) (starting in first position), the first superdiagonal stored in row \\(ku\\) (starting in second position), the first subdiagonal stored in row"
  },
  {
    "id": 18829,
    "content": "\\(ku + 2\\) (starting in first position), etc So that in general, the element \\(A\\left( {i,j} ight)\\) is stored in the memory location A(ku+1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\left\\lbrack {\\max\\left( {1,j - ku} ight),\\min\\left( {m,j + kl} ight)} ight brack\\) Also, the elements in the array \\(A\\) that do not conceptually correspond to the elements in the banded matrix (the top left \\(ku"
  },
  {
    "id": 18830,
    "content": "\\times ku\\) and bottom right \\(kl \\times kl\\) triangles) are not referenced beta host or device input scalar used for multiplication, if beta == 0 then y does not have to be a valid input This function performs the matrix-vector multiplication \\(\\textbf{y} = \\alpha\\text{ op}(A)\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(m \\times n\\) matrix stored in column-major format, \\(\\mathbf{x}\\) and"
  },
  {
    "id": 18831,
    "content": "\\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars Also, for matrix \\(A\\) \\(\\text{ op}(A) = \\begin{cases} A & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_N}$} \\\\ A^{T} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_T}$} \\\\ A^{H} & \\text{ if transa == $\\mathrm{CUBLAS\\_OP\\_C}$} \\\\ \\end{cases}\\) Param"
  },
  {
    "id": 18833,
    "content": "x device input vector at least (1+(n-1)*abs(incx)) elements if transa==CUBLAS_OP_N and at least (1+(m-1)*abs(incx)) elements otherwise"
  },
  {
    "id": 18834,
    "content": "beta host or device input scalar used for multiplication, if beta==0 then y does not have to be a valid input"
  },
  {
    "id": 18835,
    "content": "y device in/out vector at least (1+(m-1)*abs(incy)) elements if transa==CUBLAS_OP_N and at least (1+(n-1)*abs(incy)) elements otherwise incy input stride between consecutive elements of y The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 18836,
    "content": "This function performs the rank-1 update \\(A = \\begin{cases} {\\alpha\\mathbf{xy}^{T} + A} & \\text{if ger(),geru() is called} \\\\ {\\alpha\\mathbf{xy}^{H} + A} & \\text{if gerc() is called} \\\\ \\end{cases}\\) where \\(A\\) is a \\(m \\times n\\) matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar"
  },
  {
    "id": 18837,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m sbmv()  cublasStatus_t cublasSsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const float * alpha , const float * A , int lda , const float * x , int incx , const float * beta , float * y , int incy )"
  },
  {
    "id": 18838,
    "content": "cublasStatus_t cublasDsbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const double * alpha , const double * A , int lda , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18839,
    "content": "This function performs the symmetric banded matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric banded matrix with \\(k\\) subdiagonals and superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars"
  },
  {
    "id": 18840,
    "content": "If uplo == CUBLAS_FILL_MODE_LOWER then the symmetric banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1, the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i"
  },
  {
    "id": 18841,
    "content": "\\in \\lbrack j,\\min(m,j + k) brack\\) Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the bottom right \\(k \\times k\\) triangle) are not referenced If uplo == CUBLAS_FILL_MODE_UPPER then the symmetric banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k"
  },
  {
    "id": 18842,
    "content": "(starting at second position), the second superdiagonal in row k-1 (starting at third position), etc So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k),j brack\\) Also, the elements in the array A that do not conceptually correspond to the elements in the banded matrix (the top left \\(k \\times k\\)"
  },
  {
    "id": 18843,
    "content": "triangle) are not referenced uplo input indicates if matrix A lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements"
  },
  {
    "id": 18844,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spmv()  cublasStatus_t cublasSspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * AP , const float * x , int incx , const float * beta , float * y , int incy ) cublasStatus_t"
  },
  {
    "id": 18845,
    "content": "cublasDspmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * AP , const double * x , int incx , const double * beta , double * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18846,
    "content": "This function performs the symmetric packed matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the symmetric matrix \\(A\\)"
  },
  {
    "id": 18847,
    "content": "are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) Consequently, the packed format requires only \\(\\frac{n(n + 1)}{2}\\) elements for storage If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the symmetric matrix \\(A\\) are packed together"
  },
  {
    "id": 18848,
    "content": "column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\) uplo input indicates if matrix \\(A\\) lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements"
  },
  {
    "id": 18849,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spr()  cublasStatus_t cublasSspr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , float * AP ) cublasStatus_t cublasDspr ( cublasHandle_t handle , cublasFillMode_t"
  },
  {
    "id": 18850,
    "content": "uplo , int n , const double * alpha , const double * x , int incx , double * AP ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18851,
    "content": "This function performs the packed symmetric rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{T} + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar"
  },
  {
    "id": 18852,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n spr2()  cublasStatus_t cublasSspr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const float * x , int incx , const float * y , int incy , float * AP ) cublasStatus_t cublasDspr2 ("
  },
  {
    "id": 18853,
    "content": "cublasHandle_t handle , cublasFillMode_t uplo , int n , const double * alpha , const double * x , int incx , const double * y , int incy , double * AP ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18854,
    "content": "This function performs the packed symmetric rank-2 update \\(A = \\alpha\\left( {\\textbf{x}\\textbf{y}^{T} + \\textbf{y}\\textbf{x}^{T}} ight) + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in lower or upper"
  },
  {
    "id": 18856,
    "content": "This function has an alternate faster implementation using atomics that can be enabled with cublasSetAtomicsMode() Please see the section on the function cublasSetAtomicsMode() for more details about the usage of atomics"
  },
  {
    "id": 18857,
    "content": "uplo input indicates if matrix lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements This function performs the symmetric rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{T} + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in column-major format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar This function performs"
  },
  {
    "id": 18858,
    "content": "the symmetric rank-2 update \\(A = \\alpha\\left( {\\textbf{x}\\textbf{y}^{T} + \\textbf{y}\\textbf{x}^{T}} ight) + A\\) where \\(A\\) is a \\(n \\times n\\) symmetric matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar This function performs the triangular banded matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a"
  },
  {
    "id": 18860,
    "content": "Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 18861,
    "content": "\\) If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc If uplo == CUBLAS_FILL_MODE_UPPER then the triangular banded matrix \\(A\\) is stored column by column, with the"
  },
  {
    "id": 18862,
    "content": "main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting at second position), the second superdiagonal in row k-1 (starting at third position), etc So that in general, the element \\(A(i,j)\\) is stored in the memory location A(1+k+i-j,j) for \\(j = 1,\\ldots,n\\) and \\(i \\in \\lbrack\\max(1,j - k,j) brack\\) uplo input indicates if matrix A lower or upper part is stored,"
  },
  {
    "id": 18863,
    "content": "the other part is not referenced and is inferred from the stored elements diag input indicates if the elements on the main diagonal of matrix A are unity and should not be accessed This function solves the triangular banded linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular banded matrix, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors"
  },
  {
    "id": 18864,
    "content": "Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 18866,
    "content": "If uplo == CUBLAS_FILL_MODE_LOWER then the triangular banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc This function performs the triangular packed matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\)"
  },
  {
    "id": 18868,
    "content": "Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 18869,
    "content": "\\) If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular matrix"
  },
  {
    "id": 18870,
    "content": "\\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(A(i,j)\\) and \\(i \\leq j\\) This function solves the packed triangular linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular matrix stored in packed format, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are"
  },
  {
    "id": 18871,
    "content": "vectors If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the triangular matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the triangular"
  },
  {
    "id": 18872,
    "content": "matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+(j*(j+1))/2] for \\(j = 1,\\ldots,n\\) and \\(i \\leq j\\)"
  },
  {
    "id": 18873,
    "content": "diag input indicates if the elements on the main diagonal of matrix are unity and should not be accessed This function performs the triangular matrix-vector multiplication \\(\\textbf{x} = \\text{op}(A)\\textbf{x}\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\(\\mathbf{x}\\) is a vector"
  },
  {
    "id": 18874,
    "content": "Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 18878,
    "content": "This function solves the triangular linear system with a single right-hand-side \\(\\text{op}(A)\\textbf{x} = \\textbf{b}\\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, and \\(\\mathbf{x}\\) and \\(\\mathbf{b}\\) are vectors"
  },
  {
    "id": 18879,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hemv()  cublasStatus_t cublasChemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y ,"
  },
  {
    "id": 18880,
    "content": "int incy ) cublasStatus_t cublasZhemv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18881,
    "content": "This function performs the Hermitian matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in lower or upper mode, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars"
  },
  {
    "id": 18882,
    "content": "This function has an alternate faster implementation using atomics that can be enabled with Please see the section on the for more details about the usage of atomics Param"
  },
  {
    "id": 18883,
    "content": "uplo input indicates if matrix A lower or upper part is stored, the other Hermitian part is not referenced and is inferred from the stored elements"
  },
  {
    "id": 18884,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hbmv()  cublasStatus_t cublasChbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * x , int incx , const cuComplex * beta ,"
  },
  {
    "id": 18885,
    "content": "cuComplex * y , int incy ) cublasStatus_t cublasZhbmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18886,
    "content": "This function performs the Hermitian banded matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian banded matrix with \\(k\\) subdiagonals and superdiagonals, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars If uplo == CUBLAS_FILL_MODE_LOWER then the Hermitian banded matrix \\(A\\) is stored"
  },
  {
    "id": 18887,
    "content": "column by column, with the main diagonal of the matrix stored in row 1 , the first subdiagonal in row 2 (starting at first position), the second subdiagonal in row 3 (starting at first position), etc If uplo == CUBLAS_FILL_MODE_UPPER then the Hermitian banded matrix \\(A\\) is stored column by column, with the main diagonal of the matrix stored in row k+1 , the first superdiagonal in row k (starting"
  },
  {
    "id": 18888,
    "content": "at second position), the second superdiagonal in row k-1 (starting at third position), etc beta host or device input scalar used for multiplication, if beta==0 then does not have to be a valid input"
  },
  {
    "id": 18889,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpmv()  cublasStatus_t cublasChpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * AP , const cuComplex * x , int incx , const cuComplex * beta , cuComplex * y , int incy"
  },
  {
    "id": 18890,
    "content": ") cublasStatus_t cublasZhpmv ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * AP , const cuDoubleComplex * x , int incx , const cuDoubleComplex * beta , cuDoubleComplex * y , int incy ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18891,
    "content": "This function performs the Hermitian packed matrix-vector multiplication \\(\\textbf{y} = \\alpha A\\textbf{x} + \\beta\\textbf{y}\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) and \\(\\beta\\) are scalars If uplo == CUBLAS_FILL_MODE_LOWER then the elements in the lower triangular part of the Hermitian matrix \\(A\\)"
  },
  {
    "id": 18892,
    "content": "are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory location AP[i+((2*n-j+1)*j)/2] for \\(j = 1,\\ldots,n\\) and \\(i \\geq j\\) If uplo == CUBLAS_FILL_MODE_UPPER then the elements in the upper triangular part of the Hermitian matrix \\(A\\) are packed together column by column without gaps, so that the element \\(A(i,j)\\) is stored in the memory"
  },
  {
    "id": 18894,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her()  cublasStatus_t cublasCher ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * A , int lda ) cublasStatus_t cublasZher ( cublasHandle_t handle ,"
  },
  {
    "id": 18895,
    "content": "cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18896,
    "content": "This function performs the Hermitian rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in column-major format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar"
  },
  {
    "id": 18897,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her2()  cublasStatus_t cublasCher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * A , int lda )"
  },
  {
    "id": 18898,
    "content": "cublasStatus_t cublasZher2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * A , int lda ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18899,
    "content": "This function performs the Hermitian rank-2 update \\(A = \\alpha\\textbf{x}\\textbf{y}^{H} + \\overset{ˉ}{\\alpha}\\textbf{y}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in column-major format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar"
  },
  {
    "id": 18900,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpr()  cublasStatus_t cublasChpr ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const float * alpha , const cuComplex * x , int incx , cuComplex * AP ) cublasStatus_t cublasZhpr ( cublasHandle_t handle ,"
  },
  {
    "id": 18901,
    "content": "cublasFillMode_t uplo , int n , const double * alpha , const cuDoubleComplex * x , int incx , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18902,
    "content": "This function performs the packed Hermitian rank-1 update \\(A = \\alpha\\textbf{x}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) is a vector, and \\(\\alpha\\) is a scalar"
  },
  {
    "id": 18903,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n hpr2()  cublasStatus_t cublasChpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * alpha , const cuComplex * x , int incx , const cuComplex * y , int incy , cuComplex * AP ) cublasStatus_t"
  },
  {
    "id": 18904,
    "content": "cublasZhpr2 ( cublasHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * x , int incx , const cuDoubleComplex * y , int incy , cuDoubleComplex * AP ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 18905,
    "content": "This function performs the packed Hermitian rank-2 update \\(A = \\alpha\\textbf{x}\\textbf{y}^{H} + \\overset{ˉ}{\\alpha}\\textbf{y}\\textbf{x}^{H} + A\\) where \\(A\\) is a \\(n \\times n\\) Hermitian matrix stored in packed format, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are vectors, and \\(\\alpha\\) is a scalar This function performs the matrix-vector multiplication of a batch of matrices and vectors all instances"
  },
  {
    "id": 18906,
    "content": "have the same dimensions (m, n), leading dimension (lda), increments (incx, incy) and transposition (trans) for their respective A matrix, x and y vectors The address of the input matrix and vector, and the output vector of each instance of the batch are read from arrays of pointers passed to the function by the caller \\(\\textbf{y}\\lbrack i brack = \\alpha\\text{op}(A\\lbrack i"
  },
  {
    "id": 18907,
    "content": "brack)\\textbf{x}\\lbrack i brack + \\beta\\textbf{y}\\lbrack i brack,\\text{ for i} \\in \\lbrack 0,batchCount - 1 brack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) is an array of pointers to matrice \\(A\\lbrack i brack\\) stored in column-major format with dimension \\(m \\times n\\) , and \\(\\textbf{x}\\) and \\(\\textbf{y}\\) are arrays of pointers to vectors Also, for matrix \\(A\\lbrack i brack\\) ,"
  },
  {
    "id": 18908,
    "content": "\\(\\text{op}(A\\lbrack i brack) = \\left\\{ \\begin{matrix} {A\\lbrack i brack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A\\lbrack i brack}^{T} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ {A\\lbrack i brack}^{H} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) Note \\(\\textbf{y}\\lbrack i brack\\) vectors must not overlap, i"
  },
  {
    "id": 18910,
    "content": "the individual gemv operations must be computable independently; otherwise, undefined behavior is expected"
  },
  {
    "id": 18911,
    "content": "On certain problem sizes, it might be advantageous to make multiple calls to cublasgemv in different CUDA streams, rather than use this API"
  },
  {
    "id": 18914,
    "content": "xarray device input array of pointers to array, with each dimension n if trans==CUBLAS_OP_N and m otherwise"
  },
  {
    "id": 18915,
    "content": "If math mode enables fast math modes when using cublasSgemvBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors"
  },
  {
    "id": 18916,
    "content": "Otherwise it is recommended that they meet the following rule: if k % 4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 18917,
    "content": "Input matrix A and vector x, and output vector y for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance Pointers to A matrix, x and y vectors for the first instance are passed to the function by the user along with offsets in number of elements - strideA, stridex and stridey that determine the locations of input matrices and"
  },
  {
    "id": 18918,
    "content": "vectors, and output vectors in future instances \\(\\textbf{y} + i*{stridey} = \\alpha\\text{op}(A + i*{strideA})(\\textbf{x} + i*{stridex}) + \\beta(\\textbf{y} + i*{stridey}),\\text{ for i } \\in \\lbrack 0,batchCount - 1 brack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) is an array of pointers to matrix stored in column-major format with dimension \\(A\\lbrack i brack\\) \\(m \\times n\\) , and"
  },
  {
    "id": 18919,
    "content": "\\(\\textbf{x}\\) and \\(\\textbf{y}\\) are arrays of pointers to vectors Also, for matrix \\(A\\lbrack i brack\\) \\(\\text{op}(A\\lbrack i brack) = \\left\\{ \\begin{matrix} {A\\lbrack i brack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A\\lbrack i brack}^{T} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ {A\\lbrack i brack}^{H} & {\\text{if }\\textsf{trans =="
  },
  {
    "id": 18920,
    "content": "$\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) Note \\(\\textbf{y}\\lbrack i brack\\) matrices must not overlap, i"
  },
  {
    "id": 18922,
    "content": "Note In the table below, we use A[i], x[i], y[i] as notation for A matrix, and x and y vectors in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, stridex, stridey away from A[i-1], x[i-1], y[i-1] A device input * pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x n with lda>=max(1,m)"
  },
  {
    "id": 18923,
    "content": "strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] x device input * pointer to the x vector corresponding to the first instance of the batch, with each dimension n if trans==CUBLAS_OP_N and m otherwise stridex input Value of type long long int that gives the offset in number of elements between x[i] and x[i+1] beta host or device input"
  },
  {
    "id": 18924,
    "content": "scalar used for multiplication y device in/out * pointer to the y vector corresponding to the first instance of the batch, with each dimension m if trans==CUBLAS_OP_N and n otherwise stridey input Value of type long long int that gives the offset in number of elements between y[i] and y[i+1] batchCount input number of GEMVs to perform in the batch"
  },
  {
    "id": 18925,
    "content": "This function performs the matrix-matrix multiplication \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{"
  },
  {
    "id": 18926,
    "content": "\\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) and \\(\\text{op}(B)\\) is defined similarly for matrix \\(B\\) transa input operation op( A ) that is non- or (conj ) transb input operation op( B ) that is non- or (conj"
  },
  {
    "id": 18928,
    "content": "A device input array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise B device input array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise"
  },
  {
    "id": 18929,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m , n , k gemm3m()  cublasStatus_t cublasCgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex"
  },
  {
    "id": 18930,
    "content": "* B , int ldb , const cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZgemm3m ( cublasHandle_t handle , cublasOperation_t transa , cublasOperation_t transb , int m , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the"
  },
  {
    "id": 18932,
    "content": "This function performs the complex matrix-matrix multiplication, using Gauss complexity reduction algorithm"
  },
  {
    "id": 18933,
    "content": "This can lead to an increase in performance up to 25% \\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively"
  },
  {
    "id": 18934,
    "content": "Note These 2 routines are only supported on GPUs with architecture capabilities equal to or greater than 5"
  },
  {
    "id": 18936,
    "content": "transa input Operation op( A ) that is non- or (conj ) transb input Operation op( B ) that is non- or (conj ) m input Number of rows of matrix op( A ) and C"
  },
  {
    "id": 18937,
    "content": "The possible error values returned by this function and their meanings are listed in the following table: Error Value Meaning CUBLAS_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 18938,
    "content": "all instances have the same dimensions (m, n, k), leading dimensions (lda, ldb, ldc) and transpositions (transa, transb) for their respective A, B and C matrices"
  },
  {
    "id": 18939,
    "content": "The address of the input matrices and the output matrix of each instance of the batch are read from arrays of pointers passed to the function by the caller"
  },
  {
    "id": 18940,
    "content": "\\(C\\lbrack i brack = \\alpha\\text{op}(A\\lbrack i brack)\\text{op}(B\\lbrack i brack) + \\beta C\\lbrack i brack,\\text{ for i } \\in \\lbrack 0,batchCount - 1 brack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i brack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i brack)\\) \\(k"
  },
  {
    "id": 18941,
    "content": "\\times n\\) and \\(C\\lbrack i brack\\) \\(m \\times n\\) , respectively Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) and \\(\\text{op}(B\\lbrack i brack)\\) is defined"
  },
  {
    "id": 18942,
    "content": "similarly for matrix \\(B\\lbrack i brack\\) Note \\(C\\lbrack i brack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected"
  },
  {
    "id": 18943,
    "content": "On certain problem sizes, it might be advantageous to make multiple calls to cublasgemm in different CUDA streams, rather than use this API"
  },
  {
    "id": 18944,
    "content": "transa input operation op( A[i] ) that is non- or (conj ) transb input operation op( B[i] ) that is non- or (conj ) m input number of rows of matrix op( A[i] ) and C[i]"
  },
  {
    "id": 18945,
    "content": "lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise"
  },
  {
    "id": 18946,
    "content": "If math mode enables fast math modes when using cublasSgemmBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors"
  },
  {
    "id": 18947,
    "content": "Otherwise it is recommended that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 18948,
    "content": "Input matrices A, B and output matrix C for each instance of the batch are located at fixed offsets in number of elements from their locations in the previous instance Pointers to A, B and C matrices for the first instance are passed to the function by the user along with offsets in number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in"
  },
  {
    "id": 18949,
    "content": "future instances \\(C + i*{strideC} = \\alpha\\text{op}(A + i*{strideA})\\text{op}(B + i*{strideB}) + \\beta(C + i*{strideC}),\\text{ for i } \\in \\lbrack 0,batchCount - 1 brack\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are arrays of pointers to matrices stored in column-major format with dimensions \\(\\text{op}(A\\lbrack i brack)\\) \\(m \\times k\\) , \\(\\text{op}(B\\lbrack i"
  },
  {
    "id": 18951,
    "content": "the individual gemm operations must be computable independently; otherwise, undefined behavior is expected"
  },
  {
    "id": 18952,
    "content": "Note In the table below, we use A[i], B[i], C[i] as notation for A, B and C matrices in the ith instance of the batch, implicitly assuming they are respectively offsets in number of elements strideA, strideB, strideC away from A[i-1], B[i-1], C[i-1]"
  },
  {
    "id": 18953,
    "content": "A device input * pointer to the A matrix corresponding to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa==CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1] B device input * pointer to the B matrix corresponding to the first instance of the batch,"
  },
  {
    "id": 18954,
    "content": "with dimensions ldb x n with ldb>=max(1,k) if transb==CUBLAS_OP_N and ldb x k with ldb>=max(1,n) max(1,) otherwise strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1] beta host or device input scalar used for multiplication C device in/out * pointer to the C matrix corresponding to the first instance of the batch, with dimensions ldc x n"
  },
  {
    "id": 18955,
    "content": "with ldc>=max(1,m) strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1] batchCount input number of GEMMs to perform in the batch"
  },
  {
    "id": 18956,
    "content": "However, the dimensions, leading dimensions, transpositions, and scaling factors (alpha, beta) may vary between groups"
  },
  {
    "id": 18957,
    "content": "This is functionally equivalent to the following: idx = 0 ; for i = 0 : group_count - 1 for j = 0 : group_size [ i ] - 1 gemm ( transa_array [ i ], transb_array [ i ], m_array [ i ], n_array [ i ], k_array [ i ], alpha_array [ i ], Aarray [ idx ], lda_array [ i ], Barray [ idx ], ldb_array [ i ], beta_array [ i ], Carray [ idx ], ldc_array [ i ]); idx += 1 ; end end where"
  },
  {
    "id": 18958,
    "content": "\\(\\text{$\\mathrm{alpha\\_array}$}\\) and \\(\\text{$\\mathrm{beta\\_array}$}\\) are arrays of scaling factors, and \\(\\text{Aarray}\\) , \\(\\text{Barray}\\) and \\(\\text{Carray}\\) are arrays of pointers to matrices stored in column-major format For a given index, \\(\\text{idx}\\) , that is part of group \\(i\\) , the dimensions are: \\(\\text{op}(\\text{Aarray}\\lbrack\\text{idx} brack)\\) :"
  },
  {
    "id": 18959,
    "content": "\\(\\text{$\\mathrm{m\\_array}$}\\lbrack i brack \\times \\text{$\\mathrm{k\\_array}$}\\lbrack i brack\\) \\(\\text{op}(\\text{Barray}\\lbrack\\text{idx} brack)\\) : \\(\\text{$\\mathrm{k\\_array}$}\\lbrack i brack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i brack\\) \\(\\text{Carray}\\lbrack\\text{idx} brack\\) : \\(\\text{$\\mathrm{m\\_array}$}\\lbrack i brack \\times \\text{$\\mathrm{n\\_array}$}\\lbrack i brack\\) Note This API"
  },
  {
    "id": 18960,
    "content": "takes arrays of two different lengths Note \\(C\\lbrack\\text{idx} brack\\) matrices must not overlap, that is, the individual gemm operations must be computable independently; otherwise, undefined behavior is expected"
  },
  {
    "id": 18961,
    "content": "On certain problem sizes, it might be advantageous to make multiple calls to cublasgemmBatched in different CUDA streams, rather than use this API"
  },
  {
    "id": 18964,
    "content": "group_count transb_array host input array containing the operations, op( B[idx] ), that is non- or (conj ) group_count m_array host input array containing the number of rows of matrix op( A[idx] ) and C[idx] for each group group_count n_array host input array containing the number of columns of op( B[idx] ) and C[idx] for each group group_count k_array host input array containing the number of"
  },
  {
    "id": 18965,
    "content": "columns of op( A[idx] ) and rows of op( B[idx] ) for each group group_count alpha_array host input array containing the scalar used for multiplication for each group"
  },
  {
    "id": 18966,
    "content": "lda[i] x k[i] with lda[i]>=max(1,m[i]) if transa[i]==CUBLAS_OP_N and lda[i] x m[i] with lda[i]>=max(1,k[i]) otherwise"
  },
  {
    "id": 18967,
    "content": "problem_count lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group"
  },
  {
    "id": 18968,
    "content": "ldb[i] x n[i] with ldb[i]>=max(1,k[i]) if transb[i]==CUBLAS_OP_N and ldb[i] x k[i] with ldb[i]>=max(1,n[i]) otherwise"
  },
  {
    "id": 18969,
    "content": "problem_count ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group group_count beta_array host input array containing the scalar used for multiplication for each group problem_count ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each group"
  },
  {
    "id": 18970,
    "content": "group_count group_count host input number of groups group_size host input array containg the number of pointers contained in Aarray, Barray and Carray for each group group_count If math mode enables fast math modes when using cublasSgemmGroupedBatched() , pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors"
  },
  {
    "id": 18971,
    "content": "Otherwise it is required that they meet the following rule: if k%4==0 then ensure intptr_t(ptr) % 16 == 0 , The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 18972,
    "content": "This function performs the symmetric matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix} {\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\ {\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\ \\end{matrix} ight \\) where \\(A\\) is a symmetric matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices,"
  },
  {
    "id": 18974,
    "content": "A device input array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise"
  },
  {
    "id": 18975,
    "content": "beta host or device input scalar used for multiplication, if beta == 0 then C does not have to be a valid input This function performs the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) Also,"
  },
  {
    "id": 18976,
    "content": "for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ \\end{matrix} ight"
  },
  {
    "id": 18978,
    "content": "uplo input indicates if matrix C lower or upper part is stored, the other symmetric part is not referenced and is inferred from the stored elements"
  },
  {
    "id": 18979,
    "content": "A device input array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise"
  },
  {
    "id": 18980,
    "content": "beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input"
  },
  {
    "id": 18981,
    "content": "This function performs the symmetric rank- \\(2k\\) update \\(C = \\alpha(\\text{op}(A)\\text{op}(B)^{T} + \\text{op}(B)\\text{op}(A)^{T}) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively Also, for matrix"
  },
  {
    "id": 18982,
    "content": "\\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix} {A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ \\end{matrix} ight"
  },
  {
    "id": 18984,
    "content": "uplo input indicates if matrix C lower or upper part, is stored, the other symmetric part is not referenced and is inferred from the stored elements"
  },
  {
    "id": 18985,
    "content": "A device input array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise B device input array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise"
  },
  {
    "id": 18986,
    "content": "beta host or device input scalar used for multiplication, if beta==0 , then C does not have to be a valid input"
  },
  {
    "id": 18987,
    "content": "This function performs a variation of the symmetric rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{T} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a symmetric matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively Also, for matrices \\(A\\) and \\(B\\)"
  },
  {
    "id": 18988,
    "content": "\\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix} {A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ \\end{matrix} ight"
  },
  {
    "id": 18989,
    "content": "\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric A usual example is when the matrix B is a scaled form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasdgmm This function performs the"
  },
  {
    "id": 18990,
    "content": "triangular matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix} {\\alpha\\text{op}(A)B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\ {\\alpha B\\text{op}(A)} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\ \\end{matrix} ight \\) where \\(A\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(B\\) and \\(C\\) are \\(m \\times n\\)"
  },
  {
    "id": 18991,
    "content": "matrix, and \\(\\alpha\\) is a scalar Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) Notice that in order to achieve better parallelism cuBLAS differs from the BLAS API"
  },
  {
    "id": 18992,
    "content": "only for this routine The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLAS API assumes an out-of-place implementation (with results written into C) The application can obtain the in-place functionality of BLAS in the cuBLAS API by passing the address of the matrix B in place of the matrix C alpha host or device input scalar used for multiplication, if"
  },
  {
    "id": 18993,
    "content": "alpha==0 then A is not referenced and B does not have to be a valid input This function solves the triangular linear system with multiple right-hand-sides \\(\\left\\{ \\begin{matrix} {\\text{op}(A)X = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\ {X\\text{op}(A) = \\alpha B} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\ \\end{matrix} ight \\) where \\(A\\) is a"
  },
  {
    "id": 18994,
    "content": "triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\) and \\(B\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa =="
  },
  {
    "id": 18995,
    "content": "$\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) The solution \\(X\\) overwrites the right-hand-sides \\(B\\) on exit This function solves an array of triangular linear systems with multiple right-hand-sides \\(\\left\\{ \\begin{matrix} {\\text{op}(A\\lbrack i brack)X\\lbrack i brack = \\alpha B\\lbrack i brack} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\ {X\\lbrack i"
  },
  {
    "id": 18996,
    "content": "brack\\text{op}(A\\lbrack i brack) = \\alpha B\\lbrack i brack} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\ \\end{matrix} ight \\) where \\(A\\lbrack i brack\\) is a triangular matrix stored in lower or upper mode with or without the main diagonal, \\(X\\lbrack i brack\\) and \\(B\\lbrack i brack\\) are \\(m \\times n\\) matrices, and \\(\\alpha\\) is a scalar Also, for matrix \\(A\\)"
  },
  {
    "id": 18997,
    "content": "\\(\\text{op}(A\\lbrack i brack) = \\left\\{ \\begin{matrix} {A\\lbrack i brack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A^{T}\\lbrack i brack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ {A^{H}\\lbrack i brack} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) The solution \\(X\\lbrack i brack\\) overwrites the right-hand-sides \\(B\\lbrack"
  },
  {
    "id": 18998,
    "content": "i brack\\) on exit This function works for any sizes but is intended to be used for matrices of small sizes where the launch overhead is a significant factor For bigger sizes, it might be advantageous to call batchCount times the regular cublastrsm within a set of CUDA streams The current implementation is limited to devices with compute capability above or equal 2"
  },
  {
    "id": 19000,
    "content": "uplo input indicates if matrix A[i] lower or upper part is stored, the other part is not referenced and is inferred from the stored elements diag input indicates if the elements on the main diagonal of matrix A[i] are unity and should not be accessed alpha host or device input scalar used for multiplication, if alpha==0 then A[i] is not referenced and B[i] does not have to be a valid input"
  },
  {
    "id": 19002,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m hemm()  cublasStatus_t cublasChemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const"
  },
  {
    "id": 19003,
    "content": "cuComplex * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZhemm ( cublasHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , int m , int n , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const cuDoubleComplex * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 19004,
    "content": "This function performs the Hermitian matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix} {\\alpha AB + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\ {\\alpha BA + \\beta C} & {\\text{if }\\textsf{side == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\ \\end{matrix} ight \\) where \\(A\\) is a Hermitian matrix stored in lower or upper mode, \\(B\\) and \\(C\\) are \\(m \\times n\\) matrices,"
  },
  {
    "id": 19006,
    "content": "A device input array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise"
  },
  {
    "id": 19008,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If m herk()  cublasStatus_t cublasCherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const float * alpha , const cuComplex * A , int lda , const float * beta , cuComplex * C , int"
  },
  {
    "id": 19009,
    "content": "ldc ) cublasStatus_t cublasZherk ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const double * alpha , const cuDoubleComplex * A , int lda , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 19010,
    "content": "This function performs the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) is a matrix with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa =="
  },
  {
    "id": 19011,
    "content": "$\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight"
  },
  {
    "id": 19013,
    "content": "uplo input indicates if matrix C lower or upper part is stored, the other Hermitian part is not referenced"
  },
  {
    "id": 19014,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n her2k()  cublasStatus_t cublasCher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const"
  },
  {
    "id": 19015,
    "content": "float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZher2k ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 19016,
    "content": "This function performs the Hermitian rank- \\(2k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\overset{ˉ}{\\alpha}\\text{op}(B)\\text{op}(A)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively"
  },
  {
    "id": 19017,
    "content": "Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix} {A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight"
  },
  {
    "id": 19019,
    "content": "B device input array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise"
  },
  {
    "id": 19020,
    "content": "beta host or device input scalar used for multiplication, if beta==0 then C does not have to be a valid input"
  },
  {
    "id": 19021,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n herkx()  cublasStatus_t cublasCherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuComplex * alpha , const cuComplex * A , int lda , const cuComplex * B , int ldb , const"
  },
  {
    "id": 19022,
    "content": "float * beta , cuComplex * C , int ldc ) cublasStatus_t cublasZherkx ( cublasHandle_t handle , cublasFillMode_t uplo , cublasOperation_t trans , int n , int k , const cuDoubleComplex * alpha , const cuDoubleComplex * A , int lda , const cuDoubleComplex * B , int ldb , const double * beta , cuDoubleComplex * C , int ldc ) This function supports the 64-bit Integer Interface"
  },
  {
    "id": 19023,
    "content": "This function performs a variation of the Hermitian rank- \\(k\\) update \\(C = \\alpha\\text{op}(A)\\text{op}(B)^{H} + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, \\(C\\) is a Hermitian matrix stored in lower or upper mode, and \\(A\\) and \\(B\\) are matrices with dimensions \\(\\text{op}(A)\\) \\(n \\times k\\) and \\(\\text{op}(B)\\) \\(n \\times k\\) , respectively Also, for matrix \\(A\\) and \\(B\\)"
  },
  {
    "id": 19024,
    "content": "\\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix} {A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A^{H}\\text{ and }B^{H}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix} ight \\) This routine can be used when the matrix B is in such way that the result is guaranteed to be hermitian An usual example is when the matrix B is a scaled"
  },
  {
    "id": 19025,
    "content": "form of the matrix A: this is equivalent to B being the product of the matrix A and a diagonal matrix"
  },
  {
    "id": 19026,
    "content": "beta host or device input real scalar used for multiplication, if beta==0 then C does not have to be a valid input"
  },
  {
    "id": 19027,
    "content": "This function performs the matrix-matrix addition/transposition \\(C = \\alpha\\text{op}(A) + \\beta\\text{op}(B)\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times n\\) , \\(\\text{op}(B)\\) \\(m \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively The in-place mode supports the following two"
  },
  {
    "id": 19028,
    "content": "operations, \\(C = \\alpha\\text{*}C + \\beta\\text{op}(B)\\) \\(C = \\alpha\\text{op}(A) + \\beta\\text{*}C\\) For in-place mode, if C = A , ldc = lda and transa = CUBLAS_OP_N The operation includes the following special cases: the user can reset matrix C to zero by setting *alpha=*beta=0 A device input array of dimensions lda x n with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,n)"
  },
  {
    "id": 19029,
    "content": "otherwise B device input array of dimension ldb x n with ldb>=max(1,m) if transb == CUBLAS_OP_N and ldb x m with ldb>=max(1,n) otherwise This function performs the matrix-matrix multiplication \\(C = \\left\\{ \\begin{matrix} {A \\times diag(X)} & {\\text{if }\\textsf{mode == $\\mathrm{CUBLAS\\_SIDE\\_RIGHT}$}} \\\\ {diag(X) \\times A} & {\\text{if }\\textsf{mode == $\\mathrm{CUBLAS\\_SIDE\\_LEFT}$}} \\\\"
  },
  {
    "id": 19030,
    "content": "\\end{matrix} ight \\) where \\(A\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(m \\times n\\) \\(X\\) is a vector of size \\(n\\) if mode == CUBLAS_SIDE_RIGHT and of size \\(m\\) if mode == CUBLAS_SIDE_LEFT The formula of X is \\(X\\lbrack j brack = \\left\\{ \\begin{matrix} {x\\lbrack j \\times incx brack} & {\\text{if }incx \\geq 0} \\\\ {x\\lbrack(\\chi - 1) \\times |incx| - j \\times |incx|"
  },
  {
    "id": 19031,
    "content": "brack} & {\\text{if }incx geam() with *beta=0 and transa == CUBLAS_OP_N or cublasdgmm() with incx=0 and x[0]=alpha mode input left multiply if mode == CUBLAS_SIDE_LEFT or right multiply if mode == CUBLAS_SIDE_RIGHT m input number of rows of matrix A and C A device input array of dimensions lda x n with lda>=max(1,m) lda input leading dimension of two-dimensional array used to store the matrix A x"
  },
  {
    "id": 19032,
    "content": "device input one-dimensional array of size \\(|inc| \\times m\\) if mode == CUBLAS_SIDE_LEFT and \\(|inc| \\times n\\) if mode == CUBLAS_SIDE_RIGHT incx input stride of one-dimensional array x This function performs the LU factorization of each Aarray[i] for i = 0, …, batchSize-1 by the following equation \\(\\text{P}\\text{*}{Aarray}\\lbrack i brack = L\\text{*}U\\) where P is a permutation matrix which"
  },
  {
    "id": 19036,
    "content": "Pj can be constructed by j element of PivotArray[i] by the following Matlab code In Matlab PivotArray[i] is an array of base-1 Pj = eye ( n ); swap Pj ( j , : ) and Pj ( PivotArray [ i ][ j ] , : ) L and U are written back to original matrix A , and diagonal elements of L are discarded The L and U can be constructed by the following Matlab code A is a matrix of nxn after getrf L = eye ( n ); for"
  },
  {
    "id": 19037,
    "content": "j = 1 : n L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : n U ( i , i : n ) = A ( i , i : n ) end If matrix A(=Aarray[i]) is singular, getrf still works and the value of info(=infoArray[i]) reports first row index that LU factorization cannot proceed The equation P*A=L*U still holds, however L and U reconstruction needs different Matlab code as follows: A is a matrix of"
  },
  {
    "id": 19038,
    "content": "nxn after getrf L = eye ( n ); for j = 1 : k -1 L ( j + 1 : n , j ) = A ( j + 1 : n , j ) end U = zeros ( n ); for i = 1 : k -1 U ( i , i : n ) = A ( i , i : n ) end for i = k : n U ( i , k : n ) = A ( i , k : n ) end This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor"
  },
  {
    "id": 19039,
    "content": "lda input leading dimension of two-dimensional array used to store each matrix Aarray[i] PivotArray device output array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of factorization of Aarray[i]"
  },
  {
    "id": 19040,
    "content": "batchSize input number of pointers contained in A The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 19041,
    "content": "Also, for matrix \\(A\\) \\(\\text{op}(A\\lbrack i brack) = \\left\\{ \\begin{matrix} {A\\lbrack i brack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A^{T}\\lbrack i brack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ {A^{H}\\lbrack i brack} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 19042,
    "content": "\\) This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor"
  },
  {
    "id": 19043,
    "content": "devIpiv device input array of size n x batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion"
  },
  {
    "id": 19045,
    "content": "Prior to calling cublasgetriBatched, the matrix A[i] must be factorized first using the routine cublasgetrfBatched After the call of cublasgetrfBatched, the matrix pointing by Aarray[i] will contain the LU factors of the matrix A[i] and the vector pointing by (PivotArray+i) will contain the pivoting sequence Following the LU factorization, cublasgetriBatched uses forward and backward triangular"
  },
  {
    "id": 19046,
    "content": "solvers to complete inversion of matrices A[i] for i = 0, …, batchSize-1 The inversion is out-of-place, so memory space of Carray[i] cannot overlap memory space of Array[i] Aarray[i] is n*n matrix A[i] cublasDgetrfBatched ( handle , n , Aarray , lda , PivotArray , infoArray , batchSize ); check infoArray[i] to see if factorization of A[i] is successful or not Array[i] contains LU factorization of"
  },
  {
    "id": 19047,
    "content": "A[i] step 2: perform out-of-place inversion, Carray[i] = inv(A[i]) cublasDgetriBatched ( handle , n , Aarray , lda , PivotArray , Carray , ldc , infoArray , batchSize ); check infoArray[i] to see if inversion of A[i] is successful or not"
  },
  {
    "id": 19048,
    "content": "This function is intended to be used for matrices of small sizes where the launch overhead is a significant factor"
  },
  {
    "id": 19050,
    "content": "Aarray device input array of pointers to array, with each array of dimension n*n with lda>=max(1,n) PivotArray device output array of size n*batchSize that contains the pivoting sequence of each factorization of Aarray[i] stored in a linear fashion Carray device output array of pointers to array, with each array of dimension n*n with ldc>=max(1,n) ldc input leading dimension of two-dimensional"
  },
  {
    "id": 19051,
    "content": "array used to store each matrix Carray[i] infoArray device output array of size batchSize that info(=infoArray[i]) contains the information of inversion of A[i]"
  },
  {
    "id": 19053,
    "content": "A device input array of pointers to array, with each array of dimension n*n with lda>=max(1,n) Ainv device output array of pointers to array, with each array of dimension n*n with lda_inv>=max(1,n) lda_inv input leading dimension of two-dimensional array used to store each matrix Ainv[i] info device output array of size batchSize that info[i] contains the information of inversion of A[i]"
  },
  {
    "id": 19054,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If n 32 CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU 2"
  },
  {
    "id": 19059,
    "content": "Each matrix Q[i] is represented as a product of elementary reflectors and is stored in the lower part of each Aarray[i] as follows : Q[j] = H[j][1] H[j][2]"
  },
  {
    "id": 19061,
    "content": "Each H[j][i] has the form H[j][i] = I - tau[j] * v * v' where tau[j] is a real scalar, and v is a real vector with v(1:i-1) = 0 and v(i) = 1 ; v(i+1:m) is stored on exit in Aarray[j][i+1:m,i] , and tau in TauArray[j][i]"
  },
  {
    "id": 19062,
    "content": "This function find the least squares solution of a batch of overdetermined systems: it solves the least squares problem described as follows : minimize || Carray [ i ] - Aarray [ i ] * Xarray [ i ] || , with i = 0 , , batchSize -1 On exit, each Aarray[i] is overwritten with their QR factorization and each Carray[i] is overwritten with the least square solution cublasgelsBatched supports only the"
  },
  {
    "id": 19063,
    "content": "non-transpose operation and only solves over-determined systems (m >= n) trans input operation op( Aarray[i] ) that is non- or (conj ) Only non-transpose operation is currently supported m input number of rows of each Aarray[i] and Carray[i] if trans == CUBLAS_OP_N , numbers of columns of each Aarray[i] otherwise (not supported currently) n input number of columns of each Aarray[i] if trans =="
  },
  {
    "id": 19064,
    "content": "CUBLAS_OP_N , and number of rows of each Aarray[i] and Carray[i] otherwise (not supported currently) m x n with lda>=max(1,m) if trans == CUBLAS_OP_N , and n x m with lda>=max(1,n) otherwise (not supported currently) m x nrhs with ldc>=max(1,m) if trans == CUBLAS_OP_N , and n x nrhs with lda>=max(1,n) otherwise (not supported currently) info host output If info=0, the parameters passed to the"
  },
  {
    "id": 19065,
    "content": "function are valid If info 0 : the V-th diagonal element of the Aarray[i] is zero batchSize input number of pointers contained in Aarray and Carray The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 19066,
    "content": "If uplo == CUBLAS_FILL_MODE_UPPER then the elements of AP are copied into the upper triangular part of the triangular matrix A and the lower part of A is left untouched If uplo == CUBLAS_FILL_MODE_UPPER then then the upper triangular part of the triangular matrix A is copied into the array AP"
  },
  {
    "id": 19067,
    "content": "In this function the input matrices and output matrices can have a lower precision but the computation is still done in the type For example, in the type float for cublasSgemmEx() and in the type cuComplex for cublasCgemmEx()"
  },
  {
    "id": 19068,
    "content": "\\(C = \\alpha\\text{op}(A)\\text{op}(B) + \\beta C\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars, and \\(A\\) , \\(B\\) and \\(C\\) are matrices stored in column-major format with dimensions \\(\\text{op}(A)\\) \\(m \\times k\\) , \\(\\text{op}(B)\\) \\(k \\times n\\) and \\(C\\) \\(m \\times n\\) , respectively"
  },
  {
    "id": 19069,
    "content": "The matrix types combinations supported for cublasSgemmEx() are listed below: C A/B CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_8I CUDA_R_16BF CUDA_R_16F CUDA_R_32F The matrix types combinations supported for cublasCgemmEx() are listed below : C A/B CUDA_C_32F CUDA_C_8I CUDA_C_32F The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 19070,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_ARCH_MISMATCH cublasCgemmEx() is only supported for GPU with architecture capabilities equal or greater than 5"
  },
  {
    "id": 19071,
    "content": "0 CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters Atype , Btype and Ctype is not supported CUBLAS_STATUS_INVALID_VALUE If m gemm that allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run"
  },
  {
    "id": 19072,
    "content": "Note The second variant of cublasGemmEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t alpha host or device input Scaling factor for A*B of the type that corresponds to the computeType and Ctype, see the table below for details beta host or device input Scaling factor for C of the type"
  },
  {
    "id": 19074,
    "content": "For better performance, it is also recommended that IMMA kernels requirements for a regular data ordering listed here are met"
  },
  {
    "id": 19075,
    "content": "The possible error values returned by this function and their meanings are listed in the following table"
  },
  {
    "id": 19076,
    "content": "CUBLAS_STATUS_ARCH_MISMATCH cublasGemmEx() is only supported for GPU with architecture capabilities equal or greater than 5"
  },
  {
    "id": 19078,
    "content": "CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters Atype , Btype and Ctype or the algorithm, algo is not supported CUBLAS_STATUS_INVALID_VALUE If m gemmBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrix arrays, the precision of computation and the GEMM algorithm to be"
  },
  {
    "id": 19080,
    "content": "Note The second variant of cublasGemmBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType instead of cublasComputeType_t"
  },
  {
    "id": 19081,
    "content": "transa input Operation op( A[i] ) that is non- or (conj ) transb input Operation op( B[i] ) that is non- or (conj ) m input Number of rows of matrix op( A[i] ) and C[i]"
  },
  {
    "id": 19082,
    "content": "lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise"
  },
  {
    "id": 19083,
    "content": "Otherwise it is recommended that they meet the following rule: if k%8==0 then ensure intptr_t(ptr) % 16 == 0 , if k%2==0 then ensure intptr_t(ptr) % 4 == 0"
  },
  {
    "id": 19084,
    "content": "Note Compute types CUBLAS_COMPUTE_32I and CUBLAS_COMPUTE_32I_PEDANTIC are only supported with all pointers A[i] , B[i] being 4-byte aligned and lda, ldb being multiples of 4"
  },
  {
    "id": 19085,
    "content": "For a better performance, it is also recommended that IMMA kernels requirements for the regular data ordering listed here are met"
  },
  {
    "id": 19086,
    "content": "CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal to or greater than 5"
  },
  {
    "id": 19088,
    "content": "CUBLAS_STATUS_INVALID_VALUE If m gemmStridedBatched that performs the matrix-matrix multiplication of a batch of matrices and allows the user to individually specify the data types for each of the A, B and C matrices, the precision of computation and the GEMM algorithm to be run Pointers to A, B and C matrices for the first instance are passed to the function by the user along with the offsets in"
  },
  {
    "id": 19089,
    "content": "number of elements - strideA, strideB and strideC that determine the locations of input and output matrices in future instances"
  },
  {
    "id": 19090,
    "content": "Note The second variant of cublasGemmStridedBatchedEx() function is provided for backward compatibility with C++ applications code, where the computeType parameter is of cudaDataType_t instead of cublasComputeType_t"
  },
  {
    "id": 19091,
    "content": "A device input Pointer to matrix, A, corresponds to the first instance of the batch, with dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise"
  },
  {
    "id": 19092,
    "content": "strideA input Value of type long long int that gives the offset in number of elements between A[i] and A[i+1]"
  },
  {
    "id": 19093,
    "content": "B device input Pointer to matrix, B, corresponds to the first instance of the batch, with dimensions ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise"
  },
  {
    "id": 19094,
    "content": "strideB input Value of type long long int that gives the offset in number of elements between B[i] and B[i+1]"
  },
  {
    "id": 19095,
    "content": "C device in/out Pointer to matrix, C, corresponds to the first instance of the batch, with dimensions ldc x n with ldc>=max(1,m)"
  },
  {
    "id": 19096,
    "content": "strideC input Value of type long long int that gives the offset in number of elements between C[i] and C[i+1]"
  },
  {
    "id": 19097,
    "content": "CUBLAS_STATUS_ARCH_MISMATCH cublasGemmBatchedEx() is only supported for GPU with architecture capabilities equal or greater than 5"
  },
  {
    "id": 19099,
    "content": "lda_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix A[idx] for each group ldb_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix B[idx] for each group ldc_array host input array containing the leading dimensions of two-dimensional arrays used to store each matrix C[idx] for each"
  },
  {
    "id": 19101,
    "content": "cublasGemmGroupedBatchedEx() supports the following Compute Type, Scale Type, Atype/Btype, and Ctype: Compute Type Scale Type (alpha and beta) Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_32F CUDA_R_32F CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F"
  },
  {
    "id": 19102,
    "content": "CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F If Atype is CUDA_R_16F or CUDA_R_16BF or if the computeType is any of the FAST options, pointers (not the pointer arrays) placed in the GPU memory must be properly aligned to avoid misaligned memory access errors"
  },
  {
    "id": 19103,
    "content": "Otherwise it is required that they meet the following rule: if (k * AtypeSize) % 16 == 0 then ensure intptr_t(ptr) % 16 == 0 , if (k * AtypeSize) % 4 == 0 then ensure intptr_t(ptr) % 4 == 0"
  },
  {
    "id": 19104,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_INVALID_VALUE If transa_array , transb_array , m_array , n_array , k_array , alpha_array , lda_array , ldb_array , beta_array , ldc_array , or group_size are NULL or if group_count scalar used for multiplication beta host or device input scalar"
  },
  {
    "id": 19106,
    "content": "The matrix types combinations supported for cublasCsyrkEx() are listed below: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below The matrix types combinations supported for cublasCsyrk3mEx() are listed below : A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and"
  },
  {
    "id": 19107,
    "content": "their meanings are listed below The matrix types combinations supported for cublasCherkEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F CUDA_C_32F The possible error values returned by this function and their meanings are listed below The matrix types combinations supported for cublasCherk3mEx() are listed in the following table: A C CUDA_C_8I CUDA_C_32F CUDA_C_32F"
  },
  {
    "id": 19109,
    "content": "CUBLAS_STATUS_INVALID_VALUE If n nrm2 where input data, output data and compute type can be specified independently"
  },
  {
    "id": 19113,
    "content": "The datatypes combinations currently supported for cublasAxpyEx() are listed in the following table: alpha x y execution CUDA_R_32F CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible"
  },
  {
    "id": 19115,
    "content": "CUBLAS_STATUS_NOT_SUPPORTED The combination of the parameters xType , yType , and executionType is not supported CUBLAS_STATUS_INVALID_VALUE alphaType or xType or yType or executionType is not supported cublasDotEx()  cublasStatus_t cublasDotEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result ,"
  },
  {
    "id": 19116,
    "content": "cudaDataType resultType , cudaDataType executionType ); cublasStatus_t cublasDotcEx ( cublasHandle_t handle , int n , const void * x , cudaDataType xType , int incx , const void * y , cudaDataType yType , int incy , void * result , cudaDataType resultType , cudaDataType executionType ); These functions support the 64-bit Integer Interface"
  },
  {
    "id": 19117,
    "content": "These functions are an API generalization of the routines cublasdot and cublasdotc where input data, output data and compute type can be specified independently"
  },
  {
    "id": 19119,
    "content": "0 if nrot where input data, output data, cosine/sine type, and compute type can be specified independently"
  },
  {
    "id": 19121,
    "content": "The datatypes combinations currently supported for cublasRotEx() are listed below : executionType xType / yType csType CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_R_64F CUDA_C_64F The possible error values returned by this function and"
  },
  {
    "id": 19123,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU For references please refer to: srot , drot , crot , csrot , zrot , zdrot 2"
  },
  {
    "id": 19126,
    "content": "cublasScalEx()  cublasStatus_t cublasScalEx ( cublasHandle_t handle , int n , const void * alpha , cudaDataType alphaType , void * x , cudaDataType xType , int incx , cudaDataType executionType ); This function supports the 64-bit Integer Interface"
  },
  {
    "id": 19127,
    "content": "The datatypes combinations currently supported for cublasScalEx() are listed below : alpha x execution CUDA_R_32F CUDA_R_16F CUDA_R_32F CUDA_R_32F CUDA_R_16BF CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_64F CUDA_C_64F CUDA_C_64F The possible error values returned by this function and their meanings are listed below"
  },
  {
    "id": 19128,
    "content": "Error Value Meaning CUBLAS_STATUS_SUCCESS the operation completed successfully CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized CUBLAS_STATUS_NOT_SUPPORTED the combination of the parameters xType and executionType is not supported CUBLAS_STATUS_EXECUTION_FAILED the function failed to launch on the GPU CUBLAS_STATUS_INVALID_VALUE alphaType or xType or executionType is not supported"
  },
  {
    "id": 19130,
    "content": "General Description  The cuBLASLt library is a new lightweight library dedicated to GEneral Matrix-to-matrix Multiply (GEMM) operations with a new flexible API"
  },
  {
    "id": 19131,
    "content": "This new library adds flexibility in matrix data layouts, input types, compute types, and also in choosing the algorithmic implementations and heuristics through parameter programmability"
  },
  {
    "id": 19132,
    "content": "Once a set of options for the intended GEMM operation are identified by the user, these options can be used repeatedly for different inputs"
  },
  {
    "id": 19133,
    "content": "Note The cuBLASLt library does not guarantee the support of all possible sizes and configurations, however, since CUDA 12"
  },
  {
    "id": 19134,
    "content": "2 update 2, the problem size limitations on m, n, and batch size have been largely resolved The main focus of the library is to provide the most performant kernels, which might have some implied limitations Some non-standard configurations may require a user to handle them manually, typically by decomposing the problem into smaller parts (see Problem Size Limitations )"
  },
  {
    "id": 19138,
    "content": "Problem Size Limitations  There are inherent problem size limitations that are a result of limitations in CUDA grid dimensions"
  },
  {
    "id": 19139,
    "content": "For example, many kernels do not support batch sizes greater than 65535 due to a limitation on the z dimension of a grid"
  },
  {
    "id": 19140,
    "content": "In cases where a problem cannot be run by a single kernel, cuBLASLt will attempt to decompose the problem into multiple sub-problems and solve it by running the kernel on each sub-problem There are some restrictions on cuBLASLt internal problem decomposition which are summarized below: Amax computations are not supported This means that CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and"
  },
  {
    "id": 19141,
    "content": "CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER must be left unset (see cublasLtMatmulDescAttributes_t ) All matrix layouts must have CUBLASLT_MATRIX_LAYOUT_ORDER set to CUBLASLT_ORDER_COL (see cublasLtOrder_t ) cuBLASLt will not partition along the n dimension when CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_DRELU_BGRAD or CUBLASLT_EPILOGUE_DGELU_BGRAD (see cublasLtEpilogue_t ) To"
  },
  {
    "id": 19142,
    "content": "overcome these limitations, a user may want to partition the problem themself, launch kernels for each sub-problem, and compute any necessary reductions to combine the results"
  },
  {
    "id": 19146,
    "content": "Heuristics Cache  cuBLASLt uses heuristics to pick the most suitable matmul kernel for execution based on the problem sizes, GPU configuration, and other parameters"
  },
  {
    "id": 19148,
    "content": "To overcome this overhead, it is recommended to query the heuristics once using cublasLtMatmulAlgoGetHeuristic() and then reuse the result for subsequent computations using cublasLtMatmul() For the cases where querying heuristics once and then reusing them is not feasible, cuBLASLt implements a heuristics cache that maps matmul problems to kernels previously selected by heuristics The user can"
  },
  {
    "id": 19149,
    "content": "control the heuristics cache capacity with the CUBLASLT_HEURISTICS_CACHE_CAPACITY environment variable or with the cublasLtHeuristicsCacheSetCapacity() function which has higher precedence"
  },
  {
    "id": 19150,
    "content": "The capacity is measured in number of entries and might be rounded up to the nearest multiple of some factor for performance reasons"
  },
  {
    "id": 19151,
    "content": "This can be useful for workloads that do not have a steady state and for which cache operations may have higher overhead than regular heuristics computations"
  },
  {
    "id": 19152,
    "content": "Note The cache is not ideal for performance reasons, so it is sometimes necessary to increase its capacity 1"
  },
  {
    "id": 19155,
    "content": "See: cublasLtLoggerSetCallback() , cublasLtLoggerSetFile() , cublasLtLoggerOpenFile() , cublasLtLoggerSetLevel() , cublasLtLoggerSetMask() , cublasLtLoggerForceDisable() 3"
  },
  {
    "id": 19158,
    "content": "8-bit Floating Point Data Types (FP8) Usage  FP8 was first introduced with Ada and Hopper GPUs (compute capability 8"
  },
  {
    "id": 19160,
    "content": "There are two types of FP8 available: CUDA_R_8F_E4M3 is designed to be accurate at a smaller dynamic range than half precision"
  },
  {
    "id": 19161,
    "content": "In order to maintain accurate FP8 matrix multiplications, we define native compute FP8 matrix multiplication as follows: \\[D = scale_D \\cdot (\\alpha \\cdot scale_A \\cdot scale_B \\cdot \\text{op}(A) \\text{op}(B) + \\beta \\cdot scale_C \\cdot C)\\] where A, B, and C are input matrices, and scaleA, scaleB, scaleC, scaleD, alpha, and beta are input scalars"
  },
  {
    "id": 19162,
    "content": "This differs from the other matrix multiplication routines because of this addition of scaling factors for each matrix"
  },
  {
    "id": 19164,
    "content": "This means that sometimes it is necessary to use a scaling factor or its reciprocal depending on the context in which it is applied"
  },
  {
    "id": 19165,
    "content": "For FP8 matrix multiplications, epilogues and amaxD may be computed as follows: \\[\\begin{split}D_{temp}, Aux_{temp} & = \\mathop{Epilogue}(\\alpha \\cdot scale_A \\cdot scale_B \\cdot \\text{op}(A) \\text{op}(B) + \\beta \\cdot scale_C \\cdot C) \\\\ amax_{D} & = \\mathop{absmax}(D_{temp}) \\\\ amax_{Aux} & = \\mathop{absmax}(Aux_{temp}) \\\\ D & = scale_D * D_{temp} \\\\ Aux & = scale_{Aux} * Aux_{temp}"
  },
  {
    "id": 19166,
    "content": "\\\\\\end{split}\\] Here Aux is an auxiliary output of an epilogue function like GELU, scaleAux is an optional scaling factor that can be applied to Aux, and amaxAux is the maximum absolute value in Aux before scaling"
  },
  {
    "id": 19167,
    "content": "For more information, see attributes CUBLASLT_MATMUL_DESC_AMAX_D_POINTER and CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER in cublasLtMatmulDescAttributes_t"
  },
  {
    "id": 19171,
    "content": "Disabling CPU Instructions  As mentioned in the Heuristics Cache section, cuBLASLt heuristics perform some compute-intensive operations on the host CPU To speed-up the operations, the implementation detects CPU capabilities and may use special instructions, such as Advanced Vector Extensions (AVX) on x86-64 CPUs For instance, using advanced instructions may result in CPU running at a lower"
  },
  {
    "id": 19172,
    "content": "frequency, which would affect performance of the other host code The user can optionally instruct the cuBLASLt library to not use some CPU instructions with the CUBLASLT_DISABLE_CPU_INSTRUCTIONS_MASK environment variable or with the cublasLtDisableCpuInstructionsSetMask() function which has higher precedence Please check cublasLtDisableCpuInstructionsSetMask() for more information"
  },
  {
    "id": 19176,
    "content": "Atomics Synchronization  Atomics synchronization allows optimizing matmul workloads by enabling cublasLtMatmul() to have a producer or consumer relationship with another concurrently running kernel"
  },
  {
    "id": 19177,
    "content": "Conceptually, matmul is provided with an array containing 32-bit integer counters, and then: In the consumer mode, either matrix A is partitioned into chunks by rows, or matrix B is partitioned into chunks by columns 1"
  },
  {
    "id": 19178,
    "content": "A chunk can be read from memory and used in computations only when the corresponding atomic counter reaches value of 0"
  },
  {
    "id": 19179,
    "content": "The producer should execute a memory fence to ensure that the written value is visible to the concurrently running matmul kernel 2 In the producer mode, the output matrix C (or D in the out-of-place mode), is partitioned by rows or columns, and after a chunk is computed, the corresponding atomic counter is set to 0 1 The current implementation allows partitioning either the rows or the columns of"
  },
  {
    "id": 19181,
    "content": "2 One possible implementation of a memory fence is cuda::atomic_thread_fence(cuda::memory_order_seq_cst, cuda::thread_scope::thread_scope_device) (see cuda::atomic_thread_fence() for more details)"
  },
  {
    "id": 19182,
    "content": "The array of counters are passed to matmuls via the CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER compute descriptor attributes for the consumer and producer modes respectively 3 3 The current implementation allows to only enable either the producer or the consumer mode, but not both"
  },
  {
    "id": 19184,
    "content": "The number of chunks is controlled by CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS and CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS compute descriptor attributes"
  },
  {
    "id": 19186,
    "content": "For the column-major layout, the number of chunks must satisfy: \\[\\begin{split}0 \\leq \\text{$\\mathrm{NUM\\_CHUNKS\\_ROWS}$} \\leq & \\mathop{\\text{floor}}\\left( \\frac{\\text{M}}{\\text{$\\mathrm{TILE\\_SIZE\\_M}$} * \\text{$\\mathrm{CLUSTER\\_SHAPE\\_M}$}} ight) \\\\ 0 \\leq \\text{$\\mathrm{NUM\\_CHUNKS\\_COLS}$} \\leq & \\mathop{\\text{floor}}\\left( \\frac{\\text{N}}{\\text{$\\mathrm{TILE\\_SIZE\\_N}$} *"
  },
  {
    "id": 19187,
    "content": "\\text{$\\mathrm{CLUSTER\\_SHAPE\\_N}$}} ight)\\end{split}\\] For row-major layout, M and N in tile size and cluster shape must be swapped"
  },
  {
    "id": 19188,
    "content": "These restrictions mean that it is required to first query heuristic via cublasLtMatmulAlgoGetHeuristic() and inspect the result for tile and cluster shapes, and only then set the number of chunks"
  },
  {
    "id": 19189,
    "content": "The pseudocode below shows the principles of operation:   The code below shows operation when partitioning over   rows assuming column-major layout and TN case The case when partitioning is done over columns or   row-major case are handled in a similar fashion,   with the main difference being the offsets   computations"
  },
  {
    "id": 19190,
    "content": "Note that the actual implementation does not   guarantee in which order the chunks are computed,   and may employ various optimizations to improve   overall performance"
  },
  {
    "id": 19191,
    "content": "Here:   - A, B, C -- input matrices in the column-major layout   - lda -- leading dimension of matrix A   - M, N, K -- the original problem dimensions   - counters_in[] and counters_out[] -- the arrays of   input and output atomic counters   for ( int i = 0 ; i 1"
  },
  {
    "id": 19192,
    "content": "Each algorithm can support some custom options that don’t fit the description of the other configuration attributes"
  },
  {
    "id": 19193,
    "content": "See the CUBLASLT_ALGO_CAP_CUSTOM_OPTION_MAX of cublasLtMatmulAlgoCapAttributes_t for the accepted range for a specific case"
  },
  {
    "id": 19194,
    "content": "cublasLtMatmulDesc_t  The cublasLtMatmulDesc_t is a pointer to an opaque structure holding the description of the matrix multiplication operation cublasLtMatmul()"
  },
  {
    "id": 19195,
    "content": "A descriptor can be created by calling cublasLtMatmulDescCreate() and destroyed by calling cublasLtMatmulDescDestroy()"
  },
  {
    "id": 19199,
    "content": "cublasLtMatmulDescAttributes_t  cublasLtMatmulDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix multiply operation"
  },
  {
    "id": 19200,
    "content": "Use cublasLtMatmulDescGetAttribute() and cublasLtMatmulDescSetAttribute() to get and set the attribute value of a matmul descriptor"
  },
  {
    "id": 19201,
    "content": "Defines the data type used for multiply and accumulate operations, and the accumulator during the matrix multiplication The accumulator value and the value from matrix C are typically converted to scale type before final scaling The value is then converted from scale type to the type of matrix D before storing in memory"
  },
  {
    "id": 19202,
    "content": "int32_t CUBLASLT_MATMUL_DESC_POINTER_MODE Specifies alpha and beta are passed by reference, whether they are scalars on the host or on the device, or device vectors int32_t CUBLASLT_MATMUL_DESC_TRANSA Specifies the type of transformation operation that should be performed on matrix A int32_t CUBLASLT_MATMUL_DESC_TRANSB Specifies the type of transformation operation that should be performed on"
  },
  {
    "id": 19203,
    "content": "matrix B int32_t CUBLASLT_MATMUL_DESC_TRANSC Specifies the type of transformation operation that should be performed on matrix C int32_t CUBLASLT_MATMUL_DESC_FILL_MODE Indicates whether the lower or upper part of the dense matrix was filled, and consequently should be used by the function"
  },
  {
    "id": 19204,
    "content": "uint32_t CUBLASLT_MATMUL_DESC_BIAS_POINTER Bias or Bias gradient vector pointer in the device memory"
  },
  {
    "id": 19205,
    "content": "Input vector with length that matches the number of rows of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BIAS , CUBLASLT_EPILOGUE_RELU_BIAS , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_GELU_BIAS , CUBLASLT_EPILOGUE_GELU_AUX_BIAS Output vector with length that matches the number of rows of matrix D when one of the following epilogues is used:"
  },
  {
    "id": 19206,
    "content": "CUBLASLT_EPILOGUE_DRELU_BGRAD , CUBLASLT_EPILOGUE_DGELU_BGRAD , CUBLASLT_EPILOGUE_BGRADA Output vector with length that matches the number of columns of matrix D when one of the following epilogues is used: CUBLASLT_EPILOGUE_BGRADB"
  },
  {
    "id": 19207,
    "content": "Bias vector elements are the same type as alpha and beta (see CUBLASLT_MATMUL_DESC_SCALE_TYPE in this table) when matrix D datatype is CUDA_R_8I and same as matrix D datatype otherwise void * / const void * CUBLASLT_MATMUL_DESC_BIAS_BATCH_STRIDE Stride (in elements) to the next bias or bias gradient vector for strided batch operations"
  },
  {
    "id": 19208,
    "content": "Output vector for ReLu bit-mask in forward pass when CUBLASLT_EPILOGUE_RELU_AUX or CUBLASLT_EPILOGUE_RELU_AUX_BIAS epilogue is used Input vector for ReLu bit-mask in backward pass when CUBLASLT_EPILOGUE_DRELU or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used Output of GELU input matrix in forward pass when CUBLASLT_EPILOGUE_GELU_AUX_BIAS epilogue is used Input of GELU input matrix for backward"
  },
  {
    "id": 19210,
    "content": "Routines that don’t dereference this pointer, like cublasLtMatmulAlgoGetHeuristic() depend on its value to determine expected pointer alignment"
  },
  {
    "id": 19211,
    "content": "void * / const void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_LD Leading dimension for epilogue auxiliary buffer"
  },
  {
    "id": 19212,
    "content": "bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU_BGRAD , or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used GELU input matrix leading dimension in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DGELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used"
  },
  {
    "id": 19214,
    "content": "bits) when CUBLASLT_EPILOGUE_RELU_AUX , CUBLASLT_EPILOGUE_RELU_AUX_BIAS or CUBLASLT_EPILOGUE_DRELU_BGRAD epilogue is used GELU input matrix batch stride in elements when CUBLASLT_EPILOGUE_GELU_AUX_BIAS , CUBLASLT_EPILOGUE_DRELU , or CUBLASLT_EPILOGUE_DGELU_BGRAD epilogue used"
  },
  {
    "id": 19215,
    "content": "Used together with CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST when matrix D’s CUBLASLT_MATRIX_LAYOUT_BATCH_COUNT is greater than 1"
  },
  {
    "id": 19216,
    "content": "If CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO is set then CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE must be set to 0 as this mode doesn’t support batched alpha vector"
  },
  {
    "id": 19217,
    "content": "int64_t CUBLASLT_MATMUL_DESC_SM_COUNT_TARGET Number of SMs to target for parallel execution Optimizes heuristics for execution on a different number of SMs when user expects a concurrent stream to be using some of the device resources"
  },
  {
    "id": 19218,
    "content": "int32_t CUBLASLT_MATMUL_DESC_A_SCALE_POINTER Device pointer to the scale factor value that converts data in matrix A to the compute data type range If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE Default value: NULL const void* CUBLASLT_MATMUL_DESC_B_SCALE_POINTER Equivalent to"
  },
  {
    "id": 19219,
    "content": "CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix B Default value: NULL const void* CUBLASLT_MATMUL_DESC_C_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix C Default value: NULL const void* CUBLASLT_MATMUL_DESC_D_SCALE_POINTER Equivalent to CUBLASLT_MATMUL_DESC_A_SCALE_POINTER for matrix D Default value: NULL const void* CUBLASLT_MATMUL_DESC_AMAX_D_POINTER Device pointer"
  },
  {
    "id": 19220,
    "content": "to the memory location that on completion will be set to the maximum of absolute values in the output matrix Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_DATA_TYPE The type of the data that will be stored in CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER If unset (or set to the default value of -1), the data type is set to be the output matrix element data type (DType) with some"
  },
  {
    "id": 19221,
    "content": "exceptions: ReLu uses a bit-mask For FP8 kernels with an output type (DType) of CUDA_R_8F_E4M3 , the data type can be set to a non-default value if: AType and BType are CUDA_R_8F_E4M3 CType is CUDA_R_16BF or CUDA_R_16F CUBLASLT_MATMUL_DESC_EPILOGUE is set to CUBLASLT_EPILOGUE_GELU_AUX When CType is CUDA_R_16BF , the data type may be set to CUDA_R_16BF or CUDA_R_8F_E4M3 Default value: -1 int32_t"
  },
  {
    "id": 19222,
    "content": "based on cudaDataType CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_SCALE_POINTER Device pointer to the scaling factor value to convert results from compute type data range to storage data range in the auxiliary matrix that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER Default value: NULL void * CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_AMAX_POINTER Device pointer to the memory location that on completion will"
  },
  {
    "id": 19223,
    "content": "be set to the maximum of absolute values in the buffer that is set via CUBLASLT_MATMUL_DESC_EPILOGUE_AUX_POINTER If set for an unsupported matrix data, scale, and compute type combination, calling cublasLtMatmul() will return CUBLAS_INVALID_VALUE Default value: NULL void * CUBLASLT_MATMUL_DESC_FAST_ACCUM Flag for managing FP8 fast accumulation mode"
  },
  {
    "id": 19224,
    "content": "When enabled, problem execution might be faster but at the cost of lower accuracy because intermediate results will not periodically be promoted to a higher precision"
  },
  {
    "id": 19225,
    "content": "Default value: 0 - fast accumulation mode is disabled int8_t CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE Type of the bias or bias gradient vector in the device memory If unset (or set to the default value of -1), the bias vector elements are the same type as the elements of the output matrix (Dtype) with the following exceptions: IMMA kernels with computeType= CUDA_R_32I and Ctype=CUDA_R_8I where the"
  },
  {
    "id": 19226,
    "content": "bias vector elements are the same type as alpha, beta ( CUBLASLT_MATMUL_DESC_SCALE_TYPE=CUDA_R_32F ) For FP8 kernels with an output type of CUDA_R_32F , CUDA_R_8F_E4M3 or CUDA_R_8F_E5M2"
  },
  {
    "id": 19227,
    "content": "Default value: -1 int32_t based on cudaDataType CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_IN_COUNTERS_POINTER Pointer to a device array of input atomic counters consumed by a matmul"
  },
  {
    "id": 19228,
    "content": "When a counter reaches zero, computation of the corresponding chunk of the output tensor is allowed to start"
  },
  {
    "id": 19229,
    "content": "int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_OUT_COUNTERS_POINTER Pointer to a device array of output atomic counters produced by a matmul A matmul kernel sets a counter to zero when the computations of the corresponding chunk of the output tensor have completed int32_t * CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_ROWS Number of atomic synchronization chunks in the row dimension of the output"
  },
  {
    "id": 19230,
    "content": "matrix D int32_t CUBLASLT_MATMUL_DESC_ATOMIC_SYNC_NUM_CHUNKS_D_COLS Number of atomic synchronization chunks in the column dimension of the output matrix D"
  },
  {
    "id": 19231,
    "content": "cublasLtMatmulHeuristicResult_t  cublasLtMatmulHeuristicResult_t is a descriptor that holds the configured matrix multiplication algorithm descriptor and its runtime properties"
  },
  {
    "id": 19232,
    "content": "Member Description cublasLtMatmulAlgo_t algo Must be initialized with cublasLtMatmulAlgoInit() if the preference CUBLASLT_MATMUL_PERF_SEARCH_MODE is set to CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID Other fields are valid only if, after call to cublasLtMatmulAlgoGetHeuristic() , this member is set to CUBLAS_STATUS_SUCCESS"
  },
  {
    "id": 19239,
    "content": "cublasLtMatmulInnerShape_t  cublasLtMatmulInnerShape_t is an enumerated type used to configure various aspects of the internal kernel design"
  },
  {
    "id": 19244,
    "content": "cublasLtMatmulPreference_t  The cublasLtMatmulPreference_t is a pointer to an opaque structure holding the description of the preferences for cublasLtMatmulAlgoGetHeuristic() configuration"
  },
  {
    "id": 19245,
    "content": "Use cublasLtMatmulPreferenceCreate() to create one instance of the descriptor and cublasLtMatmulPreferenceDestroy() to destroy a previously created descriptor and release the resources"
  },
  {
    "id": 19249,
    "content": "cublasLtMatmulPreferenceAttributes_t  cublasLtMatmulPreferenceAttributes_t is an enumerated type used to apply algorithm search preferences while fine-tuning the heuristic function"
  },
  {
    "id": 19250,
    "content": "Use cublasLtMatmulPreferenceGetAttribute() and cublasLtMatmulPreferenceSetAttribute() to get and set the attribute value of a matmul preference descriptor"
  },
  {
    "id": 19251,
    "content": "Only algorithm configurations specifying CUBLASLT_ALGO_CONFIG_REDUCTION_SCHEME that is not masked out by this attribute are allowed"
  },
  {
    "id": 19253,
    "content": "uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_A_BYTES Minimum buffer alignment for matrix A (in bytes)"
  },
  {
    "id": 19254,
    "content": "Selecting a smaller value will exclude algorithms that can not work with matrix A, which is not as strictly aligned as the algorithms need"
  },
  {
    "id": 19255,
    "content": "uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_B_BYTES Minimum buffer alignment for matrix B (in bytes)"
  },
  {
    "id": 19256,
    "content": "Selecting a smaller value will exclude algorithms that can not work with matrix B, which is not as strictly aligned as the algorithms need"
  },
  {
    "id": 19257,
    "content": "uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_C_BYTES Minimum buffer alignment for matrix C (in bytes)"
  },
  {
    "id": 19258,
    "content": "Selecting a smaller value will exclude algorithms that can not work with matrix C, which is not as strictly aligned as the algorithms need"
  },
  {
    "id": 19259,
    "content": "uint32_t CUBLASLT_MATMUL_PREF_MIN_ALIGNMENT_D_BYTES Minimum buffer alignment for matrix D (in bytes)"
  },
  {
    "id": 19260,
    "content": "Selecting a smaller value will exclude algorithms that can not work with matrix D, which is not as strictly aligned as the algorithms need Selecting a non-zero value will exclude algorithms that report device utilization higher than specified"
  },
  {
    "id": 19261,
    "content": "cublasLtMatmulSearch_t  cublasLtMatmulSearch_t is an enumerated type that contains the attributes for heuristics search type Value Description Data Type CUBLASLT_SEARCH_BEST_FIT Request heuristics for the best algorithm for the given use case CUBLASLT_SEARCH_LIMITED_BY_ALGO_ID Request heuristics only for the pre-configured algo id"
  },
  {
    "id": 19265,
    "content": "cublasLtMatmulTile_t  cublasLtMatmulTile_t is an enumerated type used to set the tile size in rows x columns CUBLASLT_MATMUL_TILE_128x96 Tile size is 128 rows x 96 columns"
  },
  {
    "id": 19269,
    "content": "cublasLtMatmulStages_t  cublasLtMatmulStages_t is an enumerated type used to configure the size and number of shared memory buffers where input elements are staged"
  },
  {
    "id": 19270,
    "content": "CUBLASLT_MATMUL_STAGES_8xAUTO Stage size is 8, number of stages is selected automatically CUBLASLT_MATMUL_STAGES_16xAUTO Stage size is 16, number of stages is selected automatically CUBLASLT_MATMUL_STAGES_32xAUTO Stage size is 32, number of stages is selected automatically CUBLASLT_MATMUL_STAGES_64xAUTO Stage size is 64, number of stages is selected automatically CUBLASLT_MATMUL_STAGES_128xAUTO"
  },
  {
    "id": 19275,
    "content": "cublasLtNumericalImplFlags_t  cublasLtNumericalImplFlags_t : a set of bit-flags that can be specified to select implementation details that may affect numerical behavior of algorithms"
  },
  {
    "id": 19276,
    "content": "Value Description CUBLASLT_NUMERICAL_IMPL_FLAGS_FMA Specify that the implementation is based on [H,F,D]FMA (fused multiply-add) family instructions CUBLASLT_NUMERICAL_IMPL_FLAGS_HMMA Specify that the implementation is based on HMMA (tensor operation) family instructions CUBLASLT_NUMERICAL_IMPL_FLAGS_IMMA Specify that the implementation is based on IMMA (integer tensor operation) family"
  },
  {
    "id": 19277,
    "content": "instructions CUBLASLT_NUMERICAL_IMPL_FLAGS_DMMA Specify that the implementation is based on DMMA (double precision tensor operation) family instructions"
  },
  {
    "id": 19278,
    "content": "CUBLASLT_NUMERICAL_IMPL_FLAGS_TENSOR_OP_MASK Mask to filter implementations using any of the above kinds of tensor operations CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_TYPE_MASK Mask to filter implementation details about multiply-accumulate instructions used"
  },
  {
    "id": 19279,
    "content": "CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_16F Specify that the implementation’s inner dot product is using half precision accumulator CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32F Specify that the implementation’s inner dot product is using single precision accumulator CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_64F Specify that the implementation’s inner dot product is using double precision"
  },
  {
    "id": 19280,
    "content": "accumulator CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_32I Specify that the implementation’s inner dot product is using 32 bit signed integer precision accumulator CUBLASLT_NUMERICAL_IMPL_FLAGS_ACCUMULATOR_TYPE_MASK Mask to filter implementation details about accumulator used CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16F Specify that the implementation’s inner dot product multiply-accumulate instruction"
  },
  {
    "id": 19281,
    "content": "is using half-precision inputs CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_16BF Specify that the implementation’s inner dot product multiply-accumulate instruction is using bfloat16 inputs CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_TF32 Specify that the implementation’s inner dot product multiply-accumulate instruction is using TF32 inputs CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_32F Specify that the implementation’s"
  },
  {
    "id": 19282,
    "content": "inner dot product multiply-accumulate instruction is using single-precision inputs CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_64F Specify that the implementation’s inner dot product multiply-accumulate instruction is using double-precision inputs CUBLASLT_NUMERICAL_IMPL_FLAGS_INPUT_8I Specify that the implementation’s inner dot product multiply-accumulate instruction is using 8-bit integer inputs"
  },
  {
    "id": 19283,
    "content": "CUBLASLT_NUMERICAL_IMPL_FLAGS_OP_INPUT_TYPE_MASK Mask to filter implementation details about accumulator input used"
  },
  {
    "id": 19284,
    "content": "CUBLASLT_NUMERICAL_IMPL_FLAGS_GAUSSIAN Specify that the implementation applies Gauss complexity reduction algorithm to reduce arithmetic complexity of the complex matrix multiplication problem 3"
  },
  {
    "id": 19287,
    "content": "cublasLtMatrixLayout_t  The cublasLtMatrixLayout_t is a pointer to an opaque structure holding the description of a matrix layout"
  },
  {
    "id": 19288,
    "content": "Use cublasLtMatrixLayoutCreate() to create one instance of the descriptor and cublasLtMatrixLayoutDestroy() to destroy a previously created descriptor and release the resources"
  },
  {
    "id": 19292,
    "content": "cublasLtMatrixLayoutAttribute_t  cublasLtMatrixLayoutAttribute_t is a descriptor structure containing the attributes that define the details of the matrix operation"
  },
  {
    "id": 19293,
    "content": "Use cublasLtMatrixLayoutGetAttribute() and cublasLtMatrixLayoutSetAttribute() to get and set the attribute value of a matrix layout descriptor"
  },
  {
    "id": 19294,
    "content": "Attribute Name Description Data Type CUBLASLT_MATRIX_LAYOUT_TYPE Specifies the data precision type uint32_t CUBLASLT_MATRIX_LAYOUT_ORDER Specifies the memory order of the data of the matrix"
  },
  {
    "id": 19299,
    "content": "int32_t CUBLASLT_MATRIX_LAYOUT_STRIDED_BATCH_OFFSET Stride (in elements) to the next matrix for the strided batch operation"
  },
  {
    "id": 19301,
    "content": "= 0), batch stride is interpreted by cublasLtMatmul() in number of real valued sub-elements for data of type CUDA_C_16F, offset of 1024B is encoded as a stride of value 512 (since each element of the real and imaginary matrices is a 2B (16bit) floating point type) NOTE: A bug in cublasLtMatrixTransform() causes it to interpret the batch stride for a planar-complex matrix as if it was specified in"
  },
  {
    "id": 19302,
    "content": "number of complex elements Therefore an offset of 1024B must be encoded as stride value 256 when calling cublasLtMatrixTransform() (each complex element is 4B with real and imaginary values 2B each) int64_t CUBLASLT_MATRIX_LAYOUT_PLANE_OFFSET Stride (in bytes) to the imaginary plane for planar-complex layout Default value is 0, indicating that the layout is regular (real and imaginary parts of"
  },
  {
    "id": 19304,
    "content": "cublasLtMatrixTransformDesc_t  The cublasLtMatrixTransformDesc_t is a pointer to an opaque structure holding the description of a matrix transformation operation"
  },
  {
    "id": 19305,
    "content": "Use cublasLtMatrixTransformDescCreate() to create one instance of the descriptor and cublasLtMatrixTransformDescDestroy() to destroy a previously created descriptor and release the resources"
  },
  {
    "id": 19309,
    "content": "cublasLtMatrixTransformDescAttributes_t  cublasLtMatrixTransformDescAttributes_t is a descriptor structure containing the attributes that define the specifics of the matrix transform operation Use cublasLtMatrixTransformDescGetAttribute() and cublasLtMatrixTransformDescSetAttribute() to set the attribute value of a matrix transform descriptor Transform Attribute Name Description Data Type"
  },
  {
    "id": 19310,
    "content": "CUBLASLT_MATRIX_TRANSFORM_DESC_SCALE_TYPE Scale type Inputs are converted to the scale type for scaling and summation, and results are then converted to the output type to store in the memory"
  },
  {
    "id": 19311,
    "content": "int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_POINTER_MODE Specifies the scalars alpha and beta are passed by reference whether on the host or on the device int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSA Specifies the type of operation that should be performed on the matrix A int32_t CUBLASLT_MATRIX_TRANSFORM_DESC_TRANSB Specifies the type of operation that should be performed on the matrix B"
  },
  {
    "id": 19312,
    "content": "cublasLtOrder_t  cublasLtOrder_t is an enumerated type used to indicate the data ordering of the matrix"
  },
  {
    "id": 19313,
    "content": "The leading dimension is the stride (in elements) to the beginning of next column in memory The leading dimension is the stride (in elements) to the beginning of next group of 32-columns For example, if the matrix has 33 columns and 2 rows, then the leading dimension must be at least (32) * 2 = 64 CUBLASLT_ORDER_COL4_4R2_8C Data is ordered in column-major ordered tiles of composite tiles with"
  },
  {
    "id": 19314,
    "content": "total 32 columns and 8 rows A tile is composed of interleaved inner tiles of 4 columns within 4 even or odd rows in an alternating pattern The leading dimension is the stride (in elements) to the beginning of the first 32 column x 8 row tile for the next 32-wide group of columns For example, if the matrix has 33 columns and 1 row, the leading dimension must be at least (32 * 8) * 1 = 256"
  },
  {
    "id": 19315,
    "content": "CUBLASLT_ORDER_COL32_2R_4R4 Data is ordered in column-major ordered tiles of composite tiles with total 32 columns ands 32 rows Leading dimension is the stride (in elements) to the beginning of the first 32 column x 32 row tile for the next 32-wide group of columns if matrix has 33 columns and 1 row, ld must be at least (32*32)*1 = 1024"
  },
  {
    "id": 19319,
    "content": "cublasLtPointerMode_t  cublasLtPointerMode_t is an enumerated type used to set the pointer mode for the scaling factors alpha and beta"
  },
  {
    "id": 19320,
    "content": "Value Description CUBLASLT_POINTER_MODE_HOST = CUBLAS_POINTER_MODE_HOST Matches CUBLAS_POINTER_MODE_HOST, and the pointer targets a single value host memory CUBLASLT_POINTER_MODE_DEVICE = CUBLAS_POINTER_MODE_DEVICE Matches CUBLAS_POINTER_MODE_DEVICE, and the pointer targets a single value device memory"
  },
  {
    "id": 19321,
    "content": "CUBLASLT_POINTER_MODE_DEVICE_VECTOR = 2 Pointers target device memory vectors of length equal to the number of rows of matrix D CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO = 3 alpha pointer targets a device memory vector of length equal to the number of rows of matrix D, and beta is zero CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST = 4 alpha pointer targets a device memory vector"
  },
  {
    "id": 19326,
    "content": "cublasLtPointerModeMask_t  cublasLtPointerModeMask_t is an enumerated type used to define and query the pointer mode capability"
  },
  {
    "id": 19327,
    "content": "Value Description CUBLASLT_POINTER_MODE_MASK_HOST = 1 See CUBLASLT_POINTER_MODE_HOST in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_DEVICE = 2 See CUBLASLT_POINTER_MODE_DEVICE in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_DEVICE_VECTOR = 4 See CUBLASLT_POINTER_MODE_DEVICE_VECTOR in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_ZERO = 8 See"
  },
  {
    "id": 19328,
    "content": "CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO in cublasLtPointerMode_t CUBLASLT_POINTER_MODE_MASK_ALPHA_DEVICE_VECTOR_BETA_HOST = 16 See CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST in cublasLtPointerMode_t 3"
  },
  {
    "id": 19331,
    "content": "cublasLtReductionScheme_t  cublasLtReductionScheme_t is an enumerated type used to specify a reduction scheme for the portions of the dot-product calculated in parallel (i"
  },
  {
    "id": 19334,
    "content": "CUBLASLT_REDUCTION_SCHEME_INPLACE Reduction is performed “in place” using the output buffer, parts are added up in the output data type CUBLASLT_REDUCTION_SCHEME_COMPUTE_TYPE Reduction done out of place in a user-provided workspace The intermediate results are stored in the compute type in the workspace and reduced in a separate step CUBLASLT_REDUCTION_SCHEME_OUTPUT_TYPE Reduction done out of"
  },
  {
    "id": 19335,
    "content": "place in a user-provided workspace The intermediate results are stored in the output type in the workspace and reduced in a separate step"
  },
  {
    "id": 19342,
    "content": "cublasLtCreate()  cublasStatus_t cublasLtCreate ( cublasLtHandle_t * lighthandle ) This function initializes the cuBLASLt library and creates a handle to an opaque structure holding the cuBLASLt library context It allocates light hardware resources on the host and device, and must be called prior to making any other cuBLASLt library calls To use the library on multiple devices, one cuBLASLt"
  },
  {
    "id": 19343,
    "content": "handle should be created for each device Parameters: Parameter Memory Input / Output Description lightHandle Output Pointer to the allocated cuBLASLt handle for the created cuBLASLt context"
  },
  {
    "id": 19345,
    "content": "This usually happens: when cublasLtCreate() is not called first an error in the CUDA Runtime API called by the cuBLASLt routine, or an error in the hardware setup"
  },
  {
    "id": 19346,
    "content": "To correct: prior to the function call, deallocate the previously allocated memory as much as possible"
  },
  {
    "id": 19347,
    "content": "CUBLAS_STATUS_INVALID_VALUE lighthandle == NULL See cublasStatus_t for a complete list of valid return codes"
  },
  {
    "id": 19351,
    "content": "cublasLtDestroy()  cublasStatus_t cublasLtDestroy ( cublasLtHandle_t lightHandle ) This function releases hardware resources used by the cuBLASLt library Because cublasLtCreate() allocates some internal resources and the release of those resources by calling cublasLtDestroy() will implicitly call cudaDeviceSynchronize() , it is recommended to minimize the number of times these functions are"
  },
  {
    "id": 19352,
    "content": "called Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the cuBLASLt handle to be destroyed Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The cuBLASLt context was successfully destroyed CUBLAS_STATUS_INVALID_VALUE lightHandle == NULL See cublasStatus_t for a complete list of valid return codes"
  },
  {
    "id": 19356,
    "content": "cublasLtDisableCpuInstructionsSetMask()  unsigned cublasLtDisableCpuInstructionsSetMask ( unsigned mask ); Instructs cuBLASLt library to not use CPU instructions specified by the flags in the mask"
  },
  {
    "id": 19358,
    "content": "Parameters: mask – the flags combined with bitwise OR(|) operator that specify which CPU instructions should not be used Returns: the previous value of the mask"
  },
  {
    "id": 19362,
    "content": "cublasLtGetCudartVersion()  size_t cublasLtGetCudartVersion ( void ); This function returns the version number of the CUDA Runtime library Returns: size_t - The version number of the CUDA Runtime library"
  },
  {
    "id": 19366,
    "content": "cublasLtGetProperty()  cublasStatus_t cublasLtGetProperty ( libraryPropertyType type , int * value ); This function returns the value of the requested property by writing it to the memory location pointed to by the value parameter Parameters : Parameter Memory Input / Output Description type Input Of the type libraryPropertyType , whose value is requested from the property value Output Pointer"
  },
  {
    "id": 19367,
    "content": "to the host memory location where the requested information should be written Returns : Return Value Meaning CUBLAS_STATUS_SUCCESS The requested libraryPropertyType information is successfully written at the provided address CUBLAS_STATUS_INVALID_VALUE If invalid value of the type input argument or value == NULL See cublasStatus_t for a complete list of valid return codes"
  },
  {
    "id": 19371,
    "content": "cublasLtGetStatusName()  const char * cublasLtGetStatusName ( cublasStatus_t status ); Returns the string representation of a given status Returns: const char* - the NULL-terminated string"
  },
  {
    "id": 19375,
    "content": "cublasLtGetStatusString()  const char * cublasLtGetStatusString ( cublasStatus_t status ); Returns the description string for a given status"
  },
  {
    "id": 19379,
    "content": "cublasLtHeuristicsCacheGetCapacity()  cublasStatus_t cublasLtHeuristicsCacheGetCapacity ( size_t * capacity ); Returns the Heuristics Cache capacity Returns: Return Value Description CUBLAS_STATUS_SUCCESS The capacity was successfully written CUBLAS_STATUS_INVALID_VALUE The capacity was successfully set"
  },
  {
    "id": 19383,
    "content": "cublasLtHeuristicsCacheSetCapacity()  cublasStatus_t cublasLtHeuristicsCacheSetCapacity ( size_t capacity ); Sets the Heuristics Cache capacity"
  },
  {
    "id": 19389,
    "content": "cublasLtGetVersion()  size_t cublasLtGetVersion ( void ); This function returns the version number of cuBLASLt library Returns: size_t - The version number of cuBLASLt library"
  },
  {
    "id": 19393,
    "content": "cublasLtLoggerSetCallback()  cublasStatus_t cublasLtLoggerSetCallback ( cublasLtLoggerCallback_t callback ); Experimental: This function sets the logging callback function Parameters : Parameter Memory Input / Output Description callback Input Pointer to a callback function Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the callback function was successfully set"
  },
  {
    "id": 19398,
    "content": "cublasLtLoggerSetFile()  cublasStatus_t cublasLtLoggerSetFile ( FILE * file ); Experimental: This function sets the logging output file Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle Parameters : Parameter Memory Input / Output Description file Input Pointer to an open file"
  },
  {
    "id": 19403,
    "content": "cublasLtLoggerOpenFile()  cublasStatus_t cublasLtLoggerOpenFile ( const char * logFile ); Experimental: This function opens a logging output file in the given path Parameters : Parameter Memory Input / Output Description logFile Input Path of the logging output file Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging file was successfully opened"
  },
  {
    "id": 19407,
    "content": "cublasLtLoggerSetLevel()  cublasStatus_t cublasLtLoggerSetLevel ( int level ); Experimental: This function sets the value of the logging level Parameters : Parameter Memory Input / Output Description level Input Value of the logging level Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If the value was not a valid logging level CUBLAS_STATUS_SUCCESS If the logging level was"
  },
  {
    "id": 19412,
    "content": "cublasLtLoggerSetMask()  cublasStatus_t cublasLtLoggerSetMask ( int mask ); Experimental: This function sets the value of the logging mask Parameters : Parameter Memory Input / Output Description mask Input Value of the logging mask Returns : Return Value Description CUBLAS_STATUS_SUCCESS If the logging mask was successfully set"
  },
  {
    "id": 19416,
    "content": "cublasLtLoggerForceDisable()  cublasStatus_t cublasLtLoggerForceDisable (); Experimental: This function disables logging for the entire run"
  },
  {
    "id": 19421,
    "content": "cublasLtMatmul()  cublasStatus_t cublasLtMatmul ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t computeDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * B , cublasLtMatrixLayout_t Bdesc , const void * beta , const void * C , cublasLtMatrixLayout_t Cdesc , void * D , cublasLtMatrixLayout_t Ddesc , const cublasLtMatmulAlgo_t * algo , void * workspace ,"
  },
  {
    "id": 19422,
    "content": "size_t workspaceSizeInBytes , cudaStream_t stream ); This function computes the matrix multiplication of matrices A and B to produce the output matrix D, according to the following operation: D = alpha*(A*B) + beta*(C), where A , B , and C are input matrices, and alpha and beta are input scalars Note This function supports both in-place matrix multiplication ( C == D and Cdesc == Ddesc ) and"
  },
  {
    "id": 19424,
    "content": "= D , both matrices must have the same data type, number of rows, number of columns, batch size, and memory order)"
  },
  {
    "id": 19425,
    "content": "In the out-of-place case, the leading dimension of C can be different from the leading dimension of D"
  },
  {
    "id": 19426,
    "content": "The recommendations on workspaceSizeInBytes are the same as mentioned in the cublasSetWorkspace() section"
  },
  {
    "id": 19427,
    "content": "Datatypes Supported: cublasLtMatmul() supports the following computeType, scaleType, Atype/Btype, and Ctype When A, B, C, and D are Regular Column- or Row-major Matrices  computeType scaleType Atype/Btype Ctype Bias Type 5 CUBLAS_COMPUTE_16F or CUBLAS_COMPUTE_16F_PEDANTIC CUDA_R_16F CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I"
  },
  {
    "id": 19428,
    "content": "CUDA_R_32I Non-default epilogue not supported CUBLAS_COMPUTE_32F or CUBLAS_COMPUTE_32F_PEDANTIC CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_16BF 5 CUDA_R_16F CUDA_R_16F CUDA_R_16F 5 CUDA_R_8I CUDA_R_32F Non-default epilogue not supported CUDA_R_16BF CUDA_R_32F CUDA_R_32F 5 CUDA_R_16F CUDA_R_32F CUDA_R_32F 5 CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_8I 6 CUDA_C_32F 6 Non-default epilogue"
  },
  {
    "id": 19429,
    "content": "not supported CUDA_C_32F 6 CUDA_C_32F 6 CUBLAS_COMPUTE_32F_FAST_16F or CUBLAS_COMPUTE_32F_FAST_16BF or CUBLAS_COMPUTE_32F_FAST_TF32 CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F 5 CUDA_C_32F 6 CUDA_C_32F 6 CUDA_C_32F 6 Non-default epilogue not supported"
  },
  {
    "id": 19430,
    "content": "CUBLAS_COMPUTE_64F or CUBLAS_COMPUTE_64F_PEDANTIC CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F 5 CUDA_C_64F 6 CUDA_C_64F 6 CUDA_C_64F 6 Non-default epilogue not supported"
  },
  {
    "id": 19431,
    "content": "To use IMMA kernels, one of the following sets of requirements, with the first being the preferred one, must be met: Using a regular data ordering: All matrix pointers must be 4-byte aligned"
  },
  {
    "id": 19432,
    "content": "Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_HOST"
  },
  {
    "id": 19433,
    "content": "With the latter mode, the kernels support the CUBLASLT_MATMUL_DESC_ALPHA_VECTOR_BATCH_STRIDE attribute"
  },
  {
    "id": 19434,
    "content": "Using the IMMA-specific data ordering on Ampere or Turing (but not Hopper) architecture - CUBLASLT_ORDER_COL32` for matrices A, C, D, and CUBLASLT_ORDER_COL4_4R2_8C (on Turing or Ampere architecture) or CUBLASLT_ORDER_COL32_2R_4R4 (on Ampere architecture) for matrix B: Leading dimensions of matrices A, B, C must fulfill conditions specific to the memory ordering (see cublasLtOrder_t )"
  },
  {
    "id": 19437,
    "content": "Pointer mode can be CUBLASLT_POINTER_MODE_HOST , CUBLASLT_POINTER_MODE_DEVICE , CUBLASLT_POINTER_MODE_DEVICE_VECTOR or CUBLASLT_POINTER_MODE_ALPHA_DEVICE_VECTOR_BETA_ZERO"
  },
  {
    "id": 19438,
    "content": "When A, B, C, and D Use Layouts for IMMA  computeType scaleType Atype/Btype Ctype Bias Type CUBLAS_COMPUTE_32I or CUBLAS_COMPUTE_32I_PEDANTIC CUDA_R_32I CUDA_R_8I CUDA_R_32I Non-default epilogue not supported"
  },
  {
    "id": 19439,
    "content": "CUDA_R_32F CUDA_R_8I CUDA_R_8I CUDA_R_32F To use FP8 kernels, the following set of requirements must be satisfied: All matrix dimensions must meet the optimal requirements listed in Tensor Core Usage (i"
  },
  {
    "id": 19441,
    "content": "When A, B, C, and D are Planar-Complex Matrices  computeType scaleType Atype/Btype Ctype CUBLAS_COMPUTE_32F CUDA_C_32F CUDA_C_16F 6 CUDA_C_16F 6 CUDA_C_32F 6 CUDA_C_16BF 6 CUDA_C_16BF 6 CUDA_C_32F 6 NOTES: 5 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 ) ReLU, dReLu, GELU, dGELU and Bias epilogue modes (see"
  },
  {
    "id": 19442,
    "content": "CUBLASLT_MATMUL_DESC_EPILOGUE in cublasLtMatmulDescAttributes_t ) are not supported when D matrix memory order is defined as CUBLASLT_ORDER_ROW"
  },
  {
    "id": 19443,
    "content": "For best performance when using the bias vector, specify zero beta and set pointer mode to CUBLASLT_POINTER_MODE_HOST"
  },
  {
    "id": 19444,
    "content": "6 ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 ) Use of CUBLAS_ORDER_ROW together with CUBLAS_OP_C (Hermitian operator) is not supported unless all of A, B, C, and D matrices use the CUBLAS_ORDER_ROW ordering"
  },
  {
    "id": 19445,
    "content": "Parameters: Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context"
  },
  {
    "id": 19446,
    "content": "computeDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t"
  },
  {
    "id": 19447,
    "content": "A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc, Bdesc and Cdesc"
  },
  {
    "id": 19449,
    "content": "When NULL, an implicit heuritics query with default search preferences will be performed to determine actual algorithm to use"
  },
  {
    "id": 19450,
    "content": "Returns: Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized"
  },
  {
    "id": 19451,
    "content": "CUBLAS_STATUS_INVALID_VALUE If the parameters are unexpectedly NULL, in conflict or in an impossible configuration"
  },
  {
    "id": 19452,
    "content": "For example, when workspaceSizeInBytes is less than workspace required by the configured algo CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device doesn’t support the configured operation CUBLAS_STATUS_ARCH_MISMATCH If the configured operation cannot be run using the selected device CUBLAS_STATUS_SUCCESS If the operation completed successfully"
  },
  {
    "id": 19456,
    "content": "cublasLtMatmulAlgoCapGetAttribute()  cublasStatus_t cublasLtMatmulAlgoCapGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoCapAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried capability attribute for an initialized cublasLtMatmulAlgo_t descriptor structure The capability attribute value is retrieved"
  },
  {
    "id": 19457,
    "content": "from the enumerated type cublasLtMatmulAlgoCapAttributes_t For example, to get list of supported Tile IDs: cublasLtMatmulTile_t tiles [ CUBLASLT_MATMUL_TILE_END ]; size_t num_tiles , size_written ; if ( cublasLtMatmulAlgoCapGetAttribute ( algo , CUBLASLT_ALGO_CAP_TILE_IDS , tiles , sizeof ( tiles ), & size_written ) == CUBLAS_STATUS_SUCCESS ) { num_tiles = size_written / sizeof ( tiles [ 0 ]);}"
  },
  {
    "id": 19458,
    "content": "Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor If sizeInBytes is non-zero: then sizeWritten is the number of bytes actually written; if sizeInBytes is 0: then sizeWritten is the number of bytes needed to write full contents"
  },
  {
    "id": 19459,
    "content": "It checks whether the descriptor is supported on the current device, and returns the result containing the required workspace and the calculated wave count"
  },
  {
    "id": 19460,
    "content": "Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context"
  },
  {
    "id": 19461,
    "content": "operationDesc Input Handle to a previously created matrix multiplication descriptor of type cublasLtMatmulDesc_t Adesc, Bdesc, Cdesc, and Ddesc Input Handles to the previously created matrix layout descriptors of the type cublasLtMatrixLayout_t algo Input Descriptor which specifies which matrix multiplication algorithm should be used Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE"
  },
  {
    "id": 19462,
    "content": "If matrix layout descriptors or the operation descriptor do not match the algo descriptor CUBLAS_STATUS_NOT_SUPPORTED If the algo configuration or data type combination is not currently supported on the given device CUBLAS_STATUS_ARCH_MISMATCH If the algo configuration cannot be run using the selected device"
  },
  {
    "id": 19467,
    "content": "cublasLtMatmulAlgoConfigGetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigGetAttribute ( const cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor The configuration attribute value is"
  },
  {
    "id": 19469,
    "content": "Parameters : Parameter Memory Input / Output Description algo Input Pointer to the previously created opaque structure holding the matrix multiply algorithm descriptor"
  },
  {
    "id": 19474,
    "content": "cublasLtMatmulAlgoConfigSetAttribute()  cublasStatus_t cublasLtMatmulAlgoConfigSetAttribute ( cublasLtMatmulAlgo_t * algo , cublasLtMatmulAlgoConfigAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified configuration attribute for an initialized cublasLtMatmulAlgo_t descriptor The configuration attribute is an enumerant of the type"
  },
  {
    "id": 19475,
    "content": "cublasLtMatmulAlgoConfigAttributes_t Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes doesn’t match the size of the internal storage for the selected attribute CUBLAS_STATUS_SUCCESS If the attribute was set successfully"
  },
  {
    "id": 19479,
    "content": "cublasLtMatmulAlgoGetHeuristic()  cublasStatus_t cublasLtMatmulAlgoGetHeuristic ( cublasLtHandle_t lightHandle , cublasLtMatmulDesc_t operationDesc , cublasLtMatrixLayout_t Adesc , cublasLtMatrixLayout_t Bdesc , cublasLtMatrixLayout_t Cdesc , cublasLtMatrixLayout_t Ddesc , cublasLtMatmulPreference_t preference , int requestedAlgoCount , cublasLtMatmulHeuristicResult_t heuristicResultsArray []"
  },
  {
    "id": 19480,
    "content": "int * returnAlgoCount ); This function retrieves the possible algorithms for the matrix multiply operation cublasLtMatmul() function with the given input matrices A, B and C, and the output matrix D"
  },
  {
    "id": 19483,
    "content": "heuristicResultsArray[] Output Array containing the algorithm heuristics and associated runtime characteristics, returned by this function, in the order of increasing estimated compute time"
  },
  {
    "id": 19484,
    "content": "Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If requestedAlgoCount is less or equal to zero"
  },
  {
    "id": 19487,
    "content": "Note This function may load some kernels using CUDA Driver API which may fail when there is no available GPU memory"
  },
  {
    "id": 19492,
    "content": "cublasLtMatmulAlgoGetIds()  cublasStatus_t cublasLtMatmulAlgoGetIds ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int requestedAlgoCount , int algoIdsArray [], int * returnAlgoCount ); This function retrieves the IDs of all the matrix multiply algorithms"
  },
  {
    "id": 19493,
    "content": "that are valid, and can potentially be run by the cublasLtMatmul() function, for given types of the input matrices A, B and C, and of the output matrix D"
  },
  {
    "id": 19494,
    "content": "To make sure the best possible algo is contained in the list, make requestedAlgoCount large enough to receive the full list"
  },
  {
    "id": 19499,
    "content": "cublasLtMatmulAlgoInit()  cublasStatus_t cublasLtMatmulAlgoInit ( cublasLtHandle_t lightHandle , cublasComputeType_t computeType , cudaDataType_t scaleType , cudaDataType_t Atype , cudaDataType_t Btype , cudaDataType_t Ctype , cudaDataType_t Dtype , int algoId , cublasLtMatmulAlgo_t * algo ); This function initializes the matrix multiply algorithm structure for the cublasLtMatmul() , for a"
  },
  {
    "id": 19501,
    "content": "Parameters : Parameter Memory Input / Output Description lightHandle Input Pointer to the allocated cuBLASLt handle for the cuBLASLt context Atype, Btype, Ctype, and Dtype Input Datatype precision for the input and output matrices"
  },
  {
    "id": 19502,
    "content": "Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If algo is NULL or algoId is outside the recognized range CUBLAS_STATUS_NOT_SUPPORTED If algoId is not supported for given combination of data types"
  },
  {
    "id": 19507,
    "content": "cublasLtMatmulDescCreate()  cublasStatus_t cublasLtMatmulDescCreate ( cublasLtMatmulDesc_t * matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function creates a matrix multiply descriptor by allocating the memory needed to hold its opaque structure Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix"
  },
  {
    "id": 19508,
    "content": "multiply descriptor created by this function computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function creates scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function creates"
  },
  {
    "id": 19514,
    "content": "cublasLtMatmulDescInit()  cublasStatus_t cublasLtMatmulDescInit ( cublasLtMatmulDesc_t matmulDesc , cublasComputeType_t computeType , cudaDataType_t scaleType ); This function initializes a matrix multiply descriptor in a previously allocated one Parameters : Parameter Memory Input / Output Description matmulDesc Output Pointer to the structure holding the matrix multiply descriptor initialized"
  },
  {
    "id": 19515,
    "content": "by this function computeType Input Enumerant that specifies the data precision for the matrix multiply descriptor this function initializes scaleType Input Enumerant that specifies the data precision for the matrix transform descriptor this function initializes"
  },
  {
    "id": 19519,
    "content": "cublasLtMatmulDescDestroy()  cublasStatus_t cublasLtMatmulDescDestroy ( cublasLtMatmulDesc_t matmulDesc ); This function destroys a previously created matrix multiply descriptor object Parameters : Parameter Memory Input / Output Description matmulDesc Input Pointer to the structure holding the matrix multiply descriptor that should be destroyed by this function"
  },
  {
    "id": 19524,
    "content": "cublasLtMatmulDescGetAttribute()  cublasStatus_t cublasLtMatmulDescGetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply descriptor Parameters : Parameter Memory Input / Output Description matmulDesc"
  },
  {
    "id": 19525,
    "content": "Input Pointer to the previously created structure holding the matrix multiply descriptor queried by this function buf Output Memory address containing the attribute value retrieved by this function"
  },
  {
    "id": 19529,
    "content": "cublasLtMatmulDescSetAttribute()  cublasStatus_t cublasLtMatmulDescSetAttribute ( cublasLtMatmulDesc_t matmulDesc , cublasLtMatmulDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply descriptor buf Input The value to which the specified attribute should be set"
  },
  {
    "id": 19533,
    "content": "cublasLtMatmulPreferenceCreate()  cublasStatus_t cublasLtMatmulPreferenceCreate ( cublasLtMatmulPreference_t * pref ); This function creates a matrix multiply heuristic search preferences descriptor by allocating the memory needed to hold its opaque structure Parameters : Parameter Memory Input / Output Description pref Output Pointer to the structure holding the matrix multiply preferences"
  },
  {
    "id": 19539,
    "content": "cublasLtMatmulPreferenceInit()  cublasStatus_t cublasLtMatmulPreferenceInit ( cublasLtMatmulPreference_t pref ); This function initializes a matrix multiply heuristic search preferences descriptor in a previously allocated one"
  },
  {
    "id": 19543,
    "content": "cublasLtMatmulPreferenceDestroy()  cublasStatus_t cublasLtMatmulPreferenceDestroy ( cublasLtMatmulPreference_t pref ); This function destroys a previously created matrix multiply preferences descriptor object Parameters : Parameter Memory Input / Output Description pref Input Pointer to the structure holding the matrix multiply preferences descriptor that should be destroyed by this function"
  },
  {
    "id": 19548,
    "content": "cublasLtMatmulPreferenceGetAttribute()  cublasStatus_t cublasLtMatmulPreferenceGetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix multiply heuristic search preferences descriptor Parameters :"
  },
  {
    "id": 19549,
    "content": "Parameter Memory Input / Output Description pref Input Pointer to the previously created structure holding the matrix multiply heuristic search preferences descriptor queried by this function See cublasLtMatmulPreferenceAttributes_t"
  },
  {
    "id": 19553,
    "content": "cublasLtMatmulPreferenceSetAttribute()  cublasStatus_t cublasLtMatmulPreferenceSetAttribute ( cublasLtMatmulPreference_t pref , cublasLtMatmulPreferenceAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix multiply preferences descriptor Parameters : Parameter Memory Input / Output Description"
  },
  {
    "id": 19554,
    "content": "pref Input Pointer to the previously created structure holding the matrix multiply preferences descriptor queried by this function"
  },
  {
    "id": 19558,
    "content": "cublasLtMatrixLayoutCreate()  cublasStatus_t cublasLtMatrixLayoutCreate ( cublasLtMatrixLayout_t * matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function creates a matrix layout descriptor by allocating the memory needed to hold its opaque structure Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the"
  },
  {
    "id": 19559,
    "content": "matrix layout descriptor created by this function type Input Enumerant that specifies the data precision for the matrix layout descriptor this function creates"
  },
  {
    "id": 19564,
    "content": "cublasLtMatrixLayoutInit()  cublasStatus_t cublasLtMatrixLayoutInit ( cublasLtMatrixLayout_t matLayout , cudaDataType type , uint64_t rows , uint64_t cols , int64_t ld ); This function initializes a matrix layout descriptor in a previously allocated one Parameters : Parameter Memory Input / Output Description matLayout Output Pointer to the structure holding the matrix layout descriptor"
  },
  {
    "id": 19565,
    "content": "initialized by this function type Input Enumerant that specifies the data precision for the matrix layout descriptor this function initializes"
  },
  {
    "id": 19569,
    "content": "cublasLtMatrixLayoutDestroy()  cublasStatus_t cublasLtMatrixLayoutDestroy ( cublasLtMatrixLayout_t matLayout ); This function destroys a previously created matrix layout descriptor object Parameters : Parameter Memory Input / Output Description matLayout Input Pointer to the structure holding the matrix layout descriptor that should be destroyed by this function"
  },
  {
    "id": 19573,
    "content": "cublasLtMatrixLayoutGetAttribute()  cublasStatus_t cublasLtMatrixLayoutGetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to the specified matrix layout descriptor Parameters : Parameter Memory Input / Output Description matLayout"
  },
  {
    "id": 19574,
    "content": "Input Pointer to the previously created structure holding the matrix layout descriptor queried by this function See cublasLtMatrixLayoutAttribute_t"
  },
  {
    "id": 19578,
    "content": "cublasLtMatrixLayoutSetAttribute()  cublasStatus_t cublasLtMatrixLayoutSetAttribute ( cublasLtMatrixLayout_t matLayout , cublasLtMatrixLayoutAttribute_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix layout descriptor Returns : Return Value Description CUBLAS_STATUS_INVALID_VALUE If buf is NULL or"
  },
  {
    "id": 19584,
    "content": "cublasLtMatrixTransform()  cublasStatus_t cublasLtMatrixTransform ( cublasLtHandle_t lightHandle , cublasLtMatrixTransformDesc_t transformDesc , const void * alpha , const void * A , cublasLtMatrixLayout_t Adesc , const void * beta , const void * B , cublasLtMatrixLayout_t Bdesc , void * C , cublasLtMatrixLayout_t Cdesc , cudaStream_t stream ); This function computes the matrix transformation"
  },
  {
    "id": 19585,
    "content": "operation on the input matrices A and B, to produce the output matrix C, according to the below operation: C = alpha*transformation(A) + beta*transformation(B), where A , B are input matrices, and alpha and beta are input scalars"
  },
  {
    "id": 19588,
    "content": "A, B, and C Device Input Pointers to the GPU memory associated with the corresponding descriptors Adesc , Bdesc and Cdesc Adesc or Bdesc can be NULL if corresponding pointer is NULL and corresponding scalar is zero"
  },
  {
    "id": 19589,
    "content": "Returns : Return Value Description CUBLAS_STATUS_NOT_INITIALIZED If cuBLASLt handle has not been initialized"
  },
  {
    "id": 19591,
    "content": "CUBLAS_STATUS_NOT_SUPPORTED If the current implementation on the selected device does not support the configured operation"
  },
  {
    "id": 19595,
    "content": "cublasLtMatrixTransformDescCreate()  cublasStatus_t cublasLtMatrixTransformDescCreate ( cublasLtMatrixTransformDesc_t * transformDesc , cudaDataType scaleType ); This function creates a matrix transform descriptor by allocating the memory needed to hold its opaque structure Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix"
  },
  {
    "id": 19600,
    "content": "cublasLtMatrixTransformDescInit()  cublasStatus_t cublasLtMatrixTransformDescInit ( cublasLtMatrixTransformDesc_t transformDesc , cudaDataType scaleType ); This function initializes a matrix transform descriptor in a previously allocated one Parameters : Parameter Memory Input / Output Description transformDesc Output Pointer to the structure holding the matrix transform descriptor initialized"
  },
  {
    "id": 19605,
    "content": "cublasLtMatrixTransformDescDestroy()  cublasStatus_t cublasLtMatrixTransformDescDestroy ( cublasLtMatrixTransformDesc_t transformDesc ); This function destroys a previously created matrix transform descriptor object Parameters : Parameter Memory Input / Output Description transformDesc Input Pointer to the structure holding the matrix transform descriptor that should be destroyed by this function"
  },
  {
    "id": 19609,
    "content": "cublasLtMatrixTransformDescGetAttribute()  cublasStatus_t cublasLtMatrixTransformDescGetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , void * buf , size_t sizeInBytes , size_t * sizeWritten ); This function returns the value of the queried attribute belonging to a previously created matrix transform descriptor Parameters : Parameter"
  },
  {
    "id": 19610,
    "content": "Memory Input / Output Description transformDesc Input Pointer to the previously created structure holding the matrix transform descriptor queried by this function See cublasLtMatrixTransformDescAttributes_t"
  },
  {
    "id": 19614,
    "content": "cublasLtMatrixTransformDescSetAttribute()  cublasStatus_t cublasLtMatrixTransformDescSetAttribute ( cublasLtMatrixTransformDesc_t transformDesc , cublasLtMatrixTransformDescAttributes_t attr , const void * buf , size_t sizeInBytes ); This function sets the value of the specified attribute belonging to a previously created matrix transform descriptor Returns : Return Value Description"
  },
  {
    "id": 19615,
    "content": "CUBLAS_STATUS_INVALID_VALUE If buf is NULL or sizeInBytes does not match size of the internal storage for the selected attribute"
  },
  {
    "id": 19619,
    "content": "General description  The cuBLASXt API of cuBLAS exposes a multi-GPU capable host interface: when using this API the application only needs to allocate the required matrices on the host memory space"
  },
  {
    "id": 19620,
    "content": "Additionally, the current implementation supports managed memory on Linux with GPU devices that have compute capability 6"
  },
  {
    "id": 19621,
    "content": "x or greater but treats it as host memory There are no restriction on the sizes of the matrices as long as they can fit into the host memory"
  },
  {
    "id": 19622,
    "content": "The cuBLASXt API takes care of allocating the memory across the designated GPUs and dispatched the workload between them and finally retrieves the results back to the host The cuBLASXt API supports only the compute-intensive BLAS3 routines (e"
  },
  {
    "id": 19625,
    "content": "0, cuBLASXt API allows any of the matrices to be located on a GPU device Note : The cuBLASXt API is only supported on 64-bit platforms"
  },
  {
    "id": 19629,
    "content": "Tiling design approach  To be able to share the workload between multiples GPUs, the cuBLASXt API uses a tiling strategy : every matrix is divided in square tiles of user-controllable dimension BlockDim x BlockDim The resulting matrix tiling defines the static scheduling policy : each resulting tile is affected to a GPU in a round robin fashion One CPU thread is created per GPU and is"
  },
  {
    "id": 19630,
    "content": "responsible to do the proper memory transfers and cuBLAS operations to compute all the tiles that it is responsible for From a performance point of view, due to this static scheduling strategy, it is better that compute capabilites and PCI bandwidth are the same for every GPU To compute the first tile G0 from C, the CPU thread 0 responsible of GPU0, have to load 3 tiles from the first row of A and"
  },
  {
    "id": 19631,
    "content": "tiles from the first columun of B in a pipeline fashion in order to overlap memory transfer and computations and sum the results into the first tile G0 of C before to move on to the next tile G0 Example of cublasXtgemm tiling for 3 Gpus  When the tile dimension is not an exact multiple of the dimensions of C, some tiles are partially filled on the right border or/and the bottom border The current"
  },
  {
    "id": 19632,
    "content": "implementation does not pad the incomplete tiles but simply keep track of those incomplete tiles by doing the right reduced cuBLAS opearations : this way, no extra computation is done However it still can lead to some load unbalance when all GPUS do not have the same number of incomplete tiles to work on When one or more matrices are located on some GPU devices, the same tiling approach and"
  },
  {
    "id": 19633,
    "content": "workload sharing is applied However, when the computation of a tile and some data are located on the same GPU device, the memory transfer to/from the local data into tiles is bypassed and the GPU operates directly on the local data This can lead to a significant performance increase, especially when only one GPU is used for the computation The matrices can be located on any GPU device, and do not"
  },
  {
    "id": 19634,
    "content": "have to be located on the same GPU device Furthermore, the matrices can even be located on a GPU device that do not participate to the computation On the contrary of the cuBLAS API, even if all matrices are located on the same device, the cuBLASXt API is still a blocking API from the host point of view : the data results wherever located will be valid on the call return and no device"
  },
  {
    "id": 19639,
    "content": "Hybrid CPU-GPU computation  In the case of very large problems, the cuBLASXt API offers the possibility to offload some of the computation to the host CPU This feature can be setup with the routines cublasXtSetCpuRoutine() and cublasXtSetCpuRatio() The workload affected to the CPU is put aside : it is simply a percentage of the resulting matrix taken from the bottom and the right side whichever"
  },
  {
    "id": 19640,
    "content": "dimension is bigger If any of the matrices is located on a GPU device, the feature is ignored and all computation will be done only on the GPUs This feature should be used with caution because it could interfere with the CPU threads responsible of feeding the GPUs Currently, only the routine cublasXtgemm supports this feature"
  },
  {
    "id": 19644,
    "content": "Results reproducibility  Currently all cuBLASXt API routines from a given toolkit version, generate the same bit-wise results when the following conditions are respected : all GPUs particating to the computation have the same compute capabilities and the same number of SMs"
  },
  {
    "id": 19645,
    "content": "either the CPU hybrid computation is not used or the CPU Blas provided is also guaranteed to produce reproducible results"
  },
  {
    "id": 19651,
    "content": "cublasXtHandle_t  The cublasXtHandle_t type is a pointer type to an opaque structure holding the cuBLASXt API context The cuBLASXt API context must be initialized using cublasXtCreate() and the returned handle must be passed to all subsequent cuBLASXt API function calls The context should be destroyed at the end using cublasXtDestroy()"
  },
  {
    "id": 19656,
    "content": "This enum is used as parameters of the routines cublasXtSetCpuRoutine and cublasXtSetCpuRatio to setup the hybrid configuration"
  },
  {
    "id": 19657,
    "content": "Value Meaning CUBLASXT_FLOAT float or single precision type CUBLASXT_DOUBLE double precision type CUBLASXT_COMPLEX single precision complex CUBLASXT_DOUBLECOMPLEX double precision complex 4"
  },
  {
    "id": 19660,
    "content": "cublasXtBlasOp_t  The cublasXtBlasOp_t type enumerates the BLAS3 or BLAS-like routine supported by cuBLASXt API Value Meaning CUBLASXT_GEMM GEMM routine CUBLASXT_SYRK SYRK routine CUBLASXT_HERK HERK routine CUBLASXT_SYMM SYMM routine CUBLASXT_HEMM HEMM routine CUBLASXT_TRSM TRSM routine CUBLASXT_SYR2K SYR2K routine CUBLASXT_HER2K HER2K routine CUBLASXT_SPMM SPMM routine CUBLASXT_SYRKX SYRKX"
  },
  {
    "id": 19664,
    "content": "cublasXtPinningMemMode_t  The type is used to enable or disable the Pinning Memory mode through the routine cubasMgSetPinningMemMode Value Meaning CUBLASXT_PINNING_DISABLED the Pinning Memory mode is disabled CUBLASXT_PINNING_ENABLED the Pinning Memory mode is enabled 4"
  },
  {
    "id": 19666,
    "content": "cublasXtCreate()  cublasStatus_t cublasXtCreate ( cublasXtHandle_t * handle ) This function initializes the cuBLASXt API and creates a handle to an opaque structure holding the cuBLASXt API context It allocates hardware resources on the host and device and must be called prior to making any other cuBLASXt API calls Return Value Meaning CUBLAS_STATUS_SUCCESS the initialization succeeded"
  },
  {
    "id": 19667,
    "content": "CUBLAS_STATUS_ALLOC_FAILED the resources could not be allocated CUBLAS_STATUS_NOT_SUPPORTED cuBLASXt API is only supported on 64-bit platform 4"
  },
  {
    "id": 19670,
    "content": "cublasXtDestroy()  cublasStatus_t cublasXtDestroy ( cublasXtHandle_t handle ) This function releases hardware resources used by the cuBLASXt API context"
  },
  {
    "id": 19671,
    "content": "Return Value Meaning CUBLAS_STATUS_SUCCESS the shut down succeeded CUBLAS_STATUS_NOT_INITIALIZED the library was not initialized 4"
  },
  {
    "id": 19674,
    "content": "cublasXtDeviceSelect()  cublasXtDeviceSelect ( cublasXtHandle_t handle , int nbDevices , int deviceId []) This function allows the user to provide the number of GPU devices and their respective Ids that will participate to the subsequent cuBLASXt API Math function calls Currently the device configuration is static and cannot be changed between Math function calls"
  },
  {
    "id": 19676,
    "content": "Return Value Meaning CUBLAS_STATUS_SUCCESS User call was sucessful CUBLAS_STATUS_INVALID_VALUE Access to at least one of the device could not be done or a cuBLAS context could not be created on at least one of the device CUBLAS_STATUS_ALLOC_FAILED Some resources could not be allocated"
  },
  {
    "id": 19680,
    "content": "cublasXtSetBlockDim()  cublasXtSetBlockDim ( cublasXtHandle_t handle , int blockDim ) This function allows the user to set the block dimension used for the tiling of the matrices for the subsequent Math function calls This function can be called anytime and will take effect for the following Math function calls The block dimension should be chosen in a way to optimize the math operation and to"
  },
  {
    "id": 19682,
    "content": "Return Value Meaning CUBLAS_STATUS_SUCCESS the call has been successful CUBLAS_STATUS_INVALID_VALUE blockDim for type and for the corresponding short type to make a more concise and clear presentation of the implemented functions"
  },
  {
    "id": 19683,
    "content": "A host or device input array of dimensions lda x k with lda>=max(1,m) if transa == CUBLAS_OP_N and lda x m with lda>=max(1,k) otherwise B host or device input array of dimension ldb x n with ldb>=max(1,k) if transb == CUBLAS_OP_N and ldb x k with ldb>=max(1,n) otherwise A host or device input array of dimension lda x m with lda>=max(1,m) if side==CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n)"
  },
  {
    "id": 19686,
    "content": "A host or device input array of dimension lda x m with lda>=max(1,m) if side == CUBLAS_SIDE_LEFT and lda x n with lda>=max(1,n) otherwise"
  },
  {
    "id": 19687,
    "content": "beta host input scalar used for multiplication, if beta == 0 then C does not have to be a valid input"
  },
  {
    "id": 19688,
    "content": "A host or device input array of dimension lda x k with lda>=max(1,n) if trans == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise"
  },
  {
    "id": 19690,
    "content": "A host or device input array of dimension lda x k with lda>=max(1,n) if transa == CUBLAS_OP_N and lda x n with lda>=max(1,k) otherwise B host or device input array of dimensions ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise"
  },
  {
    "id": 19691,
    "content": "beta host input scalar used for multiplication, if beta==0 , then C does not have to be a valid input"
  },
  {
    "id": 19692,
    "content": "Also, for matrix \\(A\\) and \\(B\\) \\(\\text{op(}A\\text{) and op(}B\\text{)} = \\left\\{ \\begin{matrix} {A\\text{ and }B} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ {A^{T}\\text{ and }B^{T}} & {\\text{if }\\textsf{trans == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 19693,
    "content": "\\) This routine can be used when B is in such way that the result is guaranteed to be symmetric An usual example is when the matrix B is a scaled form of the matrix A : this is equivalent to B being the product of the matrix A and a diagonal matrix"
  },
  {
    "id": 19694,
    "content": "B host or device input array of dimension ldb x k with ldb>=max(1,n) if transb == CUBLAS_OP_N and ldb x n with ldb>=max(1,k) otherwise"
  },
  {
    "id": 19695,
    "content": "For an efficient computation of the product of a regular matrix with a diagonal matrix, refer to the routine cublasXtdgmm"
  },
  {
    "id": 19696,
    "content": "beta host input real scalar used for multiplication, if beta==0 then C does not have to be a valid input alpha host input scalar used for multiplication, if alpha==0 then A is not referenced and B does not have to be a valid input"
  },
  {
    "id": 19697,
    "content": "Also, for matrix \\(A\\) \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_N}$}} \\\\ A^{T} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_T}$}} \\\\ A^{H} & {\\text{if }\\textsf{transa == $\\mathrm{CUBLAS\\_OP\\_C}$}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 19698,
    "content": "\\) Notice that in order to achieve better parallelism, similarly to the cublas API, cuBLASXt API differs from the BLAS API for this routine The BLAS API assumes an in-place implementation (with results written back to B), while the cuBLASXt API assumes an out-of-place implementation (with results written into C) The application can still obtain the in-place functionality of BLAS in the cuBLASXt"
  },
  {
    "id": 19700,
    "content": "Note The packed matrix AP must be located on the host or managed memory whereas the other matrices can be located on the host or any GPU device Param"
  },
  {
    "id": 19702,
    "content": "This makes interfacing to applications written in C and C++ trivial, but the library can also be used by applications written in Fortran In particular, the cuBLAS library uses 1-based indexing and Fortran-style column-major storage for multidimensional data to simplify interfacing to Fortran applications Unfortunately, Fortran-to-C calling conventions are not standardized and differ by platform"
  },
  {
    "id": 19703,
    "content": "and toolchain In particular, differences may exist in the following areas: symbol names (capitalization, name decoration) argument passing (by value or reference) passing of string arguments (length information) passing of pointer arguments (size of the pointer) returning floating-point or compound data types (for example single-precision or complex data types) To provide maximum flexibility in"
  },
  {
    "id": 19704,
    "content": "addressing those differences, the cuBLAS Fortran interface is provided in the form of wrapper functions and is part of the Toolkit delivery The C source code of those wrapper functions is located in the src directory and provided in two different forms: the thunking wrapper interface located in the file fortran_thunking c the direct wrapper interface located in the file fortran c The code of one"
  },
  {
    "id": 19705,
    "content": "of those two files needs to be compiled into an application for it to call the cuBLAS API functions Providing source code allows users to make any changes necessary for a particular platform and toolchain The code in those two C files has been used to demonstrate interoperability with the compilers g77 3"
  },
  {
    "id": 19711,
    "content": "91 on 64-bit Linux, Intel Fortran 9 0 and Intel Fortran 10 0 on 32-bit and 64-bit Microsoft Windows XP, and g77 3"
  },
  {
    "id": 19715,
    "content": "Note that for g77, use of the compiler flag -fno-second-underscore is required to use these wrappers as provided"
  },
  {
    "id": 19716,
    "content": "Also, the use of the default calling conventions with regard to argument and return value passing is expected Using the flag -fno-f2c changes the default calling convention with respect to these two items"
  },
  {
    "id": 19717,
    "content": "The thunking wrappers allow interfacing to existing Fortran applications without any changes to the application"
  },
  {
    "id": 19718,
    "content": "During each call, the wrappers allocate GPU memory, copy source data from CPU memory space to GPU memory space, call cuBLAS, and finally copy back the results to CPU memory space and deallocate the GPU memory"
  },
  {
    "id": 19719,
    "content": "As this process causes very significant call overhead, these wrappers are intended for light testing, not for production code"
  },
  {
    "id": 19722,
    "content": "The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all BLAS functions"
  },
  {
    "id": 19723,
    "content": "To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using cuBLAS_ALLOC and cuBLAS_FREE ) and to copy data between GPU and CPU memory spaces (using cuBLAS_SET_VECTOR , cuBLAS_GET_VECTOR , cuBLAS_SET_MATRIX , and cuBLAS_GET_MATRIX )"
  },
  {
    "id": 19725,
    "content": "c map device pointers to the OS-dependent type size_t , which is 32-bit wide on 32-bit platforms and 64-bit wide on a 64-bit platforms"
  },
  {
    "id": 19726,
    "content": "One approach to deal with index arithmetic on device pointers in Fortran code is to use C-style macros, and use the C preprocessor to expand these, as shown in the example below"
  },
  {
    "id": 19727,
    "content": "On Linux and Mac OS X, one way of pre-processing is to use the option -E -x f77-cpp-input when using g77 compiler, or simply the option -cpp when using g95 or gfortran"
  },
  {
    "id": 19729,
    "content": "Inadvertently exceeding the maximum line length can lead to run-time errors that are difficult to find, so care should be taken not to exceed the 72-column limit if fixed form is retained"
  },
  {
    "id": 19730,
    "content": "The examples in this chapter show a small application implemented in Fortran 77 on the host and the same application with the non-thunking wrappers after it has been ported to use the cuBLAS library"
  },
  {
    "id": 19731,
    "content": "The second example should be compiled with ARCH_64 defined as 1 on 64-bit OS system and as 0 on 32-bit OS system"
  },
  {
    "id": 19732,
    "content": "For example for g95 or gfortran, this can be done directly on the command line by using the option -cpp -DARCH_64=1"
  },
  {
    "id": 19733,
    "content": "0 ) then write ( * , * ) \"device memory allocation failed\" call cublas_shutdown stop endif stat = cublas_set_matrix ( M , N , sizeof_real , a , M , devPtrA , M ) if ( stat 0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data download failed\" call cublas_shutdown stop endif — — Code block continues below — — call modify ( devPtrA , M , N , 2 , 3 , 16"
  },
  {
    "id": 19737,
    "content": "0 ) then call cublas_free ( devPtrA ) write ( * , * ) \"data upload failed\" call cublas_shutdown stop endif call cublas_free ( devPtrA ) call cublas_shutdown do j = 1 , N do i = 1 , M write ( * , \"(F7"
  },
  {
    "id": 19739,
    "content": "Interaction with Other Libraries and Tools  This section describes important requirements and recommendations that ensure correct use of cuBLAS with other libraries and utilities"
  },
  {
    "id": 19742,
    "content": "nvprune  nvprune enables pruning relocatable host objects and static libraries to only contain device code for the specific target architectures In case of cuBLAS, particular care must be taken if using nvprune with compute capabilities, whose minor revision number is different than 0 To reduce binary size, cuBLAS may only store major revision equivalents of CUDA binary files for kernels reused"
  },
  {
    "id": 19744,
    "content": "Therefore, to ensure that a pruned library does not fail for arbitrary problems, the user must keep binaries for a selected architecture and all prior minor architectures in its major architecture"
  },
  {
    "id": 19746,
    "content": "a to contain only sm_75 (Turing) and sm_70 (Volta) cubins: nvprune -- generate - code code = sm_70 -- generate - code code = sm_75 libcublasLt_static"
  },
  {
    "id": 19747,
    "content": "Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: Portions of the SGEMM, DGEMM, CGEMM and ZGEMM library routines were written by Vasily Volkov of the University of California Portions of the SGEMM, DGEMM and ZGEMM library routines were written by Davide Barbieri of the University of Rome Tor Vergata Portions of the DGEMM and SGEMM"
  },
  {
    "id": 19748,
    "content": "library routines optimized for Fermi architecture were developed by the University of Tennessee Subsequently, several other routines that are optimized for the Fermi architecture have been derived from these initial DGEMM and SGEMM implementations The substantial optimizations of the STRSV, DTRSV, CTRSV and ZTRSV library routines were developed by Jonathan Hogg of The Science and Technology"
  },
  {
    "id": 19749,
    "content": "Facilities Council (STFC) Subsequently, some optimizations of the STRSM, DTRSM, CTRSM and ZTRSM have been derived from these TRSV implementations Substantial optimizations of the SYMV and HEMV library routines were developed by Ahmad Abdelfattah, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST) Substantial optimizations of the TRMM and TRSM library"
  },
  {
    "id": 19750,
    "content": "routines were developed by Ali Charara, David Keyes and Hatem Ltaief of King Abdullah University of Science and Technology (KAUST)"
  },
  {
    "id": 19753,
    "content": "This product includes SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT https: sleef"
  },
  {
    "id": 19757,
    "content": "This product includes Boost C++ Libraries - free peer-reviewed portable C++ source libraries https: www boost org/ Boost Software License - Version 1"
  },
  {
    "id": 19759,
    "content": "This product includes Zstandard - a fast lossless compression algorithm, targeting real-time compression scenarios at zlib-level and better compression ratios"
  },
  {
    "id": 19760,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 19761,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 19763,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 19764,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 19765,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 19766,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 19767,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 19768,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 19769,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 19770,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 19771,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 19772,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 19773,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 19780,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 19782,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 19783,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 19787,
    "content": "pageBottom();}\nNVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12"
  },
  {
    "id": 19792,
    "content": "CUDLA_NUM_OUTPUT_TASK_STATISTICS = 4 Flag to retrieve total number of output task statistics buffer CUDLA_OUTPUT_TASK_STATISTICS_DESCRIPTORS = 5 Flag to retrieve all the output task statistics descriptors"
  },
  {
    "id": 19793,
    "content": "CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS = 1 Flag to load a module that is used to perform permanent fault diagnostics for DLA HW"
  },
  {
    "id": 19794,
    "content": "cudlaErrorInvalidParam = 1 This indicates that one or more parameters passed to the API is/are incorrect cudlaErrorOutOfResources = 2 This indicates that the API call failed due to lack of underlying resources"
  },
  {
    "id": 19795,
    "content": "cudlaErrorCreationFailed = 3 This indicates that an internal error occurred during creation of device handle"
  },
  {
    "id": 19796,
    "content": "cudlaErrorInvalidAddress = 4 This indicates that the memory object being passed in the API call has not been registered before cudlaErrorCuda = 6 This indicates that there was an error in a CUDA operation as part of the API call cudlaErrorUmd = 7 This indicates that there was an error in the DLA runtime for the API call cudlaErrorInvalidDevice = 8 This indicates that the device handle passed to"
  },
  {
    "id": 19797,
    "content": "the API call is invalid cudlaErrorInvalidAttribute = 9 This indicates that an invalid attribute is being requested cudlaErrorIncompatibleDlaSWVersion = 10 This indicates that the underlying DLA runtime is incompatible with the current cuDLA version cudlaErrorMemoryRegistered = 11 This indicates that the memory object is already registered cudlaErrorUnsupportedOperation = 13 This indicates that the"
  },
  {
    "id": 19798,
    "content": "operation being requested by the API call is unsupported cudlaErrorNvSci = 14 This indicates that the NvSci operation requested by the API call failed"
  },
  {
    "id": 19799,
    "content": "Values CUDLA_SUBMIT_NOOP = 1 Flag to specify that the submitted task must be bypassed for execution CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE = 1<<1 Flag to specify that the global lock acquire must be skipped CUDLA_SUBMIT_DIAGNOSTICS_TASK = 1<<2 Flag to specify that the submitted task is to run permanent fault diagnostics for DLA HW"
  },
  {
    "id": 19801,
    "content": "Data Structures Here are the data structures with brief descriptions: cudlaDevAttribute cudlaExternalMemoryHandleDesc cudlaExternalSemaphoreHandleDesc CudlaFence cudlaModuleAttribute cudlaModuleTensorDescriptor cudlaSignalEvents cudlaTask cudlaWaitEvents 2"
  },
  {
    "id": 19803,
    "content": "Public Variables uint32_t deviceVersion uint8_t unifiedAddressingSupported Variables uint32_t cudlaDevAttribute :: deviceVersion [inherited] DLA device version uint8_t cudlaDevAttribute :: unifiedAddressingSupported [inherited] Returns 0 if unified addressing is not supported"
  },
  {
    "id": 19806,
    "content": "cudlaExternalMemoryHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External memory handle descriptor Public Variables const void * extBufObject unsigned long long size Variables const void * cudlaExternalMemoryHandleDesc_t :: extBufObject [inherited] A handle representing an external memory object unsigned long long cudlaExternalMemoryHandleDesc_t :: size [inherited] Size of the"
  },
  {
    "id": 19809,
    "content": "cudlaExternalSemaphoreHandleDesc_t Struct Reference [ Data types used by cuDLA driver ] External semaphore handle descriptor Public Variables const void * extSyncObject Variables const void * cudlaExternalSemaphoreHandleDesc_t :: extSyncObject [inherited] A handle representing an external synchronization object"
  },
  {
    "id": 19812,
    "content": "CudlaFence Struct Reference [ Data types used by cuDLA driver ] Fence description Public Variables void * fence cudlaFenceType type Variables void * CudlaFence :: fence [inherited] Fence cudlaFenceType CudlaFence :: type [inherited] Fence type"
  },
  {
    "id": 19816,
    "content": "Public Variables cudlaModuleTensorDescriptor * inputTensorDesc uint32_t numInputTensors uint32_t numOutputTensors cudlaModuleTensorDescriptor * outputTensorDesc Variables cudlaModuleTensorDescriptor * cudlaModuleAttribute :: inputTensorDesc [inherited] Returns an array of input tensor descriptors uint32_t cudlaModuleAttribute :: numInputTensors [inherited] Returns the number of input tensors"
  },
  {
    "id": 19817,
    "content": "uint32_t cudlaModuleAttribute :: numOutputTensors [inherited] Returns the number of output tensors cudlaModuleTensorDescriptor * cudlaModuleAttribute :: outputTensorDesc [inherited] Returns an array of output tensor descriptors"
  },
  {
    "id": 19823,
    "content": "cudlaSignalEvents Struct Reference [ Data types used by cuDLA driver ] Signal events for cudlaSubmitTask Public Variables const * devPtrs CudlaFence * eofFences uint32_t numEvents Variables const * cudlaSignalEvents :: devPtrs [inherited] Array of registered synchronization objects (via cudlaImportExternalSemaphore ) CudlaFence * cudlaSignalEvents :: eofFences [inherited] Array of fences pointers"
  },
  {
    "id": 19824,
    "content": "for all the signal events corresponding to the synchronization objects uint32_t cudlaSignalEvents :: numEvents [inherited] Total number of signal events"
  },
  {
    "id": 19828,
    "content": "Public Variables const * inputTensor cudlaModule moduleHandle uint32_t numInputTensors uint32_t numOutputTensors const * outputTensor cudlaSignalEvents * signalEvents const cudlaWaitEvents * waitEvents Variables const * cudlaTask :: inputTensor [inherited] Array of input tensors const cudlaWaitEvents * cudlaTask :: waitEvents [inherited] Wait events"
  },
  {
    "id": 19831,
    "content": "cudlaWaitEvents Struct Reference [ Data types used by cuDLA driver ] Wait events for cudlaSubmitTask Public Variables uint32_t numEvents const CudlaFence * preFences Variables uint32_t cudlaWaitEvents :: numEvents [inherited] Total number of wait events"
  },
  {
    "id": 19832,
    "content": "NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result"
  },
  {
    "id": 19833,
    "content": "from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice"
  },
  {
    "id": 19834,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 19835,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 19836,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 19837,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 19838,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 19839,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 19840,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 19841,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 19842,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 19843,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 19844,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product Trademarks NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 19846,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 19847,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2021-2024 NVIDIA Corporation var switchTo5x=true; stLight"
  },
  {
    "id": 19848,
    "content": "options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite"
  },
  {
    "id": 19851,
    "content": "5 | PDF | Archive NVBLAS The User guide for NVBLAS, drop-in BLAS replacement, multi-GPUs accelerated 1 Introduction  The NVBLAS Library is a GPU-accelerated Libary that implements BLAS (Basic Linear Algebra Subprograms) It can accelerate most BLAS Level-3 routines by dynamically routing BLAS calls to one or more NVIDIA GPUs present in the system, when the charateristics of the call make it speed"
  },
  {
    "id": 19854,
    "content": "NVBLAS Overview  The NVBLAS Library is built on top of the cuBLAS Library using only the CUBLASXT API (refer to the CUBLASXT API section of the cuBLAS Documentation for more details) Depending on the charateristics of those BLAS calls, NVBLAS will redirect the calls to the GPUs present in the system or to CPU That decision is based on a simple heuristic that estimates if the BLAS call will"
  },
  {
    "id": 19855,
    "content": "execute for long enough to amortize the PCI transfers of the input and output data to the GPU Because NVBLAS does not support all standard BLAS routines, it might be necessary to associate it with an existing full BLAS Library Please refer to the Usage section for more details"
  },
  {
    "id": 19857,
    "content": "GPU Accelerated Routines  NVBLAS offloads only the compute-intensive BLAS3 routines which have the best potential for acceleration on GPUs"
  },
  {
    "id": 19858,
    "content": "The following table shows the currently supported routines: Routine Types Operation gemm S,D,C,Z Multiplication of 2 matrices syrk S,D,C,Z Symmetric rank-k update herk C,Z Hermitian rank-k update syr2k S,D,C,Z Symmetric rank-2k update her2k C,Z Hermitian rank-2k update trsm S,D,C,Z Triangular solve with multiple right-hand sides trmm S,D,C,Z Triangular matrix-matrix multiplication symm S,D,C,Z"
  },
  {
    "id": 19860,
    "content": "BLAS Symbols Interception  Standard BLAS Library implementations usually expose multiple symbols for the same routines Let’s say func is a BLAS routine name, func_ or/and func are usually defined as extern symbols The user needs to make sure that the application intended to be GPU-accelerated by NVBLAS actually calls those defined symbols Any other symbols will not be intercepted and the"
  },
  {
    "id": 19864,
    "content": "0, data can be located on any GPU device, even on GPU devices that are not configured to be part of the computation When any of the data is located on a GPU, the computation will be exclusively done on GPU whatever the size of the problem"
  },
  {
    "id": 19865,
    "content": "Also, this feature has to be used with caution: the user has to be sure that the BLAS call will indeed be intercepted by NVBLAS, otherwise it will result in a crash when the CPU BLAS tries to execute it"
  },
  {
    "id": 19867,
    "content": "Security Precaution  Because the NVBLAS Library relies on a symbols interception mechanism, it is essential to make sure it has not been compromised"
  },
  {
    "id": 19868,
    "content": "In that regard, NVBLAS should never be used from a process running at elevated privileges, such as Administrator on Windows or root on Linux"
  },
  {
    "id": 19870,
    "content": "Configuration  Because NVBLAS is a drop-in replacement of BLAS, it must be configured through an ASCII text file that describes how many and which GPUs can participate in the intercepted BLAS calls"
  },
  {
    "id": 19871,
    "content": "The format of the configuration file is based on keywords optionally followed by one or more user-defined parameters"
  },
  {
    "id": 19875,
    "content": "NVBLAS_CONFIG_FILE Environment Variable  The location and name of the configuration file must be defined by the environment variable NVBLAS_CONFIG_FILE By default, if NVBLAS_CONFIG_FILE is not defined, NVBLAS will try to open the file nvblas"
  },
  {
    "id": 19880,
    "content": "Configuration Keywords  The configuration keywords syntax is described in the following subsections"
  },
  {
    "id": 19884,
    "content": "NVBLAS_LOGFILE  This keyword defines the file where NVBLAS should print status and error messages It is advised to define this keyword early in the configuration to capture errors in parsing that file itself"
  },
  {
    "id": 19888,
    "content": "NVBLAS_TRACE_LOG_ENABLED  When this keyword is defined, every intercepted BLAS calls will be logged into the NVBLAS_LOGFILE"
  },
  {
    "id": 19895,
    "content": "dll on Windows) that NVBLAS should open to find the CPU BLAS symbols definitions Because CPU Blas libraries are often composed of multiple files, even though this keyword is set to the full path to the main file of the CPU library, it might still be necessary to define the right path to find the rest of the library files in the environment of your system On Linux, this can be done by setting the"
  },
  {
    "id": 19896,
    "content": "environment variable LD_LIBRARY_PATH whereas on Windows, this can be done by setting the environment variable PATH For a safe use of NVBLAS, the following precautions are strongly advised: The CPU BLAS Library should be located where ordinary users do not have write permissions The path specified should be absolute, not relative"
  },
  {
    "id": 19900,
    "content": "NVBLAS_GPU_LIST  This keyword defines the list of GPUs that should participate in the computation of the intercepted BLAS calls"
  },
  {
    "id": 19901,
    "content": "If not defined, only GPU device 0 is used, since that is normally the most compute-capable GPU installed in the system Also the following wildcard keywords are also accepted for simplicity : Keyword Meaning ALL All compute-capable GPUs detected on the system will be used by NVBLAS ALL0 GPU device 0, AND all others GPUs detected that have the same compute-capabilities as device 0 will be used by"
  },
  {
    "id": 19902,
    "content": "NVBLAS Note In the current release of CUBLAS, the CUBLASXT API supports two GPUs if they are on the same board such as Tesla K10 or GeForce GTX690 and one GPU otherwise If access to more GPUs devices is needed, details of the licensing are described at cublasXt"
  },
  {
    "id": 19906,
    "content": "NVBLAS_TILE_DIM  This keyword defines the tile dimension that should be used to divide the matrices involved in the computation"
  },
  {
    "id": 19907,
    "content": "Refer to cuBLAS documentation to understand the tradeoffs associated with setting this to a larger or a smaller value"
  },
  {
    "id": 19911,
    "content": "NVBLAS_GPU_DISABLED_  This keyword, appended with the name of a BLAS routine disables NVBLAS from running a specified routine on the GPU"
  },
  {
    "id": 19916,
    "content": "NVBLAS_CPU_RATIO_  This keyword, appended with the name of ta BLAS routine defines the ratio of the workload that should remain on the CPU in the event that the NVBLAS decides to offload work for that routine on the GPU"
  },
  {
    "id": 19917,
    "content": "Please refer to the cuBLAS documentation for details and for the list of routines which support this feature"
  },
  {
    "id": 19923,
    "content": "If this keyowrd is not present in the configuration file, the Pinning Memory mode will be set to CUBLASXT_PINNING_DISABLED"
  },
  {
    "id": 19924,
    "content": "Note There are some restrictions to use this feature as specified in the cuBLAS documentation of the underlying routine cublasXtSetPinningMemMode"
  },
  {
    "id": 19925,
    "content": "Specifically when NVBLAS is used in a multi-threaded applications, this option should not be used if there is a chance that matrices used by different threads overlaps while calling NVBLAS"
  },
  {
    "id": 19930,
    "content": "Configuration File Example  The following example shows a typical NVBLAS configuration file : # This is the configuration file to use NVBLAS Library # Setup the environment variable NVBLAS_CONFIG_FILE to specify your own config file NVBLAS Installation  The NVBLAS Library is part of the CUDA Toolkit, and will be installed along all the other CUDA libraries NVBLAS Library is built on top of"
  },
  {
    "id": 19933,
    "content": "Usage  To use the NVBLAS Library, the user application must be relinked against NVBLAS in addition to the original CPU Blas (technically only NVBLAS is needed unless some BLAS routines not supported by NVBLAS are used by the application) To be sure that the linker links against the exposed symbols of NVBLAS and not the ones from the CPU BLAS, the NVBLAS Library needs to be put before the CPU"
  },
  {
    "id": 19934,
    "content": "BLAS on the linkage command line On Linux, an alternative way to use NVBLAS Library is to use the LD_PRELOAD environment variable; this technique has the advantage of avoiding the relinkage step However, the user should avoid defining that environment variable globally because it will cause the NVBLAS library to be loaded by every shell command executed on the system, thus leading to a lack of"
  },
  {
    "id": 19935,
    "content": "responsiveness of the system Finally, mathematical tools and libraries often offer the opportunity to specify the BLAS Library to be used through an environment variable or a configuration file Because NVBLAS does not support all the standard BLAS routines, it might be necessary to pair NVBLAS with a full BLAS library, even though your application only calls supported NVBLAS routines Fortunately,"
  },
  {
    "id": 19936,
    "content": "those tools and libraries usually offer a way to specify multiple BLAS Libraries Please refer to the documentation of the appropriate tools and libraries for details"
  },
  {
    "id": 19939,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 19940,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 19942,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 19943,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 19944,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 19945,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 19946,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 19947,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 19948,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 19949,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 19950,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 19951,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 19952,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 19959,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 19961,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 19962,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 19968,
    "content": "5 | PDF | Archive NVVM IR Specification Reference guide to the NVVM compiler (intermediate representation) based on the LLVM IR Introduction  NVVM IR is a compiler IR (intermediate representation) based on the LLVM IR The PTX codegen part of a NVVM compiler needs to know the source language because of the difference in DCI (driver/compiler interface) Technically speaking, NVVM IR is LLVM IR with"
  },
  {
    "id": 19969,
    "content": "a set of rules, restrictions, and conventions, plus a set of supported intrinsic functions Accepted and ignored: The NVVM compiler will accept this IR feature, but will ignore the required semantics This applies to some IR features that do not have meaningful semantics on GPUs and that can be ignored"
  },
  {
    "id": 19971,
    "content": "Future versions of NVVM may either support or accept and ignore IRs that are illegal in the current version"
  },
  {
    "id": 19972,
    "content": "This document describes version 2 0 of the NVVM IR and version 3 1 of the NVVM debug metadata (see Source Level Debugging Support ) For the complete semantics of the IR, readers of this document should check the official LLVM Language Reference Manual ( https: releases llvm"
  },
  {
    "id": 19978,
    "content": "Identifiers  The name of a named global identifier must have the form: @[a-zA-Z$_][a-zA-Z$_0-9]* Note that it cannot contain the"
  },
  {
    "id": 19979,
    "content": "Linkage Types  Supported: private internal available_externally linkonce weak common linkonce_odr weak_odr external Not supported: appending extern_weak See NVVM ABI for PTX for details on how linkage types are translated to PTX"
  },
  {
    "id": 19982,
    "content": "Calling Conventions  All LLVM calling convention markings are accepted and ignored Functions and calls are generated according to the PTX calling convention"
  },
  {
    "id": 19986,
    "content": "Rules and Restrictions  When an argument with width less than 32-bit is passed, the zeroext/signext parameter attribute should be set When a value with width less than 32-bit is returned, the zeroext/signext parameter attribute should be set Arguments of aggregate or vector types that are passed by value can be passed by pointer with the byval attribute set (referred to as the by-pointer-byval"
  },
  {
    "id": 19988,
    "content": "The align attribute must be set if the type requires a non-natural alignment (natural alignment is the alignment inferred for the aggregate type according to the Data Layout section) If a function has an argument of aggregate or vector type that is passed by value directly and the type has a non-natural alignment requirement, the alignment must be annotated by the global property annotation ,"
  },
  {
    "id": 19989,
    "content": "where alignment is a 32-bit integer whose upper 16 bits represent the argument position (starting from 1) and the lower 16 bits represent the alignment If the return type of a function is an aggregate or a vector that has a non-natural alignment, then the alignment requirement must be annotated by the global property annotation , where the upper 16 bits is 0, and the lower 16 bits represent the"
  },
  {
    "id": 19990,
    "content": "alignment If annotated, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case For an indirect call instruction of a function that has a non-natural alignment for its return value or one of its arguments that is not expressed in alignment in the by-pointer-byval case, the call instruction must have an attached metadata of kind callalign The metadata"
  },
  {
    "id": 19991,
    "content": "contains a sequence of i32 fields each of which represents a non-natural alignment requirement The upper 16 bits of an i32 field represent the argument position (0 for return value, 1 for the first argument, and so on) and the lower 16 bits represent the alignment"
  },
  {
    "id": 19995,
    "content": "{ i32 8 , i32 520 }; It is not required to have an i32 metadata field for the other arguments or the return value otherwise"
  },
  {
    "id": 19996,
    "content": "If presented, the alignment must match the natural alignment or the align attribute in the by-pointer-byval case If attached, the alignment must match the natural alignment or the alignment in the by-pointer-byval case The absence of the metadata in an indirect call instruction means using natural alignment or the align attribute in the by-pointer-byval case"
  },
  {
    "id": 20002,
    "content": "Global Variables  A global variable, that is not an intrinsic global variable, may be optionally declared to reside in one of the following address spaces: global shared constant If no address space is explicitly specified, the global variable is assumed to reside in the global address space with a generic address value"
  },
  {
    "id": 20006,
    "content": "Functions  The following are not supported on functions: Alignment Explicit section Garbage collector name Prefix data Prologue Personality 3"
  },
  {
    "id": 20029,
    "content": "version named metadata may have one metadata node that contains the NVVM IR version for that module If multiple such modules are linked together, the named metadata in the linked module may have more than one metadata node with each node containing a version A metadata node with NVVM IR version takes either of the following forms: It may consist of two i32 values—the first denotes the NVVM IR"
  },
  {
    "id": 20030,
    "content": "major version number and the second denotes the minor version number If absent, the version number is assumed to be 1"
  },
  {
    "id": 20037,
    "content": "{i32 1, i32 0} It may consist of four i32 values—the first two denote the NVVM IR major and minor versions respectively The third value denotes the NVVM IR debug metadata major version number, and the fourth value denotes the corresponding minor version number The version of NVVM IR debug metadata described in this document is 3"
  },
  {
    "id": 20041,
    "content": "Parameter Attributes  Fully supported, except the following: Accepted and ignored: inreg nest Not supported: inalloca swiftself swifterror See Calling Conventions for the use of the attributes The set of supported attributes is equal to the set of attributes accepted where the attribute group is used"
  },
  {
    "id": 20044,
    "content": "Function Attributes  Supported: allocsize alwaysinline cold convergent inaccessiblememonly inaccessiblemem_or_argmemonly inlinehint minsize no-jump-tables noduplicate noinline noreturn norecurse nounwind \"null-pointer-is-valid\" optforfuzzing optnone optsize readnone readonly writeonly argmemonly speculatable strictfp Not Supported: alignstack builtin nonlazybind naked nobuiltin noimplicitfloat"
  },
  {
    "id": 20045,
    "content": "noredzone \"patchable-function\" probe-stack returns_twice sanitize_address sanitize_memory sanitize_thread sanitize_hwaddress ssp sspreq sspstrong \"stack-probe-size\" \"no-stack-arg-probe\" uwtable jumptable safestack \"thunk\" nocf_check shadowcallstack 3"
  },
  {
    "id": 20049,
    "content": "Data Layout  Only the following data layout is supported: 64-bit e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 The following data layouts are deprecated and will be removed in a future release 32-bit"
  },
  {
    "id": 20050,
    "content": "e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-i128:128:128-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 64-bit"
  },
  {
    "id": 20051,
    "content": "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64 3"
  },
  {
    "id": 20053,
    "content": "Target Triple  Only the following target triple is supported, where * can be any name: 64-bit: nvptx64-*-cuda The following target triple is deprecated, and will be removed in future release: 32-bit: nvptx-*-cuda 3"
  },
  {
    "id": 20063,
    "content": "Atomic Memory Ordering Constraints  Atomic loads and stores are not supported Other atomic operations on other than 32-bit or 64-bit operands are not supported"
  },
  {
    "id": 20068,
    "content": "Type System  Fully supported, except for the following: Floating point types half , fp128 , x86_fp80 and ppc_fp128 are not supported The non-integral pointer type is not supported"
  },
  {
    "id": 20071,
    "content": "For a constant expression that is used as the initializer of a global variable @g1 , if the constant expression contains a global identifier @g2 , then the constant expression is supported if it can be reduced to the form of bitcast+offset , where offset is an integer number (including 0 ) 6"
  },
  {
    "id": 20072,
    "content": "Inline Assembler Expressions  Inline assembler of PTX instructions is supported, with the following supported constraints: Constraint Type c i8 h i16 r i32 l i64 f f32 d f64 The inline asm metadata"
  },
  {
    "id": 20083,
    "content": "full callalign (see Rules and Restrictions for Calling Conventions) Module flags metadata ( llvm module flags ) is supported and verified, but the metadata values will be ignored The llvm"
  },
  {
    "id": 20085,
    "content": "used global variable is supported The llvm global_ctors global variable is not supported The llvm global_dtors global variable is not supported 10"
  },
  {
    "id": 20086,
    "content": "Terminator Instructions  Supported: ret br switch unreachable Unsupported: indirectbr invoke resume catchswitch catchret cleanupret 10"
  },
  {
    "id": 20100,
    "content": "cmpxchg Instruction  Supported for i32 , i64 , and i128 types, with the following restrictions: The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space The i128 type is only supported on compute_90 and above"
  },
  {
    "id": 20106,
    "content": "The pointer must be either a global pointer, a shared pointer, or a generic pointer that points to either the global address space or the shared address space"
  },
  {
    "id": 20115,
    "content": "Other Operations  Supported: icmp fcmp phi select va_arg call (See Calling Conventions for other rules and restrictions"
  },
  {
    "id": 20168,
    "content": "Address Spaces  NVVM IR has a set of predefined memory address spaces, whose semantics are similar to those defined in CUDA C/C++, OpenCL C and PTX Name Address Space Number Semantics/Example code 0 functions, code CUDA C/C++ function OpenCL C function generic 0 Can only be used to qualify the pointee of a pointer Pointers in CUDA C/C++ global 1 CUDA C/C++ __device__ OpenCL C global shared 3"
  },
  {
    "id": 20169,
    "content": "CUDA C/C++ __shared__ OpenCL C local constant 4 CUDA C/C++ __constant__ OpenCL C constant local 5 CUDA C/C++ local OpenCL C private 2, 101 and above Each global variable, that is not an intrinsic global variable, can be declared to reside in a specific non-zero address space, which can only be one of the following: global , shared or constant If a non-intrinsic global variable is declared without"
  },
  {
    "id": 20170,
    "content": "any address space number or with the address space number 0, then this global variable resides in address space global and the pointer of this global variable holds a generic pointer value The predefined NVVM memory spaces are needed for the language front-ends to model the memory spaces in the source languages For example, CUDA C/C++ __constant__ int c; __device__ int g; ; NVVM IR @c ="
  },
  {
    "id": 20171,
    "content": "addrspace(4) global i32 0, align 4 @g = addrspace(1) global [2 x i32] zeroinitializer, align 4 Address space numbers 2 and 101 or higher are reserved for NVVM compiler internal use only No language front-end should generate code that uses these address spaces directly"
  },
  {
    "id": 20177,
    "content": "In NVVM IR, a generic pointer has a pointer type with the address space generic , while a non-generic pointer has a pointer type with a non-generic address space Note that the address space number for the generic address space is 0—the default in both NVVM IR and LLVM IR Loads/stores via generic pointers are supported, as well as loads/stores via non-generic pointers Loads/stores via function"
  },
  {
    "id": 20178,
    "content": "pointers are not supported @a = addrspace(1) global i32 0, align 4 ; 'global' addrspace, @a holds a specific value @b = global i32 0, align 4 ; 'global' addrspace, @b holds a generic value @c = addrspace(4) global i32 0, align 4 ; 'constant' addrspace, @c holds a specific value Conversion  The bit value of a generic pointer that points to a specific object may be different from the bit value of a"
  },
  {
    "id": 20179,
    "content": "specific pointer that points to the same object The addrspacecast IR instruction should be used to perform pointer casts across address spaces (generic to non-generic or non-generic to generic) Casting from a generic to a non-generic pointer is undefined if the generic pointer does not point to an object in the target non-generic address space"
  },
  {
    "id": 20181,
    "content": "The following intrinsic can be used to query if the argument pointer was derived from the address of a kernel function parameter that has the grid_constant property: i1 @llvm"
  },
  {
    "id": 20184,
    "content": "grid_const(i8*) The following intrinsic can be used to query if the input generic pointer was derived from the address of a variable allocated in the shared address space, in a CTA that is part of the same cluster as the parent CTA of the invoking thread"
  },
  {
    "id": 20188,
    "content": "cluster_shared(i8*) The following intrinsics can be used to query if a generic pointer can be safely cast to a specific non-generic address space: i1 @llvm"
  },
  {
    "id": 20200,
    "content": "shared(i8*) bitcast on pointers is supported, though LLVM IR forbids bitcast from being used to change the address space of a pointer"
  },
  {
    "id": 20204,
    "content": "No Aliasing between Two Different Specific Address Spaces  Two different specific address spaces do not overlap NVVM compiler assumes two memory accesses via non-generic pointers that point to different address spaces are not aliased"
  },
  {
    "id": 20207,
    "content": "The alloca Instruction  The alloca instruction returns a generic pointer that only points to address space local"
  },
  {
    "id": 20208,
    "content": "Overview  NVVM uses Named Metadata to annotate IR objects with properties that are otherwise not representable in the IR The NVVM IR producers can use the Named Metadata to annotate the IR with properties, which the NVVM compiler can process"
  },
  {
    "id": 20211,
    "content": "Representation of Properties  For each translation unit (that is, per bitcode file), there is a named metadata called nvvm"
  },
  {
    "id": 20213,
    "content": "The first operand of each MDNode is an entity that the node is annotating using the remaining operands Multiple MDNodes may provide annotations for the same entity, in which case their first operands will be same Starting with the operand after the annotated entity, every alternate operand specifies a property"
  },
  {
    "id": 20223,
    "content": "\"kernel\", i32 1, \"maxntidx\", i32 16} If two bitcode files are being linked and both have a named metadata nvvm annotations , the linked file will have a single merged named metadata If both files define properties for the same entity foo , the linked file will have two MDNodes defining properties for foo It is illegal for the files to have conflicting properties for the same entity"
  },
  {
    "id": 20226,
    "content": "Supported Properties  Property Name Annotated On Description maxntid{x, y, z} kernel function Maximum expected CTA size from any launch"
  },
  {
    "id": 20227,
    "content": "minctasm kernel function Hint/directive to the compiler/driver, asking it to put at least these many CTAs on an SM"
  },
  {
    "id": 20228,
    "content": "grid_constant kernel function The argument is a metadata node, which contains a list of integers, where each integer n denotes that the nth parameter has the grid_constant annotation (numbering from 1)"
  },
  {
    "id": 20229,
    "content": "align function Signifies that the value in low 16-bits of the 32-bit value contains alignment of n th parameter type if its alignment is not the natural alignment"
  },
  {
    "id": 20233,
    "content": "Texture Variable and Surface Variable  A texture or a surface variable can be declared/defined as a global variable of i64 type with annotation texture or surface in the global address space A texture or surface variable must have a name, which must follow identifier naming conventions A texture or a surface variable may only have the following uses: In a metadata node As an intrinsic function"
  },
  {
    "id": 20236,
    "content": "Accessing Texture Memory or Surface Memory  Texture memory and surface memory can be accessed using texture or surface handles NVVM provides the following intrinsic function to get a texture or surface handle from a texture or surface variable p1i64 ( metadata , i64 addrspace ( 1 ) * ) The first argument to the intrinsic is a metadata holding the texture or surface variable The returned handle"
  },
  {
    "id": 20237,
    "content": "value from the intrinsic call can be used as an operand (with a constraint of l) in a PTX inline asm to access the texture or surface memory"
  },
  {
    "id": 20240,
    "content": "Atomic  Besides the atomic instructions, the following extra atomic intrinsic functions are supported"
  },
  {
    "id": 20277,
    "content": "p3f64(double addrspace(3)* address, double val) reads the single/double precision floating point value old located at the address address , computes old+val , and stores the result back to memory at the same address"
  },
  {
    "id": 20296,
    "content": "p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes ((old >= val) 0 : (old+1)) , and stores the result back to memory at the same address declare i32 @llvm"
  },
  {
    "id": 20314,
    "content": "p3i32(i32 addrspace(3)* address, i32 val) reads the 32-bit word old located at the address address , computes (((old == 0) | (old > val)) val : (old-1) ) , and stores the result back to memory at the same address"
  },
  {
    "id": 20319,
    "content": "barrier0() waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to llvm"
  },
  {
    "id": 20327,
    "content": "barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero"
  },
  {
    "id": 20333,
    "content": "barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them"
  },
  {
    "id": 20339,
    "content": "barrier0() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them"
  },
  {
    "id": 20342,
    "content": "cluster barrier(i32 %flags) Synchronize and communicate among threads in the same cluster The %flags is encoded according to the following table: %flags bits Meaning 31-8 Reserved 7-4 Memory ordering (See Cluster Barrier Memory Ordering Encoding below) 3-0 Operation mode (See Cluster Barrier Operation Mode Encoding below) Cluster Barrier Operation Mode Encoding Encoding Mode Description 0 Arrive"
  },
  {
    "id": 20343,
    "content": "Arrive at cluster barrier 1 Wait Wait at cluster barrier 2-15 RESERVED RESERVED Cluster Barrier Memory Ordering Encoding Encoding Mode Description 0 Default All synchronous memory accesses requested by the executing entry prior to arrive are performed and are visible to all the entrys in the cluster after wait 1 Relaxed All previously fenced memory accesses requested by the executing entry prior"
  },
  {
    "id": 20344,
    "content": "to arrive are performed and are visible to all the entrys in the cluster after wait 2-15 RESERVED RESERVED declare void @llvm"
  },
  {
    "id": 20350,
    "content": "membar(i32 %flags) Wait for all prior memory accesses requested by this thread to be performed at a membar level defined by the membar mode below For horizontal synchronization, a barrier should be used instead, or in addition to membar The %flags is encoded according to the following table: %flags bits Meaning 31-4 Reserved 3-0 Membar modes (See Membar Mode Encoding ) Membar Mode Encoding"
  },
  {
    "id": 20351,
    "content": "Encoding Mode Description 0 GLOBAL Membar at the global level 1 CTA Membar at the CTA level 2 SYSTEM Membar at the system level 3 RESERVED RESERVED 4 CLUSTER Membar at the cluster level, only on Hopper+ 5-15 RESERVED RESERVED 15"
  },
  {
    "id": 20353,
    "content": "Address space conversion  Note Attention: Please use the addrspacecast IR instruction for address space conversion"
  },
  {
    "id": 20356,
    "content": "Special Registers  The following intrinsic functions are provided to support reading special PTX registers: declare i32 @llvm"
  },
  {
    "id": 20435,
    "content": "Texture/Surface Access  The following intrinsic function is provided to convert a global texture/surface variable into a texture/surface handle"
  },
  {
    "id": 20441,
    "content": "The following IR definitions apply to all intrinsics in this section: type %float4 = { float, float, float, float } type %long2 = { i64, i64 } type %int4 = { i32, i32, i32, i32 } type %int2 = { i32, i32 } type %short4 = { i16, i16, i16, i16 } type %short2 = { i16, i16 } 15"
  },
  {
    "id": 20444,
    "content": "Surface Loads  In the following intrinsics, represents the surface clamp mode and can be one of the following: clamp , trap , or zero For surface load instructions that operate on 8-bit data channels, the output operands are of type i16 It is trap for the formatted stores, and can be one of the following for unformatted stores: clamp , trap , or zero For surface store instructions that operate"
  },
  {
    "id": 20451,
    "content": "i32 (i64 %tex, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 1D surface array: ;; Unformatted void @llvm"
  },
  {
    "id": 20478,
    "content": "i32 (i64 %tex, i32 %idx, i32 %x, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface: ;; Unformatted void @llvm"
  },
  {
    "id": 20501,
    "content": "i32 (i64 %tex, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 2D surface array: ;; Unformatted void @llvm"
  },
  {
    "id": 20563,
    "content": "(i64 %tex, i32 %idx, i32 %x, i32 %y, i32 %r, i32 %g, i32 %b, i32 %a) Writing a 3D surface: ;; Unformatted void @llvm"
  },
  {
    "id": 20587,
    "content": "Barrier Synchronization  The following intrinsic performs a barrier synchronization among a subset of threads in a warp"
  },
  {
    "id": 20592,
    "content": "sync(i32 %membermask) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before resuming execution"
  },
  {
    "id": 20594,
    "content": "The behavior of this intrinsic is undefined if the executing thread is not in the %membermask For compute_62 or below, all threads in %membermask must call the same @llvm"
  },
  {
    "id": 20598,
    "content": "sync() in convergence, and only threads belonging to the %membermask can be active when the intrinsic is called"
  },
  {
    "id": 20603,
    "content": "Data Movement  The following intrinsic synchronizes a subset of threads in a warp and then performs data movement among these threads"
  },
  {
    "id": 20608,
    "content": "i32(i32 %membermask, i32 %mode, i32 %a, i32 %b, i32 %c) This intrinsic causes executing thread to wait until all threads corresponding to %membermask have executed the same intrinsic with the same %membermask value before reading data from other threads in the same warp"
  },
  {
    "id": 20609,
    "content": "Each thread in the currently executing warp will compute a source lane index j based on input arguments %b , %c , and %mode If the computed source lane index j is in range, the returned i32 value will be the value of %a from lane j; otherwise, it will be the the value of %a from the current thread If the thread corresponding to lane j is inactive, then the returned i32 value is undefined The"
  },
  {
    "id": 20611,
    "content": "The argument %mode must be a constant and its encoding is specified in the following table Encoding Meaning 0 IDX 1 UP 2 DOWN 3 BFLY Argument %b specifies a source lane or source lane offset, depending on %mode Argument %c contains two packed values specifying a mask for logically splitting warps into sub-segments and an upper bound for clamping the source lane index"
  },
  {
    "id": 20612,
    "content": "How the elements of a matrix are distributed among the fragments is opaque to the user and is different for matrix A , B and the accumulator"
  },
  {
    "id": 20618,
    "content": "Store Fragments  The following intrinsics synchronize all threads in a warp and then store a fragment of a matrix for each thread Encoding Meaning 0 A fragment is row-major, B fragment is row-major 1 A fragment is row-major, B fragment is column-major 2 A fragment is column-major, B fragment is row-major 3 A fragment is column-major, B fragment is column-major Support for %satf has been removed"
  },
  {
    "id": 20619,
    "content": "and this operand must be a constant zero The behavior of these intrinsics are undefined if any thread in the warp has exited"
  },
  {
    "id": 20621,
    "content": "Source Level Debugging Support  To enable source level debugging of an IR module, NVVM IR supports debug intrinsics and debug information descriptors to express the debugging information For the complete semantics of the IR, readers of this chapter should check the official LLVM IR specialized metadata nodes documentation ( https: releases llvm"
  },
  {
    "id": 20625,
    "content": "html#specialized-metadata-nodes ) and the Source Level Debugging with LLVM Manual ( https: releases llvm"
  },
  {
    "id": 20630,
    "content": "The following metadata nodes need to be present in the module when debugging support is requested: Named metadata node"
  },
  {
    "id": 20633,
    "content": "cu Module flags metadata for \"Debug Info Version\" flag: The behavior flag should be Error Named metadata"
  },
  {
    "id": 20635,
    "content": "version containing a metadata node with the NVVM IR major and minor version values followed by the NVVM IR debug metadata major and minor version values"
  },
  {
    "id": 20638,
    "content": ", full, line info only) is controlled by the DICompileUnit’s emissionKind field: FullDebug (value: 1) : Generate symbolic debug and line information"
  },
  {
    "id": 20639,
    "content": "If there are multiple input NVVM IR modules, at most one module may have a single debug compile unit"
  },
  {
    "id": 20642,
    "content": "Linkage Types  The following table provides the mapping of NVVM IR linkage types associated with functions and global variables to PTX linker directives LLVM Linkage Type PTX Linker Directive private , internal This is the default linkage type and does not require a linker directive"
  },
  {
    "id": 20643,
    "content": "external Function with definition visible Global variable with initialization Function without definition extern Global variable without initialization common common for the global address space, otherwise"
  },
  {
    "id": 20644,
    "content": "weak available_externally , linkonce , linkonce_odr , weak , weak_odr weak All other linkage types Not supported"
  },
  {
    "id": 20647,
    "content": "Parameter Passing and Return  The following table shows the mapping of function argument and return types in NVVM IR to PTX types Source Type Size in Bits PTX Type Integer types <= 32"
  },
  {
    "id": 20664,
    "content": "align align b8 name [ size ] Where align is overall aggregate or vector alignment in bytes, name is variable name associated with aggregate or vector, and size is the aggregate or vector size in bytes"
  },
  {
    "id": 20666,
    "content": "11 Added support for Hopper+ cluster intrinsics and max_blocks_per_cluster kernel property for CUDA 11"
  },
  {
    "id": 20670,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 20671,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 20673,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 20674,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 20675,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 20676,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 20677,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 20678,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 20679,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 20680,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 20681,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 20682,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 20683,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 20690,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 20692,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 20693,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 20697,
    "content": "pageBottom();}\nNVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12"
  },
  {
    "id": 20701,
    "content": "1 ( older ) - Last updated July 1, 2024 - Send Feedback Libdevice User's Guide User's guide to libdevice Table of Contents 1"
  },
  {
    "id": 20702,
    "content": "__nv_ynf Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright ©2013-2024 NVIDIA Corporation var switchTo5x=true; stLight"
  },
  {
    "id": 20703,
    "content": "options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite"
  },
  {
    "id": 20705,
    "content": "Introduction  libNVVM API provides an interface for generating PTX code from both binary and text NVVM IR inputs"
  },
  {
    "id": 20707,
    "content": "0 IR and bitcode Support for reading the text NVVM IR representation is deprecated and may be removed in a later release"
  },
  {
    "id": 20711,
    "content": "Clients can take advantage of improved compilation speeds by spawning multiple compilation threads concurrently"
  },
  {
    "id": 20715,
    "content": "Functions const char * nvvmGetErrorString (nvvmResult result) Get the message string for the given nvvmResult code"
  },
  {
    "id": 20719,
    "content": "Values: enumerator NVVM_SUCCESS  enumerator NVVM_ERROR_OUT_OF_MEMORY  enumerator NVVM_ERROR_PROGRAM_CREATION_FAILURE  enumerator NVVM_ERROR_IR_VERSION_MISMATCH  enumerator NVVM_ERROR_INVALID_INPUT  enumerator NVVM_ERROR_INVALID_PROGRAM  enumerator NVVM_ERROR_INVALID_IR  enumerator NVVM_ERROR_INVALID_OPTION  enumerator NVVM_ERROR_NO_MODULE_IN_PROGRAM  enumerator NVVM_ERROR_COMPILATION  2"
  },
  {
    "id": 20721,
    "content": "Functions  const char * nvvmGetErrorString ( nvvmResult result )  Get the message string for the given nvvmResult code Returns Message string for the given nvvmResult code"
  },
  {
    "id": 20723,
    "content": "General Information Query  Functions nvvmResult nvvmIRVersion (int *majorIR, int *minorIR, int *majorDbg, int *minorDbg) Get the NVVM IR version nvvmResult nvvmVersion (int *major, int *minor) Get the NVVM version"
  },
  {
    "id": 20726,
    "content": "Functions  nvvmResult nvvmIRVersion ( int * majorIR , int * minorIR , int * majorDbg , int * minorDbg )  Get the NVVM IR version Returns NVVM_SUCCESS nvvmResult nvvmVersion ( int * major , int * minor )  Get the NVVM version Compilation  Functions nvvmResult nvvmAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program"
  },
  {
    "id": 20727,
    "content": "nvvmResult nvvmCompileProgram (nvvmProgram prog, int numOptions, const char **options) Compile the NVVM program nvvmResult nvvmCreateProgram (nvvmProgram *prog) Create a program, and set the value of its handle to *prog nvvmResult nvvmGetCompiledResult (nvvmProgram prog, char *buffer) Get the compiled result nvvmResult nvvmGetCompiledResultSize (nvvmProgram prog, size_t *bufferSizeRet) Get the"
  },
  {
    "id": 20728,
    "content": "size of the compiled result nvvmResult nvvmGetProgramLog (nvvmProgram prog, char *buffer) Get the Compiler/Verifier Message nvvmResult nvvmGetProgramLogSize (nvvmProgram prog, size_t *bufferSizeRet) Get the Size of Compiler/Verifier Message nvvmResult nvvmLazyAddModuleToProgram (nvvmProgram prog, const char *buffer, size_t size, const char *name) Add a module level NVVM IR to a program nvvmResult"
  },
  {
    "id": 20729,
    "content": "nvvmVerifyProgram (nvvmProgram prog, int numOptions, const char **options) Verify the NVVM program Typedefs nvvmProgram NVVM Program"
  },
  {
    "id": 20732,
    "content": "Functions  nvvmResult nvvmAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char * name )  Add a module level NVVM IR to a program The module should have NVVM IR either in the LLVM 7"
  },
  {
    "id": 20736,
    "content": "1 text representation Support for reading the text representation of NVVM IR is deprecated and may be removed in a later version"
  },
  {
    "id": 20737,
    "content": "Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmCompileProgram ( nvvmProgram prog , int numOptions , const char * * options )  Compile the NVVM program"
  },
  {
    "id": 20738,
    "content": "The target datalayout in the linked IR program is used to determine the address size (32bit vs 64bit)"
  },
  {
    "id": 20739,
    "content": "Line number (line info) only generation is also enabled via NVVM IR Debug Metadata, there is no specific libNVVM API flag for that case"
  },
  {
    "id": 20740,
    "content": "Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM NVVM_ERROR_COMPILATION nvvmResult nvvmCreateProgram ( nvvmProgram * prog )  Create a program, and set the value of its handle to *prog Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_PROGRAM nvvmResult"
  },
  {
    "id": 20741,
    "content": "nvvmDestroyProgram ( nvvmProgram * prog )  Destroy a program Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResult ( nvvmProgram prog , char * buffer )  Get the compiled result Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetCompiledResultSize ( nvvmProgram prog , size_t * bufferSizeRet )  Get the size of the compiled result Returns NVVM_SUCCESS"
  },
  {
    "id": 20742,
    "content": "NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLog ( nvvmProgram prog , char * buffer )  Get the Compiler/Verifier Message The NULL terminated message string is stored in the memory pointed to by buffer when the return value is NVVM_SUCCESS Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmGetProgramLogSize ( nvvmProgram prog , size_t * bufferSizeRet )  Get the Size of"
  },
  {
    "id": 20743,
    "content": "Compiler/Verifier Message The size of the message string (including the trailing NULL) is stored into bufferSizeRet when the return value is NVVM_SUCCESS bufferSizeRet – [out] Size of the compilation/verification log (including the trailing NULL) Returns NVVM_SUCCESS NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmLazyAddModuleToProgram ( nvvmProgram prog , const char * buffer , size_t size , const char"
  },
  {
    "id": 20745,
    "content": "A module added using this API is lazily loaded - the only symbols loaded are those that are required by module(s) loaded using nvvmAddModuleToProgram Compiler may also optimize entities in this module by making them internal to the linked NVVM IR module, making them eligible for other optimizations Due to these optimizations, this API to load a module is more efficient and should be used where"
  },
  {
    "id": 20747,
    "content": "Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_INVALID_INPUT NVVM_ERROR_INVALID_PROGRAM nvvmResult nvvmVerifyProgram ( nvvmProgram prog , int numOptions , const char * * options )  Verify the NVVM program Returns NVVM_SUCCESS NVVM_ERROR_OUT_OF_MEMORY NVVM_ERROR_IR_VERSION_MISMATCH NVVM_ERROR_INVALID_PROGRAM NVVM_ERROR_INVALID_IR NVVM_ERROR_INVALID_OPTION NVVM_ERROR_NO_MODULE_IN_PROGRAM"
  },
  {
    "id": 20750,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 20751,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 20753,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 20754,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 20755,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 20756,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 20757,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 20758,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 20759,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 20760,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 20761,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 20762,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 20763,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 20770,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 20772,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 20773,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 20778,
    "content": "CUDA for Tegra  This application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU)"
  },
  {
    "id": 20781,
    "content": "Overview  This document provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU)"
  },
  {
    "id": 20782,
    "content": "This guide is for developers who are already familiar with programming in CUDA®, and C/C++, and who want to develop applications for the Tegra® SoC Performance guidelines, best practices, terminology, and general information provided in the CUDA C++ Programming Guide and the CUDA C++ Best Practices Guide are applicable to all CUDA-capable GPU architectures, including Tegra® devices The CUDA C++"
  },
  {
    "id": 20783,
    "content": "Programming Guide and the CUDA C Best Practices Guide are available at the following web sites: CUDA C++ Programming Guide: https: docs"
  },
  {
    "id": 20789,
    "content": "Memory Management  In Tegra® devices, both the CPU (Host) and the iGPU share SoC DRAM memory A dGPU with separate DRAM memory can be connected to the Tegra device over PCIe or NVLink dGPU-connected Tegra Memory System  In Tegra, device memory, host memory, and unified memory are allocated on the same physical SoC DRAM The caching behavior in a Tegra system is different from that of an x86"
  },
  {
    "id": 20790,
    "content": "system with a dGPU The caching and accessing behavior of different memory types in a Tegra system is shown in Table 1 Characteristics of Different Memory Types in a Tegra System  Memory Type CPU iGPU Tegra-connected dGPU Device memory Not directly accessible Cached Cached Pageable host memory Cached Not directly accessible Not directly accessible Pinned host memory Uncached where compute"
  },
  {
    "id": 20793,
    "content": "Uncached Uncached Unified memory Cached Cached Not supported On Tegra, because device memory, host memory, and unified memory are allocated on the same physical SoC DRAM, duplicate memory allocations and data transfers can be avoided"
  },
  {
    "id": 20796,
    "content": "I/O Coherency  I/O coherency (also known as one-way coherency) is a feature with which an I/O device such as a GPU can read the latest updates in CPU caches"
  },
  {
    "id": 20797,
    "content": "It removes the need to perform CPU cache management operations when the same physical memory is shared between CPU and GPU The GPU cache management operations still need to be performed because the coherency is one way Please note that the CUDA driver internally performs the GPU cache management operations when managed memory or interop memory is used"
  },
  {
    "id": 20798,
    "content": "Applications should realize benefits from this HW feature without needing to make changes to the application’s code (see point 2 below)"
  },
  {
    "id": 20799,
    "content": "The following functionalities depend on I/O coherency support: cudaHostRegister() / cuMemHostRegister() is supported only on platforms which are I/O coherent"
  },
  {
    "id": 20800,
    "content": "The host register support can be queried using the device attribute cudaDevAttrHostRegisterSupported / CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED"
  },
  {
    "id": 20801,
    "content": "CPU cache for pinned memory allocated using cudaMallocHost() / cuMemHostAlloc() / cuMemAllocHost() is enabled only on platforms which are I/O coherent"
  },
  {
    "id": 20804,
    "content": "Estimating Total Allocatable Device Memory on an Integrated GPU Device  The cudaMemGetInfo() API returns the snapshot of free and total amount of memory available for allocation for the GPU"
  },
  {
    "id": 20805,
    "content": "The CPU can control the contents of DRAM and free DRAM memory by moving the contents of DMAR to SWAP area or vice versa The cudaMemGetInfo API may return a smaller size than the actually allocatable memory since the CPU may be able to free up some DRAM region by moving pages to the SWAP area In order to estimate the amount of allocatable device memory, CUDA application developers should consider"
  },
  {
    "id": 20806,
    "content": "following: On Linux and Android platforms: Device allocatable memory on Linux and Android depends mainly on the total and free sizes of swap space and main memory The following points can help users to estimate the total amount of device allocatable memory in various situations: Host allocated memory = Total used physical memory – Device allocated memory If (Host allocated memory Free Swap Space)"
  },
  {
    "id": 20807,
    "content": "then Device allocatable memory = Total Physical Memory – (Host allocated memory - Free swap space) Here, Device allocated memory is memory already allocated on the device"
  },
  {
    "id": 20808,
    "content": "It can be obtained from the NvMapMemUsed field in /proc/meminfo or from the total field of /sys/kernel/debug/nvmap/iovmm/clients If the free command is not available, the same information can be obtained from /proc/meminfo as: Total Used physical memory = MemTotal – MemFree Free swap space = SwapFree On QNX platforms: QNX does not use swap space, hence, cudaMemGetInfo free will be a fair estimate"
  },
  {
    "id": 20811,
    "content": "Porting Considerations  CUDA applications originally developed for dGPUs attached to x86 systems may require modifications to perform efficiently on Tegra systems This section describes the considerations for porting such applications to a Tegra system, such as selecting an appropriate memory buffer type (pinned memory, unified memory, and others) and selecting between iGPU and dGPU, to achieve"
  },
  {
    "id": 20815,
    "content": "Memory Selection  CUDA applications can use various kinds of memory buffers, such as device memory, pageable host memory, pinned memory, and unified memory"
  },
  {
    "id": 20816,
    "content": "Even though these memory buffer types are allocated on the same physical device, each has different accessing and caching behaviors, as shown in Table 1"
  },
  {
    "id": 20817,
    "content": "It is important to select the most appropriate memory buffer type for efficient application execution"
  },
  {
    "id": 20818,
    "content": "For example, in an application with multiple kernels, there may be buffers that are used only by the intermediate kernels of the application as input or output"
  },
  {
    "id": 20820,
    "content": "Pinned Memory Tegra® systems with different compute capabilities exhibit different behavior in terms of I/O coherency For example, Tegra® systems with compute capability greater than or equal to 7"
  },
  {
    "id": 20822,
    "content": "On Tegra® systems with I/O coherency, the CPU access time of pinned memory is as good as pageable host memory because it is cached on the CPU However, on Tegra® systems without I/O coherency, the CPU access time of pinned memory is higher, because it is not cached on the CPU Pinned memory is recommended for small buffers because the caching effect is negligible for such buffers and also because"
  },
  {
    "id": 20823,
    "content": "pinned memory does not involve any additional overhead, unlike Unified Memory With no additional overhead, pinned memory is also preferable for large buffers if the access pattern is not cache friendly on iGPU For large buffers, when the buffer is accessed only once on iGPU in a coalescing manner, performance on iGPU can be as good as unified memory on iGPU On Tegra®, using unified memory in"
  },
  {
    "id": 20824,
    "content": "applications requires additional coherency and cache maintenance operations during the kernel launch, synchronization and prefetching hint calls This coherency maintenance overhead is slightly higher on a Tegra® system with compute capability less than 7 2 as they lack I/O coherency On Tegra® devices with I/O coherency (with a compute capability of 7 2 or greater) where unified memory is cached on"
  },
  {
    "id": 20825,
    "content": "both CPU and iGPU, for large buffers which are frequently accessed by the iGPU and the CPU and the accesses on iGPU are repetitive , unified memory is preferable since repetitive accesses can offset the cache maintenance cost On Tegra® devices without I/O coherency (with a compute capability of less than 7 2), for large buffers which are frequently accessed by the CPU and the iGPU and the accesses"
  },
  {
    "id": 20826,
    "content": "on iGPU are not repetitive , unified memory is still preferable over pinned memory because pinned memory is not cached on both CPU and iGPU Pinned memory or unified memory can be used to reduce the data transfer overhead between CPU and iGPU as both memories are directly accessible from the CPU and the iGPU In an application, input and output buffers that must be accessible on both the host and"
  },
  {
    "id": 20827,
    "content": "the iGPU can be allocated using either unified memory or pinned memory Note The unified memory model requires the driver and system software to manage coherence on the current Tegra SOC Software managed coherence is by nature non-deterministic and not recommended in a safe context Evaluate the impact of unified memory overheads, pinned memory cache misses, and device memory data transfers in"
  },
  {
    "id": 20831,
    "content": "Pinned Memory  This section provides guidelines for porting applications that use pinned memory allocations in x86 systems with dGPUs to Tegra® CUDA applications developed for a dGPU attached to x86 system use pinned memory to reduce data transfer time and to overlap data transfers with kernel execution time For specific information on this topic, see “Data Transfer Between Host and Device” and"
  },
  {
    "id": 20832,
    "content": "“Asynchronous and Overlapping Transfers with Computation” at the following websites “Data Transfer Between Host and Device”: https: docs"
  },
  {
    "id": 20835,
    "content": "html#data-transfer-between-host-and-device “Asynchronous and Overlapping Transfers with Computation”: https: docs"
  },
  {
    "id": 20838,
    "content": "html#asynchronous-transfers-and-overlapping-transfers-with-computation On Tegra® systems with no I/O coherency, repetitive access of pinned memory degrades application performance, because pinned memory is not cached on the CPU in such systems"
  },
  {
    "id": 20839,
    "content": "A sample application is shown below in which a set of filters and operations (k1, k2, and k3) are applied to an image"
  },
  {
    "id": 20840,
    "content": "Pinned memory is allocated to reduce data transfer time on an x86 system with a dGPU, increasing the overall application speed"
  },
  {
    "id": 20841,
    "content": "However, targeting a Tegra® device with the same code causes a drastic increase in the execution time of the readImage() function because it repeatedly accesses an uncached buffer If the time taken by readImage() is significantly higher compared to kernels execution time, it is recommended to use unified memory to reduce the readImage() time"
  },
  {
    "id": 20842,
    "content": "Otherwise, evaluate the application with pinned memory and unified memory by removing unnecessary data transfer calls to decide best suited memory"
  },
  {
    "id": 20843,
    "content": "UseImageonCPU ( h_d ); } Porting the code on Tegra int main () { int * h_a , * d_b , * d_c , * h_d ; int height = 1024 ; int width = 1024 ; size_t sizeOfImage = width * height * sizeof ( int ); 4MB image Unified memory allocated for input and output buffer of application pipeline cudaMallocManaged ( h_a , sizeOfImage , cudaMemAttachHost ); cudaMallocManaged ( h_d , sizeOfImage ); Intermediate"
  },
  {
    "id": 20844,
    "content": "buffers not needed on CPU side So allocate them on device memory cudaMalloc ( & d_b , sizeOfImage ); cudaMalloc ( & d_c , sizeOfImage ); CPU reads Image; readImage ( h_a ); Intialize the h_a buffer - CUDA Application pipeline start - Prefetch input image data to GPU cudaStreamAttachMemAsync ( NULL , h_a , 0 , cudaMemAttachGlobal ); k1 >> ( h_a , d_b ) k2 >> ( d_b , d_c ) k3 >> ( d_c , h_d )"
  },
  {
    "id": 20845,
    "content": "Prefetch output image data to CPU cudaStreamAttachMemAsync ( NULL , h_d , 0 , cudaMemAttachHost ); cudaStreamSynchronize ( NULL ); - CUDA Application pipeline end - Use processed Image i e h_d on CPU side UseImageonCPU ( h_d ); } The cudaHostRegister() function The cudaHostRegister() function is not supported on Tegra® devices with compute capability less than 7"
  },
  {
    "id": 20847,
    "content": "Use other pinned memory allocation functions such as cudaMallocHost() and cudaHostAlloc() if cudaHostRegister() is not supported on the device GNU Atomic operations on pinned memory The GNU atomic operations on uncached memory is not supported on Tegra® CPU As pinned memory is not cached on Tegra® devices with compute capability less than 7 2, GNU atomic operations is not supported on pinned"
  },
  {
    "id": 20851,
    "content": "Effective Usage of Unified Memory on Tegra  Using unified memory in applications requires additional coherency and cache maintenance operations at kernel launch, synchronization, and prefetching hint calls"
  },
  {
    "id": 20852,
    "content": "These operations are performed synchronously with other GPU work which can cause unpredictable latencies in the application"
  },
  {
    "id": 20854,
    "content": "To prefetch the data, the cudaStreamAttachMemAsync() function can be used, in addition to the techniques described in the “Coherency and Concurrency” section of the CUDA C Programming Guide at the following link: https: docs"
  },
  {
    "id": 20858,
    "content": "The prefetching behavior of unified memory, as triggered by the changing states of the attachment flag, is shown in Table 2 Unified Memory Prefetching Behavior per Changing Attachment Flag States  Previous Flag Current Flag Prefetching Behavior cudaMemAttachGlobal/cudaMemAttachSingle cudaMemAttachHost Causes prefetch to CPU cudaMemAttachHost cudaMemAttachGlobal/ cudaMemAttachSingle Causes"
  },
  {
    "id": 20859,
    "content": "prefetch to GPU cudaMemAttachGlobal cudaMemAttachSingle No prefetch to GPU cudaMemAttachSingle cudaMemAttachGlobal No prefetch to GPU The following example shows usage of cudaStreamAttachMemAsync() to prefetch data Note However, not supported on Tegra® devices are the data prefetching techniques that use cudaMemPrefetchAsync() as described in the “Performance Tuning” section of the CUDA C++"
  },
  {
    "id": 20863,
    "content": "html#um-performance-tuning Note There are limitations in QNX system software which prevent implementation of all UVM optimizations Because of this, using cudaStreamAttachMemAsync() to prefetch hints on QNX does not benefit performance"
  },
  {
    "id": 20864,
    "content": "Any mismatch in the width and height values of frame with the values specified in cudaEGLStreamProducerConnect() leads to undefined behavior"
  },
  {
    "id": 20865,
    "content": "In the sample, the CUDA producer is sending a single frame, but it can send multiple frames over a loop The cudaEGLStreamProducerReturnFrame() call waits until it receives the released frame from the consumer Once the CUDA producer presents the first frame to EGLstream, at least one frame is always available for consumer acquisition until the producer disconnects This prevents the removal of the"
  },
  {
    "id": 20866,
    "content": "last frame from EGLStream, which would block cudaEGLStreamProducerReturnFrame () Use the EGL_NV_stream_reset extension to set EGLStream attribute EGL_SUPPORT_REUSE_NV to false to allow the last frame to be removed from EGLStream This allows removing or returning the last frame from EGLStream"
  },
  {
    "id": 20870,
    "content": "CUDA as Consumer  When CUDA is the consumer, the supported producers are CUDA, OpenGL, NvMedia, Argus, and Camera The following consumer side steps are shown in the sample code that follows: Connect consumer to EGLStream (line 5) Connect consumer to EGLStream cudaEGLStreamConsumerConnect(&conn, eglStream); consumer acquires a frame unsigned int timeout = 16000;"
  },
  {
    "id": 20871,
    "content": "cudaEGLStreamConsumerAcquireFrame(& conn, &cudaResource, eglStream, timeout); consumer gets a cuda object pointer cudaGraphicsResourceGetMappedEglFrame(&cudaEgl, cudaResource, 0, 0); size_t numElem = cudaEgl->planeDesc[0] pitch * cudaEgl->planeDesc[0]"
  },
  {
    "id": 20873,
    "content": "int checkIfOne = 1;   Checks if each value in the buffer is 1, if any value is not 1, it sets checkIfOne = 0"
  },
  {
    "id": 20874,
    "content": "} In the sample code, the CUDA consumer receives a single frame, but it can also receive multiple frames over a loop If a CUDA consumer fails to receive a new frame in the specified time limit using cudaEGLStreamConsumerAcquireFrame() , it reacquires the previous frame from EGLStream The application can use eglQueryStreamKHR() to query for the availability of new frames using If the CUDA context"
  },
  {
    "id": 20875,
    "content": "is destroyed while connected to EGLStream, the stream is placed in the EGL_STREAM_STATE_DISCONNECTED_KHR state and the connection handle is invalidated"
  },
  {
    "id": 20880,
    "content": "For example, in the previous code samples, both the producer and consumer threads are running in parallel and the K1 and K2 kernel processes access the same frame, but K2 execution in the consumer thread is guaranteed to occur only after kernel K1 in the producer thread finishes The cudaEGLStreamConsumerAcquireFrame() function waits on the GPU side until K1 finishes and ensures synchronization"
  },
  {
    "id": 20881,
    "content": "between producer and consumer Similarly, cudaEGLStreamProducerReturnFrame() in the producer thread is guaranteed to get the frame only after K2 finishes and the consumer releases the frame These non-blocking calls allow the CPU to do other computation in between, as synchronization is taken care of on the GPU side"
  },
  {
    "id": 20886,
    "content": "Data Transfer Between Producer and Consumer  Data transfer between producer and consumer is avoided when they are present on the same device In a Tegra® platform that includes a dGPU however, such as is in NVIDIA DRIVE™ PX 2, the producer and consumer can be present on different devices In that case, an additional memory copy is required internally to move the frame between Tegra® SoC DRAM and"
  },
  {
    "id": 20887,
    "content": "dGPU DRAM Note On systems where a Tegra® device is connected to a dGPU, if a producer frame uses CUDA array, both producer and consumer should be on the same GPU But if a producer frame uses CUDA device pointers, the consumer can be present on any GPU"
  },
  {
    "id": 20891,
    "content": "EGLStream Pipeline  An application can use multiple EGL streams in a pipeline to pass the frames from one API to another"
  },
  {
    "id": 20892,
    "content": "For an application where NvMedia sends a frame to CUDA for computation, CUDA sends the same frame to OpenGL for rendering after the computation"
  },
  {
    "id": 20893,
    "content": "EGLStream Pipeline  NvMedia and CUDA connect as producer and consumer respectively to one EGLStream"
  },
  {
    "id": 20894,
    "content": "Using multiple EGLStreams in pipeline fashion gives the flexibility to send frames across multiple APIs without allocating additional memory or requiring explicit data transfers The above steps can be performed in a loop to facilitate the transfer of multiple frames in the EGLStream pipeline"
  },
  {
    "id": 20897,
    "content": "EGLImage  An EGLImage interop allows an EGL client API to share image data with other EGL client APIs For example, an application can use an EGLImage interop to share an OpenGL texture with CUDA without allocating any additional memory Note An EGLImage is created using eglCreateImageKHR() and destroyed using eglDestroyImageKHR() For more information see the EGLImage specification at the"
  },
  {
    "id": 20904,
    "content": "CUDA interop with EGLImage  CUDA supports interoperation with EGLImage, allowing CUDA to read or modify the data of an EGLImage In CUDA, a single-planar EGLImage object is represented as a CUDA array or device pointer Similarly, a multi-planar EGLImage object is represented as an array of device pointers or CUDA arrays EGLImage is supported on Tegra® devices running the Linux, QNX, or Android"
  },
  {
    "id": 20906,
    "content": "An application can use cudaGraphicsResourceGetMappedEglFrame() to get a frame from the graphics resource object"
  },
  {
    "id": 20907,
    "content": "The frameType parameter in cudaEglFrame indicates if the frame is a CUDA device pointer or a CUDA array For a single planar graphics resource, an application can directly obtain a device pointer or CUDA array using cudaGraphicsResourceGetMappedPointer() or cudaGraphicsSubResourceGetMappedArray() respectively"
  },
  {
    "id": 20908,
    "content": "Also, cudaGraphicsEGLRegisterImage() expects only the ‘0’ flag as other API flags are for future use"
  },
  {
    "id": 20909,
    "content": "The pArray array is bound to a surface object to allow modification of the OpenGL texture in the changeTexture"
  },
  {
    "id": 20910,
    "content": "unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char)));   Get the modified texture values GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp));"
  },
  {
    "id": 20911,
    "content": "This function check if the OpenGL texture got modified values checkbuf(temp,hostSurf);   Clean up CUDA cudaGraphicsUnregisterResource(pResource); cudaDestroySurfaceObject(inputSurfObj); eglDestroySyncKHR(eglDisplayHandle, eglsync1); eglDestroySyncKHR(eglDisplayHandle, eglsync2); cudaEventDestroy(egl_event); cudaEventDestroy(cuda_event);"
  },
  {
    "id": 20920,
    "content": "From Network Repositories or Local Installers  The CUDA Downloads page provides step-by-step instructions on how to download and use the local installer or CUDA network repositories to install the latest Toolkit"
  },
  {
    "id": 20921,
    "content": "The CUDA upgrade package gets downloaded and installed along with the corresponding CUDA toolkit for Linux-aarch64-jetson devices For use cases where applications are built on the host and require just the CUDA upgrade package to be installed independently on the target, the corresponding Debians can be found in CUDA Repos"
  },
  {
    "id": 20923,
    "content": "8 for example, this can be installed by running the command: $ sudo apt-get install -y cuda-compat-11-8 Note This is the recommended path for CUDA upgrade for devices that have disk space (secondary storage) limitations"
  },
  {
    "id": 20927,
    "content": "* - the JIT (just-in-time) compiler for PTX files nvidia-cuda-mps-control - CUDA MPS control executable nvidia-cuda-mps-server - CUDA MPS server executable These files together implement the CUDA 11"
  },
  {
    "id": 20929,
    "content": "Example The following commands show how the CUDA Upgrade package can be installed and used to run the applications The following additional packages will be installed: cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8 The following NEW packages will be installed: cuda cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8"
  },
  {
    "id": 20935,
    "content": "31339915-1 [15 8 MB] Fetched 15 7 MB in 12s (1,338 kB/s) Selecting previously unselected package cuda-compat-11-8"
  },
  {
    "id": 20943,
    "content": "8, NumDevs = 1 Result = PASS Only a single CUDA upgrade package can be installed at any point in time on a given system While installing a new CUDA upgrade package, the previous version of the installed upgrade package will be removed and replaced with the new one The default drivers (originally installed with the NVIDIA JetPack and part of the L4T BSP) will be retained by the installer The"
  },
  {
    "id": 20944,
    "content": "application has the ability to use either the default version of CUDA (originally installed with NVIDIA JetPack) or the one installed by the upgrade package In addition to LD_LIBRARY_PATH , CUDA MPS users must set the PATH variable in order to use the nvidia-cuda-mps-* executables installed by the upgrade package before starting MPS and running the CUDA applications that use MPS The MPS"
  },
  {
    "id": 20945,
    "content": "executables installed with the upgrade package are only compatible with the CUDA driver installed with the same upgrade package, and vice versa, which can be checked with the version info Installation of the upgrade package will fail if it is not compatible with the NVIDIA JetPack version"
  },
  {
    "id": 20951,
    "content": "Use the Right Upgrade Package  The CUDA upgrade package is named after the highest toolkit that it can support"
  },
  {
    "id": 20955,
    "content": "Each CUDA release will support upgrades only for a specific set of NVIDIA JetPack releases If a new feature in the latest CUDA driver needs an updated NVIDIA JetPack SDK component/interface, it might not work and error out when used"
  },
  {
    "id": 20959,
    "content": "Check for Compatibility Support  In addition to the CUDA driver and certain compiler components, there are other drivers in the NVIDIA JetPack that remain at the default version"
  },
  {
    "id": 20960,
    "content": "A well-written application should use following error codes to determine if CUDA Upgrade is supported System administrators should be aware of these error codes to determine if there are errors in the deployment This error indicates that there is a mismatch between the versions of the upgraded CUDA driver and the already installed drivers on the system This error indicates that the system was"
  },
  {
    "id": 20961,
    "content": "updated to run with the CUDA upgrade package but the visible hardware detected by CUDA does not support this configuration"
  },
  {
    "id": 20963,
    "content": "cuDLA  DLA (Deep Learning Accelerator) is a fixed function accelerator present on the NVIDIA Tegra SoC and is used for inference applications"
  },
  {
    "id": 20964,
    "content": "The DLA HW has superior performance/W and can natively run many of the layers in modern neural networks, thus making it an attractive value proposition for embedded AI applications Programming the DLA typically consists of an offline and online step: in the offline step, an input network is parsed and compiled by the DLA compiler into a loadable and in the online step, that loadable is executed"
  },
  {
    "id": 20965,
    "content": "by the DLA HW to generate an inference result The SW stack that is currently provided by NVIDIA to perform the online or execution step consists of NvMediaDla and the DLA runtime/KMD Together, these APIs enable the user to submit a DLA task to the DLA HW for inferencing purposes DLA SW stack  It follows from the model above that users wishing to use GPU and DLA together in an application would"
  },
  {
    "id": 20966,
    "content": "have to use interop mechanisms such as EGLStreams/NvSci to share buffers as well as synchronization primitives between the GPU and DLA These interop mechanisms usually involve many steps for each buffer that is being shared and have limited ability to fine-tune the scheduling of tasks between the GPU and DLA cuDLA is an extension of the CUDA programming model that integrates DLA (Deep Learning"
  },
  {
    "id": 20967,
    "content": "Accelerator) with CUDA thereby making it possible to submit DLA tasks using CUDA programming constructs such as streams and graphs Managing shared buffers as well as synchronizing the tasks between GPU and DLA is transparently handled by cuDLA, freeing up the programmer to focus on the high-level usecase"
  },
  {
    "id": 20970,
    "content": "Developer Guide  This section describes the key principles involved in programming the DLA HW using cuDLA APIs The cuDLA interfaces expose mechanisms to initialize devices, manage memory and submit DLA tasks The detailed specification of these APIs is described in the API specification and should be referred while writing a cuDLA application"
  },
  {
    "id": 20971,
    "content": "Since cuDLA is an extension of CUDA, it is designed to work in conjunction with CUDA APIs that perform CUDA functions such as GPU management, context management etc"
  },
  {
    "id": 20972,
    "content": "Therefore, the current state of the application in terms of which GPU is selected and the current active context (and its lifecycle) are all important considerations while evaluating the behavior of a cuDLA API"
  },
  {
    "id": 20976,
    "content": "Device Model  To perform any DLA operation, it is necessary that an application first create a cuDLA device handle The cudlaCreateDevice() API creates a logical instance of a cuDLA device wherein the selected DLA HW instance is coupled with the current active GPU selected via CUDA For example, the following code snippet would create a logical instance consisting of the current GPU (set via"
  },
  {
    "id": 20977,
    "content": "cudaSetDevice() ) and DLA HW 0 Currently, cuDLA supports only iGPU on Tegra and an attempt to create a device handle by setting the current GPU as a dGPU would result in a device creation error during cudlaCreateDevice() cudlaDevHandle devHandle ; cudlaStatus ret ; ret = cudlaCreateDevice ( 0 , & devHandle , CUDLA_CUDA_DLA ); Device model  The user can create any number of such logical instances"
  },
  {
    "id": 20978,
    "content": "using cudlaCreateDevice() using any combination of GPU and DLA HW instances (subject to system resource availability): Device model - multiple instances  In addition, cudlaCreateDevice() supports an alternative flag during device creation - CUDLA_STANDALONE This flag can be used by applications when they wish to create a cuDLA device in standalone mode i e without coupling it with a GPU device"
  },
  {
    "id": 20979,
    "content": "All device submissions can be accomplished using cuDLA in standalone mode as well but in this mode there is no support for CUDA interactions Consequently, in what follows, two modes of execution are considered while describing a particular API or a particular usecase: the hybrid mode and the standalone mode The API spec has complete details about which API is supported in which mode"
  },
  {
    "id": 20983,
    "content": "Loading and Querying Modules  The cuDLA device handle needs an appropriate loadable to be associated with it before any DLA task submission occurs The loadable has information about the number of input and output tensors as well as their respective metadata and can be queried by the application to retrieve this information A typical application flow after a successful cuDLA device initialization"
  },
  {
    "id": 20984,
    "content": "would look like this (interspersed with some debug logs): DPRINTF ( \"Device created successfully \" ); Load the loadable from 'loadableData' in which the loadable binary has been copied from the location of the loadable - disk or otherwise"
  },
  {
    "id": 20985,
    "content": "err = cudlaModuleLoadFromMemory ( devHandle , loadableData , file_size , & amp ; moduleHandle , 0 ); if ( err"
  },
  {
    "id": 20987,
    "content": "uint32_t numInputTensors = 0 ; uint32_t numOutputTensors = 0 ; cudlaModuleAttribute attribute ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_NUM_INPUT_TENSORS , & amp ; attribute ); if ( err = cudlaSuccess ) { handle error } numInputTensors = attribute numInputTensors ; DPRINTF ( \"numInputTensors = %d \" , numInputTensors ); err = cudlaModuleGetAttributes ( moduleHandle ,"
  },
  {
    "id": 20988,
    "content": "CUDLA_NUM_OUTPUT_TENSORS , & amp ; attribute ); if ( err = cudlaSuccess ) { handle error } numOutputTensors = attribute numOutputTensors ; DPRINTF ( \"numOutputTensors = %d \" , numOutputTensors ); cudlaModuleTensorDescriptor * inputTensorDesc = ( cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numInputTensors ); cudlaModuleTensorDescriptor * outputTensorDesc = ("
  },
  {
    "id": 20989,
    "content": "cudlaModuleTensorDescriptor * ) malloc ( sizeof ( cudlaModuleTensorDescriptor ) * numOutputTensors ); if (( inputTensorDesc == NULL ) || ( outputTensorDesc == NULL )) { handle error } attribute inputTensorDesc = inputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_INPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err = cudlaSuccess ) { handle error } attribute"
  },
  {
    "id": 20990,
    "content": "outputTensorDesc = outputTensorDesc ; err = cudlaModuleGetAttributes ( moduleHandle , CUDLA_OUTPUT_TENSOR_DESCRIPTORS , & amp ; attribute ); if ( err = cudlaSuccess ) { handle error } Applications can use the retrieved tensor descriptors to setup their data buffers in terms of size and formats Detailed information about the contents of the tensor descriptors is present in the API specification"
  },
  {
    "id": 20995,
    "content": "Memory Model  The GPU and DLA have different MMUs that manage the VA to PA conversion while performing their respective functions"
  },
  {
    "id": 20996,
    "content": "The figure below shows an example where the GMMU performs a translation for GPU VAs and the SMMU performs a similar function for the VAs arriving from the DLA"
  },
  {
    "id": 20997,
    "content": "Virtual address to physical address conversion  In hybrid mode, before a CUDA pointer can be accessed by the DLA, it is necessary that the CUDA pointer be registered with the DLA"
  },
  {
    "id": 20998,
    "content": "This registration step creates an entry in the SMMU and returns the corresponding VA for use in task submissions The following code snippet shows an example registration for a device handle created with the flag CUDLA_CUDA_DLA :   Allocate memory on GPU"
  },
  {
    "id": 21001,
    "content": "uint64_t * bufferRegisteredPtr = NULL ; err = cudlaMemRegister ( devHandle , ( uint64_t * ) inputBufferGPU , size , & bufferRegisteredPtr , 0 ); if ( err"
  },
  {
    "id": 21002,
    "content": "= cudlaSuccess ) {   handle error } In standalone mode, cuDLA functions without the underlying CUDA device"
  },
  {
    "id": 21003,
    "content": "Consequently, in this mode, the memory allocations performed by the application (which need to be subsequently registered) need to come from outside CUDA"
  },
  {
    "id": 21004,
    "content": "On Tegra systems, cuDLA supports registration of NvSciBuf allocations via the cudlaImportExternalMemory() API as the following code snippet shows:   Allocate the NvSciBuf object"
  },
  {
    "id": 21005,
    "content": "NvSciBufObj inputBufObj ; sciError = NvSciBufObjAlloc ( reconciledInputAttrList , & inputBufObj ); if ( sciError"
  },
  {
    "id": 21006,
    "content": "= NvSciError_Success ) {   handle error } uint64_t * inputBufObjRegPtr = NULL ;   importing external memory cudlaExternalMemoryHandleDesc memDesc = { 0 }; memset ( & memDesc , 0 , sizeof ( memDesc )); memDesc size = size ; err = cudlaImportExternalMemory ( devHandle , & memDesc , & inputBufObjRegPtr , 0 ); if ( err"
  },
  {
    "id": 21010,
    "content": "Task Execution  Submitting a DLA task for execution is similar to submitting a CUDA kernel to the GPU cuDLA natively supports CUDA streams and works seamlessly with the stream semantics to ensure that all tasks intended for the DLA are executed by the DLA HW only after the previous tasks on the stream have completed execution This enables applications to setup complex processing workflows"
  },
  {
    "id": 21011,
    "content": "between the GPU and the DLA using familiar stream semantics without having to manage memory coherency and execution dependencies between GPU and DLA DLA tasks can be interspersed with GPU tasks in a given stream or multiple streams and cudlaSubmitTask() handles all the memory/execution dependencies cuDLA task execution model  The submit task API needs the input and output tensors in the form of"
  },
  {
    "id": 21012,
    "content": "the addresses registered with the DLA (using cudlaMemRegister() ) An application can pre-register all the required pointers with cuDLA and then use the registered pointers during cudlaSubmitTask() This API, in turn, ensures that the results of the previous operations on the underlying memory corresponding to the registered pointers is visible to the DLA before it begins execution of the current"
  },
  {
    "id": 21013,
    "content": "task A typical application code consisting of CUDA and cuDLA operations is shown in the snippet below: DPRINTF ( \"ALL MEMORY REGISTERED SUCCESSFULLY \" ); Copy data from CPU buffers to GPU buffers"
  },
  {
    "id": 21015,
    "content": "= cudaSuccess ) {   handle error } result = cudaMemsetAsync ( outputBufferGPU , 0 , outputTensorDesc [ 0 ]"
  },
  {
    "id": 21020,
    "content": "= cudaSuccess ) {   handle error } In standalone mode, the stream parameter in cudlaSubmitTask() must be specified as NULL as cuDLA is operating independently of CUDA"
  },
  {
    "id": 21027,
    "content": "Multithreaded User Submission  Users can specify the CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE flag during submission to a particular device handle if they are sure that submission to this particular device handle occurs only in this thread and that there is no shared data at the application level between this device handle and any other device handle which might be used in a parallel thread for submission"
  },
  {
    "id": 21028,
    "content": "This flag facilitates some optimizations in the submission path which might lead to better submission times from the application point of view"
  },
  {
    "id": 21034,
    "content": "Since DLA tasks are submitted to CUDA streams, it is sufficient to wait on the stream to complete its work in order to ensure that all DLA tasks submitted on that stream are completed In this regard DLA task synchronization is compatible with any of the different synchronization mechanisms available in CUDA – Event, Stream, Device – and the entire CUDA machinery is available for applications to"
  },
  {
    "id": 21035,
    "content": "setup different flows and usecases In standalone mode, however, the synchronization mechanisms are different given that cuDLA operates independently of CUDA"
  },
  {
    "id": 21036,
    "content": "In this mode, the cudlaTask structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively as part of cudlaSubmitTask() Each submitted task will wait for all its wait events to be signaled before beginning execution and will provide a signal event (if one is requested for during cudlaSubmitTask() ) that the application (or any other entity) can wait on"
  },
  {
    "id": 21037,
    "content": "to ensure that the submitted task has completed execution Furthermore, only NvSciSync objects can be registered and signaled as part of signal events and the fence corresponding to the signaled event is returned as part of cudlaSubmitTask() Like all memory operations, the underlying backing store for the events (in this case the NvSciSync object) must be registered with cuDLA before using it in a"
  },
  {
    "id": 21039,
    "content": "extSyncObject = syncObj1 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr1 , 0 ); if ( err = cudlaSuccess ) { handle error } memset ( & semaMemDesc , 0 , sizeof ( semaMemDesc )); semaMemDesc extSyncObject = syncObj2 ; err = cudlaImportExternalSemaphore ( devHandle , & semaMemDesc , & nvSciSyncObjRegPtr2 , 0 ); if ( err = cudlaSuccess ) { handle error }"
  },
  {
    "id": 21045,
    "content": "Events setup for cudlaSubmitTask()    Wait events NvSciSyncFence preFence = NvSciSyncFenceInitializer ; sciError = NvSciSyncObjGenerateFence ( syncObj1 , & preFence ); if ( sciError"
  },
  {
    "id": 21046,
    "content": "= NvSciError_Success ) { handle error } cudlaWaitEvents * waitEvents ; waitEvents = ( cudlaWaitEvents * ) malloc ( sizeof ( cudlaWaitEvents )); if ( waitEvents == NULL ) { handle error } waitEvents -> numEvents = 1 ; CudlaFence * preFences = ( CudlaFence * ) malloc ( waitEvents -> numEvents * sizeof ( CudlaFence )); if ( preFences == NULL ) { handle error } preFences [ 0 ] type ="
  },
  {
    "id": 21047,
    "content": "CUDLA_NVSCISYNC_FENCE ; waitEvents -> preFences = preFences ; Signal Events cudlaSignalEvents * signalEvents ; signalEvents = ( cudlaSignalEvents * ) malloc ( sizeof ( cudlaSignalEvents )); if ( signalEvents == NULL ) { handle error } signalEvents -> numEvents = 1 ; uint64_t ** devPtrs = ( uint64_t ** ) malloc ( signalEvents -> numEvents * sizeof ( uint64_t * )); if ( devPtrs == NULL ) { handle"
  },
  {
    "id": 21048,
    "content": "error } devPtrs [ 0 ] = nvSciSyncObjRegPtr2 ; signalEvents -> devPtrs = devPtrs ; signalEvents -> eofFences = ( CudlaFence * ) malloc ( signalEvents -> numEvents * sizeof ( CudlaFence )); if ( signalEvents -> eofFences == NULL ) { handle error } NvSciSyncFence eofFence = NvSciSyncFenceInitializer ; signalEvents -> eofFences [ 0 ] signalEvents = signalEvents ; err = cudlaSubmitTask ( devHandle , &"
  },
  {
    "id": 21056,
    "content": "In practice, this signal will be done by another   entity or driver that provides the data input for this particular submitted task In practice, this wait will be done by   another entity or driver that is waiting for the output of the submitted task"
  },
  {
    "id": 21063,
    "content": "Supported Synchronization Primitives in cuDLA  cuDLA supports two types of NvSciSync object primitives By default, cuDLA prioritizes sync point primitive over deterministic semaphore primitive and sets these priorities in the NvSciSync attribute list when requested by the application using cudlaGetNvSciSyncAttributes() For Deterministic semaphore, the NvSciSync attribute list used to create the"
  },
  {
    "id": 21064,
    "content": "NvSciSync object must have the value of NvSciSyncAttrKey_RequireDeterministicFences key set to true Deterministic fences allow users to enqueue a wait over the semaphore object even before corresponding signal is enqueued For such semaphore object, cuDLA guarantees that each signal operation will increment the fence value by ‘1’ Users are expected to keep track of signals enqueued on the semaphore"
  },
  {
    "id": 21071,
    "content": "Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList    Set NvSciSyncAttrKey_RequireDeterministicFences key to true in   NvScisyncAtrrList that is used to create NvSciSync object with   Deterministic Semaphore primitive"
  },
  {
    "id": 21077,
    "content": "Timestamp Support for NvSciFence  cuDLA supports the timestamp feature of NvSci in cuDLA standalone mode Timestamp support enables users to get the time at which a particular fence has been signaled cuDLA users can request timestamp support by setting the value of the NvSciSyncAttrKey_WaiterRequireTimestamps key as true while filling up the NvSci waiter attribute list The users can use this"
  },
  {
    "id": 21078,
    "content": "timestamp along with SOF(Start Of Frame) fence and EOF(End OF Frame) fence to get a snapshot of DLA clock just before start of task & after task completion respectively This enables users to calculate time taken by DLA to execute the submitted task"
  },
  {
    "id": 21084,
    "content": "Requesting Timestamp Support for NvSciSync Object  sciError fillCpuWaiterAttrList ( NvSciSyncAttrList list ) { bool cpuWaiter = true ; NvSciSyncAttrKeyValuePair keyValue [ 3 ]; memset ( keyValue , 0 , sizeof ( keyValue )); keyValue [ 0 ] len = sizeof ( cpuWaiter ); NvSciSyncAccessPerm cpuPerm = NvSciSyncAccessPerm_WaitOnly ; keyValue [ 1 ]"
  },
  {
    "id": 21085,
    "content": "Extracting Timestamp Value from Fence  Refer to these sections for more information: Registering an external semaphore: Events setup for cudlaSubmitTask() Waiting on the signal event To extract Timestamp of the fence Timestamp will be valid only after fence is signaled hence Fence must be waited up on before extracting timestamp value uint64_t eofTimestampUS = 0UL ; sciError ="
  },
  {
    "id": 21086,
    "content": "NvSciSyncFenceGetTimestamp ( reinterpret_cast ( signalEvents -> eofFences fence ), & ( eofTimestampUS )); if (( sciError = NvSciError_Success ) || ( eofTimestampUS == 0UL )) { handle error } 7"
  },
  {
    "id": 21090,
    "content": "Fault Diagnostics  To perform fault diagnostics for DLA HW, users should specify the CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS flag to load the module and CUDLA_SUBMIT_DIAGNOSTICS_TASK during task submission"
  },
  {
    "id": 21091,
    "content": "With this flag set, in standalone mode the user is not allowed to do event only submissions, where tensor information is NULL and only events (wait/signal or both) are present in task"
  },
  {
    "id": 21092,
    "content": "This diagnostic module does not expect any input tensors and so does not require input tensor memory However the user is expected to query the number of output tensors, allocate the output tensor memory, and pass the same while using the submit task"
  },
  {
    "id": 21098,
    "content": "Specifically, the task is submitted to DLA, wait/signal events are considered before and after and stream semantics are respected"
  },
  {
    "id": 21103,
    "content": "Error Reporting Model  The asynchronous nature of task execution results in two kinds of errors that can get reported via cuDLA APIs: Synchronous errors Asynchronous errors Synchronous errors are those that are reported by the cuDLA APIs as part of their return code when they are invoked in an application Asynchronous errors are those that are detected later compared to sequential program"
  },
  {
    "id": 21105,
    "content": "The typical scenario here is that each task submitted to the DLA HW executes after a particular duration of time As a result, if there are errors in the task execution, they cannot be reported as part of the task submission APIs Depending on the timing of the errors, they are reported during a subsequent cuDLA API call or after a synchronization operation HW execution errors reported as part of"
  },
  {
    "id": 21106,
    "content": "cuDLA APIs are straightforward to handle at the application level However, if there is a no cuDLA API call currently executing or about to execute in the application, then the application needs to perform extra steps to handle asynchronous errors As mentioned in the device model section, cuDLA logically associates DLA with a GPU for the purposes of execution The user needs to check for"
  },
  {
    "id": 21107,
    "content": "DLA-specific errors from CUDA synchronization operations and then check the cuDLA device handle for the exact error using cudlaGetLastError() If there are multiple cuDLA device handles in the application and each of them have submitted some tasks to cuDLA in hybrid mode, then each and every device handle much be checked for errors The underlying model here is to use CUDA to detect DLA HW errors"
  },
  {
    "id": 21109,
    "content": "The code snippet below shows an example: result = cudaStreamSynchronize ( stream ); if ( result = cudaSuccess ) { DPRINTF ( \"Error in synchronizing stream = %s \" , cudaGetErrorName ( result )); if ( result == cudaErrorExternalDevice ) { cudlaStatus hwStatus = cudlaGetLastError ( devHandle ); if ( hwStatus = cudlaSuccess ) { DPRINTF ( \"Asynchronous error in HW = %u \" , hwStatus ); } } } This error"
  },
  {
    "id": 21110,
    "content": "reporting model is compatible with CUDA Driver APIs as well and therefore if the application uses CUDA Driver APIs for synchronization, similar error codes and error handling flow is applicable In standalone mode, the model is similar with the exception that there is no corresponding mechanism to detect errors as part of synchronization operations"
  },
  {
    "id": 21111,
    "content": "In this mode, the only option that an application has to wait on the submitted tasks is to wait on the NvSciSync fence returned by the latest submission As of this writing, NvSciSync does not support reporting DLA HW errors and therefore an application is expected to wait for the fence and then query cudlaGetLastError() for any errors during execution"
  },
  {
    "id": 21114,
    "content": "Migrating from NvMediaDla to cuDLA  NvMediaDla and cuDLA have different programming models with some degree of overlap in the functionality exposed by the respective APIs The following table provides a mapping from the NvMediaDla API to the equivalent cuDLA API or functionality This is intended to be used as a reference when migrating an NvMediaDla app to a cuDLA app NvMediaDla cuDLA"
  },
  {
    "id": 21115,
    "content": "NvMediaDlaGetVersion() cudlaGetVersion() NvMediaDlaPingById() Not required as ping is done inside cudlaCreateDevice and only upon successful ping does device handle creation succeed"
  },
  {
    "id": 21116,
    "content": "NvMediaDlaCreate() cudlaCreateDevice() NvMediaDlaDestroy() cudlaDestroyDevice() NvMediaDlaGetUMDVersion() Not available NvMediaDlaGetNumEngines() cudlaDeviceGetCount() NvMediaDlaGetMaxOutstandingTasks() Not available NvMediaDlaInit() cudlaCreateDevice (but specifying number of input tasks is not available) NvMediaDlaGetInstanceId() Not available NvMediaDlaGetNumTasks() Not available"
  },
  {
    "id": 21117,
    "content": "NvMediaDlaLoadableCreate() Not required as declaring a variable of type cudlaModule is sufficient alongwith cudlaModuleLoadFromMemory() NvMediaDlaLoadableDestroy() Not required as cuDLA modules are declared as variables of type cudlaModule NvMediaDlaAppendLoadable() Not required as this is done inside cudlaModuleLoadFromMemory() NvMediaDlaSetCurrentLoadable() Not required as this is done inside"
  },
  {
    "id": 21118,
    "content": "cudlaModuleLoadFromMemory() NvMediaDlaGetNumOfInputTensors() cudlaModuleGetAttributes() NvMediaDlaGetInputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaGetNumOfOutputTensors() cudlaModuleGetAttributes() NvMediaDlaGetOutputTensorDescriptor() cudlaModuleGetAttributes() NvMediaDlaDataRegister() cudlaMemRegister() NvMediaDlaDataUnregister() cudlaMemUnregister() NvMediaDlaLoadLoadable()"
  },
  {
    "id": 21119,
    "content": "cudlaModuleLoadFromMemory() NvMediaDlaRemoveLoadable() cudlaModuleUnload() NvMediaDlaSubmit() cudlaSubmitTask() NvMediaDlaNvSciSyncGetVersion() Not available NvMediaDlaFillNvSciSyncAttrList() cudlaGetNvSciSyncAttributes() NvMediaDlaRegisterNvSciSyncObj() cudlaImportExternalSemaphore() NvMediaDlaUnregisterNvSciSyncObj() cudlaMemUnregister() NvMediaDlaSetNvSciSyncObjforEOF() Not required as"
  },
  {
    "id": 21120,
    "content": "cudlaTask structure has the required capability to specify this NvMediaDlaInsertPreNvSciSyncFence() Not required as cudlaTask structure has the required capability to specify this NvMediaDlaGetEOFNvSciSyncFence() Not required as cudlaTask structure has the required capability to retrieve this"
  },
  {
    "id": 21123,
    "content": "Profiling a cuDLA App  cuDLA APIs can be profiled using NVIDIA Nsight Systems cuDLA Release Notes  Known Issues in cuDLA 1"
  },
  {
    "id": 21125,
    "content": "1: In hybrid mode, cuDLA internally allocates memory with CUDA using the primary context As a result, before destroying/resetting a CUDA primary context, it is mandatory that all cuDLA device initializations are destroyed Before destroying a cuDLA device handle, it is important to ensure that all tasks submitted previously to the device are completed"
  },
  {
    "id": 21126,
    "content": "Failure to do so can lead to application crashes as the internal memory allocations would still be in use"
  },
  {
    "id": 21128,
    "content": "It is the application’s responsibility to ensure that there are no duplicate fences specified as part of wait events while submitting tasks"
  },
  {
    "id": 21129,
    "content": "In general, any synchronous or asynchronous error returned by cuDLA APIs must be treated as a non-recoverable error"
  },
  {
    "id": 21130,
    "content": "In this case, the application is expected to restart and initialize cuDLA again in order to submit DLA tasks The exception to this rule is cudlaErrorMemoryRegistered which is returned by cuDLA when the application tries to register a particular memory again without unregistering"
  },
  {
    "id": 21135,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 21136,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 21138,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 21139,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 21140,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 21141,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 21142,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 21143,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 21144,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 21145,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 21146,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 21147,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 21148,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 21155,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 21157,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 21158,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 21164,
    "content": "5 | PDF | Archive Incomplete-LU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS White paper describing how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods"
  },
  {
    "id": 21165,
    "content": "Introduction  The solution of large sparse linear systems is an important problem in computational mechanics, atmospheric modeling, geophysics, biology, circuit simulation and many other applications in the field of computational science and engineering"
  },
  {
    "id": 21167,
    "content": "Although the direct methods are often more reliable, they usually have large memory requirements and do not scale well on massively parallel computer platforms"
  },
  {
    "id": 21168,
    "content": "The iterative methods are more amenable to parallelism and therefore can be used to solve larger problems Currently, the most popular iterative schemes belong to the Krylov subspace family of methods"
  },
  {
    "id": 21169,
    "content": "They include Bi-Conjugate Gradient Stabilized (BiCGStab) and Conjugate Gradient (CG) iterative methods for nonsymmetric and symmetric positive definite (s"
  },
  {
    "id": 21173,
    "content": "In practice, we often use a variety of preconditioning techniques to improve the convergence of the iterative methods In this white paper we focus on the incomplete-LU and Cholesky preconditioning [11] , which is one of the most popular of these preconditioning techniques"
  },
  {
    "id": 21174,
    "content": "It computes an incomplete factorization of the coefficient matrix and requires a solution of lower and upper triangular linear systems in every iteration of the iterative method"
  },
  {
    "id": 21175,
    "content": "In order to implement the preconditioned BiCGStab and CG we use the sparse matrix-vector multiplication [3] , [15] and the sparse triangular solve [8] , [16] implemented in the cuSPARSE library"
  },
  {
    "id": 21176,
    "content": "We point out that the underlying implementation of these algorithms takes advantage of the CUDA parallel programming paradigm [5] , [9] , [13] , which allows us to explore the computational resources of the graphical processing unit (GPU)"
  },
  {
    "id": 21177,
    "content": "In our numerical experiments the incomplete factorization is performed on the CPU (host) and the resulting lower and upper triangular factors are then transferred to the GPU (device) memory before starting the iterative method However, the computation of the incomplete factorization could also be accelerated on the GPU"
  },
  {
    "id": 21178,
    "content": "We point out that the parallelism available in these iterative methods depends highly on the sparsity pattern of the coefficient matrix at hand In our numerical experiments the incomplete-LU and Cholesky preconditioned iterative methods achieve on average more than 2x speedup using the cuSPARSE and cuBLAS libraries on the GPU over the MKL [17] implementation on the CPU For example, the speedup"
  },
  {
    "id": 21179,
    "content": "for the preconditioned iterative methods with the incomplete-LU and Cholesky factorization with 0 fill-in (ilu0) is shown in Figure 1 for matrices resulting from a variety of applications Iterative Methods  In the next sections we briefly describe the methods of interest and comment on the role played in them by the parallel sparse matrix-vector multiplication and triangular solve algorithms"
  },
  {
    "id": 21181,
    "content": "Preconditioned Iterative Methods  Let us consider the linear system \\(A\\mathbf{x} = \\mathbf{f}\\) where \\(A \\in \\mathbb{R}^{n \\times n}\\) is a nonsingular coefficient matrix and \\(\\mathbf{x},\\mathbf{f} \\in \\mathbb{R}^{n}\\) are the solution and right-hand-side vectors"
  },
  {
    "id": 21182,
    "content": "In general, the iterative methods start with an initial guess and perform a series of steps that find more accurate approximations to the solution There are two types of iterative methods: (i) the stationary iterative methods, such as the splitting-based Jacobi and Gauss-Seidel (GS), and (ii) the nonstationary iterative methods, such as the Krylov subspace family of methods, which includes CG and"
  },
  {
    "id": 21183,
    "content": "BiCGStab The convergence of the iterative methods depends highly on the spectrum of the coefficient matrix and can be significantly improved using preconditioning The preconditioning modifies the spectrum of the coefficient matrix of the linear system in order to reduce the number of iterative steps required for convergence It often involves finding a preconditioning matrix \\(M\\) , such that"
  },
  {
    "id": 21184,
    "content": "\\(M^{- 1}\\) is a good approximation of \\(A^{- 1}\\) and the systems with \\(M\\) are relatively easy to solve"
  },
  {
    "id": 21185,
    "content": "matrix \\(A\\) we can let \\(M\\) be its incomplete-Cholesky factorization, so that \\(A \\approx M = {\\widetilde{R}}^{T}\\widetilde{R}\\) , where \\(\\widetilde{R}\\) is an upper triangular matrix Let us assume that \\(M\\) is nonsingular, then \\({\\widetilde{R}}^{- T}A{\\widetilde{R}}^{- 1}\\) is s"
  },
  {
    "id": 21188,
    "content": "The corresponding CG code using the cuSPARSE and cuBLAS libraries in C programming language is shown below"
  },
  {
    "id": 21191,
    "content": "The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- Cholesky upper triangular factor R (valR, csrRowPtrR, csrColIndR) have been computed and are present in the device (GPU) memory */ create the info and analyse the lower and upper triangular factors cusparseCreateSolveAnalysisInfo ( & inforRt ); cusparseCreateSolveAnalysisInfo ( & inforR ); cusparseDcsrsv_analysis ( handle ,"
  },
  {
    "id": 21192,
    "content": "CUSPARSE_OPERATION_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforRt ); cusparseDcsrsv_analysis ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , descrR , valR , csrRowPtrR , csrColIndR , inforR ); 1: compute initial residual r = f - A x0 (using initial guess in x) cusparseDcsrmv ( handle , CUSPARSE_OPERATION_NON_TRANSPOSE , n , n , 1 0 , descrA , valA , csrRowPtrA , csrColIndA , x"
  },
  {
    "id": 21197,
    "content": "The corresponding BiCGStab code using the cuSPARSE and cuBLAS libraries in C programming language is shown below"
  },
  {
    "id": 21198,
    "content": "The matrix A (valA, csrRowPtrA, csrColIndA) and the incomplete- LU lower L (valL, csrRowPtrL, csrColIndL) and upper U (valU, csrRowPtrU, csrColIndU) triangular factors have been computed and are present in the device (GPU) memory"
  },
  {
    "id": 21199,
    "content": "The sparse matrix-vector multiplication has already been extensively studied in the following references [3] , [15]"
  },
  {
    "id": 21200,
    "content": "The sparse triangular solve is not as well known, so we briefly point out the strategy used to explore parallelism in it and refer the reader to the NVIDIA technical report [8] for further details The Splitting of Total Time Taken on the GPU by the Preconditioned Iterative Method  To understand the main ideas behind the sparse triangular solve, notice that although the forward and back"
  },
  {
    "id": 21201,
    "content": "substitution is an inherently sequential algorithm for dense triangular systems, the dependencies on the previously obtained elements of the solution do not necessarily exist for the sparse triangular systems We pursue the strategy that takes advantage of the lack of these dependencies and split the solution process into two phases as mentioned in [1] , [4] , [6] , [7] , [8] , [10] , [12] , [14]"
  },
  {
    "id": 21202,
    "content": "The analysis phase builds the data dependency graph that groups independent rows into levels based on the matrix sparsity pattern The solve phase iterates across the constructed levels one-by-one and computes all elements of the solution corresponding to the rows at a single level in parallel Notice that by construction the rows within each level are independent of each other, but are dependent"
  },
  {
    "id": 21203,
    "content": "on at least one row from the previous level The analysis phase needs to be performed only once and is usually significantly slower than the solve phase, which can be performed multiple times"
  },
  {
    "id": 21204,
    "content": "This arrangement is ideally suited for the incomplete-LU and Cholesky preconditioned iterative methods Numerical Experiments  In this section we study the performance of the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods We use twelve matrices selected from The University of Florida Sparse Matrix Collection [18] in our numerical experiments"
  },
  {
    "id": 21205,
    "content": "and five nonsymmetric matrices with the respective number of rows (m), columns (n=m) and non-zero elements (nnz) are grouped and shown according to their increasing order in Table 1"
  },
  {
    "id": 21212,
    "content": "Application 1 offshore 259,789 4,242,673 yes Geophysics 2 af_shell3 504,855 17,562,051 yes Mechanics 3 parabolic_fem 525,825 3,674,625 yes General 4 apache2 715,176 4,817,870 yes Mechanics 5 ecology2 999,999 4,995,991 yes Biology 6 thermal2 1,228,045 8,580,313 yes Thermal Simulation 7 G3_circuit 1,585,478 7,660,826 yes Circuit Simulation 8 FEM_3D_thermal2 147,900 3,489,300 no Mechanics 9"
  },
  {
    "id": 21213,
    "content": "thermomech_dK 204,316 2,846,228 no Mechanics 10 ASIC_320ks 321,671 1,316,08511 no Circuit Simulation 11 cage13 445,315 7,479,343 no Biology 12 atmosmodd 1,270,432 8,814,880 no Atmospheric Model In the following experiments we use the hardware system with NVIDIA C2050 (ECC on) GPU and Intel Core i7 CPU 950 @ 3"
  },
  {
    "id": 21220,
    "content": "The MKL_NUM_THREADS and MKL_DYNAMIC environment variables are left unset to allow MKL to use the optimal number of threads"
  },
  {
    "id": 21221,
    "content": "We compute the incomplete-LU and Cholesky factorizations using the MKL routines csrilu0 and csrilut with 0 and threshold fill-in, respectively In the csrilut routine we allow three different levels of fill-in denoted by (5,10 -3 ), (10,10 -5 ) and (20,10 -7 )"
  },
  {
    "id": 21222,
    "content": "In general, the \\(\\left( k,\\mathit{tol} ight)\\) fill-in is based on \\(nnz/n + k\\) maximum allowed number of elements per row and the dropping of elements with magnitude \\(\\left| l_{ij} \\middle| , \\middle| u_{ij} \\middle| < \\mathit{tol} \\times \\left\\| \\mathbf{a}_{i}^{T} ight\\|_{2} ight \\) , where \\(l_{ij}\\) , \\(u_{ij}\\) and \\(\\mathbf{a}_{i}^{T}\\) are the elements of the lower \\(L\\) , upper \\(U\\)"
  },
  {
    "id": 21224,
    "content": "We compare the implementation of the BiCGStab and CG iterative methods using the cuSPARSE and cuBLAS libraries on the GPU and MKL on the CPU"
  },
  {
    "id": 21225,
    "content": "In our experiments we let the initial guess be zero, the right-hand-side \\(\\mathbf{f} = A\\mathbf{e}\\) where \\(\\mathbf{e}^{T}{= (1,\\ldots,1)}^{T}\\) , and the stopping criteria be the maximum number of iterations 2000 or relative residual \\(\\left\\| \\mathbf{r}_{i} ight\\|_{2}/\\left\\| \\mathbf{r}_{0} ight\\|_{2} < 10^{- 7}\\) , where \\(\\mathbf{r}_{i} = \\mathbf{f} - A\\mathbf{x}_{i}\\) is the residual at i"
  },
  {
    "id": 21226,
    "content": "-th iteration time(s) copy time(s) solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} ight\\|_{2}}{\\left\\| \\mathbf{r}_{0} ight\\|_{2}}\\) # it solve time(s) \\(\\frac{\\left\\| \\mathbf{r}_{i} ight\\|_{2}}{\\left\\| \\mathbf{r}_{0} ight\\|_{2}}\\) # it"
  },
  {
    "id": 21313,
    "content": "), achieved relative residual ( \\(\\frac{\\left\\| \\mathbf{r}_{i}  ight\\|_{2}}{\\left\\| \\mathbf{r}_{0}  ight\\|_{2}}\\) ) and time in seconds taken by the factorization (fact"
  },
  {
    "id": 21314,
    "content": "), iterative solution of the linear system (solve), and cudaMemcpy of the lower and upper triangular factors to the GPU (copy) We include the time taken to compute the incomplete-LU and Cholesky factorization as well as to transfer the triangular factors from the CPU to the GPU memory in the computed speedup"
  },
  {
    "id": 21419,
    "content": "Notice that in general in our numerical experiments the performance for the incomplete factorizations decreases as the threshold parameters are relaxed and the factorization becomes more dense, thus inhibiting parallelism due to data dependencies between rows in the sparse triangular solve For this reason, the best performance on the GPU is obtained for the incomplete-LU and Cholesky"
  },
  {
    "id": 21420,
    "content": "factorization with 0 fill-in, which will be our point of reference Performance of BiCGStab and CG with Incomplete-LU Cholesky Preconditioning  Although the incomplete factorizations with a more relaxed threshold are often closer to the exact factorization and thus result in fewer iterative steps, they are also much more expensive to compute Moreover, notice that even though the number of"
  },
  {
    "id": 21421,
    "content": "iterative steps decreases, each step is more computationally expensive As a result of these tradeoffs the total time, the sum of the time taken by the factorization and the iterative solve, for the iterative method does not necessarily decrease with a more relaxed threshold in our numerical experiments The speedup based on the total time taken by the preconditioned iterative method on the GPU with"
  },
  {
    "id": 21422,
    "content": "csrilu0 preconditioner and CPU with all four preconditioners is shown in Figure 4 Notice that for majority of matrices in our numerical experiments the implementation of the iterative method using the cuSPARSE and cuBLAS libraries does indeed outperform the MKL CPU (with all)  Finally, the average of the obtained speedups is shown in Figure 5 , where we have excluded the runs with cage13 matrix"
  },
  {
    "id": 21423,
    "content": "for ilut (10,10 -5 ) and runs with offshore and cage13 matrices for ilut (20,10 -7 ) incomplete factorizations because of their disproportional speedup Consequently, we can conclude that the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods obtain on average more than 2x speedup on the GPU over their CPU implementation Conclusion  The performance of the iterative methods"
  },
  {
    "id": 21424,
    "content": "depends highly on the sparsity pattern of the coefficient matrix at hand In our numerical experiments the incomplete-LU and Cholesky preconditioned BiCGStab and CG iterative methods implemented on the GPU using the cuSPARSE and cuBLAS libraries achieved an average of 2x speedup over their MKL implementation The sparse matrix-vector multiplication and triangular solve, which is split into a slower"
  },
  {
    "id": 21425,
    "content": "analysis phase that needs to be performed only once and a faster solve phase that can be performed multiple times, were the essential building blocks of these iterative methods In fact the obtained speedup was usually mostly influenced by the time taken by the solve phase of the algorithm Finally, we point out that the use of multiple-right-hand-sides would increase the available parallelism and"
  },
  {
    "id": 21426,
    "content": "can result in a significant relative performance improvement in the preconditioned iterative methods Also, the development of incomplete-LU and Cholesky factorizations using CUDA parallel programming paradigm can further improve the obtained speedup"
  },
  {
    "id": 21429,
    "content": "Permission to make digital or hard copies of all or part of this work for any use is granted without fee provided that copies bear this notice and the full citation on the first page"
  },
  {
    "id": 21432,
    "content": "van der Vorst, Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, SIAM, Philadelphia, PA, 1994"
  },
  {
    "id": 21434,
    "content": "Greenbaum, Solving Sparse Triangular Linear Systems using Fortran with Parallel Extensions on the NYU Ultracomputer Prototype, Report 99, NYU Ultracomputer Note, New York University, NY, April, 1986 Mayer, Parallel Algorithms for Solving Linear Systems with Sparse Triangular Matrices, Computing, pp"
  },
  {
    "id": 21440,
    "content": "Naumov, Parallel Solution of Sparse Triangular Linear Systems in the Preconditioned Iterative Methods on the GPU, NVIDIA Technical Report, NVR-2011-001, 2011"
  },
  {
    "id": 21441,
    "content": "Gupta, Parallel ICCG on a Hierarchical Memory Multiprocessor - Addressing the Triangular Solve Bottleneck, Parallel Comput"
  },
  {
    "id": 21447,
    "content": "Demmel, Optimization of Sparse Matrix-Vector Multiplication on Emerging Multicore Platforms, Parallel Comput"
  },
  {
    "id": 21452,
    "content": "html [17] Intel Math Kernel Library, http: software intel com/en-us/articles/intel-mkl [18] The University of Florida Sparse Matrix Collection, http: www"
  },
  {
    "id": 21456,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 21457,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 21459,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 21460,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 21461,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 21462,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 21463,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 21464,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 21465,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 21466,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 21467,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 21468,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 21469,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 21476,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 21478,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 21479,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2011-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 21485,
    "content": "5 | PDF | Archive Floating Point and IEEE 754 Compliance for NVIDIA GPUs White paper covering the most common issues related to NVIDIA GPUs A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in"
  },
  {
    "id": 21486,
    "content": "the CUDA C++ Programming Guide Introduction  Since the widespread adoption in 1985 of the IEEE Standard for Binary Floating-Point Arithmetic (IEEE 754-1985 [1] ) virtually all mainstream computing systems have implemented the standard, including NVIDIA with the CUDA architecture"
  },
  {
    "id": 21487,
    "content": "It is important to consider many aspects of floating point behavior in order to achieve the highest performance with the precision required for any specific application"
  },
  {
    "id": 21488,
    "content": "This is especially true in a heterogeneous computing environment where operations will be performed on different types of hardware"
  },
  {
    "id": 21489,
    "content": "Understanding some of the intricacies of floating point and the specifics of how NVIDIA hardware handles floating point is obviously important to CUDA programmers striving to implement correct numerical algorithms In addition, users of libraries such as cuBLAS and cuFFT will also find it informative to learn how NVIDIA handles floating point under the hood"
  },
  {
    "id": 21490,
    "content": "We also discuss the fused multiply-add operator, which was added to the IEEE 754 standard in 2008 [2] and is built into the hardware of NVIDIA GPUs"
  },
  {
    "id": 21491,
    "content": "In Chapter 3 we work through an example of computing the dot product of two short vectors to illustrate how different choices of implementation affect the accuracy of the final result In Chapter 4 we describe NVIDIA hardware versions and NVCC compiler options that affect floating point calculations Finally, in Chapter 6 we conclude with concrete recommendations to programmers that deal with"
  },
  {
    "id": 21496,
    "content": "Formats  Floating point encodings and functionality are defined in the IEEE 754 Standard [2] last revised in 2008"
  },
  {
    "id": 21498,
    "content": "The standard mandates binary floating point data be encoded on three fields: a one bit sign field, followed by exponent bits encoding the exponent offset by a numeric bias specific to each format, and bits encoding the significand (or fraction)"
  },
  {
    "id": 21499,
    "content": "In order to ensure consistent computations across platforms and to exchange floating point data, IEEE 754 defines basic and interchange formats The 32 and 64 bit basic binary floating point formats correspond to the C data types float and double"
  },
  {
    "id": 21500,
    "content": "Their corresponding representations have the following bit lengths: For numerical data representing finite values, the sign is either negative or positive, the exponent field encodes the exponent in base 2, and the fraction field encodes the significand without the most significant non-zero bit"
  },
  {
    "id": 21504,
    "content": "The exponents are biased by 127 and 1023, respectively, to allow exponents to extend from negative to positive"
  },
  {
    "id": 21506,
    "content": "Given that the fraction field uses a limited number of bits, not all real numbers can be represented exactly For example the mathematical value of the fraction 2/3 represented in binary is 0 10101010… which has an infinite number of bits after the binary point The value 2/3 must be rounded first in order to be represented as a floating point number with limited precision"
  },
  {
    "id": 21508,
    "content": "The value 2/3 rounded in this mode is represented in binary as: The sign is positive and the stored exponent value represents an exponent of -1"
  },
  {
    "id": 21511,
    "content": "Operations and Accuracy  The IEEE 754 standard requires support for a handful of operations These include the arithmetic operations add, subtract, multiply, divide, square root, fused-multiply-add, remainder, conversion operations, scaling, sign operations, and comparisons The results of these operations are guaranteed to be the same for all implementations of the standard, for a given format"
  },
  {
    "id": 21513,
    "content": "The rules and properties of mathematical arithmetic do not hold directly for floating point arithmetic because of floating point’s limited precision For example, the table below shows single precision values A , B , and C , and the mathematical exact value of their sum computed using different associativity"
  },
  {
    "id": 21514,
    "content": "\\(\\begin{matrix} A & = & {2^{1} \\times 1 00000000000000000000001} \\\\ B & = & {2^{0} \\times 1 00000000000000000000001} \\\\ C & = & {2^{3} \\times 1 00000000000000000000001} \\\\ {(A + B) + C} & = & {2^{3} \\times 1 01100000000000000000001011} \\\\ {A + (B + C)} & = & {2^{3} \\times 1 01100000000000000000001011} \\\\ \\end{matrix}\\) Mathematically, ( A + B ) + C does equal A + ( B + C ) Performing these same"
  },
  {
    "id": 21515,
    "content": "computations in single precision floating point arithmetic in round-to-nearest mode according to IEEE 754, we obtain: \\(\\begin{matrix} {A + B} & = & {2^{1} \\times 1"
  },
  {
    "id": 21517,
    "content": "} \\\\ {\\text{rn}(A + B)} & = & {2^{1} \\times 1 10000000000000000000010} \\\\ {B + C} & = & {2^{3} \\times 1"
  },
  {
    "id": 21519,
    "content": "} \\\\ {\\text{rn}(B + C)} & = & {2^{3} \\times 1 00100000000000000000001} \\\\ {A + B + C} & = & {2^{3} \\times 1"
  },
  {
    "id": 21521,
    "content": "} \\\\ {\\text{rn}\\left( \\text{rn}(A + B) + C  ight)} & = & {2^{3} \\times 1 01100000000000000000010} \\\\ {\\text{rn}\\left( A + \\text{rn}(B + C)  ight)} & = & {2^{3} \\times 1"
  },
  {
    "id": 21522,
    "content": "01100000000000000000001} \\\\ \\end{matrix}\\) For reference, the exact, mathematical results are computed as well in the table above Not only are the results computed according to IEEE 754 different from the exact mathematical results, but also the results corresponding to the sum rn(rn(A + B) + C) and the sum rn(A + rn(B + C)) are different from each other In this case, rn(A + rn(B + C)) is closer"
  },
  {
    "id": 21523,
    "content": "to the correct mathematical result than rn(rn(A + B) + C) This example highlights that seemingly identical computations can produce different results even if all basic operations are computed in compliance with IEEE 754"
  },
  {
    "id": 21524,
    "content": "These same results would be obtained using any microprocessor, CPU or GPU, which supports single precision floating point"
  },
  {
    "id": 21527,
    "content": "The Fused Multiply-Add (FMA)  In 2008 the IEEE 754 standard was revised to include the fused multiply-add operation ( FMA ) Without the FMA operation the result would have to be computed as \\(\\text{rn}\\left( \\text{rn}(X \\times Y) + Z ight)\\) with two rounding steps, one for multiply and one for add Let’s consider an example to illustrate how the FMA operation works using decimal arithmetic first"
  },
  {
    "id": 21528,
    "content": "for clarity Let’s compute \\(x^{2} - 1\\) with four digits of precision after the decimal point, or a total of five digits of precision including the leading digit before the decimal point For \\(x = 1"
  },
  {
    "id": 21533,
    "content": "In this case \\(\\text{rn}\\left( x^{2} - 1  ight) = 1 6006 \\times 10^{- 4}\\) which corresponds to the fused multiply-add operation \\(\\text{rn}\\left( x \\times x + ( - 1)  ight)\\) The final result is \\(\\text{rn}\\left( \\text{rn}\\left( x^{2}  ight) - 1  ight) = 1 6000 \\times 10^{- 4}\\)"
  },
  {
    "id": 21535,
    "content": "00004, and its result is closest to the correct mathematical answer The alternative of computing the FMA \\(\\text{rn}(A \\times A + B)\\) provides a result equal to the mathematical value"
  },
  {
    "id": 21536,
    "content": "In general, the fused-multiply-add operation generates more accurate results than computing one multiply followed by one add The choice of whether or not to use the fused operation depends on whether the platform provides the operation and also on how the code is compiled"
  },
  {
    "id": 21537,
    "content": "Figure 1 shows CUDA C++ code and output corresponding to inputs A and B and operations from the example above"
  },
  {
    "id": 21538,
    "content": "The code is executed on two different hardware platforms: an x86-class CPU using SSE in single precision, and an NVIDIA GPU with compute capability 2"
  },
  {
    "id": 21540,
    "content": "At the time this paper is written (Spring 2011) there are no commercially available x86 CPUs which offer hardware FMA"
  },
  {
    "id": 21542,
    "content": "0 do offer hardware FMAs, so the result of executing this code will be the more accurate one by default"
  },
  {
    "id": 21543,
    "content": "The code fragment was compiled without any special intrinsics or compiler options for either platform"
  },
  {
    "id": 21544,
    "content": "Subtractive cancellation occurs during the addition of quantities of similar magnitude with opposite signs"
  },
  {
    "id": 21545,
    "content": "In this case many of the leading bits cancel, leaving fewer meaningful bits of precision in the result Thus even if subtractive cancellation occurs during the addition there are still enough valid bits remaining in the product to get a precise result with no loss of precision"
  },
  {
    "id": 21547,
    "content": "Dot Product: An Accuracy Example  Consider the problem of finding the dot product of two short vectors \\(\\overset{ ightarrow}{a}\\) and \\(\\overset{ ightarrow}{b}\\) , both with four elements"
  },
  {
    "id": 21548,
    "content": "\\(\\overset{ ightharpoonup}{a} = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ a_{3} \\\\ a_{4} \\\\ \\end{bmatrix}\\mspace{2mu}\\quad\\overset{ ightharpoonup}{b} = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3} \\\\ b_{4} \\\\ \\end{bmatrix}\\quad\\overset{ ightharpoonup}{a} \\cdot \\overset{ ightharpoonup}{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + a_{4}b_{4}\\) This operation is easy to write mathematically, but its"
  },
  {
    "id": 21553,
    "content": "Example Algorithms  We present three algorithms which differ in how the multiplications, additions, and possibly fused multiply-adds are organized"
  },
  {
    "id": 21555,
    "content": " The serial method uses a simple loop with separate multiplies and adds to compute the do t product of the vectors"
  },
  {
    "id": 21557,
    "content": " The FMA method uses a simple loop with fused multiply-adds to compute the dot product of the vectors"
  },
  {
    "id": 21559,
    "content": "A simple improvement to the algorithm is to use the fused multiply-add to do the multiply and addition in one step to improve accuracy Yet another way to compute the dot product is to use a divide-and-conquer strategy in which we first find the dot products of the first half and the second half of the vectors, then combine these results using addition This is a recursive strategy; the base case"
  },
  {
    "id": 21560,
    "content": "is the dot product of vectors of length 1 which is a single multiply We call this algorithm the parallel algorithm because the two sub-problems can be computed in parallel as they have no dependencies The algorithm does not require a parallel implementation, however; it can still be implemented with a single thread"
  },
  {
    "id": 21563,
    "content": "Comparison  All three algorithms for computing a dot product use IEEE 754 arithmetic and can be implemented on any system that supports the IEEE standard"
  },
  {
    "id": 21564,
    "content": "In fact, an implementation of the serial algorithm on multiple systems will give exactly the same result However, results computed by an implementation of the serial algorithm may differ from those computed by an implementation of the other two algorithms"
  },
  {
    "id": 21565,
    "content": " The parallel method uses a tree to reduce all the products of individual elements into a final sum The final result can be represented as ((a 1 x b 1 ) + (a 2 x b 2 )) + ((a 3 x b 3 ) + (a 4 x b 4 ))"
  },
  {
    "id": 21567,
    "content": "CUDA and Floating Point  NVIDIA has extended the capabilities of GPUs with each successive hardware generation"
  },
  {
    "id": 21568,
    "content": "Current generations of the NVIDIA architecture such as Tesla Kxx , GTX 8xx , and GTX 9xx , support both single and double precision with IEEE 754 precision and include hardware support for fused multiply-add in both single and double precision"
  },
  {
    "id": 21569,
    "content": "The runtime library supports a function call to determine the compute capability of a GPU at runtime; the CUDA C++ Programming Guide also includes a table of compute capabilities for many different devices [7]"
  },
  {
    "id": 21572,
    "content": "Compute Capability 2 0 and Above  Devices with compute capability 2 0 and above support both single and double precision IEEE 754 including fused multiply-add in both single and double precision Operations such as square root and division will result in the floating point value closest to the correct mathematical result in both single and double precision, by default"
  },
  {
    "id": 21575,
    "content": "Rounding Modes  The IEEE 754 standard defines four rounding modes: round-to-nearest, round towards positive, round towards negative, and round towards zero Compiler intrinsics like the ones listed in the tables below can be used to select other rounding modes for individual operations mode interpretation rn round to nearest, ties to even rz round towards zero ru round towards \\(+ \\text{∞}\\) rd"
  },
  {
    "id": 21576,
    "content": "round towards \\(- \\text{∞}\\) x + y __fadd_[rn | rz | ru | rd] (x, y) addition x * y __fmul_[rn | rz | ru | rd] (x, y) multiplication fmaf (x, y, z) __fmaf_[rn | rz | ru | rd] (x, y, z) FMA 1 0f / x __frcp_[rn | rz | ru | rd] (x) reciprocal x / y __fdiv_[rn | rz | ru | rd] (x, y) division sqrtf(x) __fsqrt_[rn | rz | ru | rd] (x) square root x + y __dadd_[rn | rz | ru | rd] (x, y) addition x * y"
  },
  {
    "id": 21577,
    "content": "__dmul_[rn | rz | ru | rd] (x, y) multiplication fma (x, y, z) __fma_[rn | rz | ru | rd] (x, y, z) FMA 1 0 / x __drcp_[rn | rz | ru | rd] (x) reciprocal x / y __ddiv_[rn | rz | ru | rd] (x, y) division sqrtf(x) __dsqrt_[rn | rz | ru | rd] (x) square root 4"
  },
  {
    "id": 21579,
    "content": "Controlling Fused Multiply-add  In general, the fused multiply-add operation is faster and more accurate than performing separate multiply and add operations However, on occasion you may wish to disable the merging of multiplies and adds into fused multiply-add instructions"
  },
  {
    "id": 21580,
    "content": "To inhibit this optimization one can write the multiplies and additions using intrinsics with explicit rounding mode as shown in the previous tables"
  },
  {
    "id": 21581,
    "content": "Operations written directly as intrinsics are guaranteed to remain independent and will not be merged into fused multiply-add instructions"
  },
  {
    "id": 21585,
    "content": "Compiler Flags  Compiler flags relevant to IEEE 754 operations are -ftz={true|false} , -prec-div={true|false} , and -prec-sqrt={true|false} These flags control single precision operations on devices of compute capability of 2"
  },
  {
    "id": 21587,
    "content": "mode flags IEEE 754 mode (default) -ftz=false -prec-div=true -prec-sqrt=true fast mode -ftz=true -prec-div=false -prec-sqrt=false The default IEEE 754 mode means that single precision operations are correctly rounded and support denormals, as per the IEEE 754 standard"
  },
  {
    "id": 21588,
    "content": "In the fast mode denormal numbers are flushed to zero, and the operations division and square root are not computed to the nearest floating point value"
  },
  {
    "id": 21593,
    "content": "Differences from x86  NVIDIA GPUs differ from the x86 architecture in that rounding modes are encoded within each floating point instruction instead of dynamically using a floating point control word"
  },
  {
    "id": 21594,
    "content": "On the GPU there is no status flag to indicate when calculations have overflowed, underflowed, or have involved inexact arithmetic"
  },
  {
    "id": 21595,
    "content": "Like SSE , the precision of each GPU operation is encoded in the instruction (for x87 the precision is controlled dynamically by the floating point control word)"
  },
  {
    "id": 21599,
    "content": "Mathematical Function Accuracy  So far we have only considered simple math operations such as addition, multiplication, division, and square root"
  },
  {
    "id": 21603,
    "content": "To guarantee the correctly rounded result, it is not generally enough to compute the function to a fixed high accuracy There might still be rare cases where the error in the high accuracy result affects the rounding step at the lower accuracy It is possible to solve the dilemma for particular functions by doing mathematical analysis and formal proofs [4] , but most math libraries choose instead"
  },
  {
    "id": 21604,
    "content": "to give up the guarantee of correct rounding Instead they provide implementations of math functions and document bounds on the relative error of the functions over the input range For example, the double precision sin function in CUDA is guaranteed to be accurate to within 2 units in the last place (ulp) of the correctly rounded result In other words, the difference between the computed result and"
  },
  {
    "id": 21605,
    "content": "the mathematical result is at most ±2 with respect to the least significant bit position of the fraction part of the floating point result"
  },
  {
    "id": 21612,
    "content": "0) using a common library differs depending on whether the code is compiled in 32-bit mode or 64-bit mode"
  },
  {
    "id": 21613,
    "content": "The consequence is that different math libraries cannot be expected to compute exactly the same result for a given input Functions compiled for the GPU will use the NVIDIA CUDA math library implementation while functions compiled for the CPU will use the host compiler math library implementation (e"
  },
  {
    "id": 21616,
    "content": "Because these implementations are independent and neither is guaranteed to be correctly rounded, the results will often differ slightly"
  },
  {
    "id": 21619,
    "content": "x87 and SSE  One of the unfortunate realities of C compilers is that they are often poor at preserving IEEE 754 semantics of floating point operations [6] Just like CUDA operations, SSE operations are performed on single or double precision values, while x87 operations often use an additional internal 80-bit precision format"
  },
  {
    "id": 21620,
    "content": "Sometimes the results of a computation using x87 can depend on whether an intermediate result was allocated to a register or stored to memory Values stored to memory are rounded to the declared precision (e"
  },
  {
    "id": 21623,
    "content": "Also, x87 instructions will often be used by default for 32-bit compiles but SSE instructions will be used by default for 64-bit compiles"
  },
  {
    "id": 21625,
    "content": "When comparing CPU results to results computed on the GPU, it is generally best to compare using SSE instructions On 32-bit x86 targets without SSE it can be helpful to declare variables using volatile and force floating point values to be stored to memory ( /Op in Visual Studio and -ffloat-store in gcc )"
  },
  {
    "id": 21626,
    "content": "This moves results from extended precision registers into memory, where the precision is precisely single or double precision"
  },
  {
    "id": 21627,
    "content": "Alternately, the x87 control word can be updated to set the precision to 24 or 53 bits using the assembly instruction fldcw or a compiler option such as -mpc32 or -mpc64 in gcc"
  },
  {
    "id": 21630,
    "content": "Core Counts  As we have shown in Section 3 , the final values computed using IEEE 754 arithmetic can depend on implementation choices such as whether to use fused multiply-add or whether additions are organized in series or parallel"
  },
  {
    "id": 21631,
    "content": "One way such differences can arise is from differences between the number of concurrent threads involved in a computation"
  },
  {
    "id": 21632,
    "content": "On the GPU, a common design pattern is to have all threads in a block coordinate to do a parallel reduction on data within the block, followed by a serial reduction of the results from each block Changing the number of threads per block reorganizes the reduction; if the reduction is addition, then the change rearranges parentheses in the long string of additions Even if the same general strategy"
  },
  {
    "id": 21633,
    "content": "such as parallel reduction is used on the CPU and GPU, it is common to have widely different numbers of threads on the GPU compared to the CPU For example, the GPU implementation might launch blocks with 128 threads per block, while the CPU implementation might use 4 threads in total"
  },
  {
    "id": 21636,
    "content": "Verifying GPU Results  The same inputs will give the same results for individual IEEE 754 operations to a given precision on the CPU and GPU As we have explained, there are many reasons why the same sequence of operations may not be performed on the CPU and GPU Finally, many common mathematical functions are not required by the IEEE 754 standard to be correctly rounded so should not be expected"
  },
  {
    "id": 21637,
    "content": "to yield identical results between implementations When porting numeric code from the CPU to the GPU of course it makes sense to use the x86 CPU results as a reference Differences are not automatically evidence that the result computed by the GPU is wrong or that there is a problem on the GPU Computing results in a high precision and then comparing to results computed in a lower precision can be"
  },
  {
    "id": 21638,
    "content": "helpful to see if the lower precision is adequate for a particular application However, rounding high precision results to a lower precision is not equivalent to performing the entire computation in lower precision The results of the CPU may be computed to an unexpectedly high extended precision for some or all of the operations The GPU result will be computed using single or double precision only"
  },
  {
    "id": 21640,
    "content": "Concrete Recommendations  The key points we have covered are the following: Use the fused multiply-add operator The fused multiply-add operator on the GPU has high performance and increases the accuracy of computations Understand that a hardware fused multiply-add operation is not yet available on the CPU, which can cause differences in numerical results"
  },
  {
    "id": 21641,
    "content": "Even in the strict world of IEEE 754 operations, minor details such as organization of parentheses or thread counts can affect the final result"
  },
  {
    "id": 21643,
    "content": "0 and later are capable of single and double precision arithmetic following the IEEE 754 standard, and have hardware units for performing fused multiply-add in both single and double precision"
  },
  {
    "id": 21644,
    "content": "The math library includes all the math functions listed in the C99 standard [3] plus some additional useful functions"
  },
  {
    "id": 21645,
    "content": "These functions have been tuned for a reasonable compromise between performance and accuracy Please let us know about any functions that you require that we do not provide, or if the accuracy or performance of any of our functions does not meet your needs"
  },
  {
    "id": 21646,
    "content": "Leave comments in the NVIDIA CUDA forum 1 or join the Registered Developer Program 2 and file a bug with your feedback"
  },
  {
    "id": 21648,
    "content": "Acknowledgements  This paper was authored by Nathan Whitehead and Alex Fit-Florea for NVIDIA Corporation"
  },
  {
    "id": 21649,
    "content": "Thanks to Ujval Kapasi, Kurt Wall, Paul Sidenblad, Massimiliano Fatica, Everett Phillips, Norbert Juffa, and Will Ramey for their helpful comments and suggestions"
  },
  {
    "id": 21650,
    "content": "Permission to make digital or hard copies of all or part of this work for any use is granted without fee provided that copies bear this notice and the full citation on the first page"
  },
  {
    "id": 21653,
    "content": "[4] Catherine Daramy-Loirat, David Defour, Florent de Dinechin, Matthieu Gallet, Nicolas Gast, and Jean-Michel Muller"
  },
  {
    "id": 21659,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 21660,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 21662,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 21663,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 21664,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 21665,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 21666,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 21667,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 21668,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 21669,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 21670,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 21671,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 21672,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 21679,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 21681,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 21687,
    "content": "nvidia com/ join-nvidia-registered-developer-program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2011-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 21693,
    "content": "5 | PDF | Archive CUDA Binary Utilities The application notes for cuobjdump, nvdisasm, cu++filt, and nvprune Overview  This document introduces cuobjdump , nvdisasm , cu++filt and nvprune , four CUDA binary tools for Linux (x86, ARM and P9), Windows, Mac OS and Android"
  },
  {
    "id": 21696,
    "content": " A CUDA binary (also referred to as cubin) file is an ELF-formatted file which consists of CUDA executable code sections as well as other sections containing symbols, relocators, debug info, etc By default, the CUDA compiler driver nvcc embeds cubin files into the host executable file Note For more details on cubin files or the CUDA compilation trajectory, refer to NVIDIA CUDA Compiler Driver"
  },
  {
    "id": 21698,
    "content": "Differences between cuobjdump and nvdisasm  CUDA provides two binary utilities for examining and disassembling cubin files and host executables: cuobjdump and nvdisasm Basically, cuobjdump accepts both cubin files and host binaries while nvdisasm only accepts cubin files; but nvdisasm provides richer output options Comparison of cuobjdump and nvdisasm  cuobjdump nvdisasm Disassemble cubin Yes"
  },
  {
    "id": 21699,
    "content": "Yes Extract ptx and extract and disassemble cubin from the following input files: Host binaries Executables Object files Static libraries External fatbinary files Yes No Control flow analysis and output No Yes Advanced display options No Yes 1"
  },
  {
    "id": 21701,
    "content": "Command Option Types and Notation  This section of the document provides common details about the command line options for the following tools: cuobjdump nvdisasm nvprune Each command-line option has a long name and a short name, which are interchangeable with each other These two variants are distinguished by the number of hyphens that must precede the option name, i"
  },
  {
    "id": 21704,
    "content": "Long options are intended for use in build scripts, where size of the option is less important than descriptive value and short options are intended for interactive use The tools mentioned above recognize three types of command options: boolean options, single value options and list options Boolean options do not have an argument, they are either specified on a command line or not Examples of"
  },
  {
    "id": 21705,
    "content": "each of these option types are, respectively: Boolean option : nvdisams --print-raw Single value : nvdisasm --binary SM70 List options : cuobjdump --function \"foo,bar,foobar\" Single value options and list options must have arguments, which must follow the name of the option by either one or more spaces or an equals character When a one-character short name such as -I , -l , and -L is used, the"
  },
  {
    "id": 21706,
    "content": "value of the option may also immediately follow the option itself without being seperated by spaces or an equal character The individual values of list options may be separated by commas in a single instance of the option or the option may be repeated, or any combination of these two cases Hence, for the two sample options mentioned above that may take values, the following notations are legal: -o"
  },
  {
    "id": 21707,
    "content": "file -o=file -Idir1,dir2 -I=dir3 -I dir4,dir5 For options taking a single value, if specified multiple times, the rightmost value in the command line will be considered for that option"
  },
  {
    "id": 21712,
    "content": "bin nvdisasm warning : incompatible redefinition for option 'binary', the last value of this option was used For options taking a list of values, if specified multiple times, the values get appended to the list In the below example, functions foo and bar are considered as valid values for option --function and the duplicate value foo is ignored"
  },
  {
    "id": 21713,
    "content": "cuobjdump  cuobjdump extracts information from CUDA binary files (both standalone and those embedded in host binaries) and presents them in human readable format The output of cuobjdump includes CUDA assembly code for each kernel, CUDA ELF section headers, string tables, relocators and other CUDA specific sections For a list of CUDA assembly instruction set of each GPU architecture, see"
  },
  {
    "id": 21718,
    "content": "Fatbin ptx code: = arch = sm_70 code version = [7,0] producer = cuda host = linux compile_size = 64bit compressed identifier = add"
  },
  {
    "id": 21759,
    "content": "cubin -res-usage Resource usage: Common: GLOBAL:56 CONSTANT[3]:28 Function calculate: REG:24 STACK:8 SHARED:0 LOCAL:0 CONSTANT[0]:472 CONSTANT[2]:24 TEXTURE:0 SURFACE:0 SAMPLER:0 Function mysurf_func: REG:38 STACK:8 SHARED:4 LOCAL:0 CONSTANT[0]:532 TEXTURE:8 SURFACE:7 SAMPLER:0 Function mytexsampler_func: REG:42 STACK:0 SHARED:0 LOCAL:0 CONSTANT[0]:472 TEXTURE:4 SURFACE:0 SAMPLER:1 Note that"
  },
  {
    "id": 21764,
    "content": "Command-line Options  Table 2 contains supported command-line options of cuobjdump , along with a description of what each option does cuobjdump Command-line Options  Option (long) Option (short) Description --all-fatbin -all Dump all fatbin sections By default will only dump contents of executable fatbin (if exists), else relocatable fatbin if no executable fatbin --dump-sass -sass Dump CUDA"
  },
  {
    "id": 21767,
    "content": "Allowed values for this option: sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 , sm_72 , sm_75 , sm_80 , sm_86 , sm_87 , sm_89 , sm_90 , sm_90a"
  },
  {
    "id": 21770,
    "content": "nvdisasm  nvdisasm extracts information from standalone cubin files and presents them in human readable format The output of nvdisasm includes CUDA assembly code for each kernel, listing of ELF data sections and other CUDA specific sections nvdisasm also does control flow analysis to annotate jump/branch targets and makes the output easier to read If this information is missing from the CUDA"
  },
  {
    "id": 21771,
    "content": "binary, either use the nvdisasm option -ndf to turn off control flow analysis, or use the ptxas and nvlink option -preserve-relocs to re-generate the cubin file"
  },
  {
    "id": 21774,
    "content": "Usage  nvdisasm accepts a single input file each time it’s run The basic usage is as following: nvdisasm [options] Here’s a sample output of nvdisasm :"
  },
  {
    "id": 21775,
    "content": "headerflags @\"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM70 EF_CUDA_VIRTUAL_SM(EF_CUDA_SM70)\""
  },
  {
    "id": 21783,
    "content": "The output of the control flow from nvdisasm can be directly imported to a DOT graph visualization tool such as Graphviz"
  },
  {
    "id": 21784,
    "content": "Here’s how you can generate a PNG image ( cfg png ) of the control flow of the above cubin ( a cubin ) with nvdisasm and Graphviz: nvdisasm -cfg a"
  },
  {
    "id": 21786,
    "content": "png -Tpng Here’s the generated graph: Control Flow Graph  To generate a PNG image ( bbcfg png ) of the basic block control flow of the above cubin ( a"
  },
  {
    "id": 21789,
    "content": "png -Tpng Here’s the generated graph: Basic Block Control Flow Graph  nvdisasm is capable of showing the register (general and predicate) liveness range information"
  },
  {
    "id": 21790,
    "content": "For each line of CUDA assembly, nvdisasm displays whether a given device register was assigned, accessed, live or re-assigned"
  },
  {
    "id": 21791,
    "content": "This is useful if the user is interested in the life range of any particular register, or register usage in general"
  },
  {
    "id": 21792,
    "content": "In absence of any function inlining the output is same as the one with nvdisasm -g command Command-line Options  Table 3 contains the supported command-line options of nvdisasm , along with a description of what each option does nvdisasm Command-line Options  Option (long) Option (short) Description --base-address -base Specify the logical base address of the image to disassemble"
  },
  {
    "id": 21793,
    "content": "This option is only valid when disassembling a raw instruction binary (see option --binary ), and is ignored when disassembling an Elf file --binary -b When this option is specified, the input file is assumed to contain a raw instruction binary, that is, a sequence of binary instruction encodings as they occur in instruction memory"
  },
  {
    "id": 21794,
    "content": "Allowed values for this option: SM50 , SM52 , SM53 , SM60 , SM61 , SM62 , SM70 , SM72 , SM75 , SM80 , SM86 , SM87 , SM89 , SM90 , SM90a"
  },
  {
    "id": 21796,
    "content": "--life-range-mode -lrm This option implies option --print-life-ranges , and determines how register live range info should be printed"
  },
  {
    "id": 21797,
    "content": "count : Not at all, leaving only the # column (number of live registers); wide : Columns spaced out for readability (default); narrow : A one-character column for each register, economizing on table width Allowed values for this option: count , narrow , wide"
  },
  {
    "id": 21798,
    "content": "Dataflow analysis is normally enabled to perform branch stack analysis and annotate all instructions that jump via the GPU branch stack with inferred branch target labels"
  },
  {
    "id": 21800,
    "content": "--no-vliw -novliw Conventional mode; disassemble paired instructions in normal syntax, instead of VLIW syntax"
  },
  {
    "id": 21801,
    "content": "--output-control-flow-graph -cfg When specified output the control flow graph, where each node is a hyperblock, in a format consumable by graphviz tools (such as dot) --output-control-flow-graph-with-basic-blocks -bbcfg When specified output the control flow graph, where each node is a basicblock, in a format consumable by graphviz tools (such as dot) --print-instr-offsets-cfg -poff When"
  },
  {
    "id": 21802,
    "content": "specified, print instruction offsets in the control flow graph This should be used along with the option –output-control-flow-graph or –output-control-flow-graph-with-basic-blocks --print-instruction-encoding -hex When specified, print the encoding bytes after each disassembled operation --print-life-ranges -plr Print register life range information in a trailing column in the produced disassembly"
  },
  {
    "id": 21809,
    "content": "--separate-functions -sf Separate the code corresponding with function symbols by some new lines to let them stand out in the printed disassembly"
  },
  {
    "id": 21811,
    "content": "Instruction Set Reference  This section contains instruction set reference for NVIDIA NVIDIA ® GPU architectures"
  },
  {
    "id": 21814,
    "content": "Maxwell and Pascal Instruction Set  The Maxwell (Compute Capability 5 x) and the Pascal (Compute Capability 6 x) architectures have the following instruction set format: (instruction) (destination) (source1), (source2) Valid destination and source locations include: RX for registers SRX for special system-controlled registers PX for condition registers c[X][Y] for constant memory Table 4 lists"
  },
  {
    "id": 21815,
    "content": "valid instructions for the Maxwell and Pascal GPUs Volta Instruction Set  The Volta architecture (Compute Capability 7 x) has the following instruction set format: (instruction) (destination) (source1), (source2) Valid destination and source locations include: RX for registers SRX for special system-controlled registers PX for predicate registers c[X][Y] for constant memory Table 5 lists valid"
  },
  {
    "id": 21816,
    "content": "instructions for the Volta GPUs Turing Instruction Set  The Turing architecture (Compute Capability 7 3 and 7 5) have the following instruction set format: (instruction) (destination) (source1), (source2) Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers c[X][Y] for constant memory"
  },
  {
    "id": 21817,
    "content": "Table 6 lists valid instructions for the Turing GPUs NVIDIA Ampere GPU and Ada Instruction Set  The NVIDIA Ampere GPU and Ada architectures (Compute Capability 8 0 and 8 6) have the following instruction set format: (instruction) (destination) (source1), (source2) Valid destination and source locations include: RX for registers URX for uniform registers SRX for special system-controlled registers"
  },
  {
    "id": 21818,
    "content": "PX for predicate registers UPX for uniform predicate registers c[X][Y] for constant memory Table 7 lists valid instructions for the NVIDIA Ampere architecrture and Ada GPUs Hopper Instruction Set  The Hopper architecture (Compute Capability 9 0) has the following instruction set format: (instruction) (destination) (source1), (source2) Valid destination and source locations include: RX for"
  },
  {
    "id": 21819,
    "content": "registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers UPX for uniform predicate registers c[X][Y] for constant memory desc[URX][RY] for memory descriptors Table 8 lists valid instructions for the Hopper GPUs"
  },
  {
    "id": 21820,
    "content": "cu++filt  cu++filt decodes (demangles) low-level identifiers that have been mangled by CUDA C++ into user readable names For every input alphanumeric word, the output of cu++filt is either the demangled name if the name decodes to a CUDA C++ name, or the original name itself"
  },
  {
    "id": 21823,
    "content": "Usage  cu++filt accepts one or more alphanumeric words (consisting of letters, digits, underscores, dollars, or periods) and attepts to decipher them The basic usage is as following: cu++filt [options] To demangle an entire file, like a binary, pipe the contents of the file to cu++filt, such as in the following command: nm | cu++filt To demangle function names without printing their parameter"
  },
  {
    "id": 21824,
    "content": "types, use the following command : cu++filt -p To skip a leading underscore from mangled symbols, use the following command: cu++filt -_ Here’s a sample output of cu++filt : $ cu++filt _Z1fIiEbl bool f(long) As shown in the output, the symbol _Z1fIiEbl was successfully demangled To strip all types in the function signature and parameters, use the -p option: $ cu++filt -p _Z1fIiEbl f To skip a"
  },
  {
    "id": 21825,
    "content": "leading underscore from a mangled symbol, use the -_ option: $ cu++filt -_ __Z1fIiEbl bool f(long) To demangle an entire file, pipe the contents of the file to cu++filt: $ nm test"
  },
  {
    "id": 21827,
    "content": "cubin | cu++filt 0000000000000000 t hello(char *) 0000000000000070 t hello(char *)::display() 0000000000000000 T hello(int *) Symbols that cannot be demangled are printed back to stdout as is: $ cu++filt _ZD2 _ZD2 Multiple symbols can be demangled from the command line: $ cu++filt _ZN6Scope15Func1Enez _Z3fooIiPFYneEiEvv _ZD2 Scope1::Func1(__int128, long double, ) void foo() _ZD2 5"
  },
  {
    "id": 21829,
    "content": "Command-line Options  Table 9 contains supported command-line options of cu++filt , along with a description of what each option does"
  },
  {
    "id": 21834,
    "content": "Library Availability  cu++filt is also available as a static library (libcufilt) that can be linked against an existing project"
  },
  {
    "id": 21835,
    "content": "The following interface describes it’s usage: char* __cu_demangle(const char *id, char *output_buffer, size_t *length, int *status) This interface can be found in the file “nv_decode"
  },
  {
    "id": 21837,
    "content": "If output-buffer is NULL, memory will be malloc’d to store the demangled name and returned through the function return value length It is necessary to provide the size of the output buffer if the user is providing pre-allocated memory status *status is set to one of the following values: 0 - The demangling operation succeeded -1 - A memory allocation failure occurred -2 - Not a valid mangled id"
  },
  {
    "id": 21838,
    "content": "-3 - An input validation failure has occurred (one or more arguments are invalid) Return Value A pointer to the start of the NUL-terminated demangled name, or NULL if the demangling fails"
  },
  {
    "id": 21840,
    "content": "h\" int main() { int status; const char *real_mangled_name=\"_ZN8clstmp01I5cls01E13clstmp01_mf01Ev\"; const char *fake_mangled_name=\"B@d_iDentiFier\"; char* realname = __cu_demangle(fake_mangled_name, 0, 0, &status); printf(\"fake_mangled_name:\\t result => %s\\t status => %d \", realname, status); free(realname); size_t size = sizeof(char)*1000; realname = (char*)malloc(size);"
  },
  {
    "id": 21841,
    "content": "__cu_demangle(real_mangled_name, realname, &size, &status); printf(\"real_mangled_name:\\t result => %s\\t status => %d \", realname, status); free(realname); return 0; } This prints: fake_mangled_name: result => (null) status => -2 real_mangled_name: result => clstmp01::clstmp01_mf01() status => 0 6"
  },
  {
    "id": 21842,
    "content": "nvprune  nvprune prunes host object files and libraries to only contain device code for the specified targets"
  },
  {
    "id": 21845,
    "content": "Usage  nvprune accepts a single input file each time it’s run, emitting a new output file The basic usage is as following: nvprune [options] -o The input file must be either a relocatable host object or static library (not a host executable), and the output file will be the same format"
  },
  {
    "id": 21846,
    "content": "For example, the following will prune libcublas_static a to only contain sm_70 cubin rather than all the targets which normally exist: nvprune -arch sm_70 libcublas_static"
  },
  {
    "id": 21848,
    "content": "a will not run on any other architecture, so should only be used when you are building for a single architecture"
  },
  {
    "id": 21851,
    "content": "Command-line Options  Table 10 contains supported command-line options of nvprune , along with a description of what each option does"
  },
  {
    "id": 21852,
    "content": "-arch Specify the name of the NVIDIA GPU architecture which will remain in the object or library --generate-code -gencode This option is same format as nvcc –generate-code option, and provides a way to specify multiple architectures which should remain in the object or library"
  },
  {
    "id": 21853,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 21854,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 21856,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 21857,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 21858,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 21859,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 21860,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 21861,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 21862,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 21863,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 21864,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 21865,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 21866,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 21873,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 21875,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 21876,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2013-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 21882,
    "content": "5 | PDF | Archive Profiler User’s Guide The user manual for NVIDIA profiling tools for optimizing performance of CUDA applications Profiling Overview This document describes NVIDIA profiling tools that enable you to understand and optimize the performance of your CUDA, OpenACC or OpenMP applications The Visual Profiler is a graphical profiling tool that displays a timeline of your application’s"
  },
  {
    "id": 21883,
    "content": "CPU and GPU activity, and that includes an automated analysis engine to identify optimization opportunities The nvprof profiling tool enables you to collect and view profiling data from the command-line"
  },
  {
    "id": 21885,
    "content": "It is recommended to use next-generation tools NVIDIA Nsight Systems for GPU and CPU sampling and tracing and NVIDIA Nsight Compute for GPU kernel profiling Refer the Migrating to Nsight Tools from Visual Profiler and nvprof section for more details"
  },
  {
    "id": 21890,
    "content": "Preparing An Application For Profiling  The CUDA profiling tools do not require any application changes to enable profiling; however, by making some simple modifications and additions, you can greatly increase the usability and effectiveness profiling This section describes these modifications and how they can improve your profiling results"
  },
  {
    "id": 21893,
    "content": "Focused Profiling  By default, the profiling tools collect profile data over the entire run of your application But, as explained below, you typically only want to profile the region(s) of your application containing some or all of the performance-critical code Limiting profiling to performance-critical regions reduces the amount of profile data that both you and the tools must process, and"
  },
  {
    "id": 21894,
    "content": "focuses attention on the code where optimization will result in the greatest performance gains There are several common situations where profiling a region of the application is helpful The application is a test harness that contains a CUDA implementation of all or part of your algorithm The test harness initializes the data, invokes the CUDA functions to perform the algorithm, and then checks the"
  },
  {
    "id": 21895,
    "content": "results for correctness Using a test harness is a common and productive way to quickly iterate and test algorithm changes When profiling, you want to collect profile data for the CUDA functions implementing the algorithm, but not for the test harness code that initializes the data or checks the results"
  },
  {
    "id": 21896,
    "content": "The application operates in phases, where a different set of algorithms is active in each phase When the performance of each phase of the application can be optimized independently of the others, you want to profile each phase separately to focus your optimization efforts"
  },
  {
    "id": 21897,
    "content": "The application contains algorithms that operate over a large number of iterations, but the performance of the algorithm does not vary significantly across those iterations"
  },
  {
    "id": 21898,
    "content": "To limit profiling to a region of your application, CUDA provides functions to start and stop profile data collection cudaProfilerStart() is used to start profiling and cudaProfilerStop() is used to stop profiling (using the CUDA driver API, you get the same functionality with cuProfilerStart() and cuProfilerStop() )"
  },
  {
    "id": 21902,
    "content": "When using the start and stop functions, you also need to instruct the profiling tool to disable profiling at the start of the application For the Visual Profiler you use the Start execution with profiling enabled checkbox in the Settings View"
  },
  {
    "id": 21905,
    "content": "Marking Regions of CPU Activity  The Visual Profiler can collect a trace of the CUDA function calls made by your application The Visual Profiler shows these calls in the Timeline View , allowing you to see where each CPU thread in the application is invoking CUDA functions To understand what the application’s CPU threads are doing outside of CUDA function calls, you can use the NVIDIA Tools"
  },
  {
    "id": 21906,
    "content": "Extension API (NVTX) When you add NVTX markers and ranges to your application, the Timeline View shows when your CPU threads are executing within those regions"
  },
  {
    "id": 21910,
    "content": "Naming CPU and CUDA Resources  The Visual Profiler Timeline View shows default naming for CPU thread and GPU devices, context and streams Using custom names for these resources can improve understanding of the application behavior, especially for CUDA applications that have many host threads, devices, contexts, or streams You can use the NVIDIA Tools Extension API to assign custom names for your"
  },
  {
    "id": 21915,
    "content": "Flush Profile Data  To reduce profiling overhead, the profiling tools collect and record profile information into internal buffers"
  },
  {
    "id": 21916,
    "content": "These buffers are then flushed asynchronously to disk with low priority to avoid perturbing application behavior To avoid losing profile information that has not yet been flushed, the application being profiled should make sure, before exiting, that all GPU work is done (using CUDA synchronization calls), and then call cudaProfilerStop() or cuProfilerStop() If your CUDA application includes"
  },
  {
    "id": 21917,
    "content": "graphics that operate using a display or main loop, care must be taken to call cudaProfilerStop() or cuProfilerStop() before the thread executing that loop calls exit()"
  },
  {
    "id": 21918,
    "content": "Failure to call one of these APIs may result in the loss of some or all of the collected profile data"
  },
  {
    "id": 21919,
    "content": "For some graphics applications like the ones use OpenGL, the application exits when the escape key is pressed"
  },
  {
    "id": 21920,
    "content": "In those cases where calling the above functions before exit is not feasible, use nvprof option --timeout or set the “Execution timeout” in the Visual Profiler The profiler will force a data flush just before the timeout"
  },
  {
    "id": 21923,
    "content": "Profiling CUDA Fortran Applications  CUDA Fortran applications compiled with the PGI CUDA Fortran compiler can be profiled by nvprof and the Visual Profiler"
  },
  {
    "id": 21924,
    "content": "In cases where the profiler needs source file and line information (kernel profile analysis, global memory access pattern analysis, divergent execution analysis, etc"
  },
  {
    "id": 21928,
    "content": "​Visual Profiler  The NVIDIA Visual Profiler allows you to visualize and optimize the performance of your application The Visual Profiler displays a timeline of your application’s activity on both the CPU and GPU so that you can identify opportunities for performance improvement In addition, the Visual Profiler will analyze your application to detect potential performance bottlenecks and direct"
  },
  {
    "id": 21929,
    "content": "you on how to take action to eliminate or reduce those bottlenecks The Visual Profiler is available as both a standalone application and as part of Nsight Eclipse Edition The standalone version of the Visual Profiler, nvvp , is included in the CUDA Toolkit for all supported OSes except for macOS"
  },
  {
    "id": 21931,
    "content": "0, Visual Profiler and nvprof don’t support macOS as the target platform Visual Profiler is provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on macOS Within Nsight Eclipse Edition, the Visual Profiler is located in the Profile Perspective and is activated when an application is run in profile mode"
  },
  {
    "id": 21941,
    "content": "1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes"
  },
  {
    "id": 21942,
    "content": "To run Visual Profiler on OpenSUSE15 or SLES15: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/lib64/jvm/jre-1"
  },
  {
    "id": 21947,
    "content": "10: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java Note The -vm option is only required when JRE 1"
  },
  {
    "id": 21954,
    "content": "0-0 To run Visual Profiler on Fedora 29: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/bin/java Note The -vm option is only required when JRE 1"
  },
  {
    "id": 21958,
    "content": "Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm /usr/bin/java Note The -vm option is only required when JRE 1"
  },
  {
    "id": 21960,
    "content": "To run Visual Profiler on Windows: Make sure that you invoke Visual Profiler with the command-line option included as shown below: nvvp -vm \"C:\\Program Files\\Java\\jdk1"
  },
  {
    "id": 21970,
    "content": "1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes Windows Oracle JRE 1 8 (may require paid updates) OpenJDK JRE 1 8 Linux Oracle JRE 1 8 (may require paid updates) OpenJDK JRE 1 8 Mac Oracle JRE 1 8 (may require paid updates) Note JRE 1"
  },
  {
    "id": 21979,
    "content": "Modify Your Application For Profiling  The Visual Profiler does not require any application changes; however, by making some simple modifications and additions, you can greatly increase its usability and effectiveness Section Preparing An Application For Profiling describes how you can focus your profiling efforts and add extra annotations to your application that will greatly improve your"
  },
  {
    "id": 21984,
    "content": "Creating a Session  The first step in using the Visual Profiler to profile your application is to create a new profiling session You can create a new session by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu"
  },
  {
    "id": 21985,
    "content": "Optionally, you can also specify the working directory, arguments, multi-process profiling option and environment"
  },
  {
    "id": 21986,
    "content": "The muti-process profiling options are: Profile child processes - If selected, profile all processes launched by the specified application Profile all processes - If selected, profile every CUDA process launched on the same system by the same user who launched nvprof"
  },
  {
    "id": 21987,
    "content": "In this mode the Visual Profiler will launch nvprof and user needs to run his application in another terminal outside the Visual Profiler User can exit this mode by pressing “Cancel” button on progress dialog in Visual Profiler to load the profile data Profile current process only - If selected, only profile specified application CUDA options: Start execution with profiling enabled - If selected"
  },
  {
    "id": 21988,
    "content": "profile data is collected from the start of application execution If not selected profile data is not collected until cudaProfilerStart() is called in the application Enable concurrent kernel profiling - This option should be selected for an application that uses CUDA streams to launch kernels that can execute concurrently If the application uses only a single stream (and therefore cannot have"
  },
  {
    "id": 21989,
    "content": "concurrent kernel execution), deselecting this option may decrease profiling overhead Enable CUDA API tracing in the timeline - If selected, the CUDA driver and runtime API call trace is collected and displayed on timeline Enable power, clock, and thermal profiling - If selected, power, clock, and thermal conditions on the GPUs will be sampled and displayed on the timeline Enable unified memory"
  },
  {
    "id": 21990,
    "content": "profiling - If selected for the GPU that supports Unified Memory, the Unified Memory related memory traffic to and from each GPU is collected on your system and displayed on timeline Replay application to collect events and metrics - If selected, the whole application is re-run instead of replaying each kernel, in order to collect all events/metrics"
  },
  {
    "id": 21991,
    "content": "Run guided analysis - If selected, the guided analysis is run immediately after the creation of a new session"
  },
  {
    "id": 21992,
    "content": "CPU (host) options: Profile execution on the CPU - If selected the CPU threads are sampled and data collected about the CPU performance is shown in the CPU Details View"
  },
  {
    "id": 21993,
    "content": "Enable OpenACC profiling - If selected and an OpenACC application is profiled, OpenACC activities will be recorded and displayed on a new OpenACC timeline Enable CPU thread tracing - If enabled, selected CPU thread API calls will be recorded and displayed on a new thread API timeline For performance reasons, only those API calls that influence concurrent execution are recorded and collection of"
  },
  {
    "id": 21994,
    "content": "this data is not supported on Windows This option should be selected for dependency analysis of applications with multiple CPU threads using CUDA Timeline Options: Load data for time range - If selected the start and end time stamps for the range of data to be loaded can be specified If a timeline is un-checked, the data associated with that timeline will not be loaded and it will not be displayed"
  },
  {
    "id": 21995,
    "content": "Note If some timelines are disabled by un-checking the option the analyses results which use this timeline data will be incorrect"
  },
  {
    "id": 22000,
    "content": "Analyzing Your Application  If the Don’t run guided analysis option was not selected when you created your session, the Visual Profiler will immediately run your application to collect the data needed for the first stage of guided analysis As described in the Analysis View section, you can use the guided analysis system to get recommendations on performance limiting behavior in your application"
  },
  {
    "id": 22004,
    "content": "Exploring the Timeline  In addition to the guided analysis results, you will see a timeline for your application showing the CPU and GPU activity that occurred as your application executed Read Timeline View and Properties View to learn how to explore the profiling information that is available in the timeline Navigating the Timeline describes how you can zoom and scroll the timeline to focus on"
  },
  {
    "id": 22009,
    "content": "Looking at the Details  In addition to the results provided in the Analysis View , you can also look at the specific metric and event values collected as part of the analysis You can collect specific metric and event values that reveal how the kernels in your application are behaving You collect metrics and events as described in the GPU Details View section"
  },
  {
    "id": 22013,
    "content": "Improve Loading of Large Profiles  Some applications launch many tiny kernels, making them prone to very large (100s of megabytes or larger) output, even for application runs of only a few seconds"
  },
  {
    "id": 22014,
    "content": "The Visual Profiler needs roughly the same amount of memory as the size of the profile it is opening/importing"
  },
  {
    "id": 22015,
    "content": "The Java virtual machine may use a fraction of the main memory if no “max heap size” setting is specified So depending on the size of main memory, the Visual Profiler may fail to load some large files If the Visual Profiler fails to load a large profile, try setting the max heap size that JVM is allowed to use according to main memory size"
  },
  {
    "id": 22016,
    "content": "On macOS the nvvp ini file is present in folder /Developer/{cuda_install_dir}/libnvvp/nvvp app/Contents/MacOS/ The nvvp ini configuration file looks like this: -startup plugins/org"
  },
  {
    "id": 22039,
    "content": "DefaultType=mozilla To force the JVM to use 3 gigabytes of memory, for example, add a new line with ‑Xmx3G after ‑vmargs"
  },
  {
    "id": 22040,
    "content": "For example, if your system has 24GB of system memory, and you happen to know that you won’t need to run any other memory-intensive applications at the same time as the Visual Profiler, so it’s okay for the profiler to take up the vast majority of that space"
  },
  {
    "id": 22041,
    "content": "So you might pick, say, 22GB as the maximum heap size, leaving a few gigabytes for the OS, GUI, and any other programs that might be running"
  },
  {
    "id": 22043,
    "content": "ini configuration settings can also be modified: Increase the default heap size (the one Java automatically starts up with) to, say, 2GB ( -Xms ) Tell Java to run in 64-bit mode instead of the default 32-bit mode (only works on 64-bit systems); this is required if you want heap sizes >4GB"
  },
  {
    "id": 22044,
    "content": "( -d64 ) Enable Javas parallel garbage collection system, which helps both to decrease the required memory space for a given input size as well as to catch out of memory errors more gracefully"
  },
  {
    "id": 22045,
    "content": "( -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode ) Note: most installations require administrator/root-level access to modify this file"
  },
  {
    "id": 22054,
    "content": "In addition to this you can use timeline options Load data for time range and Enable timelines in the session mentioned in the Creating a Session section to limit the data which is loaded and displayed"
  },
  {
    "id": 22057,
    "content": "Sessions  A session contains the settings, data, and profiling results associated with your application Each session is saved in a separate file; so you can delete, move, copy, or share a session by simply deleting, moving, copying, or sharing the session file There are two types of sessions: an executable session that is associated with an application that is executed and profiled from within"
  },
  {
    "id": 22062,
    "content": "Executable Session  You can create a new executable session for your application by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu Once a session is created, you can edit the session’s settings as described in the Settings View"
  },
  {
    "id": 22063,
    "content": "To analyze your application and to collect metric and event values, the Visual Profiler will execute your application multiple times To get accurate profiling results, it is important that your application conform to the requirements detailed in Application Requirements"
  },
  {
    "id": 22067,
    "content": "Import Session  You create an import session from the output of nvprof by using the Import… option in the File menu Selecting this option opens the import dialog which guides you through the import process Because an executable application is not associated with an import session, the Visual Profiler cannot execute the application to collect additional profile data Also, the GPU Details View"
  },
  {
    "id": 22068,
    "content": "will show any imported event and metrics values but new metrics and events cannot be selected and collected for the import session"
  },
  {
    "id": 22073,
    "content": "Import Single-Process nvprof Session  Using the import dialog you can select one or more nvprof data files for import into the new session You must have one nvprof data file that contains the timeline information for the session You can optionally enable other options such as --system-profiling on , but you should not collect any events or metrics as that will distort the timeline so that it is"
  },
  {
    "id": 22074,
    "content": "not representative of the applications true behavior You may optionally specify one or more event/metric data files that contain event and metric values for the application These data files should be collected by running nvprof with one or both of the --events and --metrics options To collect all the events and metrics that are needed for the analysis system, you can simply use the"
  },
  {
    "id": 22075,
    "content": "--analysis-metrics option along with the --kernels option to select the kernel(s) to collect events and metrics for If you are importing multiple nvprof output files into the session, it is important that your application conform to the requirements detailed in Application Requirements"
  },
  {
    "id": 22080,
    "content": "Import Multi-Process nvprof Session  Using the import wizard you can select multiple nvprof data files for import into the new multi-process session Select the Multiple Processes option in the Import nvprof Data dialog as shown in the figure below When importing timeline data from multiple processes you may not specify any event/metric data files for those processes Multi-processes profiling is"
  },
  {
    "id": 22086,
    "content": "Import Command-Line Profiler Session  Support for command-line profiler (using the environment variable COMPUTE_PROFILE) has been dropped, but CSV files generated using earlier versions can still be imported Using the import wizard you can select one or more command-line profiler generated CSV files for import into the new session When you import multiple CSV files, their contents are combined"
  },
  {
    "id": 22087,
    "content": "and displayed in a single timeline The command-line profiler CSV file must be generated with the gpustarttimestamp and streamid configuration parameters It is fine to include other configuration parameters, including events"
  },
  {
    "id": 22090,
    "content": "Application Requirements  To collect performance data about your application, the Visual Profiler must be able to execute your application repeatedly in a deterministic manner Due to software and hardware limitations, it is not possible to collect all the necessary profile data in a single execution of your application Each time your application is run, it must operate on the same data and"
  },
  {
    "id": 22091,
    "content": "perform the same kernel and memory copy invocations in the same order Specifically, For a device, the order of context creation must be the same each time the application executes For a multi-threaded application where each thread creates its own context(s), care must be taken to ensure that the order of those context creations is consistent across multiple runs For example, it may be necessary to"
  },
  {
    "id": 22092,
    "content": "create the contexts on a single thread and then pass the contexts to the other threads Alternatively, the NVIDIA Tools Extension API can be used to provide a custom name for each context As long as the same custom name is applied to the same context on each execution of the application, the Visual Profiler will be able to correctly associate those contexts across multiple runs For a context, the"
  },
  {
    "id": 22093,
    "content": "order of stream creation must be the same each time the application executes Alternatively, the NVIDIA Tools Extension API can be used to provide a custom name for each stream As long as the same custom name is applied to the same stream on each execution of the application, the Visual Profiler will be able to correctly associate those streams across multiple runs Within a stream, the order of"
  },
  {
    "id": 22097,
    "content": "Visual Profiler Views  The Visual Profiler is organized into views Together, the views allow you to analyze and visualize the performance of your application This section describes each view and how you use it while profiling your application"
  },
  {
    "id": 22101,
    "content": "Timeline View  The Timeline View shows CPU and GPU activity that occurred while your application was being profiled"
  },
  {
    "id": 22103,
    "content": "Along the top of the view is a horizontal ruler that shows elapsed time from the start of application profiling Along the left of the view is a vertical ruler that describes what is being shown for each horizontal row of the timeline, and that contains various controls for the timeline These controls are described in Timeline Controls The timeline view is composed of timeline rows Each row shows"
  },
  {
    "id": 22104,
    "content": "intervals that represent the start and end times of the activities that correspond to the type of the row For example, timeline rows representing kernels have intervals representing the start and end times of executions of that kernel"
  },
  {
    "id": 22105,
    "content": "These sub-rows are created dynamically as necessary depending on how much activity overlap there is The placement of intervals within certain sub-rows does not convey any particular meaning Intervals are just packed into sub-rows using a heuristic that attempts to minimize the number of needed sub-rows"
  },
  {
    "id": 22106,
    "content": "The types of timeline rows that are displayed in the Timeline View are: Process A timeline will contain a Process row for each application profiled Thread A timeline will contain a Thread row for each CPU thread in the profiled application that performed either a CUDA driver or CUDA runtime API call Runtime API A timeline will contain a Runtime API row for each CPU thread that performs a CUDA"
  },
  {
    "id": 22107,
    "content": "Runtime API call Driver API A timeline will contain a Driver API row for each CPU thread that performs a CUDA Driver API call OpenACC A timeline will contain one or multiple OpenACC rows for each CPU thread that calls OpenACC directives Within one timeline, OpenACC activities on rows further down are called from within activities on the rows above Each interval in the row represents how long the"
  },
  {
    "id": 22109,
    "content": "The application may be in multiple states at the same time, this is shown by drawing multiple rows where some intervals overlap"
  },
  {
    "id": 22110,
    "content": "Pthread A timeline will contain one Pthread row for each CPU thread that performs Pthread API calls, given that host thread API calls have been recorded during measurement Note that for performance reasons, only selected Pthread API calls may have been recorded Markers and Ranges A timeline will contain a single Markers and Ranges row for each CPU thread that uses the NVIDIA Tools Extension API"
  },
  {
    "id": 22111,
    "content": "to annotate a time range or marker Each interval in the row represents the duration of a time range, or the instantaneous point of a marker Profiling Overhead A timeline will contain a single Profiling Overhead row for each process Each interval in the row represents the duration of execution of some activity required for profiling"
  },
  {
    "id": 22112,
    "content": "These intervals represent activity that does not occur when the application is not being profiled Device A timeline will contain a Device row for each GPU device utilized by the application being profiled The name of the timeline row indicates the device ID in square brackets followed by the name of the device After running the Compute Utilization analysis, the row will contain an estimate of the"
  },
  {
    "id": 22113,
    "content": "compute utilization of the device over time If power, clock, and thermal profiling are enabled, the row will also contain points representing those readings Unified Memory A timeline will contain a Unified Memory row for each CPU thread and device that uses unified memory The Unified memory may contain CPU Page Faults, GPU Page Faults, Data Migration (DtoH) and Data Migration (HtoD) rows When"
  },
  {
    "id": 22114,
    "content": "creating a session user can select segment mode or non-segment mode for Unified Memory timelines In the segment mode the timeline is split into equal width segments and only aggregated data values for each time segment are shown In non-segment mode each interval on the timeline will represent the actual data collected and the properties for each interval can be viewed Under properties for the"
  },
  {
    "id": 22115,
    "content": "timeline the property which is used for selecting the color is given and also a legend displays the mapping of colors to different range of property values In the non-segment mode each interval on the timeline corresponds to one data migration from device to host In the non-segment mode each interval on the timeline corresponds to one GPU page fault group In the non-segment mode each interval on"
  },
  {
    "id": 22116,
    "content": "the timeline corresponds to one data migration from host to device The name of the timeline row indicates the context ID or the custom context name if the NVIDIA Tools Extension API was used to name the context A context may contain up to four memcpy rows for device-to-host, host-to-device, device-to-device, and peer-to-peer memory copies Compute A timeline will contain a Compute row for each"
  },
  {
    "id": 22117,
    "content": "context that performs computation on the GPU All kernel activity, including kernels launched using CUDA Dynamic Parallelism, is shown on the Compute row The Kernel rows following the Compute row show activity of each individual application kernel Each interval in a row represents the duration of execution of an instance of that kernel in the containing context Each row is labeled with a percentage"
  },
  {
    "id": 22118,
    "content": "that indicates the total execution time of all instances of that kernel compared to the total execution time of all kernels For each context, the kernels are ordered top to bottom by this execution time percentage For CUDA Dynamic Parallelism applications, the kernels are organized in a hierarchy that represents the parent/child relationship between the kernels Kernels that use CUDA Dynamic"
  },
  {
    "id": 22119,
    "content": "Parallelism to launch other kernels can be expanded using the ‘+’ icon to show the kernel rows representing those child kernels For kernels that don’t launch child kernels, the kernel execution is represented by a solid interval, showing the time that that instance of the kernel was executing on the GPU For kernels that launch child kernels, the interval can also include a hollow part at the end"
  },
  {
    "id": 22120,
    "content": "The hollow part represents the time after the kernel has finished executing where it is waiting for child kernels to finish executing The CUDA Dynamic Parallelism execution model requires that a parent kernel not complete until all child kernels complete and this is what the hollow part is showing The Focus control described in Timeline Controls can be used to control display of the parent/child"
  },
  {
    "id": 22121,
    "content": "timelines Stream A timeline will contain a Stream row for each stream used by the application (including both the default stream and any application created streams) Each interval in a Stream row represents the duration of a memcpy or kernel execution performed on that stream"
  },
  {
    "id": 22126,
    "content": "Timeline Controls  The Timeline View has several controls that you use to control how the timeline is displayed Some of these controls also influence the presentation of data in the GPU Details View and the Analysis View"
  },
  {
    "id": 22127,
    "content": "Resizing the Vertical Timeline Ruler The width of the vertical ruler can be adjusted by placing the mouse pointer over the right edge of the ruler"
  },
  {
    "id": 22129,
    "content": "You may want to reorder these rows to aid in visualizing related kernels and streams, or to move unimportant kernels and streams to the bottom of the timeline"
  },
  {
    "id": 22130,
    "content": "Filtering Timelines Memcpy and Kernel rows can be filtered to exclude their activities from presentation in the GPU Details View and the Analysis View When a row is filtered, any intervals on that row are dimmed to indicate their filtered status Expanding and Collapsing Timelines Groups of timeline rows can be expanded and collapsed using the [+] and [-] controls just to the left of the row"
  },
  {
    "id": 22131,
    "content": "labels There are three expand/collapse states: Collapsed No timeline rows contained in the collapsed row are shown Intervals associated with collapsed rows may not be shown in the GPU Details View and the Analysis View , depending on the filtering mode set for those views (see view documentation for more information) For example, if you collapse a device row, then all memcpys, memsets, and kernels"
  },
  {
    "id": 22132,
    "content": "associated with that device are excluded from the results shown in those views The coloring mode can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar In kernel coloring mode, each type of kernel is assigned a unique color (that is, all activity intervals in a kernel row have the same color) In stream coloring"
  },
  {
    "id": 22133,
    "content": "mode, each stream is assigned a unique color (that is, all memcpy and kernel activity occurring on a stream are assigned the same color) In process coloring mode, each process is assigned a unique color (that is, all memcpy and kernel activity occurring in a process are assigned the same color) Focusing Kernel Timelines For applications using CUDA Dynamic Parallelism, the Timeline View displays a"
  },
  {
    "id": 22134,
    "content": "hierarchy of kernel activity that shows the parent/child relationship between kernels The focus timeline control can be used to focus the displayed parent/child relationships to a specific, limited set of “family trees” The focus timeline mode can be selected and deselected in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar To see the"
  },
  {
    "id": 22135,
    "content": "“family tree” of a particular kernel, select a kernel and then enable Focus mode All kernels except those that are ancestors or descendants of the selected kernel will be hidden Use the “Don’t Focus” option to disable focus mode and restore all kernels to the Timeline view Dependency Analysis Controls There are two modes for visualizing dependency analysis results in the timeline: Focus Critical"
  },
  {
    "id": 22136,
    "content": "Path and Highlight Execution Dependencies These modes can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the Visual Profiler toolbar These options become available after the Dependency Analysis application analysis stage has been run (see Unguided Application Analysis ) Navigating the Timeline  The timeline can be scrolled,"
  },
  {
    "id": 22137,
    "content": "zoomed, and focused in several ways to help you better understand and visualize your application’s performance Zooming The zoom controls are available in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar Zoom-in reduces the timespan displayed in the view, zoom-out increases the timespan displayed in the view, and zoom-to-fit"
  },
  {
    "id": 22138,
    "content": "scales the view so that the entire timeline is visible You can also zoom-in and zoom-out with the mouse wheel while holding the Ctrl key (for macOS use the Command key) Select a region of the timeline by holding Ctrl (for macOS use the Command key) while left-clicking and dragging the mouse The highlighted region will be expanded to occupy the entire view when the mouse button is released The"
  },
  {
    "id": 22139,
    "content": "timeline can be scrolled horizontally with the scrollbar or by using the mouse wheel while holding the Shift key Highlighting/Correlation When you move the mouse pointer over an activity interval on the timeline, that interval is highlighted in all places where the corresponding activity is shown For example, if you move the mouse pointer over an interval representing a kernel execution, that"
  },
  {
    "id": 22140,
    "content": "kernel execution is also highlighted in the Stream and in the Compute timeline row When a kernel or memcpy interval is highlighted, the corresponding driver or runtime API interval will also highlight This allows you to see the correlation between the invocation of a driver or runtime API or OpenACC directive on the CPU and the corresponding activity on the GPU When a single interval or row is"
  },
  {
    "id": 22141,
    "content": "selected, the information about that interval or row is pinned in the Properties View In the GPU Details View , the detailed information for the selected interval is shown in the table Measuring Time Deltas Measurement rulers can be created by left-click dragging in the horizontal ruler at the top of the timeline After a ruler is created, it can be resized by dragging the vertical guide lines that"
  },
  {
    "id": 22142,
    "content": "appear over the timeline If the mouse is dragged over a timeline interval, the guideline will snap to the nearest edge of that interval"
  },
  {
    "id": 22148,
    "content": "This is more apparent if the data file being loaded is big, or the application has generated a lot of data"
  },
  {
    "id": 22149,
    "content": "At the same time, a spinning circle replaces the icon of the current session tab, indicating the timeline is not fully loaded To reduce its memory footprint, the profiler may skip loading some timeline contents if they are not visible at the current zoom level These contents will be automatically loaded when they become visible on a new zoom level"
  },
  {
    "id": 22154,
    "content": "Dependency Analysis Controls  The profiler allows the visualization of dependency analysis results in the timeline once the respective analysis stage has been run"
  },
  {
    "id": 22155,
    "content": "Focus Critical Path visualizes the critical path through the application by focusing on all intervals on the critical path and fading others"
  },
  {
    "id": 22156,
    "content": "When the mode is enabled and any timeline interval is selected (by left-clicking it), the selected interval will have focus"
  },
  {
    "id": 22157,
    "content": "This allows you to “follow” the critical path through the execution and to inspect individual intervals Highlight Execution Dependencies allows you to analyze the execution dependencies for each interval (Note that for certain intervals, no dependency information is collected) When this mode is enabled, the highlighting color changes from yellow (representing correlated intervals) to red"
  },
  {
    "id": 22158,
    "content": "(representing dependencies) Both the selected interval as well as all incoming and outgoing dependencies are highlighted"
  },
  {
    "id": 22162,
    "content": "Analysis View  The Analysis View is used to control application analysis and to display the analysis results In guided mode the analysis system will guide you through multiple analysis stages to help you understand the likely performance limiters and optimization opportunities in your application In unguided mode you can manually explore all the analysis results collected for your application"
  },
  {
    "id": 22163,
    "content": "The left part of the view provides step-by-step directions to help you analyze and optimize your application The right part of the view shows detailed analysis results appropriate for each part of the analysis"
  },
  {
    "id": 22168,
    "content": "Guided Application Analysis  In guided mode, the analysis view will guide you step-by-step through analysis of your entire application with specific analysis guidance provided for each kernel within your application Guided analysis starts with CUDA Application Analysis and from there will guide you to optimization opportunities within your application"
  },
  {
    "id": 22173,
    "content": "Unguided Application Analysis  In unguided analysis mode each application analysis stage has a Run analysis button that can be used to generate the analysis results for that stage When the Run analysis button is selected, the profiler will execute the application to collect the profiling data needed to perform the analysis The green check-mark next to an analysis stage indicates that the"
  },
  {
    "id": 22174,
    "content": "analysis results for that stage are available Each analysis result contains a brief description of the analysis and a More… link to detailed documentation on the analysis When you select an analysis result, the timeline rows or intervals associated with that result are highlighted in the Timeline View When a single kernel instance is selected in the timeline, additional kernel-specific analysis"
  },
  {
    "id": 22175,
    "content": "stages are available Each kernel-specific analysis stage has a Run analysis button that operates in the same manner as for the application analysis stages The following figure shows the analysis results for the Divergent Execution analysis stage Some kernel instance analysis results, like Divergent Execution, are associated with specific source-lines within the kernel The source file associated"
  },
  {
    "id": 22181,
    "content": "PC Sampling View  Devices with compute capability 5 2 and higher, excluding mobile devices, have a feature for PC sampling In this feature PC and state of warp are sampled at regular interval for one of the active warps per SM The warp state indicates if that warp issued an instruction in a cycle or why it was stalled and could not issue an instruction When a warp that is sampled is stalled,"
  },
  {
    "id": 22182,
    "content": "there is a possibility that in the same cycle some other warp is issuing an instruction Hence the stall for the sampled warp need not necessarily indicate that there is a hole in the instruction issue pipeline Devices with compute capability 6 0 and higher have a new feature that gives latency reasons While collecting these samples, there is no instruction issued in the respective warp scheduler"
  },
  {
    "id": 22183,
    "content": "and hence these give the latency reasons The latency reasons will be one of the stall reasons in Warp State section except ‘not selected’ stall reason The profiler collects this information and presents it in the Kernel Profile - PC Sampling view After clicking on the source file or device function the Kernel Profile - PC Sampling view is opened"
  },
  {
    "id": 22184,
    "content": "The hotspots shown next to the vertical scroll bar are determined by the number of samples collected for each source and assembly line The distribution of the stall reasons is shown as a stacked bar for each source and assembly line"
  },
  {
    "id": 22186,
    "content": "0 and higher, Visual Profiler show two views: ‘Kernel Profile - PC Sampling’ which gives the warp state view and ‘Kernel Profile - PC Sampling - Latency’ which gives the latency reasons"
  },
  {
    "id": 22187,
    "content": "The tables in result section give percentage distribution for total latency samples, issue pipeline busy samples and instruction issued samples"
  },
  {
    "id": 22188,
    "content": "The blog post Pinpoint Performance Problems with Instruction-Level Profiling shows how PC Sampling can be used to optimize a CUDA kernel"
  },
  {
    "id": 22195,
    "content": "The green nodes in the diagram depict logical memory space whereas blue nodes depicts actual hardware unit on the chip"
  },
  {
    "id": 22196,
    "content": "For the various caches the reported percentage number states the cache hit rate; that is the ratio of requests that could be served with data locally available to the cache over all requests made"
  },
  {
    "id": 22197,
    "content": "The links between the nodes in the diagram depict the data paths between the SMs to the memory spaces into the memory system The data paths from the SMs to the memory spaces (Global, Local, Texture, Surface and Shared) report the total number of memory instructions executed, it includes both read and write operations The data path between memory spaces and “Unified Cache” or “Shared Memory”"
  },
  {
    "id": 22199,
    "content": "The arrow pointing to right direction indicates WRITE operation whereas the arrow pointing to left direction indicates the READ operations"
  },
  {
    "id": 22204,
    "content": "NVLink view  NVIDIA NVLink is a high-bandwidth, energy-efficient interconnect that enables fast communication between the CPU and GPU, and between GPUs Visual Profiler collects NVLink topology and NVLink transmit/receive throughput metrics and maps the metrics on to the topology NVLink information is presented in the Results section of Examine GPU Usage in CUDA Application Analysis in Guided"
  },
  {
    "id": 22205,
    "content": "Analysis NVLink Analysis shows topology that shows the logical NVLink connections between different devices A logical link comprises of 1 to 4 physical NVLinks of same properties connected between two devices Visual profiler lists the properties and achieved utilization for logical NVLinks in ‘Logical NVLink Properties’ table It also lists the transmit and receive throughputs for logical NVLink in"
  },
  {
    "id": 22210,
    "content": "Source-Disassembly View  The Source-Disassembly View is used to display the analysis results for a kernel at the source and assembly instruction level To be able to view the kernel source you need to compile the code using the -lineinfo option This view is displayed for the following types of analysis: Global Memory Access Pattern Analysis Shared Memory Access Pattern Analysis Divergent"
  },
  {
    "id": 22211,
    "content": "Execution Analysis Kernel Profile - Instruction Execution Analysis Kernel Profile - PC Sampling Analysis As part of the Guided Analysis or Unguided Analysis for a kernel the analysis results are displayed under the Analysis view After clicking on the source file or device function the Source-Disassembly view is opened If the source file is not found a dialog is opened to select and point to the"
  },
  {
    "id": 22212,
    "content": "new location of the source file The Source-Disassembly view contains: High level source Assembly instructions Hotspots at the source level Hotspots at the assembly instruction level Columns for profiling data aggregated to the source level Columns for profiling data collected at the assembly instruction level The information shown in the Source-Disassembly view can be customized by the following"
  },
  {
    "id": 22213,
    "content": "toolbar options: View menu - Select one or more out of the available profiler data columns to display Maximize the source view Maximize the disassembly view Hotspots are colored based on level of importance - low, medium or high Hovering the mouse over the hotspot displays the value of the profiler data, the level of importance and the source or disassembly line You can click on a hotspot at the"
  },
  {
    "id": 22214,
    "content": "source level or assembly instruction level to view the source or disassembly line corresponding to the hotspot In the disassembly view the assembly instructions corresponding to the selected source line are highlighted You can click on the up and down arrow buttons displayed at the right of the disassembly column header to navigate to the next or previous instruction block"
  },
  {
    "id": 22218,
    "content": "GPU Details View  The GPU Details View displays a table of information for each memory copy and kernel execution in the profiled application For kernels, the table will also contain a column for each metric or event value collected for that kernel In the figure, the Achieved Occupancy column shows the value of that metric for each of the kernel executions You can sort the data by column by left"
  },
  {
    "id": 22219,
    "content": "clicking on the column header, and you can rearrange the columns by left clicking on a column header and dragging it to its new location If you select a row in the table, the corresponding interval will be selected in the Timeline View Similarly, if you select a kernel or memcpy interval in the Timeline View the table will be scrolled to show the corresponding data If you hover the mouse over a"
  },
  {
    "id": 22220,
    "content": "column header, a tooltip will display the data shown in that column For a column containing event or metric data, the tooltip will describe the corresponding event or metric The information shown in the GPU Details View can be filtered in various ways using the menu accessible from the Details View toolbar The following modes are available: Filter By Selection - If selected, the GPU Details View"
  },
  {
    "id": 22221,
    "content": "shows data only for the selected kernel and memcpy intervals Show Hidden Timeline Data - If not selected, data is shown only for kernels and memcpys that are visible in the timeline Kernels and memcpys that are not visible because they are inside collapsed parts of the timeline are not shown Show Filtered Timeline Data - If not selected, data is shown only for kernels and memcpys that are in"
  },
  {
    "id": 22222,
    "content": "timeline rows that are not filtered Collecting Events and Metrics Specific event and metric values can be collected for each kernel and displayed in the details table Use the toolbar icon in the upper right corner of the view to configure the events and metrics to collect for each device, and to run the application to collect those events and metrics Show Summary Data By default the table shows"
  },
  {
    "id": 22223,
    "content": "one row for each memcpy and kernel invocation Use the toolbar icon in the upper right corner of the view to select or deselect summary format Formatting Table Contents The numbers in the table can be displayed either with or without grouping separators Use the toolbar icon in the upper right corner of the view to select or deselect grouping separators Exporting Details The contents of the table"
  },
  {
    "id": 22228,
    "content": "CPU Details View  CPU Details view This view details the amount of time your application spends executing functions on the CPU Each thread is sampled periodically to capture its callstack and the summary of these measurements are displayed in this view You can manipulate the view by selecting different orientations for organizing the callstack: Top-down, Bottom-up, Code Structure (3), choosing"
  },
  {
    "id": 22229,
    "content": "which thread to view (1), and by sorting or highlighting a specific thread (7, 8) All the threads profiled are shown in one view when the ‘all threads’ option is selected (default) This column displays a tree of events representing the structure of the application’s execution on the CPU The following modes are available: Top-down (callers first) call tree view - The CPU details tree is organized"
  },
  {
    "id": 22230,
    "content": "as a call tree with each function shown as a child of its caller Bottom-up (callees first) call tree view - The CPU details tree is organized in such a way that each function is shown as a child of any functions it calls In this mode you can quickly identify the call path that contributes the most time to the application’s execution Code structure (file and line) tree view - The CPU details tree"
  },
  {
    "id": 22231,
    "content": "shows which functions belong to each source file and library as well as how much of the application’s execution is attributed to a given line of source code In every mode the time listed for each function is ‘inclusive’ and includes time spent both in this function and any functions that it calls This column displays the total amount of time spent by all threads in this event as a percentage of"
  },
  {
    "id": 22232,
    "content": "the total amount of time spent in all events This column displays a bar denoting a range where the amount of time spent in an event by any thread is always within this this range Also, if there is space, a small ‘diamond’ is drawn in the middle of the bar where the mean time is spent in this event across all threads"
  },
  {
    "id": 22233,
    "content": "On the left is a vertical scale showing the same minimum and maximum values as shown on the range chart"
  },
  {
    "id": 22234,
    "content": "If the cell for the given event / thread combination is greyed out then no time was spent by this thread in this event (for this example both threads 1 and 2 spent no time in the event ‘x_solve’) Furthermore, the thread(s) with the minimum or maximum amount of time spent in the event across all threads are annotated with the ‘triangle / line’ In this example thread 3 spent the most and thread 6"
  },
  {
    "id": 22235,
    "content": "the least amount of time in the event ‘x_solve’ To reorder the rows by the time spent on a given thread click on the thread column header Having highlighted thread 3 we now see a vertical line on the range chart showing the amount of time this thread spent in this event compared to the range across all thread"
  },
  {
    "id": 22236,
    "content": "CPU Threads CPU Source Code You can open the CPU Source View for any function by double-clicking on it in the tree"
  },
  {
    "id": 22237,
    "content": "Sometimes a file within a specific directory is being sought, in this case you should give the path to where this directory resides"
  },
  {
    "id": 22239,
    "content": "For this reason a function will only appear in this view if it was sampled during execution If a function was not sampled the time it was running is accounted to the function that called it"
  },
  {
    "id": 22240,
    "content": "In order to gather a CPU profile that is representative of the application’s performance the code of interest must execute for enough to gather enough samples"
  },
  {
    "id": 22241,
    "content": "Tip The file and line information is gathered from the application’s debug information obtained by the compiler"
  },
  {
    "id": 22242,
    "content": "To ensure that this information is available it is recommended that you compile with ‘-g’ or a similar option"
  },
  {
    "id": 22246,
    "content": "OpenACC Details View  OpenACC table view The OpenACC Details View displays each OpenACC runtime activity executed by the profiled application"
  },
  {
    "id": 22247,
    "content": "Each activity is grouped by source location: each activity which occurs at the same file and line number in the application’s source code is placed under a node labeled with the source location Each activity shows the amount of time spent by the profiled application as both a unit of time and as a percentage of the total time this application was executing any OpenACC activity There are two ways"
  },
  {
    "id": 22248,
    "content": "to count how much time is spent in a particular OpenACC activity: Show the Inclusive durations (counting any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the total time spent in each activity including any activities that were executed as the result of this activity In this case the amount of time spent in each activity occurring"
  },
  {
    "id": 22249,
    "content": "at a given application source location is totaled and displayed on the row displaying the source location Show the Exclusive durations (excluding any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the time spent only in a given activity In this case the amount of time spent at a given source location is always zero—time is attributed"
  },
  {
    "id": 22254,
    "content": "OpenMP Details View  OpenMP table view The OpenMP Details view displays the activity of the OpenMP runtime on the CPU"
  },
  {
    "id": 22255,
    "content": "The time your application spends in a parallel region or idling is shown both on the timeline and is summarized in this view The reference for the percentage of time spent in each type of activity is the time from the start of the first parallel region to the end of the last parallel region The sum of the percentages of each activity type often exceeds 100% because the OpenMP runtime can be in"
  },
  {
    "id": 22260,
    "content": "Properties View  The Properties View shows information about the row or interval highlighted or selected in the Timeline View If a row or interval is not selected, the displayed information tracks the motion of the mouse pointer If a row or interval is selected, the displayed information is pinned to that row or interval When an OpenACC interval with an associated source file is selected, this"
  },
  {
    "id": 22261,
    "content": "filename is shown in the Source File table entry Double-clicking on the filename opens the respective source file if it is available on the file-system"
  },
  {
    "id": 22265,
    "content": "Console View  The Console View shows stdout and stderr output of the application each time it executes If you need to provide stdin input to your application, do so by typing into the console view"
  },
  {
    "id": 22269,
    "content": "Settings View  The Settings View allows you to specify execution settings for the application being profiled As shown in the following figure, the Executable settings tab allows you to specify the executable file, the working directory, the command-line arguments, and the environment for the application Exection Timeout The Executable settings tab also allows you to specify an optional execution"
  },
  {
    "id": 22270,
    "content": "timeout If the execution timeout is specified, the application execution will be terminated after that number of seconds If the execution timeout is not specified, the application will be allowed to continue execution until it terminates normally Start execution with profiling enabled The Start execution with profiling enabled checkbox is set by default to indicate that application profiling"
  },
  {
    "id": 22271,
    "content": "begins at the start of application execution If you are using cudaProfilerStart() and cudaProfilerStop() to control profiling within your application as described in Focused Profiling , then you should uncheck this box Enable concurrent kernel profiling The Enable concurrent kernel profiling checkbox is set by default to enable profiling of applications that exploit concurrent kernel execution"
  },
  {
    "id": 22272,
    "content": "Disabling concurrent kernel execution can reduce profiling overhead in some cases and so may be appropriate for applications that do not exploit concurrent kernels Enable power, clock, and thermal profiling The Enable power, clock, and thermal profiling checkbox can be set to enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application"
  },
  {
    "id": 22276,
    "content": "CPU Source View  The CPU source code view allows you to inspect the files that comprise the profiled application’s CPU source This view can be opened in the CPU Details View by double-clicking on a function in the tree–the source file that corresponds to this function is then opened When compiling using the PGI® compilers annotations can be added to this view (see Common Compiler Feedback Format"
  },
  {
    "id": 22277,
    "content": "for more information) PGI compilers save information about how your program was optimized, or why a particular optimization was not made This can be combined with the CPU Details View to help identify why certain lines of code performed the way they did"
  },
  {
    "id": 22278,
    "content": "For example, the message may tell you about the following: vector instructions generated by the compiler"
  },
  {
    "id": 22279,
    "content": "compute-intensity of a loop, a ratio computation to memory operations–higher numbers mean that there is more computation than memory loads and stores"
  },
  {
    "id": 22280,
    "content": "information about parallelization, with a hint for how it might be possible to make the loop run in parallel if the compiler could not auto-parallelize it"
  },
  {
    "id": 22283,
    "content": "Customizing the Profiler  When you first start the Visual Profiler , and after closing the Welcome page, you will be presented with a default placement of the views By moving and resizing the views, you can customize the profiler to meet your development needs Any changes you make are restored the next time you start the profiler"
  },
  {
    "id": 22287,
    "content": "Resizing a View  To resize a view, simply left click and drag on the dividing area between the views All views stacked together in one area are resized at the same time"
  },
  {
    "id": 22291,
    "content": "Reordering a View  To reorder a view in a stacked set of views, left click and drag the view tab to the new location within the view stack"
  },
  {
    "id": 22295,
    "content": "Moving a View  to move a view, left click the view tab and drag it to its new location You can place the view in a new location, or stack it in the same location as other views"
  },
  {
    "id": 22299,
    "content": "Undocking a View  You can undock a view from the profiler window so that the view occupies its own stand-alone window You may want to do this to take advantage of multiple monitors or to maximum the size of an individual view To dock a view, left click the view tab (not the window decoration) and drag it into the profiler window"
  },
  {
    "id": 22303,
    "content": "Opening and Closing a View  Use the X icon on a view tab to close a view To open a view, use the View menu"
  },
  {
    "id": 22306,
    "content": "Command Line Arguments  When the Visual Profiler is started from the command line, it is possible, using command line arguments, to specify executable to start new session with or import profile files exported from nvprof using one of the following patterns: Start new executable session by launching nvvp with name of executable followed, optionally, by its arguments: nvvp executableName [["
  },
  {
    "id": 22307,
    "content": "executableArguments ] ] Import single-process nvprof session by launching nvvp with single nvprof file as argument(see nvprof’s export/import options section for more details): nvvp data nvprof Import multi-process nvprof session, by launching nvvp with multiple nvprof files as arguments: nvvp data1 nvprof"
  },
  {
    "id": 22309,
    "content": "​nvprof  The nvprof profiling tool enables you to collect and view profiling data from the command-line nvprof enables the collection of a timeline of CUDA-related activities on both CPU and GPU, including kernel execution, memory transfers, memory set and CUDA API calls and events or metrics for CUDA kernels Profiling results are displayed in the console after the profiling data is collected,"
  },
  {
    "id": 22310,
    "content": "and may also be saved for laterviewing by either nvprof or the Visual Profiler To profile an application from the command-line: nvprof [ options ] [ application ] [ application - arguments ] To view the full help page, type nvprof --help"
  },
  {
    "id": 22316,
    "content": "CUDA Profiling Options  Option Values Default Description aggregate-mode on, off on Turn on/off aggregate mode for events and metrics specified by subsequent --events and --metrics options"
  },
  {
    "id": 22318,
    "content": "analysis-metrics N/A N/A Collect profiling data that can be imported to Visual Profiler’s “analysis” mode"
  },
  {
    "id": 22320,
    "content": "continuous-sampling-interval {interval in milliseconds} 2 milliseconds Set the continuous mode sampling interval in milliseconds"
  },
  {
    "id": 22321,
    "content": "dependency-analysis N/A N/A Generate event dependency graph for host and device activities and run dependency analysis"
  },
  {
    "id": 22322,
    "content": "device-buffer-size {size in MBs} 8 MB Set the device memory size (in MBs) reserved for storing profiling data for non-CDP operations, especially for concurrent kernel tracing, for each buffer on a context device-cdp-buffer-size {size in MBs} 8 MB Set the device memory size (in MBs) reserved for storing profiling data for CDP operations for each buffer on a context"
  },
  {
    "id": 22323,
    "content": "devices {comma-separated device IDs}, all N/A Change the scope of subsequent --events , --metrics , --query-events and --query-metrics options event-collection-mode kernel, continuous kernel Choose event collection mode for all events/metrics kernel: Events/metrics are collected only for durations of kernel executions continuous: Events/metrics are collected for duration of application This mode"
  },
  {
    "id": 22324,
    "content": "is incompatible with --profile-all-processes or --profile-child-processes or --replay-mode kernel or --replay-mode application events (e) {comma-separated event names}, all N/A Specify the events to be profiled on certain device(s) kernel-latency-timestamps on, off off Turn on/off collection of kernel latency timestamps, namely queued and submitted The queued timestamp is captured when a kernel"
  },
  {
    "id": 22325,
    "content": "launch command was queued into the CPU command buffer The submitted timestamp denotes when the CPU command buffer containing this kernel launch was submitted to the GPU kernels {kernel name}, {[context id/name]:[stream id/name]:[kernel name]:[invocation]} N/A Change the scope of subsequent --events , --metrics options {[context id/name]:[stream id/name]:[kernel name]:[invocation]}: The"
  },
  {
    "id": 22326,
    "content": "context/stream IDs, names, kernel name and invocation can be regular expressions If [context id/name] or [stream id/name] is a positive number, it’s strictly matched against the CUDA context/stream ID Otherwise it’s treated as a regular expression and matched against the context/stream name specified by the NVTX library If the invocation count is a positive number, it’s strictly matched against"
  },
  {
    "id": 22327,
    "content": "the invocation of the kernel Example: --kernels \"1:foo:bar:2\" will profile any kernel whose name contains “bar” and is the 2nd instance on context 1 and on stream named “foo” metrics (m) {comma-separated metric names}, all N/A Specify the metrics to be profiled on certain device(s) Note: --metrics all does not include some metrics which are needed for Visual Profiler’s source level analysis"
  },
  {
    "id": 22328,
    "content": "pc-sampling-period {period in cycles} Between 5 and 12 based on the setup Specify PC Sampling period in cycles, at which the sampling records will be dumped"
  },
  {
    "id": 22329,
    "content": "profile-all-processes N/A N/A Profile all processes launched by the same user who launched this nvprof instance profile-api-trace none, runtime, driver, all all Turn on/off CUDA runtime/driver API tracing none: turn off API tracing runtime: only turn on CUDA runtime API tracing driver: only turn on CUDA driver API tracing all: turn on all API tracing profile-child-processes N/A N/A Profile the"
  },
  {
    "id": 22330,
    "content": "application and all child processes launched by it If it’s disabled, the application can use {cu,cuda}Profiler{Start,Stop} to turn on/off profiling"
  },
  {
    "id": 22331,
    "content": "profiling-semaphore-pool-size {count} 65536 Set the profiling semaphore pool size reserved for storing profiling data for serialized kernels and memory operations for each context"
  },
  {
    "id": 22332,
    "content": "replay-mode disabled, kernel, application kernel Choose replay mode used when not all events/metrics can be collected in a single run disabled: replay is disabled, events/metrics couldn’t be profiled will be dropped kernel: each kernel invocation is replayed application: the entire application is replayed skip-kernel-replay-save-restore on, off off If enabled, this option can vastly improve"
  },
  {
    "id": 22333,
    "content": "kernel replay speed, as save and restore of the mutable state for each kernel pass will be skipped Skipping of save/restore of input/output buffers allows you to specify that all profiled kernels on the context do not change the contents of their input buffers during execution, or call device malloc/free or new/delete, that leave the device heap in a different state Specifically, a kernel can"
  },
  {
    "id": 22334,
    "content": "malloc and free a buffer in the same launch, but it cannot call an unmatched malloc or an unmatched free Note: incorrectly using this mode while one of the kernels does modify the input buffer or uses unmatched malloc/free will result in undefined behavior, including kernel execution failure and/or corrupted device data on: skip save/restore of the input/output buffers off: save/restore"
  },
  {
    "id": 22335,
    "content": "input/output buffers for each kernel replay pass source-level-analysis (a) global_access, shared_access, branch, instruction_execution, pc_sampling N/A Specify the source level metrics to be profiled on a certain kernel invocation One or more of these may be specified, separated by commas global_access: global access shared_access: shared access branch: divergent branch instruction_execution:"
  },
  {
    "id": 22336,
    "content": "instruction execution pc_sampling: pc sampling, available only for GM20X+ Note: Use --export-profile to specify an export file track-memory-allocations on, off off Turn on/off tracking of memory operations, which involves recording timestamps, memory size, memory type and program counters of the memory allocations and frees unified-memory-profiling per-process-device, off per-process-device"
  },
  {
    "id": 22337,
    "content": "Configure unified memory profiling per-process-device: collect counts for each process and each device off: turn off unified memory profiling See Unified Memory Profiling for more information"
  },
  {
    "id": 22341,
    "content": "CPU Profiling Options  Option Values Default Description cpu-profiling on, off off Turn on CPU profiling cpu-profiling-explain-ccff {filename} N/A Set the path to a PGI pgexplain xml file that should be used to interpret Common Compiler Feedback Format (CCFF) messages cpu-profiling-frequency {frequency} 100Hz Set the CPU profiling frequency in samples per second cpu-profiling-mode flat,"
  },
  {
    "id": 22342,
    "content": "top-down, bottom-up bottom-up Set the output mode of CPU profiling flat: Show flat profile top-down: Show parent functions at the top bottom-up: Show parent functions at the bottom cpu-profiling-percentage-threshold {threshold} 0 (i"
  },
  {
    "id": 22344,
    "content": "function: Each level in the stack trace represents a distinct function instruction: Each level in the stack trace represents a distinct instruction address cpu-profiling-show-ccff on, off off Choose whether to print Common Compiler Feedback Format (CCFF) messages embedded in the binary cpu-profiling-show-library on, off off Choose whether to print the library name for each sample"
  },
  {
    "id": 22345,
    "content": "cpu-profiling-thread-mode separated, aggregated aggregated Set the thread mode of CPU profiling separated: Show separate profile for each thread aggregated: Aggregate data from all threads cpu-profiling-unwind-stack on, off on Choose whether to unwind the CPU call-stack at each sample point openacc-profiling on, off on Enable/disable recording information from the OpenACC profiling interface"
  },
  {
    "id": 22346,
    "content": "openmp-profiling on, off off Enable/disable recording information from the OpenMP profiling interface See OpenMP for more information"
  },
  {
    "id": 22350,
    "content": "Print Options  Option Values Default Description context-name {name} N/A Name of the CUDA context %p in the context name string is replaced with the process ID of the application being profiled"
  },
  {
    "id": 22351,
    "content": "normalized-time-unit (u) s, ms, us, ns, col, auto auto Specify the unit of time that will be used in the output s: second ms: millisecond us: microsecond ns: nanosecond col: a fixed unit for each column auto: the scale is chosen for each value based on its length"
  },
  {
    "id": 22352,
    "content": "openacc-summary-mode exclusive, inclusive exclusive Set how durations are computed in the OpenACC summary api - only turn on CUDA runtime and driver API tracing gpu - only turn on CUDA GPU tracing print-api-summary N/A N/A Print a summary of CUDA runtime/driver API calls print-gpu-summary N/A N/A Print a summary of the activities on the GPU (including CUDA kernels and memcpy’s/memset’s)"
  },
  {
    "id": 22353,
    "content": "print-gpu-trace N/A N/A Print individual kernel invocations (including CUDA memcpy’s/memset’s) and sort them in chronological order"
  },
  {
    "id": 22354,
    "content": "%p in the process name string is replaced with the process ID of the application being profiled %p in the stream name string is replaced with the process ID of the application being profiled %% in the stream name string is replaced with %"
  },
  {
    "id": 22358,
    "content": "IO Options  Option Values Default Description export-profile (o) {filename} N/A Export the result file which can be imported later or opened by the NVIDIA Visual Profiler"
  },
  {
    "id": 22359,
    "content": "%p in the file name string is replaced with the process ID of the application being profiled Note: If the application being profiled creates child processes, or if --profile-all-processes is used, the %p format is needed to get correct export files for each process"
  },
  {
    "id": 22360,
    "content": "force-overwrite (f) N/A N/A Force overwriting all output files (any existing files will be overwritten)"
  },
  {
    "id": 22361,
    "content": "log-file {filename} N/A Make nvprof send all its output to the specified file, or one of the standard channels"
  },
  {
    "id": 22362,
    "content": "print-nvlink-topology N/A N/A Print nvlink topology print-pci-topology N/A N/A Print PCI topology help (h) N/A N/A Print help information version (V) N/A N/A Print version information of this tool"
  },
  {
    "id": 22366,
    "content": "In this mode, nvprof outputs a single result line for each kernel function and each type of CUDA memory copy/set performed by the application For each kernel, nvprof outputs the total time of all instances of the kernel or type of memory copy as well as the average, minimum, and maximum time"
  },
  {
    "id": 22367,
    "content": "Output of nvprof (except for tables) are prefixed with = , being the process ID of the application being profiled"
  },
  {
    "id": 22368,
    "content": "Here’s a simple example of running nvprof on the CUDA sample matrixMul : $ nvprof matrixMul [ Matrix Multiply Using CUDA ] - Starting == 27694 == NVPROF is profiling process 27694 , command : matrixMul GPU Device 0 : \"GeForce GT 640M LE\" with compute capability 3"
  },
  {
    "id": 22371,
    "content": "708 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example"
  },
  {
    "id": 22372,
    "content": "If multiple CUDA capable devices are profiled, nvprof --print-summary-per-gpu can be used to print one summary per GPU"
  },
  {
    "id": 22373,
    "content": "If your application uses Dynamic Parallelism, the output will contain one column for the number of host-launched kernels and one for the number of device-launched kernels"
  },
  {
    "id": 22374,
    "content": "Here’s an example of running nvprof on the CUDA Dynamic Parallelism sample cdpSimpleQuicksort : $ nvprof cdpSimpleQuicksort == 27325 == NVPROF is profiling process 27325 , command : cdpSimpleQuicksort Running on GPU 0 ( Tesla K20c ) Initializing data : Running quicksort on 128 elements Launching kernel on the GPU Validating results : OK == 27325 == Profiling application : cdpSimpleQuicksort =="
  },
  {
    "id": 22385,
    "content": "GPU-Trace and API-Trace Modes  GPU-Trace and API-Trace modes can be enabled individually or together GPU-Trace mode provides a timeline of all activities taking place on the GPU in chronological order"
  },
  {
    "id": 22386,
    "content": "For each kernel or memory copy, detailed information such as kernel parameters, shared memory usage and memory transfer throughput are shown The number shown in the square brackets after the kernel name correlates to the CUDA API that launched that kernel"
  },
  {
    "id": 22387,
    "content": "Here’s an example: $ nvprof -- print - gpu - trace matrixMul == 27706 == NVPROF is profiling process 27706 , command : matrixMul == 27706 == Profiling application : matrixMul [ Matrix Multiply Using CUDA ] - Starting"
  },
  {
    "id": 22392,
    "content": "707 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example"
  },
  {
    "id": 22393,
    "content": "This number includes registers used internally by the CUDA driver and / or tools and can be more than what the compiler shows API-trace mode shows the timeline of all CUDA runtime and driver API calls invoked on the host in chronological order"
  },
  {
    "id": 22394,
    "content": "Here’s an example: $nvprof -- print - api - trace matrixMul == 27722 == NVPROF is profiling process 27722 , command : matrixMul == 27722 == Profiling application : matrixMul [ Matrix Multiply Using CUDA ] - Starting"
  },
  {
    "id": 22398,
    "content": "708 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : OK Note : For peak performance , please refer to the matrixMulCUBLAS example"
  },
  {
    "id": 22401,
    "content": "2130 us cuDeviceGetCount 108 42 ms 840 ns cuDeviceGet 108 42 ms 22 459 us cuDeviceGetName 108 45 ms 11 782 us cuDeviceTotalMem 108 46 ms 945 ns cuDeviceGetAttribute 149 37 ms 23"
  },
  {
    "id": 22418,
    "content": "901 ms cudaDeviceReset Note Due to the way the profiler is setup, the first “cuInit()” driver API call is never traced"
  },
  {
    "id": 22422,
    "content": "Event/metric Summary Mode  To see a list of all available events on a particular NVIDIA GPU, use the --query-events option To see a list of all available metrics on a particular NVIDIA GPU, use the --query-metrics option Here’s an example: $ nvprof -- events warps_launched , local_load -- metrics ipc matrixMul [ Matrix Multiply Using CUDA ] - Starting == 6461 == NVPROF is profiling process 6461"
  },
  {
    "id": 22425,
    "content": "== 6461 == Warning : Some kernel ( s ) will be replayed on device 0 in order to collect all events / metrics"
  },
  {
    "id": 22428,
    "content": "511 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : Result = PASS NOTE : The CUDA Samples are not meant for performance measurements"
  },
  {
    "id": 22429,
    "content": "== 6461 == Profiling application : matrixMul == 6461 == Profiling result : == 6461 == Event result : Invocations Event Name Min Max Avg Device \"GeForce GTX TITAN (0)\" Kernel : void matrixMulCUDA ( float * , float * , float * , int , int ) 301 warps_launched 6400 6400 6400 301 local_load 0 0 0 == 6461 == Metric result : Invocations Metric Name Metric Description Min Max Avg Device \"GeForce GTX"
  },
  {
    "id": 22430,
    "content": "TITAN (0)\" Kernel : void matrixMulCUDA ( float * , float * , float * , int , int ) 301 ipc Executed IPC 1"
  },
  {
    "id": 22433,
    "content": "291500 If the specified events/metrics can’t be profiled in a single run of the application, nvprof by default replays each kernel multiple times until all the events/metrics are collected In “application replay” mode, nvprof re-runs the whole application instead of replaying each kernel, in order to collect all events/metrics In some cases this mode can be faster than kernel replay mode if the"
  },
  {
    "id": 22434,
    "content": "application allocates large amount of device memory Replay can also be turned off entirely, in which case the profiler will not collect some events/metrics Note Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU Note If a large number of events or metrics are requested, no matter"
  },
  {
    "id": 22439,
    "content": "Event/metric Trace Mode  In event/metric trace mode, event and metric values are shown for each kernel execution"
  },
  {
    "id": 22440,
    "content": "For example, multiprocessor specific events are aggregated across all multiprocessors on the GPU For example, in the following example, the “branch” event value is shown for each multiprocessor on the GPU: $ nvprof -- aggregate - mode off -- events local_load -- print - gpu - trace matrixMul [ Matrix Multiply Using CUDA ] - Starting == 6740 == NVPROF is profiling process 6740 , command :"
  },
  {
    "id": 22445,
    "content": "822 msec , Size = 131072000 Ops , WorkgroupSize = 1024 threads / block Checking computed result for correctness : Result = PASS NOTE : The CUDA Samples are not meant for performance measurements"
  },
  {
    "id": 22446,
    "content": "== 6740 == Profiling application : matrixMul == 6740 == Profiling result : Device Context Stream Kernel local_load ( 0 ) local_load ( 1 )"
  },
  {
    "id": 22447,
    "content": "GeForce GTX TIT 1 7 void matrixMulCUDA Note Although --aggregate-mode applies to metrics, some metrics are only available in aggregate mode and some are only available in non-aggregate mode"
  },
  {
    "id": 22457,
    "content": "Concurrent Kernels  Concurrent-kernel profiling is supported, and is turned on by default This forces concurrent kernel executions to be serialized when a CUDA application is run with nvprof"
  },
  {
    "id": 22461,
    "content": "Profiling Scope  When collecting events/metrics, nvprof profiles all kernels launched on all visible CUDA devices by default --devices applies to --events , --metrics , --query-events and --query-metrics options that follows it It limits these options to collect events/metrics only on the devices specified by , which can be a list of device ID numbers separated by comma It limits these options"
  },
  {
    "id": 22462,
    "content": "to collect events/metrics only on the kernels specified by , which has the following syntax: or ::: Each string in the angle brackets can be a standard Perl regular expression"
  },
  {
    "id": 22463,
    "content": "If invocation is a positive number, it’s strictly matched against the invocation of the kernel If the context/stream string is a positive number, it’s strictly matched against the cuda context/stream ID Otherwise it’s treated as a regular expression and matched against the context/stream name provided by the NVIDIA Tools Extension"
  },
  {
    "id": 22464,
    "content": "Both --devices and --kernels can be specified multiple times, with distinct events/metrics associated --events , --metrics , --query-events and --query-metrics are controlled by the nearest scope options before them As an example, the following command, nvprof --devices 0 --metrics ipc --kernels \"1:foo:bar:2\" --events local_load a out collects metric ipc on all kernels launched on device 0 It"
  },
  {
    "id": 22465,
    "content": "also collects event local_load for any kernel whose name contains bar and is the 2nd instance launched on context 1 and on stream named foo on device 0"
  },
  {
    "id": 22469,
    "content": "Multiprocess Profiling  By default, nvprof only profiles the application specified by the command-line argument"
  },
  {
    "id": 22470,
    "content": "To profile all processes launched by an application, use the --profile-child-processes option nvprof also has a “profile all processes” mode, in which it profiles every CUDA process launched on the same system by the same user who launched nvprof"
  },
  {
    "id": 22475,
    "content": "System Profiling  For devices that support system profiling, nvprof can enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application"
  },
  {
    "id": 22480,
    "content": "Unified Memory Profiling  For GPUs that support Unified Memory, nvprof collects the Unified Memory related memory traffic to and from each GPU on your system"
  },
  {
    "id": 22482,
    "content": "On multi-GPU configurations without P2P support between any pair of devices that support Unified Memory, managed memory allocations are placed in zero-copy memory In certain cases, the environment variable CUDA_MANAGED_FORCE_DEVICE_ALLOC can be set to force managed allocations to be in device memory and to enable migration on these hardware configurations Normally, using the environment variable"
  },
  {
    "id": 22484,
    "content": "Please refer to the environment variables section in the CUDA C++ Programming Guide for further details"
  },
  {
    "id": 22488,
    "content": "CPU Thread Tracing  In order to allow a correct Dependency Analysis , nvprof can collect information about CPU-side threading APIs"
  },
  {
    "id": 22489,
    "content": "Recording this information is necessary if the application uses multiple CPU threads and at least two of these threads call the CUDA API"
  },
  {
    "id": 22491,
    "content": "Filtered calls include pthread_mutex_lock and pthread_mutex_unlock when those do not cause any concurrent thread to block"
  },
  {
    "id": 22492,
    "content": "Note CPU thread tracing starts after the first CUDA API call, from the thread issuing this call cuInit from its main thread before spawning any other user threads that call the CUDA API"
  },
  {
    "id": 22498,
    "content": "Adjust Units  By default, nvprof adjusts the time units automatically to get the most precise time values The --normalized-time-unit options can be used to get fixed time units throughout the results"
  },
  {
    "id": 22502,
    "content": "CSV  For each profiling mode, option --csv can be used to generate output in comma-separated values (CSV) format"
  },
  {
    "id": 22507,
    "content": "Export/Import  For each profiling mode, option --export-profile can be used to generate a result file This file is not human-readable, but can be imported back to nvprof using the option --import-profile , or into the Visual Profiler"
  },
  {
    "id": 22508,
    "content": "Thus, exporting profiles to slower devices such as a network drive may slow down the execution of the application"
  },
  {
    "id": 22512,
    "content": "Demangling  By default, nvprof demangles C++ function names Use option --demangling off to turn this feature off"
  },
  {
    "id": 22517,
    "content": "Use %p in the filename to be replaced by the process ID of nvprof , %h by the hostname , %q{ENV} by the value of environment variable ENV , and %% by %"
  },
  {
    "id": 22521,
    "content": "Dependency Analysis  nvprof can run a Dependency Analysis after the application has been profiled, using the --dependency-analysis option"
  },
  {
    "id": 22523,
    "content": "The option --print-dependency-analysis-trace can be specified to change from a summary output to a trace output, showing computed metrics such as time on the critical path per function instance rather than per function type An example for dependency analysis summary output with all computed metrics aggregated per function type is shown below The summary contains an entry named Other , referring"
  },
  {
    "id": 22526,
    "content": "== 20704 == Dependency Analysis : == 20704 == Analysis progress : 100 % Critical path ( % ) Critical path Waiting time Name % s s 92"
  },
  {
    "id": 22567,
    "content": "CPU Sampling  Sometimes it’s useful to profile the CPU portion of your application, in order to better understand the bottlenecks and identify potential hotspots for the entire CUDA application For the CPU portion of the application, nvprof is able to sample the program counter and call stacks at a certain frequency For instance, the bottom-up view (shown above) can be useful in identifying the"
  },
  {
    "id": 22568,
    "content": "“hot” functions in which the application is spending most of its time The top-down view gives a break-down of the application execution time, starting from the main function, allowing you to find “call paths” which are executed frequently Note When using the CPU profiling feature on POSIX systems, the profiler samples the application by sending periodic signals"
  },
  {
    "id": 22578,
    "content": "The result stack traces might not be complete under some compiler optimizations, notably frame pointer omission and function inlining"
  },
  {
    "id": 22582,
    "content": "OpenACC  On 64bit Linux platforms, nvprof supports recording OpenACC activities using the CUPTI Activity API This allows to investigate the performance on the level of OpenACC constructs in addition to the underlying, compiler-generated CUDA API calls OpenACC profiling in nvprof requires the targeted application to use PGI OpenACC runtime 19"
  },
  {
    "id": 22584,
    "content": "Even though recording OpenACC activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by nvprof The CUPTI OpenACC activities are mapped to the original OpenACC constructs using their source file and line information For acc_enqueue_launch activities, it will furthermore show the launched CUDA kernel name"
  },
  {
    "id": 22585,
    "content": "which is generated by the OpenACC compiler OpenACC Options  Table 1 contains OpenACC profiling related command-line options of nvprof --print-openacc-trace Print a detailed trace of all recorded OpenACC activities, including each activity’s timestamp and duration --print-openacc-constructs Include the name of the OpenACC parent construct that caused an OpenACC activity to be emitted Note that for"
  },
  {
    "id": 22593,
    "content": "OpenACC Summary Modes  nvprof supports two modes for presenting OpenACC activity durations in the OpenACC summary mode (enabled with --print-openacc-summary ): “exclusive” and “inclusive”"
  },
  {
    "id": 22594,
    "content": "This includes the time spent in this activity but excludes the runtime of all of its children (callees)"
  },
  {
    "id": 22595,
    "content": "As an example, consider the OpenACC acc_compute_construct which itself calls acc_enqueue_launch to launch a kernel to the device and acc_implicit_wait , which waits on the completion of this kernel In “inclusive” mode, the duration for acc_compute_construct will include the time spent in acc_enqueue_launch and acc_implicit_wait In the summary profile, this is helpful to identify if a long"
  },
  {
    "id": 22596,
    "content": "acc_compute_construct represents a high launch overhead or rather a long wait (synchronization) time"
  },
  {
    "id": 22599,
    "content": "OpenMP  On 64bit Linux platforms, nvprof supports recording OpenMP activities OpenMP profiling in nvprof requires the targeted application to use a runtime supporting the OpenMP Tools interface (OMPT) Even though recording OpenMP activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by nvprof An"
  },
  {
    "id": 22600,
    "content": "example for the OpenMP summary output is shown below: == 20854 == NVPROF is profiling process 20854 , command : Type Time ( % ) Time Calls Avg Min Max Name OpenMP ( incl ) : 99"
  },
  {
    "id": 22609,
    "content": "OpenMP Options  Table 2 contains OpenMP profiling related command-line options of nvprof OpenMP Options  Option Description --print-openmp-summary Print a summary of all recorded OpenMP activities"
  },
  {
    "id": 22611,
    "content": "Remote Profiling  Remote profiling is the process of collecting profile data from a remote system that is different than the host system at which that profile data will be viewed and analyzed Or you can use nvprof to collect the profile data on the remote system and then use nvvp on the host system to view and analyze the data"
  },
  {
    "id": 22614,
    "content": "Remote Profiling With Visual Profiler  This section describes how to perform remote profiling by using the remote capabilities of nsight and the Visual Profiler Nsight Eclipse Edition supports full remote development including remote building, debugging, and profiling"
  },
  {
    "id": 22615,
    "content": "Using these capabilities you can create a project and launch configuration that allows you to remotely profile your application"
  },
  {
    "id": 22616,
    "content": "As shown in the following figure, when creating a new session or editing an existing session you can specify that the application being profiled resides on a remote system Once you have configured your session to use a remote application, you can perform all profiler functions in the same way as you would with a local application, including timeline generation, guided analysis, and event and"
  },
  {
    "id": 22617,
    "content": "metric collection To use the Visual Profiler remote profiling you must install the same version of the CUDA Toolkit on both the host and remote systems It is not necessary for the host system to have an NVIDIA GPU, but ensure that the CUDA Toolkit installed on the host system supports the target device The host and remote systems may run different operating systems or have different CPU"
  },
  {
    "id": 22622,
    "content": "One-hop remote profiling  In certain remote profiling setups, the machine running the actual CUDA program is not accessible from the machine running the Visual Profiler"
  },
  {
    "id": 22623,
    "content": "These two machines are connected via an intermediate machine, which we refer to as the login node The profiling data generated will be copied over to the login node, so that it can be used by the Visual Profiler on the host To configure one-hop profiling, you need to do the following one-time setup: Copy the one-hop profiling Perl script onto the login node In Visual Profiler’s New Session"
  },
  {
    "id": 22624,
    "content": "wizard, use the Configure button to open the toolkit configuration window Here, use the radio button to select the custom script option, and browse to point to the Perl script on the login node"
  },
  {
    "id": 22629,
    "content": "Remote Profiling With nvprof  This section describes how to perform remote profiling by running nvprof manually on the remote system and then importing the collected profile data into the Visual Profiler"
  },
  {
    "id": 22633,
    "content": "Collect Data On Remote System  There are three common remote profiling use cases that can be addressed by using nvprof and the Visual Profiler Timeline The first use case is to collect a timeline of the application executing on the remote system The timeline should be collected in a way that most accurately reflects the behavior of the application You should copy this file back to the host"
  },
  {
    "id": 22634,
    "content": "system and then import it into the Visual Profiler as described in the next section Metrics And Events The second use case is to collect events or metrics for all kernels in an application for which you have already collected a timeline Collecting events or metrics for all kernels will significantly change the overall performance characteristics of the application because all kernel executions"
  },
  {
    "id": 22635,
    "content": "will be serialized on the GPU Even though overall application performance is changed, the event or metric values for individual kernels will be correct and so you can merge the collected event and metric values onto a previously collected timeline to get an accurate picture of the applications behavior prof You can collect any number of events and metrics for each nvprof invocation, and you can"
  },
  {
    "id": 22636,
    "content": "invoke nvprof multiple times to collect multiple metrics prof files You should copy these files back to the host system and then import it into the Visual Profiler as described in the next section Analysis For Individual Kernel The third common remote profiling use case is to collect the metrics needed by the analysis system for an individual kernel When imported into the Visual Profiler this data"
  },
  {
    "id": 22637,
    "content": "will enable the analysis system to analyze the kernel and report optimization opportunities for that kernel It is important that the --kernels option appear before the --analysis-metrics option so that metrics are collected only for the kernel(s) specified by kernel specifier prof The profile data will be collected in analysis prof"
  },
  {
    "id": 22641,
    "content": "View And Analyze Data  The collected profile data is viewed and analyzed by importing it into the Visual Profiler on the host system Timeline, Metrics And Events To view collected timeline data, the timeline prof file can be imported into the Visual Profiler as described in Import Single-Process nvprof Session If metric or event data was also collected for the application, the corresponding"
  },
  {
    "id": 22642,
    "content": "metrics prof file(s) can be imported into the Visual Profiler along with the timeline so that the events and metrics collected for each kernel are associated with the corresponding kernel in the timeline Guided Analysis For Individual Kernel To view collected analysis data for an individual kernel, the analysis prof file can be imported into the Visual Profiler as described in Import"
  },
  {
    "id": 22643,
    "content": "Single-Process nvprof Session The timeline will show just the individual kernel that we specified during data collection After importing, the guided analysis system can be used to explore the optimization opportunities for the kernel"
  },
  {
    "id": 22645,
    "content": "NVIDIA Tools Extension  NVIDIA Tools Extension (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications Applications which integrate NVTX can use the Visual Profiler to capture and visualize these events and ranges The sample program below shows the use of marker events, range events, and resource naming"
  },
  {
    "id": 22646,
    "content": "void Wait(int waitMilliseconds) { nvtxNameOsThread(“MAIN”); nvtxRangePush(__FUNCTION__); nvtxMark(\"Waiting \"); Sleep(waitMilliseconds); nvtxRangePop(); } int main(void) { nvtxNameOsThread(\"MAIN\"); nvtxRangePush(__FUNCTION__); Wait(); nvtxRangePop(); } 5"
  },
  {
    "id": 22648,
    "content": "NVTX API Overview  Files The core NVTX API is defined in file nvToolsExt h, whereas CUDA-specific extensions to the NVTX interface are defined in nvToolsExtCuda"
  },
  {
    "id": 22651,
    "content": "On Linux the NVTX shared library is called libnvToolsExt so and on macOS the shared library is called libnvToolsExt"
  },
  {
    "id": 22656,
    "content": "Function Calls All NVTX API functions start with an nvtx name prefix and may end with one of the three suffixes: A, W, or Ex NVTX functions with these suffixes exist in multiple variants, performing the same core functionality with different parameter encodings Depending on the version of the NVTX library, available encodings may include ASCII (A), Unicode (W), or event structure (Ex) The CUDA"
  },
  {
    "id": 22657,
    "content": "implementation of NVTX only implements the ASCII (A) and event structure (Ex) variants of the API, the Unicode (W) versions are not supported and have no effect when called"
  },
  {
    "id": 22658,
    "content": "For example, the nvtxRangeStart() function returns a unique range identifier and nvtxRangePush() function outputs the current stack level"
  },
  {
    "id": 22659,
    "content": "It is recommended not to use the returned values as part of conditional code in the instrumented application The returned values can differ between various implementations of the NVTX library and, consequently, having added dependencies on the return values might work with one tool, but may fail with another"
  },
  {
    "id": 22662,
    "content": "NVTX API Events  Markers are used to describe events that occur at a specific time during the execution of an application, while ranges detail the time span in which they occur"
  },
  {
    "id": 22663,
    "content": "This information is presented alongside all of the other captured data, which makes it easier to understand the collected information"
  },
  {
    "id": 22664,
    "content": "The Ex version of the marker and range APIs also allows category, color, and payload attributes to be associated with the event using the event attributes structure"
  },
  {
    "id": 22668,
    "content": "NVTX Markers  A marker is used to describe an instantaneous event A marker can contain a text message or specify additional information using the event attributes structure Use nvtxMarkEx() to create a marker containing additional attributes specified by the event attribute structure"
  },
  {
    "id": 22669,
    "content": "The nvtxMarkW() function is not supported in the CUDA implementation of NVTX and has no effect if called"
  },
  {
    "id": 22671,
    "content": "NVTX Range Start/Stop  A start/end range is used to denote an arbitrary, potentially non-nested, time span"
  },
  {
    "id": 22672,
    "content": "A range can contain a text message or specify additional information using the event attributes structure Use nvtxRangeStartEx() to create a range containing additional attributes specified by the event attribute structure"
  },
  {
    "id": 22673,
    "content": "The nvtxRangeStartW() function is not supported in the CUDA implementation of NVTX and has no effect if called"
  },
  {
    "id": 22674,
    "content": "For the correlation of a start/end pair, a unique correlation ID is created that is returned from nvtxRangeStartA() or nvtxRangeStartEx() , and is then passed into nvtxRangeEnd()"
  },
  {
    "id": 22675,
    "content": "Code Example non-overlapping range nvtxRangeId_t id1 = nvtxRangeStartA ( \"My range\" ); nvtxRangeEnd ( id1 ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib ascii = \"my start/stop range\" ; nvtxRangeId_t id2 = nvtxRangeStartEx ( & eventAttrib ); nvtxRangeEnd ( id2 ); overlapping ranges nvtxRangeId_t r1 = nvtxRangeStartA ( \"My range 0\" ); nvtxRangeId_t r2 = nvtxRangeStartA ( \"My range 1\" );"
  },
  {
    "id": 22679,
    "content": "Use nvtxRangePushEx() to create a range containing additional attributes specified by the event attribute structure"
  },
  {
    "id": 22680,
    "content": "The nvtxRangePushW() function is not supported in the CUDA implementation of NVTX and has no effect if called"
  },
  {
    "id": 22682,
    "content": "Code Example nvtxRangePushA ( \"outer\" ); nvtxRangePushA ( \"inner\" ); nvtxRangePop ();   end \"inner\" range nvtxRangePop ();   end \"outer\" range nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib"
  },
  {
    "id": 22683,
    "content": "Event Attributes Structure  The events attributes structure, nvtxEventAttributes_t , is used to describe the attributes of an event The layout of the structure is defined by a specific version of NVTX and can change between different versions of the Tools Extension library Attributes Markers and ranges can use attributes to provide additional information for an event or to guide the tool’s"
  },
  {
    "id": 22684,
    "content": "visualization of the data Each of the attributes is optional and if left unspecified, the attributes fall back to a default value Payload The payload attribute can be used to provide additional data for markers and ranges Initialization The caller should always perform the following three tasks when using attributes: Zero the structure Set the version field Set the size field Zeroing the structure"
  },
  {
    "id": 22685,
    "content": "sets all the event attributes types and values to the default value The version and size field are used by NVTX to handle multiple versions of the attributes structure It is recommended that the caller use the following method to initialize the event attributes structure NVTX Synchronization Markers  The NVTX synchronization module provides functions to support tracking additional synchronization"
  },
  {
    "id": 22686,
    "content": "details of the target application Naming OS synchronization primitives may allow users to better understand the data collected by traced synchronization APIs Additionally, annotating a user-defined synchronization object can allow the user to tell the tools when the user is building their own synchronization system that does not rely on the OS to provide behaviors, and instead uses techniques like"
  },
  {
    "id": 22688,
    "content": "Code Example class MyMutex { volatile long bLocked ; nvtxSyncUser_t hSync ; public : MyMutex ( const char * name , nvtxDomainHandle_t d ) { bLocked = 0 ; nvtxSyncUserAttributes_t attribs = { 0 }; attribs ascii = name ; hSync = nvtxDomainSyncUserCreate ( d , & attribs ); } ~ MyMutex () { nvtxDomainSyncUserDestroy ( hSync ); } bool Lock () { nvtxDomainSyncUserAcquireStart ( hSync ); atomic compiler"
  },
  {
    "id": 22689,
    "content": "intrinsic bool acquired = __sync_bool_compare_and_swap ( & bLocked , 0 , 1 ); if ( acquired ) { nvtxDomainSyncUserAcquireSuccess ( hSync ); } else { nvtxDomainSyncUserAcquireFailed ( hSync ); } return acquired ; } void Unlock () { nvtxDomainSyncUserReleasing ( hSync ); bLocked = false ; } }; 5"
  },
  {
    "id": 22691,
    "content": "Each domain maintains its own categories thread range stacks registered strings The function nvtxDomainDestroy() marks the end of the domain Destroying a domain unregisters and destroys all objects associated with it such as registered strings, resource objects, named categories, and started ranges"
  },
  {
    "id": 22692,
    "content": "Code Example nvtxDomainHandle_t domain = nvtxDomainCreateA ( \"Domain_A\" ); nvtxMarkA ( \"Mark_A\" ); nvtxEventAttributes_t attrib = { 0 }; attrib ascii = \"Mark A Message\" ; nvtxDomainMarkEx ( NULL , & attrib ); nvtxDomainDestroy ( domain ); 5"
  },
  {
    "id": 22694,
    "content": "NVTX Resource Naming  NVTX resource naming allows custom names to be associated with host OS threads and CUDA resources such as devices, contexts, and streams The nvtxNameOsThreadW() function is not supported in the CUDA implementation of NVTX and has no effect if called"
  },
  {
    "id": 22695,
    "content": "Windows nvtxNameOsThread ( GetCurrentThreadId (), \"MAIN_THREAD\" );   Linux/Mac nvtxNameOsThread ( pthread_self (), \"MAIN_THREAD\" ); CUDA Runtime Resources The nvtxNameCudaDeviceA() and nvtxNameCudaStreamA() functions are used to name CUDA device and stream objects, respectively"
  },
  {
    "id": 22696,
    "content": "The nvtxNameCudaDeviceW() and nvtxNameCudaStreamW() functions are not supported in the CUDA implementation of NVTX and have no effect if called"
  },
  {
    "id": 22697,
    "content": "nvtxNameCudaDeviceA ( 0 , \"my cuda device 0\" ); cudaStream_t cudastream ; cudaStreamCreate ( & cudastream ); nvtxNameCudaStreamA ( cudastream , \"my cuda stream\" ); CUDA Driver Resources The nvtxNameCuDeviceA() , nvtxNameCuContextA() and nvtxNameCuStreamA() functions are used to name CUDA driver device, context and stream objects, respectively"
  },
  {
    "id": 22698,
    "content": "The nvtxNameCuDeviceW() , nvtxNameCuContextW() and nvtxNameCuStreamW() functions are not supported in the CUDA implementation of NVTX and have no effect if called"
  },
  {
    "id": 22699,
    "content": "CUdevice device ; cuDeviceGet ( & device , 0 ); nvtxNameCuDeviceA ( device , \"my device 0\" ); CUcontext context ; cuCtxCreate ( & context , 0 , device ); nvtxNameCuContextA ( context , \"my context\" ); cuStream stream ; cuStreamCreate ( & stream , 0 ); nvtxNameCuStreamA ( stream , \"my stream\" ); 5"
  },
  {
    "id": 22701,
    "content": "NVTX String Registration  Registered strings are intended to increase performance by lowering instrumentation overhead String may be registered once and the handle may be passed in place of a string where an the APIs may allow"
  },
  {
    "id": 22702,
    "content": "The nvtxDomainRegisterStringW() function is not supported in the CUDA implementation of NVTX and has no effect if called"
  },
  {
    "id": 22703,
    "content": "nvtxDomainHandle_t domain = nvtxDomainCreateA ( \"Domain_A\" ); nvtxStringHandle_t message = nvtxDomainRegisterStringA ( domain , \"registered string\" ); nvtxEventAttributes_t eventAttrib = { 0 }; eventAttrib"
  },
  {
    "id": 22704,
    "content": "Automatic MPI Annotation with NVTX  You can annotate MPI calls with NVTX markers to profile, trace and visualize them It can get tedious to wrap every MPI call with NVTX markers, but there are two ways to do this automatically: Built-in annotation nvprof has a built-in option that supports two MPI implementations - OpenMPI and MPICH If you have either of these installed on your system, you can"
  },
  {
    "id": 22705,
    "content": "use the --annotate-mpi option and specify your installed MPI implementation If you use this option, nvprof will generate NVTX markers every time your application makes MPI calls Additionally, we use NVTX to rename the current thread and current device object to indicate the MPI rank For example if you have OpenMPI installed, you can annotate your application using the command: $ mpirun - np 2"
  },
  {
    "id": 22706,
    "content": "nvprof -- annotate - mpi openmpi / my_mpi_app This will give you output that looks something like this: NVTX result : Thread \"MPI Rank 0\" ( id = 583411584 ) Domain \"\" Range \"MPI_Reduce\" Type Time ( % ) Time Calls Avg Min Max Name Range : 100"
  },
  {
    "id": 22711,
    "content": "NVTX result : Thread \"MPI Rank 1\" ( id = 199923584 ) Domain \"\" Range \"MPI_Reduce\" Type Time ( % ) Time Calls Avg Min Max Name Range : 100"
  },
  {
    "id": 22715,
    "content": "Custom annotation If your system has a version of MPI that is not supported by nvprof, or if you want more control over which MPI functions are annotated and how the NVTX markers are generated, you can create your own annotation library, and use the environment variable LD_PRELOAD to intercept MPI calls and wrap them with NVTX markers You can create this annotation library conveniently using the"
  },
  {
    "id": 22719,
    "content": "Manual MPI Profiling  To use nvprof to collect the profiles of the individual MPI processes, you must tell nvprof to send its output to unique files However, you can now easily do it utilizing the %h , %p and %q{ENV} features of the --export-profile argument to the nvprof command / my_mpi_app Alternatively, one can make use of the new feature to turn on profiling on the nodes of interest using"
  },
  {
    "id": 22720,
    "content": "the --profile-all-processes argument to nvprof / my_mpi_app Any processes that run on the node where the --profile-all-processes is running will automatically get profiled Note that the %q{OMPI_COMM_WORLD_RANK} option will not work here, because this environment variable will not be available in the shell where nvprof is running"
  },
  {
    "id": 22722,
    "content": "5, you can name threads and CUDA contexts just as you name output files with the options –process-name and –context-name, by passing a string like \"MPI Rank %q{OMPI_COMM_WORLD_RANK}\" as a parameter"
  },
  {
    "id": 22723,
    "content": "This feature is useful to spot resources associated with a specific rank when user imports multiple files into the same time-line in the Visual Profiler"
  },
  {
    "id": 22724,
    "content": "$ mpirun - np 2 - host c0 -0 , c0 -1 nvprof -- process - name \"MPI Rank %q{OMPI_COMM_WORLD_RANK}\" -- context - name \"MPI Rank %q{OMPI_COMM_WORLD_RANK}\" - o output"
  },
  {
    "id": 22725,
    "content": "Further Reading  Details about what types of additional arguments to use with nvprof can be found in the Multiprocess Profiling and Redirecting Output section Additional information about how to view the data with the Visual Profiler can be found in the Import Single-Process nvprof Session and Import Multi-Process nvprof Session sections The blog post Profiling MPI Applications shows how to use"
  },
  {
    "id": 22726,
    "content": "new output file naming of nvprof introduced in CUDA 6 5 and NVTX library to name various resources to analyze the performance of a MPI application The blog post Track MPI Calls in the Visual Profiler shows how Visual Profiler, combined with PMPI and NVTX can give interesting insights into how the MPI calls in your application interact with the GPU"
  },
  {
    "id": 22728,
    "content": "MPS Profiling  You can collect profiling data for a CUDA application using Multi-Process Service(MPS) with nvprof and then view the timeline by importing the data in the Visual Profiler"
  },
  {
    "id": 22731,
    "content": "MPS profiling with Visual Profiler  Visual Profiler can be run on a particular MPS client or for all MPS clients Event or metric profiling results in serialization - only one MPS client will execute at a time nvidia-cuda-mps-control -d In Visual Profiler open “New Session” wizard using main menu “File->New Session” Run the application in a separate terminal To end profiling press the “Cancel”"
  },
  {
    "id": 22732,
    "content": "button on progress dialog in Visual Profiler Note that the profiling output also includes data for the CUDA MPS server processes which have process name nvidia-cuda-mps-server"
  },
  {
    "id": 22735,
    "content": "MPS profiling with nvprof  nvprof can be run on a particular MPS client or for all MPS clients nvidia-cuda-mps-control -d Run nvprof with --profile-all-processes argument and to generate separate output files for each process use the %p feature of the --export-profile argument nvprof --profile-all-processes -o output_%p Run the application in a separate terminal Exit nvprof by typing “Ctrl-c”"
  },
  {
    "id": 22738,
    "content": "Viewing nvprof MPS timeline in Visual Profiler  Import the nvprof generated data files for each process using the multi-process import option"
  },
  {
    "id": 22741,
    "content": "Dependency Analysis  The dependency analysis feature enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams"
  },
  {
    "id": 22742,
    "content": "It allows to compute the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams"
  },
  {
    "id": 22745,
    "content": "Background  The dependency analysis in nvprof and the Visual Profiler is based on execution traces of applications"
  },
  {
    "id": 22746,
    "content": "A trace captures all relevant activities such as API function calls or CUDA kernels along with their timestamps and durations Given this execution trace and a model of the dependencies between those activities on different threads/streams, a dependency graph can be constructed"
  },
  {
    "id": 22747,
    "content": "Typical dependencies modelled in this graph would be that a CUDA kernel can not start before its respective launch API call or that a blocking CUDA stream synchronization call can not return before all previously enqueued work in this stream has been completed"
  },
  {
    "id": 22748,
    "content": "A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream Given the previous stream synchronization example, the synchronizing API call is blocked for the time it has to wait on any GPU activity in the respective CUDA stream Knowledge about where wait states occur and how long functions are blocked is helpful to"
  },
  {
    "id": 22749,
    "content": "identify optimization opportunities for more high-level concurrency in the application In addition to individual wait states, the critical path through the captured event graph enables to pinpoint those function calls, kernel and memory copies that are responsible for the total application runtime The critical path is the longest path through an event graph that does not contain wait states, i"
  },
  {
    "id": 22754,
    "content": "Metrics  Waiting Time A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream In the example below, the blocking CUDA synchronization API calls are waiting on their respective kernels to finish executing on the GPU Instead of waiting immediately, one should attempt to overlap the kernel executions with concurrent"
  },
  {
    "id": 22755,
    "content": "CPU work with a similar runtime, thereby reducing the time that any computing device (CPU or GPU) is blocked Time on Critical Path The critical path is the longest path through an event graph that does not contain wait states, i"
  },
  {
    "id": 22757,
    "content": "Activities with a high time on the critical path have a high direct impact on the application runtime In the example pictured below, copy_kernel is on the critical path since the CPU is blocked waiting for it to finish in cudeDeviceSynchronize"
  },
  {
    "id": 22758,
    "content": "Reducing the kernel runtime allows the CPU to return earlier from the API call and continue program execution Since no execution stream is waiting on this kernel to finish, reducing its duration will likely not improve the overall application runtime"
  },
  {
    "id": 22761,
    "content": "Support  The following programming APIs are currently supported for dependency analysis CUDA runtime and driver API POSIX threads (Pthreads), POSIX mutexes and condition variables Dependency analysis is available in Visual Profiler and nvprof A Dependency Analysis stage can be selected in the Unguided Application Analysis and new Dependency Analysis Controls are available for the timeline See"
  },
  {
    "id": 22765,
    "content": "Limitations  The dependency and wait time analysis between different threads and CUDA streams only takes into account execution dependencies stated in the respective supported API contracts"
  },
  {
    "id": 22766,
    "content": "For example, asynchronous memory copies enqueued into independent CUDA streams will not be marked dependent even if the concrete GPU has only a single copy engine"
  },
  {
    "id": 22767,
    "content": "For example, a CPU thread actively polling for a value at some memory location (busy-waiting) will not be considered blocked on another concurrent activity"
  },
  {
    "id": 22768,
    "content": "The dependency analysis has only limited support for applications using CUDA Dynamic Parallelism (CDP) CDP kernels can use CUDA API calls from the GPU which are not tracked via the CUPTI Activity API Therefore, the analysis cannot determine the full dependencies and waiting time for CDP kernels As a result the critical path will always include the last CDP kernel of each host-launched kernel The"
  },
  {
    "id": 22769,
    "content": "dependency analysis does not support API functions cudaLaunchCooperativeKernelMultiDevice or cuLaunchCooperativeKernelMultiDevice Kernel launched by either of these API functions might not be tracked correctly"
  },
  {
    "id": 22771,
    "content": "Metrics Reference  This section contains detailed descriptions of the metrics that can be collected by nvprof and the Visual Profiler"
  },
  {
    "id": 22772,
    "content": "A scope value of “Single-context” indicates that the metric can only be accurately collected when a single context (CUDA or graphic) is executing on the GPU A scope value of “Multi-context” indicates that the metric can be accurately collected when multiple contexts are executing on the GPU A scope value of “Device” indicates that the metric will be collected at device level, that is it will"
  },
  {
    "id": 22773,
    "content": "include values for all the contexts executing on the GPU Note that, NVLink metrics collected for kernel mode exhibit the behavior of “Single-context”"
  },
  {
    "id": 22776,
    "content": "Metrics for Capability 5 x  Devices with compute capability 5 x implement the metrics shown in the following table Note that for some metrics the “Multi-context” scope is supported only for specific devices Multi-context * dram_utilization The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10 Multi-context * dram_write_bytes Total bytes written from L2"
  },
  {
    "id": 22777,
    "content": "cache to DRAM Multi-context * eligible_warps_per_cycle Average number of warps that are eligible to issue per active cycle Multi-context flop_count_dp Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate) Multi-context flop_count_dp_add Number of double-precision floating-point add operations executed by non-predicated"
  },
  {
    "id": 22778,
    "content": "threads Multi-context flop_count_dp_fma Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads Multi-context flop_count_dp_mul Number of double-precision floating-point multiply operations executed by non-predicated threads Multi-context flop_count_hp Number of half-precision floating-point operations executed by non-predicated threads (add,"
  },
  {
    "id": 22779,
    "content": "multiply and multiply-accumulate) Multi-context * flop_count_hp_add Number of half-precision floating-point add operations executed by non-predicated threads Multi-context * flop_count_hp_fma Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads Multi-context * flop_count_hp_mul Number of half-precision floating-point multiply operations executed"
  },
  {
    "id": 22780,
    "content": "by non-predicated threads Multi-context * flop_count_sp Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate) Multi-context flop_count_sp_add Number of single-precision floating-point add operations executed by non-predicated threads Multi-context flop_count_sp_fma Number of single-precision floating-point"
  },
  {
    "id": 22781,
    "content": "multiply-accumulate operations executed by non-predicated threads Multi-context flop_count_sp_mul Number of single-precision floating-point multiply operations executed by non-predicated threads Multi-context flop_count_sp_special Number of single-precision floating-point special operations executed by non-predicated threads Multi-context flop_dp_efficiency Ratio of achieved to peak"
  },
  {
    "id": 22782,
    "content": "double-precision floating-point operations Multi-context flop_hp_efficiency Ratio of achieved to peak half-precision floating-point operations Multi-context * flop_sp_efficiency Ratio of achieved to peak single-precision floating-point operations Multi-context gld_efficiency Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage"
  },
  {
    "id": 22783,
    "content": "Multi-context * gld_requested_throughput Requested global memory load throughput Multi-context gld_throughput Global memory load throughput Multi-context * gld_transactions Number of global memory load transactions Multi-context * gld_transactions_per_request Average number of global memory load transactions performed for each global memory load Multi-context * global_atomic_requests Total number"
  },
  {
    "id": 22784,
    "content": "of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global loads in unified l1/tex cache Multi-context * global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global"
  },
  {
    "id": 22785,
    "content": "store requests from Multiprocessor Multi-context gst_efficiency Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage Multi-context * gst_requested_throughput Requested global memory store throughput Multi-context gst_throughput Global memory store throughput Multi-context * gst_transactions Number of global memory store transactions"
  },
  {
    "id": 22786,
    "content": "Multi-context * gst_transactions_per_request Average number of global memory store transactions performed for each global memory store Multi-context * half_precision_fu_utilization The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions and integer instructions on a scale of 0 to 10 Multi-context * l2_local_load_bytes Bytes read from L2 for misses"
  },
  {
    "id": 22787,
    "content": "in Unified Cache for local loads Multi-context * l2_read_throughput Memory read throughput seen at L2 cache for all read requests Multi-context * l2_read_transactions Memory read transactions seen at L2 cache for all read requests Multi-context * l2_surface_atomic_store_bytes Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS) Multi-context *"
  },
  {
    "id": 22788,
    "content": "l2_surface_load_bytes Bytes read from L2 for misses in Unified Cache for surface loads Multi-context * l2_surface_reduction_bytes Bytes written to L2 from Unified Cache for surface reductions Multi-context * l2_surface_store_bytes Bytes written to L2 from Unified Cache for surface stores Multi-context * l2_tex_hit_rate Hit rate at L2 cache for all requests from texture cache Multi-context *"
  },
  {
    "id": 22789,
    "content": "l2_tex_read_hit_rate Hit rate at L2 cache for all read requests from texture cache Multi-context * l2_tex_read_throughput Memory read throughput seen at L2 cache for read requests from the texture cache Multi-context * l2_tex_read_transactions Memory read transactions seen at L2 cache for read requests from the texture cache Multi-context * l2_tex_write_hit_rate Hit Rate at L2 cache for all write"
  },
  {
    "id": 22790,
    "content": "requests from texture cache Multi-context sysmem_utilization The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10 Multi-context * sysmem_write_bytes Number of bytes written to system memory Multi-context * sysmem_write_throughput System memory write throughput Multi-context * sysmem_write_transactions Number of system memory write transactions"
  },
  {
    "id": 22791,
    "content": "Multi-context * sysmem_write_utilization The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10 Multi-context dram_utilization The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10 Multi-context dram_write_bytes Total bytes written from L2 cache to DRAM Multi-context dram_write_throughput Device memory"
  },
  {
    "id": 22792,
    "content": "write throughput Multi-context eligible_warps_per_cycle Average number of warps that are eligible to issue per active cycle Multi-context flop_count_dp Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate) Multi-context flop_count_hp Number of half-precision floating-point operations executed by non-predicated threads (add,"
  },
  {
    "id": 22793,
    "content": "multiply, and multiply-accumulate) Multi-context flop_count_hp_add Number of half-precision floating-point add operations executed by non-predicated threads Multi-context flop_count_hp_fma Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads Multi-context flop_count_hp_mul Number of half-precision floating-point multiply operations executed by"
  },
  {
    "id": 22794,
    "content": "non-predicated threads Multi-context flop_count_sp Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate) Multi-context flop_dp_efficiency Ratio of achieved to peak double-precision floating-point operations Multi-context flop_hp_efficiency Ratio of achieved to peak half-precision floating-point operations Multi-context"
  },
  {
    "id": 22795,
    "content": "flop_sp_efficiency Ratio of achieved to peak single-precision floating-point operations Multi-context gld_efficiency Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage Multi-context gld_requested_throughput Requested global memory load throughput Multi-context gld_throughput Global memory load throughput Multi-context gld_transactions"
  },
  {
    "id": 22796,
    "content": "Number of global memory load transactions Multi-context gld_transactions_per_request Average number of global memory load transactions performed for each global memory load Multi-context global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global loads in unified l1/tex cache Multi-context"
  },
  {
    "id": 22797,
    "content": "global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor Multi-context global_store_requests Total number of global store requests from Multiprocessor Multi-context l2_local_load_bytes Bytes read from L2 for misses in Unified Cache for local loads Multi-context"
  },
  {
    "id": 22798,
    "content": "l2_read_throughput Memory read throughput seen at L2 cache for all read requests Multi-context l2_read_transactions Memory read transactions seen at L2 cache for all read requests Multi-context l2_surface_atomic_store_bytes Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS) Multi-context l2_surface_load_bytes Bytes read from L2 for misses in Unified Cache for"
  },
  {
    "id": 22799,
    "content": "surface loads Multi-context l2_surface_reduction_bytes Bytes written to L2 from Unified Cache for surface reductions Multi-context l2_surface_store_bytes Bytes written to L2 from Unified Cache for surface stores Multi-context l2_tex_hit_rate Hit rate at L2 cache for all requests from texture cache Multi-context l2_tex_read_hit_rate Hit rate at L2 cache for all read requests from texture cache"
  },
  {
    "id": 22800,
    "content": "Multi-context l2_tex_read_throughput Memory read throughput seen at L2 cache for read requests from the texture cache Multi-context l2_tex_read_transactions Memory read transactions seen at L2 cache for read requests from the texture cache Multi-context l2_tex_write_hit_rate Hit Rate at L2 cache for all write requests from texture cache Device nvlink_overhead_data_transmitted Ratio of overhead"
  },
  {
    "id": 22801,
    "content": "data to the total data, transmitted through NVLink Device nvlink_total_data_received Total data bytes received through NVLinks including headers Device nvlink_total_data_transmitted Total data bytes transmitted through NVLinks including headers Device nvlink_total_nratom_data_transmitted Total non-reduction atomic data bytes transmitted through NVLinks Device nvlink_total_ratom_data_transmitted"
  },
  {
    "id": 22802,
    "content": "Total reduction atomic data bytes transmitted through NVLinks This is available for compute capability 6"
  },
  {
    "id": 22804,
    "content": "Device nvlink_total_response_data_received Total response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests Device nvlink_total_write_data_transmitted Total write data bytes transmitted through NVLinks Device nvlink_user_data_received User data bytes received through NVLinks, doesn’t include headers Device"
  },
  {
    "id": 22805,
    "content": "nvlink_user_data_transmitted User data bytes transmitted through NVLinks, doesn’t include headers Device nvlink_user_nratom_data_transmitted Total non-reduction atomic user data bytes transmitted through NVLinks Device nvlink_user_ratom_data_transmitted Total reduction atomic user data bytes transmitted through NVLinks Device nvlink_user_response_data_received Total user response data bytes"
  },
  {
    "id": 22806,
    "content": "received through NVLink, response data includes data for read requests and result of non-reduction atomic requests Device nvlink_user_write_data_transmitted User write data bytes transmitted through NVLinks"
  },
  {
    "id": 22807,
    "content": "Multi-context sysmem_write_bytes Number of bytes written to system memory Multi-context sysmem_write_throughput System memory write throughput Multi-context sysmem_write_transactions Number of system memory write transactions Multi-context sysmem_write_utilization The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10 Multi-context"
  },
  {
    "id": 22808,
    "content": "tex_cache_hit_rate Unified cache hit rate Multi-context tex_cache_throughput Unified cache throughput Multi-context tex_cache_transactions Unified cache read transactions Multi-context tex_fu_utilization The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10 Multi-context tex_utilization The utilization level of"
  },
  {
    "id": 22809,
    "content": "the unified cache relative to the peak utilization on a scale of 0 to 10 Multi-context texture_load_requests Total number of texture Load requests from Multiprocessor Multi-context unique_warps_launched Number of warps launched Multi-context warp_execution_efficiency Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor Multi-context"
  },
  {
    "id": 22810,
    "content": "warp_nonpred_execution_efficiency Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor Multi-context 9"
  },
  {
    "id": 22812,
    "content": "Metrics for Capability 7 x  Devices with compute capability 7 x implement the metrics shown in the following table"
  },
  {
    "id": 22813,
    "content": "Multi-context global_atomic_requests Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor Multi-context global_hit_rate Hit rate for global load and store in unified l1/tex cache Multi-context global_load_requests Total number of global load requests from Multiprocessor Multi-context global_reduction_requests Total number of global reduction requests from Multiprocessor"
  },
  {
    "id": 22814,
    "content": "Multi-context global_store_requests Total number of global store requests from Multiprocessor Multi-context gst_requested_throughput Requested global memory store throughput Multi-context gst_throughput Global memory store throughput Multi-context gst_transactions Number of global memory store transactions Multi-context gst_transactions_per_request Average number of global memory store"
  },
  {
    "id": 22815,
    "content": "transactions performed for each global memory store Multi-context half_precision_fu_utilization The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions on a scale of 0 to 10"
  },
  {
    "id": 22816,
    "content": "Device nvlink_total_ratom_data_transmitted Total reduction atomic data bytes transmitted through NVLinks"
  },
  {
    "id": 22817,
    "content": "The warp can have following states: Instruction issued - An instruction or a pair of independent instructions was issued from a warp"
  },
  {
    "id": 22818,
    "content": "The stall reason distribution can be seen at source level in PC Sampling View or at kernel level in Latency analysis using ‘Examine Stall Reasons’ Stalled for instruction fetch - The next instruction was not yet available"
  },
  {
    "id": 22819,
    "content": "To reduce instruction fetch stalls: If large loop have been unrolled in kernel, try reducing them If the kernel contains many calls to small function, try inlining more of them with the __inline__ or __forceinline__ qualifiers Conversely, if inlining many functions or large functions, try __noinline__ to disable inlining of those functions"
  },
  {
    "id": 22820,
    "content": "Occasional calls to __syncthreads() will then keep the warps in sync which may improve instruction cache hit rate"
  },
  {
    "id": 22821,
    "content": "Stalled for execution dependency - The next instruction is waiting for one or more of its inputs to be computed by earlier instruction(s) To reduce execution dependency stalls, try to increase instruction-level parallelism (ILP)"
  },
  {
    "id": 22822,
    "content": "This can be done by, for example, increasing loop unrolling or processing several elements per thread"
  },
  {
    "id": 22823,
    "content": "Stalled for memory dependency - The next instruction is waiting for a previous memory accesses to complete To reduce the memory dependency stalls Try to improve memory coalescing and/or efficiency of bytes fetched (alignment, etc"
  },
  {
    "id": 22825,
    "content": "Look at the source level analysis ‘Global Memory Access Pattern’ and/or the metrics gld_efficiency and gst_efficiency"
  },
  {
    "id": 22826,
    "content": "Try to increase memory-level parallelism (MLP): the number of independent memory operations in flight per thread Loop unrolling, loading vector types such as float4, and processing multiple elements per thread are all ways to increase memory-level parallelism"
  },
  {
    "id": 22827,
    "content": "Consider moving frequently-accessed data closer to SM, such as by use of shared memory or read-only data cache"
  },
  {
    "id": 22828,
    "content": "If local memory accesses are high, consider increasing register count per thread to reduce spilling, even at the expense of occupancy since local memory accesses are cached only in L2 for GPUs with compute capability major = 5"
  },
  {
    "id": 22829,
    "content": "Stalled for memory throttle - A large number of outstanding memory requests prevents forward progress On GPUs with compute capability major = 3, memory throttle indicates high number of memory replays To reduce memory throttle stalls: Try to find ways to combine several memory transactions into one (e"
  },
  {
    "id": 22832,
    "content": "Check for un-coalesced memory accesses using the source level analysis ‘Global Memory Access Pattern’ and/or the profiler metrics gld_efficiency and gst_efficiency; minimize them wherever possible On GPUs with compute capability major >= 3, consider using read-only data cache using LDG for un-coalesced global reads Stalled for texture - The texture sub-system is fully utilized or has too many"
  },
  {
    "id": 22833,
    "content": "outstanding requests To reduce texture stalls: Consider combining several texture fetch operations into one (e"
  },
  {
    "id": 22836,
    "content": "On GPUs with compute capability major = 3: If __syncthreads() is being used because of data exchange through shared memory within a threadblock, consider whether warp shuffle operations can be used in place of some of these exchange/synchronize sequences"
  },
  {
    "id": 22837,
    "content": "Stalled for constant memory dependency - The warp is stalled on a miss in the cache for __constant__ memory and immediate"
  },
  {
    "id": 22841,
    "content": "To reduce these stalls, Consider reducing use of __constant__ or increase kernel runtime by increasing block count Consider increasing number of items processed per thread Consider merging several kernels that use the same __constant__ data to amortize the cost of misses in the constant cache"
  },
  {
    "id": 22842,
    "content": "Stalled for pipe busy - The warp is stalled because the functional unit required to execute the next instruction is busy To reduce stalls due to pipe busy: Prefer high-throughput operations over low-throughput operations"
  },
  {
    "id": 22845,
    "content": ", order-of-operations changes) that may be mathematically valid but unsafe for the compiler to do automatically"
  },
  {
    "id": 22846,
    "content": "Stalled for not selected - Warp was ready but did not get a chance to issue as some other warp was selected for issue"
  },
  {
    "id": 22847,
    "content": "This reason generally indicates that kernel is possibly optimized well but in some cases, you may be able to decrease occupancy without impacting latency hiding, and doing so may help improve cache hit rates"
  },
  {
    "id": 22851,
    "content": "Migrating to Nsight Tools from Visual Profiler and nvprof  Visual Profiler and nvprof will be deprecated in a future CUDA release"
  },
  {
    "id": 22852,
    "content": "The new tools are powerful, fast, and feature rich, allowing you to find solutions even more quickly"
  },
  {
    "id": 22853,
    "content": "NVIDIA Nsight Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs; from large servers to our smallest SoC Refer to the Migrating from NVIDIA nvprof section section in the NVIDIA Nsight Systems User Guide NVIDIA Nsight"
  },
  {
    "id": 22855,
    "content": "It provides detailed performance metrics and API debugging via a user interface and command line tool Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results"
  },
  {
    "id": 22856,
    "content": "Also refer to the blog posts on how to move your development to the next-generation tools: Migrating to Nsight Tools from Visual Profiler and nvprof Transitioning to Nsight Systems from Visual Profiler and nvprof Using Nsight Compute to Inspect your Kernels Table 7"
  },
  {
    "id": 22857,
    "content": "Which tools are available on which GPU architectures  GPU architecture Visual Profiler and nvprof Nsight Systems Nsight Compute Maxwell Yes No No Pascal Yes Yes No Volta Yes Yes Yes Turing Yes* Yes Yes Ampere and later GPU architectures No Yes Yes * Only Tracing functionality is supported - Timeline, Activity, API"
  },
  {
    "id": 22858,
    "content": "The following table maps the key features of Visual Profiler and nvprof to the NVIDIA Nsight tools Table 8"
  },
  {
    "id": 22859,
    "content": "Mapping of key Visual Profiler and nvprof features  Visual Profiler/nvprof feature categories Nsight Systems Nsight Compute Timeline/Activity/API Tracing Yes CPU Sampling Yes OpenACC Yes OpenMP Yes MPI Yes MPS Yes Application Dependency Analysis Unified Memory Transfers Yes Unified Memory Page Faults Yes Application Unified Memory Analysis Application NVLink Analysis Yes (per kernel) Events and"
  },
  {
    "id": 22860,
    "content": "Metrics (per kernel) Yes Guided and Unguided Kernel Analysis Yes Kernel Source-Disassembly View Yes Kernel PC Sampling Yes NVTX Yes Yes Remote Profiling Yes Yes 12"
  },
  {
    "id": 22862,
    "content": "0, Visual Profiler and nvprof won’t support macOS as the target platform Thus it’s required to set the path to the CUPTI library before launching Visual Profiler and nvprof on Windows CUPTI library can be found at \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\\\extras\\CUPTI\\lib64\" for Windows"
  },
  {
    "id": 22863,
    "content": "A security vulnerability issue required profiling tools to disable features using GPU performance counters for non-root or non-admin users when using a Windows 419"
  },
  {
    "id": 22867,
    "content": "On other platforms, you can either start profiling as root or using sudo, or by enabling non-admin profiling"
  },
  {
    "id": 22869,
    "content": "Note Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms only, Tegra platforms require root or sudo access"
  },
  {
    "id": 22870,
    "content": "Use of the environment variable LD_PRELOAD to load some versions of MPI libraries may result in a crash on Linux platforms"
  },
  {
    "id": 22871,
    "content": "To ensure that all profile data is collected and flushed to a file, cudaDeviceSynchronize() followed by either cudaProfilerStop() or cuProfilerStop() should be called before the application exits"
  },
  {
    "id": 22872,
    "content": "Concurrent kernel mode can add significant overhead if used on kernels that execute a large number of blocks and that have short execution durations"
  },
  {
    "id": 22873,
    "content": "If the kernel launch rate is very high, the device memory used to collect profiling data can run out When profiling an application that uses CUDA Dynamic Parallelism (CDP) there are several limitations to the profiling tools CDP kernel launch tracing has a limitation for devices with compute capability 7"
  },
  {
    "id": 22875,
    "content": "Profiler traces all the host launched kernels until it encounters a host launched kernel which launches child kernels The Visual Profiler timeline does not display CUDA API calls invoked from within device-launched kernels The Visual Profiler does not display detailed event, metric, and source-level results for device-launched kernels Event, metric, and source-level results collected for"
  },
  {
    "id": 22876,
    "content": "CPU-launched kernels will include event, metric, and source-level results for the entire call-tree of kernels launched from within that kernel Events/metrics collected for CPU-launched kernels will include events/metrics for the entire call-tree of kernels launched from within that kernel"
  },
  {
    "id": 22877,
    "content": "When profiling an application in which a device kernel was stopped due to an assertion the profiling data will be incomplete and a warning or error message is displayed"
  },
  {
    "id": 22878,
    "content": "For dependency analysis, in cases where activity timestamps in the trace are slightly distorted such that they violate the programming model constraints, no dependencies or waiting times can be analyzed"
  },
  {
    "id": 22879,
    "content": "Devices with compute capability 6 0 and higher introduce a new feature, compute preemption, to give fair chance for all compute contexts while running long tasks With compute preemption feature- If multiple contexts are running in parallel it is possible that long kernels will get preempted If kernel has been preempted, the time the kernel spends preempted is still counted towards kernel duration"
  },
  {
    "id": 22880,
    "content": "This can affect the kernel optimization priorities given by Visual Profiler as there is randomness introduced due to preemption"
  },
  {
    "id": 22881,
    "content": "The following are known issues with the current release: Events and metrics collection for a MPS client can result in higher counts than expected on devices with compute capability 7 0 and higher, since MPS client may get preempted due to termination of another MPS client Events warps_launched and sm_cta_launched and metric inst_per_warp might provide higher counts than expected on devices with"
  },
  {
    "id": 22884,
    "content": "Metric unique_warps_launched can be used in place of warps_launched to get correct count of actual warps launched as it is not affected by compute preemption"
  },
  {
    "id": 22885,
    "content": "To avoid compute preemption affecting profiler results try to isolate the context being profiled: Run the application on secondary GPU where display is not connected On Linux if the application is running on the primary GPU where the display driver is connected then unload the display driver"
  },
  {
    "id": 22886,
    "content": "When the kernel is scheduled for the first time, all the pages allocated using cudaMallocManaged and that are required for execution of the kernel are fetched in the global memory when GPU faults are generated Profiler requires multiple passes to collect all the metrics required for kernel analysis"
  },
  {
    "id": 22888,
    "content": "0 and higher and platforms supporting Unified memory, in the first kernel iteration the GPU faults will be generated and all pages will be fetched in the global memory The time taken from trace will include the time required to fetch the pages but most of the metrics profiled in multiple iterations will not include time/cycles required to fetch the pages"
  },
  {
    "id": 22889,
    "content": "CUDA device enumeration and order, typically controlled through environment variables CUDA_VISIBLE_DEVICES and CUDA_DEVICE_ORDER , should remain the same for the profiler and the application"
  },
  {
    "id": 22891,
    "content": "On such systems, either set option --devices to supported devices in nvprof , or set environment variable CUDA_VISIBLE_DEVICES before launching nvprof or the Visual Profiler"
  },
  {
    "id": 22892,
    "content": "Because of the low resolution of the timer on Windows, the start and end timestamps can be same for activities having short execution duration on Windows"
  },
  {
    "id": 22893,
    "content": "As a result, the nvprof and Visual Profiler report the following warning: “Found N invalid records in the result"
  },
  {
    "id": 22894,
    "content": "” Profiler cannot interoperate with other Nvidia tools such as cuda-gdb, cuda-memcheck, Nsight Systems and Nsight Compute"
  },
  {
    "id": 22895,
    "content": "OpenACC profiling might fail when OpenACC library is linked statically in the user application This happens due to the missing definition of the OpenACC API routines needed for the OpenACC profiling, as compiler might ignore definitions for the functions not used in the application"
  },
  {
    "id": 22898,
    "content": "Refer to the webpages CUPTI 11 7 and CUPTI 11 8 for location of the CUPTI packages having the support for these Kepler devices"
  },
  {
    "id": 22899,
    "content": "Profiler is not supported on below system configurations: 64-bit ARM Server CPU architecture (arm64 SBSA)"
  },
  {
    "id": 22900,
    "content": "Visual Profiler The following are known issues related to Visual Profiler: Visual Profiler requires Java Runtime Environment (JRE) 1"
  },
  {
    "id": 22901,
    "content": "8 to be available on the local system When these analyses are attempted on a device where the metric is not available the analysis results will show that the required data is “not available”"
  },
  {
    "id": 22902,
    "content": "Using the mouse wheel button to scroll does not work within the Visual Profiler on Windows Since Visual Profiler uses nvprof for collecting profiling data, nvprof limitations also apply to Visual Profiler Visual Profiler cannot load profiler data larger than the memory size limited by JVM or available memory on the system Visual Profiler global menus do not show properly or are empty on some"
  },
  {
    "id": 22903,
    "content": "versions of Ubuntu One workaround is to set environment variable “UBUNTU_MENUPROXY=0” before running Visual Profiler In the Visual Profiler the NVLink Analysis diagram can be incorrect after scrolling the diagram Visual Profiler might not be able to show NVLink events on the timeline when large number of samples are collected For unified memory profiling on a remote setup having different version"
  },
  {
    "id": 22904,
    "content": "of GCC than host machine, Visual Profiler might not be able to show the source code location for CPU page fault events For unified memory profiling on a remote setup having different architecture than the host machine (x86 versus POWER), Visual Profiler might not be able to show the source code location for CPU page fault and allocation tracking events The workaround is to run nvprof on the target"
  },
  {
    "id": 22905,
    "content": "and load the nvprof output in the Visual Profiler For remote profiling, the CUDA Toolkit installed on the host system must support the target device on the remote system Visual Profiler might show strange symbol fonts on platforms which don’t have required fonts installed"
  },
  {
    "id": 22906,
    "content": "When using remote profiling if there is a connection failure due to key exchange failure, then you will get an error message “Unable to establish shell connection to ‘user @ xxx ’”"
  },
  {
    "id": 22907,
    "content": "Check the SSH daemon config file (default path is /etc/ssh/sshd_config) on the target Comment out lines starting with: KexAlgorithms HostbasedAcceptedKeyTypes Ciphers HostKey AuthorizedKeysFile Re-generate keys sudo ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key Restart sshd service sudo services sshd restart Accessing the local help document from Visual Profiler leads to HTTP Error 500"
  },
  {
    "id": 22908,
    "content": "nvprof The following are known issues related to nvprof : nvprof cannot profile processes that fork() but do not then exec() nvprof assumes it has access to the temporary directory on the system, which it uses to store temporary profiling data When multiple nvprof processes are run simultaneously on the same node, there is an issue of contention for files under the temporary directory Multiple"
  },
  {
    "id": 22909,
    "content": "nvprof processes running concurrently using application replay may generate incorrect results or no results at all"
  },
  {
    "id": 22910,
    "content": "To profile application on Android $TMPDIR environment variable has to be defined and point to a user-writable folder"
  },
  {
    "id": 22911,
    "content": "nvprof tries to disable auto boost by default, it might fail to do so in some conditions, but profiling will continue"
  },
  {
    "id": 22912,
    "content": "Profiling a C++ application which overloads the new operator at the global scope and uses any CUDA APIs like cudaMalloc() or cudaMallocManaged() inside the overloaded new operator will result in a hang"
  },
  {
    "id": 22913,
    "content": "NVTX annotations will not work when profiling all processes using the nvprof option --profile-all-processes"
  },
  {
    "id": 22914,
    "content": "It is advised to set the environment variable NVTX_INJECTION64_PATH to point to the profiler injection library, libcuinj64"
  },
  {
    "id": 22917,
    "content": "Events and Metrics The following are known issues related to Events and Metrics profiling: Profiling features for devices with compute capability 7"
  },
  {
    "id": 22919,
    "content": "Visual Profiler does not support Guided Analysis, some stages under Unguided Analysis and events and metrics collection for devices with compute capability 7"
  },
  {
    "id": 22923,
    "content": "Also nvprof does not support query and collection of events and metrics, source level analysis and other options used for profiling on devices with compute capability 7"
  },
  {
    "id": 22925,
    "content": "Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU"
  },
  {
    "id": 22926,
    "content": "This includes synchronization between the host and the device build upon value-based CUDA stream synchronization APIs such as cuStreamWaitValue32() and cuStreamWriteValue32()"
  },
  {
    "id": 22927,
    "content": "Event and metric collection requiring multiple passes will not work with the nvprof kernel replay option for any kernel performing IPC or data communication between the kernel and CPU, kernel and regular CPU allocated memory, kernel and Peer GPU, or kernel and other Peer devices (e"
  },
  {
    "id": 22929,
    "content": "For an application that uses multiple CUDA contexts, these metrics will only be collected for one of the contexts The metrics that can be collected only for a single CUDA context are indicated in the metric reference tables"
  },
  {
    "id": 22930,
    "content": "Some metric values are calculated assuming a kernel is large enough to occupy all device multiprocessors with approximately the same amount of work If a kernel launch does not have this characteristic, then those metric values may not be accurate"
  },
  {
    "id": 22931,
    "content": "The profilers may fail to collect events or metrics when “application replay” mode is turned on For applications that allocate large amount of device memory, the profiler may take significant time to collect all events or metrics when “kernel replay” mode is used"
  },
  {
    "id": 22933,
    "content": "Tools include Nsight Compute, Nsight Systems, Nsight Graphics, and applications that use either CUPTI or PerfKit API (NVPM) to read event values"
  },
  {
    "id": 22934,
    "content": "More than one application is using the GPU at the same time Visual Profiler is profiling a CUDA application To fix this issue please close all applications and just run the one with Visual Profiler Interacting with the active desktop should be avoided while the application is generating event information Please note that for some types of event Visual Profiler gathers events for only one context"
  },
  {
    "id": 22936,
    "content": "When collecting events or metrics with the --events , --metrics , or --analysis-metrics options, nvprof will use kernel replay to execute each kernel multiple times as needed to collect all the requested data If a large number of events or metrics are requested then a large number of replays may be required, resulting in a significant increase in application execution time To see a list of all"
  },
  {
    "id": 22937,
    "content": "available events on a particular device, type nvprof --query-events Enabling certain events can cause GPU kernels to run longer than the driver’s watchdog time-out limit In these cases the driver will terminate the GPU kernel resulting in an application error and profiling data will not be available Please disable the driver watchdog time out before profiling such long running CUDA kernels On"
  },
  {
    "id": 22939,
    "content": "For Windows, detailed information about TDR (Timeout Detection and Recovery) and how to disable it is available at https: docs"
  },
  {
    "id": 22941,
    "content": "com/en-us/windows-hardware/drivers/display/timeout-detection-and-recovery nvprof can give out of memory error for event and metrics profiling, it could be due to large number of instructions in the kernel"
  },
  {
    "id": 22946,
    "content": "Profiling is not supported for multidevice cooperative kernels, that is, kernels launched by using the API functions cudaLaunchCooperativeKernelMultiDevice or cuLaunchCooperativeKernelMultiDevice Profiling is not supported for CUDA kernel nodes launched by a CUDA Graph"
  },
  {
    "id": 22972,
    "content": "Visual Profiler extends remote profiling support to macOS host running version 11 (Big Sur) on Intel x86_64 architecture Profiler changes in CUDA 11 2 List of changes done as part of the CUDA Toolkit 11"
  },
  {
    "id": 22979,
    "content": "0, Visual Profiler and nvprof won’t support Mac as the target platform Visual Profiler will be provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on Mac Profiler changes in CUDA 10 2 List of changes done as part of the CUDA Toolkit 10"
  },
  {
    "id": 22981,
    "content": "Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms Note that events and metrics profiling is still restricted for non-root and non-admin users Thus it’s required to set the path to the CUPTI library before launching Visual Profiler and nvprof CUPTI library can be found at /usr/local extras/CUPTI/lib64 or /usr/local targets lib for POSIX"
  },
  {
    "id": 22983,
    "content": "Profilers no longer turn off the performance characteristics of CUDA Graph when tracing the application"
  },
  {
    "id": 22984,
    "content": "Profiler changes in CUDA 10 1 Update 2 List of changes done as part of the CUDA Toolkit 10 1 Update 2 release"
  },
  {
    "id": 22985,
    "content": "A security vulnerability issue required profiling tools to disable all the features for non-root or non-admin users"
  },
  {
    "id": 22999,
    "content": "The Visual Profiler allows to switch multiple segments to non-segment mode for Unified Memory profiling on the timeline The Visual Profiler shows a summary view of the memory hierarchy of the CUDA programming model The Visual Profiler can correctly import profiler data generated by nvprof when the option --kernels kernel-filter is used"
  },
  {
    "id": 23000,
    "content": "nvprof supports display of basic PCIe topolgy including PCI bridges between NVIDIA GPUs and Host Bridge"
  },
  {
    "id": 23001,
    "content": "To view and analyze bandwidth of memory transfers over PCIe topologies, new set of metrics to collect total data bytes transmitted and recieved through PCIe are added"
  },
  {
    "id": 23002,
    "content": "The Visual Profiler and nvprof added support for new metrics: Instruction executed for different types of load and store Total number of cached global/local load requests from SM to texture cache Global atomic/non-atomic/reduction bytes written to L2 cache from texture cache Surface atomic/non-atomic/reduction bytes written to L2 cache from texture cache Hit rate at L2 cache for all requests from"
  },
  {
    "id": 23003,
    "content": "texture cache Device memory (DRAM) read and write bytes The utilization level of the multiprocessor function units that execute tensor core instructions for devices with compute capability 7"
  },
  {
    "id": 23004,
    "content": "0 nvprof allows to collect tracing infromation along with the profiling information in the same pass"
  },
  {
    "id": 23007,
    "content": "The Visual Profiler shows the breakdown of the time spent on the CPU for each thread in the CPU Details View Profiler changes in CUDA 9 0 List of changes done as part of the CUDA Toolkit 9"
  },
  {
    "id": 23010,
    "content": "com/NVIDIA/cuda-profiler There are several enhancements to Unified Memory profiling: The Visual Profiler now associates unified memory events with the source code at which the memory is allocated The Visual Profiler now correlates a CPU page fault to the source code resulting in the page fault New Unified Memory profiling events for page thrashing, throttling and remote map are added The Visual"
  },
  {
    "id": 23011,
    "content": "Profiler provides an option to switch between segment and non-segment mode on the timeline The Visual Profiler supports filtering of Unified Memory profiling events based on the virtual address, migration reason or the page fault access type The Visual Profiler supports new options to make it easier to do multi-hop remote profiling The Visual Profiler supports remote profiling to systems"
  },
  {
    "id": 23012,
    "content": "supporting ssh key exchange algorithms with a key length of 2048 bits Profiler changes in CUDA 8 0 List of changes done as part of the CUDA Toolkit 8"
  },
  {
    "id": 23016,
    "content": "Visual Profiler and nvprof now support dependency analysis which enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams"
  },
  {
    "id": 23017,
    "content": "It allows computing the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams"
  },
  {
    "id": 23018,
    "content": "Unified Memory profiling now provides GPU page fault information on devices with compute capability 6"
  },
  {
    "id": 23019,
    "content": "0 and 64 bit Linux platforms Unified Memory profiling now provides CPU page fault information on 64 bit Linux platforms"
  },
  {
    "id": 23020,
    "content": "There is now a single integrated view for the different source level analysis results collected for a kernel instance"
  },
  {
    "id": 23021,
    "content": "The PC sampling feature is enhanced to point out the true latency issues for devices with compute capability 6"
  },
  {
    "id": 23023,
    "content": "If the new NVIDIA Tools Extension API(NVTX) feature of domains is used then Visual Profiler and nvprof will show the NVTX markers and ranges grouped by domain The Visual Profiler now adds a default file extension nvvp if an extension is not specified when saving or opening a session file The Visual Profiler now supports timeline filtering options in create new session and import dialogs Profiler"
  },
  {
    "id": 23026,
    "content": "Visual Profiler now supports profiling child processes and profiling all processes launched on the same system For profiling CUDA applications using Multi-Process Service(MPS) see MPS profiling with Visual Profiler Visual Profiler import now supports browsing and selecting files on a remote system"
  },
  {
    "id": 23029,
    "content": "Profiler changes in CUDA 7 0 The profiling tools contain a number of changes and new features as part of the CUDA Toolkit 7"
  },
  {
    "id": 23031,
    "content": "The Visual Profiler has been updated with several enhancements: Performance is improved when loading large data file"
  },
  {
    "id": 23032,
    "content": "Unified memory profiling is enhanced by providing fine grain data transfers to and from the GPU, coupled with more accurate timestamps with each transfer"
  },
  {
    "id": 23033,
    "content": "nvprof has been updated with several enhancements: All events and metrics for devices with compute capability 3"
  },
  {
    "id": 23038,
    "content": "The Visual Profiler kernel memory analysis has been updated with several enhancements: ECC overhead is added which provides a count of memory transactions required for ECC Under L2 cache a split up of transactions for L1 Reads, L1 Writes, Texture Reads, Atomic and Noncoherent reads is shown Under L1 cache a count of Atomic transactions is shown The Visual Profiler kernel profile analysis view has"
  },
  {
    "id": 23039,
    "content": "been updated with several enhancements: Initially the instruction with maximum execution count is highlighted A bar is shown in the background of the counter value for the “Exec Count” column to make it easier to identify instruction with high execution counts The current assembly instruction block is highlighted using two horizontal lines around the block"
  },
  {
    "id": 23040,
    "content": "Also “next” and “previous” buttons are added to move to the next or previous block of assembly instructions"
  },
  {
    "id": 23041,
    "content": "nvprof now supports a new application replay mode for collecting multiple events and metrics This is useful for cases when the kernel uses a large amount of device memory and use of kernel replay can be slow due to a high overhead of saving and restoring device memory for each kernel replay run Visual Profiler also supports this new application replay mode and it can enabled in the Visual"
  },
  {
    "id": 23042,
    "content": "Profiler “New Session” dialog Visual Profiler now displays peak single precision flops and double precision flops for a GPU under device properties"
  },
  {
    "id": 23043,
    "content": "Improved source-to-assembly code correlation for CUDA Fortran applications compiled by the PGI CUDA Fortran compiler"
  },
  {
    "id": 23046,
    "content": "Both profilers allow you to see the Unified Memory related memory traffic to and from each GPU on your system You can import multiple timeline data sets collected with nvprof into nvvp and view them on the same timeline to see how they are sharing the GPU(s)"
  },
  {
    "id": 23048,
    "content": "The Visual Profiler now supports a remote profiling mode that allows you to collect a profile on a remote Linux system and view the timeline, analysis results, and detailed results on your local Linux, Mac, or Windows system The Visual Profiler analysis system now includes a side-by-side source and disassembly view annotated with instruction execution counts, inactive thread counts, and"
  },
  {
    "id": 23050,
    "content": "This new view enables you to find hotspots and inefficient code sequences within your kernels The Visual Profiler analysis system has been updated with several new analysis passes: 1) kernel instructions are categorized into classes so that you can see if instruction mix matches your expectations, 2) inefficient shared memory access patterns are detected and reported, and 3) per-SM activity level"
  },
  {
    "id": 23051,
    "content": "is presented to help you detect detect load-balancing issues across the blocks of your kernel The report is a PDF version of the per-kernel information presented by the guided analysis system You can import profile data collected from another system and view and analyze it on your GPU-less system"
  },
  {
    "id": 23055,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 23056,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 23058,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 23059,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 23060,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 23061,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 23062,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 23063,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 23064,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 23065,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 23066,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 23067,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 23068,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 23075,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 23077,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 23078,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 23082,
    "content": "pageBottom();}\nIntroduction Release Notes Installation and Setup CUDA Debugger Getting Started with the CUDA Debugger Build and Run Control GPU Execution Inspect State Advanced Topics Reference Reference Release Information Archives Copyright and License Notices EULA nsight-visual-studio-edition » NVIDIA Nsight Visual Studio Edition v2024"
  },
  {
    "id": 23084,
    "content": "1 | Archive NVIDIA Nsight Visual Studio Edition  Introduction NVIDIA® Nsight™ Visual Studio Edition is an application development environment which brings GPU computing into Microsoft Visual Studio"
  },
  {
    "id": 23085,
    "content": "allows you to build and debug integrated GPU kernels and native CPU code as well as inspect the state of the CPU, GPU, and memory"
  },
  {
    "id": 23086,
    "content": "Release Notes See the latest features and updates for this version of NVIDIA Nsight Visual Studio Edition Installation and Setup This chapter walks you through the system requirements for NVIDIA Nsight Visual Studio Edition, and the steps you’ll need to install and get started using the software CUDA Debugger  Getting Started with the CUDA Debugger This section provides a walkthrough and"
  },
  {
    "id": 23087,
    "content": "tutorial for using the CUDA Debugger with NVIDIA Nsight Visual Studio Edition Build and Run This section details how to configure the properties of a CUDA project, launching the CUDA Debugger, and how to attach debugging to a running CUDA Process Control GPU Execution In this section, learn more about how to control GPU execution, set GPU breakpoints, and use global freeze Inspect State In this"
  },
  {
    "id": 23088,
    "content": "section, learn more about how to use various state inspection features of the CUDA Debugger, such as specifying the debugger context, viewing memory and variables, using the CUDA Info View, and using the CUDA Warp Watch Advanced Topics In this section, learn more about advanced CUDA topics, such as PTX and SASS assembly debugging, as well as how to use the CUDA Memory Checker Reference  Reference"
  },
  {
    "id": 23089,
    "content": "Additional resources for learning more about working with NVIDIA Nsight Visual Studio Edition Release Information  Archives Find documentation for previous versions of NVIDIA Nsight Visual Studio Edition Copyright And License Notices  EULA This document is the End User License Agreement (EULA) for NVIDIA Nsight Visual Studio Edition This document contains specific license terms and conditions"
  },
  {
    "id": 23090,
    "content": "for NVIDIA Nsight Visual Studio Edition By accepting this agreement, you agree to comply with all the terms and conditions applicable to the specific product(s) included herein NvRules API Training Training Release Information Archives Copyright and Licenses Copyright and Licenses NsightCompute » Nsight Compute Documentation v2024"
  },
  {
    "id": 23092,
    "content": "1 | Archive Nsight Compute Documentation  Nsight Compute  Release Notes Release notes, including new features and important bug fixes"
  },
  {
    "id": 23093,
    "content": "Kernel Profiling Guide Kernel Profiling Guide with metric types and meaning, data collection modes and FAQ for common problems Information on workflows and options for the command line, including multi-process profiling and NVTX filtering"
  },
  {
    "id": 23094,
    "content": "Developer Interfaces  Customization Guide User manual on customizing NVIDIA Nsight Compute tools or integrating them with custom workflows"
  },
  {
    "id": 23095,
    "content": "Information on writing section files, rules for automatic result analysis and scripting access to report files"
  },
  {
    "id": 23096,
    "content": "Release Information  Archives Find documentation for previous versions of NVIDIA Nsight Compute Copyright And Licenses  Copyright and Licenses Information on the NVIDIA Software License Agreement as well as third party software and tools used by Nsight Compute"
  },
  {
    "id": 23099,
    "content": "enable(true); }); Release Notes Installation Guide User Guide Copyright and Licenses Archives Archives nsight-systems » Nsight Systems v2024 4 | Archive Nsight Systems  Release Notes Release notes and known issues Copyright and Licenses Information on the NVIDIA Software License Agreement as well as third party software and tools used by Nsight Systems Developer Interfaces  Archives"
  },
  {
    "id": 23102,
    "content": "5 | PDF | Archive Nsight Eclipse Plugins Edition Getting Started Guide The user guide for using Nsight Eclipse Plugins Edition Introduction  This guide introduces Nsight Eclipse Plugins Edition and provides instructions necessary to start using this tool For a detailed description of Eclipse CDT features consult the integrated help “C/C++ Development User Guide” available from inside Nsight"
  },
  {
    "id": 23106,
    "content": "About Nsight Eclipse Plugins Edition  NVIDIA ® Nsight™ Eclipse Edition is a unified CPU plus GPU integrated development environment (IDE) for developing CUDA ® applications on Linux and Mac OS X for the x86, POWER and ARM platforms Nsight Eclipse Plugins can be installed on vanilla Eclipse using the standard Help->Install New Software The principal features are as follows: Edit, build, debug and"
  },
  {
    "id": 23107,
    "content": "profile CUDA-C applications CUDA aware source code editor – syntax highlighting, code completion and inline help Graphical user interface for debugging heterogeneous applications Profiler integration – Launch visual profiler as an external application with the CUDA application built in this IDE to easily identify performance bottlenecks For more information about Eclipse Platform, visit http:"
  },
  {
    "id": 23110,
    "content": "Installing Nsight Eclipse Edition  Nsight Eclipse Plugins archive is part of the CUDA Toolkit Nsight Eclipse Plugins archive can be installed using the Help -> Install New Software… Menu on Eclipse 2"
  },
  {
    "id": 23113,
    "content": "Installing CUDA Toolkit  To install CUDA Toolkit: Visit the NVIDIA CUDA Toolkit download page: https: developer"
  },
  {
    "id": 23120,
    "content": "Configure CUDA Toolkit Path  When Eclipse is first launched with Nsight Eclipse plugins in the new workspace, NVIDIA usage data collection dialog will be displayed as below Usage data collection page  To get started, CUDA Toolkit path must be configured in Eclipse with Nsight Plugins: Open the Preferences page, Window > Preferences CUDA toolkit path can be also specified in the project"
  },
  {
    "id": 23121,
    "content": "properties page in order to use different toolkit for a project For QNX: When QNX is selected as Target OS, a dialog will be displayed to set the QNX_HOST and QNX_TARGET environment variables if they were not already set QNX_HOST environment variable identifies the directory that holds the host-related components: QNX_TARGET environment variable identifies the directory that holds the"
  },
  {
    "id": 23124,
    "content": "Nsight Eclipse Main Window  On the first run Eclipse will ask to pick a workspace location The workspace is a folder where Nsight will store its settings, local files history and caches"
  },
  {
    "id": 23125,
    "content": "The main window is divided into the following areas: Editor - displays source files that are opened for editing Project Explorer - displays project files Outline - displays structure of the source file in the current editor Problems - displays errors and warnings detected by static code analysis in IDE or by a compiler during the build Console - displays make output during the build or output"
  },
  {
    "id": 23129,
    "content": "Creating a New Project  From the main menu, open the new project wizard - File > New… > CUDA C/C++ Project Specify the project name and project files location"
  },
  {
    "id": 23130,
    "content": "Importing CUDA Samples  The CUDA samples are an optional component of the CUDA Toolkit installation Nsight provides a mechanism to import these samples and work with them easily: Note Samples that use the CUDA driver API (suffixed with “Drv”) are not supported by Nsight"
  },
  {
    "id": 23131,
    "content": "From the main menu, open the new project wizard - File > New… > CUDA C/C++ Project Specify the project name and project files location Press Next… Specify the project parameters on the next wizard page"
  },
  {
    "id": 23136,
    "content": "From the main menu, open the new project wizard - File > New… > CUDA C/C++ Project Select project type “Makefile project” and choose “Empty Project” Specify the project name and project files location Right click on the project - Import… > General > File System On the next wizard page, select the location of cuHook sample(Samples/7_CUDALibraries/cuHook) Select all the source files and makefile"
  },
  {
    "id": 23137,
    "content": "and Finish the wizard Build the project by clicking on the hammer button on the main toolbar To run the sample, from the main menu - Run > Run Configurations… > Select the executable > Go to Environment tab > New… > enter Name=LD_PRELOAD, Value="
  },
  {
    "id": 23142,
    "content": "Configure Build Settings  To define build settings: In the C/C++ Projects view, right-click your project, and select Properties The following are the categories of Nvcc linker settings that can be configured for the selected project Note All options field in the main page is not editable and it’s the collection of options set in the child categories"
  },
  {
    "id": 23143,
    "content": "When you are cross compiling for different target os, the library search path should point to the appropriate location where the target os libraries are present"
  },
  {
    "id": 23144,
    "content": "The following are the categories of Nvcc Compiler settings that can be configured for the selected project Additionally, set the number of threads that the compiler will use during the compiliation process (“split compilation”)"
  },
  {
    "id": 23145,
    "content": "CUDA - Generate code for different real architectures with the PTX for the same vitrual architectures"
  },
  {
    "id": 23148,
    "content": "Debugging CUDA Applications  Nsight must be running and at least one project must exist Make sure the project executable is compiled and no error markers are shown on the project"
  },
  {
    "id": 23150,
    "content": "Debugging CUDA application  Additional debugger options can be set in the debug configuration dialog through Run > Debug Configurations"
  },
  {
    "id": 23151,
    "content": "Remote development of CUDA Applications  Nsight Eclipse Edition also supports remote development of CUDA application starting with CUDA Toolkit 6"
  },
  {
    "id": 23153,
    "content": "The picture below shows how Nsight Eclipse Edition can be used for local as well as remote development: For remote development you do not need any NVIDIA GPU on your host system The remote target system can be a Linux x86 or POWER system with an NVIDIA GPU or an Tegra-based ARM system In the cross compilation mode the project resides on the host system and the cross compilation is also done on"
  },
  {
    "id": 23154,
    "content": "the host system To cross compile select the target cross compile architecture in CPU architecture drop down in the project properties page: 2"
  },
  {
    "id": 23159,
    "content": "The remote machine must be accessible via SSH and CUDA Toolkit must be installed on both machines Note If there is a firewall between the host and the target, it must be set up to let RSP messages through, or SSH port-forwarding must be used"
  },
  {
    "id": 23161,
    "content": "Select a remote connection from a drop-down list or press the Add connection… button to create a new one If you are creating a new remote connection, enter the host name(or IP address) as well as the user name For Android devices: To configure the remote connection using Android debug bridge, select the Android debug bridge from the Remote Connection drop-down list, Android device must be"
  },
  {
    "id": 23162,
    "content": "connected to the host system using USB port Type the full path to cuda-gdbserver on the remote system or select one using the Browse… button Click on “Add new path” or on the Browse… button to specify the path to the shared libraries the remote application depends on Click on the Finish button to finish the new debug configuration wizard and start debugging the application Improving Remote"
  },
  {
    "id": 23163,
    "content": "Debugging Performance  When doing remote debugging, it can be useful to host copies of the target libraries in a local sysroot"
  },
  {
    "id": 23166,
    "content": "You can modify ‘CUDA GDB sysroot (for remote debugging)’ to point to a local sysroot directory to improve debugging performance"
  },
  {
    "id": 23169,
    "content": "Profiling CUDA applications  Nsight must be running and at least one project must exist Nsight Eclipse Edition profiling features are based on the NVIDIA Visual Profiler ( nvvp ) code Nsight Eclipse Plugins Edition will launch the Visual Profiler as an external tool with the executable and other information from the selected project Nsight Eclipse will launch the Visual Profiler to specify extra"
  },
  {
    "id": 23170,
    "content": "profiler options with the executable information already passed from the selected project Build CUDA Projects inside a Docker Container  You can build and debug C/C++ and CUDA projects in a Docker container using Nsight Eclipse Edition To get started, you need to first pull and install the Docker image that encapsulates the CUDA toolkit and cross platform tool chains You can get the Docker images"
  },
  {
    "id": 23171,
    "content": "from NVIDIA GPU Cloud Then you can use Nsight Eclipse Edition to build CUDA projects in a Docker container Open the Preferences page, Window > Preferences and go to: CUDA > Container Settings Select the option if you want to build the projects inside the Docker container Make sure the CUDA toolkit path that is specified in the CUDA preferences is the path of the CUDA toolkit inside a Docker"
  },
  {
    "id": 23172,
    "content": "container Select the Connection and the Image dropdown will display all the Docker images that are currently installed The preferences that are set here will be automatically displayed in the project setup wizard You can choose to modify the container settings for the individual projects from the project setup wizard To create a project, From the main menu, open the new project wizard: File > New…"
  },
  {
    "id": 23173,
    "content": "> CUDA C/C++ Project Specify the project name and project files location If you need to mount any other directories that contains the include files/libraries and etc to the docker container, you can mount those directories from the project property page The project is now built in the chosen Docker container the executable will be available on the host"
  },
  {
    "id": 23176,
    "content": "Remote debugging using CUDA GDB inside Docker container  From Nsight Eclipse, you can remote debug the applications running on the target using the CUDA GDB inside the Docker container running on the host The remote machine must be accessible via SSH and CUDA Toolkit must be installed on target machine Create a new debug configuration under CUDA GDB Container Launcher either double clicking or"
  },
  {
    "id": 23177,
    "content": "using right click menu If you are creating a new remote connection, click on the manage button in Remote Connection enter the host name(or IP address) as well as the user name Also select the CUDA toolkit location on the target and choose the location to where to upload the executable From the “Container” tab, select the connection and Docker image that contains the CUDA GDB Click on the Apply"
  },
  {
    "id": 23178,
    "content": "button to save the changes and click on the Debug button to start the debug session This action will upload the local executable to the target system and will start CUDA GDB Server on the target And the Docker container will be started on the host and CUDA GDB running inside the docker container will establish the remote debug session with the target Importing Nsight Eclipse Projects  The"
  },
  {
    "id": 23179,
    "content": "projects that are created with Nsight Eclipse Edition can be imported into the Eclipse workbench with Nsight Eclipse plugins Right click on the Nsight Eclipse project and go to - Export > C/C++ > C/C++ Project Settings > Next menu Create a CUDA C/C++ Project from the main menu File > New > CUDA C/C++ Project Specify the project name and choose Empty project type with CUDA toolchains Import >"
  },
  {
    "id": 23180,
    "content": "General > File System >(From directory) or copy the source files from the existing project Import the project settings like include paths and symbols using the following right click menu Import > C/C++ > C/C++ Project Settings >Next… Select the location of the project settigns file and select the project and configuration on the next wizard page The project settings will be imported from the file"
  },
  {
    "id": 23184,
    "content": "Enabling Dark Theme in Eclipse  To work comfortably in dark environments, Eclipse offers a dark theme mode that’s easy to activate: Open Eclipse Preferences: Click on “Windows” in the top menu"
  },
  {
    "id": 23185,
    "content": "More Information  More information about the Eclipse CDT features and other topics is available in the Help contents More information about CUDA, CUDA Toolkit and other tools is available on CUDA web page at http: developer"
  },
  {
    "id": 23188,
    "content": "Known Issues  Executable must exist in order to start debug session for the first time Nsight will not automatically perform build when starting debug session for a given project for the first time Note To manually build the project, select it (or any file within the project) in a Project Explorer view and click hammer icon on the main window toolbar"
  },
  {
    "id": 23190,
    "content": "Mac OS X users may be prompted to install Java Runtime Environment (JRE) when running Nsight Eclipse Edition for the first time Nsight Eclipse Plugin Edition requires functioning Java Runtime Environment to be present on the local system to run Nsight Eclipse Plugin Edition does not provide compilation support for using the QNX qcc and q++ compilers The workaround to compile using qcc and q++ is"
  },
  {
    "id": 23191,
    "content": "Specify the q++ path in CCBIN field on toolkit configuration page on project properties dialog as shown below You can access toolkit configuration page by clicking main menu Project > Properties > C/C++ Build > CUDA Toolkit Change default CONF to gcc_ntoaarch64le in the file ${QNX_HOST}/etc/qcc/gcc/5"
  },
  {
    "id": 23194,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 23195,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 23197,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 23198,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 23199,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 23200,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 23201,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 23202,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 23203,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 23204,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 23205,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 23206,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 23207,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 23214,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 23216,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 23217,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 23223,
    "content": "5 | PDF | Archive Nsight Eclipse Plugins Installation Guide The user guide for installing Nsight Eclipse Plugins Introduction  This guide provides the procedures to install the Nsight Eclipse Edition Plugins in users own eclipse environment Nsight Eclipse Plugins offers full-featured IDE that provides an all-in-one integrated environment to edit, build, debug and profile CUDA-C applications"
  },
  {
    "id": 23226,
    "content": "Install plugins using Eclipse IDE  You can install Nsight Eclipse plugins in your own Eclipse environment or download and install Eclipse IDE for C/C++ developers"
  },
  {
    "id": 23231,
    "content": "zip) that contains the plugins using Archive button or Enter the full path of zip file Nsight EE plugns zip file can be found in /usr/local/cuda-11"
  },
  {
    "id": 23233,
    "content": "Click OK on the “Security Warning” dialog to ignore the warning message about unsigned content (This warning message is displayed for all the plugins that are not signed by Eclipse"
  },
  {
    "id": 23237,
    "content": "Uninstall plugins using Eclipse IDE  Launch Eclipse and go to Help > Installation Details menu Select “Cuda Developer Tools” and “Cuda Remote Launch” options from the dialog Click on the Uninstall button menu to verify"
  },
  {
    "id": 23240,
    "content": "Install Using Script  To install or uninstall the Nsight Eclipse Plugins using the script, run the installation script provided in the bin directory of the toolkit"
  },
  {
    "id": 23244,
    "content": "sh : 'install' or 'uninstall' : eclipse installation directory To install the Nsight Eclipse Plugins, run the following command: $ /usr/local/cuda-11"
  },
  {
    "id": 23249,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 23250,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 23252,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 23253,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 23254,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 23255,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 23256,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 23257,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 23258,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 23259,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 23260,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 23261,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 23262,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 23269,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 23271,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 23272,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2017-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 23276,
    "content": "pageBottom();} NVIDIA Compute Sanitizer Documentation Search In: Entire Site Just This Document clear search search Compute Sanitizer Release Notes Tools Compute Sanitizer User Manual Developer Interfaces Compute Sanitizer API Reference Manual Sanitizer Api NVTX API for Compute Sanitizer Reference Manual Copyright And Licenses Copyright and Licenses Search Results Compute Sanitizer Release Notes"
  },
  {
    "id": 23277,
    "content": "Release notes and known issues Tools Compute Sanitizer User Manual Developer Interfaces Compute Sanitizer API Reference Manual Sanitizer API The NVIDIA Compute Sanitizer API NVTX API for Compute Sanitizer Reference Manual Copyright And Licenses Copyright and Licenses Information on the NVIDIA Software License Agreement as well as third party software and tools used by Compute Sanitizer"
  },
  {
    "id": 23278,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2019-2024 NVIDIA Corporation _satellite"
  },
  {
    "id": 23280,
    "content": "options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false});\n1"
  },
  {
    "id": 23282,
    "content": "5 | PDF | Archive CUDA-GDB The user manual for CUDA-GDB, the NVIDIA tool for debugging CUDA applications on Linux and QNX systems Introduction  This document introduces CUDA-GDB, the NVIDIA ® CUDA ® debugger for Linux and QNX targets"
  },
  {
    "id": 23285,
    "content": " CUDA-GDB is the NVIDIA tool for debugging CUDA applications running on Linux and QNX The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware"
  },
  {
    "id": 23286,
    "content": "This enables developers to debug applications without the potential variations introduced by simulation and emulation environments"
  },
  {
    "id": 23287,
    "content": "Supported Features  CUDA-GDB is designed to present the user with a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application Just as programming in CUDA C is an extension to C programming, debugging with CUDA-GDB is a natural extension to debugging with GDB The existing GDB debugging features are inherently present for debugging the"
  },
  {
    "id": 23288,
    "content": "host code, and additional features have been provided to support debugging CUDA device code CUDA-GDB allows the user to set breakpoints, to single-step CUDA applications, and also to inspect and modify the memory and variables of any given thread running on the hardware CUDA-GDB supports debugging all CUDA applications, whether they use the CUDA driver API, the CUDA runtime API, or both CUDA-GDB"
  },
  {
    "id": 23289,
    "content": "supports debugging kernels that have been compiled for specific CUDA architectures, such as sm_75 or sm_80 , but also supports debugging kernels compiled at runtime, referred to as just-in-time compilation, or JIT compilation for short"
  },
  {
    "id": 23292,
    "content": "About This Document  This document is the main documentation for CUDA-GDB and is organized more as a user manual than a reference manual The rest of the document will describe how to install and use CUDA-GDB to debug CUDA kernels and how to use the new CUDA commands that have been added to GDB It is assumed that the user already knows the basic GDB commands used to debug host applications"
  },
  {
    "id": 23297,
    "content": "See GDB 13 2 changes Support removal notice Support for the macOS host client of CUDA-GDB has been removed"
  },
  {
    "id": 23298,
    "content": "Features Multi build feature that supports native Python and TUI mode across all supported platforms If no supported Python or libncurses is detected, the wrapper will fallback to a cuda-gdb binary with Python and TUI support disabled"
  },
  {
    "id": 23300,
    "content": "Fixed issue where break_on_launch breakpoints were missed for back to back launches of the same kernel"
  },
  {
    "id": 23301,
    "content": "Fixed issue with incorrectly reporting breakpoint hit events as SIGTRAP when breakpoint is hit in divergent thread"
  },
  {
    "id": 23307,
    "content": "Features Performance enhancement which reduces the number of overall CUDA Debugger API calls Performance enhancement when loading large cubins with device functions using a large number of GPU registers"
  },
  {
    "id": 23309,
    "content": "3 Release macOS host client deprecation notice Support for the macOS host client of CUDA-GDB is deprecated"
  },
  {
    "id": 23310,
    "content": "New $_cuda_const_bank(bank, offset) convenience function to obtain address of offset in constant bank"
  },
  {
    "id": 23311,
    "content": "Performance enhancements added which reduce overhead when running applications with many CUDA threads"
  },
  {
    "id": 23312,
    "content": "Added support for opening of GPU core dumps when no valid warps are present on the device Fixed issue where CUDA Cluster coordinates were being displayed when no CUDA Cluster was present"
  },
  {
    "id": 23314,
    "content": "2 Release Features Enabled printing of extended error messages when a CUDA Debugger API error is encountered"
  },
  {
    "id": 23315,
    "content": "Fixed issue with attaching to an application using CUDA Lazy Loading when debugging remotely with cuda-gdbserver"
  },
  {
    "id": 23316,
    "content": "12 1 Release CUDA Driver API added for controlling core dump behavior CTK 12 1 and the r530 driver adds new APIs that allow developers to enable/configure core dump settings programmatically inside their application instead of using environment variables See GDB 12 1 changes Texture and surface reference support removed CTK 12 0 removed support for the Texture and Surface Reference API"
  },
  {
    "id": 23319,
    "content": "This will support coredumps when issues are detected which can then be opened and inspected with CUDA-GDB, similar to other coredumps"
  },
  {
    "id": 23320,
    "content": "Debugging of applications using CUDA Dynamic Parallelism Support for debugging applications using CUDA Dynamic Parallelism with the classic debugger backend or on Maxwell GPUs has been removed by default for applications compiled with the CTK 12"
  },
  {
    "id": 23322,
    "content": "Debugging can be accomplished in these situations by recompiling the application while passing the -DCUDA_FORCE_CDP1_IF_SUPPORTED flag"
  },
  {
    "id": 23324,
    "content": "Changed internal CUDA Dynamic Parallelsim detection breakpoint to be set only when break_on_launch is enabled"
  },
  {
    "id": 23325,
    "content": "For Maxwell debugging, or to force the old classic debugging backend, set CUDBG_USE_LEGACY_DEBUGGER to 1 in your environment"
  },
  {
    "id": 23327,
    "content": "7 Release Features Major break_on_launch performance enhancements to use new KERNEL_READY notification mechanism instead of setting manual breakpoints"
  },
  {
    "id": 23328,
    "content": "Fixed Issues Fixed follow-fork child to avoid hanging behavior when both parent and child processes use CUDA"
  },
  {
    "id": 23329,
    "content": "Added a missing dlsym of a libpython function that was causing errors with some versions of libpython"
  },
  {
    "id": 23333,
    "content": "4 Update 1 Release Known Issues with Fedora 34 CUDA-GDB has known issues with debugging on Fedora 34 and may not be reliable"
  },
  {
    "id": 23334,
    "content": "Changed python behavior to dlopen libpython libraries that match the version of the python3 interpreter in PATH"
  },
  {
    "id": 23336,
    "content": "Fixed cuda_register_name and cuda_special_register_name to avoid returning old cached result on error"
  },
  {
    "id": 23346,
    "content": "Updated DWARF parser Old binaries might need to be recompiled in order to ensure CUDA-specific DWARF info are up to date"
  },
  {
    "id": 23347,
    "content": "However, macOS can still be used as the host system (where CUDA-GDB runs under macOS, using cuda-gdbserver to debug a remote target) The download for the macOS version of CUDA-GDB can be found at the following location: Download Here 10"
  },
  {
    "id": 23348,
    "content": "1 Release Enhanced debugging with only linenumber information Several enhancements were made to CUDA-GDB support for debugging programs compiled with -lineinfo but not with -G See also Compilation With Linenumber Information 10"
  },
  {
    "id": 23349,
    "content": "0 Release Turing Uniform Register Support Support added for examining and modifying uniform registers on Turing GPUs"
  },
  {
    "id": 23351,
    "content": "2 Release User induced core dump support For the devices that support compute preemption, user induced core dump support is added"
  },
  {
    "id": 23354,
    "content": "1 Release Volta-MPS core dump support GPU core dump generation is supported on Volta-MPS Lightweight GPU core dump support CUDA-GDB supports reading lightweight GPU core dump files"
  },
  {
    "id": 23358,
    "content": "New environment variables: CUDA_ENABLE_COREDUMP_ON_EXCEPTION , CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION and CUDA_COREDUMP_FILE can be used to enable and configure this feature"
  },
  {
    "id": 23360,
    "content": "5 Release CUDA Fortran Support CUDA-GDB supports CUDA Fortran debugging on 64-bit Linux operating systems"
  },
  {
    "id": 23362,
    "content": "0 Release Unified Memory Support Managed variables can be read and written from either a host thread or a device thread The debugger also annotates memory addresses that reside in managed memory with @managed The list of statically allocated managed variables can be accessed through a new info cuda managed command"
  },
  {
    "id": 23363,
    "content": "Android Support CUDA-GDB can now be used to debug Android native applications either locally or remotely"
  },
  {
    "id": 23364,
    "content": "Single-Stepping Optimizations CUDA-GDB can now use optimized methods to single-step the program, which accelerate single-stepping most of the time"
  },
  {
    "id": 23365,
    "content": "Faster Remote Debugging A lot of effort has gone into making remote debugging considerably faster, up to 2 orders of magnitude"
  },
  {
    "id": 23366,
    "content": "Kernel Entry Breakpoints The set cuda break_on_launch option will now break on kernels launched from the GPU"
  },
  {
    "id": 23368,
    "content": "0), the instruction that triggers an exception will be reported accurately The application keeps making forward progress and the PC at which the debugger stops may not match that address but an extra output message identifies the origin of the exception"
  },
  {
    "id": 23369,
    "content": "Live Range Optimizations To mitigate the issue of variables not being accessible at some code addresses, the debugger offers two new options"
  },
  {
    "id": 23370,
    "content": "With set cuda value_extrapolation , the latest known value is displayed with (possibly) prefix With set cuda ptx_cache , the latest known value of the PTX register associated with a source variable is displayed with the (cached) prefix"
  },
  {
    "id": 23371,
    "content": "New kernel events verbosity options have been added: set cuda kernel_events , set cuda kernel_events_depth Also set cuda defer_kernel_launch_notifications has been deprecated and has no effect any more"
  },
  {
    "id": 23373,
    "content": "5 Release Kernel Launch Trace Two new commands, info cuda launch trace and info cuda launch children , are introduced to display the kernel launch trace and the children kernel of a given kernel when Dynamic Parallelism is used"
  },
  {
    "id": 23374,
    "content": "Single-GPU Debugging (BETA) CUDA-GDB can now be used to debug a CUDA application on the same GPU that is rendering the desktop GUI"
  },
  {
    "id": 23375,
    "content": "This feature also enables debugging of long-running or indefinite CUDA kernels that would otherwise encounter a launch timeout"
  },
  {
    "id": 23376,
    "content": "In addition, multiple CUDA-GDB sessions can debug CUDA applications context-switching on the same GPU"
  },
  {
    "id": 23377,
    "content": "For information on enabling this, please see Single-GPU Debugging with the Desktop Manager Running and Multiple Debuggers Remote GPU Debugging CUDA-GDB in conjunction with CUDA-GDBSERVER can now be used to debug a CUDA application running on the remote host"
  },
  {
    "id": 23379,
    "content": "0 Release Dynamic Parallelism Support CUDA-GDB fully supports Dynamic Parallelism, a new feature introduced with the 5"
  },
  {
    "id": 23381,
    "content": "The debugger is able to track the kernels launched from another kernel and to inspect and modify variables like any other CPU-launched kernel When attached, all the usual features of the debugger are available to the user, as if the application had been launched from the debugger"
  },
  {
    "id": 23382,
    "content": "Attach on exception Using the environment variable CUDA_DEVICE_WAITS_ON_EXCEPTION , the application will run normally until a device exception occurs Then the application will wait for the debugger to attach itself to it for further debugging"
  },
  {
    "id": 23383,
    "content": "API Error Reporting Checking the error code of all the CUDA driver API and CUDA runtime API function calls is vital to ensure the correctness of a CUDA application"
  },
  {
    "id": 23386,
    "content": "The user can inspect the local variables of those subroutines and visit the call frame stack as if the routines were not inlined"
  },
  {
    "id": 23388,
    "content": "2 Release Kepler Support The primary change in Release 4 2 of CUDA-GDB is the addition of support for the new Kepler architecture"
  },
  {
    "id": 23390,
    "content": "1 Release Source Base Upgraded to GDB 7 2 Until now, CUDA-GDB was based on GDB 6 6 on Linux, and GDB 6"
  },
  {
    "id": 23397,
    "content": "Now, multiple CUDA-GDB sessions are allowed to co-exist as long as the GPUs are not shared between the applications being processed"
  },
  {
    "id": 23398,
    "content": "For instance, one CUDA-GDB process can debug process foo using GPU 0 while another CUDA-GDB process debugs process bar using GPU 1"
  },
  {
    "id": 23399,
    "content": "The command increases the precision of CUDA exceptions by automatically single-stepping through portions of code"
  },
  {
    "id": 23400,
    "content": "Under normal execution, the thread and instruction where an exception occurred may be imprecisely reported However, the exact instruction that generates the exception can be determined if the program is being single-stepped when the exception occurs"
  },
  {
    "id": 23401,
    "content": "Therefore ‘autostep’ aides the user by allowing them to specify sections of code where they suspect an exception could occur These sections are automatically single-stepped through when the program is running, and any exception that occurs within these sections is precisely reported"
  },
  {
    "id": 23402,
    "content": "Multiple Context Support On GPUs with compute capability of SM20 or higher, debugging multiple contexts on the same GPU is now supported"
  },
  {
    "id": 23403,
    "content": "Device Assertions Support The R285 driver released with the 4 1 version of the toolkit supports device assertions"
  },
  {
    "id": 23404,
    "content": "CUDA_GDB supports the assertion call and stops the execution of the application when the assertion is hit"
  },
  {
    "id": 23405,
    "content": "Use the ‘set cuda hide_internal_frames’ option to expose/hide the system call frames (hidden by default)"
  },
  {
    "id": 23406,
    "content": "Temporary Directory By default, the debugger API will use /tmp as the directory to store temporary files To select a different directory, the $TMPDIR environment variable and the API CUDBG_APICLIENT_PID variable must be set"
  },
  {
    "id": 23408,
    "content": "Getting Started  The CUDA toolkit can be installed by following instructions in the Quick Start Guide"
  },
  {
    "id": 23409,
    "content": "Further steps should be taken to set up the debugger environment, build the application, and run the debugger"
  },
  {
    "id": 23415,
    "content": "Temporary Directory  By default, CUDA-GDB uses /tmp as the directory to store temporary files Note The user must have write and execute permission to the temporary directory used by CUDA-GDB Note The value of $TMPDIR must be the same in the environment of the application and CUDA-GDB Note Since /tmp folder does not exist on Android device, the $TMPDIR environment variable must be set and point"
  },
  {
    "id": 23420,
    "content": "Using the CUDA-GDB debugger on Jetson and Drive Tegra devices  By default, on Jetson and Drive Tegra devices, GPU debugging is supported only if cuda-gdb and cuda-gdbserver are launched by a user who is a member of the debug group To add the current user to the debug group run this command: sudo usermod -a -G debug $USER 3"
  },
  {
    "id": 23422,
    "content": "Debug Compilation  NVCC, the NVIDIA CUDA compiler driver, provides a mechanism for generating the debugging information necessary for CUDA-GDB to work properly The -g -G option pair must be passed to NVCC when an application is compiled for ease of debugging with CUDA-GDB; for example, nvcc -g -G foo cu -o foo Using this line to compile the CUDA application foo"
  },
  {
    "id": 23423,
    "content": "cu forces -O0 compilation, with the exception of very limited dead-code eliminations and register-spilling optimizations"
  },
  {
    "id": 23424,
    "content": "makes the compiler include debug information in the executable To compile your CUDA Fortran code with debgging information necessary for CUDA-GDB to work properly, pgfortran, the PGI CUDA Fortran compiler, must be invoked with -g option Also, for the ease of debugging and forward compatibility with the future GPU architectures, it is recommended to compile the code with -Mcuda=nordc option; for"
  },
  {
    "id": 23425,
    "content": "example, pgfortran -g -Mcuda=nordc foo cuf -o foo For more information about the available compilation flags, please consult the PGI compiler documentation"
  },
  {
    "id": 23429,
    "content": "Compilation With Linenumber Information  Several enhancements were made to cuda-gdb’s support for debugging programs compiled with -lineinfo but not with -G The user may step into code that has no linenumber information, leading to an inability to determine which source-file/linenumber the code at the PC belongs to"
  },
  {
    "id": 23430,
    "content": "When debugging OptiX/RTCore code, the following should be kept in mind: NVIDIA internal code cannot be debugged or examined by the user OptiX/RTCode debugging is limited to -lineinfo , and building this code with full debug infomation ( -G ) is not supported OptiX/RTCode code is highly optimized, and as such the notes above about debugging optimized code apply"
  },
  {
    "id": 23434,
    "content": "Compiling For Specific GPU architectures  By default, the compiler will only generate code for the compute_52 PTX and sm_52 cubins For later GPUs, the kernels are recompiled at runtime from the PTX for the architecture of the target GPU(s) Compiling for a specific virtual architecture guarantees that the application will work for any GPU architecture after that, for a trade-off in performance It"
  },
  {
    "id": 23435,
    "content": "is highly recommended to compile the application once and for all for the GPU architectures targeted by the application, and to generate the PTX code for the latest virtual architecture for forward compatibility"
  },
  {
    "id": 23439,
    "content": "For instance, to compile an application for a GPU with compute capability 7 0, add the following flag to the compilation command: -gencode arch=compute_70,code=sm_70 To compile PTX code for any future architecture past the compute capability 7 0, add the following flag to the compilation command: -gencode arch=compute_70,code=compute_70 For additional information, please consult the compiler"
  },
  {
    "id": 23445,
    "content": "Single-GPU Debugging with the Desktop Manager Running  For devices with compute capability 6 0 and higher CUDA-GDB can be used to debug CUDA applications on the same GPU that is running the desktop GUI Additionally for devices with compute capability less than 6 0 software preemption can be used to debug CUDA applications on the same GPU that is running the desktop GUI"
  },
  {
    "id": 23446,
    "content": "There are two ways to enable this functionality: Note This is a BETA feature available on Linux and is only supported on Maxwell"
  },
  {
    "id": 23447,
    "content": "Use the following command: set cuda software_preemption on Export the following environment variable: CUDA_DEBUGGER_SOFTWARE_PREEMPTION=1 Either of the options above will activate software preemption"
  },
  {
    "id": 23448,
    "content": "When the GPU hits a breakpoint or any other event that would normally cause the GPU to freeze, CUDA-GDB releases the GPU for use by the desktop or other applications This enables CUDA-GDB to debug a CUDA application on the same GPU that is running the desktop GUI, and also enables debugging of multiple CUDA applications context-switching on the same GPU"
  },
  {
    "id": 23452,
    "content": "Multi-GPU Debugging  Multi-GPU debugging designates the scenario where the application is running on more than one CUDA-capable device Multi-GPU debugging is not much different than single-GPU debugging except for a few additional CUDA-GDB commands that let you switch between the GPUs"
  },
  {
    "id": 23453,
    "content": "Once paused, you can use info cuda kernels to view all the active kernels and the GPUs they are running on"
  },
  {
    "id": 23454,
    "content": "Note If the CUDA_VISIBLE_DEVICES environment is used, only the specified devices are suspended and resumed"
  },
  {
    "id": 23455,
    "content": "To switch to an active kernel, use cuda kernel , where n is the ID of the kernel retrieved from info cuda kernels"
  },
  {
    "id": 23457,
    "content": "When a breakpoint is set in such a kernel, by either name or file name and line number, it will be resolved arbitrarily to only one instance of that kernel With the runtime API, the exact instance to which the breakpoint will be resolved cannot be controlled With the driver API, the user can control the instance to which the breakpoint will be resolved to by setting the breakpoint right after its"
  },
  {
    "id": 23462,
    "content": "Remote Debugging  There are multiple methods to remote debug an application with CUDA-GDB In addition to using SSH or VNC from the host system to connect to the target system, it is also possible to use the target remote GDB feature Using this option, the local cuda-gdb (client) connects to the cuda-gdbserver process (the server) running on the target system Setting remote debugging that way is"
  },
  {
    "id": 23463,
    "content": "a 2-step process: Launch the cuda-gdbserver on the remote host cuda-gdbserver can be launched on the remote host in different operation modes To launch a new application in debug mode, invoke cuda-gdb server as follows: $ cuda-gdbserver :1234 app_invocation Where 1234 is the TCP port number that cuda-gdbserver will listen to for incoming connections from cuda-gdb , and app-invocation is the"
  },
  {
    "id": 23464,
    "content": "invocation command to launch the application, arguments included Option 2: Attach cuda-gdbserver to the running process To attach cuda-gdbserver to an already running process, the --attach option followed by process identification number (PID) must be used: $ cuda-gdbserver :1234 --attach 5678 Where 1234 is the TCP port number and 5678 is process identifier of the application cuda-gdbserver must"
  },
  {
    "id": 23465,
    "content": "be attached to Launch cuda-gdb on the client Configure cuda-gdb to connect to the remote target using either: (cuda-gdb) target remote or (cuda-gdb) target extended-remote It is recommended to use set sysroot command if libraries installed on the debug target might differ from the ones installed on the debug host For example, cuda-gdb could be configured to connect to remote target as follows:"
  },
  {
    "id": 23472,
    "content": "2 is the IP address or domain name of the remote target, and 1234 is the TCP port previously previously opened by cuda-gdbserver"
  },
  {
    "id": 23479,
    "content": "0, several debugging sessions may take place simultaneously as long as the CUDA devices are used exclusively"
  },
  {
    "id": 23480,
    "content": "For instance, one instance of CUDA-GDB can debug a first application that uses the first GPU while another instance of CUDA-GDB debugs a second application that uses the second GPU The exclusive use of a GPU is achieved by specifying which GPU is visible to the application by using the CUDA_VISIBLE_DEVICES environment variable"
  },
  {
    "id": 23481,
    "content": "$ CUDA_VISIBLE_DEVICES=1 cuda-gdb my_app Additionally for devices with compute capability less than 6"
  },
  {
    "id": 23482,
    "content": "0, with software preemption enabled ( set cuda software_preemption on ), multiple CUDA-GDB instances can be used to debug CUDA applications context-switching on the same GPU"
  },
  {
    "id": 23486,
    "content": "Attaching/Detaching  CUDA-GDB can attach to and detach from a CUDA application running on GPUs with compute capability 2 0 and beyond, using GDB’s built-in commands for attaching to or detaching from a process Additionally, if the environment variable CUDA_DEVICE_WAITS_ON_EXCEPTION is set to 1 prior to running the CUDA application, the application will run normally until a device exception"
  },
  {
    "id": 23487,
    "content": "occurs Note By default on some Linux distributions, the debugger cannot attach to an already running processes due to security settings"
  },
  {
    "id": 23488,
    "content": "In order to enable the attach feature of the CUDA debugger, either cuda-gdb should be launched as root, or /proc/sys/kernel/yama/ptrace_scope should be set to zero, using the following command: $ sudo sh -c \"echo 0 >/proc/sys/kernel/yama/ptrace_scope\" To make the change permanent, edit /etc/sysctl"
  },
  {
    "id": 23494,
    "content": "As much as possible, CUDA-GDB command names will be similar to the equivalent GDB commands used for debugging host code For instance, the GDB command to display the host threads and switch to host thread 1 are, respectively: (cuda-gdb) info threads (cuda-gdb) thread 1 To display the CUDA threads and switch to cuda thread 1, the user only has to type: (cuda-gdb) info cuda threads (cuda-gdb) cuda"
  },
  {
    "id": 23497,
    "content": "Getting Help  As with GDB commands, the built-in help for the CUDA commands is accessible from the cuda-gdb command line by using the help command: (cuda-gdb) help cuda name_of_the_cuda_command (cuda-gdb) help set cuda name_of_the_cuda_option (cuda-gdb) help info cuda name_of_the_info_cuda_command Moreover, all the CUDA commands can be auto-completed by pressing the TAB key, as with any other"
  },
  {
    "id": 23507,
    "content": "To use DDD with CUDA-GDB, launch DDD with the following command: ddd --debugger cuda-gdb cuda-gdb must be in your $PATH"
  },
  {
    "id": 23511,
    "content": "Environment variables set in the application environment or programmatically from the application with the CUDA Driver API"
  },
  {
    "id": 23512,
    "content": "Compilation for GPU core dump generation GPU core dumps will be generated regardless of compilation flags used to generate the GPU application"
  },
  {
    "id": 23513,
    "content": "For the best debugging experience, it is recommended to compile the application with the -g -G or the -lineinfo option with NVCC See Compiling the Application for more information on passing compilation flags for debugging"
  },
  {
    "id": 23514,
    "content": "Enabling GPU core dump generation on exception with environment variables Set the CUDA_ENABLE_COREDUMP_ON_EXCEPTION environment variable to 1 in order to enable generating a GPU core dump when a GPU exception is encountered Set the CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION environment variable to 0 in order to disable generating a CPU core dump when a GPU exception is encountered Set the"
  },
  {
    "id": 23515,
    "content": "CUDA_ENABLE_LIGHTWEIGHT_COREDUMP environment variable to 1 in order to enable generating lightweight corefiles instead of full corefiles When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application Controlling behavior of GPU core dump generation The CUDA_COREDUMP_GENERATION_FLAGS environment variable can be used when generating GPU core dumps to"
  },
  {
    "id": 23516,
    "content": "deviate from default generation behavior These flags can be used to accomplish tasks such as reducing the size of the generated GPU core dump or other desired behaviors that deviate from the defaults GPU core dump CUDA_COREDUMP_GENERATION_FLAGS  Environment Variable flag Description skip_nonrelocated_elf_images Disables including copies of nonrelocated elf images in the GPU core dump Note Setting"
  },
  {
    "id": 23517,
    "content": "the CUDA_ENABLE_LIGHTWEIGHT_COREDUMP environment variable to 1 is equivalent to CUDA_COREDUMP_GENERATION_FLAGS=\"skip_nonrelocated_elf_images,skip_global_memory,skip_shared_memory,skip_local_memory\" Note Setting the CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION environment variable to 0 is equivalent to CUDA_COREDUMP_GENERATION_FLAGS=\"skip_abort\" Limitations and notes for core dump generation The following"
  },
  {
    "id": 23518,
    "content": "limitations apply to core dump support: For Windows WDDM, GPU core dump is only supported on a GPU with compute capability 6"
  },
  {
    "id": 23520,
    "content": "GPU core dump is unsupported for the Windows Subsystem for Linux on GPUs running in SLI mode Multi-GPU setups are supported, but SLI mode cannot be enabled in the Driver Control Panel GPU core dump is supported for the Windows Subsystem for Linux only when the hardware scheduling mode is enabled Generating a CPU core dump with CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION is currently unsupported on the"
  },
  {
    "id": 23524,
    "content": "If an MPS client triggers a core dump, every other client running on the same MPS server will fault The indirectly faulting clients will also generate a core dump if they have core dump generation enabled GPU core dump is unsupported when other developer tools, including CUDA-GDB, are interacting with the application"
  },
  {
    "id": 23527,
    "content": "When generating a coredump on exception, if the kernel exits before the exception has been recognized it may result in failure to generate the corefile"
  },
  {
    "id": 23528,
    "content": "Note The user should not send the application process a signal and ensure that the application process does not automatically terminate while the coredump generation is in process"
  },
  {
    "id": 23530,
    "content": "6, the compute-sanitizer tool can generate a GPU core dump when an error is detected by using the --generate-coredump yes option See the compute-sanitizer documentation for more information: https: docs"
  },
  {
    "id": 23533,
    "content": "html#coredump Note CPU core dumps will be located in a distribution specific location Examining the /proc/sys/kernel/core_pattern file will typically hint at the name/location of the CPU core dump"
  },
  {
    "id": 23534,
    "content": "Note NVIDIA vGPU platforms must explicitly enable debugging support to perform GPU core dump generation Please reference the Virtual GPU Software User Guide for information on how to enable debugging on vGPU Note NVIDIA Jetson and Drive Tegra devices must explicitly enable debugging support to perform GPU core dump generation Note When generating core dumps on NVIDIA Drive Tegra devices running"
  },
  {
    "id": 23535,
    "content": "QNX, core dump generation may hang when generating CPU core dumps Note If core dumps are not generated when running programs built with OptiX/RTCore, try setting the environment variable OPTIX_FORCE_DEPRECATED_LAUNCHER to 1 Note If core dumps are not generated when running programs on Windows Subsystem for Linux, ensure the debug interface is enabled via setting the registry key"
  },
  {
    "id": 23536,
    "content": ">HKEY_LOCAL_MACHINE\\SOFTWARE\\NVIDIA Corporation\\GPUDebugger\\EnableInterface to (DWORD) 1 Note GPU core dump is supported on GPUs running with Confidential Compute mode only with devtools mode Naming of GPU core dump files By default, a GPU core dump is created in the current working directory"
  },
  {
    "id": 23538,
    "content": "nvcudmp where TIME is the number of seconds since the Epoch, HOSTNAME is the host name of the machine running the CUDA application and PID is the process identifier of the CUDA application"
  },
  {
    "id": 23539,
    "content": "The CUDA_COREDUMP_FILE environment variable can be used to define a template that is used to change the name of a GPU core dump file The template can either be an absolute path or a relative path to the current working directory The template can contain % specifiers which are substituted by the following patterns when a GPU core dump is created: Specifier Description %h Host name of the machine"
  },
  {
    "id": 23540,
    "content": "running the CUDA application %p Process identifier of the CUDA application %t Time as the number of seconds since the Epoch, 1970-01-01 00:00:00 +0000 (UTC) As an example, setting CUDA_COREDUMP_FILE to: export CUDA_COREDUMP_FILE=newName"
  },
  {
    "id": 23547,
    "content": "%p\" Would result in GPU core dumps being written to the user’s home directory with the same name logic as in the above example"
  },
  {
    "id": 23550,
    "content": "Coredumps may be piped to shell commands via CUDA_COREDUMP_FILE with the following format: export CUDA_COREDUMP_FILE='| cmd > file' For example, to pipe a coredump to gzip use: export CUDA_COREDUMP_FILE='| gzip -9 > cuda-coredump"
  },
  {
    "id": 23552,
    "content": "Enabling user induced GPU core dump generation For the devices that support compute preemption, the user can interrupt a running CUDA process to generate the GPU core dump Set the CUDA_ENABLE_USER_TRIGGERED_COREDUMP environment variable to 1 in order to enable generating a user induced GPU core dump"
  },
  {
    "id": 23553,
    "content": "Setting this environment variable will open a communication pipe for each subsequently running CUDA process To change the default pipe file name, set the CUDA_COREDUMP_PIPE environment variable to a specific pipe name The default pipe name is in the following format: corepipe cuda"
  },
  {
    "id": 23554,
    "content": "HOSTNAME PID where HOSTNAME is the host name of machine running the CUDA application and PID is the process identifier of the CUDA application"
  },
  {
    "id": 23555,
    "content": "Displaying core dump generation progress By default, when an application crashes and generates a GPU core dump, the application may appear to be unresponsive or frozen until fully generated Set the CUDA_COREDUMP_SHOW_PROGRESS environment variable to 1 in order to print core dump generation progress messages to stderr coredump: Writing out device table coredump: Finalizing coredump: All done"
  },
  {
    "id": 23556,
    "content": "Enabling GPU core dump generation with the CUDA Driver API The Driver API has equivalent settings for all of the environment variables, with the added feature of being able to set different core dump settings per-context instead of globally"
  },
  {
    "id": 23557,
    "content": "Use cuCoredumpGetAttributeGlobal and cuCoredumpSetAttributeGlobal to fetch or set the global attribute Use cuCoredumpGetAttribute and cuCoredumpSetAttribute to fetch or set the per context attribute"
  },
  {
    "id": 23558,
    "content": "The table below lists the environment variables and the equivalent CUcoredumpSettings flags that are available to manage core dump settings with the Coredump Attributes Control API"
  },
  {
    "id": 23559,
    "content": "Note The CU_COREDUMP_ENABLE_USER_TRIGGER setting can only be set globally in the driver API and CU_COREDUMP_PIPE must be set (if desired) before user-triggered core dumps are enabled"
  },
  {
    "id": 23560,
    "content": "GPU core dump configuration parameters  Environment Variable Description Environment Variable: CUDA_ENABLE_COREDUMP_ON_EXCEPTION CUcoredumpSettings Flag: CU_COREDUMP_ENABLE_ON_EXCEPTION Enables GPU core dump generation for exceptions Environment Variable: CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION CUcoredumpSettings Flag: CU_COREDUMP_TRIGGER_HOST Triggers host (CPU) core dump after GPU core dump is"
  },
  {
    "id": 23561,
    "content": "complete Environment Variable: CUDA_ENABLE_LIGHTWEIGHT_COREDUMP CUcoredumpSettings Flag: CU_COREDUMP_LIGHTWEIGHT When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application Environment Variable: CUDA_ENABLE_USER_TRIGGERED_COREDUMP CUcoredumpSettings Flag: CU_COREDUMP_ENABLE_USER_TRIGGER Enables user triggerable core dumps by writing to a pipe defined"
  },
  {
    "id": 23562,
    "content": "in the COREDUMP_PIPE setting Environment Variable: CUDA_COREDUMP_FILE CUcoredumpSettings Flag: CU_COREDUMP_FILE Filename template for the GPU core dump Environment Variable: CUDA_COREDUMP_PIPE CUcoredumpSettings Flag: CU_COREDUMP_PIPE Filename template for the user pipe trigger Inspecting GPU and GPU+CPU core dumps in cuda-gdb Use the following command to load the GPU core dump into the debugger"
  },
  {
    "id": 23566,
    "content": "Then, issue standard cuda-gdb commands to further investigate application state on the device at the moment it was aborted"
  },
  {
    "id": 23567,
    "content": "Use the following command to load CPU and GPU core dumps into the debugger (cuda-gdb) target core core cpu core cuda This will open the core dump file and print the exception encountered during program execution"
  },
  {
    "id": 23568,
    "content": "Then, issue standard cuda-gdb commands to further investigate application state on the host and the device at the moment it was aborted"
  },
  {
    "id": 23569,
    "content": "Kernel Focus  A CUDA application may be running several host threads and many device threads To simplify the visualization of information about the state of application, commands are applied to the entity in focus When the focus is set to a host thread, the commands will apply only to that host thread (unless the application is fully resumed, for instance) On the device side, the focus is always"
  },
  {
    "id": 23575,
    "content": "Software and hardware coordinates can be used interchangeably and simultaneously as long as they remain coherent Note If software preemption is enabled ( set cuda software_preemption on ), hardware coordinates corresponding to a device thread are likely to change upon resuming execution on the device However, software coordinates will remain intact and will not change for the lifetime of the"
  },
  {
    "id": 23579,
    "content": "Current Focus  To inspect the current focus, use the cuda command followed by the coordinates of interest: (cuda-gdb) cuda device sm warp lane block thread block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0 (cuda-gdb) cuda kernel block thread kernel 1, block (0,0,0), thread (0,0,0) (cuda-gdb) cuda kernel kernel 1 5"
  },
  {
    "id": 23581,
    "content": "Switching Focus  To switch the current focus, use the cuda command followed by the coordinates to be changed: (cuda-gdb) cuda device 0 sm 1 warp 2 lane 3 [Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread (67,0,0), device 0, sm 1, warp 2, lane 3] 374 int totalThreads = gridDim"
  },
  {
    "id": 23583,
    "content": "x; If the specified focus is not fully defined by the command, the debugger will assume that the omitted coordinates are set to the coordinates in the current focus, including the subcoordinates of the block and thread"
  },
  {
    "id": 23584,
    "content": "(cuda-gdb) cuda thread (15) [Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread (15,0,0), device 0, sm 1, warp 0, lane 15] 374 int totalThreads = gridDim"
  },
  {
    "id": 23586,
    "content": "x; The parentheses for the block and thread arguments are optional (cuda-gdb) cuda block 1 thread 3 [Switching focus to CUDA kernel 1, grid 2, block (1,0,0), thread (3,0,0), device 0, sm 3, warp 0, lane 3] 374 int totalThreads = gridDim"
  },
  {
    "id": 23589,
    "content": "Program Execution  Applications are launched the same way in CUDA-GDB as they are with GDB by using the run command"
  },
  {
    "id": 23590,
    "content": "Interrupting the Application  If the CUDA application appears to be hanging or stuck in an infinite loop, it is possible to manually interrupt the application by pressing CTRL+C"
  },
  {
    "id": 23591,
    "content": "At that point, the program can be inspected, modified, single-stepped, resumed, or terminated at the user’s discretion"
  },
  {
    "id": 23592,
    "content": "It is not possible to break into and debug applications that have been launched outside the debugger"
  },
  {
    "id": 23595,
    "content": "Single Stepping  Single-stepping device code is supported However, unlike host code single-stepping, device code single-stepping works at the warp level This means that single-stepping a device kernel advances all the active threads in the warp currently in focus"
  },
  {
    "id": 23596,
    "content": "In order to advance the execution of more than one warp, a breakpoint must be set at the desired location and then the application must be fully resumed"
  },
  {
    "id": 23597,
    "content": "A special case is single-stepping over thread barrier calls like: __syncthreads() or cluster-wide barriers"
  },
  {
    "id": 23598,
    "content": "In this case, an implicit temporary breakpoint is set immediately after the barrier and all threads are resumed until the temporary breakpoint is hit"
  },
  {
    "id": 23599,
    "content": "To force a function to not be inlined by the compiler, the __noinline__ keyword must be added to the function declaration"
  },
  {
    "id": 23600,
    "content": "Asynchronous SASS instructions executed on the device, such as the warpgroup instructions, at prior PCs are not guaranteed to be complete"
  },
  {
    "id": 23601,
    "content": "The following list defines single-step behavior when encountering these APIs: When encountering device side kernel launches (denoted by the >> launch syntax), the step and next commands will have the same behavior, and both will step over the launch call"
  },
  {
    "id": 23603,
    "content": "0), stepping into the deprecated cudaDeviceSynchronize() results in undefined behavior When stepping a device grid launch to completion, focus will automatically switch back to the CPU The cuda kernel focus switching command must be used to switch to another grid of interest (if one is still resident)"
  },
  {
    "id": 23606,
    "content": "Breakpoints and Watchpoints  There are multiple ways to set a breakpoint on a CUDA application The commands used to set a breakpoint on device code are the same as the commands used to set a breakpoint on host code If a breakpoint is set on device code, the breakpoint will be marked pending until the ELF image of the kernel is loaded When a breakpoint is set, it forces all resident GPU threads"
  },
  {
    "id": 23607,
    "content": "to stop at this location when it reaches the corresponding PC When a breakpoint is hit by one thread, there is no guarantee that the other threads will hit the breakpoint at the same time Therefore the same breakpoint may be hit several times, and the user must be careful with checking which thread(s) actually hit(s) the breakpoint The disable command can be used to prevent hitting the breakpoint"
  },
  {
    "id": 23611,
    "content": "Symbolic Breakpoints  To set a breakpoint at the entry of a function, use the break command followed by the name of the function or method: (cuda-gdb) break my_function (cuda-gdb) break my_class::my_method For templatized functions and methods, the full signature must be given: (cuda-gdb) break int my_templatized_function(int) The mangled name of the function can also be used To find the mangled"
  },
  {
    "id": 23612,
    "content": "name of a function, you can use the following command: (cuda-gdb) set demangle-style none (cuda-gdb) info function my_function_name (cuda-gdb) set demangle-style auto 7"
  },
  {
    "id": 23614,
    "content": "Line Breakpoints  To set a breakpoint on a specific line number, use the following syntax: (cuda-gdb) break my_file cu:185 If the specified line corresponds to an instruction within templatized code, multiple breakpoints will be created, one for each instance of the templatized code"
  },
  {
    "id": 23617,
    "content": "Address Breakpoints  To set a breakpoint at a specific address, use the break command with the address as argument: (cuda-gdb) break *0x1afe34d0 The address can be any address on the device or the host"
  },
  {
    "id": 23620,
    "content": "Kernel Entry Breakpoints  To break on the first instruction of every launched kernel, set the break_on_launch option to application: (cuda-gdb) set cuda break_on_launch application See set cuda break_on_launch for more information"
  },
  {
    "id": 23623,
    "content": "Conditional Breakpoints  To make the breakpoint conditional, use the optional if keyword or the cond command"
  },
  {
    "id": 23624,
    "content": "Info CUDA Commands  These are commands that display information about the GPU and the application’s CUDA state The available options are: devices information about all the devices sms information about all the active SMs in the current device warps information about all the active warps in the current SM lanes information about all the active lanes in the current warp kernels information about"
  },
  {
    "id": 23625,
    "content": "all the active kernels blocks information about all the active blocks in the current kernel threads information about all the active threads in the current kernel launch trace information about the parent kernels of the kernel in focus launch children information about the kernels launched by the kernels in focus contexts information about all the contexts A filter can be applied to every info"
  },
  {
    "id": 23626,
    "content": "cuda command A restriction can be any of the following: device n sm n warp n lane n kernel n grid n block x[,y] or block (x[,y]) thread x[,y[,z]] or thread (x[,y[,z]]) breakpoint all and breakpoint n where n , x , y , z are integers, or one of the following special keywords: current , any , and all Note The breakpoint all and breakpoint n filter are only effective for the info cuda threads command"
  },
  {
    "id": 23631,
    "content": "(cuda-gdb) info cuda devices Dev PCI Bus/Dev ID Name Description SM Type SMs Warps/SM Lanes/Warp Max Regs/Lane Active SMs Mask 0 06:00"
  },
  {
    "id": 23632,
    "content": "0 GeForce GTX TITAN Z GK110B sm_35 15 64 32 256 0x00000000 1 07:00 0 GeForce GTX TITAN Z GK110B sm_35 15 64 32 256 0x00000000 8"
  },
  {
    "id": 23635,
    "content": "info cuda sms  This command shows all the SMs for the device and the associated active warps on the SMs"
  },
  {
    "id": 23636,
    "content": "(cuda-gdb) info cuda sms SM Active Warps Mask Device 0 * 0 0xffffffffffffffff 1 0xffffffffffffffff 2 0xffffffffffffffff 3 0xffffffffffffffff 4 0xffffffffffffffff 5 0xffffffffffffffff 6 0xffffffffffffffff 7 0xffffffffffffffff 8 0xffffffffffffffff"
  },
  {
    "id": 23640,
    "content": "info cuda warps  This command takes you one level deeper and prints all the warps information for the SM in focus"
  },
  {
    "id": 23641,
    "content": "(cuda-gdb) info cuda warps Wp /Active Lanes Mask/ Divergent Lanes Mask/Active Physical PC/Kernel/BlockIdx Device 0 SM 0 * 0 0xffffffff 0x00000000 0x000000000000001c 0 (0,0,0) 1 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 2 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 3 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 4 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 5 0xffffffff"
  },
  {
    "id": 23642,
    "content": "0x00000000 0x0000000000000000 0 (0,0,0) 6 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0) 7 0xffffffff 0x00000000 0x0000000000000000 0 (0,0,0)"
  },
  {
    "id": 23646,
    "content": "info cuda lanes  This command displays all the lanes (threads) for the warp in focus This command supports filters and the default is device current sm current warp current lane all"
  },
  {
    "id": 23647,
    "content": "(cuda-gdb) info cuda lanes Ln State Physical PC ThreadIdx Device 0 SM 0 Warp 0 * 0 active 0x000000000000008c (0,0,0) 1 active 0x000000000000008c (1,0,0) 2 active 0x000000000000008c (2,0,0) 3 active 0x000000000000008c (3,0,0) 4 active 0x000000000000008c (4,0,0) 5 active 0x000000000000008c (5,0,0) 6 active 0x000000000000008c (6,0,0) 7 active 0x000000000000008c (7,0,0) 8 active 0x000000000000008c"
  },
  {
    "id": 23648,
    "content": "(8,0,0) 9 active 0x000000000000008c (9,0,0) 10 active 0x000000000000008c (10,0,0) 11 active 0x000000000000008c (11,0,0) 12 active 0x000000000000008c (12,0,0) 13 active 0x000000000000008c (13,0,0) 14 active 0x000000000000008c (14,0,0) 15 active 0x000000000000008c (15,0,0) 16 active 0x000000000000008c (16,0,0)"
  },
  {
    "id": 23653,
    "content": "It prints the SM mask, kernel ID, and the grid ID for each kernel with the associated dimensions and arguments"
  },
  {
    "id": 23654,
    "content": "(cuda-gdb) info cuda kernels Kernel Parent Dev Grid Status SMs Mask GridDim BlockDim Name Args * 1 - 0 2 Active 0x00ffffff (240,1,1) (128,1,1) acos_main parms="
  },
  {
    "id": 23655,
    "content": "This command will also show grids that have been launched on the GPU with Dynamic Parallelism Kernels with a negative grid ID have been launched from the GPU, while kernels with a positive grid ID have been launched from the CPU"
  },
  {
    "id": 23659,
    "content": "info cuda blocks  This command displays all the active or running blocks for the kernel in focus (cuda-gdb) info cuda blocks BlockIdx To BlockIdx Count State Kernel 1 * (0,0,0) (191,0,0) 192 running Coalescing can be turned off as follows in which case more information on the Device and the SM get displayed: (cuda-gdb) set cuda coalescing off The following is the output of the same command when"
  },
  {
    "id": 23660,
    "content": "coalescing is turned off (cuda-gdb) info cuda blocks BlockIdx State Dev SM Kernel 1 * (0,0,0) running 0 0 (1,0,0) running 0 3 (2,0,0) running 0 6 (3,0,0) running 0 9 (4,0,0) running 0 12 (5,0,0) running 0 15 (6,0,0) running 0 18 (7,0,0) running 0 21 (8,0,0) running 0 1"
  },
  {
    "id": 23664,
    "content": "info cuda threads  This command displays the application’s active CUDA blocks and threads with the total count of threads in those blocks"
  },
  {
    "id": 23665,
    "content": "Also displayed are the virtual PC and the associated source file and the line number information The outputs are coalesced by default as follows: (cuda-gdb) info cuda threads BlockIdx ThreadIdx To BlockIdx ThreadIdx Count Virtual PC Filename Line Device 0 SM 0 * (0,0,0 (0,0,0) (0,0,0) (31,0,0) 32 0x000000000088f88c acos"
  },
  {
    "id": 23669,
    "content": "(cuda-gdb) info cuda threads BlockIdx ThreadIdx Virtual PC Dev SM Wp Ln Filename Line Kernel 1 * (0,0,0) (0,0,0) 0x000000000088f88c 0 0 0 0 acos cu 376 (0,0,0) (1,0,0) 0x000000000088f88c 0 0 0 1 acos cu 376 (0,0,0) (2,0,0) 0x000000000088f88c 0 0 0 2 acos cu 376 (0,0,0) (3,0,0) 0x000000000088f88c 0 0 0 3 acos cu 376 (0,0,0) (4,0,0) 0x000000000088f88c 0 0 0 4 acos cu 376 (0,0,0) (5,0,0)"
  },
  {
    "id": 23670,
    "content": "0x000000000088f88c 0 0 0 5 acos cu 376 (0,0,0) (6,0,0) 0x000000000088f88c 0 0 0 6 acos cu 376 (0,0,0) (7,0,0) 0x000000000088f88c 0 0 0 7 acos cu 376 (0,0,0) (8,0,0) 0x000000000088f88c 0 0 0 8 acos cu 376 (0,0,0) (9,0,0) 0x000000000088f88c 0 0 0 9 acos cu 376"
  },
  {
    "id": 23671,
    "content": "If some threads are not currently running on the hardware, they will create holes in the thread ranges For instance, if a kernel consist of 2 blocks of 16 threads, and only the 8 lowest threads are active, then 2 coalesced ranges will be printed: one range for block 0 thread 0 to 7, and one range for block 1 thread 0 to 7"
  },
  {
    "id": 23672,
    "content": "(cuda-gdb) info cuda threads breakpoint all BlockIdx ThreadIdx Virtual PC Dev SM Wp Ln Filename Line Kernel 0 (1,0,0) (0,0,0) 0x0000000000948e58 0 11 0 0 infoCommands cu 12 (1,0,0) (1,0,0) 0x0000000000948e58 0 11 0 1 infoCommands cu 12 (1,0,0) (2,0,0) 0x0000000000948e58 0 11 0 2 infoCommands cu 12 (1,0,0) (3,0,0) 0x0000000000948e58 0 11 0 3 infoCommands cu 12 (1,0,0) (4,0,0) 0x0000000000948e58 0"
  },
  {
    "id": 23673,
    "content": "11 0 4 infoCommands cu 12 (1,0,0) (5,0,0) 0x0000000000948e58 0 11 0 5 infoCommands cu 12 (cuda-gdb) info cuda threads breakpoint 2 lane 1 BlockIdx ThreadIdx Virtual PC Dev SM Wp Ln Filename Line Kernel 0 (1,0,0) (1,0,0) 0x0000000000948e58 0 11 0 1 infoCommands cu 12 8"
  },
  {
    "id": 23676,
    "content": "info cuda launch trace  This command displays the kernel launch trace for the kernel in focus For each kernel in the trace, the command prints the level of the kernel in the trace, the kernel ID, the device ID, the grid Id, the status, the kernel dimensions, the kernel name, and the kernel arguments (cuda-gdb) info cuda launch trace Lvl Kernel Dev Grid Status GridDim BlockDim Invocation * 0 3 0"
  },
  {
    "id": 23677,
    "content": "-7 Active (32,1,1) (16,1,1) kernel3(c=5) 1 2 0 -5 Terminated (240,1,1) (128,1,1) kernel2(b=3) 2 1 0 2 Active (240,1,1) (128,1,1) kernel1(a=1) A kernel that has been launched but that is not running on the GPU will have a Pending status For the few cases, when the debugger cannot determine if a kernel is pending or terminated, the status is set to Undetermined Note With set cuda software_preemption"
  },
  {
    "id": 23682,
    "content": "info cuda launch children  This command displays the list of non-terminated kernels launched by the kernel in focus For each kernel, the kernel ID, the device ID, the grid Id, the kernel dimensions, the kernel name, and the kernel parameters are displayed (cuda-gdb) info cuda launch children Kernel Dev Grid GridDim BlockDim Invocation * 3 0 -7 (1,1,1) (1,1,1) kernel5(a=3) 18 0 -8 (1,1,1)"
  },
  {
    "id": 23687,
    "content": "info cuda contexts  This command enumerates all the CUDA contexts running on all GPUs (cuda-gdb) info cuda contexts Context Dev State 0x080b9518 0 inactive * 0x08067948 0 active 8"
  },
  {
    "id": 23690,
    "content": "info cuda managed  This command shows all the static managed variables on the device or on the host depending on the focus (cuda-gdb) info cuda managed Static managed variables on device 0 are: managed_var = 3 managed_consts = {one = 1, e = 2"
  },
  {
    "id": 23694,
    "content": "Disassembly  The device SASS code can be disassembled using the standard GDB disassembly instructions such as x/i and display/i"
  },
  {
    "id": 23695,
    "content": "(cuda-gdb) x/4i $pc-32 0xa689a8 : MOV R0, c[0x0][0x34] 0xa689b8 : MOV R3, c[0x0][0x28] 0xa689c0 : IMUL R2, R0, R3 => 0xa689c8 : MOV R0, c[0x0][0x28] Note For disassembly instruction to work properly, cuobjdump must be installed and present in your $PATH"
  },
  {
    "id": 23697,
    "content": "0) and newer architectures, if an instruction triggers an exception it will be prefixed with *> For example, consider the following exception: CUDA Exception: Warp Illegal Address The exception was triggered at PC 0x555555c08620 (memexceptions_kernel cu:17) Thread 1 \"memexceptions\" received signal CUDA_EXCEPTION_14, Warp Illegal Address [Switching focus to CUDA kernel 0, grid 1, block (0,0,0),"
  },
  {
    "id": 23698,
    "content": "thread (0,0,0), device 0, sm 0, warp 0, lane 0] 0x0000555555c08fb0 in exception_kernel>> (data=0x7fffccc00000, exception=MMU_FAULT) at memexceptions_kernel cu:50 50 } (cuda-gdb) The disas command can be used to view both the PC and the error PC that triggered the exception (cuda-gdb) disas $pc,+16 Dump of assembler code from 0x555555c08fb0 to 0x555555c08fc0: => 0x0000555555c08fb0 : ERRBAR End of"
  },
  {
    "id": 23699,
    "content": "assembler dump (cuda-gdb) disas $errorpc,+16 Dump of assembler code from 0x555555c08620 to 0x555555c08630: *> 0x0000555555c08620 : ST"
  },
  {
    "id": 23707,
    "content": "Registers  The device registers code can be inspected/modified using the standard GDB commands such as info registers (cuda-gdb) info registers $R0 $R1 $R2 $R3 R0 0xf0 240 R1 0xfffc48 16776264 R2 0x7800 30720 R3 0x80 128 The registers are also accessible as $R built-in variables, for example: (cuda-gdb) printf \"%d %d \", $R0*$R3, $R2 30720 30720 Values of predicate and CC registers can be"
  },
  {
    "id": 23709,
    "content": "(cuda-gdb) info registers system P0 0x1 1 P1 0x1 1 P2 0x0 0 P3 0x0 0 P4 0x0 0 P5 0x0 0 P6 0x1 1 CC 0x0 0 8"
  },
  {
    "id": 23711,
    "content": "Const banks  Memory allocated in the constant address space of GPU memory resides in two dimensional arrays called constant banks"
  },
  {
    "id": 23712,
    "content": "The memory address of a given bank/offset pair is obtained via the convenience function $_cuda_const_bank(bank, offset)"
  },
  {
    "id": 23713,
    "content": "(cuda-gdb) disass $pc,+16 Dump of assembler code from 0x7fffd5043d40 to 0x7fffd5043d50: => 0x00007fffd5043d40 : MOV R0, c[0x0][0xc] End of assembler dump"
  },
  {
    "id": 23714,
    "content": "Event Notifications  As the application is making forward progress, CUDA-GDB notifies the users about kernel events and context events Within CUDA-GDB, kernel refers to the device code that executes on the GPU, while context refers to the virtual address space on the GPU for the kernel You can enable output of CUDA context and kernel events to review the flow of the active contexts and kernels"
  },
  {
    "id": 23718,
    "content": "Context Events  Any time a CUDA context is created, pushed, popped, or destroyed by the application, CUDA-GDB can optionally display a notification message [Context Create of context 0xad2fe60 on Device 0] [Context Destroy of context 0xad2fe60 on Device 0] By default, context event notification is disabled (cuda-gdb) set cuda context_events off CUDA-GDB does not display the context event"
  },
  {
    "id": 23719,
    "content": "notification messages (default) (cuda-gdb) set cuda context_events on CUDA-GDB displays the context event notification messages"
  },
  {
    "id": 23722,
    "content": "Kernel Events  Any time CUDA-GDB is made aware of the launch or the termination of a CUDA kernel, a notification message can be displayed The message includes the kernel id, the kernel name, and the device to which the kernel belongs [Launch of CUDA Kernel 1 (kernel3) on Device 0] [Termination of CUDA Kernel 1 (kernel3) on Device 0] The kernel event notification policy is controlled with"
  },
  {
    "id": 23723,
    "content": "kernel_events and kernel_events_depth options (cuda-gdb) set cuda kernel_events none Possible options are: none no kernel, application or system (default) application kernel launched by the user application system any kernel launched by the driver, such as memset all any kernel, application and system (cuda-gdb) set cuda kernel_events_depth 0 Controls the maximum depth of the kernels after which"
  },
  {
    "id": 23724,
    "content": "no kernel event notifications will be displayed A value of zero means that there is no maximum and that all the kernel notifications are displayed A value of one means that the debugger will display kernel event notifications only for kernels launched from the CPU (default)"
  },
  {
    "id": 23727,
    "content": "Checking API Errors  CUDA-GDB can automatically check the return code of any driver API or runtime API call Three modes are supported: hide CUDA API call failures are not reported ignore Warning message is printed for every fatal CUDA API call failure (default) stop The application is stopped when a CUDA API call returns a fatal error ignore_all Warning message is printed for every CUDA API call"
  },
  {
    "id": 23728,
    "content": "failure stop_all The application is stopped when a CUDA API call returns any error Note The success return code and other non-error return codes are ignored For the runtime API, they are cudaSuccess and cudaErrorNotReady"
  },
  {
    "id": 23731,
    "content": "GPU Error Reporting  With improved GPU error reporting in CUDA-GDB, application bugs are now easier to identify and easy to fix"
  },
  {
    "id": 23732,
    "content": "The following table shows the new errors that are reported on GPUs with compute capability sm_20 and higher"
  },
  {
    "id": 23733,
    "content": "Note Continuing the execution of your application after these errors are found can lead to application termination or indeterminate results"
  },
  {
    "id": 23734,
    "content": "Note Warp errors may result in instructions to continue executing before the exception is recognized and reported The reported $errorpc shall contain the precise address of the instruction that caused the exception If the warp exits after the instruction causing exception has executed, but before the exception has been recognized and reported, it may result in the exception not being reported"
  },
  {
    "id": 23735,
    "content": "To help avoid this scenario of unreported exceptions: For Volta+ architectures, compile the application with -G"
  },
  {
    "id": 23737,
    "content": "CUDA Exception Codes  Exception Code Precision of the Error Scope of the Error Description CUDA_EXCEPTION_0 : “Device Unknown Exception” Unknown Global error on the GPU This is a global GPU error caused by the application which does not match any of the listed error codes below"
  },
  {
    "id": 23738,
    "content": "Potentially, this may be due to Device Hardware Stack overflows or a kernel generating an exception very close to its termination"
  },
  {
    "id": 23739,
    "content": "CUDA_EXCEPTION_1 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0"
  },
  {
    "id": 23740,
    "content": "CUDA_EXCEPTION_2 : “Lane User Stack Overflow” Precise Per lane/thread error This occurs when a thread exceeds its stack memory limit CUDA_EXCEPTION_3 : “Device Hardware Stack Overflow” Precise Global error on the GPU This occurs when the application triggers a global hardware stack overflow"
  },
  {
    "id": 23742,
    "content": "CUDA_EXCEPTION_4 : “Warp Illegal Instruction” Precise Warp error This occurs when any thread within a warp has executed an illegal instruction CUDA_EXCEPTION_5 : “Warp Out-of-range Address” Precise Warp error This occurs when any thread within a warp accesses an address that is outside the valid range of local or shared memory regions CUDA_EXCEPTION_6 : “Warp Misaligned Address” Precise Warp"
  },
  {
    "id": 23743,
    "content": "error This occurs when any thread within a warp accesses an address in the local or shared memory segments that is not correctly aligned CUDA_EXCEPTION_7 : “Warp Invalid Address Space” Precise Warp error This occurs when any thread within a warp executes an instruction that accesses a memory space not permitted for that instruction CUDA_EXCEPTION_8 : “Warp Invalid PC” Precise Warp error This"
  },
  {
    "id": 23744,
    "content": "occurs when any thread within a warp advances its PC beyond the 40-bit address space CUDA_EXCEPTION_9 : “Warp Hardware Stack Overflow” Precise Warp error This occurs when any thread in a warp triggers a hardware stack overflow CUDA_EXCEPTION_10 : “Device Illegal Address” Precise Global error This occurs when a thread accesses an illegal(out of bounds) global address"
  },
  {
    "id": 23745,
    "content": "CUDA_EXCEPTION_11 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0"
  },
  {
    "id": 23746,
    "content": "CUDA_EXCEPTION_12 : “Warp Assert” Precise Per warp This occurs when any thread in the warp hits a device side assertion"
  },
  {
    "id": 23747,
    "content": "CUDA_EXCEPTION_13 : “Deprecated” Deprecated Deprecated This exception is deprecated and should be treated as CUDA_EXCEPTION_0"
  },
  {
    "id": 23748,
    "content": "CUDA_EXCEPTION_14 : “Warp Illegal Address” Precise Per warp This occurs when a thread accesses an illegal(out of bounds) global/local/shared address CUDA_EXCEPTION_15 : “Invalid Managed Memory Access” Precise Per host thread This occurs when a host thread attempts to access managed memory currently used by the GPU CUDA_EXCEPTION_17 : “Cluster Out-of-range Address” Not precise Per Cuda Cluster"
  },
  {
    "id": 23749,
    "content": "This occurs when any thread within a block accesses an address that is outside the valid range of shared memory regions belonging to the cluster CUDA_EXCEPTION_18 : “Cluster Target Block Not Present” Not precise Per Cuda Cluster This occurs when any thread within a block accesses another block that is outside the valid range of blocks belonging to the cluster"
  },
  {
    "id": 23752,
    "content": "Autostep  Autostep is a command to increase the precision of CUDA exceptions to the exact lane and instruction, when they would not have been otherwise"
  },
  {
    "id": 23753,
    "content": "Under normal execution, an exception may be reported several instructions after the exception occurred, or the exact thread where an exception occurred may not be known unless the exception is a lane error However, the precise origin of the exception can be determined if the program is being single-stepped when the exception occurs"
  },
  {
    "id": 23754,
    "content": "Single- stepping manually is a slow and tedious process; stepping takes much longer than normal execution and the user has to single-step each warp individually"
  },
  {
    "id": 23755,
    "content": "Autostep aides the user by allowing them to specify sections of code where they suspect an exception could occur, and these sections are automatically and transparently single- stepped the program is running"
  },
  {
    "id": 23757,
    "content": "The precise origin of an exception will be reported if the exception occurs within these sections Thus the exact instruction and thread where an exception occurred can be found quickly and with much less effort by using autostep Autostep Usage autostep [LOCATION] autostep [LOCATION] for LENGTH [lines|instructions] LOCATION may be anything that you use to specify the location of a breakpoint, such"
  },
  {
    "id": 23758,
    "content": "as a line number, function name, or an instruction address preceded by an asterisk LENGTH specifies the size of the autostep window in number of lines or instructions ( lines and instructions can be shortened, e"
  },
  {
    "id": 23761,
    "content": "In case of divergence, the length of the autostep window is determined by the number of lines or instructions the first active lane in each warp executes Divergent lanes are also single stepped, but the instructions they execute do not count towards the length of the autostep window If a breakpoint occurs while inside an autostep window, the warp where the breakpoint was hit will not continue"
  },
  {
    "id": 23762,
    "content": "autostepping when the program is resumed If an autostep is encountered while another autostep is being executed, then the second autostep is ignored If an autostep is set before the location of a memory error and no memory error is hit, then it is possible that the chosen window is too small This may be caused by the presence of function calls between the address of the autostep location and the"
  },
  {
    "id": 23763,
    "content": "instruction that triggers the memory error In that situation, either increase the size of the window to make sure that the faulty instruction is included, or move to the autostep location to an instruction that will be executed closer in time to the faulty instruction"
  },
  {
    "id": 23764,
    "content": "Related Commands Autosteps and breakpoints share the same numbering so most commands that work with breakpoints will also work with autosteps (cuda-gdb) info autosteps Num Type Disp Enb Address What 1 autostep keep y 0x0000000000401234 in merge at sort cu:30 for 49 instructions 3 autostep keep y 0x0000000000489913 in bubble at sort cu:94 for 11 lines disable autosteps disables an autostep"
  },
  {
    "id": 23765,
    "content": "ignore n i tells the debugger to not single-step the next i times the debugger enters the window for autostep n"
  },
  {
    "id": 23768,
    "content": "Walk-Through Examples  The chapter contains two CUDA-GDB walk-through examples: Example: bitreverse Example: autostep Example: MPI CUDA Application 11"
  },
  {
    "id": 23770,
    "content": "Example: bitreverse  This section presents a walk-through of CUDA-GDB by debugging a sample application–called bitreverse –that performs a simple 8 bit reversal on a data set Source Code 1 #include 2 #include 3 4 Simple 8-bit bit reversal Compute test 5 6 #define N 256 7 8 __global__ void bitreverse(void *data) { 9 unsigned int *idata = (unsigned int*)data; 10 extern __shared__ int array[]; 11"
  },
  {
    "id": 23771,
    "content": "12 array[threadIdx x] = idata[threadIdx x]; 13 14 array[threadIdx x] = ((0xf0f0f0f0 & array[threadIdx x]) >> 4) | 15 ((0x0f0f0f0f & array[threadIdx x]) > 2) | 17 ((0x33333333 & array[threadIdx x]) > 1) | 19 ((0x55555555 & array[threadIdx x]) >>(d); 36 37 cudaMemcpy(odata, d, sizeof(int)*N, 38 cudaMemcpyDeviceToHost); 39 40 for (i = 0; i %u \", idata[i], odata[i]); 42 43 cudaFree((void*)d); 44"
  },
  {
    "id": 23775,
    "content": "Walking through the Code  Begin by compiling the bitreverse cu CUDA application for debugging by entering the following command at a shell prompt: $ nvcc -g -G bitreverse cu -o bitreverse This command assumes that the source file name is bitreverse"
  },
  {
    "id": 23777,
    "content": "See also Debug Compilation Start the CUDA debugger by entering the following command at a shell prompt: $ cuda-gdb bitreverse Set breakpoints"
  },
  {
    "id": 23779,
    "content": "(cuda-gdb) run Starting program: /Users/CUDA_User1/docs/bitreverse Reading symbols for shared libraries"
  },
  {
    "id": 23782,
    "content": "cu:25 25 void *d = NULL; int i; At this point, commands can be entered to advance execution or to print the program state"
  },
  {
    "id": 23783,
    "content": "done [Context Create of context 0x80f200 on Device 0] [Launch of CUDA Kernel 0 (bitreverse>>) on Device 0] Breakpoint 3 at 0x8667b8: file bitreverse"
  },
  {
    "id": 23785,
    "content": "[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0] Breakpoint 2, bitreverse>> (data=0x110000) at bitreverse"
  },
  {
    "id": 23786,
    "content": "cu:9 9 unsigned int *idata = (unsigned int*)data; CUDA−GDB has detected that a CUDA device kernel has been reached"
  },
  {
    "id": 23787,
    "content": "Since thread ( 0,0,0 ) reverses the value of 0 , switch to a different thread to show more interesting data: (cuda-gdb) cuda thread 170 [Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (170,0,0), device 0, sm 0, warp 5, lane 10] Delete the breakpoints and continue the program to completion: (cuda-gdb) delete breakpoints Delete all breakpoints (y or n) y (cuda-gdb) continue"
  },
  {
    "id": 23789,
    "content": "Example: autostep  This section shows how to use the autostep command and demonstrates how it helps increase the precision of memory error reporting"
  },
  {
    "id": 23790,
    "content": "This will cause CUDA_EXCEPTION_10 \"Device Illegal Address\" to be thrown when we try to access the integer that corresponds with block 3, thread 39 This exception should occur at line 16 when we try to write to that value"
  },
  {
    "id": 23794,
    "content": "Debugging with Autosteps  Compile the example and start CUDA−GDB as normal We begin by running the program: (cuda-gdb) run Starting program: /home/jitud/cudagdb_test/autostep_ex/example [Thread debugging using libthread_db enabled] [New Thread 0x7ffff5688700 (LWP 9083)] [Context Create of context 0x617270 on Device 0] [Launch of CUDA Kernel 0 (example>>) on Device 0] Program received signal"
  },
  {
    "id": 23795,
    "content": "CUDA_EXCEPTION_10, Device Illegal Address [Switching focus to CUDA kernel 0, grid 1, block (1,0,0), thread (0,0,0), device 0, sm 1, warp 0, lane 0] 0x0000000000796f60 in example (data=0x200300000) at example"
  },
  {
    "id": 23796,
    "content": "cu:17 17 *(data[idx1]) = value3; As expected, we received a CUDA_EXCEPTION_10 Since CUDA_EXCEPTION_10 is a Global error, there is no thread information that is reported, so we would manually have to inspect all 512 threads To get more accurate information, we reason that since CUDA_EXCEPTION_10 is a memory access error, it must occur on code that accesses memory"
  },
  {
    "id": 23797,
    "content": "This happens on lines 11, 12, 16, 17, and 18, so we set two autostep windows for those areas: (cuda-gdb) autostep 11 for 2 lines Breakpoint 1 at 0x796d18: file example"
  },
  {
    "id": 23799,
    "content": "Created autostep of length 2 lines (cuda-gdb) autostep 16 for 3 lines Breakpoint 2 at 0x796e90: file example"
  },
  {
    "id": 23801,
    "content": "Created autostep of length 3 lines Finally, we run the program again with these autosteps: (cuda-gdb) run The program being debugged has been started already"
  },
  {
    "id": 23803,
    "content": "(y or n) y [Termination of CUDA Kernel 0 (example>>) on Device 0] Starting program: /home/jitud/cudagdb_test/autostep_ex/example [Thread debugging using libthread_db enabled] [New Thread 0x7ffff5688700 (LWP 9089)] [Context Create of context 0x617270 on Device 0] [Launch of CUDA Kernel 1 (example>>) on Device 0] [Switching focus to CUDA kernel 1, grid 1, block (0,0,0), thread (0,0,0), device 0, sm"
  },
  {
    "id": 23804,
    "content": "0, warp 0, lane 0] Program received signal CUDA_EXCEPTION_10, Device Illegal Address [Current focus set to CUDA kernel 1, grid 1, block (3,0,0), thread (32,0,0), device 0, sm 1, warp 3, lane 0] Autostep precisely caught exception at example cu:16 (0x796e90) This time we correctly caught the exception at line 16 Even though CUDA_EXCEPTION_10 is a global error, we have now narrowed it down to a warp"
  },
  {
    "id": 23805,
    "content": "error, so we now know that the thread that threw the exception must have been in the same warp as block 3, thread 32 In this example, we have narrowed down the scope of the error from 512 threads down to 32 threads just by setting two autosteps and re−running the program"
  },
  {
    "id": 23808,
    "content": "Example: MPI CUDA Application  For large scale MPI CUDA application debugging, NVIDIA recommends using parallel debuggers supplied by our partners Allinea and Totalview However, for debugging smaller applications, or for debugging just a few processes in a large application, CUDA-GDB can be used"
  },
  {
    "id": 23809,
    "content": "If the cluster nodes have xterm support, launch CUDA-GDB in the same way you would launch gdb with your job launcher For example: $ mpirun -np 4 -host nv1,nv2 xterm -e cuda-gdb a out You may have to export the DISPLAY variable to make sure that the xterm finds its way back to your display For example: $ mpirun -np 4 -host nv1,nv2 -x DISPLAY=host"
  },
  {
    "id": 23813,
    "content": "When xterm is not supported by your cluster environment, you can insert a spin loop inside your program, ssh to the compute node(s), and attach onto the MPI processes Somewhere near the start of your program, add a code snippet similar to the following: { int i = 0 ; char host [ 256 ]; printf ( \"PID %d on node %s is ready for attach \" , getpid (), host ); fflush ( stdout ); while ( 0 == i ) {"
  },
  {
    "id": 23815,
    "content": "Set the variable i to 1 to break out of the loop: $ mpirun -np 2 -host nv1,nv2 a out PID 20060 on node nv1 is ready for attach PID 5488 on node nv2 is ready for attach $ ssh nv1 [nv1]$ cuda-gdb --pid 5488 $ ssh nv2 [nv2]$ cuda-gdb --pid 20060 For larger applications, you can conditionalize the spin loop based on the MPI rank using the MPI_Comm_rank function"
  },
  {
    "id": 23817,
    "content": "0, the software preemption workaround described in Multiple Debuggers does not work with MPI applications If CUDA_VISIBLE_DEVICES is set, it may cause problems with the GPU selection logic in the MPI application"
  },
  {
    "id": 23820,
    "content": "Tips and Tricks  This section serves as reference to advanced settings and various tips and tricks users of CUDA-GDB can utilize which are not documented elsewhere"
  },
  {
    "id": 23823,
    "content": "set cuda break_on_launch  To break on the first instruction of every launched kernel, set the break_on_launch option to application: (cuda-gdb) set cuda break_on_launch application Possible options are: none no kernel, application or system (default) application kernel launched by the user application system any kernel launched by the driver, such as memset all any kernel, application and system"
  },
  {
    "id": 23824,
    "content": "Those automatic breakpoints are not displayed by the info breakpoints command and are managed separately from individual breakpoints Turning off the option will not delete other individual breakpoints set to the same address and vice-versa"
  },
  {
    "id": 23827,
    "content": "set cuda launch_blocking  When enabled, the kernel launches are synchronous as if the environment variable CUDA_LAUNCH_BLOCKING had been set to 1 (cuda-gdb) set cuda launch_blocking off The kernel launches are launched synchronously or asynchronously as dictacted by the application"
  },
  {
    "id": 23828,
    "content": "If the application has already started, the change will only take affect after the current session has terminated"
  },
  {
    "id": 23831,
    "content": "set cuda notify  Any time a CUDA event occurs, the debugger needs to be notified The host thread to receive that special signal is determined with the set cuda notify option (cuda-gdb) set cuda notify youngest The host thread with the smallest thread id will receive the notification signal (default) (cuda-gdb) set cuda notify random An arbitrary host thread will receive the notification signal"
  },
  {
    "id": 23834,
    "content": "set cuda ptx_cache  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC On CUDA devices, the variables may not be live all the time and will be reported as “Optimized Out” CUDA-GDB offers an option to circumvent this limitation by caching the value of the variable at the PTX register level Each source variable is compiled into a PTX"
  },
  {
    "id": 23835,
    "content": "register, which is later mapped to one or more hardware registers Using the debug information emitted by the compiler, the debugger may be able cache the value of a PTX register based on the latest hardware register it was mapped to at an earlier time When enabled, the cached value will be displayed as the normal value read from an actual hardware register and indicated with the (cached) prefix"
  },
  {
    "id": 23839,
    "content": "set cuda single_stepping_optimizations  Single-stepping can take a lot of time When enabled, this option tells the debugger to use safe tricks to accelerate single-stepping (cuda-gdb) set cuda single_stepping_optimizations off The debugger will not try to accelerate single-stepping (cuda-gdb) set cuda single_stepping_optimizations on The debugger will use safe techniques to accelerate"
  },
  {
    "id": 23845,
    "content": "set cuda thread_selection  When the debugger must choose an active thread to focus on, the decision is guided by a heuristics (cuda-gdb) set cuda thread_selection logical The thread with the lowest blockIdx/threadIdx coordinates is selected (cuda-gdb) set cuda thread_selection physical The thread with the lowest dev/sm/warp/lane coordinates is selected"
  },
  {
    "id": 23848,
    "content": "set cuda value_extrapolation  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC CUDA-GDB offers an option to opportunistically circumvent this limitation by extrapolating the value of a variable when the debugger would otherwise mark it as optimized out If the register that was used to store the value of a variable has been reused"
  },
  {
    "id": 23849,
    "content": "since the last time the variable was seen as live, then the reported value will be wrong (cuda-gdb) set cuda value_extrapolation off The debugger only read the value of live variables (cuda-gdb) set cuda value_extrapolation on The debugger will attempt to extrapolate the value of variables beyound their respecitve live ranges"
  },
  {
    "id": 23853,
    "content": "Debugging Docker Containers  When debugging an application within a Docker container, the PTRACE capability needs to be enabled"
  },
  {
    "id": 23855,
    "content": "To enable the PTRACE capability, add the following to your Docker run command: --cap-add=SYS_PTRACE 12"
  },
  {
    "id": 23857,
    "content": "Switching to Classic Debugger Backend  A new debugger backend named the Unified Debugger (UD) has been introduced on Linux platforms with the CTK 11"
  },
  {
    "id": 23859,
    "content": "UD allows for a unified debugger backend shared with debugging tools such as cuda-gdb and NVIDIA® Nsight™ VSE The previous debugger backend, known as the classic debugger backend, can still be used by setting CUDBG_USE_LEGACY_DEBUGGER to 1 in the environment before starting CUDA-GDB Users must switch to the classic debugger backend to debug their applications on Maxwell GPUs"
  },
  {
    "id": 23862,
    "content": "Thread Block Clusters  CUDA applications that make use of Thread Block Clusters will see the cluster index displayed in the CUDA focus"
  },
  {
    "id": 23863,
    "content": "Both cluster index and cluster dimension can be queried by printing the convenience variables clusterIdx and clusterDim"
  },
  {
    "id": 23866,
    "content": "Debugging OptiX/RTCore applications  When debugging programs built with OptiX/RTCore, it may be necessary to set the environment variable OPTIX_FORCE_DEPRECATED_LAUNCHER to 1"
  },
  {
    "id": 23867,
    "content": "If breakpoints are unable to be hit, try setting this environment variable before starting your application"
  },
  {
    "id": 23869,
    "content": "Debugging on Windows Subsystem for Linux  If you are unable to use the debugger on Windows Subsystem for Linux, make sure the debug interface is enabled by setting the registry key >HKEY_LOCAL_MACHINE\\SOFTWARE\\NVIDIA Corporation\\GPUDebugger\\EnableInterface to (DWORD) 1 13"
  },
  {
    "id": 23870,
    "content": "Supported Platforms  Host Platform Requirements CUDA-GDB is supported on all the platforms supported by the CUDA toolkit with which it is shipped GPU Requirements Debugging is supported on all CUDA-capable GPUs supported by the current CUDA release GDB Python integration GDB Python integration is supported in cuda-gdb with a multiple builds mechanism in order to support multiple python3"
  },
  {
    "id": 23871,
    "content": "interpreters across different platforms The cuda-gdb program is a shell script that selects the associated supported cuda-gdb binary based on the version of python available on the system Support exists for the following Python versions: Python 3 8 , Python 3 9 , Python 3 10 , Python 3 11 , and Python 3 12 Windows Subsystem for Linux (WSL) cuda-gdb supports debugging CUDA application on WSL2"
  },
  {
    "id": 23872,
    "content": "Make sure this capability is enabled via the registry key >HKEY_LOCAL_MACHINE\\SOFTWARE\\NVIDIA Corporation\\GPUDebugger\\EnableInterface set to (DWORD) 1"
  },
  {
    "id": 23875,
    "content": "Common Issues on Supported Operating Systems  The following are known issues with the current release on supported operating systems and how to fix them"
  },
  {
    "id": 23884,
    "content": "10 Specific commands to install the proper libpython are below RHEL 8/9 $ sudo yum -y install python3-libs Debian 10/11/12 $ sudo apt-get -y install libpython3-stdlib Fedora 39 $ sudo yum -y install python3-libs OpenSUSE 15 $ sudo zypper install -y libpython3 Ubuntu 20"
  },
  {
    "id": 23888,
    "content": "5 has a known issue when CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 that will prevent coredump file generation and may cause a segmentation fault"
  },
  {
    "id": 23889,
    "content": "Setting a breakpoint on a line within a __device__ or __global__ function before its module is loaded may result in the breakpoint being temporarily set on the first line of a function below in the source code As soon as the module for the targeted function is loaded, the breakpoint will be reset properly In those situations, the breakpoint can be safely ignored, and the application can be resumed"
  },
  {
    "id": 23890,
    "content": "When remotely debugging 32-bit applications on a 64-bit server, the cuda-gdbserver binary used must be 32-bit"
  },
  {
    "id": 23891,
    "content": "Attaching to a CUDA application with Software Preemption enabled in cuda-gdb is not supported Attaching to the MPS server process (nvidia-cuda-mps-server) using cuda-gdb, or starting the MPS server with cuda-gdb is not supported If a CUDA application is started in the MPS client mode with cuda-gdb, the MPS client will wait until all other MPS clients have terminated, and will then run as non-MPS"
  },
  {
    "id": 23893,
    "content": "Significant performance degradation will occur when the debugger steps over inlined routines Because inlined code blocks may have multiple exit points, under the hood, the debugger steps every single instruction until an exit point is reached, which incurs considerable cost for large routines"
  },
  {
    "id": 23894,
    "content": "The following actions are recommended to avoid this problem: Avoid using __forceinline__ when declaring a function"
  },
  {
    "id": 23895,
    "content": "(For code is compiled with debug information, only routines declared with the __forceinline__ keyword are actually inlined) Use the until command to step over inlined subroutines"
  },
  {
    "id": 23896,
    "content": "On Jetson, calls to the cuda API might result in the debugger jumping to _dl_catch_exception() On Jetson and Drive devices GPU debugging works correctly only if the debugger is run with the root permissions Changes to devfs node permissions are required for the debugger to work without running as root"
  },
  {
    "id": 23897,
    "content": "Debugger can miss reporting an induced trap( __trap() ) in case it is the next instruction executed after the device resumes from a breakpoint Debugger can miss reporting breakpoints or exceptions during resume in case new warps are launched on a previously empty SM"
  },
  {
    "id": 23898,
    "content": "Use of Python scripting functionality will expose cuda-gdb to the same vulnerabilities as those in the system libpython version"
  },
  {
    "id": 23899,
    "content": "Debugger doesn’t support accesses to shared memory allocations that are imported from other processes using the CUDA IPC APIs Attempts to access these shared memory allocations by the debugger will result in an error stating access to memory allocations shared via IPC is not supported"
  },
  {
    "id": 23900,
    "content": "break_on_launch will not function with OptiX/RTCore programs unless OPTIX_FORCE_DEPRECATED_LAUNCHER is set to 1"
  },
  {
    "id": 23901,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 23902,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 23904,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 23905,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 23906,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 23907,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 23908,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 23909,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 23910,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 23911,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 23912,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 23913,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 23914,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 23921,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 23923,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 23924,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 23929,
    "content": "--suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning ) 4"
  },
  {
    "id": 23935,
    "content": "5 | PDF | Archive NVIDIA CUDA Compiler Driver NVCC The documentation for nvcc , the CUDA compiler driver"
  },
  {
    "id": 23936,
    "content": "CUDA Programming Model  The CUDA Toolkit targets a class of applications whose control part runs as a process on a general purpose computing device, and which use one or more NVIDIA GPUs as coprocessors for accelerating single program, multiple data (SPMD) parallel jobs"
  },
  {
    "id": 23937,
    "content": "Such jobs are self-contained, in the sense that they can be executed and completed by a batch of GPU threads entirely without intervention by the host process, thereby gaining optimal benefit from the parallel graphics hardware"
  },
  {
    "id": 23938,
    "content": "The GPU code is implemented as a collection of functions in a language that is essentially C++, but with some annotations for distinguishing them from the host code, plus annotations for distinguishing different types of data memory that exists on the GPU"
  },
  {
    "id": 23939,
    "content": "Such functions may have parameters, and they can be called using a syntax that is very similar to regular C function calling, but slightly extended for being able to specify the matrix of GPU threads that must execute the called function"
  },
  {
    "id": 23944,
    "content": "CUDA Sources  Source files for CUDA applications consist of a mixture of conventional C++ host code, plus GPU device functions The CUDA compilation trajectory separates the device functions from the host code, compiles the device functions using the proprietary NVIDIA compilers and assembler, compiles the host code using a C++ host compiler that is available, and afterwards embeds the compiled"
  },
  {
    "id": 23946,
    "content": "In the linking stage, specific CUDA runtime libraries are added for supporting remote SPMD procedure calling and for providing explicit GPU manipulation such as allocation of GPU memory buffers and host-GPU data transfer"
  },
  {
    "id": 23950,
    "content": "Purpose of NVCC  The compilation trajectory involves several splitting, compilation, preprocessing, and merging steps for each CUDA source file It is the purpose of nvcc , the CUDA compiler driver, to hide the intricate details of CUDA compilation from developers It accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the"
  },
  {
    "id": 23951,
    "content": "compilation process All non-CUDA compilation steps are forwarded to a C++ host compiler that is supported by nvcc , and nvcc translates its options to appropriate host compiler command line options"
  },
  {
    "id": 23954,
    "content": "Supported Host Compilers  A general purpose C++ host compiler is needed by nvcc in the following situations: During non-CUDA phases (except the run phase), because these phases will be forwarded by nvcc to this compiler During CUDA phases, for several preprocessing stages and host code compilation (see also The CUDA Compilation Trajectory ) nvcc assumes that the host compiler is installed with"
  },
  {
    "id": 23955,
    "content": "the standard method designed by the compiler provider If the host compiler installation is non-standard, the user must make sure that the environment is set appropriately and use relevant nvcc compile options The following documents provide detailed information about supported host compilers: NVIDIA CUDA Installation Guide for Linux NVIDIA CUDA Installation Guide for Microsoft Windows On all"
  },
  {
    "id": 23957,
    "content": "exe on Windows) found in the current execution search path will be used, unless specified otherwise with appropriate options (see File and Path Specifications ) Note, nvcc does not support the compilation of file paths that exceed the maximum path length limitations of the host system To support the compilation of long file paths, please refer to the documentation for your system"
  },
  {
    "id": 23961,
    "content": "NVCC Identification Macro  nvcc predefines the following macros: __NVCC__ Defined when compiling C/C++/CUDA source files __CUDACC_RDC__ Defined when compiling CUDA source files in relocatable device code mode (see NVCC Options for Separate Compilation ) __CUDACC_EWP__ Defined when compiling CUDA source files in extensible whole program mode (see Options for Specifying Behavior of Compiler/Linker"
  },
  {
    "id": 23962,
    "content": ") __CUDACC_DEBUG__ Defined when compiling CUDA source files in the device-debug mode (see Options for Specifying Behavior of Compiler/Linker )"
  },
  {
    "id": 23963,
    "content": "__CUDACC_RELAXED_CONSTEXPR__ Defined when the --expt-relaxed-constexpr flag is specified on the command line __CUDACC_EXTENDED_LAMBDA__ Defined when the --expt-extended-lambda or --extended-lambda flag is specified on the command line"
  },
  {
    "id": 23964,
    "content": "__NVCC_DIAG_PRAGMA_SUPPORT__ Defined when the CUDA frontend compiler supports diagnostic control with the nv_diag_suppress , nv_diag_error , nv_diag_warning , nv_diag_default , nv_diag_once , and nv_diagnostic pragmas"
  },
  {
    "id": 23967,
    "content": "NVCC Phases  A compilation phase is a logical translation step that can be selected by command line options to nvcc A single compilation phase can still be broken up by nvcc into smaller steps, but these smaller steps are just implementations of the phase: they depend on seemingly arbitrary capabilities of the internal tools that nvcc uses, and all of these internals may change with a new"
  },
  {
    "id": 23968,
    "content": "release of the CUDA Toolkit Hence, only compilation phases are stable across releases, and although nvcc provides options to display the compilation steps that it executes, these are for debugging purposes only and must not be copied and used in build scripts nvcc phases are selected by a combination of command line options and input file name suffixes, and the execution of these phases may be"
  },
  {
    "id": 23969,
    "content": "modified by other command line options In phase selection, the input file suffix defines the phase input, while the command line option defines the required output of the phase The following paragraphs list the recognized file name suffixes and the supported compilation phases A full explanation of the nvcc command line options can be found in NVCC Command Options"
  },
  {
    "id": 23972,
    "content": "Supported Input File Suffixes  The following table defines how nvcc interprets its input files: Input File Suffix Description"
  },
  {
    "id": 23977,
    "content": "ptx PTX intermediate assembly file (see Figure 1 ) cubin CUDA device code binary file (CUBIN) for a single GPU architecture (see Figure 1 ) fatbin CUDA fat binary file that may contain multiple PTX and CUBIN files (see Figure 1 )"
  },
  {
    "id": 23981,
    "content": "lib Library file res Resource file so Shared object file Note that nvcc does not make any distinction between object, library or resource files"
  },
  {
    "id": 23985,
    "content": "Supported Phases  The following table specifies the supported compilation phases, plus the option to nvcc that enables the execution of each phase It also lists the default name of the output file generated by each phase, which takes effect when no explicit output file name is specified using the option --output-file : Phase nvcc Option Default Output File Name Long Name Short Name CUDA"
  },
  {
    "id": 23992,
    "content": "This output file can be compiled by the host compiler that was used by nvcc to preprocess the cu file C/C++ preprocessing --preprocess -E C/C++ compilation to object file --compile -c Source file name with suffix replaced by o on Linux or obj on Windows Cubin generation from CUDA source files --cubin -cubin Source file name with suffix replaced by cubin Cubin generation from PTX intermediate"
  },
  {
    "id": 23993,
    "content": "files --cubin -cubin Source file name with suffix replaced by cubin PTX generation from CUDA source files --ptx -ptx Source file name with suffix replaced by ptx Fatbinary generation from source, PTX or cubin files --fatbin -fatbin Source file name with suffix replaced by fatbin Linking relocatable device code"
  },
  {
    "id": 23995,
    "content": "o on other platforms Cubin generation from linked relocatable device code --device-link --cubin -dlink -cubin a_dlink cubin Fatbinary generation from linked relocatable device code --device-link --fatbin -dlink -fatbin a_dlink fatbin Linking an executable a"
  },
  {
    "id": 23997,
    "content": "out on other platforms Constructing an object file archive, or library --lib -lib a lib on Windows or a"
  },
  {
    "id": 23998,
    "content": "a on other platforms make dependency generation --generate-dependencies -M make dependency generation without headers in system paths"
  },
  {
    "id": 23999,
    "content": "--optix-ir -optix-ir Source file name with suffix replaced by optixir Running an executable --run -run Notes: The last phase in this list is more of a convenience phase"
  },
  {
    "id": 24000,
    "content": "It allows running the compiled and linked executable without having to explicitly set the library path to the CUDA dynamic libraries"
  },
  {
    "id": 24003,
    "content": "The CUDA Compilation Trajectory  CUDA compilation works as follows: the input program is preprocessed for device compilation and is compiled to CUDA binary ( cubin ) and/or PTX intermediate code, which are placed in a fatbinary The input program is preprocessed once again for host compilation and is synthesized to embed the fatbinary and transform CUDA specific C++ extensions into standard C++"
  },
  {
    "id": 24004,
    "content": "constructs Then the C++ host compiler compiles the synthesized host code with the embedded fatbinary into a host object The embedded fatbinary is inspected by the CUDA runtime system whenever the device code is launched by the host program to obtain an appropriate fatbinary image for the current GPU CUDA programs are compiled in the whole program compilation mode by default, i"
  },
  {
    "id": 24006,
    "content": ", the device code cannot reference an entity from a separate file For more information on the separate compilation and the whole program compilation, see Using Separate Compilation in CUDA"
  },
  {
    "id": 24007,
    "content": "Command Option Types and Notation  Each nvcc option has a long name and a short name, which are interchangeable with each other These two variants are distinguished by the number of hyphens that must precede the option name: long names must be preceded by two hyphens, while short names must be preceded by a single hyphen Long options are intended for use in build scripts, where the size of the"
  },
  {
    "id": 24008,
    "content": "option is less important than the descriptive value nvcc recognizes three types of command options: boolean options, single value options, and list options Boolean options do not have an argument; they are either specified on the command line or not Examples of each of these option types are, respectively: --verbose (switch to verbose mode), --output-file (specify output file), and --include-path"
  },
  {
    "id": 24009,
    "content": "(specify include path) Single value options and list options must have arguments, which must follow the name of the option itself by either one of more spaces or an equals character When a one-character short name such as -I , -l , and -L is used, the value of the option may also immediately follow the option itself without being seperated by spaces or an equal character The individual values of"
  },
  {
    "id": 24010,
    "content": "list options may be separated by commas in a single instance of the option, or the option may be repeated, or any combination of these two cases Hence, for the two sample options mentioned above that may take values, the following notations are legal: -o file -o=file -Idir1,dir2 -I=dir3 -I dir4,dir5 Unless otherwise specified, long option names are used throughout this document However, short"
  },
  {
    "id": 24014,
    "content": "Command Option Description  This section presents tables of nvcc options The option type in the tables can be recognized as follows: Boolean options do not have arguments specified in the first column, while the other two types do Long options are described in the first column of the options table, and short options occupy the second column"
  },
  {
    "id": 24027,
    "content": "--objdir-as-tempdir ( -objtemp )  Create all intermediate files in the same directory as the object file Using this option will ensure that the intermediate file name that is embedded in the object file will not change in multiple compiles of the same file If the same file is compiled with two different options, ex"
  },
  {
    "id": 24028,
    "content": ", ‘nvcc -c t cu’ and ‘nvcc -c -ptx t cu’, then the files should be compiled in different directories"
  },
  {
    "id": 24029,
    "content": "Compiling them in the same directory can either cause the compilation to fail or produce incorrect results"
  },
  {
    "id": 24034,
    "content": "--pre-include file, ( -include )  Specify header files that must be pre-included during preprocessing"
  },
  {
    "id": 24039,
    "content": "--library library, ( -l )  Specify libraries to be used in the linking stage without the library file extension The libraries are searched for on the library search paths that have been specified using option --library-path (see Libraries )"
  },
  {
    "id": 24045,
    "content": "name = definition - The contents of definition are tokenized and preprocessed as if they appear during translation phase three in a #define directive The definition will be truncated by embedded new line characters"
  },
  {
    "id": 24061,
    "content": "--output-directory directory ( -odir )  Specify the directory of the output file This option is intended for letting the dependency generation step (see --generate-dependencies ) generate a rule that defines the target object file in the proper directory"
  },
  {
    "id": 24066,
    "content": "--dependency-output file ( -MF )  Specify the dependency output file This option specifies the output file for the dependency generation step (see --generate-dependencies ) The option --generate-dependencies or --generate-nonystem-dependencies must be specified if a dependency output file is set"
  },
  {
    "id": 24071,
    "content": "--generate-dependency-targets ( -MP )  Add an empty target for each dependency This option adds phony targets to the dependency generation step (see --generate-dependencies ) intended to avoid makefile errors if old dependencies are deleted The input files are not emitted as phony targets"
  },
  {
    "id": 24076,
    "content": "--compiler-bindir directory ( -ccbin )  Specify the directory in which the default host compiler executable resides The host compiler executable name can be also specified to ensure that the correct host compiler is selected"
  },
  {
    "id": 24077,
    "content": "In addition, driver prefix options ( --input-drive-prefix , --dependency-drive-prefix , or --drive-prefix ) may need to be specified, if nvcc is executed in a Cygwin shell or a MinGW shell on Windows"
  },
  {
    "id": 24082,
    "content": "--allow-unsupported-compiler ( -allow-unsupported-compiler )  Disable nvcc check for supported host compiler versions Using an unsupported host compiler may cause compilation failure or incorrect run time execution"
  },
  {
    "id": 24088,
    "content": "--archiver-binary executable ( -arbin )  Specify the path of the archiver tool used create static library with --lib"
  },
  {
    "id": 24093,
    "content": "--cudart { none | shared | static } ( -cudart )  Specify the type of CUDA runtime library to be used: no CUDA runtime library, shared/dynamic CUDA runtime library, or static CUDA runtime library Allowed Values none shared static Default The static CUDA runtime library is used by default"
  },
  {
    "id": 24098,
    "content": "--cudadevrt { none | static } ( -cudadevrt )  Specify the type of CUDA device runtime library to be used: no CUDA device runtime library, or static CUDA device runtime library Allowed Values none static Default The static CUDA device runtime library is used by default"
  },
  {
    "id": 24103,
    "content": "--libdevice-directory directory ( -ldir )  Specify the directory that contains the libdevice library files Libdevice library files are located in the nvvm/libdevice directory in the CUDA Toolkit"
  },
  {
    "id": 24108,
    "content": "--target-directory string ( -target-dir )  Specify the subfolder name in the targets directory where the default include and library paths are located"
  },
  {
    "id": 24112,
    "content": "Options for Specifying the Compilation Phase  Options of this category specify up to which stage the input files must be compiled"
  },
  {
    "id": 24125,
    "content": "--lib ( -lib )  Compile all input files into object files, if necessary, and add the results to the specified library output file Default Output File Name a lib on Windows or a a on other platforms is used as the default output file name"
  },
  {
    "id": 24133,
    "content": "fatbin files into an object file with executable device code, which can be passed to the host linker"
  },
  {
    "id": 24134,
    "content": "Default Output File Name a_dlink obj on Windows or a_dlink o on other platforms is used as the default output file name When this option is used in conjunction with --fatbin , a_dlink fatbin is used as the default output file name When this option is used in conjunction with --cubin , a_dlink cubin is used as the default output file name"
  },
  {
    "id": 24144,
    "content": "cu input file into an object file that contains relocatable device code Default Output File Name The source file name extension is replaced by"
  },
  {
    "id": 24146,
    "content": "o on other platforms to create the default output file name For example, the default output file name for x"
  },
  {
    "id": 24171,
    "content": "ii is appended to the basename of the source file name to create the default output file name For example, the default output file name for x"
  },
  {
    "id": 24193,
    "content": "Default Output File Name The source file name extension is replaced by fatbin to create the default output file name For example, the default output file name for x"
  },
  {
    "id": 24203,
    "content": "Default Output File Name The source file name extension is replaced by cubin to create the default output file name For example, the default output file name for x"
  },
  {
    "id": 24212,
    "content": "Default Output File Name The source file name extension is replaced by ptx to create the default output file name For example, the default output file name for x"
  },
  {
    "id": 24230,
    "content": "--generate-dependencies ( -M )  Generate a dependency file that can be included in a Makefile for the"
  },
  {
    "id": 24236,
    "content": "nvcc uses a fixed prefix to identify dependencies in the preprocessed file ( ‘ #line 1 ’ on Linux and ‘ # 1 ’ on Windows) The files mentioned in source location directives starting with this prefix will be included in the dependency list"
  },
  {
    "id": 24241,
    "content": "--generate-nonsystem-dependencies ( -MM )  Same as --generate-dependencies but skip header files found in system directories (Linux only)"
  },
  {
    "id": 24246,
    "content": "--generate-dependencies-with-compile ( -MD )  Generate a dependency file and compile the input file The dependency file can be included in a Makefile for the"
  },
  {
    "id": 24251,
    "content": "cu input file The dependency file name is computed as follows: If -MF is specified, then the specified file is used as the dependency file name If -o is specified, the dependency file name is computed from the specified file name by replacing the suffix with ‘"
  },
  {
    "id": 24255,
    "content": "If the dependency file name is computed based on either -MF or -o , then multiple input files are not supported"
  },
  {
    "id": 24260,
    "content": "--generate-nonsystem-dependencies-with-compile ( -MMD )  Same as --generate-dependencies-with-compile but skip header files found in system directories (Linux only)"
  },
  {
    "id": 24261,
    "content": "This feature is not supported with link-time-optimization ( -dlto ), the lto_NN -arch target, or with -gencode"
  },
  {
    "id": 24262,
    "content": "Default Output File Name The source file name extension is replaced by optixir to create the default output file name For example, the default output file name for x"
  },
  {
    "id": 24270,
    "content": "This step is intended for developers who do not want to be bothered with setting the necessary environment variables; these are set temporarily by nvcc"
  },
  {
    "id": 24283,
    "content": "--debug ( -g )  Generate debug information for host code It is not intended for profiling; use --generate-line-info instead for profiling"
  },
  {
    "id": 24288,
    "content": "--extensible-whole-program ( -ewp )  Generate extensible whole program device code, which allows some calls to not be resolved until linking with libcudadevrt"
  },
  {
    "id": 24294,
    "content": "Inlining pass may be invoked multiple times by the compiler and a function not inlined in an earlier pass may be inlined in a subsequent pass"
  },
  {
    "id": 24300,
    "content": "When specified along with -G , enables limited debug information generation for optimized device code (currently, only line number information)"
  },
  {
    "id": 24306,
    "content": "--dlink-time-opt ( -dlto )  Perform link-time optimization of device code Link-time optimization must be specified at both compile and link time; at compile time it stores high-level intermediate code, then at link time it links together and optimizes the intermediate code"
  },
  {
    "id": 24307,
    "content": "The options -dlto -arch=sm_NN will add a lto_NN target; if you want to only add a lto_NN target and not the compute_NN that -arch=sm_NN usually generates, use -arch=lto_NN"
  },
  {
    "id": 24317,
    "content": "--split-compile number ( -split-compile )  [Experimental] Perform compiler optimizations in parallel Split compilation attempts to reduce compile time by enabling the compiler to run certain optimization passes concurrently It does this by splitting the device code into smaller translation units, each containing one or more device functions, and running optimization passes on each unit"
  },
  {
    "id": 24319,
    "content": "The option accepts a numerical value that specifies the maximum number of threads the compiler can use One can also allow the compiler to use the maximum threads available on the system by setting --split-compile=0 This option can work in conjunction with device Link Time Optimization ( -dlto ) as well as --threads"
  },
  {
    "id": 24324,
    "content": "--ftemplate-backtrace-limit limit ( -ftemplate-backtrace-limit )  Set the maximum number of template instantiation notes for a single warning or error to limit"
  },
  {
    "id": 24330,
    "content": "--ftemplate-depth limit ( -ftemplate-depth )  Set the maximum instantiation depth for template classes to limit"
  },
  {
    "id": 24332,
    "content": "exe) and “–fno-exceptions” (for other host compilers) during host compiler invocation These flags are added to the host compiler invocation before any flags passed directly to the host compiler with “-Xcompiler” Default (on Windows) On Windows, nvcc passes /EHsc to the host compiler by default"
  },
  {
    "id": 24338,
    "content": "--x { c | c++ | cu } ( -x )  Explicitly specify the language for the input files, rather than letting the compiler choose a default based on the file name suffix Allowed Values c c++ cu Default The language of the source code is determined based on the file name suffix"
  },
  {
    "id": 24343,
    "content": "--std { c++03 | c++11 | c++14 | c++17 | c++20 } ( -std )  Select a particular C++ dialect Allowed Values c++03 c++11 c++14 c++17 c++20 Default The default C++ dialect depends on the host compiler nvcc matches the default C++ dialect that the host compiler uses"
  },
  {
    "id": 24348,
    "content": "--no-host-device-initializer-list ( -nohdinitlist )  Do not consider member functions of std::initializer_list as __host__ __device__ functions implicitly"
  },
  {
    "id": 24353,
    "content": "--expt-relaxed-constexpr ( -expt-relaxed-constexpr )  Experimental flag : Allow host code to invoke ``__device__ constexpr`` functions, and device code to invoke ``__host__ constexpr`` functions"
  },
  {
    "id": 24359,
    "content": "--extended-lambda ( -extended-lambda )  Allow __host__ , __device__ annotations in lambda declarations"
  },
  {
    "id": 24374,
    "content": "--host-linker-script { use-lcs | gen-lcs } ( -hls )  Use the host linker script (GNU/Linux only) to enable support for certain CUDA specific requirements, while building executable files or shared libraries Allowed Values use-lcs Prepares a host linker script and enables host linker to support relocatable device object files that are larger in size, that would otherwise, in certain cases, cause"
  },
  {
    "id": 24375,
    "content": "the host linker to fail with relocation truncation error gen-lcs Generates a host linker script that can be passed to host linker manually, in the case where host linker is invoked separately outside of nvcc This option can be combined with -shared or -r option to generate linker scripts that can be used while generating host shared libraries or host relocatable links respectively The file"
  },
  {
    "id": 24376,
    "content": "generated using this options must be provided as the last input file to the host linker A linker script may already be in used and passed to the host linker using the host linker option --script (or -T ), then the generated host linker script must augment the existing linker script In such cases, the option -aug-hls must be used to generate linker script that contains only the augmentation parts A"
  },
  {
    "id": 24377,
    "content": "host linker option, such as -z with a non-default argument, that can modify the default linker script internally, is incompatible with this option and the behavior of any such usage is undefined Default Value use-lcs is used as the default type"
  },
  {
    "id": 24382,
    "content": "--augment-host-linker-script ( -aug-hls )  Enables generation of host linker script that augments an existing host linker script (GNU/Linux only) See option --host-linker-script for more details"
  },
  {
    "id": 24387,
    "content": "--host-relocatable-link ( -r )  When used in combination with -hls=gen-lcs , controls the behaviour of -hls=gen-lcs and sets it to generate host linker script that can be used in host relocatable link ( ld -r linkage) This option currently is effective only when used with -hls=gen-lcs ; in all other cases, this option is ignored currently"
  },
  {
    "id": 24391,
    "content": "Options for Passing Specific Phase Options  These flags allow for passing specific options directly to the internal compilation tools that nvcc encapsulates, without burdening nvcc with too-detailed knowledge on these tools ( -Xcompiler )  Specify options directly to the compiler/preprocessor"
  },
  {
    "id": 24406,
    "content": "--ptxas-options options, ( -Xptxas )  Specify options directly to ptxas , the PTX optimizing assembler"
  },
  {
    "id": 24419,
    "content": "--forward-unknown-to-host-compiler ( -forward-unknown-to-host-compiler )  Forward unknown options to the host compiler"
  },
  {
    "id": 24420,
    "content": "An ‘unknown option’ is a command line argument that starts with - followed by another character, and is not a recognized nvcc flag or an argument for a recognized nvcc flag If the unknown option is followed by a separate command line argument, the argument will not be forwarded, unless it begins with the - character For example: nvcc -forward-unknown-to-host-compiler -foo=bar a cu will forward"
  },
  {
    "id": 24421,
    "content": "-foo=bar to host compiler nvcc -forward-unknown-to-host-compiler -foo bar a cu will report an error for bar argument nvcc -forward-unknown-to-host-compiler -foo -bar a cu will forward -foo and -bar to host compiler"
  },
  {
    "id": 24426,
    "content": "--forward-unknown-to-host-linker ( -forward-unknown-to-host-linker )  Forward unknown options to the host linker For example: nvcc -forward-unknown-to-host-linker -foo=bar a cu will forward -foo=bar to host linker nvcc -forward-unknown-to-host-linker -foo -bar a cu will forward -foo and -bar to host linker"
  },
  {
    "id": 24431,
    "content": "--dont-use-profile ( -noprof )  Do not use configurations from the nvcc profile file for compilation"
  },
  {
    "id": 24436,
    "content": "--threads number ( -t )  Specify the maximum number of threads to be used to execute the compilation steps in parallel This option can be used to improve the compilation speed when compiling for multiple architectures If number is 0, the number of threads used is the number of CPUs on the machine"
  },
  {
    "id": 24451,
    "content": "--keep-dir directory ( -keep-dir )  Keep all intermediate files that are generated during internal compilation steps in this directory"
  },
  {
    "id": 24456,
    "content": "--clean-targets ( -clean )  Delete all the non-temporary files that the same nvcc command would generate without this option Instead, all of the non-temporary files that nvcc would otherwise create will be deleted"
  },
  {
    "id": 24461,
    "content": "--run-args arguments, ( -run-args )  Specify command line arguments for the executable when used in conjunction with --run"
  },
  {
    "id": 24467,
    "content": "This is done by executing the appropriate command file available for the MSVC installation detected or specified"
  },
  {
    "id": 24468,
    "content": "If the environment used to invoke nvcc has already been configured, this option can be used to skip this step"
  },
  {
    "id": 24474,
    "content": "On Windows, all command line arguments that refer to file names must be converted to the Windows native format before they are passed to pure Windows executables"
  },
  {
    "id": 24481,
    "content": "On Windows, when generating dependency files (see --generate-dependencies ), all file names must be converted appropriately for the instance of make that is used Some instances of make have trouble with the colon in absolute paths in the native Windows format, which depends on the environment in which the make instance has been compiled Or leave these file names in the native Windows format by"
  },
  {
    "id": 24487,
    "content": "--drive-prefix prefix ( -dp )  Specify the drive prefix This option specifies prefix as both --input-drive-prefix and --dependency-drive-prefix"
  },
  {
    "id": 24492,
    "content": "--dependency-target-name target ( -MT )  Specify the target name of the generated rule when generating a dependency file (see --generate-dependencies )"
  },
  {
    "id": 24497,
    "content": "--no-align-double  Specify that -malign-double should not be passed as a compiler argument on 32-bit platforms"
  },
  {
    "id": 24504,
    "content": "--default-stream { legacy | null | per-thread } ( -default-stream )  Specify the stream that CUDA commands from the compiled program will be sent to by default Allowed Values legacy The CUDA legacy stream (per context, implicitly synchronizes with other streams) per-thread Normal CUDA stream (per thread, does not implicitly synchronize with other streams) null Deprecated alias for legacy Default"
  },
  {
    "id": 24513,
    "content": "--gpu-architecture ( -arch )  Specify the name of the class of NVIDIA virtual GPU architecture for which the CUDA input files must be compiled With the exception as described for the shorthand below, the architecture specified with this option must be a virtual architecture (such as compute_50) Normally, this option alone does not trigger assembly of the generated PTX for a real architecture"
  },
  {
    "id": 24514,
    "content": "(that is the role of nvcc option --gpu-code , see below); rather, its purpose is to control preprocessing and compilation of the input to PTX For convenience, in case of simple nvcc compilations, the following shorthand is supported If no value for option --gpu-code is specified, then the value of this option defaults to the value of --gpu-architecture In this situation, as the only exception to"
  },
  {
    "id": 24515,
    "content": "the description above, the value specified for --gpu-architecture may be a real architecture (such as a sm_50), in which case nvcc uses the specified real architecture and its closest virtual architecture as the effective architecture values For example, nvcc --gpu-architecture=sm_50 is equivalent to nvcc --gpu-architecture=compute_50 --gpu-code=sm_50,compute_50 When -arch=native is specified,"
  },
  {
    "id": 24516,
    "content": "nvcc detects the visible GPUs on the system and generates codes for them, no PTX program will be generated for this option It is a warning if there are no visible supported GPU on the system, and the default architecture will be used If -arch=all is specified, nvcc embeds a compiled code image for all supported architectures (sm_*) , and a PTX program for the highest major virtual architecture For"
  },
  {
    "id": 24517,
    "content": "-arch=all-major , nvcc embeds a compiled code image for all supported major versions (sm_*0) , plus the earliest supported, and adds a PTX program for the highest major virtual architecture See Virtual Architecture Feature List for the list of supported virtual architectures and GPU Feature List for the list of supported real architectures Default sm_52 is used as the default value; PTX is"
  },
  {
    "id": 24523,
    "content": "--gpu-code code, nvcc embeds a compiled code image in the resulting executable for each specified code architecture, which is a true binary load image for each real architecture (such as sm_50), and PTX code for the virtual architecture (such as compute_50) During runtime, such embedded PTX code is dynamically compiled by the CUDA runtime system if no binary load image is found for the current"
  },
  {
    "id": 24524,
    "content": "GPU Architectures specified for options --gpu-architecture and --gpu-code may be virtual as well as real , but the code architectures must be compatible with the arch architecture When the --gpu-code option is used, the value for the --gpu-architecture option must be a virtual PTX architecture For instance, --gpu-architecture=compute_60 is not compatible with --gpu-code=sm_52 , because the earlier"
  },
  {
    "id": 24525,
    "content": "compilation stages will assume the availability of compute_60 features that are not present on sm_52"
  },
  {
    "id": 24530,
    "content": "--generate-code specification ( -gencode )  This option provides a generalization of the --gpu-architecture=arch --gpu-code=code, Where use of the previous options generates code for different real architectures with the PTX for the same virtual architecture, option --generate-code allows multiple PTX generations for different virtual architectures is equivalent to"
  },
  {
    "id": 24531,
    "content": "--generate-code=arch=arch,code=code, --generate-code options may be repeated for different virtual architectures"
  },
  {
    "id": 24536,
    "content": "--relocatable-device-code { true | false } ( -rdc )  Enable or disable the generation of relocatable device code Allowed Values true false Default The generation of relocatable device code is disabled"
  },
  {
    "id": 24541,
    "content": "--entries entry, PTX generated for all entry functions, but only the selected entry functions are assembled Default nvcc generates code for all entry functions"
  },
  {
    "id": 24546,
    "content": "--maxrregcount amount ( -maxrregcount )  Specify the maximum amount of registers that GPU functions can use"
  },
  {
    "id": 24547,
    "content": "Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism"
  },
  {
    "id": 24548,
    "content": "A value less than the minimum registers required by ABI will be bumped up by the compiler to ABI minimum limit User program may not be able to make use of all registers as some registers are reserved by compiler"
  },
  {
    "id": 24554,
    "content": "--use_fast_math ( -use_fast_math )  Make use of fast math library --use_fast_math implies --ftz=true --prec-div=false --prec-sqrt=false --fmad=true"
  },
  {
    "id": 24559,
    "content": "--ftz { true | false } ( -ftz )  Control single-precision denormals support Allowed Values true false Default This option is set to false and nvcc preserves denormal values"
  },
  {
    "id": 24564,
    "content": "--prec-div { true | false } ( -prec-div )  This option controls single-precision floating-point division and reciprocals --prec-div=true enables the IEEE round-to-nearest mode and --prec-div=false enables the fast approximation mode Allowed Values true false Default This option is set to true and nvcc enables the IEEE round-to-nearest mode"
  },
  {
    "id": 24569,
    "content": "--prec-sqrt { true | false } ( -prec-sqrt )  This option controls single-precision floating-point square root --prec-sqrt=true enables the IEEE round-to-nearest mode and --prec-sqrt=false enables the fast approximation mode --use_fast_math implies --prec-sqrt=false"
  },
  {
    "id": 24574,
    "content": "--fmad { true | false } ( -fmad )  This option enables (disables) the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA) Allowed Values true false Default This option is set to true and nvcc enables the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or"
  },
  {
    "id": 24580,
    "content": "--extra-device-vectorization ( -extra-device-vectorization )  This option enables more aggressive device code vectorization"
  },
  {
    "id": 24586,
    "content": "--keep-device-functions ( -keep-device-functions )  In whole program compilation mode, preserve user defined external linkage __device__ function definitions in generated PTX"
  },
  {
    "id": 24591,
    "content": "--jump-table-density percentage ( -jtd )  Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx"
  },
  {
    "id": 24617,
    "content": "--Wno-deprecated-gpu-targets ( -Wno-deprecated-gpu-targets )  Suppress warnings about deprecated GPU target architectures"
  },
  {
    "id": 24622,
    "content": "--Wno-deprecated-declarations ( -Wno-deprecated-declarations )  Suppress warning on use of a deprecated entity"
  },
  {
    "id": 24632,
    "content": "--Wdefault-stream-launch ( -Wdefault-stream-launch )  Generate warning when an explicit stream argument is not provided in the >> kernel launch syntax"
  },
  {
    "id": 24637,
    "content": "--Wmissing-launch-bounds ( -Wmissing-launch-bounds )  Generate warning when a __global__ function does not have an explicit __launch_bounds__ annotation"
  },
  {
    "id": 24642,
    "content": "--Wext-lambda-captures-this ( -Wext-lambda-captures-this )  Generate warning when an extended lambda implicitly captures this"
  },
  {
    "id": 24643,
    "content": "The following is the list of warning kinds accepted by this option: all-warnings Treat all warnings as errors"
  },
  {
    "id": 24644,
    "content": "The compiler will generate an error instead of a warning for a call from a __host__ __device__ to a __host__ function default-stream-launch Generate error when an explicit stream argument is not provided in the >> kernel launch syntax missing-launch-bounds Generate warning when a __global__ function does not have an explicit __launch_bounds__ annotation"
  },
  {
    "id": 24651,
    "content": "--display-error-number ( -err-no )  This option displays a diagnostic number for any message generated by the CUDA frontend compiler (note: not the host compiler)"
  },
  {
    "id": 24656,
    "content": "--no-display-error-number ( -no-err-no )  This option disables the display of a diagnostic number for any message generated by the CUDA frontend compiler (note: not the host compiler) ( -diag-error )  Emit error for specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor)"
  },
  {
    "id": 24661,
    "content": "--diag-suppress errNum, ( -diag-suppress )  Suppress specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor)"
  },
  {
    "id": 24666,
    "content": "--diag-warn errNum, ( -diag-warn )  Emit warning for specified diagnostic message(s) generated by the CUDA frontend compiler (note: does not affect diagnostics generated by the host compiler/preprocessor)"
  },
  {
    "id": 24672,
    "content": "This option implies --nvlink-options=--verbose when --relocatable-device-code=true is set Otherwise, it implies --ptxas-options=--verbose"
  },
  {
    "id": 24683,
    "content": "--time filename ( -time )  Generate a comma separated value table with the time taken by each compilation phase, and append it at the end of the file given as the option argument"
  },
  {
    "id": 24689,
    "content": "--qpp-config config ( -qpp-config )  Specify the configuration ([[compiler/]version,][target]) when using q++ host compiler"
  },
  {
    "id": 24695,
    "content": "--list-gpu-code ( -code-ls )  List the non-accelerated gpu architectures (sm_XX) supported by the tool and exit If both –list-gpu-code and –list-gpu-arch are set, the list is displayed using the same format as the –generate-code value"
  },
  {
    "id": 24700,
    "content": "--list-gpu-arch ( -arch-ls )  List the non-accelerated virtual device architectures (compute_XX) supported by the tool and exit If both –list-gpu-arch and –list-gpu-code are set, the list is displayed using the same format as the –generate-code value"
  },
  {
    "id": 24709,
    "content": "Ptxas Options  The following table lists some useful ptxas options which can be specified with nvcc option -Xptxas"
  },
  {
    "id": 24715,
    "content": "--allow-expensive-optimizations ( -allow-expensive-optimizations )  Enable (disable) to allow compiler to perform expensive optimizations using maximum available resources (memory and compile-time)"
  },
  {
    "id": 24728,
    "content": "--disable-optimizer-constants ( -disable-optimizer-consts )  Disable use of optimizer constant bank"
  },
  {
    "id": 24761,
    "content": "Allowed values for this option: compute_50 , compute_52 , compute_53 , compute_60 , compute_61 , compute_62 , compute_70 , compute_72 , compute_75 , compute_80 , compute_86 , compute_87 , compute_89 , compute_90 , lto_50 , lto_52 , lto_53 , lto_60 , lto_61 , lto_62 , lto_70 , lto_72 , lto_75 , lto_80 , lto_86 , lto_87 , lto_89 , lto_90 , sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 ,"
  },
  {
    "id": 24774,
    "content": "--maxrregcount amount ( -maxrregcount )  Semantics same as nvcc option --maxrregcount ( -optf )  Semantics same as nvcc option --options-file"
  },
  {
    "id": 24781,
    "content": "--preserve-relocs ( -preserve-relocs )  This option will make ptxas to generate relocatable references for variables and preserve relocations generated for them in linked executable"
  },
  {
    "id": 24794,
    "content": "--suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning )  Suppress the warning on use of multicast::cluster modifier on cp async bulk{"
  },
  {
    "id": 24807,
    "content": "--warn-on-double-precision-use ( -warn-double-usage )  Warning if double(s) are used in an instruction"
  },
  {
    "id": 24841,
    "content": "--override-directive-values ( -override-directive-values )  Override the PTX directives values by the corresponding option values"
  },
  {
    "id": 24848,
    "content": "--make-errors-visible-at-exit ( -make-errors-visible-at-exit )  Generate required instructions at exit point to make memory faults and errors visible at exit"
  },
  {
    "id": 24853,
    "content": "NVLINK Options  The following is a list of some useful nvlink options which can be specified with nvcc option --nvlink-options"
  },
  {
    "id": 24880,
    "content": "--suppress-arch-warning ( -suppress-arch-warning )  Suppress the warning that otherwise is printed when object does not contain code for target arch"
  },
  {
    "id": 24886,
    "content": "--suppress-stack-size-warning ( -suppress-stack-size-warning )  Suppress the warning that otherwise is printed when stack size cannot be determined"
  },
  {
    "id": 24898,
    "content": "--dump-callgraph-no-demangle ( -dump-callgraph-no-demangle )  Dump callgraph information without demangling"
  },
  {
    "id": 24916,
    "content": "--ignore-host-info ( -ignore-host-info )  Ignore information about host references, so don’t remove device code that could potentially be referenced by host"
  },
  {
    "id": 24930,
    "content": "--kernels-used ( -kernels-used )  Specify kernels that are used If this option is used, then any other kernels are considered dead-code and removed"
  },
  {
    "id": 24942,
    "content": "--suppress-debug-info ( -suppress-debug-info )  Do not preserve debug symbols in output This option is ignored if used without –debug option"
  },
  {
    "id": 24948,
    "content": "--variables-used ( -variables used )  Specify variables that are used If this option is used, then any other variables are considered dead-code and potentially removed unless have other accesses from device code"
  },
  {
    "id": 24951,
    "content": "NVCC Environment Variables  NVCC_PREPEND_FLAGS and NVCC_APPEND_FLAGS: The nvcc command line flags can be augmented using the following environment variables, if set: NVCC_PREPEND_FLAGS Flags to be injected before the normal nvcc command line For example, after setting: export NVCC_PREPEND_FLAGS='-G -keep -arch=sm_60' export NVCC_APPEND_FLAGS='-DNAME=\" foo \"' The following invocation: nvcc foo cu"
  },
  {
    "id": 24952,
    "content": "-o foo Becomes equivalent to: nvcc -G -keep -arch=sm_60 foo cu -o foo -DNAME=\" foo \" These environment variables can be useful for injecting nvcc flags globally without modifying build scripts The additional flags coming from either NVCC_PREPEND_FLAGS or NVCC_APPEND_FLAGS will be listed in the verbose log ( --verbose )"
  },
  {
    "id": 24953,
    "content": "NVCC_CCBIN: A default host compiler can be set using the environment variable NVCC_CCBIN For example, after setting: export NVCC_CCBIN='gcc' nvcc will choose gcc as the host compiler if --compiler-bindir is not set If NVCC_CCBIN and --compiler-bindir are both set, nvcc will choose the host compiler specified by --compiler-bindir For example: export NVCC_CCBIN='gcc' nvcc foo cu -ccbin='clang' -o"
  },
  {
    "id": 24956,
    "content": "GPU Compilation  This chapter describes the GPU compilation model that is maintained by nvcc , in cooperation with the CUDA driver"
  },
  {
    "id": 24960,
    "content": "GPU Generations  In order to allow for architectural evolution, NVIDIA GPUs are released in different generations New generations introduce major improvements in functionality and/or chip architecture, while GPU models within the same generation show minor configuration differences that moderately affect functionality, performance, or both"
  },
  {
    "id": 24961,
    "content": "For example, a CUDA application that has been compiled for a Fermi GPU will very likely not run on a Kepler GPU (and vice versa)"
  },
  {
    "id": 24962,
    "content": "This is because the instruction set and instruction encodings of a generation is different from those of other generations Binary compatibility within one GPU generation can be guaranteed under certain conditions because they share the basic instruction set"
  },
  {
    "id": 24963,
    "content": "This is the case when two GPU versions do not show functional differences (for instance when one version is a scaled down version of the other), or when one version is functionally included in the other An example of the latter is the base Maxwell version sm_50 whose functionality is a subset of all other Maxwell versions: any code compiled for sm_50 will run on all other Maxwell GPUs"
  },
  {
    "id": 24966,
    "content": "GPU Feature List  The following table lists the names of the current GPU architectures, annotated with the functional capabilities that they provide"
  },
  {
    "id": 24967,
    "content": "There are other differences, such as amounts of register and processor clusters, that only affect execution performance"
  },
  {
    "id": 24968,
    "content": "In the CUDA naming scheme, GPUs are named sm_xy , where x denotes the GPU generation number, and y the version in that generation"
  },
  {
    "id": 24969,
    "content": "Additionally, to facilitate comparing GPU capabilities, CUDA attempts to choose its GPU names such that if x1y1 can be used to implicitly call both the device and host linkers"
  },
  {
    "id": 24970,
    "content": "This works because if the device linker does not see any relocatable code it does not do anything Libraries  The device linker has the ability to read the static host library formats ("
  },
  {
    "id": 24973,
    "content": "The --library and --library-path options can be used to pass libraries to both the device and host linker The library name is specified without the library file extension when the --library option is used"
  },
  {
    "id": 24976,
    "content": "o --library-path= --library=foo Alternatively, the library name, including the library file extension, can be used without the --library option on Windows"
  },
  {
    "id": 24979,
    "content": "lib --library-path= Note that the device linker ignores any objects that do not have relocatable device code"
  },
  {
    "id": 24988,
    "content": "h\" __global__ void foo ( void ) { __shared__ int a [ N ]; a [ threadIdx x - 1 ]; bar (); } int main ( void ) { unsigned int i ; int * dg , hg [ N ]; int sum = 0 ; foo >> (); if ( cudaGetSymbolAddress (( void ** ) & dg , g )){ printf ( \"couldn't get the symbol addr \" ); return 1 ; } if ( cudaMemcpy ( hg , dg , N * sizeof ( int ), cudaMemcpyDeviceToHost )){ printf ( \"couldn't memcpy \" ); return 1 ;"
  },
  {
    "id": 24989,
    "content": "} for ( i = 0 ; i --library=cudart Note that all desired target architectures must be passed to the device linker, as that specifies what will be in the final executable (some objects or libraries may contain device code for multiple architectures, and the link step can then choose what to put in the final executable)"
  },
  {
    "id": 24990,
    "content": "If you want to use the driver API to load a linked cubin, you can request just the cubin: nvcc --gpu-architecture=sm_50 --device-link a"
  },
  {
    "id": 24993,
    "content": "cubin The objects could be put into a library and used with: nvcc --gpu-architecture=sm_50 --device-c a"
  },
  {
    "id": 24998,
    "content": "A PTX file can be compiled to a host object file and then linked by using: nvcc --gpu-architecture=sm_50 --device-c a ptx An example that uses libraries, host linker, and dynamic parallelism would be: nvcc --gpu-architecture=sm_50 --device-c a"
  },
  {
    "id": 25006,
    "content": "o --library=gpu --library-path= \\ --library=cudadevrt --library=cudart It is possible to do multiple device links within a single host executable, as long as each device link is independent of the other"
  },
  {
    "id": 25007,
    "content": "This requirement of independence means that they cannot share code across device executables, nor can they share addresses (e"
  },
  {
    "id": 25009,
    "content": ", a device function address can be passed from host to device for a callback only if the device link sees both the caller and potential callback callee; you cannot pass an address from one device executable to another, as those are separate address spaces)"
  },
  {
    "id": 25012,
    "content": "Optimization Of Separate Compilation  Separately compiled code may not have as high of performance as whole program code because of the inability to inline code across files A way to still get optimal performance is to use link-time optimization, which stores intermediate code which is then linked together to perform high level optimizations If only some of the files are compiled with -dlto ,"
  },
  {
    "id": 25013,
    "content": "then those will be linked and optimized together while the rest uses the normal separate compilation"
  },
  {
    "id": 25014,
    "content": "A side effect is that this shifts some of the compile time to the link phase, and there may be some scalability issues with really large codes"
  },
  {
    "id": 25015,
    "content": "If you want to compile using -gencode to build for multiple arch, use -dc -gencode arch=compute_NN,code=lto_NN to specify the intermediate IR to be stored (where NN is the SM architecture version)"
  },
  {
    "id": 25023,
    "content": "Object Compatibility  Only relocatable device code with the same ABI version, link-compatible SM target architecture, and same pointer size (32 or 64) can be linked together Link-compatible SM architectures are ones that have compatible SASS binaries that can combine without translating, e"
  },
  {
    "id": 25025,
    "content": "An object could have been compiled for a different architecture but also have PTX available, in which case the device linker will JIT the PTX to cubin for the desired architecture and then link"
  },
  {
    "id": 25026,
    "content": "If Link Time Optimization is used with -dlto , the intermediate LTOIR is only guaranteed to be compatible within a major release (e"
  },
  {
    "id": 25028,
    "content": "If a kernel is limited to a certain number of registers with the launch_bounds attribute or the --maxrregcount option, then all functions that the kernel calls must not use more than that number of registers; if they exceed the limit, then a link error will be given"
  },
  {
    "id": 25032,
    "content": "JIT Linking Support  JIT linking means doing an implicit relink of the code at load time If the cubin does not match the target architecture at load time, the driver re-invokes the device linker to generate cubin for the target architecture, by first JIT’ing the PTX for each object to the appropriate cubin, and then linking together the new cubin If PTX or cubin for the target architecture is"
  },
  {
    "id": 25033,
    "content": "not found for an object, then the link will fail Implicit JIT linking of the LTO intermediates is not supported at this time, although they can be explicitly linked with the nvJitLink library"
  },
  {
    "id": 25040,
    "content": "But actually there is implicit host code generated whenever a device symbol can be accessed from the host side, either via a launch or an API call like cudaGetSymbolAddress() Plus, for JIT linking to work all device code must be passed to the host linker, else the host executable will not contain device code needed for the JIT link So a general rule is that the device linker and host linker must"
  },
  {
    "id": 25041,
    "content": "see the same host object files (if the object files have any device references in them—if a file is pure host then the device linker doesn’t need to see it) If an object file containing device code is not passed to the host linker, then you will see an error message about the function __cudaRegisterLinkedBinary_name calling an undefined or unresolved symbol __fatbinwrap_name"
  },
  {
    "id": 25045,
    "content": "Using __CUDA_ARCH__  In separate compilation, __CUDA_ARCH__ must not be used in headers such that different objects could contain different behavior If a weak function or template function is defined in a header and its behavior depends on __CUDA_ARCH__ , then the instances of that function in the objects could conflict if the objects are compiled for different compute arch"
  },
  {
    "id": 25047,
    "content": "h contains: template __device__ T * getptr ( void ) { #if __CUDA_ARCH__ == 500 return NULL ; /* no address */ #else __shared__ T arr [ 256 ]; return arr ; #endif } Then if a"
  },
  {
    "id": 25050,
    "content": "cu expects a non-NULL address, and compile with: nvcc --gpu-architecture=compute_50 --device-c a cu nvcc --gpu-architecture=compute_52 --device-c b cu nvcc --gpu-architecture=sm_52 a"
  },
  {
    "id": 25052,
    "content": "o At link time only one version of the getptr is used, so the behavior would depend on which version is picked"
  },
  {
    "id": 25054,
    "content": "cu and b cu must be compiled for the same compute arch, or __CUDA_ARCH__ should not be used in the shared header function"
  },
  {
    "id": 25058,
    "content": "Device Code in Libraries  If a device function with non-weak external linkage is defined in a library as well as a non-library object (or another library), the device linker will complain about the multiple definitions (this differs from traditional host linkers that may ignore the function definition from the library object, if it was already found in an earlier object)"
  },
  {
    "id": 25059,
    "content": "Cross Compilation  Cross compilation is controlled by using the following nvcc command line option: --compiler-bindir is used for cross compilation, where the underlying host compiler is capable of generating objects for the target platform On an x86 system, if a CUDA toolkit installation has been configured to support cross compilation to both Tegra and non-Tegra ARM targets, then nvcc will use"
  },
  {
    "id": 25060,
    "content": "the non-Tegra configuration by default, when an ARM host cross compiler has been specified To use the Tegra configuration instead, pass “ -target-dir aarch64-linux ” to nvcc"
  },
  {
    "id": 25063,
    "content": "Keeping Intermediate Phase Files  nvcc stores intermediate results by default into temporary files that are deleted immediately before it completes"
  },
  {
    "id": 25064,
    "content": "The location of the temporary file directories used are, depending on the current platform, as follows: Windows Value of environment variable TEMP is used"
  },
  {
    "id": 25065,
    "content": "Option --keep makes nvcc store these intermediate files in the current directory or in the directory specified by --keep-dir instead, with names as described in Supported Phases"
  },
  {
    "id": 25068,
    "content": "Cleaning Up Generated Files  All files generated by a particular nvcc command can be cleaned up by repeating the command, but with additional option --clean-targets"
  },
  {
    "id": 25069,
    "content": "This option is particularly useful after using --keep , because the --keep option usually leaves quite an amount of intermediate files around"
  },
  {
    "id": 25070,
    "content": "Because using --clean-targets will remove exactly what the original nvcc command created, it is important to exactly repeat all of the options in the original command"
  },
  {
    "id": 25071,
    "content": "For instance, in the following example, omitting --keep , or adding --compile will have different cleanup effects"
  },
  {
    "id": 25072,
    "content": "Printing Code Generation Statistics  A summary on the amount of used registers and the amount of memory needed per compiled device function can be printed by passing option --resource-usage to nvcc : $ nvcc --resource-usage acos"
  },
  {
    "id": 25073,
    "content": "cu -arch sm_80 ptxas info : 1536 bytes gmem ptxas info : Compiling entry function 'acos_main' for 'sm_80' ptxas info : Function properties for acos_main 0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads ptxas info : Used 6 registers, 1536 bytes smem, 32 bytes cmem[0] As shown in the above example, the amount of statically allocated global memory (gmem) is listed"
  },
  {
    "id": 25074,
    "content": "Global memory and some of the constant banks are module scoped resources and not per kernel resources"
  },
  {
    "id": 25075,
    "content": "Spill stores and loads represent stores and loads done on stack memory which are being used for storing variables that couldn’t be allocated to physical registers"
  },
  {
    "id": 25076,
    "content": "Similarly number of registers, amount of shared memory and total space in constant bank allocated is shown"
  },
  {
    "id": 25080,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 25081,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 25083,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 25084,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 25085,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 25086,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 25087,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 25088,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 25089,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 25090,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 25091,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 25092,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 25093,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 25100,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 25102,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 25103,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 25109,
    "content": "5 | PDF | Archive vGPUs and CUDA vGPUs that support CUDA This page describes the support for CUDA® on NVIDIA® virtual GPU software"
  },
  {
    "id": 25116,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 25117,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 25119,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 25120,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 25121,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 25122,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 25123,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 25124,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 25125,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 25126,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 25127,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 25128,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 25129,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 25138,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 25140,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 25141,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 25142,
    "content": "NVIDIA GPUDirect Storage Overview Guide The NVIDIA® Magnum IO GPUDirect® Storage Overview Guide provides a high-level overview of GDS cuFile API Reference Guide The NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the cuFile API reference that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those"
  },
  {
    "id": 25143,
    "content": "APIs, which are part of the GDS technology NVIDIA GPUDirect Storage Release Notes Release information for NVIDIA® Magnum IO GPUDirect® Storage Getting Started with NVIDIA GPUDirect Storage Getting started information for NVIDIA® Magnum IO GPUDirect® Storage Understanding GPUDirect Storage NVIDIA GPUDirect Storage Best Practices Guide The Best Practices guide provides information about the lessons"
  },
  {
    "id": 25144,
    "content": "that were learned when building and massively scaling GPU accelerated I/O storage infrastructures NVIDIA GPUDirect Storage Benchmarking and Configuration Guide The Benchmarking and Configuration Guide helps you evaluate and test GDS functionality and performance by using sample applications NVIDIA GPUDirect Storage Installation and Troubleshooting Guide This guide describes how to install, debug,"
  },
  {
    "id": 25145,
    "content": "and isolate the performance and functional problems that are related to GDS and is intended for systems administrators and developers NVIDIA GPUDirect Storage O_DIRECT Requirements Guide The O_DIRECT Guide helps you understand how GDS provides significant benefit when it can leverage the O_DIRECT fcntl h file mode for a direct data path between GPU memory and storage"
  },
  {
    "id": 25146,
    "content": "Aerospace Hardware / Semiconductor Architecture / Engineering / Construction Manufacturing Media & Entertainment Restaurant / Quick-Service Energy HPC / Scientific Computing IT Specialist Public Sector Financial Services Dev / IT Operations Consumer Internet Cloud Services Telecommunications Gaming Healthcare & Life Sciences Agriculture Academia / Higher Education Retail / Consumer Packaged Goods"
  },
  {
    "id": 25148,
    "content": "Topics NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage NVIDIA GPUDirect Storage Design Guide 1 Troubleshooting GDS issues Understanding GPUDirect Storage NVIDIA GPUDirect Storage Best Practices Guide 1"
  },
  {
    "id": 25149,
    "content": "cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister 3"
  },
  {
    "id": 25152,
    "content": "Benchmarking Storage Performance NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 1 Running Data Verification Tests Using GPUDirect Storage NVIDIA GPUDirect Storage Installation and Troubleshooting Guide 6"
  },
  {
    "id": 25157,
    "content": "com Home About NVIDIA ‎NVIDIA Developer Developer Home Blog Resources Contact Us Developer Program Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2024 NVIDIA Corporation _satellite"
  },
  {
    "id": 25170,
    "content": "5 | PDF | Archive Developing a Linux Kernel Module using GPUDirect RDMA The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs Overview  GPUDirect RDMA is a technology introduced in Kepler-class GPUs and CUDA 5"
  },
  {
    "id": 25171,
    "content": "0 that enables a direct path for data exchange between the GPU and a third-party peer device using standard features of PCI Express"
  },
  {
    "id": 25172,
    "content": "Examples of third-party devices are: network interfaces, video acquisition devices, storage adapters"
  },
  {
    "id": 25173,
    "content": "A number of limitations can apply, the most important being that the two devices must share the same upstream PCI Express root complex"
  },
  {
    "id": 25175,
    "content": "A few straightforward changes must be made to device drivers to enable this functionality with a wide range of hardware devices"
  },
  {
    "id": 25176,
    "content": "This document introduces the technology and describes the steps necessary to enable an GPUDirect RDMA connection to NVIDIA GPUs on Linux How GPUDirect RDMA Works  When setting up GPUDirect RDMA communication between two peers, all physical addresses are the same from the PCI Express devices’ point of view"
  },
  {
    "id": 25177,
    "content": "Each device has six BAR registers at most, so it can have up to six active 32bit BAR regions The PCI Express device issues reads and writes to a peer device’s BAR addresses in the same way that they are issued to system memory Traditionally, resources like BAR windows are mapped to user or kernel address space using the CPU’s MMU as memory mapped I/O (MMIO) addresses"
  },
  {
    "id": 25178,
    "content": "However, because current operating systems don’t have sufficient mechanisms for exchanging MMIO regions between drivers, the NVIDIA kernel driver exports functions to perform the necessary address translations and mappings"
  },
  {
    "id": 25179,
    "content": "To add GPUDirect RDMA support to a device driver, a small amount of address mapping code within the kernel driver must be modified The APIs and control flow involved with GPUDirect RDMA are very similar to those used with standard DMA transfers"
  },
  {
    "id": 25183,
    "content": "Standard DMA Transfer  First, we outline a standard DMA Transfer initiated from userspace In this scenario, the following components are present: Userspace program Userspace communication library Kernel driver for the device interested in doing DMA transfers The general sequence is as follows: The userspace program requests a transfer via the userspace communication library The communication"
  },
  {
    "id": 25184,
    "content": "library must make sure the memory region corresponding to the virtual address and size is ready for the transfer The kernel driver receives the virtual address and size from the userspace communication library It then asks the kernel to translate the virtual address range to a list of physical pages and make sure they are ready to be transferred to or from After the transfer is done, the"
  },
  {
    "id": 25189,
    "content": "GPUDirect RDMA Transfers  For the communication to support GPUDirect RDMA transfers some changes to the sequence above have to be introduced"
  },
  {
    "id": 25190,
    "content": "First of all, two new components are present: Userspace CUDA library NVIDIA kernel driver As described in Basics of UVA CUDA Memory Management , programs using the CUDA library have their address space split between GPU and CPU virtual addresses, and the communication library has to implement two separate paths for them The userspace CUDA library provides a function that lets the communication"
  },
  {
    "id": 25191,
    "content": "library distinguish between CPU and GPU addresses Moreover, for GPU addresses it returns additional metadata that is required to uniquely identify the GPU memory represented by the address The difference between the paths for CPU and GPU addresses is in how the memory is pinned and unpinned For CPU memory this is handled by built-in Linux Kernel functions ( get_user_pages() and put_page() )"
  },
  {
    "id": 25192,
    "content": "However, in the GPU memory case the pinning and unpinning has to be handled by functions provided by the NVIDIA Kernel driver"
  },
  {
    "id": 25198,
    "content": "For memory buffers owned by the calling process (which is typical) tokens can be replaced by zero (0) in the kernel-mode function nvidia_p2p_get_pages()"
  },
  {
    "id": 25199,
    "content": "This new feature is meant to make it easier for existing third party software stacks to adopt RDMA for GPUDirect"
  },
  {
    "id": 25201,
    "content": "It is necessary to ensure correct synchronization behavior of the CUDA API when operation on memory which may be read by RDMA for GPUDirect"
  },
  {
    "id": 25202,
    "content": "cuPointerGetAttribute() has been extended to return a globally unique numeric identifier, which in turn can be used by lower-level libraries to detect buffer reallocations happening in user-level code (see Userspace API ) It provides an alternative method to detect reallocations when intercepting CUDA allocation and deallocation APIs is not possible"
  },
  {
    "id": 25203,
    "content": "The kernel-mode memory pinning feature has been extended to work in combination with Multi-Process Service (MPS)"
  },
  {
    "id": 25206,
    "content": "While the page table returned by nvidia_p2p_get_pages() is valid for managed memory buffers and provides a mapping of GPU memory at any given moment in time, the GPU device copy of that memory may be incoherent with the writable copy of the page which is not on the GPU Using the page table in this circumstance may result in accessing stale data, or data loss, because of a DMA write access to"
  },
  {
    "id": 25207,
    "content": "device memory that is subsequently overwritten by the Unified Memory run-time cuPointerGetAttribute() may be used to determine if an address is being managed by the Unified Memory runtime Every time a device memory region is pinned, new GPU BAR space is allocated unconditionally, even when pinning overlapping or duplicate device memory ranges, i"
  },
  {
    "id": 25214,
    "content": "0: On the IBM POWER8 platform, GPUDirect RDMA is not supported, though it is not explicitly disabled"
  },
  {
    "id": 25215,
    "content": "Now when a device memory region is pinned, GPU BAR space might be shared with pre-existing mappings As a consequence, when unpinning a region, its whole BAR space will not be returned if even only a subset of its BAR space is shared"
  },
  {
    "id": 25217,
    "content": "It can be used as a template for implementing an interception framework for CUDA memory de/allocation APIs"
  },
  {
    "id": 25221,
    "content": "0: The nvidia_p2p_page_table struct has been extended to include a new member, without breaking binary compatibility"
  },
  {
    "id": 25222,
    "content": "The minor version in the NVIDIA_P2P_PAGE_TABLE_VERSION macro has been updated accordingly The nvidia_p2p_dma_mapping structure, the nvidia_p2p_dma_map_pages() and nvidia_p2p_dma_unmap_pages() APIs, the NVIDIA_P2P_DMA_MAPPING_VERSION macro have been introduced"
  },
  {
    "id": 25223,
    "content": "These APIs can be used by third party device drivers to map and unmap the GPU BAR pages into their device’s I/O address space"
  },
  {
    "id": 25224,
    "content": "The main use case is on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources"
  },
  {
    "id": 25225,
    "content": "The NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE and NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE macros have been introduced"
  },
  {
    "id": 25226,
    "content": "These are meant to be called by third-party device drivers to check for runtime binary compatibility, for example in case of changes to the data structure’s layout On the IBM POWER8 platform, when using the above APIs, GPUDirect RDMA is reported to work correctly restricted to the case where the GPU and the third party device are connected through a supported PCIe switch"
  },
  {
    "id": 25239,
    "content": "4  Added a new a kernel module, nvidia-peermem , which provides NVIDIA InfiniBand-based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory"
  },
  {
    "id": 25244,
    "content": "2  In drivers released from the R515 up to the R535 branches, except for newer R525 and R535 releases mentioned below, there is a race bug which may show up as a kernel null-pointer dereference"
  },
  {
    "id": 25245,
    "content": "This happens when the GPU invokes the (hereby I/O) kernel driver invalidation callback, the one which was registered during the call to nvidia_p2p_get_pages, concurrently with the I/O driver calling nvidia_p2p_put_pages The race bug does not affect the persistent mapping case, as in that case an invalidation callback is not supported nor needed The bug fix required the following API change:"
  },
  {
    "id": 25247,
    "content": "Instead, nvidia_p2p_put_pages_persistent and nvidia_p2p_get_pages_persistent have been introduced and should be used instead when requesting a persistent mapping"
  },
  {
    "id": 25248,
    "content": "The use of those new persistent APIs can be guarded by the NVIDIA_P2P_CAP_GET_PAGES_PERSISTENT_API preprocessor macro, for example when writing portable drivers"
  },
  {
    "id": 25249,
    "content": "Although deprecated when running GPU drivers from the R470 branch and newer, customers still using the off-tree nv_peer_mem module ( https: github"
  },
  {
    "id": 25250,
    "content": "com/Mellanox/nv_peer_memory ) and needing the persistent mapping feature will have to switch to nvidia-peermem"
  },
  {
    "id": 25257,
    "content": "Design Considerations  When designing a system to utilize GPUDirect RDMA, there a number of considerations which should be taken into account"
  },
  {
    "id": 25260,
    "content": "Lazy Unpinning Optimization  Pinning GPU device memory in BAR is an expensive operation, taking up to milliseconds"
  },
  {
    "id": 25261,
    "content": "The most straightforward implementation using GPUDirect RDMA would pin memory before each transfer and unpin it right after the transfer is complete"
  },
  {
    "id": 25262,
    "content": "Unfortunately, this would perform poorly in general, as pinning and unpinning memory are expensive operations"
  },
  {
    "id": 25263,
    "content": "The rest of the steps required to perform an RDMA transfer, however, can be performed quickly without entering the kernel (the DMA list can be cached and replayed using MMIO registers/command lists)"
  },
  {
    "id": 25264,
    "content": "This takes advantage of the fact that it is likely that the same memory region will be used for future DMA transfers thus lazy unpinning saves pin/unpin operations An example implementation of lazy unpinning would keep a set of pinned memory regions and only unpin some of them (for example the least recently used one) if the total size of the regions reached some threshold, or if pinning a new"
  },
  {
    "id": 25268,
    "content": "Registration Cache  Communication middleware often employs an optimization called a registration cache, or pin-down cache, to minimize pinning overhead"
  },
  {
    "id": 25270,
    "content": "For networking middleware, such caches are usually implemented in user-space, as they are used in combination with hardware capable of user-mode message injection"
  },
  {
    "id": 25271,
    "content": "CUDA UVA memory address layout enables GPU memory pinning to work with these caches by taking into account just a few design considerations"
  },
  {
    "id": 25272,
    "content": "In the CUDA environment, this is even more important as the amount of memory which can be pinned may be significantly more constrained than for host memory"
  },
  {
    "id": 25273,
    "content": "As the GPU BAR space is typically mapped using 64KB pages, it is more resource efficient to maintain a cache of regions rounded to the 64KB boundary Even more so, as two memory areas which are in the same 64KB boundary would allocate and return the same BAR mapping"
  },
  {
    "id": 25274,
    "content": "Registration caches usually rely on the ability to intercept deallocation events happening in the user application, so that they can unpin the memory and free important HW resources, e"
  },
  {
    "id": 25276,
    "content": "To implement a similar mechanism for GPU memory, an implementation has two options: Instrument all CUDA allocation and deallocation APIs"
  },
  {
    "id": 25277,
    "content": "There is a sample application, 7_CUDALibraries/cuHook , showing how to intercept calls to CUDA APIs at run-time, which can be used to detect GPU memory de/allocations"
  },
  {
    "id": 25278,
    "content": "While intercepting CUDA APIs is beyond the scope of this document, an approach to performing tag checks is available starting with CUDA 6"
  },
  {
    "id": 25280,
    "content": "It involves the usage of the CU_POINTER_ATTRIBUTE_BUFFER_ID attribute in cuPointerGetAttribute() (or cuPointerGetAttributes() if more attributes are needed) to detect memory buffer deallocations or reallocations"
  },
  {
    "id": 25281,
    "content": "The API will return a different ID value in case of reallocation or an error if the buffer address is no longer valid"
  },
  {
    "id": 25282,
    "content": "Note Using tag checks introduces an extra call into the CUDA API on each memory buffer use, so this approach is most appropriate when the additional latency is not a concern"
  },
  {
    "id": 25285,
    "content": "Unpin Callback  When a third party device driver pins the GPU pages with nvidia_p2p_get_pages() it must also provide a callback function that the NVIDIA driver will call if it needs to revoke access to the mapping This callback occurs synchronously , giving the third party driver the opportunity to clean up and remove any references to the pages in question (i"
  },
  {
    "id": 25288,
    "content": "The user callback function may block for a few milliseconds , although it is recommended that the callback complete as quickly as possible Care has to be taken not to introduce deadlocks as waiting within the callback for the GPU to do anything is not safe The callback must call nvidia_p2p_free_page_table() (not nvidia_p2p_put_pages() ) to free the memory pointed to by page_table The"
  },
  {
    "id": 25289,
    "content": "corresponding mapped memory areas will only be unmapped by the NVIDIA driver after returning from the callback Note that the callback will be invoked in two scenarios: If the userspace program explicitly deallocates the corresponding GPU memory, e"
  },
  {
    "id": 25291,
    "content": "before the third party kernel driver has a chance to unpin the memory with nvidia_p2p_put_pages() In the latter case there can be tear-down ordering issues between closing the file descriptor of the third party kernel driver and that of the NVIDIA kernel driver In the case the file descriptor for the NVIDIA kernel driver is closed first, the nvidia_p2p_put_pages() callback will be invoked A"
  },
  {
    "id": 25292,
    "content": "proper software design is important as the NVIDIA kernel driver will protect itself from reentrancy issues with locks before invoking the callback"
  },
  {
    "id": 25293,
    "content": "The third party kernel driver will almost certainly take similar actions, so dead-locking or live-locking scenarios may arise if careful consideration is not taken"
  },
  {
    "id": 25296,
    "content": "Supported Systems  General remarks Even though the only theoretical requirement for GPUDirect RDMA to work between a third-party device and an NVIDIA GPU is that they share the same root complex, there exist bugs (mostly in chipsets) causing it to perform badly, or not work at all in certain setups"
  },
  {
    "id": 25297,
    "content": "We can distinguish between three situations, depending on what is on the path between the GPU and the third-party device: PCIe switches only single CPU/IOH CPU/IOH QPI/HT CPU/IOH The first situation, where there are only PCIe switches on the path, is optimal and yields the best performance The second one, where a single CPU/IOH is involved, works, but yields worse performance ( especially"
  },
  {
    "id": 25298,
    "content": "peer-to-peer read bandwidth has been shown to be severely limited on some processor architectures ) Finally, the third situation, where the path traverses a QPI/HT link, may be extremely performance-limited or even not work reliably"
  },
  {
    "id": 25299,
    "content": "Tip lspci can be used to check the PCI topology: $ lspci - t Platform support For IBM POWER8 platform, GPUDirect RDMA and P2P are not supported, but are not explicitly disabled GPUDirect RDMA is supported on Jetson AGX Xavier platform starting from CUDA 10 1 and on Drive AGX Xavier Linux based platforms from CUDA 11"
  },
  {
    "id": 25301,
    "content": "On ARM64, the necessary peer-to-peer functionality depends on both the hardware and the software of the particular platform"
  },
  {
    "id": 25302,
    "content": "So while GPUDirect RDMA is not explicitly disabled on non-Jetson and non-Drive platforms, there are no guarantees that it will be fully functional"
  },
  {
    "id": 25303,
    "content": "IOMMUs GPUDirect RDMA currently relies upon all physical addresses being the same from the different PCI devices’ point of view This makes it incompatible with IOMMUs performing any form of translation other than 1:1, hence they must be disabled or configured for pass-through translation for GPUDirect RDMA to work"
  },
  {
    "id": 25307,
    "content": "NVIDIA GPUs currently expose multiple BARs, and some of them can back arbitrary device memory, making GPUDirect RDMA possible"
  },
  {
    "id": 25308,
    "content": "Large BARs can pose a problem for the BIOS, especially on older motherbords, related to compatibility support for 32bit operating systems"
  },
  {
    "id": 25309,
    "content": "On those motherboards the bootstrap can stop during the early POST phase, or the GPU may be misconfigured and so unusable"
  },
  {
    "id": 25310,
    "content": "If this appears to be occuring it might be necessary to enable some special BIOS feature to deal with the large BAR issue"
  },
  {
    "id": 25314,
    "content": "Tokens Usage  Warning Starting in CUDA 6 0, tokens should be considered deprecated, though they are still supported As can be seen in Userspace API and Kernel API , one method for pinning and unpinning memory requires two tokens in addition to the GPU virtual address These tokens, p2pToken and vaSpaceToken , are necessary to uniquely identify a GPU VA space The tokens are consistent within a"
  },
  {
    "id": 25317,
    "content": ", all memory obtained through cudaMalloc() within the same CUDA context will have the same p2pToken and vaSpaceToken ) However, a given GPU virtual address need not map to the same context/GPU for its entire lifetime As a concrete example: cudaSetDevice ( 0 ) ptr0 = cudaMalloc (); cuPointerGetAttribute ( & return_data , CU_POINTER_ATTRIBUTE_P2P_TOKENS , ptr0 ); Returns [p2pToken = 0xabcd,"
  },
  {
    "id": 25318,
    "content": "vaSpaceToken = 0x1] cudaFree ( ptr0 ); cudaSetDevice ( 1 ); ptr1 = cudaMalloc (); assert ( ptr0 == ptr1 ); The CUDA driver is free (although not guaranteed) to reuse the VA, even on a different GPU cuPointerGetAttribute ( & return_data , CU_POINTER_ATTRIBUTE_P2P_TOKENS , ptr0 ); Returns [p2pToken = 0x0123, vaSpaceToken = 0x2] That is, the same address, when passed to cuPointerGetAttribute , may"
  },
  {
    "id": 25320,
    "content": "Therefore, the third party communication library must call cuPointerGetAttribute() for every pointer it operates on"
  },
  {
    "id": 25321,
    "content": "Security implications The two tokens act as an authentication mechanism for the NVIDIA kernel driver If you know the tokens, you can map the address space corresponding to them, and the NVIDIA kernel driver doesn’t perform any additional checks When no tokens are used, the NVIDIA driver limits the Kernel API to the process which owns the memory allocation"
  },
  {
    "id": 25324,
    "content": "Synchronization and Memory Ordering  GPUDirect RDMA introduces a new independent GPU data flow path exposed to third party devices and it is important to understand how these devices interact with the GPU’s relaxed memory model"
  },
  {
    "id": 25325,
    "content": "Properly registering a BAR mapping of CUDA memory is required for that mapping to remain consistent with CUDA APIs operations on that memory Only CUDA synchronization and work submission APIs provide memory ordering of GPUDirect RDMA operations Registration for CUDA API Consistency Registration is necessary to ensure the CUDA API memory operations visible to a BAR mapping happen before the API"
  },
  {
    "id": 25326,
    "content": "call returns control to the calling CPU thread This provides a consistent view of memory to a device using GPUDirect RDMA mappings when invoked after a CUDA API in the thread"
  },
  {
    "id": 25327,
    "content": "This is a strictly more conservative mode of operation for the CUDA API and disables optimizations, thus it may negatively impact performance"
  },
  {
    "id": 25328,
    "content": "This behavior is enabled on a per-allocation granularity either by calling cuPointerSetAttribute() with the CU_POINTER_ATTRIBUTE_SYNC_MEMOPS attribute, or p2p tokens are retrieved for a buffer when using the legacy path"
  },
  {
    "id": 25329,
    "content": "An example situation would be Read-after-Write dependency between a cuMemcpyDtoD() and subsequent GPUDirect RDMA read operation on the destination of the copy As an optimization the device-to-device memory copy typically returns asynchronously to the calling thread after queuing the copy to the GPU scheduler However, in this circumstance that will lead to inconsistent data read via the BAR"
  },
  {
    "id": 25331,
    "content": "CUDA APIs for Memory Ordering Only CPU initiated CUDA APIs provide ordering of GPUDirect memory operations as observed by the GPU"
  },
  {
    "id": 25332,
    "content": "That is, despite a third party device having issued all PCIE transactions, a running GPU kernel or copy operation may observe stale data or data that arrives out-of-order until a subsequent CPU initiated CUDA work submission or synchronization API"
  },
  {
    "id": 25333,
    "content": "To ensure that memory updates are visible to CUDA kernels or copies, an implementation should ensure that all writes to the GPU BAR happen before control is returned to the CPU thread which will invoke the dependent CUDA API"
  },
  {
    "id": 25334,
    "content": "An example situation for a network communication scenario is when a network RDMA write operation is completed by the third party network device and the data is written to the GPU BAR mapping Though reading back the written data either through GPU BAR or a CUDA memory copy operation, will return the newly written data, a concurrently running GPU kernel to that network write might observe stale"
  },
  {
    "id": 25335,
    "content": "data, the data partially written, or the data written out-of-order In short, a GPU kernel is wholly inconsistent with concurrent RDMA for GPUDirect operations and accessing the memory overwritten by the third party device in such a situation would be considered a data race To resolve this inconsistency and remove the data race the DMA write operation must complete with respect to the CPU thread"
  },
  {
    "id": 25342,
    "content": "It can be used to understand the application usage of BAR space, the primary resource consumed by GPUDirect RDMA mappings"
  },
  {
    "id": 25343,
    "content": "GPU memory is pinned in fixed size chunks, so the amount of space reflected here might be unexpected In addition, a certain amount of BAR space is reserved by the driver for internal use, so not all available memory may be usable via GPUDirect RDMA"
  },
  {
    "id": 25344,
    "content": "Note that the same ability is offered programmatically through the nvmlDeviceGetBAR1MemoryInfo() NVML API"
  },
  {
    "id": 25347,
    "content": "Pinning GPU memory  Correct behavior requires using cuPointerSetAttribute() on the memory address to enable proper synchronization behavior in the CUDA driver void pin_buffer ( void * address , size_t size ) { unsigned int flag = 1 ; CUresult status = cuPointerSetAttribute ( & flag , CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , address ); if ( CUDA_SUCCESS == status ) { GPU path pass_to_kernel_driver ("
  },
  {
    "id": 25348,
    "content": "address , size ); } else { CPU path } } This is required so that the GPU memory buffer is treated in a special way by the CUDA driver, so that CUDA memory transfers are guaranteed to always be synchronous with respect to the host"
  },
  {
    "id": 25349,
    "content": "for boundary alignment requirement #define GPU_BOUND_SHIFT 16 #define GPU_BOUND_SIZE ((u64)1 page_table , free_callback , & my_state ); if ( ret == 0 ) { Succesfully pinned, page_table can be accessed } else { Pinning failed } } Note how the start address is aligned to a 64KB boundary before calling the pinning functions If the function succeeds the memory has been pinned and the page_table"
  },
  {
    "id": 25354,
    "content": "Unpinning GPU memory  In the kernel driver, invoke nvidia_p2p_put_pages() void unpin_memory ( void * address , size_t size , nvidia_p2p_page_table_t * page_table ) { nvidia_p2p_put_pages ( 0 , 0 , address , size , page_table ); } See Kernel API for details on nvidia_p2p_put_pages() Note that nvidia_p2p_put_pages() must be called from within the same process context as the one from which the"
  },
  {
    "id": 25358,
    "content": "Handling the free callback  The NVIDIA kernel driver invokes free_callback(data) as specified in the nvidia_p2p_get_pages() call if it needs to revoke the mapping void free_callback ( void * data ) { my_state * state = data ; wait_for_pending_transfers ( state ); nvidia_p2p_free_pages ( state -> page_table ); } The NVIDIA kernel driver handles the unmapping so nvidia_p2p_put_pages() should not"
  },
  {
    "id": 25362,
    "content": "Buffer ID Tag Check for A Registration Cache  Remember that a solution built around Buffer ID tag checking is not recommended for latency sensitive implementations Instead, instrumentation of CUDA allocation and deallocation APIs to provide callbacks to the registration cache is recommended, removing tag checking overhead from the critical path The first time a device memory buffer is"
  },
  {
    "id": 25363,
    "content": "encountered and recognized as not yet pinned, the pinned mapping is created and the associated buffer ID is retrieved and stored together in the cache entry"
  },
  {
    "id": 25364,
    "content": "The cuMemGetAddressRange() function can be used to obtain the size and starting address for the whole allocation, which can then be used to pin it"
  },
  {
    "id": 25365,
    "content": "As nvidia_p2p_get_pages() will need a pointer aligned to 64K, it is useful to directly align the cached address"
  },
  {
    "id": 25366,
    "content": "Also, as the BAR space is currently mapped in chunks of 64KB, it is more resource efficient to round the whole pinning to 64KB"
  },
  {
    "id": 25367,
    "content": "struct buf represents an entry of the registration cache struct buf { CUdeviceptr pointer ; size_t size ; CUdeviceptr aligned_pointer ; size_t aligned_size ; int is_pinned ; uint64_t id ;   buffer id obtained right after pinning }; Once created, every time a registration cache entry will be used it must be first checked for validity"
  },
  {
    "id": 25368,
    "content": "One way to do this is to use the Buffer ID provided by CUDA as a tag to check for deallocation or reallocation"
  },
  {
    "id": 25369,
    "content": "int buf_is_gpu_pinning_valid ( struct buf * buf ) { uint64_t buffer_id ; int retcode ; assert ( buf -> is_pinned ); get the current buffer id retcode = cuPointerGetAttribute ( & buffer_id , CU_POINTER_ATTRIBUTE_BUFFER_ID , buf -> pointer ); if ( CUDA_ERROR_INVALID_VALUE == retcode ) { the device pointer is no longer valid it could have been deallocated return ERROR_INVALIDATED ; } else if ("
  },
  {
    "id": 25370,
    "content": "CUDA_SUCCESS = retcode ) { handle more serious errors here return ERROR_SERIOUS ; } if ( buf -> id = buffer_id ) the original buffer has been deallocated and the cached mapping should be invalidated and the buffer re-pinned return ERROR_INVALIDATED ; return 0 ; } When the buffer identifier changes the corresponding memory buffer has been reallocated so the corresponding kernel-space page table"
  },
  {
    "id": 25371,
    "content": "will not be valid anymore Thus the Buffer IDs provide a tag to keep the pin-down cache consistent with the kernel-space page table without requiring the kernel driver to up-call into the user-space If CUDA_ERROR_INVALID_VALUE is returned from cuPointerGetAttribute() , the program should assume that the memory buffer has been deallocated or is otherwise not a valid GPU memory buffer in the"
  },
  {
    "id": 25372,
    "content": "registration cache code if ( buf -> is_pinned && buf_is_gpu_pinning_valid ( buf )) { regcache_invalidate_entry ( buf ); pin_buffer ( buf ); } 3"
  },
  {
    "id": 25378,
    "content": "Navigate to the output directory: cd /kernel/ Within this directory, build the NVIDIA module for your kernel: make module After this is done, the Module symvers file under your kernel build directory contains symbol information for nvidia"
  },
  {
    "id": 25383,
    "content": "Using nvidia-peermem  The NVIDIA GPU driver package provides a kernel module, nvidia-peermem , which provides NVIDIA InfiniBand based HCAs (Host Channel Adapters) direct peer-to-peer read and write access to the NVIDIA GPU’s video memory"
  },
  {
    "id": 25384,
    "content": "It allows GPUDirect RDMA-based applications to use GPU computing power with the RDMA interconnect without needing to copy data to host memory"
  },
  {
    "id": 25385,
    "content": "NVIDIA OFED (Open Fabrics Enterprise Distribution), or MLNX_OFED, introduces an API between the InfiniBand Core and peer memory clients such as NVIDIA GPUs The nvidia-peermem module registers the NVIDIA GPU with the InfiniBand subsystem by using peer-to-peer APIs provided by the NVIDIA GPU driver The kernel must have the required support for RDMA peer memory either through additional patches to"
  },
  {
    "id": 25387,
    "content": "It is possible that the nv_peer_mem module from the GitHub project may be installed and loaded on the system Installation of nvidia-peermem will not affect the functionality of the existing nv_peer_mem module Additionally, it is encouraged to uninstall the nv_peer_mem package to avoid any conflict with nvidia-peermem since only one module can be loaded at any time To stop the nv_peer_mem service:"
  },
  {
    "id": 25388,
    "content": "# service nv_peer_mem stop Check if nv_peer_mem ko is still loaded after stopping the service: # lsmod | grep nv_peer_mem If nv_peer_mem ko is still loaded, unload it using: # rmmod nv_peer_mem Uninstall the nv_peer_mem package: For DEB-based OS: # dpkg -P nvidia-peer-memory # dpkg -P nvidia-peer-memory-dkms For RPM-based OS: # rpm -e nvidia_peer_memory After ensuring kernel support and installing"
  },
  {
    "id": 25389,
    "content": "the GPU driver, nvidia-peermem can be loaded with the following command with root privileges in a terminal window: # modprobe nvidia-peermem Note Note: If the NVIDIA GPU driver is installed before MLNX_OFED, the GPU driver must be uninstalled and installed again to make sure nvidia-peermem is compiled with the RDMA APIs that are provided by MLNX_OFED"
  },
  {
    "id": 25393,
    "content": "Basics of UVA CUDA Memory Management  Unified virtual addressing (UVA) is a memory address management system enabled by default in CUDA 4"
  },
  {
    "id": 25395,
    "content": "On UVA-supported configurations, when the CUDA runtime initializes, the virtual address (VA) range of the application is partitioned into two areas: the CUDA-managed VA range and the OS-managed VA range All CUDA-managed pointers are within this VA range, and the range will always fall within the first 40 bits of the process’s VA space CUDA VA Space Addressing  Subsequently, within the CUDA VA"
  },
  {
    "id": 25396,
    "content": "space, addresses can be subdivided into three types: GPU A page backed by GPU memory This will not be accessible from the host and the VA in question will never have a physical backing on the host This partitioning allows the CUDA runtime to determine the physical location of a memory object by its pointer value within the reserved CUDA VA space Addresses are subdivided into these categories at"
  },
  {
    "id": 25397,
    "content": "page granularity; all memory within a page is of the same type GPUDirect RDMA operates exclusively on GPU pages (created by cudaMalloc() ) that are within this CUDA VA space"
  },
  {
    "id": 25400,
    "content": "Userspace API  Data structures typedef struct CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_st { unsigned long long p2pToken ; unsigned int vaSpaceToken ; } CUDA_POINTER_ATTRIBUTE_P2P_TOKENS ; Function cuPointerSetAttribute() CUresult cuPointerSetAttribute ( void * data , CUpointer_attribute attribute , CUdeviceptr pointer ); In GPUDirect RDMA scope, the interesting usage is when"
  },
  {
    "id": 25401,
    "content": "CU_POINTER_ATTRIBUTE_SYNC_MEMOPS is passed as the attribute : unsigned int flag = 1 ; cuPointerSetAttribute ( & flag , CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , pointer ); Parameters data [in] A pointer to a unsigned int variable containing a boolean value attribute [in] In GPUDirect RDMA scope should always be CU_POINTER_ATTRIBUTE_SYNC_MEMOPS"
  },
  {
    "id": 25402,
    "content": "Returns CUDA_SUCCESS if pointer points to GPU memory and the CUDA driver was able to set the new behavior for the whole device memory allocation It is used to explicitly enable a strictly synchronizing behavior on the whole memory allocation pointed to by pointer , and by doing so disabling all data transfer optimizations which might create problems with concurrent RDMA and CUDA memory copy"
  },
  {
    "id": 25403,
    "content": "operations This API has CUDA synchronizing behavior, so it should be considered expensive and possibly invoked only once per buffer"
  },
  {
    "id": 25404,
    "content": "Function cuPointerGetAttribute() CUresult cuPointerGetAttribute ( const void * data , CUpointer_attribute attribute , CUdeviceptr pointer ); This function has two different attributes related to GPUDirect RDMA: CU_POINTER_ATTRIBUTE_P2P_TOKENS and CU_POINTER_ATTRIBUTE_BUFFER_ID Warning CU_POINTER_ATTRIBUTE_P2P_TOKENS has been deprecated in CUDA 6 0 When CU_POINTER_ATTRIBUTE_P2P_TOKENS is passed as"
  },
  {
    "id": 25405,
    "content": "the attribute , data is a pointer to CUDA_POINTER_ATTRIBUTE_P2P_TOKENS : CUDA_POINTER_ATTRIBUTE_P2P_TOKENS tokens ; cuPointerGetAttribute ( & tokens , CU_POINTER_ATTRIBUTE_P2P_TOKENS , pointer ); In this case, the function returns two tokens for use with the Kernel API"
  },
  {
    "id": 25406,
    "content": "This function may be called at any time, including before CUDA initialization, and it has CUDA synchronizing behavior, as in CU_POINTER_ATTRIBUTE_SYNC_MEMOPS , so it should be considered expensive and should be invoked only once per buffer"
  },
  {
    "id": 25407,
    "content": "Note that values set in tokens can be different for the same pointer value during a lifetime of a user-space program Note that for security reasons the value set in p2pToken will be randomized, to prevent it from being guessed by an adversary"
  },
  {
    "id": 25410,
    "content": "When CU_POINTER_ATTRIBUTE_BUFFER_ID is passed as the attribute , data is expected to point to a 64bit unsigned integer variable, like uint64_t uint64_t buf_id ; cuPointerGetAttribute ( & buf_id , CU_POINTER_ATTRIBUTE_BUFFER_ID , pointer ); Parameters data [out] A pointer to a 64 bits variable where the buffer id will be stored"
  },
  {
    "id": 25411,
    "content": "Some general remarks follow: cuPointerGetAttribute() and cuPointerSetAttribute() are CUDA driver API functions only In particular, cuPointerGetAttribute() is not equivalent to cudaPointerGetAttributes() , as the required functionality is only present in the former function This in no way limits the scope where GPUDirect RDMA may be used as cuPointerGetAttribute() is compatible with the CUDA"
  },
  {
    "id": 25412,
    "content": "Runtime API This is so as the additional overhead associated with the CUDA runtime API to driver API call sequence would introduce unneeded overhead and cuPointerGetAttribute() can be on the critical path, e"
  },
  {
    "id": 25414,
    "content": "Whenever possible, we suggest to combine multiple calls to cuPointerGetAttribute by using cuPointerGetAttributes Function ``cuPointerGetAttributes()`` CUresult cuPointerGetAttributes ( unsigned int numAttributes , CUpointer_attribute * attributes , void ** data , CUdeviceptr ptr ); This function can be used to inspect multiple attributes at once"
  },
  {
    "id": 25415,
    "content": "The one most probably related to GPUDirect RDMA are CU_POINTER_ATTRIBUTE_BUFFER_ID , CU_POINTER_ATTRIBUTE_MEMORY_TYPE and CU_POINTER_ATTRIBUTE_IS_MANAGED"
  },
  {
    "id": 25419,
    "content": "h header that is distributed in the NVIDIA Driver package Please refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values of the functions described below"
  },
  {
    "id": 25420,
    "content": "Preprocessor macros NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE() and NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE() preprocessor macros are meant to be called by third-party device drivers to check for runtime binary compatibility"
  },
  {
    "id": 25421,
    "content": "Structure nvidia_p2p_page typedef struct nvidia_p2p_page { uint64_t physical_address ; union nvidia_p2p_request_registers { struct { uint32_t wreqmb_h ; uint32_t rreqmb_h ; uint32_t rreqmb_0 ; uint32_t reserved [ 3 ]; } fermi ; } registers ; } nvidia_p2p_page_t ; In the nvidia_p2p_page structure only the physical_address field is relevant to GPUDirect RDMA Structure nvidia_p2p_page_table typedef"
  },
  {
    "id": 25422,
    "content": "struct nvidia_p2p_page_table { uint32_t version ; uint32_t page_size ; struct nvidia_p2p_page ** pages ; uint32_t entries ; uint8_t * gpu_uuid ; } nvidia_p2p_page_table_t ; The version field of the page table should be checked by using NVIDIA_P2P_PAGE_TABLE_VERSION_COMPATIBLE() before accessing the other fields Structure nvidia_p2p_dma_mapping typedef struct nvidia_p2p_dma_mapping { uint32_t"
  },
  {
    "id": 25423,
    "content": "version ; enum nvidia_p2p_page_size_type page_size_type ; uint32_t entries ; uint64_t * dma_addresses ; } nvidia_p2p_dma_mapping_t ; The version field of the dma mapping should be passed to NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE() before accessing the other fields Function nvidia_p2p_get_pages() int nvidia_p2p_get_pages ( uint64_t p2p_token , uint32_t va_space_token , uint64_t virtual_address ,"
  },
  {
    "id": 25424,
    "content": "uint64_t length , struct nvidia_p2p_page_table ** page_table , void ( * free_callback )( void * data ), void * data ); This function makes the pages underlying a range of GPU virtual memory accessible to a third-party device"
  },
  {
    "id": 25425,
    "content": "Warning This is an expensive operation and should be performed as infrequently as possible - see Lazy Unpinning Optimization"
  },
  {
    "id": 25426,
    "content": "Function nvidia_p2p_put_pages() int nvidia_p2p_put_pages ( uint64_t p2p_token , uint32_t va_space_token , uint64_t virtual_address , struct nvidia_p2p_page_table * page_table ); This function releases a set of pages previously made accessible to a third-party device Function nvidia_p2p_free_page_table() int nvidia_p2p_free_page_table ( struct nvidia_p2p_page_table * page_table ); This function"
  },
  {
    "id": 25427,
    "content": "frees a third-party P2P page table and is meant to be invoked during the execution of the nvidia_p2p_get_pages() callback Function nvidia_p2p_dma_map_pages() int nvidia_p2p_dma_map_pages ( struct pci_dev * peer , struct nvidia_p2p_page_table * page_table , struct nvidia_p2p_dma_mapping ** dma_mapping ); This function makes the physical pages retrieved using nvidia_p2p_get_pages() accessible to a"
  },
  {
    "id": 25429,
    "content": "It is required on platforms where the I/O addresses of PCIe resources, used for PCIe peer-to-peer transactions, are different from the physical addresses used by the CPU to access those same resources"
  },
  {
    "id": 25430,
    "content": "On some platforms, this function relies on a correct implementation of the dma_map_resource() Linux kernel function"
  },
  {
    "id": 25431,
    "content": "Function nvidia_p2p_dma_unmap_pages() int nvidia_p2p_dma_unmap_pages ( struct pci_dev * peer , struct nvidia_p2p_page_table * page_table , struct nvidia_p2p_dma_mapping * dma_mapping ); This function unmaps the physical pages previously mapped to the third-party device by nvidia_p2p_dma_map_pages()"
  },
  {
    "id": 25432,
    "content": "It is not meant to be called from within the nvidia_p2p_get_pages() invalidation callback Function nvidia_p2p_free_dma_mapping() int nvidia_p2p_free_dma_mapping ( struct nvidia_p2p_dma_mapping * dma_mapping ); This function is meant to be called from within the nvidia_p2p_get_pages() invalidation callback Note that the deallocation of the I/O mappings may be deferred, for example after returning"
  },
  {
    "id": 25436,
    "content": "Porting to Tegra  GPUDirect RDMA is supported on Jetson AGX Xavier platform from CUDA 10 1, on DRIVE AGX Xavier Linux based platforms from CUDA 11 2 and on Jetson Orin platform from CUDA 11"
  },
  {
    "id": 25438,
    "content": "Owing to hardware and software specific divergence of Tegra vis-a-vis Linux-Desktop, already developed applications needs to be slightly modified in order to port them to Tegra"
  },
  {
    "id": 25447,
    "content": "Changing the allocator  GPUDirect RDMA on Desktop allows applications to operate exclusively on GPU pages allocated using cudaMalloc() On Tegra, applications will have to change the memory allocator from cudaMalloc() to cudaHostAlloc()"
  },
  {
    "id": 25448,
    "content": "Applications can either: Treat the returned pointer as if it is a device pointer, provided that the iGPU supports UVA or cudaDevAttrCanUseHostPointerForRegisteredMem device attribute is a non-zero value when queried using cudaDeviceGetAttribute() for iGPU Get the device pointer corresponding to the host memory allocated using cudaHostGetDevicePointer() Once the application has the device pointer,"
  },
  {
    "id": 25453,
    "content": "Modification to Kernel API  The declarations under Tegra API column of the following table can be found in the nv-p2p"
  },
  {
    "id": 25454,
    "content": "h header that is distributed in the NVIDIA Driver package Refer to the inline documentation contained in that header file for a detailed description of the parameters and the return values"
  },
  {
    "id": 25455,
    "content": "Other highlights  The length of the requested mapping and base address must be a multiple of 4KB, failing which leads to an error"
  },
  {
    "id": 25456,
    "content": "Unlike the Desktop version, callback registered at nvidia_p2p_get_pages() will always be triggered when nvidia_p2p_put_pages() is invoked"
  },
  {
    "id": 25457,
    "content": "It is the reponsibilty of the kernel driver to free the page_table allocated by calling nvidia_p2p_free_page_table()"
  },
  {
    "id": 25458,
    "content": "Note that, similar to the Desktop version, the callback will also triggered in scenarios explained in Unpin Callback"
  },
  {
    "id": 25459,
    "content": "Since cudaHostAlloc() can be allocated with cudaHostAllocWriteCombined flag or default flag, applications are expected to excercise caution when mapping the memory to userspace, for example using standard linux mmap() In this regard: When GPU memory is allocated as writecombined, the userspace mapping should also be done as writecombined by passing the vm_page_prot member of vm_area_struct to the"
  },
  {
    "id": 25460,
    "content": "standard linux interface: `pgprot_writecombine() `__ When GPU memory is allocated as default, no modifcations to the vm_page_prot member of vm_area_struct should be done"
  },
  {
    "id": 25465,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 25466,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 25468,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 25469,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 25470,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 25471,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 25472,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 25473,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 25474,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 25475,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 25476,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 25477,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 25478,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 25485,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 25487,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 25488,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2012-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 25492,
    "content": "pageBottom();}\nNVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12"
  },
  {
    "id": 25496,
    "content": "1 ( older ) - Last updated July 1, 2007-2024 - Send Feedback Debugger API The API reference guide for the CUDA debugger"
  },
  {
    "id": 25497,
    "content": "Deprecated List Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024 NVIDIA Corporation var switchTo5x=true; stLight"
  },
  {
    "id": 25498,
    "content": "options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); _satellite"
  },
  {
    "id": 25502,
    "content": "Namespaces Copyright and Licenses NVIDIA Software License Agreement Third Party Licenses Boost Flatbuffers Font - Cascadia Mono Font - Open Sans Font - Roboto Microsoft Detours Notices Notice Trademarks Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact © Copyright 2018-2024, NVIDIA Corporation &"
  },
  {
    "id": 25504,
    "content": "Why CUDA Compatibility v555 | PDF CUDA Compatibility CUDA Compatibility describes the use of new CUDA toolkit components on systems with older base installations Why CUDA Compatibility  The NVIDIA® CUDA® Toolkit enables developers to build NVIDIA GPU accelerated compute applications for desktop computers, enterprise, and data centers to hyperscalers It consists of the CUDA compiler toolchain"
  },
  {
    "id": 25505,
    "content": "including the CUDA runtime (cudart) and various CUDA libraries and tools To build an application, a developer has to install only the CUDA Toolkit and necessary libraries required for linking In order to run a CUDA application, the system should have a CUDA enabled GPU and an NVIDIA display driver that is compatible with the CUDA Toolkit that was used to build the application itself If the"
  },
  {
    "id": 25506,
    "content": "application relies on dynamic linking for libraries, then the system should have the right version of such libraries as well Figure 1 Components of CUDA  Every CUDA toolkit also ships with an NVIDIA display driver package for convenience The driver package includes both the user mode CUDA driver (libcuda so) and kernel mode components necessary to run the application Typically, upgrading a CUDA"
  },
  {
    "id": 25507,
    "content": "Toolkit involves upgrading both the toolkit and the driver to get the bleeding edge toolkit and driver capabilities CUDA Compatibility guarantees allow for upgrading only certain components and that will be the focus of the rest of this document We will see how the upgrade to a new CUDA Toolkit can be simplified to not always require a full system upgrade"
  },
  {
    "id": 25511,
    "content": "CUDA 11 and Later Defaults to Minor Version Compatibility  From CUDA 11 onwards, applications compiled with a CUDA Toolkit release from within a CUDA major release family can run, with limited feature-set, on systems having at least the minimum required driver version as indicated below This minimum required driver can be different from the driver packaged with the CUDA Toolkit but should belong"
  },
  {
    "id": 25514,
    "content": "x Minimum Required Driver Versions (Refer to CUDA Release Notes)  CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 12"
  },
  {
    "id": 25528,
    "content": "While applications built against any of the older CUDA Toolkits always continued to function on newer drivers due to binary backward compatibility, before CUDA 11, applications built against newer CUDA Toolkit releases were not supported on older drivers without forward compatibility package"
  },
  {
    "id": 25530,
    "content": "x minor release, then the minimum required driver version is the same as the driver that’s packaged as part of that toolkit release Consequently, the minimum required driver version changed for every new CUDA Toolkit minor release until CUDA 11"
  },
  {
    "id": 25532,
    "content": "Therefore, system administrators always have to upgrade drivers in order to support applications built against CUDA Toolkits from 10"
  },
  {
    "id": 25534,
    "content": "Table 2 CUDA Toolkit 10 x Minimum Required Driver Versions  CUDA Toolkit Linux x86_64 Minimum Required Driver Version Windows Minimum Required Driver Version CUDA 10"
  },
  {
    "id": 25546,
    "content": "0, as shown below: Minimum required driver version guidance can be found in the CUDA Toolkit Release Notes Note that if the minimum required driver version is not installed in the system, applications will return an error as shown below"
  },
  {
    "id": 25549,
    "content": "Application Considerations for Minor Version Compatibility  Developers and system admins should note two important caveats when relying on minor version compatibility If either of these caveats are limiting, then a new CUDA driver from the same minor version of the toolkit that the application was built with or later is required Limited feature set Sometimes features introduced in a CUDA Toolkit"
  },
  {
    "id": 25550,
    "content": "version may actually span both the toolkit and the driver In such cases an application that relies on features introduced in a newer version of the toolkit and driver may return the following error on older drivers: cudaErrorCallRequiresNewerDriver"
  },
  {
    "id": 25551,
    "content": "Application developers can avoid running into this problem by having the application explicitly check for the availability of features"
  },
  {
    "id": 25552,
    "content": "Applications using PTX will see runtime issues Applications that compile device code to PTX will not work on older drivers"
  },
  {
    "id": 25553,
    "content": "PTX Developers should refer to the CUDA Compatibility Developers Guide and PTX programming guide in the CUDA C++ Programming Guide for details on this limitation"
  },
  {
    "id": 25556,
    "content": "Deployment Considerations for Minor Version Compatibility  As described, applications that directly rely only on the CUDA runtime can be deployed in the following two scenarios: CUDA driver that’s installed on the system is newer than the runtime CUDA runtime is newer than the CUDA driver on the system but they are from the same major release of CUDA Toolkit"
  },
  {
    "id": 25557,
    "content": "In scenario 2, system admins should be aware of the aforementioned limitations and should be able to tell why an application may be failing if they run into any issues"
  },
  {
    "id": 25558,
    "content": "Minor version compatibility has another benefit that offers flexibility in the use and deployment of libraries Applications that use libraries that support minor version compatibility can be deployed on systems with a different version of the toolkit and libraries without recompiling the application for the difference in the library version This holds true for both older and newer versions of the"
  },
  {
    "id": 25559,
    "content": "libraries provided they are all from the same major release family Figure 3 NVRTC supports minor version compatibility from CUDA 11 3 onwards  However, if an application is unable to leverage the minor version compatibility due to any of the aforementioned reasons, then the Forward Compatibility model can be used as an alternative even though Forward Compatibility is mainly intended for"
  },
  {
    "id": 25564,
    "content": "Forward Compatibility Support Across Major Toolkit Versions  Increasingly, data centers and enterprises may not want to update the NVIDIA GPU Driver across major release versions due to the rigorous testing and validation that happens before any system level driver installations are done"
  },
  {
    "id": 25567,
    "content": "Figure 4 Forward Compatibility Upgrade Path  Forward Compatibility is applicable only for systems with NVIDIA Data Center GPUs or select NGC Server Ready SKUs of RTX cards"
  },
  {
    "id": 25568,
    "content": "It’s mainly intended to support applications built on newer CUDA Toolkits to run on systems installed with an older NVIDIA Linux GPU driver from different major release families"
  },
  {
    "id": 25569,
    "content": "This new forward-compatible upgrade path requires the use of a special package called “CUDA compat package”"
  },
  {
    "id": 25575,
    "content": "From Network Repositories or Local Installers  The CUDA compat package is available in the local installers or the CUDA network repositories provided by NVIDIA as cuda-compat-12"
  },
  {
    "id": 25577,
    "content": "On Ubuntu, for example: The compat package will then be installed to the versioned toolkit location typically found in the toolkit directory"
  },
  {
    "id": 25579,
    "content": "8 and later only) These files should be kept together as the CUDA driver is dependent on the libnvidia-ptxjitcompiler"
  },
  {
    "id": 25581,
    "content": "Example: CUDA Compatibility is installed and the application can now run successfully as shown below In this example, the user sets LD_LIBRARY_PATH to include the files installed by the cuda-compat-12-1 package Check the files installed under /usr/local/cuda/compat : The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 12 1 application: 3"
  },
  {
    "id": 25584,
    "content": "Manually Installing from Runfile  The cuda-compat package files can also be extracted from the appropriate datacenter driver ‘runfile’ installers ("
  },
  {
    "id": 25588,
    "content": "Copy the four CUDA compatibility upgrade files, listed at the start of this section, into a user- or root-created directory"
  },
  {
    "id": 25590,
    "content": "Note Symlinks under /usr/local/cuda/compat need to be created manually when using the runfile installer"
  },
  {
    "id": 25596,
    "content": "Use the Right Compat Package  CUDA forward compat packages should be used only in the following situations when forward compatibility is required across major releases"
  },
  {
    "id": 25600,
    "content": "But when performing a full system upgrade, when choosing to install both the toolkit and the driver, remove any forward compatible packages present in the system"
  },
  {
    "id": 25603,
    "content": "13 which is the minimum required driver version for the 12 x toolkits, then the cuda-compat package is not required in most cases"
  },
  {
    "id": 25605,
    "content": "x and 12 x applications will be supported due to backward compatibility and future 12 x applications will be supported due to minor-version compatibility"
  },
  {
    "id": 25606,
    "content": "But there are feature restrictions that may make this option less desirable for your scenario - for example: Applications requiring PTX JIT compilation support"
  },
  {
    "id": 25607,
    "content": "Unlike the minor-version compatibility that is defined between CUDA runtime and CUDA driver, forward compatibility is defined between the kernel driver and the CUDA driver, and hence such restrictions do not apply In order to circumvent the limitation, a forward compatibility package may be used in such scenarios as well Table 3 CUDA Application Compatibility Support Matrix  NVIDIA Kernel Mode"
  },
  {
    "id": 25625,
    "content": "02+ (CUDA 12 5) 12-5 C X C X C Not required 12-4 C C Not required X 12-3 C C X X 12-2 C Not required X X 12-1 C X X X 12-0 C X X X 11-8 C X X X 11-7 C X X X 11-6 C X X X 11-5 C X X X 11-4 Not required X X X C - Compatible X - Not compatible Branches R525, R515, R510, R465, R460, R455, R450, R440, R418, R410, R396, R390 are end of life and are not supported targets for compatibility"
  },
  {
    "id": 25628,
    "content": "Examples of how to read this table: The CUDA 12-4 compat package is “C”ompatible with driver versions 470, 535"
  },
  {
    "id": 25629,
    "content": "It is “Not required” for 550, as 12 4 was paired with 550 and therefore no extra packages are needed The CUDA “12-3” release is not-compatible (“X”) with driver version 550 as it was released prior to the driver Binaries created in 12"
  },
  {
    "id": 25634,
    "content": "Feature Exceptions  There are specific features in the CUDA driver that require kernel-mode support and will only work with a newer kernel mode driver"
  },
  {
    "id": 25635,
    "content": "Table 4 Forward-Compatible Feature-Driver Support Matrix  CUDA Forward Compatible Upgrade CUDA - OpenGL/Vulkan Interop cuMemMap* set of functionalities System Base Installation: 525 (>="
  },
  {
    "id": 25639,
    "content": "02) Driver 11-x No Yes [1] [1] This relies on CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED and CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED , which should be queried if you intend to use the full range of this functionality"
  },
  {
    "id": 25645,
    "content": "Check for Compatibility Support  In addition to the CUDA driver and certain compiler components, there are other drivers in the system installation stack (for example, OpenCL) that remain on the old version"
  },
  {
    "id": 25646,
    "content": "A well-written application should use following error codes to determine if CUDA Forward Compatible Upgrade is supported System administrators should be aware of these error codes to determine if there are errors in the deployment This error indicates that there is a mismatch between the versions of the display driver and the CUDA driver This error indicates that the system was upgraded to run"
  },
  {
    "id": 25647,
    "content": "with forward compatibility but the visible hardware detected by CUDA does not support this configuration"
  },
  {
    "id": 25650,
    "content": "Deployment Model for Forward Compatibility  There are two models of deployment for the CUDA compat package Shared deployment: Allows sharing the same compat package across installed toolkits in the system Download and extract the latest forward compatibility package with the highest toolkit version in its name"
  },
  {
    "id": 25654,
    "content": "The user can set LD_LIBRARY_PATH to include the files installed before running the CUDA 11 1 application: $ LD_LIBRARY_PATH =/ usr / local / cuda / compat : $LD_LIBRARY_PATH Per-application deployment: Individual applications can choose a package of their choice and place it as part of the Modules system tied to the toolkit and the libraries Using the Modules system, the admin, or the user, can"
  },
  {
    "id": 25655,
    "content": "set up ‘module’ scripts for each version of each toolkit package, and then load the module script for the toolkit as needed $ module load cuda / 11 0 There is an important consideration to the per-application deployment approach: older forward compatibility packages are not supported on new driver versions Therefore the module load scripts should proactively query the system for whether the"
  },
  {
    "id": 25656,
    "content": "compatibility package can be used before loading the files In the cases where the module script cannot use CUDA compatible upgrade, a fallback path to the default system’s installed CUDA driver can provide a more consistent experience and this can be achieved using RPATH"
  },
  {
    "id": 25658,
    "content": "Conclusion  The CUDA driver maintains backward compatibility to continue support of applications built on older toolkits Using a compatible minor driver version, applications build on CUDA Toolkit 11 and newer are supported on any driver from within the corresponding major release Using the CUDA Forward Compatibility package, system administrators can run applications built using a newer toolkit"
  },
  {
    "id": 25659,
    "content": "even when an older driver that does not satisfy the minimum required driver version is installed on the system This forward compatibility allows the CUDA deployments in data centers and enterprises to benefit from the faster release cadence and the latest features and performance of CUDA Toolkit CUDA compatibility helps users by: Faster upgrades to the latest CUDA releases: Enterprises or data"
  },
  {
    "id": 25660,
    "content": "centers with NVIDIA GPUs have complex workflows and require advance planning for NVIDIA driver rollouts Not having to update the driver for newer CUDA releases can mean that new versions of the software can be made available faster to users without any delays Faster upgrades of the CUDA libraries: Users can upgrade to the latest software libraries and applications built on top of CUDA (for"
  },
  {
    "id": 25661,
    "content": "example, math libraries or deep learning frameworks) without an upgrade to the entire CUDA Toolkit or driver This is possible as these libraries and frameworks do not have a direct dependency on the CUDA runtime, compiler or driver"
  },
  {
    "id": 25666,
    "content": "Area CUDA Forward Compatible Upgrade CUDA Minor Version Compatibility Compatibility Across older drivers from different major release versions of CUDA"
  },
  {
    "id": 25670,
    "content": "4 UMD (User Mode Driver) and later will extend forward compatibility support to select NGC Ready NVIDIA RTX boards"
  },
  {
    "id": 25671,
    "content": "All GPU products supported OS distributions supported Linux only Windows, Linux Features supported Some features such as (CUDA-GL interop, Power 9 ATS, cuMemMap APIs) are not supported"
  },
  {
    "id": 25672,
    "content": "Users may have to incorporate checks in their application when using new features in the minor release (that require a new driver) to ensure graceful errors"
  },
  {
    "id": 25673,
    "content": "CUDA releases supported All CUDA releases supported through the lifetime of the datacenter driver branch"
  },
  {
    "id": 25675,
    "content": "1) EOLs in March 2022 - so all CUDA versions released (including major releases) during this timeframe are supported"
  },
  {
    "id": 25677,
    "content": "Does CUDA forward compatible upgrades work intra-branch Users can upgrade the kernel mode driver within the same branch"
  },
  {
    "id": 25678,
    "content": "This use-case is supported only for drivers on LLB and LTS branches of driver for select GPUs Which GPUs are supported by the driver"
  },
  {
    "id": 25679,
    "content": "The CUDA compatible upgrade is meant to ease the management of large production systems for enterprise customers"
  },
  {
    "id": 25681,
    "content": "4 UMD (User Mode Driver) and later will extend forward compatibility support to select NGC Ready NVIDIA RTX boards It’s important to note that HW support is defined by the kernel mode driver and as such, newer CUDA drivers on their own will not enable new HW support Hardware Generation Compute Capability CTK Support Latest Forward Compatibility Package Support Driver Current Minimum Driver in"
  },
  {
    "id": 25702,
    "content": "If we build our CUDA application using CUDA 11 0, can it continue to be used with newer NVIDIA drivers (such as CUDA 11"
  },
  {
    "id": 25710,
    "content": "CUDA applications typically statically include all the libraries (for example cudart, CUDA math libraries such as cuBLAS, cuFFT) they need, so they should work on new drivers or CUDA Toolkit installations In other words, since CUDA is backward compatible, existing CUDA applications can continue to be used with newer CUDA versions"
  },
  {
    "id": 25712,
    "content": "x driver that supports the CUDA minor version compatibility The minimum driver version required is 450"
  },
  {
    "id": 25720,
    "content": "By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features A subset of CUDA APIs don’t need a new driver and they can all be used without any driver dependencies To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully This situation is not"
  },
  {
    "id": 25721,
    "content": "different from what is available today where developers use macros to compile out features based on CUDA versions Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release"
  },
  {
    "id": 25722,
    "content": "There are some issues that admins can advise the application developers to accommodate in their code"
  },
  {
    "id": 25723,
    "content": "CUDA minor version compatibility and CUDA forward compatible upgrade both work when using either NGC Deep Learning Framework containers or using containers that are based on the official CUDA base images The images include the CUDA compatible upgrade libraries and the NVIDIA Container Toolkit (nvidia-docker2) has logic to correctly load the required libraries I’m running an NGC container and see"
  },
  {
    "id": 25729,
    "content": "What could be wrong It is possible you are either running a wrong version of the NVIDIA driver on the system or your system does not have an NVIDIA Data Center GPU"
  },
  {
    "id": 25731,
    "content": "Notices  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 25732,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 25734,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 25735,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 25736,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 25737,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 25738,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 25739,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 25740,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 25741,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 25742,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 25743,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 25744,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 25747,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 25749,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 25750,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 25754,
    "content": "pageBottom();} NVIDIA NVIDIA Data Center GPU Driver Documentation Search In: Entire Site Just This Document clear search search NVIDIA Data Center GPU NVIDIA Multi-Instance GPU User Guide 1 Changelog Search Results NVIDIA Multi-Instance GPU User Guide ( PDF ) - Last updated March 26, 2024 - NVIDIA Multi-Instance GPU User Guide User guide for Multi-Instance GPU on the NVIDIA® GPUs This edition of"
  },
  {
    "id": 25755,
    "content": "the user guide describes the Multi-Instance GPU feature first introduced with the NVIDIA® Ampere architecture Introduction The new Multi-Instance GPU (MIG) feature allows GPUs (starting with NVIDIA Ampere architecture) to be securely partitioned into up to seven separate GPU Instances for CUDA applications, providing multiple users with separate GPU resources for optimal GPU utilization This"
  },
  {
    "id": 25756,
    "content": "feature is particularly beneficial for workloads that do not fully saturate the GPU's compute capacity and therefore users may want to run different workloads in parallel to maximize utilization"
  },
  {
    "id": 25757,
    "content": "For Cloud Service Providers (CSPs), who have multi-tenant use cases, MIG ensures one client cannot impact the work or scheduling of other clients, in addition to providing enhanced isolation for customers"
  },
  {
    "id": 25758,
    "content": "With MIG, each instance's processors have separate and isolated paths through the entire memory system - the on-chip crossbar ports, L2 cache banks, memory controllers, and DRAM address busses are all assigned uniquely to an individual instance This ensures that an individual user's workload can run with predictable throughput and latency, with the same L2 cache allocation and DRAM bandwidth,"
  },
  {
    "id": 25760,
    "content": "MIG can partition available GPU compute resources (including streaming multiprocessors or SMs, and GPU engines such as copy engines or decoders), to provide a defined quality of service (QoS) with fault isolation for different clients such as VMs, containers or processes"
  },
  {
    "id": 25761,
    "content": "MIG enables multiple GPU Instances to run in parallel on a single, physical NVIDIA Ampere GPU With MIG, users will be able to see and schedule jobs on their new virtual GPU Instances as if they were physical GPUs"
  },
  {
    "id": 25762,
    "content": "MIG works with Linux operating systems, supports containers using Docker Engine, with support for Kubernetes and virtual machines using hypervisors such as Red Hat Virtualization and VMware vSphere MIG supports the following deployment configurations: Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported"
  },
  {
    "id": 25763,
    "content": "hypervisors MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single GPU, while preserving the isolation guarantees that vGPU provides For more information on GPU partitioning using vGPU and MIG, refer to the technical brief MIG Overview The purpose of this document is to introduce the concepts behind MIG, deployment considerations and provide examples of MIG management to"
  },
  {
    "id": 25765,
    "content": "Supported GPU Products Product Architecture Microarchitecture Compute Capability Memory Size Max Number of Instances H100-SXM5 Hopper GH100 9 0 80GB 7 H100-PCIE Hopper GH100 9 0 80GB 7 H100-SXM5 Hopper GH100 9 0 94GB 7 H100-PCIE Hopper GH100 9 0 94GB 7 H100 on GH200 Hopper GH100 9"
  },
  {
    "id": 25766,
    "content": "0 96GB 7 A100-SXM4 NVIDIA Ampere GA100 8 0 40GB 7 A100-SXM4 NVIDIA Ampere GA100 8 0 80GB 7 A100-PCIE NVIDIA Ampere GA100 8 0 40GB 7 A100-PCIE NVIDIA Ampere GA100 8 0 80GB 7 A30 NVIDIA Ampere GA100 8"
  },
  {
    "id": 25767,
    "content": "0 24GB 4 Additionally, MIG is supported on systems that include the supported products above such as DGX, DGX Station and HGX Supported Configurations Supported deployment configurations with MIG include Bare-metal, including containers and Kubernetes GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors Virtualization MIG can be used"
  },
  {
    "id": 25768,
    "content": "with two types of virtualization: Under Linux guests on supported hypervisors, when MIG-supported GPUs are in GPU pass-through, the same workflows , tools and profiles available on bare-metal can be used MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single MIG-supported GPU, while preserving the isolation guarantees that vGPU provides To configure a GPU for use with vGPU VMs,"
  },
  {
    "id": 25770,
    "content": "Concepts Terminology This section introduces some terminology used to describe the concepts behind MIG"
  },
  {
    "id": 25772,
    "content": "It encapsulates all the resources necessary to execute operations on the GPU, including a distinct address space, memory allocations, etc A GPU context has the following properties: Fault isolation Individually scheduled Distinct address space GPU Engine A GPU engine is what executes work on the GPU The most commonly used engine is the Compute/Graphics engine that executes the compute"
  },
  {
    "id": 25773,
    "content": "instructions Other engines include the copy engine (CE) that is responsible for performing DMAs, NVDEC for video decoding, NVENC for encoding, etc GPU Memory Slice A GPU memory slice is the smallest fraction of the GPU's memory, including the corresponding memory controllers and cache A GPU memory slice is roughly one eighth of the total GPU memory resources, including both capacity and bandwidth"
  },
  {
    "id": 25774,
    "content": "A GPU SM slice is roughly one seventh of the total number of SMs available in the GPU when configured in MIG mode GPU Slice A GPU slice is the smallest fraction of the GPU that combines a single GPU memory slice and a single GPU SM slice GPU Instance A GPU Instance (GI) is a combination of GPU slices and GPU engines (DMAs, NVDECs, etc"
  },
  {
    "id": 25776,
    "content": "Anything within a GPU instance always shares all the GPU memory slices and other GPU engines, but it's SM slices can be further subdivided into compute instances (CI) Each GPU slice includes dedicated GPU memory resources which limit both the available capacity and bandwidth, and provide memory QoS Each GPU memory slice gets 1/8 of the total GPU memory resources and each GPU SM slice gets 1/7 of"
  },
  {
    "id": 25777,
    "content": "the total number of SMs A Compute Instance (CI) contains a subset of the parent GPU instance's SM slices and other GPU engines (DMAs, NVDECs, etc"
  },
  {
    "id": 25779,
    "content": "Partitioning Using the concepts introduced above, this section provides an overview of how the user can create various partitions on the GPU"
  },
  {
    "id": 25780,
    "content": "For illustration purposes, the document will use the A100-40GB as an example, but the process is similar for other GPUs that support MIG GPU Instance Partitioning of the GPU happens using memory slices, so the A100-40GB GPU can be thought of having 8x5GB memory slices and 7 SM slices as shown in the diagram below Available Slices on A100 As explained above, then to create a GPU Instance (GI)"
  },
  {
    "id": 25782,
    "content": "In the diagram below, a 5GB memory slice is combined with 1 compute slice to create a 1g 5gb GI profile: Figure 3 Combining Memory and Compute Slices Similarly, 4x5GB memory slices can be combined with 4x1 compute slices to create the 4g 5gb GI profile: Figure 4 Combining Memory and Compute Slices Compute Instance The compute slices of a GPU Instance can be further subdivided into multiple"
  },
  {
    "id": 25783,
    "content": "Compute Instances (CI), with the CIs sharing the engines and memory of the parent GI, but each CI has dedicated SM resources Using the same 4g 20gb example above, a CI may be created to consume only the first compute slice as shown below: Figure 5 Combining Memory and Compute Slices In this case, 4 different CIs can be created by choosing any of the compute slices Two compute slices can also be"
  },
  {
    "id": 25784,
    "content": "combined together to create a 2c 4g 20gb profile: Figure 6 Combining Memory and Compute Slices In this example, 3 compute slices can also be combined to create a 3c 4g 20gb profile or all 4 can be combined to create a 4c 4g 20gb profile When all 4 compute slices are combined, the profile is simply referred to as the 4g 20gb profile"
  },
  {
    "id": 25785,
    "content": "The NVIDIA driver APIs provide a number of “GPU Instance Profiles” and users can create GIs by specifying one of these profiles On a given GPU, multiple GIs can be created from a mix and match of these profiles, so long as enough slices are available to satisfy the request"
  },
  {
    "id": 25786,
    "content": "For A100-SXM4-80GB, the profile names will change according to the memory proportion - for example, 1g"
  },
  {
    "id": 25791,
    "content": "For a list of all supported combinations of profiles on MIG-enabled GPUs, refer to the section on supported profiles GPU Instance Profiles on A100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g 5gb 1/8 1/7 0 NVDECs /0 JPEG /0 OFA 1/8 1 7 MIG 1g 5gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can"
  },
  {
    "id": 25792,
    "content": "include media extensions) MIG 1g 10gb 1/8 1/7 1 NVDECs /0 JPEG /0 OFA 1/8 1 4 MIG 2g 10gb 2/8 2/7 1 NVDECs /0 JPEG /0 OFA 2/8 2 3 MIG 3g 20gb 4/8 3/7 2 NVDECs /0 JPEG /0 OFA 4/8 3 2 MIG 4g 20gb 4/8 4/7 2 NVDECs /0 JPEG /0 OFA 4/8 4 1 MIG 7g 40gb Full 7/7 5 NVDECs /1 JPEG /1 OFA Full 7 1 The diagram below shows a pictorial representation of how to build all valid combinations of GPU instances MIG"
  },
  {
    "id": 25793,
    "content": "Profiles on A100 In this diagram, a valid combination can be built by starting with an instance profile on the left and combining it with other instance profiles as you move to the right, such that no two profiles overlap vertically For a list of all supported combinations and placements of profiles on A100 and A30, refer to the section on supported profiles"
  },
  {
    "id": 25794,
    "content": "Note that prior to NVIDIA driver release R510, the combination of a (4 memory, 4 compute) and a (4 memory, 3 compute) profile was not supported"
  },
  {
    "id": 25795,
    "content": "Profile Placements on A100 Note that the diagram represents the physical layout of where the GPU Instances will exist once they are instantiated on the GPU As GPU Instances are created and destroyed at different locations, fragmentation can occur, and the physical position of one GPU Instance will play a role in which other GPU Instances can be instantiated next to it"
  },
  {
    "id": 25796,
    "content": "CUDA Concurrency Mechanisms MIG has been designed to be largely transparent to CUDA applications - so that the CUDA programming model remains unchanged to minimize programming effort"
  },
  {
    "id": 25797,
    "content": "CUDA already exposes multiple technologies for running work in parallel on the GPU and it is worthwhile showcasing how these technologies compare to MIG"
  },
  {
    "id": 25798,
    "content": "Note that streams and MPS are part of the CUDA programming model and thus work when used with GPU Instances CUDA Streams are a CUDA Programming model feature where, in a CUDA application, different work can be submitted to independent queues and be processed independently by the GPU CUDA streams can only be used within a single process and don't offer much isolation - the address space is shared,"
  },
  {
    "id": 25800,
    "content": "It's commonly used by MPI jobs that cooperate, but it has also been used for sharing the GPU resources among unrelated applications, while accepting the challenges that such a solution brings"
  },
  {
    "id": 25801,
    "content": "MPS currently does not offer error isolation between clients and while streaming multiprocessors used by each MPS client can be optionally limited to a percentage of all SMs, the scheduling hardware is still shared"
  },
  {
    "id": 25802,
    "content": "Lastly, MIG is the new form of concurrency offered by NVIDIA GPUs while addressing some of the limitations with the other CUDA technologies for running parallel work"
  },
  {
    "id": 25803,
    "content": "CUDA Concurrency Mechanisms Streams MPS MIG Partition Type Single Process Logical Physical Max Partitions Unlimited 48 7 SM Performance Isolation No Yes (by percentage, not partitioning) Yes Memory Protection No Yes Yes Memory Bandwidth QoS No No Yes Error Isolation No No Yes Cross-Partition Interop Always IPC Limited IPC Reconfigure Dynamic Process Launch When Idle Deployment Considerations MIG"
  },
  {
    "id": 25805,
    "content": "System Considerations The following system considerations are relevant for when the GPU is in MIG mode The /proc mechanism for system-level interfaces is deprecated as of 450"
  },
  {
    "id": 25807,
    "content": "06 and it is recommended to use the /dev based system-level interface for controlling access mechanisms of MIG devices through cgroups"
  },
  {
    "id": 25808,
    "content": "Supported configurations include Bare-metal, including containers GPU pass-through virtualization to Linux guests on top of supported hypervisors vGPU on top of supported hypervisors MIG allows multiple vGPUs (and thereby VMs) to run in parallel on a single A100, while preserving the isolation guarantees that vGPU provides"
  },
  {
    "id": 25809,
    "content": "On NVIDIA Ampere GPUs, similar to ECC mode, MIG mode setting is persistent across reboots until the user toggles the setting explicitly All daemons holding handles on driver modules need to be stopped before MIG enablement"
  },
  {
    "id": 25810,
    "content": "This is true for systems such as DGX which may be running system health monitoring services such as nvsm or GPU health monitoring or telemetry services such as DCGM"
  },
  {
    "id": 25811,
    "content": "Other MIG management, such as creating and destroying instances, requires superuser by default, but can be delegated to non-privileged users by adjusting permissions to MIG capabilities in /proc/"
  },
  {
    "id": 25812,
    "content": "Application Considerations Users should note the following considerations when the A100 is in MIG mode: No graphics APIs are supported (e"
  },
  {
    "id": 25815,
    "content": ") No GPU to GPU P2P (either PCIe or NVLink) is supported CUDA applications treat a Compute Instance and its parent GPU Instance as a single CUDA device See this section on device enumeration by CUDA CUDA IPC across GPU instances is not supported using cuda-memcheck or compute-sanitizer) is supported CUDA MPS is supported on top of MIG The only limitation is that the maximum number of clients (48)"
  },
  {
    "id": 25816,
    "content": "is lowered proportionally to the Compute Instance size GPUDirect RDMA is supported when used from GPU Instances MIG Device Names By default, a MIG device consists of a single “GPU Instance” and a single “Compute Instance” The table below highlights a naming convention to refer to a MIG device by its GPU Instance's compute slice count and its total memory in GB (rather than just its memory slice"
  },
  {
    "id": 25817,
    "content": "count) When only a single CI is created (that consumes the entire compute capacity of the GI), then the CI sizing is implied in the device name MIG Device Name Note: The description below shows the profile names on the A100-SXM4-40GB product Device names when using a single CI Memory 20gb 10gb 5gb GPU Instance 3g 2g 1g Compute Instance 3c 2c 1c MIG Device 3g 20gb 2g 10gb 1g"
  },
  {
    "id": 25818,
    "content": "5gb GPC GPC GPC GPC GPC GPC Each GI can be further sub-divided into multiple CIs as required by users depending on their workloads"
  },
  {
    "id": 25820,
    "content": "20gb device into a set of sub-devices with different Compute Instance slice counts Device names when using multiple CIs Memory 20gb 20gb GPU Instance 3g 3g Compute Instance 1c 1c 1c 2c 1c MIG Device 1c 3g 20gb 1c 3g 20gb 1c 3g 20gb 2c 3g 20gb 1c 3g"
  },
  {
    "id": 25821,
    "content": "20gb GPC GPC GPC GPC GPC GPC Device Enumeration GPU Instances (GIs) and Compute Instances (CIs) are enumerated in the new /proc filesystem layout for MIG $ ls -l /proc/driver/nvidia-caps/ -r--r--r-- 1 root root 0 Nov 21 21:22 mig-minors -r--r--r-- 1 root root 0 Nov 21 21:22 nvlink-minors -r--r--r-- 1 root root 0 Nov 21 21:22 sys-minors The corresponding device nodes (in mig-minors ) are created"
  },
  {
    "id": 25823,
    "content": "CUDA Device Enumeration MIG supports running CUDA applications by specifying the CUDA device on which the application should be run With CUDA 11/R450 and CUDA 12/R525, only enumeration of a single MIG instance is supported In other words, regardless of how many MIG devices are created (or made available to a container), a single CUDA process can only enumerate a single MIG device CUDA is limited"
  },
  {
    "id": 25824,
    "content": "to use a single CI and will pick the first one available if several of them are visible To summarize, there are two constraints: CUDA can only enumerate a single compute instance CUDA will not enumerate non-MIG GPU if any compute instance is enumerated on any other GPU Note that these constraints may be relaxed in future NVIDIA driver releases for MIG"
  },
  {
    "id": 25830,
    "content": "01 +), the example below shows how MIG devices are assigned GPU UUIDs in an 8-GPU system with each GPU configured differently"
  },
  {
    "id": 25831,
    "content": "A30 MIG Profiles The following diagram shows the profiles supported on the NVIDIA A30: Figure 10 GPU Instance Profiles on A30 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g 6gb 1/4 1/4 0 NVDECs /0 JPEG /0 OFA 1/4 1 4 MIG 1g 6gb+me 1/4 1/4 1 NVDEC /1 JPEG /1 OFA 1/4 1 1 (A single 1g profile can include media"
  },
  {
    "id": 25832,
    "content": "extensions) MIG 2g 12gb 2/4 2/4 2 NVDECs /0 JPEG /0 OFA 2/4 2 2 MIG 2g 12gb+me 2/4 2/4 2 NVDECs /1 JPEG /1 OFA 2/4 2 1 (A single 2g profile can include media extensions) MIG 4g 24gb Full 4/4 4 NVDECs /1 JPEG /1 OFA Full 4 1 Note: The 1g 6gb+me profile is only available starting with R470 drivers A100 MIG Profiles The following diagram shows the profiles supported on the NVIDIA A100: Figure 11"
  },
  {
    "id": 25833,
    "content": "Profiles on A100 The table below shows the supported profiles on the A100-SXM4-40GB product For A100-SXM4-80GB, the profile names will change according to the memory proportion - for example, 1g 10gb , 1g 10gb+me , 1g 20gb , 2g 20gb , 3g 40gb , 4g 40gb , 7g 80gb respectively GPU Instance Profiles on A100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines"
  },
  {
    "id": 25834,
    "content": "Number of Instances Available MIG 1g 5gb 1/8 1/7 0 NVDECs /0 JPEG /0 OFA 1/8 1 7 MIG 1g 5gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g 10gb 1/8 1/7 1 NVDECs /0 JPEG /0 OFA 1/8 1 4 MIG 2g 10gb 2/8 2/7 1 NVDECs /0 JPEG /0 OFA 2/8 2 3 MIG 3g 20gb 4/8 3/7 2 NVDECs /0 JPEG /0 OFA 4/8 3 2 MIG 4g 20gb 4/8 4/7 2 NVDECs /0 JPEG /0 OFA 4/8 4 1 MIG 7g"
  },
  {
    "id": 25835,
    "content": "40gb Full 7/7 5 NVDECs /1 JPEG /1 OFA Full 7 1 Note: The 1g 5gb+me profile is only available starting with R470 drivers The 1g 10gb profile is only available starting with R525 drivers"
  },
  {
    "id": 25838,
    "content": "H100 MIG Profiles The following diagram shows the profiles supported on the NVIDIA H100: Figure 12 Profiles on H100 The table below shows the supported profiles on the H100 80GB product (PCIe and SXM5) GPU Instance Profiles on H100 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g 10gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8"
  },
  {
    "id": 25839,
    "content": "1 7 MIG 1g 10gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g 20gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g 20gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g 40gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g 40gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g 80gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 The table below shows the"
  },
  {
    "id": 25840,
    "content": "supported profiles on the H100 94GB product (PCIe and SXM5) Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g 11gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g 11gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g 22gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g 22gb"
  },
  {
    "id": 25841,
    "content": "2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g 44gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g 44gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g 88gb Full 7/7 7 NVDECs /7 JPEG /1 OFA Full 8 1 The table below shows the supported profiles on the H100 96GB product (H100 on GH200) Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances"
  },
  {
    "id": 25842,
    "content": "Available MIG 1g 12gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g 12gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g 24gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g 24gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g 48gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g 48gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g 96gb Full 7/7 7"
  },
  {
    "id": 25845,
    "content": "H200 MIG Profiles The following diagram shows the profiles supported on the NVIDIA H200: Figure 13 GPU Instance Profiles on H200 Profile Name Fraction of Memory Fraction of SMs Hardware Units L2 Cache Size Copy Engines Number of Instances Available MIG 1g"
  },
  {
    "id": 25846,
    "content": "18gb 1/8 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 7 MIG 1g 18gb+me 1/8 1/7 1 NVDEC /1 JPEG /1 OFA 1/8 1 1 (A single 1g profile can include media extensions) MIG 1g 35gb 1/4 1/7 1 NVDECs /1 JPEG /0 OFA 1/8 1 4 MIG 2g 35gb 2/8 2/7 2 NVDECs /2 JPEG /0 OFA 2/8 2 3 MIG 3g 71gb 4/8 3/7 3 NVDECs /3 JPEG /0 OFA 4/8 3 2 MIG 4g 71gb 4/8 4/7 4 NVDECs /4 JPEG /0 OFA 4/8 4 1 MIG 7g 141gb Full 7/7 7 NVDECs /7 JPEG /1"
  },
  {
    "id": 25847,
    "content": "OFA Full 8 1 Getting Started with MIG Prerequisites The following prerequisites and minimum software versions are recommended when using supported GPUs in MIG mode MIG is supported only on GPUs and systems listed here It is recommended to install the latest NVIDIA datacenter driver The minimum versions are provided below: If using H100, then CUDA 12 and NVIDIA driver R525 ( >= 525 53 ) or later If"
  },
  {
    "id": 25850,
    "content": "02 ) or later Linux operating system distributions supported by CUDA If running containers or using Kubernetes, then: NVIDIA Container Toolkit ( nvidia-docker2 ): v2"
  },
  {
    "id": 25856,
    "content": "0 or later MIG can be managed programmatically using NVIDIA Management Library (NVML) APIs or its command-line-interface, nvidia-smi"
  },
  {
    "id": 25857,
    "content": "Note that for brevity, some of the nvidia-smi output in the following examples may be cropped to showcase the relevant sections of interest For more information on the MIG commands, see the nvidia-smi man page or nvidia-smi mig --help For information on the MIG management APIs, see the NVML header ( nvml h ) included in the CUDA Toolkit packages ( cuda-nvml-dev-* ; installed under"
  },
  {
    "id": 25858,
    "content": "/usr/local/cuda/include/nvml h ) For automated tooling support with configuring MIG, refer to the NVIDIA MIG Part ition Ed itor (or mig-parted ) tools For example, running nvidia-smi shows that MIG mode is disabled: $ nvidia-smi -i 0 +-+ | NVIDIA-SMI 450"
  },
  {
    "id": 25866,
    "content": "0 Off | 0 | | N/A 29C P0 62W / 400W | 0MiB / 40537MiB | 6% Default | | | | Disabled | +-+-+-+ MIG mode can be enabled on a per-GPU basis with the following command: nvidia-smi -i -mig 1 When MIG is enabled on the GPU, depending on the GPU product, the driver will attempt to reset the GPU so that MIG mode can take effect $ nvidia-smi -i 0 --query-gpu=pci bus_id,mig mode current --format=csv pci"
  },
  {
    "id": 25867,
    "content": "bus_id, mig mode current 00000000:36:00 0, Enabled GPU Reset on Hopper+ GPUs Starting with the Hopper generation of GPUs, enabling MIG mode no longer requires a GPU reset to take effect (and thus the driver does not attempt to reset the GPU in the background) Note that MIG mode ( Disabled or Enabled states) is only persistent as long as the driver is resident in the system (i"
  },
  {
    "id": 25869,
    "content": "MIG mode is no longer persistent across system reboots (there is no longer a status bit stored in the GPU InfoROM) GPU Reset on Ampere GPUs On NVIDIA Ampere GPUs, when MIG mode is enabled, the driver will attempt to reset the GPU so that MIG mode can take effect Note that MIG mode ( Disabled or Enabled states) is persistent across system reboots (there is a status bit stored in the GPU InfoROM)"
  },
  {
    "id": 25870,
    "content": "Note: If you are using MIG inside a VM with NVIDIA Ampere GPUs (A100 or A30) in passthrough, then you may need to reboot the VM to allow the GPU to be in MIG mode as in some cases, GPU reset is not allowed via the hypervisor for security reasons This can be seen in the following example: $ sudo nvidia-smi -i 0 -mig 1 Warning: MIG mode is in pending enable state for GPU 00000000:00:03 0:Not"
  },
  {
    "id": 25871,
    "content": "Supported Reboot the system or try nvidia-smi --gpu-reset to make MIG mode effective on GPU 00000000:00:03"
  },
  {
    "id": 25873,
    "content": "For example, on DGX systems, you may encounter the following message: $ sudo nvidia-smi -i 0 -mig 1 Warning: MIG mode is in pending enable state for GPU 00000000:07:00 0:In use by another client 00000000:07:00"
  },
  {
    "id": 25876,
    "content": "Please first kill all processes using the device and retry the command or reboot the system to make MIG mode effective"
  },
  {
    "id": 25877,
    "content": "In this specific DGX example, you would have to stop the nvsm and dcgm services, enable MIG mode on the desired GPU and then restore the monitoring services: $ sudo systemctl stop nvsm $ sudo systemctl stop dcgm $ sudo nvidia-smi -i 0 -mig 1 Enabled MIG Mode for GPU 00000000:07:00"
  },
  {
    "id": 25879,
    "content": "As described in the Device Nodes section, granting read access to mig/config capabilities allows non-root users to manage instances once the GPU has been configured into MIG mode $ ls -l /proc/driver/nvidia/capabilities/* /proc/driver/nvidia/capabilities/mig: total 0 -r- 1 root root 0 May 24 16:10 config -r--r--r-- 1 root root 0 May 24 16:10 monitor List GPU Instance Profiles The NVIDIA driver"
  },
  {
    "id": 25880,
    "content": "provides a number of profiles that users can opt-in for when configuring the MIG feature in A100 The profiles are the sizes and capabilities of the GPU instances that can be created by the user The driver also provides information about the placements, which indicate the type and number of instances that can be created $ nvidia-smi mig -lgip +-+ | GPU instance profiles: | | GPU Name ID Instances"
  },
  {
    "id": 25894,
    "content": "The placement index shown indicates how the profiles are mapped on the GPU as shown in the supported profiles tables"
  },
  {
    "id": 25895,
    "content": "$ nvidia-smi mig -lgipp GPU 0 Profile ID 19 Placements: {0,1,2,3,4,5,6}:1 GPU 0 Profile ID 20 Placements: {0,1,2,3,4,5,6}:1 GPU 0 Profile ID 15 Placements: {0,2,4,6}:2 GPU 0 Profile ID 14 Placements: {0,2,4}:2 GPU 0 Profile ID 9 Placements: {0,4}:4 GPU 0 Profile ID 5 Placement : {0}:4 GPU 0 Profile ID 0 Placement : {0}:8 The command shows that the user can create two instances of type 3g 20gb"
  },
  {
    "id": 25896,
    "content": "(profile ID 9) or seven instances of 1g 5gb (profile ID 19) Creating GPU Instances Before starting to use MIG, the user needs to create GPU instances using the -cgi option One of three options can be used to specify the instance profiles to be created: Profile ID (e"
  },
  {
    "id": 25899,
    "content": "20gb ) Once the GPU instances are created, one needs to create the corresponding Compute Instances (CI) Note: Without creating GPU instances (and corresponding compute instances), CUDA workloads cannot be run on the GPU"
  },
  {
    "id": 25900,
    "content": "Thus, the user or system administrator needs to recreate the desired MIG configurations if the GPU or system is reset For automated tooling support for this purpose, refer to the NVIDIA MIG Part ition Ed itor (or mig-parted ) tool , including creating a systemd service that could recreate the MIG geometry at system startup"
  },
  {
    "id": 25901,
    "content": "The following example shows how the user can create GPU instances (and corresponding compute instances) In this example, the user can create two GPU instances (of type 3g"
  },
  {
    "id": 25903,
    "content": "If a mixed geometry of the profiles is specified by the user, then the NVIDIA driver chooses the placement of the various profiles After the instances are created, the placement of the profiles can be observed: $ sudo nvidia-smi mig -cgi 19,14,5 Successfully created GPU instance ID 13 on GPU 0 using profile MIG 1g 5gb (ID 19) Successfully created GPU instance ID 5 on GPU 0 using profile MIG 2g"
  },
  {
    "id": 25904,
    "content": "10gb (ID 14) Successfully created GPU instance ID 1 on GPU 0 using profile MIG 4g 20gb (ID 5) $ sudo nvidia-smi mig -lgi +-+ | GPU instances: | | GPU Name Profile Instance Placement | | ID ID Start:Size | |=| | 0 MIG 1g 5gb 19 13 6:1 | +-+ | 0 MIG 2g 10gb 14 5 4:2 | +-+ | 0 MIG 4g 20gb 5 1 0:4 | +-+ Example 2: Creation of a 3-2-1-1 geometry Note: Due to a known issue with the APIs, the profile ID"
  },
  {
    "id": 25905,
    "content": "9 or 3g 20gb must be specified first in order $ sudo nvidia-smi mig -cgi 19,19,14,9 Successfully created GPU instance ID 13 on GPU 0 using profile MIG 1g 5gb (ID 19) Successfully created GPU instance ID 11 on GPU 0 using profile MIG 1g 5gb (ID 19) Successfully created GPU instance ID 3 on GPU 0 using profile MIG 2g 10gb (ID 14) Unable to create a GPU instance on GPU 0 using profile 9: Insufficient"
  },
  {
    "id": 25906,
    "content": "Resources Failed to create GPU instances: Insufficient Resources Specify the correct order for the 3g 20gb profile In this example, the BlackScholes CUDA sample is run simultaneously on the two GIs created on the A100 From the previous example, the utilization is displayed as N/A when running CUDA programs: $ nvidia-smi +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared |"
  },
  {
    "id": 25907,
    "content": "| ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |=+=+=+=| | 0 1 0 0 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ | 0 2 0 1 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ +-+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=| | 0 1 0 6217 C inux/release/BlackScholes 253MiB | | 0 2 0 6223"
  },
  {
    "id": 25908,
    "content": "C inux/release/BlackScholes 253MiB | +-+ For monitoring MIG devices on MIG capable GPUs such as the A100, including attribution of GPU metrics (including utilization and other profiling metrics), it is recommended to use NVIDIA DCGM v2"
  },
  {
    "id": 25912,
    "content": "Compute Instances As explained earlier in this document, a further level of concurrency can be achieved by using Compute Instances (CIs)"
  },
  {
    "id": 25913,
    "content": "The following example shows how 3 CUDA processes (BlackScholes CUDA sample) can be run on the same GI"
  },
  {
    "id": 25914,
    "content": "First, list the available CI profiles available using our prior configuration of creating 2 GIs on the A100"
  },
  {
    "id": 25915,
    "content": "$ sudo nvidia-smi mig -lcip -gi 1 +-+ | Compute instance profiles: | | GPU GPU Name Profile Instances Exclusive Shared | | Instance ID Free/Total SM DEC ENC OFA | | ID CE JPEG | |=| | 0 1 MIG 1c"
  },
  {
    "id": 25918,
    "content": "3g 20gb 1 0/1 28 2 0 0 | | 3 0 | +-+ | 0 1 MIG 3g 20gb 2* 0/1 42 2 0 0 | | 3 0 | +-+ Create 3 CIs, each of type 1c compute capacity (profile ID 0) on the first GI $ sudo nvidia-smi mig -cci 0,0,0 -gi 1 Successfully created compute instance on GPU 0 GPU instance ID 1 using profile MIG 1c 3g 20gb (ID 0) Successfully created compute instance on GPU 0 GPU instance ID 1 using profile MIG 1c 3g 20gb"
  },
  {
    "id": 25919,
    "content": "(ID 0) Successfully created compute instance on GPU 0 GPU instance ID 1 using profile MIG 1c 3g 20gb (ID 0) Using nvidia-smi, the following CIs are now created on GI 1 The following example shows how the CIs and GIs created in the previous examples can be destroyed Note: If the intention is to destroy all the CIs and GIs, then this can be accomplished with the following commands: $ sudo nvidia-smi"
  },
  {
    "id": 25920,
    "content": "mig -dci && sudo nvidia-smi mig -dgi Successfully destroyed compute instance ID 0 from GPU 0 GPU instance ID 1 Successfully destroyed compute instance ID 1 from GPU 0 GPU instance ID 1 Successfully destroyed compute instance ID 2 from GPU 0 GPU instance ID 1 Successfully destroyed GPU instance ID 1 from GPU 0 Successfully destroyed GPU instance ID 2 from GPU 0 In this example, we delete the"
  },
  {
    "id": 25921,
    "content": "specific CIs created under GI 1 Note: On Ampere GPUs (A100 or A30), NVML (and nvidia-smi ) does not support attribution of utilization metrics to MIG devices From the previous example, the utilization is displayed as N/A when running CUDA programs: $ nvidia-smi +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | |"
  },
  {
    "id": 25922,
    "content": "ECC| | |=+=+=+=| | 0 1 0 0 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ | 0 2 0 1 | 268MiB / 20096MiB | 42 0 | 3 0 2 0 0 | | | 4MiB / 32767MiB | | | +-+-+-+-+ +-+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=| | 0 1 0 6217 C"
  },
  {
    "id": 25923,
    "content": "inux/release/BlackScholes 253MiB | | 0 2 0 6223 C inux/release/BlackScholes 253MiB | +-+ MIG with CUDA MPS As described in the section on CUDA concurrency mechanisms, CUDA Multi-Process Service (MPS) enables co-operative multi-process CUDA applications to be processed concurrently on the GPU"
  },
  {
    "id": 25924,
    "content": "MPS and MIG can work together, potentially achieving even higher levels of utilization for certain workloads Refer to the MPS documentation to understand the architecture and provisioning sequence for MPS Workflow In summary, the workflow for running with MPS is as follows: Configure the desired MIG geometry on the GPU Setup the CUDA_MPS_PIPE_DIRECTORY variable to point to unique directories so"
  },
  {
    "id": 25925,
    "content": "that the multiple MPS servers and clients can communicate with each other using named pipes and Unix domain sockets Note: The MPS documentation recommends setting up EXCLUSIVE_PROCESS mode to ensure that a single MPS server is using the GPU However, this mode is not supported when the GPU is in MIG mode as we use multiple MPS servers (one per MIG GPU instance) Configure GPU Instances Follow the"
  },
  {
    "id": 25926,
    "content": "steps outlined in the previous sections to configure the desired MIG geometry on the GPU For this example, we configure the GPU into a 3g 20gb , 3g"
  },
  {
    "id": 25935,
    "content": "docker com | sh \\ && sudo systemctl start docker \\ && sudo systemctl enable docker Install NVIDIA Container Toolkit Now install the NVIDIA Container Toolkit (previously known as nvidia-docker2 )"
  },
  {
    "id": 25945,
    "content": "io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring gpg \\ && curl -s -L https: nvidia"
  },
  {
    "id": 25951,
    "content": "d/nvidia-container-toolkit list Install the NVIDIA Container Toolkit packages (and their dependencies): $ sudo apt-get install -y nvidia-docker2 \\ && sudo systemctl restart docker Running Containers To run containers on specific MIG devices - whether these are GIs or specific underlying CIs, then the NVIDIA_VISIBLE_DEVICES variable (or the --gpus option with Docker 19"
  },
  {
    "id": 25953,
    "content": "NVIDIA_VISIBLE_DEVICES supports the following formats to specify MIG devices: MIG-  when using R450 and R460 drivers or MIG- starting with R470 drivers"
  },
  {
    "id": 25955,
    "content": "03, the --gpus option can be used to specify MIG devices by using the following format: ‘“device=MIG-device”’ , where MIG-device can follow either of the format specified above for NVIDIA_VISIBLE_DEVICES"
  },
  {
    "id": 25956,
    "content": "The following example shows running nvidia-smi from within a CUDA container using both formats As can be seen in the example, only one MIG device as chosen is visible to the container when using either format"
  },
  {
    "id": 25957,
    "content": "$ sudo docker run --runtime=nvidia \\ -e NVIDIA_VISIBLE_DEVICES=MIG-c7384736-a75d-5afc-978f-d2f1294409fd \\ nvidia/cuda nvidia-smi +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |=+=+=+=| | 0 1 0 0 | 11MiB / 20224MiB | 42 0 | 3 0 2 0 0 | +-+-+-+-+ +-+ | Processes: | | GPU GI CI PID Type Process name GPU Memory |"
  },
  {
    "id": 25958,
    "content": "| ID ID Usage | |=| | No running processes found | +-+ # For Docker versions = 19 03 $ sudo docker run --gpus '\"device=0:0\"' \\ nvidia/cuda nvidia-smi -L GPU 0: A100-SXM4-40GB (UUID: GPU-e86cb44c-6756-fd30-cd4a-1e6da3caf9b0) MIG 3g 20gb Device 0: (UUID: MIG-c7384736-a75d-5afc-978f-d2f1294409fd) A more complex example is to run a TensorFlow container to do a training run using GPUs on the MNIST"
  },
  {
    "id": 25960,
    "content": "io/nvidia/pytorch:20 11-py3 \\ /bin/bash -c 'cd /opt/pytorch/examples/upstream/mnist && python main py' = == PyTorch == = NVIDIA Release 20 11 (build 17345815) PyTorch Version 1"
  },
  {
    "id": 25963,
    "content": "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU (Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright (c) 2006 Idiap Research Institute"
  },
  {
    "id": 25964,
    "content": "(Samy Bengio) Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz) Copyright (c) 2015 Google Inc Copyright (c) 2015 Yangqing Jia Copyright (c) 2013-2016 The Caffe contributors All rights reserved"
  },
  {
    "id": 25966,
    "content": "9920512it [00:01, 7880654 53it/s] 32768it [00:00, 129950 31it/s] 1654784it [00:00, 2353765 88it/s] 8192it [00:00, 41020"
  },
  {
    "id": 25969,
    "content": "py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor You may want to copy the array to protect its data or make it writeable before converting it to a tensor"
  },
  {
    "id": 25975,
    "content": "MIG with Slurm Slurm is a workload manager that is widely used at high performance computing centers such as government labs, universities"
  },
  {
    "id": 25976,
    "content": "Device Nodes and Capabilities Currently, the NVIDIA kernel driver exposes its interfaces through a few system-wide device nodes"
  },
  {
    "id": 25977,
    "content": "/dev ├── nvidiactl ├── nvidia-modeset ├── nvidia-uvm ├── nvidia-uvm-tools ├── nvidia-nvswitchctl ├── nvidia0 └── nvidia1 Starting with CUDA 11/R450, a new abstraction known as nvidia-capabilities has been introduced"
  },
  {
    "id": 25978,
    "content": "The idea being that access to a specific capability is required to perform certain actions through the driver"
  },
  {
    "id": 25980,
    "content": "For example, the mig-config capability allows one to create and destroy MIG instances on any MIG-capable GPU (e"
  },
  {
    "id": 25982,
    "content": "Likewise, the fabric-mgmt capability allows one to run the Fabric Manager as a non-root but privileged daemon Without this capability, all attempts to launch the Fabric Manager as a non-root user will fail"
  },
  {
    "id": 25983,
    "content": "The following sections walk through the system level interface for managing these new nvidia-capabilities , including the steps necessary to grant and revoke access to them System Level Interface There are two different system-level interfaces available to work with nvidia-capabilities"
  },
  {
    "id": 25984,
    "content": "The /proc based interface relies on user-permissions and mount namespaces to limit access to a particular capability, while the /dev based interface relies on cgroups Technically, the /dev based interface also relies on user-permissions as a second-level access control mechanism (on the actual device node files themselves), but the primary access control mechanism is cgroups"
  },
  {
    "id": 25987,
    "content": "06) supports both mechanisms, but going forward the /dev based interface is the preferred method and the /proc based interface is deprecated For now, users can choose the desired interface by using the nv_cap_enable_devfs parameter on the nvidia ko kernel module: When nv_cap_enable_devfs=0 the /proc based interface is enabled A setting of nv_cap_enable_devfs=0 is the default for the R450 driver"
  },
  {
    "id": 25991,
    "content": "An example of loading the nvidia kernel module with this parameter set can be seen below: $ modprobe nvidia nv_cap_enable_devfs=1 /dev based nvidia-capabilities The system level interface for interacting with /dev based capabilities is actually through a combination of /proc and /dev First, a new major device is now associated with nvidia-caps and can be read from the standard /proc/devices file"
  },
  {
    "id": 25992,
    "content": "$ cat /proc/devices | grep nvidia-caps 508 nvidia-caps Second, the exact same set of files exist under /proc/driver/nvidia/capabilities These files no longer control access to the capability directly and instead, the contents of these files point at a device node under /dev , through which cgroups can be used to control access to the capability This can be seen in the example below: $ cat"
  },
  {
    "id": 25993,
    "content": "/proc/driver/nvidia/capabilities/mig/config DeviceFileMinor: 1 DeviceFileMode: 256 DeviceFileModify: 1 The combination of the device major for nvidia-caps and the value of DeviceFileMinor in this file indicate that the mig-config capability (which allows a user to create and destroy MIG devices) is controlled by the device node with a major:minor of 238:1 As such, one will need to use cgroups to"
  },
  {
    "id": 25994,
    "content": "grant a process read access to this device in order to configure MIG devices The purpose of the DeviceFileMode and DeviceFileModify fields in this file are explained later on in this section The standard location for these device nodes is under /dev/nvidia-caps as seen in the example below: $ ls -l /dev/nvidia-caps total 0 cr- 1 root root 508, 1 Nov 21 17:16 nvidia-cap1 cr--r--r-- 1 root root 508,"
  },
  {
    "id": 25995,
    "content": "2 Nov 21 17:16 nvidia-cap2 Unfortunately, these device nodes cannot be automatically created/deleted by the NVIDIA driver at the same time it creates/deletes files underneath /proc/driver/nvidia/capabilities (due to GPL compliance issues) Instead, a user-level program called nvidia-modprobe is provided, that can be invoked from user-space in order to do this For example: $ nvidia-modprobe \\ -f"
  },
  {
    "id": 25996,
    "content": "/proc/driver/nvidia/capabilities/mig/config \\ -f /proc/driver/nvidia/capabilities/mig/monitor $ ls -l /dev/nvidia-caps total 0 cr- 1 root root 508, 1 Nov 21 17:16 nvidia-cap1 cr--r--r-- 1 root root 508, 2 Nov 21 17:16 nvidia-cap2 nvidia-modprobe looks at the DeviceFileMode in each capability file and creates the device node with the permissions indicated (e"
  },
  {
    "id": 25998,
    "content": "Programs such as nvidia-smi will automatically invoke nvidia-modprobe (when available) to create these device nodes on your behalf In other scenarios it is not necessarily required to use nvidia-modprobe to create these device nodes, but it does make the process simpler If you actually want to prevent nvidia-modprobe from ever creating a particular device node on your behalf, you can do the"
  },
  {
    "id": 25999,
    "content": "following: # Give a user write permissions to the capability file under /proc $ chmod +uw /proc/driver/nvidia/capabilities/mig/config # Update the file with a “DeviceFileModify” setting of 0 $ echo \"DeviceFileModify: 0\" > /proc/driver/nvidia/capabilities/mig/config You will then be responsible for managing creation of the device node referenced by /proc/driver/nvidia/capabilities/mig/config going"
  },
  {
    "id": 26000,
    "content": "forward If you want to change that in the future, simply reset it to a value of \"DeviceFileModify: 1\" with the same command sequence This is important in the context of containers because we may want to give a container access to a certain capability even if it doesn't exist in the /proc hierarchy yet For example, granting a container the mig-config capability implies that we should also grant it"
  },
  {
    "id": 26001,
    "content": "capabilities to access all possible gis and cis that could be created for any GPU on the system Otherwise the container will have no way of working with those gis and cis once they have actually been created One final thing to note about /dev based capabilities is that the minor numbers for all possible capabilities are predetermined and can be queried under various files of the form:"
  },
  {
    "id": 26002,
    "content": "/proc/driver/nvidia-caps/*-minors For example, all capabilities related to MIG can be looked up as: $ cat /proc/driver/nvidia-caps/mig-minors config 1 monitor 2 gpu0/gi0/access 3 gpu0/gi0/ci0/access 4 gpu0/gi0/ci1/access 5 gpu0/gi0/ci2/access 6 gpu31/gi14/ci6/access 4321 gpu31/gi14/ci7/access 4322 The format of the content follows: GPU/gi/ci Note that the GPU device minor number can be obtained by"
  },
  {
    "id": 26003,
    "content": "using either of these mechanisms: The NVML API nvmlDeviceGetMinorNumber() so it returns the device minor number Or use the PCI BDF available under /proc/driver/nvidia/gpus/domain:bus:device:function/information"
  },
  {
    "id": 26004,
    "content": "For example, if the MIG geometry was created as below: +-+ | MIG devices: | +-+-+-+-+ | GPU GI CI MIG | Memory-Usage | Vol| Shared | | ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG| | | | ECC| | |=+=+=+=| | 0 1 0 0 | 19MiB / 40192MiB | 14 0 | 3 0 3 0 3 | | | 0MiB / 65535MiB | | | +-+ +-+-+ | 0 1 1 1 | | 14 0 | 3 0 3 0 3 | | | | | | +-+ +-+-+ | 0 1 2 2 | | 14 0 | 3 0 3 0 3 | | | | | |"
  },
  {
    "id": 26005,
    "content": "+-+-+-+-+ Then the corresponding device nodes: /dev/nvidia-cap12 , /dev/nvidia-cap13 and /dev/nvidia-cap14 and /dev/nvidia-cap15 would be created"
  },
  {
    "id": 26006,
    "content": "/proc based nvidia-capabilities ( **Deprecated** ) The system level interface for interacting with /proc based nvidia-capabilities is rooted at /proc/driver/nvidia/capabilities"
  },
  {
    "id": 26007,
    "content": "Files underneath this hierarchy are used to represent each capability, with read access to these files controlling whether a user has a given capability or not"
  },
  {
    "id": 26008,
    "content": "Thus a MIG device is identified by the following format: MIG- As an example, having read access to the following paths would allow one to run workloads on the MIG device represented by : /proc/driver/nvidia/capabilities/gpu0/mig/gi0/access /proc/driver/nvidia/capabilities/gpu0/mig/gi0/ci0/access Note, that there is no access file representing a capability to run workloads on gpu0 (only on gi0 and"
  },
  {
    "id": 26010,
    "content": "This is because the traditional mechanism of using cgroups to control access to top level GPU devices (and any required meta devices) is still required"
  },
  {
    "id": 26011,
    "content": "As shown earlier in the document, the cgroups mechanism applies to: /dev/nvidia0 /dev/nvidiactl /dev/nvidiactl-uvm"
  },
  {
    "id": 26012,
    "content": "In the context of containers, a new mount namespace should be overlaid on top of the path for /proc/driver/nvidia/capabilities , and only those capabilities a user wishes to grant to a container should be bind-mounted in Since the host’s user/group information is retained across the bind-mount, it must be ensured that the correct user permissions are set for these capabilities on the host before"
  },
  {
    "id": 26014,
    "content": "Changelog 11/17/2022 (author: PR): Includes the following changes: Updates for Hopper, CUDA 12 0/R525 Reorginzation of several chapters Added more information on /dev based capabilities 7/19/2022 (author: PR): Includes the following changes: Added a chapter on virtualization 8/26/2021 (author: PR): Includes the following changes: Improve explanation of GPU Partitioning 6/30/2021 (author: PR):"
  },
  {
    "id": 26015,
    "content": "Includes the following changes: Add info on unique UUIDs for MIG devices 4/22/2021 (author: PR): Includes the following changes: Added information for Slurm and CUDA MPS 4/14/2021 (author: PR): Includes the following changes: Add additional supported products 2/17/2021 (author: PR): Includes the following changes: Add note about persistence of MIG devices 8/7/2020 (author: PR): Added information"
  },
  {
    "id": 26018,
    "content": "Notices Notice This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors"
  },
  {
    "id": 26019,
    "content": "contained herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements,"
  },
  {
    "id": 26021,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 26022,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 26023,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 26024,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 26025,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 26026,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 26027,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 26028,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 26029,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 26030,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 26031,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product Trademarks NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the Unites States and other countries Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 26032,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact var switchTo5x=true; stLight"
  },
  {
    "id": 26033,
    "content": "options({publisher: \"998dc202-a267-4d8e-bce9-14debadb8d92\", doNotHash: false, doNotCopy: false, hashAddressBar: false}); if (typeof _satellite"
  },
  {
    "id": 26036,
    "content": "EFLOW User’s Guide v12 5 | PDF | Archive EFLOW User’s Guide Describes how CUDA and NVIDIA GPU accelerated cloud native applications can be deployed on EFLOW enabled Windows devices Introduction  Azure IoT Edge For Linux on Windows, otherwise referred to as EFLOW, is a Microsoft Technology for the deployment of Linux AI containers on Windows Edge devices This document details how NVIDIA® CUDA®"
  },
  {
    "id": 26037,
    "content": "and NVIDIA GPU accelerated cloud native applications can be deployed on such EFLOW-enabled Windows devices EFLOW has the following components: The Windows host OS with virtualization enabled A Linux virtual machine IoT Edge Runtime IoT Edge Modules, or otherwise any docker-compatible containerized application (runs on moby/containerd) GPU-accelerated IoT Edge Modules support for GeForce RTX GPUs"
  },
  {
    "id": 26038,
    "content": "is based on the GPU Paravirtualization that was foundational to CUDA on Windows Subsystem on Linux CUDA on WSL 2 boosted the productivity of CUDA developers by enabling them to build, develop, and containerize GPU accelerated NVIDIA AI/ML Linux applications on Windows desktop computers before deployment on Linux instances on the cloud A containerized NVIDIA GPU accelerated Linux application that"
  },
  {
    "id": 26039,
    "content": "is either hosted on Azure IoT Hub or NGC registry can be seamlessly deployed at the edge such as a retail service center or hospitals These edge deployments are typically IT managed devices entrenched with Windows devices for manageability but the advent of AI/ML use cases in this space seek the convergence for Linux and Windows applications not only to coexist but also seamlessly communicate on"
  },
  {
    "id": 26040,
    "content": "the same device Because CUDA support on EFLOW is predominantly based on WSL 2, refer to the Software Support, Limitations and Known Issues sections in the CUDA on WSL 2 document to stay abreast of the scope of NVIDIA software support available on EFLOW as well The following sections details installation of EFLOW, prerequisites for out-of-the-box CUDA support, followed by sample instructions for"
  },
  {
    "id": 26044,
    "content": "Setup and Installation  Follow the Microsoft EFLOW documentation page for various installation options suiting your needs: For up-to-date installation instructions, visit http: aka"
  },
  {
    "id": 26046,
    "content": "For quick setup, we have included the steps for installation through Powershell in the following sections"
  },
  {
    "id": 26050,
    "content": "Driver Installation  On the target Windows device, first install an NVIDIA GeForce or NVIDIA RTX GPU Windows driver that is compatible with the NVIDIA GPU on your device"
  },
  {
    "id": 26051,
    "content": "EFLOW VM supports deploying containerized CUDA applications and hence only the driver must be installed on the host system"
  },
  {
    "id": 26053,
    "content": "Because EFLOW is based on WSL, the restrictions of the software stack for a hybrid Linux on Windows environment apply, and not all of the NVIDIA software stack is supported"
  },
  {
    "id": 26059,
    "content": "Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All Path : Online : True RestartNeeded : False Set execution policy and verify"
  },
  {
    "id": 26060,
    "content": "Set-ExecutionPolicy -ExecutionPolicy AllSigned -Force Get-ExecutionPolicy AllSigned Download and install EFLOW"
  },
  {
    "id": 26067,
    "content": ">Get-EflowHostConfiguration | format-list FreePhysicalMemoryInMB : 35502 NumberOfLogicalProcessors : {64, 64} DiskInfo : @{Drive=C:; FreeSizeInGB=798} GpuInfo : @{Count=1; SupportedPassthroughTypes=System"
  },
  {
    "id": 26069,
    "content": "By default, EFLOW only reserves 1024MB of system memory for use for the workloads and that is insufficient to support GPU accelerated configurations For GPU acceleration, you will have to reserve system memory explicitly at EFLOW deployment; otherwise there will not be sufficient system memory for your containerized applications to run In order to prevent out of memory errors, reserve memory"
  },
  {
    "id": 26070,
    "content": "explicitly as required; see example below (Refer to command line argument options available for deploying EFLOW in the official documentation for more details)"
  },
  {
    "id": 26075,
    "content": "Windows 10/11 (Pro, Enterprise, IoT Enterprise) - Windows 10 users must use the November 2021 update build 19044"
  },
  {
    "id": 26077,
    "content": "Deploy-Eflow only allocates 1024 MB memory by default, set it to a larger value to prevent OOM issue, check MS documents for more details at https: learn"
  },
  {
    "id": 26087,
    "content": "Connecting to the EFLOW VM  Get-EflowVmAddr [10/13/2022 11:41:16] Querying IP and MAC addresses from virtual machine (IPP1-1490-EFLOW) - Virtual machine MAC: 00:15:5d:b2:40:c7 - Virtual machine IP : 172"
  },
  {
    "id": 26095,
    "content": "Running GPU-accelerated Containers  Let us run an N-body simulation containerized CUDA sample from NGC, but this time inside EFLOW"
  },
  {
    "id": 26097,
    "content": "io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark Unable to find image 'nvcr io/nvidia/k8s/cuda-sample:nbody' locally nbody: Pulling from nvidia/k8s/cuda-sample 22c5ef60a68e: Pull complete 1939e4248814: Pull complete 548afb82c856: Pull complete a424d45fd86f: Pull complete 207b64ab7ce6: Pull complete f65423f1b49b: Pull complete 2b60900a3ea5: Pull complete e9bff09d04df: Pull complete"
  },
  {
    "id": 26098,
    "content": "edc14edf1b04: Pull complete 1f37f461c076: Pull complete 9026fb14bf88: Pull complete Digest: sha256:59261e419d6d48a772aad5bb213f9f1588fcdb042b115ceb7166c89a51f03363 Status: Downloaded newer image for nvcr io/nvidia/k8s/cuda-sample:nbody Run \"nbody -benchmark [-numbodies=]\" to measure performance -fullscreen (run n-body simulation in fullscreen mode) -fp64 (use double precision floating point values"
  },
  {
    "id": 26099,
    "content": "for simulation) -hostmem (stores simulation data in host memory) -benchmark (run benchmark to measure performance) -numbodies= (number of bodies (>= 1) to run in simulation) -device= (where d=0,1,2 for the CUDA device to use) -numdevices= (where i=(number of CUDA devices > 0) to use for simulation) -compare (compares simulation results running once on the default GPU and once on the CPU) -cpu (run"
  },
  {
    "id": 26100,
    "content": "n-body simulation on the CPU) -tipsy= (load a tipsy model file for simulation) NOTE: The CUDA Samples are not meant for performance measurements > Windowed mode > Simulation data stored in video memory > Single precision floating point simulation > 1 Devices used for simulation GPU Device 0: \"Ampere\" with compute capability 8"
  },
  {
    "id": 26108,
    "content": "7”, need add “–env NVIDIA_DISABLE_REQUIRE=1” The CUDA version cannot be determined correctly from the driver on the host when launching the container"
  },
  {
    "id": 26116,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 26117,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 26119,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 26120,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 26121,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 26122,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 26123,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 26124,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 26125,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 26126,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 26127,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 26128,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 26129,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 26136,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 26138,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 26139,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 26144,
    "content": "NVIDIA GPU Accelerated Computing on WSL 2 v12 5 | PDF | Archive CUDA on WSL User Guide The guide for using NVIDIA CUDA on Windows Subsystem for Linux NVIDIA GPU Accelerated Computing on WSL 2  WSL or Windows Subsystem for Linux is a Windows feature that enables users to run native Linux applications, containers and command-line tools directly on Windows 11 and later OS builds CUDA support in"
  },
  {
    "id": 26145,
    "content": "this user guide is specifically for WSL 2, which is the second generation of WSL that offers the following benefits Linux applications can run as is in WSL 2 WSL 2 is characteristically a VM with a Linux WSL Kernel in it that provides full compatibility with mainstream Linux kernel allowing support for native Linux applications including popular Linux distros WSL 2 is tightly integrated with the"
  },
  {
    "id": 26146,
    "content": "Microsoft Windows operating system, which allows it to run Linux applications alongside and even interop with other Windows desktop and modern store apps Typically, developers working across both Linux and Windows environments have a very disruptive workflow install Linux and Windows in separate partitions on the same or different hard disks on the system and boot to the OS of choice"
  },
  {
    "id": 26147,
    "content": "Also this has historically restricted the development of seamless, well integrated tools and software systems across two dominant ecosystems WSL enables users to have a seamless transition across the two environments without the need for a resource intensive traditional virtual machine and to improve productivity and develop using tools and integrate their workflow"
  },
  {
    "id": 26148,
    "content": "More importantly WSL 2 enables applications that were hitherto only available on Linux to be available on Windows WSL 2 support for GPU allows for these applications to benefit from GPU accelerated computing and expands the domain of applications that can be developed on WSL 2 With NVIDIA CUDA support for WSL 2, developers can leverage NVIDIA GPU accelerated computing technology for data science,"
  },
  {
    "id": 26149,
    "content": "machine learning and inference on Windows through WSL GPU acceleration also serves to bring down the performance overhead of running an application inside a WSL like environment close to near-native by being able to pipeline more parallel work on the GPU with less CPU intervention NVIDIA driver support for WSL 2 includes not only CUDA but also DirectX and Direct ML support"
  },
  {
    "id": 26152,
    "content": "com/en-us/windows/win32/direct3d12/gpu-tensorflow-wsl WSL 2 is a key enabler in making GPU acceleration to be seamlessly shared between Windows and Linux applications on the same system a reality"
  },
  {
    "id": 26153,
    "content": "This offers flexibility and versatility while also serving to open up GPU accelerated computing by making it more accessible"
  },
  {
    "id": 26154,
    "content": "Illustration of the possibilities with NVIDIA CUDA software stack on WSL 2  This document describes a workflow for getting started with running CUDA applications or containers in a WSL 2 environment"
  },
  {
    "id": 26157,
    "content": "NVIDIA Compute Software Support on WSL 2  This table captures the readiness and suggested software versions for NVIDIA software stack for WSL 2 Package Suggested Versions Installation NVIDIA Windows Driver x86 Use the latest Windows x86 production driver Windows x86 drivers can be directly downloaded from https: www nvidia"
  },
  {
    "id": 26165,
    "content": "CUDA Toolkit and CUDA Developer Tools Preview Support Compute Sanitizer - Pascal and later Nsight Systems CLI, and CUPTI (Trace) - Volta and later Developer tools - Debuggers - Pascal and later (Using driver r535+) Developer tools - Profilers - Volta and later (Using Windows 10 OS build 19044+ with driver r545+ or using Windows 11 with driver r525+ ) Latest Linux CUDA toolkit package - WSL-Ubuntu"
  },
  {
    "id": 26176,
    "content": "Getting Started with CUDA on WSL 2  To get started with running CUDA on WSL, complete these steps in order: 2"
  },
  {
    "id": 26178,
    "content": "Step 1: Install NVIDIA Driver for GPU Support  Install NVIDIA GeForce Game Ready or NVIDIA RTX Quadro Windows 11 display driver on your system with a compatible GeForce or NVIDIA RTX/Quadro card from https: www nvidia"
  },
  {
    "id": 26185,
    "content": "Step 2: Install WSL 2  Launch your preferred Windows Terminal / Command Prompt / Powershell and install WSL: wsl exe --install Ensure you have the latest WSL kernel: wsl exe --update 2"
  },
  {
    "id": 26188,
    "content": "exe The default distro is Ubuntu To update the distro to your favorite distro from the command line and to review other WSL commands, refer to the following resources: https: docs"
  },
  {
    "id": 26192,
    "content": "com/en-us/windows/wsl/basic-commands From this point you should be able to run any existing Linux application which requires CUDA"
  },
  {
    "id": 26195,
    "content": "CUDA Support for WSL 2  The latest NVIDIA Windows GPU Driver will fully support WSL 2 With CUDA support in the driver, existing applications (compiled elsewhere on a Linux system for the same target GPU) can run unmodified within the WSL environment CUDA Toolkit support for WSL is still in preview stage as developer tools such as profilers are not available yet However, CUDA application"
  },
  {
    "id": 26196,
    "content": "development is fully supported in the WSL2 environment, as a result, users should be able to compile new CUDA Linux applications with the latest CUDA Toolkit for x86 Linux Once a Windows NVIDIA GPU driver is installed on the system, CUDA becomes available within WSL 2 The CUDA driver installed on Windows host will be stubbed inside the WSL 2 as libcuda so , therefore users must not install any"
  },
  {
    "id": 26197,
    "content": "NVIDIA GPU Linux driver within WSL 2 One has to be very careful here as the default CUDA Toolkit comes packaged with a driver, and it is easy to overwrite the WSL 2 NVIDIA driver with the default installation We recommend developers to use a separate CUDA Toolkit for WSL 2 (Ubuntu) available from the CUDA Toolkit Downloads page to avoid this overwriting This WSL-Ubuntu CUDA toolkit installer will"
  },
  {
    "id": 26198,
    "content": "not overwrite the NVIDIA driver that was already mapped into the WSL 2 environment First, remove the old GPG key: sudo apt-key del 7fa2af80 Option 1: Installation of Linux x86 CUDA Toolkit using WSL-Ubuntu Package - Recommended The CUDA WSL-Ubuntu local installer does not contain the NVIDIA Linux GPU driver, so by following the steps on the CUDA download page for WSL-Ubuntu , you will be able to"
  },
  {
    "id": 26199,
    "content": "get just the CUDA toolkit installed on WSL Option 2: Installation of Linux x86 CUDA Toolkit using Meta Package If you installed the toolkit using the WSL-Ubuntu package, please skip this section Meta packages do not contain the driver, so by following the steps on the download page for Ubuntu , you will be able to get just the CUDA toolkit installed on WSL The installation instructions for the"
  },
  {
    "id": 26200,
    "content": "CUDA Toolkit can be found in the CUDA Toolkit download page for each installer But DO NOT choose the “ cuda ”, “ cuda-12-x ”, or “ cuda-drivers ” meta-packages under WSL 2 as these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2 You can also install other components of the toolkit by choosing the right meta-package"
  },
  {
    "id": 26202,
    "content": "WSL 2 Support Constraints  WSL 2 GPU acceleration will be available on Pascal and later GPU architecture on both GeForce and Quadro product SKUs in WDDM mode"
  },
  {
    "id": 26206,
    "content": "Known Limitations for Linux CUDA Applications  The following table lists the known limitations on WSL 2 that may affect CUDA applications that use some of these features that are fully supported on Linux"
  },
  {
    "id": 26207,
    "content": "Unified Memory - Full Managed Memory Support is not available on Windows native and therefore WSL 2 will not support it for the foreseeable future UVM full features will not be available and therefore applications relying on UVM full features may not work If your application is using Managed Memory, your application could see reduced performance and high system memory usage"
  },
  {
    "id": 26209,
    "content": "Pinned system memory (example: System memory that an application makes resident for GPU accesses) availability for applications is limited"
  },
  {
    "id": 26210,
    "content": "For example, some deep learning training workloads, depending on the framework, model and dataset size used, can exceed this limit and may not work"
  },
  {
    "id": 26212,
    "content": "On multi-GPU systems it is not possible to filter for specific GPU devices by using specific index numbers to enumerate GPUs"
  },
  {
    "id": 26215,
    "content": "Features Not Yet Supported  The following table lists the set of features that are currently not supported"
  },
  {
    "id": 26216,
    "content": "Windows Insider Preview and Windows 10 Support  If you are on Windows 11 please skip this section Windows 11 is generally available to the public and therefore does not require special registration All the instructions at the beginning of this user guide were mainly focused toward Windows 11 users If you are looking to use WSL 2 on Windows 10 or to be on the bleeding edge of WSL 2 development,"
  },
  {
    "id": 26217,
    "content": "you may want to register for the Windows Insider Program and choose the appropriate flighting channel (previously fast rings) and get the latest build for your needs You can check your build version number by running winver via the Run command"
  },
  {
    "id": 26223,
    "content": "Container Runtime Initialization Errors  In some cases, when running a Docker container, you may encounter nvidia-container-cli : initialization error : $ sudo docker run --gpus all nvcr"
  },
  {
    "id": 26224,
    "content": "io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark docker: Error response from daemon: OCI runtime create failed: container_linux"
  },
  {
    "id": 26225,
    "content": "go:349: starting container process caused \"process_linux go:449: container init caused \\\"process_linux go:432: running prestart hook 0 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\\\ \\\\\\\"\\\"\": unknown"
  },
  {
    "id": 26226,
    "content": "ERRO[0000] error waiting for container: context canceled This usually indicates that the right Windows OS build or Microsoft Windows Insider Preview Builds (Windows 10 only), WSL 2, NVIDIA drivers and NVIDIA Container Toolkit may not be installed correctly Review the known issues and changelog sections to ensure the right versions of the driver and container toolkit are installed"
  },
  {
    "id": 26227,
    "content": "Ensure you have followed through the steps listed under Setup under Running CUDA containers; especially ensure that the docker daemon is still running $ sudo service docker stop $ sudo service docker start Or start the daemon directly and see if that resolves the issue: $ sudo dockerd If you are still running into this issue, use the dxdiag tools from the Run dialog and provide the diagnostic"
  },
  {
    "id": 26228,
    "content": "logs to NVIDIA by posting in the Developer Forums or by filing a report You can also use the CUDA on WSL 2 Developer Forums to get in touch with NVIDIA product and engineering teams for help"
  },
  {
    "id": 26232,
    "content": "Checking WSL Kernel Version  Ensure you have the latest kernel by running the following command in PowerShell: $ wsl cat /proc/version Linux version 5"
  },
  {
    "id": 26240,
    "content": "20200220) #1 SMP Fri Apr 2 22:23:49 UTC 2021 If you don’t have the latest WSL kernel, you will see the following blocking warning upon trying to launch a Linux distribution within the WSL 2 container: 5"
  },
  {
    "id": 26242,
    "content": "Traditional Virtual Machines vs WSL 2  Whether to efficiently use hardware resources or to improve productivity, virtualization is a more widely used solution in both consumer and enterprise space"
  },
  {
    "id": 26243,
    "content": "There are different types of virtualizations, and it is beyond the scope of this document to delve into the specifics"
  },
  {
    "id": 26244,
    "content": "But traditional virtualization solutions require installation and setup of a virtualization management software to manage the guest virtual machines Although WSL 2 is itself a Virtual Machine, unlike traditional VMs it is easy to setup as it is provided by the host operating system provider and is quite lightweight Applications running within WSL see less overhead compared to traditional VMs"
  },
  {
    "id": 26245,
    "content": "especially if they require access to the hardware or perform privileged operations compared to when run directly on the system While VMs allow applications to be run unmodified, due to constraints from setup and performance overhead, they are not the best option in many situations"
  },
  {
    "id": 26248,
    "content": "Containers vs WSL 2  While a VM provides a secure self-contained, execution environment with a complete user space for the application, containers enable application composability without the overhead of VMs Containers compose all the dependencies of the applications such as libraries, files etc"
  },
  {
    "id": 26250,
    "content": "Containers run on the operating system that is installed on the system directly and therefore do not provide full isolation from other containers like a VM does, but keeps overhead negligible as a result To learn more about differences between VMs and containers, refer to https: docs"
  },
  {
    "id": 26256,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 26257,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 26259,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 26260,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 26261,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 26262,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 26263,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 26264,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 26265,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 26266,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 26267,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 26268,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 26269,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 26276,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 26278,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 26279,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 26291,
    "content": "deviceQuery  This application enumerates the properties of the CUDA devices present in the system and displays them in a human readable format"
  },
  {
    "id": 26294,
    "content": "vectorAdd  This application is a very basic demo that implements element by element vector addition"
  },
  {
    "id": 26297,
    "content": "bandwidthTest  This application provides the memcopy bandwidth of the GPU and memcpy bandwidth across PCI‑e This application is capable of measuring device to device copy bandwidth, host to device copy bandwidth for pageable and page-locked memory, and device to host copy bandwidth for pageable and page-locked memory busGrind  Provides detailed statistics about peer-to-peer memory bandwidth"
  },
  {
    "id": 26299,
    "content": "Arguments: Options Explanation -h print usage -p [0,1] enable or disable pinned memory tests (default on) -u [0,1] enable or disable unpinned memory tests (default off) -e [0,1] enable or disable p2p enabled memory tests (default off) -d [0,1] enable or disable p2p disabled memory tests (default off) -a enable all tests -n disable all tests Order of parameters matters"
  },
  {
    "id": 26301,
    "content": "/BusGrind -n -p 1 -e 1 Run all pinned and P2P tests /BusGrind -n -u 1 Runs only unpinned tests /BusGrind -a Runs all tests (pinned, unpinned, p2p enabled, p2p disabled) 2"
  },
  {
    "id": 26303,
    "content": "nbody  This demo does an efficient all-pairs simulation of a gravitational n-body simulation in CUDA Adding “-numbodies=num_of_bodies” to the command line will allow users to set # of bodies for simulation Adding “-numdevices=N” to the command line option will cause the sample to use N devices (if available) for simulation"
  },
  {
    "id": 26304,
    "content": "In this mode, the position and velocity data for all bodies are read from system memory using “zero copy” rather than from device memory"
  },
  {
    "id": 26305,
    "content": "For a small number of devices (4 or fewer) and a large enough number of bodies, bandwidth is not a bottleneck so we can achieve strong scaling across these devices"
  },
  {
    "id": 26306,
    "content": "Arguments: Options Explanation -fullscreen run n-body simulation in fullscreen mode -fp64 use double precision floating point values for simulation -hostmem stores simulation data in host memory -benchmark run benchmark to measure performance -numbodies=N number of bodies (>= 1) to run in simulation -device=d where d=0,1,2… for the CUDA device to use -numdevices=i where i=(number of CUDA devices"
  },
  {
    "id": 26307,
    "content": "> 0) to use for simulation -compare compares simulation results running once on the default GPU and once on the CPU -cpu run n-body simulation on the CPU -tipsy=file bin load a tipsy model file for simulation 2"
  },
  {
    "id": 26309,
    "content": "oceanFFT  This is a graphical demo which simulates an ocean height field using the CUFFT library, and renders the result using OpenGL"
  },
  {
    "id": 26312,
    "content": "randomFog  This is a graphical demo which does pseudo- and quasi- random numbers visualization produced by CURAND On creation, randomFog generates 200,000 random coordinates in spherical coordinate space (radius, angle rho, angle theta) with curand’s XORWOW algorithm The following keys can be used to control the output: Keys Function s Generate new set of random nos and display as spherical"
  },
  {
    "id": 26313,
    "content": "coordinates (Sphere) e Generate new set of random nos and display on a spherical surface (shEll) b Generate new set of random nos and display as cartesian coordinates (cuBe/Box) p Generate new set of random nos and display on a cartesian plane (Plane) i, l, j Rotate the negative Z-axis up, right, down and left respectively a Toggle auto-rotation t Toggle 10x zoom z Toggle axes display x Select"
  },
  {
    "id": 26314,
    "content": "XORWOW generator (default) c Select Sobol’ generator v Select scrambled Sobol’ generator r Reset XORWOW (i"
  },
  {
    "id": 26316,
    "content": "reset to initial seed) and regenerate ] Increment the number of Sobol’ dimensions and regenerate [ Reset the number of Sobol’ dimensions to 1 and regenerate Increment the number of displayed points by 8,000 (max"
  },
  {
    "id": 26317,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 26318,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 26320,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 26321,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 26322,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 26323,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 26324,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 26325,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 26326,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 26327,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 26328,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 26329,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 26330,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 26337,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 26339,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 26340,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2016-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 26345,
    "content": "Introduction  The PTX Compiler APIs are a set of APIs which can be used to compile a PTX program into GPU assembly code The APIs accept PTX programs in character string form and create handles to the compiler that can be used to obtain the GPU assembly code The GPU assembly code string generated by the APIs can be loaded by cuModuleLoadData and cuModuleLoadDataEx , and linked with other modules"
  },
  {
    "id": 26346,
    "content": "by cuLinkAddData or nvJitLinkAddData API from nvjitlink of the CUDA Driver API The main use cases for these PTX Compiler APIs are: With CUDA driver APIs, compilation and loading are tied together This allows applications to perform early compilation and caching of the GPU assembly code PTX Compiler APIs allow users to use runtime compilation for the latest PTX version that is supported as part of"
  },
  {
    "id": 26347,
    "content": "CUDA Toolkit release This support may not be available in the PTX JIT compiler present in the CUDA Driver if the application is running with an older driver installed in the system With PTX Compiler APIs, clients can implement a custom caching mechanism with the compiled GPU assembly The clients get fine grain control and can specify the compiler options during compilation"
  },
  {
    "id": 26351,
    "content": "System Requirements  PTX Compiler library requires the following system configuration: POSIX threads support for non-Windows platform"
  },
  {
    "id": 26355,
    "content": "Installation  PTX Compiler library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vPTXCompiler"
  },
  {
    "id": 26356,
    "content": "h lib\\x64 vptxcompiler_static lib doc\\pdf\\PTX_Compiler_API_User_Guide pdf On Linux: include/nvPTXCompiler"
  },
  {
    "id": 26359,
    "content": "Thread Safety  All PTX Compiler API functions are thread safe and may be invoked by multiple threads concurrently"
  },
  {
    "id": 26361,
    "content": "User Interface  This chapter presents the PTX Compiler APIs PTX-Compiler Handle  Typedefs nvPTXCompilerHandle nvPTXCompilerHandle represents a handle to the PTX Compiler"
  },
  {
    "id": 26365,
    "content": "Typedefs  typedef struct nvPTXCompiler * nvPTXCompilerHandle  nvPTXCompilerHandle represents a handle to the PTX Compiler To compile a PTX program string, an instance of nvPTXCompiler must be created and the handle to it must be obtained using the API nvPTXCompilerCreate() Then the compilation can be done using the API nvPTXCompilerCompile()"
  },
  {
    "id": 26368,
    "content": "Error codes  Enumerations nvPTXCompileResult The nvPTXCompiler APIs return the nvPTXCompileResult codes to indicate the call result"
  },
  {
    "id": 26372,
    "content": "Enumerations  enum nvPTXCompileResult  The nvPTXCompiler APIs return the nvPTXCompileResult codes to indicate the call result"
  },
  {
    "id": 26373,
    "content": "Values: enumerator NVPTXCOMPILE_SUCCESS  enumerator NVPTXCOMPILE_ERROR_INVALID_COMPILER_HANDLE  enumerator NVPTXCOMPILE_ERROR_INVALID_INPUT  enumerator NVPTXCOMPILE_ERROR_COMPILATION_FAILURE  enumerator NVPTXCOMPILE_ERROR_INTERNAL  enumerator NVPTXCOMPILE_ERROR_OUT_OF_MEMORY  enumerator NVPTXCOMPILE_ERROR_COMPILER_INVOCATION_INCOMPLETE  enumerator NVPTXCOMPILE_ERROR_UNSUPPORTED_PTX_VERSION"
  },
  {
    "id": 26376,
    "content": "API Versioning  The PTX compiler APIs are versioned so that any new features or API changes can be done by bumping up the API version"
  },
  {
    "id": 26377,
    "content": "Functions nvPTXCompileResult nvPTXCompilerGetVersion (unsigned int *major, unsigned int *minor) Queries the current major and minor version of PTX Compiler APIs being used"
  },
  {
    "id": 26381,
    "content": "Functions  nvPTXCompileResult nvPTXCompilerGetVersion ( unsigned int * major , unsigned int * minor )  Queries the current major and minor version of PTX Compiler APIs being used Parameters major – [out] Major version of the PTX Compiler APIs minor – [out] Minor version of the PTX Compiler APIs Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL 4"
  },
  {
    "id": 26383,
    "content": "Compilation APIs  Functions nvPTXCompileResult nvPTXCompilerCompile (nvPTXCompilerHandle compiler, int numCompileOptions, const char *const *compileOptions) Compile a PTX program with the given compiler options nvPTXCompileResult nvPTXCompilerCreate (nvPTXCompilerHandle *compiler, size_t ptxCodeLen, const char *ptxCode) Obtains the handle to an instance of the PTX compiler initialized with the"
  },
  {
    "id": 26384,
    "content": "given PTX program ptxCode nvPTXCompileResult nvPTXCompilerDestroy (nvPTXCompilerHandle *compiler) Destroys and cleans the already created PTX compiler nvPTXCompileResult nvPTXCompilerGetCompiledProgram (nvPTXCompilerHandle compiler, void *binaryImage) Obtains the image of the compiled program nvPTXCompileResult nvPTXCompilerGetCompiledProgramSize (nvPTXCompilerHandle compiler, size_t"
  },
  {
    "id": 26385,
    "content": "*binaryImageSize) Obtains the size of the image of the compiled program nvPTXCompileResult nvPTXCompilerGetErrorLog (nvPTXCompilerHandle compiler, char *errorLog) Query the error message that was seen previously for the handle nvPTXCompileResult nvPTXCompilerGetErrorLogSize (nvPTXCompilerHandle compiler, size_t *errorLogSize) Query the size of the error message that was seen previously for the"
  },
  {
    "id": 26386,
    "content": "handle nvPTXCompileResult nvPTXCompilerGetInfoLog (nvPTXCompilerHandle compiler, char *infoLog) Query the information message that was seen previously for the handle nvPTXCompileResult nvPTXCompilerGetInfoLogSize (nvPTXCompilerHandle compiler, size_t *infoLogSize) Query the size of the information message that was seen previously for the handle"
  },
  {
    "id": 26390,
    "content": "Functions  nvPTXCompileResult nvPTXCompilerCompile ( nvPTXCompilerHandle compiler , int numCompileOptions , const char * const * compileOptions )  Compile a PTX program with the given compiler options Parameters compiler – [inout] A handle to PTX compiler initialized with the PTX program which is to be compiled The compiled program can be accessed using the handle numCompileOptions – [in]"
  },
  {
    "id": 26391,
    "content": "Length of the array compileOptions compileOptions – [in] Compiler options with which compilation should be done Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_OUT_OF_MEMORY NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE NVPTXCOMPILE_ERROR_COMPILATION_FAILURE NVPTXCOMPILE_ERROR_UNSUPPORTED_PTX_VERSION NVPTXCOMPILE_ERROR_UNSUPPORTED_DEVSIDE_SYNC nvPTXCompileResult"
  },
  {
    "id": 26392,
    "content": "nvPTXCompilerCreate ( nvPTXCompilerHandle * compiler , size_t ptxCodeLen , const char * ptxCode )  Obtains the handle to an instance of the PTX compiler initialized with the given PTX program ptxCode Parameters compiler – [out] Returns a handle to PTX compiler initialized with the PTX program ptxCode ptxCodeLen – [in] Size of the PTX program ptxCode passed as string ptxCode – [in] The PTX program"
  },
  {
    "id": 26393,
    "content": "which is to be compiled passed as string Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_OUT_OF_MEMORY NVPTXCOMPILE_ERROR_INTERNAL nvPTXCompileResult nvPTXCompilerDestroy ( nvPTXCompilerHandle * compiler )  Destroys and cleans the already created PTX compiler Parameters compiler – [in] A handle to the PTX compiler which is to be destroyed Returns NVPTXCOMPILE_SUCCESS"
  },
  {
    "id": 26394,
    "content": "NVPTXCOMPILE_ERROR_OUT_OF_MEMORY NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetCompiledProgram ( nvPTXCompilerHandle compiler , void * binaryImage )  Obtains the image of the compiled program Parameters compiler – [in] A handle to PTX compiler on which nvPTXCompilerCompile() has been performed Client should allocate memory for binaryImage"
  },
  {
    "id": 26395,
    "content": "Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE NVPTXCOMPILE_ERROR_COMPILER_INVOCATION_INCOMPLETE nvPTXCompileResult nvPTXCompilerGetCompiledProgramSize ( nvPTXCompilerHandle compiler , size_t * binaryImageSize )  Obtains the size of the image of the compiled program binaryImageSize – [out] The size of the image of the compiled program Returns"
  },
  {
    "id": 26396,
    "content": "NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE NVPTXCOMPILE_ERROR_COMPILER_INVOCATION_INCOMPLETE nvPTXCompileResult nvPTXCompilerGetErrorLog ( nvPTXCompilerHandle compiler , char * errorLog )  Query the error message that was seen previously for the handle errorLog – [out] The error log which was produced in previous call to nvPTXCompilerCompiler()"
  },
  {
    "id": 26397,
    "content": "Clients should allocate memory for errorLog Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetErrorLogSize ( nvPTXCompilerHandle compiler , size_t * errorLogSize )  Query the size of the error message that was seen previously for the handle errorLogSize – [out] The size of the error log in bytes which was produced"
  },
  {
    "id": 26398,
    "content": "in previous call to nvPTXCompilerCompiler() Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetInfoLog ( nvPTXCompilerHandle compiler , char * infoLog )  Query the information message that was seen previously for the handle infoLog – [out] The information log which was produced in previous call to"
  },
  {
    "id": 26399,
    "content": "nvPTXCompilerCompiler() Clients should allocate memory for infoLog Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE nvPTXCompileResult nvPTXCompilerGetInfoLogSize ( nvPTXCompilerHandle compiler , size_t * infoLogSize )  Query the size of the information message that was seen previously for the handle infoLogSize – [out] The size of the information"
  },
  {
    "id": 26400,
    "content": "log in bytes which was produced in previous call to nvPTXCompilerCompiler() Returns NVPTXCOMPILE_SUCCESS NVPTXCOMPILE_ERROR_INTERNAL NVPTXCOMPILE_ERROR_INVALID_PROGRAM_HANDLE 5 Compilation Options  This chapter describes options supported by nvPTXCompilerCompile() API"
  },
  {
    "id": 26401,
    "content": "Option names with two preceding dashes ( -- ) are long option names and option names with one preceding dash ( - ) are short option names When a compile option takes an argument, an assignment operator ( = ) is used to separate the compile option argument from the compile option name, e"
  },
  {
    "id": 26404,
    "content": "Alternatively, the compile option name and the argument can be specified in separate strings without an assignment operator,"
  },
  {
    "id": 26407,
    "content": "--allow-expensive-optimizations ( -allow-expensive-optimizations ) Enable (disable) to allow compiler to perform expensive optimizations using maximum available resources (memory and compile-time)"
  },
  {
    "id": 26408,
    "content": "--device-function-maxrregcount N ( -func-maxrregcount ) When compiling with -c option, specify the maximum number of registers that device functions can use This option is ignored for whole-program compilation and does not affect registers used by entry functions If neither --device-function-maxrregcount nor --maxrregcount is specified, then no maximum is assumed"
  },
  {
    "id": 26409,
    "content": "Note Under certain situations, static device functions can safely inherit a higher register count from the caller entry function"
  },
  {
    "id": 26410,
    "content": "Value less than the minimum registers required by ABI will be bumped up by the compiler to ABI minimum limit"
  },
  {
    "id": 26412,
    "content": "--dont-merge-basicblocks ( -no-bb-merge ) Prevents basic block merging, at a slight perfomance cost Normally ptx compiler attempts to merge consecutive basic blocks as part of its optimization process"
  },
  {
    "id": 26413,
    "content": "--extensible-whole-program ( -ewp ) Generate extensible whole program device code, which allows some calls to not be resolved until linking with libcudadevrt"
  },
  {
    "id": 26414,
    "content": "--fmad ( -fmad ) Enables (disables) the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA) Default value: true --force-load-cache ( -flcm ) Force specified cache modifier on global/generic load"
  },
  {
    "id": 26416,
    "content": "Allowed values for this option: compute_50 , compute_52 , compute_53 , compute_60 , compute_61 , compute_62 , compute_70 , compute_72 , compute_73 , compute_75 , compute_80 , compute_86 , compute_87 , compute_89 , compute_90 , compute_90a , sm_50 , sm_52 , sm_53 , sm_60 , sm_61 , sm_62 , sm_70 , sm_72 , sm_73 , sm_75 , sm_80 , sm_86 , sm_87 , sm_89 , sm_90 , sm_90a Default value: sm_52"
  },
  {
    "id": 26417,
    "content": "--maxrregcount N ( -maxrregcount ) Specify the maximum amount of registers that GPU functions can use"
  },
  {
    "id": 26418,
    "content": "Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism"
  },
  {
    "id": 26419,
    "content": "User program may not be able to make use of all registers as some registers are reserved by compiler"
  },
  {
    "id": 26420,
    "content": "--preserve-relocs ( -preserve-relocs ) This option will make ptx compiler to generate relocatable references for variables and preserve relocations generated for them in linked executable"
  },
  {
    "id": 26421,
    "content": "--return-at-end ( -ret-end ) Prevents optimizing return instruction at end of program Normally ptx compiler optimizes return at the end of program"
  },
  {
    "id": 26422,
    "content": "--suppress-async-bulk-multicast-advisory-warning ( -suppress-async-bulk-multicast-advisory-warning ) Suppress the warning on use of multicast::cluster modifier on cp async bulk{"
  },
  {
    "id": 26424,
    "content": "--suppress-stack-size-warning ( -suppress-stack-size-warning ) Suppress the warning that otherwise is printed when stack size cannot be determined"
  },
  {
    "id": 26425,
    "content": "--warn-on-double-precision-use ( -warn-double-usage ) Warning if double(s) are used in an instruction"
  },
  {
    "id": 26429,
    "content": "minnctapersm directive specified --override-directive-values ( -override-directive-values ) Override the PTX directives values by the corresponding option values"
  },
  {
    "id": 26430,
    "content": "--make-errors-visible-at-exit ( -make-errors-visible-at-exit ) Generate required instructions at exit point to make memory faults and errors visible at exit"
  },
  {
    "id": 26432,
    "content": "Basic Usage  This section of the document uses a simple example, Vector Addition , shown in Figure 1 to explain how to use PTX Compiler APIs to compile this PTX program"
  },
  {
    "id": 26433,
    "content": "Equivalent CUDA source for the simple vector addition extern \"C\" __global__ void simpleVectorAdd(float *x, float *y, float *out) { size_t tid = blockIdx"
  },
  {
    "id": 26436,
    "content": "x; out[tid] = x[tid] + y[tid]; } With this PTX program as a string, we can create the compiler and obtain a handle to it as shown in Figure 3 Compiler creation and initialization of a program nvPTXCompilerHandle compiler; nvPTXCompilerCreate(&compiler, (size_t)strlen(ptxCode), ptxCode); Compilation can now be done by specifying the compile options as shown in Figure 4 Compilation of the PTX"
  },
  {
    "id": 26437,
    "content": "program const char* compile_options[] = { \"--gpu-name=sm_70\", \"--verbose\" }; nvPTXCompilerCompile(compiler, 2, compile_options); The compiled GPU assembly code can now be obtained And to allocate memory, we need to query the size of the image of the compiled GPU assembly code which is done as shown in Figure 5 Query size of the compiled assembly image nvPTXCompilerGetCompiledProgramSize(compiler,"
  },
  {
    "id": 26438,
    "content": "&elfSize); The image of the compiled GPU assembly code can now be queried as shown in Figure 6 Query the compiled assembly image elf = (char*) malloc(elfSize); nvPTXCompilerGetCompiledProgram(compiler, (void*)elf); When the compiler is not needed anymore, it can be destroyed as shown in Figure 7"
  },
  {
    "id": 26440,
    "content": "lib Linux: gcc simpleVectorAddition c -o simpleVectorAddition \\ -I $CUDA_PATH/include \\ -L $CUDA_PATH/lib64 \\ libnvptxcompiler_static a -lcuda -lm -lpthread \\ -Wl,-rpath,$CUDA_PATH/lib64 7"
  },
  {
    "id": 26442,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 26443,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 26445,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 26446,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 26447,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 26448,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 26449,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 26450,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 26451,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 26452,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 26453,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 26454,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 26455,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 26464,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 26466,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 26467,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2020-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 26473,
    "content": "5 | PDF | Archive cuSOLVER API Reference The API reference guide for cuSOLVER, a GPU accelerated library for decompositions and linear system solutions for both dense and sparse matrices Introduction  The cuSolver library is a high-level package based on the cuBLAS and cuSPARSE libraries It consists of two modules corresponding to two sets of API: The cuSolver API on a single GPU The cuSolverMG"
  },
  {
    "id": 26474,
    "content": "API on a single node multiGPU Each of these can be used independently or in concert with other toolkit libraries To simplify the notation, cuSolver denotes single GPU API and cuSolverMg denotes multiGPU API The intent of cuSolver is to provide useful LAPACK-like features, such as common matrix factorization and triangular solve routines for dense matrices, a sparse least-squares solver and an"
  },
  {
    "id": 26475,
    "content": "eigenvalue solver In addition cuSolver provides a new refactorization library useful for solving sequences of matrices with a shared sparsity pattern The first part of cuSolver is called cuSolverDN, and deals with dense matrix factorization and solve routines such as LU, QR, SVD and LDLT, as well as useful utilities such as matrix and vector permutations Next, cuSolverSP provides a new set of"
  },
  {
    "id": 26476,
    "content": "sparse routines based on a sparse QR factorization Not all matrices have a good sparsity pattern for parallelism in factorization, so the cuSolverSP library also provides a CPU path to handle those sequential-like matrices For those matrices with abundant parallelism, the GPU path will deliver higher performance The final part is cuSolverRF, a sparse re-factorization package that can provide very"
  },
  {
    "id": 26477,
    "content": "good performance when solving a sequence of matrices where only the coefficients are changed but the sparsity pattern remains the same It is the responsibility of the developer to allocate memory and to copy data between GPU memory and CPU memory using standard CUDA runtime API routines, such as cudaMalloc() , cudaFree() , cudaMemcpy() , and cudaMemcpyAsync() By now, cuSolverMg supports 1-D column"
  },
  {
    "id": 26478,
    "content": "block cyclic layout and provides symmetric eigenvalue solver Note The cuSolver library requires hardware with a CUDA Compute Capability (CC) of 5 0 or higher"
  },
  {
    "id": 26479,
    "content": "Please see the CUDA C++ Programming Guide for a list of the Compute Capabilities corresponding to all NVIDIA GPUs"
  },
  {
    "id": 26482,
    "content": "cuSolverDN: Dense LAPACK  The cuSolverDN library was designed to solve dense linear systems of the form \\(Ax = b\\) where the coefficient matrix \\(A\\in R^{nxn}\\) , right-hand-side vector \\(b\\in R^{n}\\) and solution vector \\(x\\in R^{n}\\) The cuSolverDN library provides QR factorization and LU with partial pivoting to handle a general matrix A , which may be non-symmetric The cuSolverDN library"
  },
  {
    "id": 26483,
    "content": "also provides a helpful bidiagonalization routine and singular value decomposition (SVD) The cuSolverDN library targets computationally-intensive and popular routines in LAPACK, and provides an API compatible with LAPACK The user can accelerate these time-consuming routines with cuSolverDN and keep others in LAPACK without a major change to existing code"
  },
  {
    "id": 26486,
    "content": "cuSolverSP: Sparse LAPACK  The cuSolverSP library was mainly designed to a solve sparse linear system \\(Ax = b\\) and the least-squares problem \\(x = {argmin}{||}A*z - b{||}\\) where sparse matrix \\(A\\in R^{mxn}\\) , right-hand-side vector \\(b\\in R^{m}\\) and solution vector \\(x\\in R^{n}\\)"
  },
  {
    "id": 26487,
    "content": "If matrix A is symmetric/Hermitian, the user has to provide a full matrix, ie fill missing lower or upper part If matrix A is symmetric positive definite and the user only needs to solve \\(Ax = b\\) , Cholesky factorization can work and the user only needs to provide the lower triangular part of A"
  },
  {
    "id": 26488,
    "content": "On top of the linear and least-squares solvers, the cuSolverSP library provides a simple eigenvalue solver based on shift-inverse power method, and a function to count the number of eigenvalues contained in a box in the complex plane"
  },
  {
    "id": 26491,
    "content": "cuSolverRF: Refactorization  The cuSolverRF library was designed to accelerate solution of sets of linear systems by fast re-factorization when given new coefficients in the same sparsity pattern \\(A_{i}x_{i} = f_{i}\\) where a sequence of coefficient matrices \\(A_{i}\\in R^{nxn}\\) , right-hand-sides \\(f_{i}\\in R^{n}\\) and solutions \\(x_{i}\\in R^{n}\\) are given for i=1,"
  },
  {
    "id": 26493,
    "content": "The cuSolverRF library is applicable when the sparsity pattern of the coefficient matrices \\(A_{i}\\) as well as the reordering to minimize fill-in and the pivoting used during the LU factorization remain the same across these linear systems In that case, the first linear system ( i=1 ) requires a full LU factorization, while the subsequent linear systems ( i=2, ,k ) require only the LU"
  },
  {
    "id": 26494,
    "content": "re-factorization Notice that because the sparsity pattern of the coefficient matrices, the reordering and pivoting remain the same, the sparsity pattern of the resulting triangular factors \\(L_{i}\\) and \\(U_{i}\\) also remains the same Therefore, the real difference between the full LU factorization and LU re-factorization is that the required memory is known ahead of time"
  },
  {
    "id": 26497,
    "content": "Naming Conventions  The cuSolverDN library provides two different APIs; legacy and generic The functions in the legacy API are available for data types float , double , cuComplex , and cuDoubleComplex The naming convention for the legacy API is as follows: cusolverDn where can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic"
  },
  {
    "id": 26499,
    "content": "can be Cholesky factorization ( potrf ), LU with partial pivoting ( getrf ), QR factorization ( geqrf ) and Bunch-Kaufman factorization ( sytrf )"
  },
  {
    "id": 26500,
    "content": "The functions in the generic API provide a single entry point for each routine and support for 64-bit integers to define matrix and vector dimensions"
  },
  {
    "id": 26501,
    "content": "The naming convention for the generic API is data-agnostic and is as follows: cusolverDn where can be Cholesky factorization ( potrf ), LU with partial pivoting ( getrf ) and QR factorization ( geqrf )"
  },
  {
    "id": 26502,
    "content": "The cuSolverSP library functions are available for data types float , double , cuComplex , and cuDoubleComplex The naming convention is as follows: cusolverSp[Host] [][] where cuSolverSp is the GPU path and cusolverSpHost is the corresponding CPU path can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic type, respectively The"
  },
  {
    "id": 26503,
    "content": "can be ls , lsq , eig , eigs , corresponding to linear solver, least-square solver, eigenvalue solver and number of eigenvalues in a box, respectively For example, qr (sparse QR factorization) is used in linear solver and least-square solver"
  },
  {
    "id": 26504,
    "content": "All of the functions have the return type cusolverStatus_t and are explained in more detail in the chapters that follow"
  },
  {
    "id": 26505,
    "content": "cuSolverSP API  Routine Data format Operation Output format Based on csrlsvlu csr linear solver (ls) vector (v) LU (lu) with partial pivoting csrlsvqr csr linear solver (ls) vector (v) QR factorization (qr) csrlsvchol csr linear solver (ls) vector (v) Cholesky factorization (chol) csrlsqvqr csr least-square solver (lsq) vector (v) QR factorization (qr) csreigvsi csr eigenvalue solver (eig)"
  },
  {
    "id": 26506,
    "content": "vector (v) shift-inverse csreigs csr number of eigenvalues in a box (eigs) csrsymrcm csr Symmetric Reverse Cuthill-McKee (symrcm) The cuSolverRF library routines are available for data type double"
  },
  {
    "id": 26507,
    "content": "Most of the routines follow the naming convention: cusolverRf __[ [Host] ](…) where the trailing optional Host qualifier indicates the data is accessed on the host versus on the device, which is the default"
  },
  {
    "id": 26508,
    "content": "The can be Setup , Analyze , Refactor , Solve , ResetValues , AccessBundledFactors and ExtractSplitFactors"
  },
  {
    "id": 26512,
    "content": "Asynchronous Execution  The cuSolver library functions prefer to keep asynchronous execution as much as possible Developers can always use the cudaDeviceSynchronize() function to ensure that the execution of a particular cuSolver library routine has completed A developer can also use the cudaMemcpy() routine to copy data from the device to the host and vice versa, using the"
  },
  {
    "id": 26513,
    "content": "cudaMemcpyDeviceToHost and cudaMemcpyHostToDevice parameters, respectively In this case there is no need to add a call to cudaDeviceSynchronize() because the call to cudaMemcpy() with the above parameters is blocking and completes only when the results are ready on the host"
  },
  {
    "id": 26519,
    "content": "Z would yield MAJOR_VERSION=X , MINOR_VERSION=Y , PATCH_LEVEL=Z ) typedef enum libraryPropertyType_t { MAJOR_VERSION , MINOR_VERSION , PATCH_LEVEL } libraryPropertyType ; The following code can show the version of cusolver library int major = -1 , minor = -1 , patch = -1 ; cusolverGetProperty ( MAJOR_VERSION , & major ); cusolverGetProperty ( MINOR_VERSION , & minor ); cusolverGetProperty ("
  },
  {
    "id": 26524,
    "content": "High Precision Package  The cusolver library uses high precision for iterative refinement when necessary"
  },
  {
    "id": 26528,
    "content": "It is not a reference for the cuSolver API data types and functions; that is provided in subsequent chapters"
  },
  {
    "id": 26532,
    "content": "Thread Safety  The library is thread-safe, and its functions can be called from multiple host threads"
  },
  {
    "id": 26536,
    "content": "Scalar Parameters  In the cuSolver API, the scalar parameters can be passed by reference on the host"
  },
  {
    "id": 26540,
    "content": "Parallelism with Streams  If the application performs several small independent computations, or if it makes data transfers in parallel with the computation, then CUDA streams can be used to overlap these tasks To achieve the overlap of computation between the tasks, the developer should: Create CUDA streams using the function cudaStreamCreate() , and Set the stream to be used by each individual"
  },
  {
    "id": 26541,
    "content": "cuSolver library routine by calling, for example, cusolverDnSetStream() , just prior to calling the actual cuSolverDN routine The computations performed in separate streams would then be overlapped automatically on the GPU, when possible This approach is especially useful when the computation performed by a single task is relatively small, and is not enough to fill the GPU with work, or when there"
  },
  {
    "id": 26546,
    "content": "How to Link cusolver Library  cusolver library provides dynamic library libcusolver so and static library libcusolver_static"
  },
  {
    "id": 26561,
    "content": "Link Third-party LAPACK Library  Starting with CUDA 10 1 update 2, NVIDIA LAPACK library libcusolver_lapack_static a is a subset of LAPACK and only contains GPU accelerated stedc and bdsqr The user has to link libcusolver_static a with libcusolver_lapack_static"
  },
  {
    "id": 26567,
    "content": "1 update 2, the third-party LAPACK library no longer affects the behavior of cusolver library, neither functionality nor performance"
  },
  {
    "id": 26572,
    "content": "There are no symbol conflicts between libcusolver_lapack_static a and other third-party LAPACK libraries, which allows linking the same application to libcusolver_lapack_static a and another third-party LAPACK library"
  },
  {
    "id": 26574,
    "content": "so will not pick up any routines from the third-party LAPACK library even if you link the application with it"
  },
  {
    "id": 26578,
    "content": "Convention of info  Each LAPACK routine returns an info which indicates the position of invalid parameter To be consistent with base-1 in LAPACK, cusolver does not report invalid handle into info Instead, cusolver returns CUSOLVER_STATUS_NOT_INITIALIZED for invalid handle"
  },
  {
    "id": 26582,
    "content": "Usage of _bufferSize  There is no cudaMalloc inside cuSolver library, the user must allocate the device workspace explicitly"
  },
  {
    "id": 26583,
    "content": "The routine xyz_bufferSize is to query the size of workspace of the routine xyz , for example xyz = potrf To make the API simple, xyz_bufferSize follows almost the same signature of xyz even it only depends on some parameters, for example, device pointer is not used to decide the size of workspace In most cases, xyz_bufferSize is called in the beginning before actual device data (pointing by a"
  },
  {
    "id": 26585,
    "content": "See: cusolverDnLoggerSetCallback() , cusolverDnLoggerSetFile() , cusolverDnLoggerOpenFile() , cusolverDnLoggerSetLevel() , cusolverDnLoggerSetMask() , cusolverDnLoggerForceDisable()"
  },
  {
    "id": 26589,
    "content": "Deterministic Results  Throughout this documentation, a function is declared as deterministic if it computes the exact same bitwise results for every execution with the same input parameters, hard- and software environment Conversely, a non-deterministic function might compute bitwise different results due to a varying order of floating point operations, e"
  },
  {
    "id": 26591,
    "content": ", a sum s of four values a , b , c , d can be computed in different orders: s = (a + b) + (c + d) s = (a + (b + c)) + d s = a + (b + (c + d)) … Due to the non-associativity of floating point arithmetic, all results might be bitwise different"
  },
  {
    "id": 26592,
    "content": "For improved performance of some functions, it is possible to allow non-deterministic results with cusolverDnSetDeterministicMode()"
  },
  {
    "id": 26598,
    "content": "cuSolverDN Types  The float , double , cuComplex , and cuDoubleComplex data types are supported The first two are standard C data types, while the last two are exported from cuComplex"
  },
  {
    "id": 26605,
    "content": "cusolverDnHandle_t  This is a pointer type to an opaque cuSolverDN context, which the user must initialize by calling cusolverDnCreate() prior to calling any other library function"
  },
  {
    "id": 26606,
    "content": "An un-initialized Handle object will lead to unexpected behavior, including crashes of cuSolverDN The handle created and returned by cusolverDnCreate() must be passed to every cuSolverDN function"
  },
  {
    "id": 26611,
    "content": "cublasFillMode_t  The type indicates which part (lower or upper) of the dense matrix was filled and consequently should be used by the function Notice that BLAS implementations often use Fortran characters ‘L’ or ‘l’ (lower) and ‘U’ or ‘u’ (upper) to describe which part of the matrix is filled"
  },
  {
    "id": 26616,
    "content": "cublasOperation_t  The cublasOperation_t type indicates which operation needs to be performed with the dense matrix Notice that BLAS implementations often use Fortran characters ‘N’ or ‘n’ (non-transpose), ‘T’ or ‘t’ (transpose) and ‘C’ or ‘c’ (conjugate transpose) to describe which operations needs to be performed with the dense matrix"
  },
  {
    "id": 26621,
    "content": "cusolverEigType_t  The cusolverEigType_t type indicates which type of eigenvalue the solver is Value Meaning CUSOLVER_EIG_TYPE_1 A*x = lambda*B*x CUSOLVER_EIG_TYPE_2 A*B*x = lambda*x CUSOLVER_EIG_TYPE_3 B*A*x = lambda*x Notice that LAPACK implementations often use Fortran integer 1 (A*x = lambda*B*x), 2 (A*B*x = lambda*x), 3 (B*A*x = lambda*x) to indicate which type of eigenvalue the solver is"
  },
  {
    "id": 26626,
    "content": "cusolverEigMode_t  The cusolverEigMode_t type indicates whether or not eigenvectors are computed Notice that LAPACK implementations often use Fortran character 'N' (only eigenvalues are computed), 'V' (both eigenvalues and eigenvectors are computed) to indicate whether or not eigenvectors are computed"
  },
  {
    "id": 26631,
    "content": "cusolverIRSRefinement_t  The cusolverIRSRefinement_t type indicates which solver type would be used for the specific cusolver function"
  },
  {
    "id": 26632,
    "content": "More details about the refinement process can be found in Azzam Haidar, Stanimire Tomov, Jack Dongarra, and Nicholas J"
  },
  {
    "id": 26635,
    "content": "Harnessing GPU tensor cores for fast FP16 arithmetic to speed up mixed-precision iterative refinement solvers"
  },
  {
    "id": 26636,
    "content": "In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis (SC ‘18)"
  },
  {
    "id": 26637,
    "content": "Value Meaning CUSOLVER_IRS_REFINE_NOT_SET Solver is not set; this value is what is set when creating the params structure"
  },
  {
    "id": 26638,
    "content": "CUSOLVER_IRS_REFINE_NONE No refinement solver, the IRS solver performs a factorization followed by a solve without any refinement For example if the IRS solver was cusolverDnIRSXgesv() , this is equivalent to a Xgesv routine without refinement and where the factorization is carried out in the lowest precision If for example the main precision was CUSOLVER_R_64F and the lowest was CUSOLVER_R_64F"
  },
  {
    "id": 26639,
    "content": "as well, then this is equivalent to a call to cusolverDnDgesv() CUSOLVER_IRS_REFINE_GMRES GMRES (Generalized Minimal Residual) based iterative refinement solver In recent study, the GMRES method has drawn the scientific community attention for its ability to be used as refinement solver that outperforms the classical iterative refinement method CUSOLVER_IRS_REFINE_CLASSICAL_GMRES Classical"
  },
  {
    "id": 26640,
    "content": "iterative refinement solver that uses the GMRES (Generalized Minimal Residual) internally to solve the correction equation at each iteration We call the classical refinement iteration the outer iteration while the GMRES is called inner iteration Note that if the tolerance of the inner GMRES is set very low, lets say to machine precision, then the outer classical refinement iteration will performs"
  },
  {
    "id": 26641,
    "content": "only one iteration and thus this option will behave like CUSOLVER_IRS_REFINE_GMRES CUSOLVER_IRS_REFINE_GMRES_GMRES Similar to CUSOLVER_IRS_REFINE_CLASSICAL_GMRES which consists of classical refinement process that uses GMRES to solve the inner correction system; here it is a GMRES (Generalized Minimal Residual) based iterative refinement solver that uses another GMRES internally to solve the"
  },
  {
    "id": 26647,
    "content": "cusolverDnIRSParams_t  This is a pointer type to an opaque cusolverDnIRSParams_t structure, which holds parameters for the iterative refinement linear solvers such as cusolverDnXgesv()"
  },
  {
    "id": 26648,
    "content": "Use corresponding helper functions described below to either Create/Destroy this structure or Set/Get solver parameters"
  },
  {
    "id": 26653,
    "content": "cusolverDnIRSInfos_t  This is a pointer type to an opaque cusolverDnIRSInfos_t structure, which holds information about the performed call to an iterative refinement linear solver (e"
  },
  {
    "id": 26656,
    "content": "Use corresponding helper functions described below to either Create/Destroy this structure or retrieve solve information"
  },
  {
    "id": 26661,
    "content": "cusolverDnFunction_t  The cusolverDnFunction_t type indicates which routine needs to be configured by cusolverDnSetAdvOptions()"
  },
  {
    "id": 26667,
    "content": "cusolverAlgMode_t  The cusolverAlgMode_t type indicates which algorithm is selected by cusolverDnSetAdvOptions()"
  },
  {
    "id": 26668,
    "content": "The set of algorithms supported for each routine is described in detail along with the routine’s documentation"
  },
  {
    "id": 26680,
    "content": "Parameters Parameter Memory In/out Description logLevel output See cuSOLVERDn Logging functionName output The name of the API that logged this message"
  },
  {
    "id": 26686,
    "content": "cusolverDeterministicMode_t  The cusolverDeterministicMode_t type indicates whether multiple cuSolver function executions with the same input have the same bitwise equal result (deterministic) or might have bitwise different results (non-deterministic) In comparison to cublasAtomicsMode_t , which only includes the usage of atomic functions, cusolverDeterministicMode_t includes all"
  },
  {
    "id": 26687,
    "content": "non-deterministic programming patterns The deterministic mode can be set and queried using cusolverDnSetDeterministicMode() and cusolverDnGetDeterministicMode() routines, respectively CUSOLVER_ALLOW_NON_DETERMINISTIC_RESULTS Allow non-deterministic results"
  },
  {
    "id": 26698,
    "content": "cusolverDirectMode_t  Specifies the order in which the elementary reflectors are multiplied to form the block reflector"
  },
  {
    "id": 26708,
    "content": "cusolverSpHandle_t  This is a pointer type to an opaque cuSolverSP context, which the user must initialize by calling cusolverSpCreate() prior to calling any other library function"
  },
  {
    "id": 26709,
    "content": "An un-initialized Handle object will lead to unexpected behavior, including crashes of cuSolverSP The handle created and returned by cusolverSpCreate() must be passed to every cuSolverSP function"
  },
  {
    "id": 26714,
    "content": "cusparseMatDescr_t  We have chosen to keep the same structure as exists in cuSPARSE to describe the shape and properties of a matrix typedef struct { cusparseMatrixType_t MatrixType ; cusparseFillMode_t FillMode ; cusparseDiagType_t DiagType ; cusparseIndexBase_t IndexBase ; } cusparseMatDescr_t ; Please read documentation of the cuSPARSE Library to understand each field of cusparseMatDescr_t"
  },
  {
    "id": 26719,
    "content": "cusolverStatus_t  This is a status type returned by the library functions and it can have the following values"
  },
  {
    "id": 26720,
    "content": "This is usually caused by the lack of a prior call, an error in the CUDA Runtime API called by the cuSolver routine, or an error in the hardware setup To correct: call cusolverDnCreate() prior to the function call; and check that the hardware, an appropriate version of the driver, and the cuSolver library are correctly installed To correct: prior to the function call, deallocate previously"
  },
  {
    "id": 26722,
    "content": "CUSOLVER_STATUS_INVALID_VALUE An unsupported value or parameter was passed to the function (a negative vector size, for example)"
  },
  {
    "id": 26723,
    "content": "CUSOLVER_STATUS_ARCH_MISMATCH The function requires a feature absent from the device architecture; usually caused by the lack of support for atomic operations or double precision"
  },
  {
    "id": 26726,
    "content": "This is often caused by a launch failure of the kernel on the GPU, which can be caused by multiple reasons"
  },
  {
    "id": 26727,
    "content": "To correct: check that the hardware, an appropriate version of the driver, and the cuSolver library are correctly installed Also, check that the memory passed as a parameter to the routine is not being deallocated prior to the routine’s completion"
  },
  {
    "id": 26729,
    "content": "batched version is not supported or M geqrf() , cusolverDnsyevd() , cusolverDnsyevdx() , cusolverDngesvd() (if m > n ), cusolverDngesvdj() , cusolverDnXgeqrf() , cusolverDnXsyevd() , cusolverDnXsyevdx() , cusolverDnXgesvd() (if m > n ), cusolverDnXgesvdr() and cusolverDnXgesvdp()"
  },
  {
    "id": 26736,
    "content": "cusolverDnGetDeterministicMode()  cusolverStatus_t cusolverDnGetDeterministicMode ( cusolverDnHandle_t handle , cusolverDeterministicMode_t * mode ) This function queries the deterministic mode which is set for handle CUSOLVER_STATUS_INVALID_VALUE mode is a NULL pointer"
  },
  {
    "id": 26741,
    "content": "cusolverDnCreateSyevjInfo()  cusolverStatus_t cusolverDnCreateSyevjInfo ( syevjInfo_t * info ); This function creates and initializes the structure of syevj , syevjBatched and sygvj to default values"
  },
  {
    "id": 26747,
    "content": "cusolverDnDestroySyevjInfo()  cusolverStatus_t cusolverDnDestroySyevjInfo ( syevjInfo_t info ); This function destroys and releases any memory required by the structure"
  },
  {
    "id": 26753,
    "content": "cusolverDnXsyevjSetTolerance()  cusolverStatus_t cusolverDnXsyevjSetTolerance ( syevjInfo_t info , double tolerance ) This function configures tolerance of syevj"
  },
  {
    "id": 26759,
    "content": "cusolverDnXsyevjSetMaxSweeps()  cusolverStatus_t cusolverDnXsyevjSetMaxSweeps ( syevjInfo_t info , int max_sweeps ) This function configures maximum number of sweeps in syevj max_sweeps host input Maximum number of sweeps"
  },
  {
    "id": 26764,
    "content": "cusolverDnXsyevjSetSortEig()  cusolverStatus_t cusolverDnXsyevjSetSortEig ( syevjInfo_t info , int sort_eig ) If sort_eig is zero, the eigenvalues are not sorted sort_eig host input If sort_eig is zero, the eigenvalues are not sorted"
  },
  {
    "id": 26769,
    "content": "cusolverDnXsyevjGetResidual()  cusolverStatus_t cusolverDnXsyevjGetResidual ( cusolverDnHandle_t handle , syevjInfo_t info , double * residual ) This function reports residual of syevj or sygvj"
  },
  {
    "id": 26770,
    "content": "If the user calls this function after syevjBatched , the error CUSOLVER_STATUS_NOT_SUPPORTED is returned CUSOLVER_STATUS_NOT_SUPPORTED Does not support batched version"
  },
  {
    "id": 26775,
    "content": "cusolverDnXsyevjGetSweeps()  cusolverStatus_t cusolverDnXsyevjGetSweeps ( cusolverDnHandle_t handle , syevjInfo_t info , int * executed_sweeps ) This function reports number of executed sweeps of syevj or sygvj executed_sweeps host output Number of executed sweeps"
  },
  {
    "id": 26780,
    "content": "cusolverDnCreateGesvdjInfo()  cusolverStatus_t cusolverDnCreateGesvdjInfo ( gesvdjInfo_t * info ); This function creates and initializes the structure of gesvdj and gesvdjBatched to default values Parameter Memory In/out Meaning info host output The pointer to the structure of gesvdj"
  },
  {
    "id": 26785,
    "content": "cusolverDnDestroyGesvdjInfo()  cusolverStatus_t cusolverDnDestroyGesvdjInfo ( gesvdjInfo_t info ); This function destroys and releases any memory required by the structure Parameter Memory In/out Meaning info host input The structure of gesvdj"
  },
  {
    "id": 26790,
    "content": "cusolverDnXgesvdjSetTolerance()  cusolverStatus_t cusolverDnXgesvdjSetTolerance ( gesvdjInfo_t info , double tolerance ) This function configures tolerance of gesvdj tolerance host input Accuracy of numerical singular values"
  },
  {
    "id": 26795,
    "content": "cusolverDnXgesvdjSetMaxSweeps()  cusolverStatus_t cusolverDnXgesvdjSetMaxSweeps ( gesvdjInfo_t info , int max_sweeps ) This function configures the maximum number of sweeps in gesvdj"
  },
  {
    "id": 26800,
    "content": "cusolverDnXgesvdjSetSortEig()  cusolverStatus_t cusolverDnXgesvdjSetSortEig ( gesvdjInfo_t info , int sort_svd ) If sort_svd is zero, the singular values are not sorted sort_svd host input If sort_svd is zero, the singular values are not sorted"
  },
  {
    "id": 26805,
    "content": "cusolverDnXgesvdjGetResidual()  cusolverStatus_t cusolverDnXgesvdjGetResidual ( cusolverDnHandle_t handle , gesvdjInfo_t info , double * residual ) This function reports residual of gesvdj"
  },
  {
    "id": 26806,
    "content": "If the user calls this function after gesvdjBatched , the error CUSOLVER_STATUS_NOT_SUPPORTED is returned"
  },
  {
    "id": 26807,
    "content": "cusolverDnXgesvdjGetSweeps()  cusolverStatus_t cusolverDnXgesvdjGetSweeps ( cusolverDnHandle_t handle , gesvdjInfo_t info , int * executed_sweeps ) This function reports number of executed sweeps of gesvdj"
  },
  {
    "id": 26808,
    "content": "cusolverDnIRSParamsCreate()  cusolverStatus_t cusolverDnIRSParamsCreate ( cusolverDnIRSParams_t * params ); This function creates and initializes the structure of parameters for an IRS solver such as the cusolverDnIRSXgesv() or the cusolverDnIRSXgels() functions to default values The params structure created by this function can be used by one or more call to the same or to a different IRS solver"
  },
  {
    "id": 26810,
    "content": "2, the behavior was different and a new params structure was needed to be created per each call to an IRS solver Also note that the user can also change configurations of the params and then call a new IRS instance, but be careful that the previous call was done because any change to the configuration before the previous call was done could affect it Parameter Memory In/out Meaning params host"
  },
  {
    "id": 26811,
    "content": "output Pointer to the cusolverDnIRSParams_t Params structure Status Returned CUSOLVER_STATUS_SUCCESS The structure was created and initialized successfully"
  },
  {
    "id": 26816,
    "content": "cusolverDnIRSParamsDestroy()  cusolverStatus_t cusolverDnIRSParamsDestroy ( cusolverDnIRSParams_t params ); This function destroys and releases any memory required by the Params structure Parameter Memory In/out Meaning params host input The cusolverDnIRSParams_t Params structure CUSOLVER_STATUS_IRS_INFOS_NOT_DESTROYED Not all the Infos structure associated with this Params structure have been"
  },
  {
    "id": 26822,
    "content": "cusolverDnIRSParamsSetSolverPrecisions()  cusolverStatus_t cusolverDnIRSParamsSetSolverPrecisions ( cusolverDnIRSParams_t params , cusolverPrecType_t solver_main_precision , cusolverPrecType_t solver_lowest_precision ); This function sets both the main and the lowest precision for the Iterative Refinement Solver (IRS) By lowest precision, we mean the solver is allowed to use as lowest"
  },
  {
    "id": 26823,
    "content": "computational precision during the LU factorization process Note that the user has to set both the main and lowest precision before the first call to the IRS solver because they are NOT set by default with the params structure creation, as it depends on the Input Output data type and user request"
  },
  {
    "id": 26824,
    "content": "It is a wrapper to both cusolverDnIRSParamsSetSolverMainPrecision() and cusolverDnIRSParamsSetSolverLowestPrecision()"
  },
  {
    "id": 26828,
    "content": "More precisely, it depends on many factors, but for large matrices sizes, it is the ratio of the matrix-matrix rank-k product (e"
  },
  {
    "id": 26831,
    "content": "For instance, if the inout precision is real double precision CUSOLVER_R_64F and the lowest precision is CUSOLVER_R_32F, then we can expect a speedup of at most 2X for large problem sizes"
  },
  {
    "id": 26832,
    "content": "A reasonable strategy should take the number of right-hand sides, the size of the matrix as well as the convergence rate into account"
  },
  {
    "id": 26834,
    "content": "solver_main_precision host input Allowed Inputs/Outputs datatype (for example CUSOLVER_R_FP64 for a real double precision data) solver_lowest_precision host input Allowed lowest compute type (for example CUSOLVER_R_16F for half precision computation) Supported Inputs/Outputs data type and lower precision for the IRS solver  Inputs/Outputs Data Type (e"
  },
  {
    "id": 26836,
    "content": ", main precision) Supported values for the lowest precision CUSOLVER_C_64F CUSOLVER_C_64F, CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_C_32F CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_R_64F CUSOLVER_R_64F, CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 CUSOLVER_R_32F CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF,"
  },
  {
    "id": 26841,
    "content": "cusolverDnIRSParamsSetSolverMainPrecision()  cusolverStatus_t cusolverDnIRSParamsSetSolverMainPrecision ( cusolverDnIRSParams_t params , cusolverPrecType_t solver_main_precision ); This function sets the main precision for the Iterative Refinement Solver (IRS) Note that the user has to set both the main and lowest precision before a first call to the IRS solver because they are NOT set by"
  },
  {
    "id": 26842,
    "content": "default with the params structure creation, as it depends on the Input Output data type and user request user can set it by either calling this function or by calling cusolverDnIRSParamsSetSolverPrecisions() which set both the main and the lowest precision together All possible combinations of main/lowest precision are described in the table in the cusolverDnIRSParamsSetSolverPrecisions() section"
  },
  {
    "id": 26843,
    "content": "above See the table in the cusolverDnIRSParamsSetSolverPrecisions() section above for the supported precisions"
  },
  {
    "id": 26848,
    "content": "cusolverDnIRSParamsSetSolverLowestPrecision()  cusolverStatus_t cusolverDnIRSParamsSetSolverLowestPrecision ( cusolverDnIRSParams_t params , cusolverPrecType_t lowest_precision_type ); This function sets the lowest precision that will be used by Iterative Refinement Solver The ratio of the performance of the lowest precision over the main precision (e"
  },
  {
    "id": 26851,
    "content": "lowest_precision_type host input Allowed lowest compute type (for example CUSOLVER_R_16F for half precision computation)"
  },
  {
    "id": 26856,
    "content": "cusolverDnIRSParamsSetRefinementSolver()  cusolverStatus_t cusolverDnIRSParamsSetRefinementSolver ( cusolverDnIRSParams_t params , cusolverIRSRefinement_t solver ); This function sets the refinement solver to be used in the Iterative Refinement Solver functions such as the cusolverDnIRSXgesv() or the cusolverDnIRSXgels() functions Note that the user has to set the refinement algorithm before a"
  },
  {
    "id": 26859,
    "content": "Parameter Memory In/out Meaning params host in/out The cusolverDnIRSParams_t Params structure solver host input Type of the refinement solver to be used by the IRS solver such as cusolverDnIRSXgesv() or cusolverDnIRSXgels() CUSOLVER_IRS_REFINE_NOT_SET Solver is not set, this value is what is set when creating the params structure CUSOLVER_IRS_REFINE_NONE No refinement solver; the IRS solver"
  },
  {
    "id": 26860,
    "content": "performs a factorization followed by a solve without any refinement For example, if the IRS solver was cusolverDnIRSXgesv() , this is equivalent to a Xgesv routine without refinement and where the factorization is carried out in the lowest precision"
  },
  {
    "id": 26861,
    "content": "Note that if the tolerance of the inner GMRES is set very low, let say to machine precision, then the outer classical refinement iteration will performs only one iteration and thus this option will behaves like CUSOLVER_IRS_REFINE_GMRES CUSOLVER_IRS_REFINE_GMRES_GMRES Similar to CUSOLVER_IRS_REFINE_CLASSICAL_GMRES which consists of classical refinement process that uses GMRES to solve the inner"
  },
  {
    "id": 26862,
    "content": "correction system, here it is a GMRES (Generalized Minimal Residual) based iterative refinement solver that uses another GMRES internally to solve the preconditioned system"
  },
  {
    "id": 26867,
    "content": "cusolverDnIRSParamsSetTol()  cusolverStatus_t cusolverDnIRSParamsSetTol ( cusolverDnIRSParams_t params , double val ); This function sets the tolerance for the refinement solver"
  },
  {
    "id": 26868,
    "content": "By default it is such that all the RHS satisfy: RNRM LAMCH(‘Epsilon’) BWDMAX, the value BWDMAX is fixed to 1"
  },
  {
    "id": 26870,
    "content": "Our goal is to give the user more control such a way he can investigate and control every detail of the IRS solver"
  },
  {
    "id": 26871,
    "content": "Note that the tolerance value is always in real double precision whatever the Inputs/Outputs datatype is val host input Double precision real value to which the refinement tolerance will be set"
  },
  {
    "id": 26876,
    "content": "cusolverDnIRSParamsSetTolInner()  cusolverStatus_t cusolverDnIRSParamsSetTolInner ( cusolverDnIRSParams_t params , double val ); This function sets the tolerance for the inner refinement solver when the refinement solver consists of two-levels solver (e"
  },
  {
    "id": 26879,
    "content": "It is not referenced in case of one level refinement solver such as CUSOLVER_IRS_REFINE_CLASSICAL or CUSOLVER_IRS_REFINE_GMRES For example, if the Refinement Solver was set to CUSOLVER_IRS_REFINE_CLASSICAL_GMRES, setting this tolerance mean that the inner GMRES solver will converge to that tolerance at each outer iteration of the classical refinement solver"
  },
  {
    "id": 26880,
    "content": "Note the, the tolerance value is always in real double precision whatever the Inputs/Outputs datatype is val host input Double precision real value to which the tolerance of the inner refinement solver will be set"
  },
  {
    "id": 26885,
    "content": "cusolverDnIRSParamsSetMaxIters()  cusolverStatus_t cusolverDnIRSParamsSetMaxIters ( cusolverDnIRSParams_t params , int max_iters ); This function sets the total number of allowed refinement iterations after which the solver will stop Total means any iteration which means the sum of the outer and the inner iterations (inner is meaningful when two-levels refinement solver is set) max_iters host"
  },
  {
    "id": 26891,
    "content": "cusolverDnIRSParamsSetMaxItersInner()  cusolverStatus_t cusolverDnIRSParamsSetMaxItersInner ( cusolverDnIRSParams_t params , cusolver_int_t maxiters_inner ); This function sets the maximal number of iterations allowed for the inner refinement solver The inner refinement solver will stop after reaching either the inner tolerance or the MaxItersInner value Note that this value could not be larger"
  },
  {
    "id": 26892,
    "content": "than the MaxIters since MaxIters is the total number of allowed iterations Note that if the user calls cusolverDnIRSParamsSetMaxIters after calling this function, SetMaxIters has priority and will overwrite MaxItersInner to the minimum value of (MaxIters, MaxItersInner) Parameter Memory In/out Meaning params host in/out The cusolverDnIRSParams_t Params structure maxiters_inner host input Maximum"
  },
  {
    "id": 26893,
    "content": "number of allowed inner iterations for the inner refinement solver Meaningful when the refinement solver is a two-levels solver such as CUSOLVER_IRS_REFINE_CLASSICAL_GMRES or CUSOLVER_IRS_REFINE_GMRES_GMRES CUSOLVER_STATUS_IRS_PARAMS_INVALID If the value was larger than MaxIters"
  },
  {
    "id": 26898,
    "content": "cusolverDnIRSParamsEnableFallback()  cusolverStatus_t cusolverDnIRSParamsEnableFallback ( cusolverDnIRSParams_t params ); This function enable the fallback to the main precision in case the Iterative Refinement Solver (IRS) failed to converge In other term, if the IRS solver failed to converge, the solver will return a no convergence code (e"
  },
  {
    "id": 26901,
    "content": "cusolverStatus_t cusolverDnSpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n ,"
  },
  {
    "id": 26902,
    "content": "cuComplex * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnZpotrf_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , float"
  },
  {
    "id": 26903,
    "content": "* Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnDpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , double * Workspace , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n ,"
  },
  {
    "id": 26904,
    "content": "cuComplex * A , int lda , cuComplex * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnZpotrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * Workspace , int Lwork , int * devInfo ); This function computes the Cholesky factorization of a Hermitian positive-definite matrix"
  },
  {
    "id": 26905,
    "content": "If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only the lower triangular part of A is processed, and replaced by the lower triangular Cholesky factor L \\(A = L*L^{H}\\) If input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by upper triangular Cholesky factor U"
  },
  {
    "id": 26906,
    "content": "\\(A = U^{H}*U\\) The user has to provide working space which is pointed by input parameter Workspace The input parameter Lwork is size of the working space, and it is returned by potrf_bufferSize()"
  },
  {
    "id": 26907,
    "content": "some leading minor of A is not positive definite, or equivalently some diagonal elements of L or U is not a real number The output parameter devInfo would indicate smallest leading minor of A which is not positive definite If output parameter devInfo = -i (less than zero), the i-th parameter is wrong (not counting handle) API of potrf Parameter Memory In/out Meaning handle host input Handle to"
  },
  {
    "id": 26909,
    "content": "uplo host input Indicates if matrix A lower or upper part is stored; the other part is not referenced If input parameter uplo is CUBLAS_FILL_MODE_LOWER , A is lower triangular Cholesky factor L corresponding to \\(A = L*L^{H}\\) If input parameter uplo is CUBLAS_FILL_MODE_UPPER , A is upper triangular Cholesky factor U corresponding to \\(A = U^{H}*U\\)"
  },
  {
    "id": 26910,
    "content": "API of potrs Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 26911,
    "content": "uplo host input Indicates if matrix A lower or upper part is stored, the other part is not referenced"
  },
  {
    "id": 26912,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( npotri()  These helper functions calculate the necessary size of work buffers"
  },
  {
    "id": 26913,
    "content": "cusolverStatus_t cusolverDnSpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n ,"
  },
  {
    "id": 26914,
    "content": "cuComplex * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnZpotri_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , float"
  },
  {
    "id": 26915,
    "content": "* Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnDpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , double * Workspace , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n ,"
  },
  {
    "id": 26916,
    "content": "cuComplex * A , int lda , cuComplex * Workspace , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnZpotri ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * Workspace , int Lwork , int * devInfo ); This function computes the inverse of a positive-definite matrix A using the Cholesky factorization \\(A = L*L^{H} = U^{H}*U\\) computed"
  },
  {
    "id": 26917,
    "content": "by potrf() A is a n×n matrix containing the triangular factor L or U computed by the Cholesky factorization"
  },
  {
    "id": 26918,
    "content": "Only lower or upper part is meaningful and the input parameter uplo indicates which part of the matrix is used If the input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced the by lower triangular part of the inverse of A If the input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by the"
  },
  {
    "id": 26919,
    "content": "upper triangular part of the inverse of A The user has to provide the working space which is pointed to by input parameter Workspace The input parameter Lwork is the size of the working space, returned by potri_bufferSize() some leading minor of L or U , is null, the output parameter devInfo would indicate the smallest leading minor of L or U which is not positive definite If the output parameter"
  },
  {
    "id": 26920,
    "content": "devInfo = -i (less than zero), the i-th parameter is wrong (not counting the handle) API of potri Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 26921,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngetrf()  These helper functions calculate the size of work buffers needed"
  },
  {
    "id": 26922,
    "content": "cusolverStatus_t cusolverDnSgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , int * Lwork );"
  },
  {
    "id": 26923,
    "content": "cusolverStatus_t cusolverDnZgetrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real single and double precision, respectively cusolverStatus_t cusolverDnSgetrf ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , float * Workspace , int * devIpiv , int * devInfo ); cusolverStatus_t cusolverDnDgetrf"
  },
  {
    "id": 26924,
    "content": "( cusolverDnHandle_t handle , int m , int n , double * A , int lda , double * Workspace , int * devIpiv , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCgetrf ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , cuComplex * Workspace , int * devIpiv , int * devInfo ); cusolverStatus_t"
  },
  {
    "id": 26925,
    "content": "cusolverDnZgetrf ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * Workspace , int * devIpiv , int * devInfo ); This function computes the LU factorization of a m×n matrix \\(P*A = L*U\\) where A is a m×n matrix, P is a permutation matrix, L is a lower triangular matrix with unit diagonal, and U is an upper triangular matrix"
  },
  {
    "id": 26927,
    "content": "No matter LU factorization failed or not, the output parameter devIpiv contains pivoting sequence, row i is interchanged with row devIpiv(i)"
  },
  {
    "id": 26928,
    "content": "The user can choose the legacy implementation with minimal workspace by Getrf and cusolverDnSetAdvOptions(params, CUSOLVERDN_GETRF, CUSOLVER_ALG_1)"
  },
  {
    "id": 26929,
    "content": "API of getrf Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 26930,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n array of dimension lda * n with lda is not less than max(1,m)"
  },
  {
    "id": 26932,
    "content": "The generic API has two different types, dataTypeA is data type of the matrix A , computeType is compute type of the operation valid combination of data type and compute type DataTypeA ComputeType Meaning CUDA_R_32F CUDA_R_32F SGETRF CUDA_R_64F CUDA_R_64F DGETRF CUDA_C_32F CUDA_C_32F CGETRF CUDA_C_64F CUDA_C_64F ZGETRF Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 26933,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngetrs()  Please visit cuSOLVER Library Samples - getrf for a code example"
  },
  {
    "id": 26934,
    "content": "The input parameter trans is defined by \\(\\text{op}(A) = \\left\\{ \\begin{matrix} A & {\\text{if~}\\textsf{trans\\ ==\\ CUBLAS\\_OP\\_N}} \\\\ A^{T} & {\\text{if~}\\textsf{trans\\ ==\\ CUBLAS\\_OP\\_T}} \\\\ A^{H} & {\\text{if~}\\textsf{trans\\ ==\\ CUBLAS\\_OP\\_C}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 26936,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n array of dimension ldb * nrhs with ldb is not less than max(1,n)"
  },
  {
    "id": 26937,
    "content": "The generic API has two different types, dataTypeA is data type of the matrix A and dataTypeB is data type of the matrix B Valid combination of data type and compute type DataTypeA dataTypeB Meaning CUDA_R_32F CUDA_R_32F SGETRS CUDA_R_64F CUDA_R_64F DGETRS CUDA_C_32F CUDA_C_32F CGETRS CUDA_C_64F CUDA_C_64F ZGETRS Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 26938,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngesv()  These functions are modelled after functions DSGESV and ZCGESV from LAPACK"
  },
  {
    "id": 26939,
    "content": "They compute the solution of a system of linear equations with one or multiple right hand sides using mixed precision iterative refinement techniques based on the LU factorization Xgesv These functions are similar in term of functionalities to the full precision LU solver ( Xgesv , where X denotes Z,C,D,S) but it uses lower precision internally in order to provide faster time to solution, from"
  },
  {
    "id": 26940,
    "content": "here comes the name mixed precision Mixed precision iterative refinement techniques means that the solver compute an LU factorization in lower precision and then iteratively refine the solution to achieve the accuracy of the Inputs/Outputs datatype precision The corresponds to the Inputs/Outputs datatype precision while represent the internal lower precision at which the factorization will be"
  },
  {
    "id": 26942,
    "content": "Functions API are designed to be as close as possible to LAPACK API to be considered as a quick and easy drop-in replacement"
  },
  {
    "id": 26943,
    "content": "gesv() functions are designated by two floating point precisions The corresponds to the main precision (e"
  },
  {
    "id": 26945,
    "content": ", Inputs/Outputs datatype precision) and the represent the internal lower precision at which the factorization will be carried on cusolvergesv() first attempts to factorize the matrix in lower precision and use this factorization within an iterative refinement procedure to obtain a solution with same normwise backward error as the main precision If the approach fails to converge, then the method"
  },
  {
    "id": 26946,
    "content": "fallback to the main precision factorization and solve (Xgesv) such a way that there is always a good solution at the output of these functions If is equal to , then it is not a mixed precision process but rather a full one precision factorization, solve and refinement within the same main precision"
  },
  {
    "id": 26947,
    "content": "The iterative refinement process is stopped if ITER > ITERMAX or for all the RHS we have: RNRM LAMCH(‘Epsilon’) The value ITERMAX and BWDMAX are fixed to 50 and 1"
  },
  {
    "id": 26949,
    "content": "A CUSOLVER_STATUS_SUCCESS indicates that the function finished with success otherwise, it indicates if one of the API arguments is incorrect, or if the function did not finish with success"
  },
  {
    "id": 26953,
    "content": ", dsgesv and zcgesv ), we provide a large set of mixed precision functions that include half, bfloat and tensorfloat as a lower precision as well as same precision functions (e"
  },
  {
    "id": 26956,
    "content": "Tensor Float (TF32), introduced with NVIDIA Ampere Architecture GPUs, is the most robust tensor core accelerated compute mode for the iterative refinement solver"
  },
  {
    "id": 26957,
    "content": "It is able to solve the widest range of problems in HPC arising from different applications and provides up to 4X and 5X speedup for real and complex systems, respectively"
  },
  {
    "id": 26958,
    "content": "On Volta and Turing architecture GPUs, half precision tensor core acceleration is recommended In cases where the iterative refinement solver fails to converge to the desired accuracy (main precision, INOUT data precision), it is recommended to use main precision as internal lowest precision (i"
  },
  {
    "id": 26961,
    "content": "lddb host input Leading dimension of two-dimensional array used to store matrix of right hand sides B lddx host input Leading dimension of two-dimensional array used to store matrix of solution vectors X"
  },
  {
    "id": 26962,
    "content": "lwork_bytes host output Pointer to a variable where required size of temporary workspace in bytes will be stored"
  },
  {
    "id": 26963,
    "content": "If not - will contains the factorization of the matrix A in the main precision ( A = P * L * U , where P - permutation matrix defined by vector ipiv, L and U - lower and upper triangular matrices) dipiv device output Vector that defines permutation for the factorization - row i was interchanged with row ipiv[i] dB device input Set of right hand sides B of size n-by-nrhs dWorkspace device input"
  },
  {
    "id": 26965,
    "content": "niters host output If iter is 0 : iter is a number of iterations solver performed to reach convergence criteria dinfo device output Status of the IRS solver on the return"
  },
  {
    "id": 26966,
    "content": "The factorization has been completed, but the factor U is exactly singular, so the solution could not be computed"
  },
  {
    "id": 26967,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed, for example: ngesv() functions, but wrapped in a more generic and expert interface that gives user more control to parametrize the function as well as it provides more information on output"
  },
  {
    "id": 26968,
    "content": "cusolverDnIRSXgesv() allows additional control of the solver parameters such as setting: the main precision (Inputs/Outputs precision) of the solver the lowest precision to be used internally by the solver the refinement solver type the maximum allowed number of iterations in the refinement phase the tolerance of the refinement solver the fallback to main precision and more through the"
  },
  {
    "id": 26969,
    "content": "configuration parameters structure gesv_irs_params and its helper functions For more details about what configuration can be set and its meaning please refer to all the functions in the cuSolverDN Helper Function Section that start with cusolverDnIRSParamsxxxx() Moreover, cusolverDnIRSXgesv() provides additional information on the output such as the convergence history (e"
  },
  {
    "id": 26972,
    "content": "For more details about what information can be retrieved and its meaning please refer to all the functions in the cuSolverDN Helper Function Section that start with cusolverDnIRSInfosxxxx() The function returns value describes the results of the solving process"
  },
  {
    "id": 26973,
    "content": "A CUSOLVER_STATUS_SUCCESS indicates that the function finished with success otherwise, it indicates if one of the API arguments is incorrect, or if the configurations of params/infos structure is incorrect or if the function did not finish with success"
  },
  {
    "id": 26975,
    "content": "User should provide the required workspace allocated on device for the cusolverDnIRSXgesv() function The amount of bytes required for the function can be queried by calling the respective function cusolverDnIRSXgesv_bufferSize() Note that, if the user would like a particular configuration to be set via the params structure, it should be set before the call to cusolverDnIRSXgesv_bufferSize() to"
  },
  {
    "id": 26977,
    "content": "In cases where the iterative refinement solver fails to converge to the desired accuracy (main precision, INOUT data precision), it is recommended to use main precision as internal lowest precision The following table provides all possible combinations values for the lowest precision corresponding to the Inputs/Outputs data type Note that if the lowest precision matches the Inputs/Outputs"
  },
  {
    "id": 26978,
    "content": "datatype, then the main precision factorization will be used Supported Inputs/Outputs data type and lower precision for the IRS solver  Inputs/Outputs Data Type (e"
  },
  {
    "id": 26980,
    "content": ", main precision) Supported values for the lowest precision CUSOLVER_C_64F CUSOLVER_C_64F, CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_C_32F CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_R_64F CUSOLVER_R_64F, CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 CUSOLVER_R_32F CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF,"
  },
  {
    "id": 26981,
    "content": "CUSOLVER_R_TF32 The cusolverDnIRSXgesv_bufferSize() function returns the required workspace buffer size in bytes for the corresponding cusolverDnXgesv() call with the given gesv_irs_params configuration"
  },
  {
    "id": 26982,
    "content": "cusolverStatus_t cusolverDnIRSXgesv_bufferSize ( cusolverDnHandle_t handle , cusolverDnIRSParams_t gesv_irs_params , cusolver_int_t n , cusolver_int_t nrhs , size_t * lwork_bytes ); Table 5 Parameters of cusolverDnIRSXgesv_bufferSize() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDn library context params host input Xgesv configuration parameters n host"
  },
  {
    "id": 26984,
    "content": "Note that nrhs is limited to 1 if the selected IRS refinement solver is CUSOLVER_IRS_REFINE_GMRES, CUSOLVER_IRS_REFINE_GMRES_GMRES, CUSOLVER_IRS_REFINE_CLASSICAL_GMRES"
  },
  {
    "id": 26985,
    "content": "lwork_bytes host out Pointer to a variable, where the required size in bytes, of the workspace will be stored after a call to cusolverDnIRSXgesv_bufferSize"
  },
  {
    "id": 26986,
    "content": "cusolverStatus_t cusolverDnIRSXgesv ( cusolverDnHandle_t handle , cusolverDnIRSParams_t gesv_irs_params , cusolverDnIRSInfos_t gesv_irs_infos , int n , int nrhs , void * dA , int ldda , void * dB , int lddb , void * dX , int lddx , void * dWorkspace , size_t lwork_bytes , int * dinfo ); Table 6"
  },
  {
    "id": 26987,
    "content": "Parameters of cusolverDnIRSXgesv() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDn library context gesv_irs_params host input Configuration parameters structure, can serve one or more calls to any IRS solver gesv_irs_infos host in/out Info structure, where information about a particular solve will be stored Thus different calls requires different"
  },
  {
    "id": 26989,
    "content": "Note that, nrhs is limited to 1 if the selected IRS refinement solver is CUSOLVER_IRS_REFINE_GMRES, CUSOLVER_IRS_REFINE_GMRES_GMRES, CUSOLVER_IRS_REFINE_CLASSICAL_GMRES On return - will contain the factorization of the matrix A in the main precision ( A = P * L * U , where P - permutation matrix defined by vector ipiv, L and U - lower and upper triangular matrices) if the iterative refinement"
  },
  {
    "id": 26990,
    "content": "solver was set to CUSOLVER_IRS_REFINE_NONE and the lowest precision is equal to the main precision (Inputs/Outputs datatype), or if the iterative refinement solver did not converge and the fallback to main precision was enabled (fallback enabled is the default setting); unchanged otherwise"
  },
  {
    "id": 26992,
    "content": "Should be at least what was returned by cusolverDnIRSXgesv_bufferSize() function niters host output If iter is 0 : iter is a number of iterations solver performed to reach convergence criteria dinfo device output Status of the IRS solver on the return"
  },
  {
    "id": 26993,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed, for example: n1, and refinement solver was set to CUSOLVER_IRS_REFINE_GMRES"
  },
  {
    "id": 26995,
    "content": "CUSOLVER_STATUS_ALLOC_FAILED CPU memory allocation failed, most likely during the allocation of the residual array that store the residual norms"
  },
  {
    "id": 27001,
    "content": "cusolverStatus_t cusolverDnSgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , int * Lwork );"
  },
  {
    "id": 27002,
    "content": "cusolverStatus_t cusolverDnZgeqrf_bufferSize ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSgeqrf ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , float * TAU , float * Workspace , int Lwork , int * devInfo ); cusolverStatus_t"
  },
  {
    "id": 27003,
    "content": "cusolverDnDgeqrf ( cusolverDnHandle_t handle , int m , int n , double * A , int lda , double * TAU , double * Workspace , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCgeqrf ( cusolverDnHandle_t handle , int m , int n , cuComplex * A , int lda , cuComplex * TAU , cuComplex * Workspace , int Lwork , int *"
  },
  {
    "id": 27004,
    "content": "devInfo ); cusolverStatus_t cusolverDnZgeqrf ( cusolverDnHandle_t handle , int m , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * TAU , cuDoubleComplex * Workspace , int Lwork , int * devInfo ); This function computes the QR factorization of a m×n matrix \\(A = Q*R\\) where A is an m×n matrix, Q is an m×n matrix, and R is a n×n upper triangular matrix"
  },
  {
    "id": 27006,
    "content": "The matrix Q is not formed explicitly, instead, a sequence of householder vectors are stored in lower triangular part of A The leading nonzero element of householder vector is assumed to be 1 such that output parameter TAU contains the scaling factor τ If v is original householder vector, q is the new householder vector corresponding to τ , satisfying the following relation \\(I - 2*v*v^{H} = I -"
  },
  {
    "id": 27007,
    "content": "\\tau*q*q^{H}\\) If output parameter devInfo = -i (less than zero), the i-th parameter is wrong (not counting handle)"
  },
  {
    "id": 27008,
    "content": "API of geqrf Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27009,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngels()  These functions compute the solution of a system of linear equations with one or multiple right hand sides using mixed precision iterative refinement techniques based on the QR factorization Xgels These functions are similar in term of functionalities to the full precision LAPACK QR (least squares) solver (Xgels, where X"
  },
  {
    "id": 27010,
    "content": "denotes Z,C,D,S) but it uses lower precision internally in order to provide faster time to solution, from here comes the name mixed precision Mixed precision iterative refinement techniques means that the solver compute an QR factorization in lower precision and then iteratively refine the solution to achieve the accuracy of the Inputs/Outputs datatype precision"
  },
  {
    "id": 27012,
    "content": "gels() functions are designated by two floating point precisions The corresponds to the main precision (e"
  },
  {
    "id": 27014,
    "content": ", Inputs/Outputs datatype precision) and the represent the internal lower precision at which the factorization will be carried on cusolvergels() first attempts to factorize the matrix in lower precision and use this factorization within an iterative refinement procedure to obtain a solution with same normwise backward error as the main precision If the approach fails to converge, then the method"
  },
  {
    "id": 27015,
    "content": "fallback to the main precision factorization and solve (Xgels) such a way that there is always a good solution at the output of these functions"
  },
  {
    "id": 27016,
    "content": "The iterative refinement process is stopped if: ITER > ITERMAX or for all the RHS we have: RNRM LAMCH('Epsilon') The values ITERMAX and BWDMAX are fixed to 50 and 1"
  },
  {
    "id": 27019,
    "content": "We provide a large set of mixed precision functions that include half, bfloat and tensorfloat as a lower precision as well as same precision functions (e"
  },
  {
    "id": 27022,
    "content": "The following table specifies which precisions will be used for which interface function: Tensor Float (TF32), introduced with NVIDIA Ampere Architecture GPUs, is the most robust tensor core accelerated compute mode for the iterative refinement solver"
  },
  {
    "id": 27023,
    "content": "In cases where the iterative refinement solver fails to converge to the desired accuracy (main precision, INOUT data precision), it is recommended to use main precision as internal lowest precision (i"
  },
  {
    "id": 27026,
    "content": "Parameters of cusolverDngels_bufferSize() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDN library context Parameters of cusolverDngels() functions  Parameter Memory In/out Meaning handle host input Handle to the cusolverDN library context"
  },
  {
    "id": 27027,
    "content": "Should be at least what was returned by cusolverDngels_bufferSize() function niters host output If iter is 0 : iter is a number of iterations solver performed to reach convergence criteria dinfo device output Status of the IRS solver on the return"
  },
  {
    "id": 27028,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed, for example: ngels() functions, but wrapped in a more generic and expert interface that gives user more control to parametrize the function as well as it provides more information on output"
  },
  {
    "id": 27029,
    "content": "cusolverDnIRSXgels() allows additional control of the solver parameters such as setting: the main precision (Inputs/Outputs precision) of the solver, the lowest precision to be used internally by the solver, the refinement solver type the maximum allowed number of iterations in the refinement phase the tolerance of the refinement solver the fallback to main precision and others through the"
  },
  {
    "id": 27030,
    "content": "configuration parameters structure gels_irs_params and its helper functions Moreover, cusolverDnIRSXgels() provides additional information on the output such as the convergence history (e"
  },
  {
    "id": 27033,
    "content": "For more details about what information can be retrieved and its meaning please refer to all the functions in the cuSolverDN Helper Function Section that start with cusolverDnIRSInfosxxxx()"
  },
  {
    "id": 27034,
    "content": "Users should provide the required workspace allocated on device for the cusolverDnIRSXgels() function The amount of bytes required for the function can be queried by calling the respective function cusolverDnIRSXgels_bufferSize() Note that, if the user would like a particular configuration to be set via the params structure, it should be set before the call to cusolverDnIRSXgels_bufferSize() to"
  },
  {
    "id": 27036,
    "content": "Note that if the lowest precision matches the Inputs/Outputs datatype, then main precision factorization will be used Tensor Float (TF32), introduced with NVIDIA Ampere Architecture GPUs, is the most robust tensor core accelerated compute mode for the iterative refinement solver Supported Inputs/Outputs data type and lower precision for the IRS solver :class: table-no-stripes  Inputs/Outputs"
  },
  {
    "id": 27039,
    "content": ", main precision) Supported values for the lowest precision CUSOLVER_C_64F CUSOLVER_C_64F, CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_C_32F CUSOLVER_C_32F, CUSOLVER_C_16F, CUSOLVER_C_16BF, CUSOLVER_C_TF32 CUSOLVER_R_64F CUSOLVER_R_64F, CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF, CUSOLVER_R_TF32 CUSOLVER_R_32F CUSOLVER_R_32F, CUSOLVER_R_16F, CUSOLVER_R_16BF,"
  },
  {
    "id": 27040,
    "content": "CUSOLVER_R_TF32 The cusolverDnIRSXgels_bufferSize() function return the required workspace buffer size in bytes for the corresponding cusolverDnXgels() call with given gels_irs_params configuration"
  },
  {
    "id": 27041,
    "content": "cusolverStatus_t cusolverDnIRSXgels_bufferSize ( cusolverDnHandle_t handle , cusolverDnIRSParams_t gels_irs_params , cusolver_int_t m , cusolver_int_t n , cusolver_int_t nrhs , size_t * lwork_bytes ); Parameters of cusolverDnIRSXgels_bufferSize() functions Parameter Memory In/out Meaning handle host input Handle to the cusolverDn library context"
  },
  {
    "id": 27049,
    "content": "cusolverStatus_t cusolverDnSormqr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasOperation_t trans , int m , int n , int k , const float * A , int lda , const float * tau , float * C , int ldc , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDormqr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasOperation_t trans , int m , int n , int k , const"
  },
  {
    "id": 27050,
    "content": "double * A , int lda , const double * tau , double * C , int ldc , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively"
  },
  {
    "id": 27051,
    "content": "Q = H(1) H(2) … H(k) Q is of order m if side = CUBLAS_SIDE_LEFT and of order n if side = CUBLAS_SIDE_RIGHT"
  },
  {
    "id": 27052,
    "content": "The input parameter lwork is size of the working space, and it is returned by geqrf_bufferSize() or ormqr_bufferSize()"
  },
  {
    "id": 27053,
    "content": "The user can combine geqrf , ormqr and trsm to complete a linear solver or a least-square solver API of ormqr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDn library context"
  },
  {
    "id": 27057,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,norgqr()  These helper functions calculate the size of work buffers needed"
  },
  {
    "id": 27058,
    "content": "cusolverStatus_t cusolverDnSorgqr_bufferSize ( cusolverDnHandle_t handle , int m , int n , int k , const float * A , int lda , const float * tau , int * lwork ); cusolverStatus_t cusolverDnDorgqr_bufferSize ( cusolverDnHandle_t handle , int m , int n , int k , const double * A , int lda , const double * tau , int * lwork ); cusolverStatus_t cusolverDnCungqr_bufferSize ( cusolverDnHandle_t handle"
  },
  {
    "id": 27059,
    "content": ", int m , int n , int k , const cuComplex * A , int lda , const cuComplex * tau , int * lwork ); cusolverStatus_t cusolverDnZungqr_bufferSize ( cusolverDnHandle_t handle , int m , int n , int k , const cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , int * lwork ); The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSorgqr ("
  },
  {
    "id": 27060,
    "content": "cusolverDnHandle_t handle , int m , int n , int k , float * A , int lda , const float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDorgqr ( cusolverDnHandle_t handle , int m , int n , int k , double * A , int lda , const double * tau , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively"
  },
  {
    "id": 27061,
    "content": "cusolverStatus_t cusolverDnCungqr ( cusolverDnHandle_t handle , int m , int n , int k , cuComplex * A , int lda , const cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZungqr ( cusolverDnHandle_t handle , int m , int n , int k , cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This"
  },
  {
    "id": 27063,
    "content": "}*{H(k)}\\) where Q is a unitary matrix formed by a sequence of elementary reflection vectors stored in A"
  },
  {
    "id": 27065,
    "content": "API of orgqr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27068,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n,km , k>n or ldasytrf()  These helper functions calculate the size of the needed buffers"
  },
  {
    "id": 27069,
    "content": "cusolverStatus_t cusolverDnSsytrf_bufferSize ( cusolverDnHandle_t handle , int n , float * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnDsytrf_bufferSize ( cusolverDnHandle_t handle , int n , double * A , int lda , int * Lwork ); cusolverStatus_t cusolverDnCsytrf_bufferSize ( cusolverDnHandle_t handle , int n , cuComplex * A , int lda , int * Lwork ); cusolverStatus_t"
  },
  {
    "id": 27070,
    "content": "cusolverDnZsytrf_bufferSize ( cusolverDnHandle_t handle , int n , cuDoubleComplex * A , int lda , int * Lwork ); The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , int * ipiv , float * work , int lwork , int * devInfo ); cusolverStatus_t"
  },
  {
    "id": 27071,
    "content": "cusolverDnDsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , int * ipiv , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , int * ipiv , cuComplex * work ,"
  },
  {
    "id": 27072,
    "content": "int lwork , int * devInfo ); cusolverStatus_t cusolverDnZsytrf ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , int * ipiv , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes the Bunch-Kaufman factorization of a n×n symmetric indefinite matrix A is a n×n symmetric matrix, only lower or upper part is meaningful"
  },
  {
    "id": 27073,
    "content": "If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced by lower triangular factor L and block diagonal matrix D \\(P*A*P^{T} = L*D*L^{T}\\) If input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by upper triangular factor U and block diagonal matrix D"
  },
  {
    "id": 27074,
    "content": "\\(P*A*P^{T} = U*D*U^{T}\\) The user has to provide working space which is pointed by input parameter work The input parameter lwork is size of the working space, and it is returned by sytrf_bufferSize()"
  },
  {
    "id": 27075,
    "content": "If devIpiv(i) = k > 0 , D(i,i) is 1x1 block, and i-th row/column of A is interchanged with k-th row/column of A If uplo is CUBLAS_FILL_MODE_UPPER and devIpiv(i-1) = devIpiv(i) = -m array of dimension lda * n with lda is not less than max(1,n)"
  },
  {
    "id": 27076,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( npotrfBatched()  The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSpotrfBatched ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * Aarray [], int lda , int * infoArray , int batchSize ); cusolverStatus_t cusolverDnDpotrfBatched ( cusolverDnHandle_t handle ,"
  },
  {
    "id": 27077,
    "content": "cublasFillMode_t uplo , int n , double * Aarray [], int lda , int * infoArray , int batchSize ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCpotrfBatched ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * Aarray [], int lda , int * infoArray , int batchSize ); cusolverStatus_t cusolverDnZpotrfBatched ("
  },
  {
    "id": 27078,
    "content": "cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * Aarray [], int lda , int * infoArray , int batchSize ); This function computes the Cholesky factorization of a sequence of Hermitian positive-definite matrices Each Aarray[i] for i=0,1,"
  },
  {
    "id": 27079,
    "content": ", batchSize-1 is a n×n Hermitian matrix, only lower or upper part is meaningful If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced by lower triangular Cholesky factor L"
  },
  {
    "id": 27080,
    "content": "The output parameter infoArray would indicate smallest leading minor of A which is not positive definite If potrfBatched returns CUSOLVER_STATUS_INVALID_VALUE , infoArray[0] = -i (less than zero), meaning that the i-th parameter is wrong (not counting handle) If potrfBatched returns CUSOLVER_STATUS_SUCCESS but infoArray[i] = k is positive, then i-th matrix is not positive definite and the"
  },
  {
    "id": 27081,
    "content": "Cholesky factorization failed at row k For example, if uplo is CUBLAS_FILL_MODE_UPPER , upper triangle of A contains Cholesky factor U and lower triangle of A is destroyed after potrfBatched API of potrfBatched  Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context uplo host input Indicates if lower or upper part is stored; the other part is used as a"
  },
  {
    "id": 27083,
    "content": "Aarray device in/out Array of pointers to array of dimension lda * n with lda is not less than max(1,n) lda host input Leading dimension of two-dimensional array used to store each matrix Aarray[i]"
  },
  {
    "id": 27084,
    "content": "if potrfBatched returns CUSOLVER_STATUS_INVALID_VALUE , infoArray[0] = -i (less than zero) means the i-th parameter is wrong (not counting handle) if potrfBatched returns CUSOLVER_STATUS_SUCCESS , infoArray[i] = 0 means the Cholesky factorization of i-th matrix is successful, and infoArray[i] = k means the leading submatrix of order k of i-th matrix is not positive definite"
  },
  {
    "id": 27085,
    "content": "For example, if uplo is CUBLAS_FILL_MODE_UPPER , upper triangle of A contains Cholesky factor U and lower triangle of A is destroyed after potrsBatched API of potrsBatched Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27087,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngebrd()  These helper functions calculate the size of work buffers needed"
  },
  {
    "id": 27088,
    "content": "cusolverStatus_t cusolverDnSgebrd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * Lwork ); cusolverStatus_t cusolverDnDgebrd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * Lwork ); cusolverStatus_t cusolverDnCgebrd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * Lwork ); cusolverStatus_t cusolverDnZgebrd_bufferSize ( cusolverDnHandle_t handle , int m"
  },
  {
    "id": 27089,
    "content": ", int n , int * Lwork ); The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSgebrd ( cusolverDnHandle_t handle , int m , int n , float * A , int lda , float * D , float * E , float * TAUQ , float * TAUP , float * Work , int Lwork , int * devInfo ); cusolverStatus_t cusolverDnDgebrd ( cusolverDnHandle_t handle , int m , int n , double * A ,"
  },
  {
    "id": 27090,
    "content": "int lda , double * D , double * E , double * TAUQ , double * TAUP , double * Work , int Lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively"
  },
  {
    "id": 27091,
    "content": "API of gebrd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27094,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,norgbr()  These helper functions calculate the size of work buffers needed"
  },
  {
    "id": 27095,
    "content": "cusolverStatus_t cusolverDnSorgbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n , int k , float * A , int lda , const float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDorgbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n , int k , double * A , int lda , const double * tau , double * work , int lwork , int * devInfo );"
  },
  {
    "id": 27096,
    "content": "The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCungbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n , int k , cuComplex * A , int lda , const cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZungbr ( cusolverDnHandle_t handle , cublasSideMode_t side , int m , int n ,"
  },
  {
    "id": 27097,
    "content": "int k , cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This function generates one of the unitary matrices Q or P**H determined by gebrd when reducing a matrix A to bidiagonal form: \\(Q^{H}*A*P = B\\) Q and P**H are defined as products of elementary reflectors H(i) or G(i) respectively"
  },
  {
    "id": 27099,
    "content": "API of orgbr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27100,
    "content": "k host input If side = CUBLAS_SIDE_LEFT , the number of columns in the original m-by-k matrix reduced by gebrd if side = CUBLAS_SIDE_RIGHT , the number of rows in the original k-by-n matrix reduced by gebrd A device in/out array of dimension lda * n On entry, the vectors which define the elementary reflectors, as returned by gebrd lda >= max(1,m); tau device input array of dimension min(m,k) if"
  },
  {
    "id": 27101,
    "content": "side is CUBLAS_SIDE_LEFT ; of dimension min(n,k) if side is CUBLAS_SIDE_RIGHT ; tau(i) must contain the scalar factor of the elementary reflector H(i) or G(i), which determines Q or P**T, as returned by gebrd in its array argument TAUQ or TAUP"
  },
  {
    "id": 27102,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nsytrd()  These helper functions calculate the size of work buffers needed"
  },
  {
    "id": 27103,
    "content": "cusolverStatus_t cusolverDnSsytrd ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , float * d , float * e , float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsytrd ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , double * d , double * e , double * tau , double * work , int lwork , int *"
  },
  {
    "id": 27104,
    "content": "devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnChetrd ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * d , float * e , cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t CUDENSEAPI cusolverDnZhetrd ( cusolverDnHandle_t handle , cublasFillMode_t"
  },
  {
    "id": 27105,
    "content": "uplo , int n , cuDoubleComplex * A , int lda , double * d , double * e , cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This function reduces a general symmetric (Hermitian) n×n matrix A to real symmetric tridiagonal form T by an orthogonal transformation: \\(Q^{H}*A*Q = T\\) As an output, A contains T and householder reflection vectors If uplo = CUBLAS_FILL_MODE_UPPER"
  },
  {
    "id": 27106,
    "content": ", the diagonal and first superdiagonal of A are overwritten by the corresponding elements of the tridiagonal matrix T , and the elements above the first superdiagonal, with the array tau , represent the orthogonal matrix Q as a product of elementary reflectors; If uplo = CUBLAS_FILL_MODE_LOWER , the diagonal and first subdiagonal of A are overwritten by the corresponding elements of the"
  },
  {
    "id": 27107,
    "content": "tridiagonal matrix T , and the elements below the first subdiagonal, with the array tau , represent the orthogonal matrix Q as a product of elementary reflectors"
  },
  {
    "id": 27109,
    "content": "API of sytrd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27110,
    "content": "If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of A contains the upper triangular part of the matrix A , and the strictly lower triangular part of A is not referenced If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of A contains the lower triangular part of the matrix A , and the strictly upper triangular part of A is not referenced The"
  },
  {
    "id": 27111,
    "content": "off-diagonal elements of the tridiagonal matrix T : if uplo = CUBLAS_FILL_MODE_UPPER , E(i) = A(i,i+1)"
  },
  {
    "id": 27112,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( normtr()  These helper functions calculate the size of work buffers needed"
  },
  {
    "id": 27113,
    "content": "cusolverStatus_t cusolverDnSormtr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , int m , int n , float * A , int lda , float * tau , float * C , int ldc , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDormtr ( cusolverDnHandle_t handle , cublasSideMode_t side , cublasFillMode_t uplo , cublasOperation_t trans , int m"
  },
  {
    "id": 27114,
    "content": ", int n , double * A , int lda , double * tau , double * C , int ldc , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively"
  },
  {
    "id": 27115,
    "content": "The operation on Q is defined by \\(\\text{op}(Q) = \\left\\{ \\begin{matrix} Q & {\\text{if~}\\textsf{transa\\ ==\\ CUBLAS\\_OP\\_N}} \\\\ Q^{T} & {\\text{if~}\\textsf{transa\\ ==\\ CUBLAS\\_OP\\_T}} \\\\ Q^{H} & {\\text{if~}\\textsf{transa\\ ==\\ CUBLAS\\_OP\\_C}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 27116,
    "content": "\\) The user has to provide working space which is pointed by input parameter work The input parameter lwork is size of the working space, and it is returned by ormtr_bufferSize()"
  },
  {
    "id": 27117,
    "content": "API of ormtr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27118,
    "content": "side host input side = CUBLAS_SIDE_LEFT , apply Q or Q**T from the Left; side = CUBLAS_SIDE_RIGHT , apply Q or Q**T from the Right"
  },
  {
    "id": 27119,
    "content": "uplo host input uplo = CUBLAS_FILL_MODE_LOWER : Lower triangle of A contains elementary reflectors from sytrd uplo = CUBLAS_FILL_MODE_UPPER : Upper triangle of A contains elementary reflectors from sytrd"
  },
  {
    "id": 27120,
    "content": "A device in/out array of dimension lda * m if side = CUBLAS_SIDE_LEFT ; lda * n if side = CUBLAS_SIDE_RIGHT tau device output array of dimension (m-1) if side is CUBLAS_SIDE_LEFT ; of dimension (n-1) if side is CUBLAS_SIDE_RIGHT ; The vector tau is from sytrd , so tau(i) is the scalar of i-th elementary reflection vector"
  },
  {
    "id": 27121,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,norgtr()  These helper functions calculate the size of work buffers needed"
  },
  {
    "id": 27122,
    "content": "cusolverStatus_t cusolverDnSorgtr_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const float * A , int lda , const float * tau , int * lwork ); cusolverStatus_t cusolverDnDorgtr_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const double * A , int lda , const double * tau , int * lwork ); cusolverStatus_t cusolverDnCungtr_bufferSize ("
  },
  {
    "id": 27123,
    "content": "cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const cuComplex * A , int lda , const cuComplex * tau , int * lwork ); cusolverStatus_t cusolverDnZungtr_bufferSize ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , const cuDoubleComplex * A , int lda , const cuDoubleComplex * tau , int * lwork ); The S and D data types are real valued single and double precision,"
  },
  {
    "id": 27124,
    "content": "respectively cusolverStatus_t cusolverDnSorgtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , float * A , int lda , const float * tau , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDorgtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , double * A , int lda , const double * tau , double * work , int lwork , int * devInfo ); The C and Z data"
  },
  {
    "id": 27125,
    "content": "types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCungtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuComplex * A , int lda , const cuComplex * tau , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZungtr ( cusolverDnHandle_t handle , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , const"
  },
  {
    "id": 27126,
    "content": "cuDoubleComplex * tau , cuDoubleComplex * work , int lwork , int * devInfo ); This function generates a unitary matrix Q which is defined as the product of n-1 elementary reflectors of order n, as returned by sytrd : The user has to provide working space which is pointed by input parameter work The input parameter lwork is size of the working space, and it is returned by orgtr_bufferSize()"
  },
  {
    "id": 27127,
    "content": "API of orgtr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27128,
    "content": "A device in/out array of dimension lda * n On entry, matrix A from sytrd contains the elementary reflectors tau device input array of dimension (n-1) tau(i) is the scalar of i-th elementary reflection vector"
  },
  {
    "id": 27129,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( ngesvd()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27130,
    "content": "cusolverStatus_t cusolverDnSgesvd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * lwork ); cusolverStatus_t cusolverDnDgesvd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * lwork ); cusolverStatus_t cusolverDnCgesvd_bufferSize ( cusolverDnHandle_t handle , int m , int n , int * lwork ); cusolverStatus_t cusolverDnZgesvd_bufferSize ( cusolverDnHandle_t handle , int m"
  },
  {
    "id": 27131,
    "content": ", int n , int * lwork ); The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverDnSgesvd ( cusolverDnHandle_t handle , signed char jobu , signed char jobvt , int m , int n , float * A , int lda , float * S , float * U , int ldu , float * VT , int ldvt , float * work , int lwork , float * rwork , int * devInfo ); cusolverStatus_t cusolverDnDgesvd ("
  },
  {
    "id": 27132,
    "content": "cusolverDnHandle_t handle , signed char jobu , signed char jobvt , int m , int n , double * A , int lda , double * S , double * U , int ldu , double * VT , int ldvt , double * work , int lwork , double * rwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCgesvd ( cusolverDnHandle_t handle , signed char jobu ,"
  },
  {
    "id": 27133,
    "content": "signed char jobvt , int m , int n , cuComplex * A , int lda , float * S , cuComplex * U , int ldu , cuComplex * VT , int ldvt , cuComplex * work , int lwork , float * rwork , int * devInfo ); cusolverStatus_t cusolverDnZgesvd ( cusolverDnHandle_t handle , signed char jobu , signed char jobvt , int m , int n , cuDoubleComplex * A , int lda , double * S , cuDoubleComplex * U , int ldu ,"
  },
  {
    "id": 27134,
    "content": "cuDoubleComplex * VT , int ldvt , cuDoubleComplex * work , int lwork , double * rwork , int * devInfo ); This function computes the singular value decomposition (SVD) of a m×n matrix A and corresponding the left and/or right singular vectors"
  },
  {
    "id": 27135,
    "content": "The SVD is written \\(A = U*\\Sigma*V^{H}\\) where Σ is an m×n matrix which is zero except for its min(m,n) diagonal elements, U is an m×m unitary matrix, and V is an n×n unitary matrix"
  },
  {
    "id": 27136,
    "content": "The diagonal elements of Σ are the singular values of A ; they are real and non-negative, and are returned in descending order"
  },
  {
    "id": 27138,
    "content": "if bdsqr did not converge, devInfo specifies how many superdiagonals of an intermediate bidiagonal form did not converge to zero If devInfo >0 and rwork is not NULL, rwork contains the unconverged superdiagonal elements of an upper bidiagonal matrix This is slightly different from LAPACK which puts unconverged superdiagonal elements in work if type is real ; in rwork if type is complex"
  },
  {
    "id": 27139,
    "content": "API of gesvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27140,
    "content": "jobu host input Specifies options for computing all or part of the matrix U : = ‘A’: all m columns of U are returned in array U: = ‘S’: the first min(m,n) columns of U (the left singular vectors) are returned in the array U; = ‘O’: the first min(m,n) columns of U (the left singular vectors) are overwritten on the array A; = ‘N’: no columns of U (no left singular vectors) are computed jobvt host"
  },
  {
    "id": 27141,
    "content": "input Specifies options for computing all or part of the matrix V**T: = ‘A’: all N rows of V**T are returned in the array VT; = ‘S’: the first min(m,n) rows of V**T (the right singular vectors) are returned in the array VT; = ‘O’: the first min(m,n) rows of V**T (the right singular vectors) are overwritten on the array A; = ‘N’: no rows of V**T (no right singular vectors) are computed"
  },
  {
    "id": 27142,
    "content": "It contains the unconverged superdiagonal elements of an upper bidiagonal matrix if devInfo > 0 If devInfo > 0 , devInfo indicates how many superdiagonals of an intermediate bidiagonal form did not converge to zero"
  },
  {
    "id": 27143,
    "content": "List of input arguments for cusolverDnGesvd_bufferSize and cusolverDnGesvd : API of cusolverDnGesvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context workspaceInBytes host input Size in bytes of pBuffer , returned by cusolverDnGesvd_bufferSize"
  },
  {
    "id": 27144,
    "content": "if info > 0 , info indicates how many superdiagonals of an intermediate bidiagonal form did not converge to zero"
  },
  {
    "id": 27145,
    "content": "The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeS is data type of the vector S and dataTypeU is data type of the matrix U , dataTypeVT is data type of the matrix VT , computeType is compute type of the operation Valid combination of data type and compute type DataTypeA DataTypeS DataTypeU DataTypeVT ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F"
  },
  {
    "id": 27146,
    "content": "CUDA_R_32F CUDA_R_32F SGESVD CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DGESVD CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CGESVD CUDA_C_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZGESVD Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27147,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngesvdj()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27148,
    "content": "cusolverStatus_t cusolverDnSgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , float * A , int lda , float * S , float * U , int ldu , float * V , int ldv , float * work , int lwork , int * info , gesvdjInfo_t params ); cusolverStatus_t cusolverDnDgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , double * A , int lda ,"
  },
  {
    "id": 27149,
    "content": "double * S , double * U , int ldu , double * V , int ldv , double * work , int lwork , int * info , gesvdjInfo_t params ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , cuComplex * A , int lda , float * S , cuComplex * U , int ldu , cuComplex *"
  },
  {
    "id": 27150,
    "content": "V , int ldv , cuComplex * work , int lwork , int * info , gesvdjInfo_t params ); cusolverStatus_t cusolverDnZgesvdj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int econ , int m , int n , cuDoubleComplex * A , int lda , double * S , cuDoubleComplex * U , int ldu , cuDoubleComplex * V , int ldv , cuDoubleComplex * work , int lwork , int * info , gesvdjInfo_t params ); This function"
  },
  {
    "id": 27151,
    "content": "computes the singular value decomposition (SVD) of a m×n matrix A and corresponding the left and/or right singular vectors"
  },
  {
    "id": 27152,
    "content": "The SVD is written: \\(A = U*\\Sigma*V^{H}\\) where Σ is an m×n matrix which is zero except for its min(m,n) diagonal elements, U is an m×m unitary matrix, and V is an n×n unitary matrix"
  },
  {
    "id": 27154,
    "content": "gesvdj iteratively generates a sequence of unitary matrices to transform matrix A to the following form \\(U^{H}*A*V = S + E\\) where S is diagonal and diagonal of E is zero"
  },
  {
    "id": 27155,
    "content": "In practice, Jacobi method stops if \\({||E||}_{F}\\leq\\operatorname{eps}*{||A||}_{F}\\) where eps is given tolerance"
  },
  {
    "id": 27156,
    "content": "The default value is machine accuracy but The user can use function cusolverDnXgesvdjSetTolerance to set a priori tolerance"
  },
  {
    "id": 27157,
    "content": "The second parameter is maximum number of sweeps which controls number of iterations of Jacobi method"
  },
  {
    "id": 27158,
    "content": "The default value is 100 but the user can use function cusolverDnXgesvdjSetMaxSweeps to set a proper bound"
  },
  {
    "id": 27160,
    "content": "The input parameter lwork is the size of the working space, and it is returned by gesvdj_bufferSize() If output parameter info = -i (less than zero), the i-th parameter is wrong (not counting handle) API of gesvdj Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27161,
    "content": "jobz host input Specifies options to either compute singular value only or singular vectors as well: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute singular values only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute singular values and singular vectors"
  },
  {
    "id": 27163,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngesvdjBatched()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27164,
    "content": "cusolverStatus_t cusolverDnSgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , float * A , int lda , float * S , float * U , int ldu , float * V , int ldv , float * work , int lwork , int * info , gesvdjInfo_t params , int batchSize ); cusolverStatus_t cusolverDnDgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , double * A , int"
  },
  {
    "id": 27165,
    "content": "lda , double * S , double * U , int ldu , double * V , int ldv , double * work , int lwork , int * info , gesvdjInfo_t params , int batchSize ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , cuComplex * A , int lda , float * S , cuComplex * U , int"
  },
  {
    "id": 27166,
    "content": "ldu , cuComplex * V , int ldv , cuComplex * work , int lwork , int * info , gesvdjInfo_t params , int batchSize ); cusolverStatus_t cusolverDnZgesvdjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , int m , int n , cuDoubleComplex * A , int lda , double * S , cuDoubleComplex * U , int ldu , cuDoubleComplex * V , int ldv , cuDoubleComplex * work , int lwork , int * info , gesvdjInfo_t"
  },
  {
    "id": 27167,
    "content": "params , int batchSize ); This function computes singular values and singular vectors of a sequence of general m×n matrices \\(A_{j} = U_{j}*\\Sigma_{j}*V_{j}^{H}\\) where \\(\\Sigma_{j}\\) is a real m×n diagonal matrix which is zero except for its min(m,n) diagonal elements \\(U_{j}\\) (left singular vectors) is a m×m unitary matrix and \\(V_{j}\\) (right singular vectors) is a n×n unitary matrix The"
  },
  {
    "id": 27168,
    "content": "diagonal elements of \\(\\Sigma_{j}\\) are the singular values of \\(A_{j}\\) in either descending order or non-sorting order"
  },
  {
    "id": 27169,
    "content": "It requires that all matrices are of the same size m,n no greater than 32 and are packed in contiguous way, \\(A = \\begin{pmatrix} {A0} & {A1} & \\cdots \\\\ \\end{pmatrix}\\) Each matrix is column-major with leading dimension lda , so the formula for random access is \\(A_{k}\\operatorname{(i,j)} = {A\\lbrack\\ i\\ +\\ lda*j\\ +\\ lda*n*k brack}\\) The parameter S also contains singular values of each matrix"
  },
  {
    "id": 27170,
    "content": "in contiguous way, \\(S = \\begin{pmatrix} {S0} & {S1} & \\cdots \\\\ \\end{pmatrix}\\) The formula for random access of S is \\(S_{k}\\operatorname{(j)} = {S\\lbrack\\ j\\ +\\ min(m,n)*k brack}\\)"
  },
  {
    "id": 27171,
    "content": "Except for tolerance and maximum sweeps, gesvdjBatched can either sort the singular values in descending order (default) or chose as-is (without sorting) by the function cusolverDnXgesvdjSetSortEig If the user packs several tiny matrices into diagonal blocks of one matrix, non-sorting option can separate singular values of those tiny matrices gesvdjBatched cannot report residual and executed"
  },
  {
    "id": 27173,
    "content": "The input parameter lwork is the size of the working space, and it is returned by gesvdjBatched_bufferSize()"
  },
  {
    "id": 27174,
    "content": "If the function returns CUSOLVER_STATUS_INVALID_VALUE , the first element info[0] = -i (less than zero) indicates i-th parameter is wrong (not counting handle) Otherwise, if info[i] = min(m,n)+1 , gesvdjBatched does not converge on i-th matrix under given tolerance and maximum sweeps"
  },
  {
    "id": 27175,
    "content": "API of gesvdjBatched Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27176,
    "content": "A device in/out array of dimension lda * n * batchSize with lda is not less than max(1,n) If CUSOLVER_STATUS_INVALID_VALUE is returned, info[0] = -i (less than zero) indicates i-th parameter is wrong (not counting handle) if info[i] = min(m,n)+1 , gesvdjBatched dose not converge on i-th matrix under given tolerance and maximum sweeps"
  },
  {
    "id": 27177,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,ngesvdaStridedBatched()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27178,
    "content": "gesvda computes eigenvalues of A**T*A , or A**H*A (if A is complex), to approximate singular values and singular vectors"
  },
  {
    "id": 27179,
    "content": "It generates matrices U and V and transforms the matrix A to the following form \\(U^{H}*A*V = S + E\\) where S is diagonal and E depends on rounding errors"
  },
  {
    "id": 27180,
    "content": "To certain conditions, U , V and S approximate singular values and singular vectors up to machine zero of single precision In other words, the accuracy of singular values and left singular vectors depend on the distance between singular value and zero The input parameter rank decides the number of singular values and singular vectors are computed in parameter S , U and V"
  },
  {
    "id": 27181,
    "content": "Otherwise, h_RnrmF reports \\({||}U*S*V^{H}{||} - {||S||}\\) in Frobenius norm sense, that is, how far U is from unitary"
  },
  {
    "id": 27182,
    "content": "It requires that all matrices are of the same size m,n and are packed in a contiguous way, \\(A = \\begin{pmatrix} {A0} & {A1} & \\cdots \\\\ \\end{pmatrix}\\) Each matrix is column-major with leading dimension lda , so the formula for random access is \\(A_{k}\\operatorname{(i,j)} = {A\\lbrack\\ i\\ +\\ lda*j\\ +\\ strideA*k brack}\\) Similarly, the formula for random access of S is \\(S_{k}\\operatorname{(j)} ="
  },
  {
    "id": 27183,
    "content": "{S\\lbrack\\ j\\ +\\ StrideS*k brack}\\) , the formula for random access of U is \\(U_{k}\\operatorname{(i,j)} = {U\\lbrack\\ i\\ +\\ ldu*j\\ +\\ strideU*k brack}\\) and the formula for random access of V is \\(V_{k}\\operatorname{(i,j)} = {V\\lbrack\\ i\\ +\\ ldv*j\\ +\\ strideV*k brack}\\)"
  },
  {
    "id": 27184,
    "content": "The input parameter lwork is the size of the working space, and it is returned by gesvdaStridedBatched_bufferSize()"
  },
  {
    "id": 27185,
    "content": "Otherwise, if info[i] = min(m,n)+1 , gesvdaStridedBatched does not converge on i-th matrix under given tolerance"
  },
  {
    "id": 27186,
    "content": "Remark 2: if the user is confident on the accuracy of singular values and singular vectors, for example, certain conditions hold (required singular value is far from zero), then the performance can be improved by passing a null pointer to h_RnrmF , i"
  },
  {
    "id": 27188,
    "content": "API of gesvda Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27189,
    "content": "A device input array of dimension strideA * batchSize with lda is not less than max(1,m) strideA host input Value of type long long int that gives the address offset between A[i] and A[i+1] Sj is of dimension rank * 1 strideS host input Value of type long long int that gives the address offset between S[i] and S[i+1] strideU host input Value of type long long int that gives the address offset"
  },
  {
    "id": 27190,
    "content": "between U[i] and U[i+1] strideV host input Value of type long long int that gives the address offset between V[i] and V[i+1]"
  },
  {
    "id": 27191,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nsyevd()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27192,
    "content": "cusolverStatus_t cusolverDnSsyevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsyevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double * W , double * work , int lwork , int * devInfo"
  },
  {
    "id": 27193,
    "content": "); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCheevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * W , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZheevd ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t"
  },
  {
    "id": 27194,
    "content": "uplo , int n , cuDoubleComplex * A , int lda , double * W , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes eigenvalues and eigenvectors of a symmetric (Hermitian) n×n matrix A"
  },
  {
    "id": 27195,
    "content": "The standard symmetric eigenvalue problem is \\(A*V = V*\\Lambda\\) where Λ is a real n×n diagonal matrix"
  },
  {
    "id": 27197,
    "content": "If devInfo = i (greater than zero), i off-diagonal elements of an intermediate tridiagonal form did not converge to zero"
  },
  {
    "id": 27199,
    "content": "API of syevd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27200,
    "content": "jobz host input Specifies options to either compute eigenvalue only or compute eigen-pair: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute eigenvalues only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute eigenvalues and eigenvectors"
  },
  {
    "id": 27201,
    "content": "If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of A contains the upper triangular part of the matrix A If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of A contains the lower triangular part of the matrix A On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and devInfo = 0, A contains the orthonormal eigenvectors of the matrix A"
  },
  {
    "id": 27203,
    "content": "If devInfo = i (> 0) , devInfo indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero; Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero; The generic API has"
  },
  {
    "id": 27204,
    "content": "three different types, dataTypeA is data type of the matrix A , dataTypeW is data type of the matrix W and computeType is compute type of the operation Valid combination of data type and compute type DataTypeA DataTypeW ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F SSYEVD CUDA_R_64F CUDA_R_64F CUDA_R_64F DSYEVD CUDA_C_32F CUDA_R_32F CUDA_C_32F CHEEVD CUDA_C_64F CUDA_R_64F CUDA_C_64F ZHEEVD"
  },
  {
    "id": 27205,
    "content": "Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsyevdx()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27206,
    "content": "cusolverStatus_t cusolverDnSsyevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , float * A , int lda , float vl , float vu , int il , int iu , int * h_meig , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsyevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range ,"
  },
  {
    "id": 27207,
    "content": "cublasFillMode_t uplo , int n , double * A , int lda , double vl , double vu , int il , int iu , int * h_meig , double * W , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCheevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n"
  },
  {
    "id": 27208,
    "content": ", cuComplex * A , int lda , float vl , float vu , int il , int iu , int * h_meig , float * W , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZheevdx ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double vl , double vu , int il , int iu , int * h_meig , double * W ,"
  },
  {
    "id": 27209,
    "content": "cuDoubleComplex * work , int lwork , int * devInfo ); This function computes all or selection of the eigenvalues and optionally eigenvectors of a symmetric (Hermitian) n×n matrix A"
  },
  {
    "id": 27210,
    "content": "The standard symmetric eigenvalue problem is: \\(A*V = V*\\Lambda\\) where Λ is a real n×h_meig diagonal matrix h_meig is the number of eigenvalues/eigenvectors computed by the routine, h_meig is equal to n when the whole spectrum (e"
  },
  {
    "id": 27214,
    "content": "API of syevdx Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27215,
    "content": "range host input Specifies options to which selection of eigenvalues and optionally eigenvectors that need to be computed: range = CUSOLVER_EIG_RANGE_ALL : all eigenvalues/eigenvectors will be found, will becomes the classical syevd/heevd routine; range = CUSOLVER_EIG_RANGE_V : all eigenvalues/eigenvectors in the half-open interval (vl,vu] will be found; range = CUSOLVER_EIG_RANGE_I : the il-th"
  },
  {
    "id": 27216,
    "content": "through iu-th eigenvalues/eigenvectors will be found; uplo host input Specifies which part of A is stored If range = CUSOLVER_EIG_RANGE_V , the lower and upper bounds of the interval to be searched for eigenvalues Note that, if eigenvalues are very close to each other, it is well known that two different eigenvalues routines might find slightly different number of eigenvalues inside the same"
  },
  {
    "id": 27217,
    "content": "interval This is due to the fact that different eigenvalue algorithms, or even same algorithm but different run might find eigenvalues within some rounding error close to the machine precision Thus, if the user wants to be sure not to miss any eigenvalue within the interval bound, we suggest that the user subtract/add epsilon (machine precision) to the interval bound such as (vl=vl-eps, vu=vu+eps]"
  },
  {
    "id": 27218,
    "content": "If range = CUSOLVER_EIG_RANGE_I , the indices (in ascending order) of the smallest and largest eigenvalues to be returned"
  },
  {
    "id": 27219,
    "content": "If devInfo = i (> 0) , devInfo indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero"
  },
  {
    "id": 27220,
    "content": "Thus, if the user want to be sure not to miss any eigenvalue within the interval bound, we suggest that, the user subtract/add epsilon (machine precision) to the interval bound such as (vl=vl-eps, vu=vu+eps]"
  },
  {
    "id": 27221,
    "content": "0 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero; The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeW is data type of the matrix W and computeType is compute type of the operation Valid combination of data type and compute type DataTypeA DataTypeW ComputeType Meaning CUDA_R_32F CUDA_R_32F"
  },
  {
    "id": 27222,
    "content": "CUDA_R_32F SSYEVDX CUDA_R_64F CUDA_R_64F CUDA_R_64F DSYEVDX CUDA_C_32F CUDA_R_32F CUDA_C_32F CHEEVDX CUDA_C_64F CUDA_R_64F CUDA_C_64F ZHEEVDX Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27223,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsygvd()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27224,
    "content": "cusolverStatus_t cusolverDnSsygvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * B , int ldb , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsygvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n ,"
  },
  {
    "id": 27225,
    "content": "double * A , int lda , double * B , int ldb , double * W , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnChegvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , cuComplex * B , int ldb , float * W ,"
  },
  {
    "id": 27226,
    "content": "cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZhegvd ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb , double * W , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes eigenvalues and eigenvectors of a symmetric"
  },
  {
    "id": 27228,
    "content": "The generalized symmetric-definite eigenvalue problem is \\({eig(A,B)} = \\left\\{ \\begin{matrix} {A*V = B*V*\\Lambda} & {\\text{if~}\\textsf{itype\\ =\\ CUSOLVER\\_EIG\\_TYPE\\_1}} \\\\ {A*B*V = V*\\Lambda} & {\\text{if~}\\textsf{itype\\ =\\ CUSOLVER\\_EIG\\_TYPE\\_2}} \\\\ {B*A*V = V*\\Lambda} & {\\text{if~}\\textsf{itype\\ =\\ CUSOLVER\\_EIG\\_TYPE\\_3}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 27230,
    "content": "The eigenvectors are normalized as follows: \\(\\left\\{ \\begin{matrix} {V^{H}*B*V = I} & {\\text{if~}\\textsf{itype\\ =\\ CUSOLVER\\_EIG\\_TYPE\\_1,\\ CUSOLVER\\_EIG\\_TYPE\\_2}} \\\\ {V^{H}*{inv(B)}*V = I} & {\\text{if~}\\textsf{itype\\ =\\ CUSOLVER\\_EIG\\_TYPE\\_3}} \\\\ \\end{matrix}  ight"
  },
  {
    "id": 27231,
    "content": "\\) The user has to provide working space which is pointed by input parameter work The input parameter lwork is size of the working space, and it is returned by sygvd_bufferSize()"
  },
  {
    "id": 27233,
    "content": "The factorization of B could not be completed and no eigenvalues or eigenvectors were computed if jobz = CUSOLVER_EIG_MODE_VECTOR, A contains the orthogonal eigenvectors of the matrix A"
  },
  {
    "id": 27234,
    "content": "API of sygvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27235,
    "content": "itype host input Specifies the problem type to be solved: itype =CUSOLVER_EIG_TYPE_1: A*x = (lambda)*B*x"
  },
  {
    "id": 27236,
    "content": "jobz host input Specifies options to either compute eigenvalue only or compute eigen-pair: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute eigenvalues only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute eigenvalues and eigenvectors"
  },
  {
    "id": 27237,
    "content": "If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of B contains the upper triangular part of the matrix B If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of B contains the lower triangular part of the matrix B On exit, if devInfo is less than n , B is overwritten by triangular factor U or L from the Cholesky factorization of B"
  },
  {
    "id": 27238,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsygvdx()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27239,
    "content": "cusolverStatus_t cusolverDnSsygvdx ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , float * A , int lda , float * B , int ldb , float vl , float vu , int il , int iu , int * h_meig , float * W , float * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnDsygvdx ( cusolverDnHandle_t handle ,"
  },
  {
    "id": 27240,
    "content": "cusolverEigType_t itype , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , double * A , int lda , double * B , int ldb , double vl , double vu , int il , int iu , int * h_meig , double * W , double * work , int lwork , int * devInfo ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnChegvdx ("
  },
  {
    "id": 27241,
    "content": "cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , cuComplex * A , int lda , cuComplex * B , int ldb , float vl , float vu , int il , int iu , int * h_meig , float * W , cuComplex * work , int lwork , int * devInfo ); cusolverStatus_t cusolverDnZhegvdx ( cusolverDnHandle_t handle , cusolverEigType_t itype ,"
  },
  {
    "id": 27242,
    "content": "cusolverEigMode_t jobz , cusolverEigRange_t range , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb , double vl , double vu , int il , int iu , int * h_meig , double * W , cuDoubleComplex * work , int lwork , int * devInfo ); This function computes all or selection of the eigenvalues and optionally eigenvectors of a symmetric (Hermitian) n×n"
  },
  {
    "id": 27246,
    "content": "API of sygvdx Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27247,
    "content": "itype host input Specifies the problem type to be solved: itype =CUSOLVER_EIG_TYPE_1: A*x = (lambda)*B*x itype =CUSOLVER_EIG_TYPE_2: A*B*x = (lambda)*x itype =CUSOLVER_EIG_TYPE_3: B*A*x = (lambda)*x jobz host input Specifies options to either compute eigenvalue only or compute eigen-pair: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute eigenvalues only; jobz = CUSOLVER_EIG_MODE_VECTOR : Compute"
  },
  {
    "id": 27248,
    "content": "eigenvalues and eigenvectors range host input Specifies options to which selection of eigenvalues and optionally eigenvectors that need to be computed: range = CUSOLVER_EIG_RANGE_ALL : all eigenvalues/eigenvectors will be found, will becomes the classical syevd/heevd routine; range = CUSOLVER_EIG_RANGE_V : all eigenvalues/eigenvectors in the half-open interval (vl,vu] will be found; range ="
  },
  {
    "id": 27249,
    "content": "CUSOLVER_EIG_RANGE_I : the il-th through iu-th eigenvalues/eigenvectors will be found; uplo host input Specifies which part of A and B are stored"
  },
  {
    "id": 27250,
    "content": "Thus, if the user want to be sure not to miss any eigenvalue within the interval bound, we suggest that, the user subtract/add epsilon (machine precision) to the interval bound such as ( vl = vl - eps , vu = vu + eps ]"
  },
  {
    "id": 27251,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsyevj()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27252,
    "content": "cusolverStatus_t cusolverDnSsyevj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * W , float * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnDsyevj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double * W , double * work , int lwork"
  },
  {
    "id": 27253,
    "content": ", int * info , syevjInfo_t params ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCheevj ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * W , cuComplex * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnZheevj ( cusolverDnHandle_t"
  },
  {
    "id": 27254,
    "content": "handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double * W , cuDoubleComplex * work , int lwork , int * info , syevjInfo_t params ); This function computes eigenvalues and eigenvectors of a symmetric (Hermitian) n×n matrix A"
  },
  {
    "id": 27255,
    "content": "The standard symmetric eigenvalue problem is \\(A*Q = Q*\\Lambda\\) where Λ is a real n×n diagonal matrix"
  },
  {
    "id": 27257,
    "content": "syevj iteratively generates a sequence of unitary matrices to transform matrix A to the following form \\(V^{H}*A*V = W + E\\) where W is diagonal and E is symmetric without diagonal"
  },
  {
    "id": 27258,
    "content": "In practice, Jacobi method stops if \\({||E||}_{F}\\leq\\operatorname{eps}*{||A||}_{F}\\) where eps is the given tolerance"
  },
  {
    "id": 27259,
    "content": "The default value is machine accuracy but The user can use function cusolverDnXsyevjSetTolerance to set a priori tolerance The default value is 100 but the user can use function cusolverDnXsyevjSetMaxSweeps to set a proper bound"
  },
  {
    "id": 27260,
    "content": "The Jacobi method has quadratic convergence, so the accuracy is not proportional to number of sweeps After syevj , the user can query residual by function cusolverDnXsyevjGetResidual and number of executed sweeps by function cusolverDnXsyevjGetSweeps However the user needs to be aware that residual is the Frobenius norm of E , not accuracy of individual eigenvalue, i"
  },
  {
    "id": 27262,
    "content": "\\({residual}={||E||}_{F} = {{||}\\Lambda - W{||}}_{F}\\) The same as syevd , the user has to provide working space pointed by input parameter work The input parameter lwork is the size of the working space, and it is returned by syevj_bufferSize()"
  },
  {
    "id": 27263,
    "content": "API of syevj Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27264,
    "content": "On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and info = 0, A contains the orthonormal eigenvectors of the matrix A"
  },
  {
    "id": 27266,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsygvj()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27267,
    "content": "cusolverStatus_t cusolverDnSsygvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * B , int ldb , float * W , float * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnDsygvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t"
  },
  {
    "id": 27268,
    "content": "uplo , int n , double * A , int lda , double * B , int ldb , double * W , double * work , int lwork , int * info , syevjInfo_t params ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnChegvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda ,"
  },
  {
    "id": 27269,
    "content": "cuComplex * B , int ldb , float * W , cuComplex * work , int lwork , int * info , syevjInfo_t params ); cusolverStatus_t cusolverDnZhegvj ( cusolverDnHandle_t handle , cusolverEigType_t itype , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , cuDoubleComplex * B , int ldb , double * W , cuDoubleComplex * work , int lwork , int * info , syevjInfo_t params );"
  },
  {
    "id": 27270,
    "content": "This function computes eigenvalues and eigenvectors of a symmetric (Hermitian) n×n matrix-pair ( A , B )"
  },
  {
    "id": 27271,
    "content": "The generalized symmetric-definite eigenvalue problem is \\({eig(A,B)} = \\left\\{ \\begin{matrix} {A*V = B*V*\\Lambda} & {\\text{if }\\textsf{itype = CUSOLVER_EIG_TYPE_1}} \\\\ {A*B*V = V*\\Lambda} & {\\text{if }\\textsf{itype = CUSOLVER_EIG_TYPE_2}} \\\\ {B*A*V = V*\\Lambda} & {\\text{if }\\textsf{itype = CUSOLVER_EIG_TYPE_3}} \\\\ \\end{matrix} ight \\) where the matrix B is positive definite The eigenvectors are"
  },
  {
    "id": 27272,
    "content": "normalized as follows: \\(\\left\\{ \\begin{matrix} {V^{H}*B*V = I} & {\\text{if }\\textsf{itype = CUSOLVER_EIG_TYPE_1, CUSOLVER_EIG_TYPE_2}} \\\\ {V^{H}*{inv(B)}*V = I} & {\\text{if }\\textsf{itype = CUSOLVER_EIG_TYPE_3}} \\\\ \\end{matrix} ight"
  },
  {
    "id": 27273,
    "content": "\\) This function has the same functionality as sygvd except that syevd in sygvd is replaced by syevj in sygvj Therefore, sygvj inherits properties of syevj , the user can use cusolverDnXsyevjSetTolerance and cusolverDnXsyevjSetMaxSweeps to configure tolerance and maximum sweeps sygvj first computes Cholesky factorization of matrix B , \\(B = L*L^{H}\\) transform the problem to standard eigenvalue"
  },
  {
    "id": 27274,
    "content": "problem, then calls syevj For example, the standard eigenvalue problem of type I is \\(M*Q = Q*\\Lambda\\) where matrix M is symmetric \\(M = L^{-1}*A*L^{-H}\\) The residual is the result of syevj on matrix M , not A The input parameter lwork is the size of the working space, and it is returned by sygvj_bufferSize() On exit, if info is less than n , B is overwritten by triangular factor U or L from the"
  },
  {
    "id": 27275,
    "content": "Cholesky factorization of B If info = i (> 0) , info indicates either B is not positive definite or syevj (called by sygvj ) does not converge"
  },
  {
    "id": 27276,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( nsyevjBatched()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27277,
    "content": "cusolverStatus_t cusolverDnSsyevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , float * A , int lda , float * W , float * work , int lwork , int * info , syevjInfo_t params , int batchSize ); cusolverStatus_t cusolverDnDsyevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , double * A , int lda , double *"
  },
  {
    "id": 27278,
    "content": "W , double * work , int lwork , int * info , syevjInfo_t params , int batchSize ); The C and Z data types are complex valued single and double precision, respectively cusolverStatus_t cusolverDnCheevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuComplex * A , int lda , float * W , cuComplex * work , int lwork , int * info , syevjInfo_t params ,"
  },
  {
    "id": 27279,
    "content": "int batchSize ); cusolverStatus_t cusolverDnZheevjBatched ( cusolverDnHandle_t handle , cusolverEigMode_t jobz , cublasFillMode_t uplo , int n , cuDoubleComplex * A , int lda , double * W , cuDoubleComplex * work , int lwork , int * info , syevjInfo_t params , int batchSize ); This function computes eigenvalues and eigenvectors of a sequence of symmetric (Hermitian) n×n matrices \\(A_{j}*Q_{j} ="
  },
  {
    "id": 27280,
    "content": "Q_{j}*\\Lambda_{j}\\) where \\(\\Lambda_{j}\\) is a real n×n diagonal matrix The diagonal elements of \\(\\Lambda_{j}\\) are the eigenvalues of \\(A_{j}\\) in either ascending order or non-sorting order"
  },
  {
    "id": 27281,
    "content": "It requires that all matrices are of the same size n and are packed in contiguous way, \\(A = \\begin{pmatrix} {A0} & {A1} & \\cdots \\\\ \\end{pmatrix}\\) Each matrix is column-major with leading dimension lda , so the formula for random access is \\(A_{k}\\operatorname{(i,j)} = {A\\lbrack\\ i\\ +\\ lda*j\\ +\\ lda*n*k brack}\\) The parameter W also contains eigenvalues of each matrix in contiguous way, \\(W ="
  },
  {
    "id": 27282,
    "content": "\\begin{pmatrix} {W0} & {W1} & \\cdots \\\\ \\end{pmatrix}\\) The formula for random access of W is \\(W_{k}\\operatorname{(j)} = {W\\lbrack\\ j\\ +\\ n*k brack}\\)"
  },
  {
    "id": 27283,
    "content": "Except for tolerance and maximum sweeps, syevjBatched can either sort the eigenvalues in ascending order (default) or chose as-is (without sorting) by the function cusolverDnXsyevjSetSortEig"
  },
  {
    "id": 27284,
    "content": "If the user packs several tiny matrices into diagonal blocks of one matrix, non-sorting option can separate spectrum of those tiny matrices"
  },
  {
    "id": 27285,
    "content": "syevjBatched cannot report residual and executed sweeps by function cusolverDnXsyevjGetResidual and cusolverDnXsyevjGetSweeps"
  },
  {
    "id": 27286,
    "content": "The input parameter lwork is the size of the working space, and it is returned by syevjBatched_bufferSize()"
  },
  {
    "id": 27287,
    "content": "Otherwise, if info[i] = n+1 , syevjBatched does not converge on i-th matrix under given tolerance and maximum sweeps"
  },
  {
    "id": 27289,
    "content": "API of syevjBatched Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27290,
    "content": "If uplo = CUBLAS_FILL_MODE_UPPER , the leading n-by-n upper triangular part of Aj contains the upper triangular part of the matrix Aj If uplo = CUBLAS_FILL_MODE_LOWER , the leading n-by-n lower triangular part of Aj contains the lower triangular part of the matrix Aj On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and info[j] = 0, Aj contains the orthonormal eigenvectors of the matrix Aj"
  },
  {
    "id": 27291,
    "content": "If info[i] = n+1 , syevjBatched does not converge on i-th matrix under given tolerance and maximum sweeps"
  },
  {
    "id": 27292,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n array of dimension lda * n with lda is not less than max(1,m)"
  },
  {
    "id": 27293,
    "content": "workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgetrf_bufferSize workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgetrf_bufferSize"
  },
  {
    "id": 27294,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n array of dimension ldb * nrhs with ldb is not less than max(1,n)"
  },
  {
    "id": 27295,
    "content": "The generic API has two different types: dataTypeA is data type of the matrix A and dataTypeB is data type of the matrix B cusolverDnXgetrs only supports the following four combinations: Valid combination of data type and compute type DataTypeA dataTypeB Meaning CUDA_R_32F CUDA_R_32F SGETRS CUDA_R_64F CUDA_R_64F DGETRS CUDA_C_32F CUDA_C_32F CGETRS CUDA_C_64F CUDA_C_64F ZGETRS Status Returned"
  },
  {
    "id": 27298,
    "content": "API of larft Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27299,
    "content": "direct host input Specifies the order in which the elementary reflectors are multiplied to form the block reflector storev host input Specifies how the vectors which define the elementary reflectors are stored k host input The order of the triangular factor T (= the number of elementary reflectors) If direct == CUBLAS_DIRECT_FORWARD , T is upper triangular; if direct == CUBLAS_DIRECT_BACKWARD , T"
  },
  {
    "id": 27301,
    "content": "workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXlarft_bufferSize workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXlarft_bufferSize"
  },
  {
    "id": 27302,
    "content": "The generic API has four different types: dataTypeV is data type of the array V dataTypeTau is data type of the array tau dataTypeT is data type of the array T computeType is compute type of the operation cusolverDnXlarft only supports the following four combinations Valid combinations of data types and compute types DataTypeV DataTypeTau DataTypeT ComputeType Meaning CUDA_R_32F CUDA_R_32F"
  },
  {
    "id": 27303,
    "content": "CUDA_R_32F CUDA_R_32F SLARFT CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DLARFT CUDA_C_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CLARFT CUDA_C_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZLARFT Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27304,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( n == 0 , k > n , or storev == CUBLAS_STOREV_ROWWISE )"
  },
  {
    "id": 27309,
    "content": "Dense Eigenvalue Solver Reference (64-bit API)  This section describes eigenvalue solver API of cuSolverDN, including bidiagonalization and SVD"
  },
  {
    "id": 27314,
    "content": "cusolverDnXgesvd()  The helper functions below can calculate the sizes needed for pre-allocated buffer"
  },
  {
    "id": 27315,
    "content": "The user has to provide device and host working spaces which are pointed by input parameters bufferOnDevice and bufferOnHost The input parameters workspaceInBytesOnDevice (and workspaceInBytesOnHost ) is size in bytes of the device (and host) working space, and it is returned by cusolverDnXgesvd_bufferSize()"
  },
  {
    "id": 27316,
    "content": "if bdsqr did not converge, info specifies how many superdiagonals of an intermediate bidiagonal form did not converge to zero"
  },
  {
    "id": 27317,
    "content": "Table of algorithms supported by cusolverDnXgesvd CUSOLVER_ALG_0 or NULL Default algorithm List of input arguments for cusolverDnXgesvd_bufferSize and cusolverDnXgesvd : API of cusolverDnXgesvd Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgesvd_bufferSize"
  },
  {
    "id": 27318,
    "content": "workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgesvd_bufferSize"
  },
  {
    "id": 27319,
    "content": "If info > 0 , info indicates how many superdiagonals of an intermediate bidiagonal form did not converge to zero"
  },
  {
    "id": 27320,
    "content": "Remark 2: the routine returns V , not \\(V^{H}\\) List of input arguments for cusolverDnXgesvdp_bufferSize and cusolverDnXgesvdp : API of cusolverDnXgesvdp Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27321,
    "content": "jobz host input Specifies options to either compute singular values only or compute singular vectors as well: jobz = CUSOLVER_EIG_MODE_NOVECTOR : Compute singular values only"
  },
  {
    "id": 27322,
    "content": "workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgesvdp_bufferSize workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgesvdp_bufferSize"
  },
  {
    "id": 27323,
    "content": "The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeS is data type of the vector S and dataTypeU is data type of the matrix U , dataTypeV is data type of the matrix V , computeType is compute type of the operation cusolverDnXgesvdp only supports the following four combinations: Valid combination of data type and compute type DataTypeA DataTypeS DataTypeU"
  },
  {
    "id": 27324,
    "content": "DataTypeV ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F CUDA_R_32F SGESVDP CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DGESVDP CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CGESVDP CUDA_C_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZGESVDP Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27325,
    "content": "Remark 2: the routine returns V , not \\(V^{H}\\) List of input arguments for cusolverDnXgesvdr_bufferSize and cusolverDnXgesvdr : API of cusolverDnXgesvdr Parameter Memory In/out Meaning handle host input Handle to the cuSolverDN library context"
  },
  {
    "id": 27326,
    "content": "jobu host input Specifies options for computing all or part of the matrix U : = ‘S’: the first k columns of U (the left singular vectors) are returned in the array U; = ‘N’: no columns of U (no left singular vectors) are computed jobv host input Specifies options for computing all or part of the matrix V: = ‘S’: the first k rows of V (the right singular vectors) are returned in the array V; ="
  },
  {
    "id": 27328,
    "content": "workspaceInBytesOnDevice host input Size in bytes of bufferOnDevice , returned by cusolverDnXgesvdr_bufferSize workspaceInBytesOnHost host input Size in bytes of bufferOnHost , returned by cusolverDnXgesvdr_bufferSize"
  },
  {
    "id": 27329,
    "content": "The generic API has five different types, dataTypeA is data type of the matrix A , dataTypeS is data type of the vector S and dataTypeU is data type of the matrix U , dataTypeV is data type of the matrix V , computeType is compute type of the operation Valid combination of data type and compute type DataTypeA DataTypeS DataTypeU DataTypeV ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F"
  },
  {
    "id": 27330,
    "content": "CUDA_R_32F CUDA_R_32F SGESVDR CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F CUDA_R_64F DGESVDR CUDA_C_32F CUDA_R_32F CUDA_C_32F CUDA_C_32F CUDA_C_32F CGESVDR CUDA_C_64F CUDA_R_64F CUDA_C_64F CUDA_C_64F CUDA_C_64F ZGESVDR Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27331,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero"
  },
  {
    "id": 27332,
    "content": "The generic API has three different types, dataTypeA is data type of the matrix A , dataTypeW is data type of the matrix W and computeType is compute type of the operation"
  },
  {
    "id": 27333,
    "content": "0 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero"
  },
  {
    "id": 27334,
    "content": "cusolverDnXsyevdx only supports the following four combinations: Valid combination of data type and compute type DataTypeA DataTypeW ComputeType Meaning CUDA_R_32F CUDA_R_32F CUDA_R_32F SSYEVDX CUDA_R_64F CUDA_R_64F CUDA_R_64F DSYEVDX CUDA_C_32F CUDA_R_32F CUDA_C_32F CHEEVDX CUDA_C_64F CUDA_R_64F CUDA_C_64F ZHEEVDX Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27335,
    "content": "If matrix A is symmetric/Hermitian and only lower/upper part is used or meaningful, the user has to extend the matrix into its missing upper/lower part, otherwise the result would be wrong"
  },
  {
    "id": 27336,
    "content": "The linear system is solved by sparse LU with partial pivoting: \\(P*A = L*U\\) cusolver library provides three reordering schemes, symrcm symamd , and csrmetisnd to reduce zero fill-in which dramatically affects the performance of LU factorization The input parameter reorder can enable symrcm ( symamd or csrmetisnd ) if reorder is 1 (2, or 3), otherwise, no reordering is performed If reorder is"
  },
  {
    "id": 27340,
    "content": "\\({|U(j,j)|} array of nnzA \\(( =\\) csrRowPtrA(n) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A csrRowPtrA device host Integer array of n \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one csrColIndA device host Integer array of nnzA \\(( =\\) csrRowPtrA(n) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A"
  },
  {
    "id": 27341,
    "content": "Output Parameter cusolverSp MemSpace *Host MemSpace Description x device host Solution vector of size n , x = inv(A)*b"
  },
  {
    "id": 27342,
    "content": "Otherwise, first index j such that U(j,j)≈0 Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27343,
    "content": "The linear system is solved by sparse QR factorization, \\(A\\ =\\ Q*R\\) If A is singular under given tolerance ( max(tol,0) ), then some diagonal elements of R is zero, i"
  },
  {
    "id": 27345,
    "content": "\\({|R(j,j)|} array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A csrRowPtrA device host Integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one csrColIndA device host Integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A"
  },
  {
    "id": 27346,
    "content": "Output Parameter cusolverSp MemSpace *Host MemSpace Description x device host Solution vector of size m , x = inv(A)*b"
  },
  {
    "id": 27347,
    "content": "Otherwise, first index j such that R(j,j)≈0 Status Returned CUSOLVER_STATUS_SUCCESS The operation completed successfully"
  },
  {
    "id": 27348,
    "content": "The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL and upper triangular part of A is ignored (if parameter reorder is zero) In other words, suppose input matrix A is decomposed as \\(A = L + D + U\\) , where L is lower triangular, D is diagonal and U is upper triangular"
  },
  {
    "id": 27349,
    "content": "The function would ignore U and regard A as a symmetric matrix with the formula \\(A = L + D + L^{H}\\)"
  },
  {
    "id": 27350,
    "content": "If parameter reorder is nonzero, the user has to extend A to a full matrix, otherwise the solution would be wrong"
  },
  {
    "id": 27351,
    "content": "The linear system is solved by sparse Cholesky factorization, \\(A = G*G^{H}\\) where G is the Cholesky factor, a lower triangular matrix"
  },
  {
    "id": 27352,
    "content": "The output parameter singularity has two meanings: If A is not positive definite, there exists some integer k such that A(0:k, 0:k) is not positive definite there exists some integer k such that \\(G\\begin{pmatrix} {k,k} \\\\ \\end{pmatrix} array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A"
  },
  {
    "id": 27353,
    "content": "Output Parameter cusolverSp MemSpace *Host MemSpace Description x device host Solution vector of size m , x = inv(A)*b"
  },
  {
    "id": 27354,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nnzcsrlsqvqr()  The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverSpScsrlsqvqr [ Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , const float * b , float"
  },
  {
    "id": 27355,
    "content": "tol , int * rankA , float * x , int * p , float * min_norm ); cusolverStatus_t cusolverSpDcsrlsqvqr [ Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , const double * b , double tol , int * rankA , double * x , int * p , double * min_norm ); The C and Z data types are complex"
  },
  {
    "id": 27356,
    "content": "valued single and double precision, respectively cusolverStatus_t cusolverSpCcsrlsqvqr [ Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cuComplex * b , float tol , int * rankA , cuComplex * x , int * p , float * min_norm ); cusolverStatus_t cusolverSpZcsrlsqvqr ["
  },
  {
    "id": 27357,
    "content": "Host ]( cusolverSpHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , const cuDoubleComplex * b , double tol , int * rankA , cuDoubleComplex * x , int * p , double * min_norm ); This function solves the following least-square problem: \\(x = {argmin}{||}A*z - b{||}\\) A is an m×n sparse"
  },
  {
    "id": 27358,
    "content": "matrix that is defined in CSR storage format by the three arrays csrValA , csrRowPtrA , and csrColIndA"
  },
  {
    "id": 27359,
    "content": "b is the right-hand-side vector of size m , and x is the least-square solution vector of size n If A is square, symmetric/Hermitian and only lower/upper part is used or meaningful, the user has to extend the matrix into its missing upper/lower part, otherwise the result is wrong This function only works if m is greater or equal to n , in other words, A is a tall matrix The least-square problem is"
  },
  {
    "id": 27362,
    "content": "Suppose rank of A is k , less than n , the permutation matrix P reorders columns of A in the following sense: \\(A*P^{T} = \\begin{pmatrix} A_{1} & A_{2} \\\\ \\end{pmatrix} = \\begin{pmatrix} Q_{1} & Q_{2} \\\\ \\end{pmatrix}\\begin{pmatrix} R_{11} & R_{12} & R_{22} \\\\ \\end{pmatrix}\\) where \\(R_{11}\\) and A have the same rank, but \\(R_{22}\\) is almost zero, i"
  },
  {
    "id": 27367,
    "content": "||\\) or in matrix form \\(\\begin{pmatrix} R_{11} & R_{12} & R_{22} \\\\ \\end{pmatrix}\\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\end{pmatrix} = \\begin{pmatrix} c_{1} \\\\ c_{2} \\\\ \\end{pmatrix}\\) The output parameter min_norm is \\(\\left"
  },
  {
    "id": 27369,
    "content": "|| \\\\ {{subject\\ to}R_{11}*y_{1} + R_{12}*y_{2} = c_{1}} \\\\ \\end{matrix}\\) Or equivalently another least-square problem min|| R 1 1 \\ R 1 2 I * y 2 - R 1 1 \\ c 1 O || The output parameter x is \\(P^{T}*y\\) , the solution of least-square problem"
  },
  {
    "id": 27370,
    "content": "Input Parameter cusolverSp MemSpace *Host MemSpace Description handle host host Handle to the cuSolver library context"
  },
  {
    "id": 27372,
    "content": "csrValA device host array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A"
  },
  {
    "id": 27375,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,n,nnzcsreigvsi()  The S and D data types are real valued single and double precision, respectively cusolverStatus_t cusolverSpScsreigvsi [ Host ]( cusolverSpHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const float * csrValA , const int * csrRowPtrA , const int * csrColIndA , float mu0 , const float * x0 ,"
  },
  {
    "id": 27376,
    "content": "int maxite , float tol , float * mu , float * x ); cusolverStatus_t cusolverSpDcsreigvsi [ Host ]( cusolverSpHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const double * csrValA , const int * csrRowPtrA , const int * csrColIndA , double mu0 , const double * x0 , int maxite , double tol , double * mu , double * x ); The C and Z data types are complex valued single and double"
  },
  {
    "id": 27377,
    "content": "precision, respectively cusolverStatus_t cusolverSpCcsreigvsi [ Host ]( cusolverSpHandle_t handle , int m , int nnz , const cusparseMatDescr_t descrA , const cuComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , cuComplex mu0 , const cuComplex * x0 , int maxite , float tol , cuComplex * mu , cuComplex * x ); cusolverStatus_t cusolverSpZcsreigvsi ( cusolverSpHandle_t handle , int"
  },
  {
    "id": 27378,
    "content": "m , int nnz , const cusparseMatDescr_t descrA , const cuDoubleComplex * csrValA , const int * csrRowPtrA , const int * csrColIndA , cuDoubleComplex mu0 , const cuDoubleComplex * x0 , int maxite , double tol , cuDoubleComplex * mu , cuDoubleComplex * x ); This function solves the simple eigenvalue problem \\(A*x = \\lambda*x\\) by shift-inverse method A is an m×m sparse matrix that is defined in CSR"
  },
  {
    "id": 27380,
    "content": "The output parameter x is the approximated eigenvector of size m , The following shift-inverse method corrects eigenpair step-by-step until convergence The shift-inverse method will converge to the eigenvalue mu nearest mu0 if mu is a singleton It is useful when shift-inverse method does not converge because the tolerance is too small or the desired eigenvalue is not a singleton Shift-Inverse"
  },
  {
    "id": 27381,
    "content": "Method Given a initial guess of eigenvalue μ0 and initial vector x0 x (0) = x0 of unit length for j = 0 : maxite solve ( A - μ0 * I ) * x (k+1) = x (k) normalize x (k+1) to unit length compute approx"
  },
  {
    "id": 27382,
    "content": "eigenvalue μ = x H * A * x where x = x (k+1) if || A * x (k+1) - μ * x (k+1) || array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A"
  },
  {
    "id": 27383,
    "content": "Output Parameter cusolverSp MemSpace *Host MemSpace Description mu device host Approximated eigenvalue nearest mu0 under tolerance"
  },
  {
    "id": 27384,
    "content": "This number may not be accurate due to several reasons: The contour C is close to some eigenvalues or even passes through some eigenvalues Even though csreigs may not be accurate, it still can give the user some idea how many eigenvalues in a region where the resolution of disk theorem is bad For example, standard 3-point stencil of finite difference of Laplacian operator is a tridiagonal matrix,"
  },
  {
    "id": 27385,
    "content": "and disk theorem would show “all eigenvalues are in the interval [0, 4*N^2]” where N is number of grids"
  },
  {
    "id": 27386,
    "content": "Input Parameter cusolverSp MemSpace *Host MemSpace Description handle host host Handle to the cuSolverSP library context Output Parameter cusolverSp MemSpace *Host MemSpace Description num_eigs host host Number of algebraic eigenvalues in a box"
  },
  {
    "id": 27387,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( m,nnz array of nnzA \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A csrRowPtrA host Integer array of n+1 elements that contains the start of every row and the end of the last row plus one csrColIndA host Integer array of nnzA column indices of the nonzero elements of matrix A CUSOLVER_STATUS_INVALID_VALUE"
  },
  {
    "id": 27388,
    "content": "Invalid parameters were passed ( n,nnzA array of nnzA*batchSize nonzero elements of matrices A0, A1,"
  },
  {
    "id": 27390,
    "content": "csrRowPtrA device Integer array of m+1 elements that contains the start of every row and the end of the last row plus one csrColIndA device Integer array of nnzA column indices of the nonzero elements of each matrix Aj b device array of m*batchSize of right-hand-side vectors b0, b1,"
  },
  {
    "id": 27392,
    "content": "pBuffer device Buffer allocated by the user, the size is returned by cusolverSpXcsrqrBufferInfoBatched()"
  },
  {
    "id": 27393,
    "content": "Output Parameter cusolverSp MemSpace Description x device array of m*batchSize of solution vectors x0, x1,"
  },
  {
    "id": 27399,
    "content": "cusolverRfGetNumericBoostReport()  cusolverStatus_t cusolverRfGetNumericBoostReport ( cusolverRfHandle_t handle , cusolverRfNumericBoostReport_t * report ); This routine gets the report whether numeric boosting was used in the cusolverRfRefactor() and cusolverRfSolve() routines report host output The enumerated boosting report type"
  },
  {
    "id": 27403,
    "content": "cusolverRfGetResetValuesFastMode()  cusolverStatus_t cusolverRfGetResetValuesFastMode ( cusolverRfHandle_t handle , cusolverRfResetValuesFastMode_t * fastMode ); This routine gets the mode used in the cusolverRfResetValues routine fastMode host output The enumerated mode type"
  },
  {
    "id": 27407,
    "content": "cusolverRfGet_Algs()  cusolverStatus_t cusolverRfGet_Algs ( cusolverRfHandle_t handle , cusolverRfFactorization_t * fact_alg , cusolverRfTriangularSolve_t * solve_alg ); This routine gets the algorithm used for the refactorization in cusolverRfRefactor() and the triangular solve in cusolverRfSolve()"
  },
  {
    "id": 27412,
    "content": "cusolverRfRefactor()  cusolverStatus_t cusolverRfRefactor ( cusolverRfHandle_t handle ); This routine performs the LU re-factorization: \\(A = L*U\\) exploring the available parallelism on the GPU"
  },
  {
    "id": 27413,
    "content": "It is assumed that a prior call to the cusolverRfAnalyze() was done in order to find the available parallelism"
  },
  {
    "id": 27414,
    "content": "This routine may be called multiple times, once for each of the linear systems: \\(A_{i}x_{i} = f_{i}\\) There are some constraints to the combination of algorithms used for refactorization and solving routines, cusolverRfRefactor() and cusolverRfSolve() The table below summarizes the supported combinations of algorithms: Compatible algorithms for solving and refactorization routines"
  },
  {
    "id": 27415,
    "content": "Factorization Solving CUSOLVERRF_FACTORIZATION_ALG0 TRIANGULAR_SOLVE_ALG1 CUSOLVERRF_FACTORIZATION_ALG1 TRIANGULAR_SOLVE_ALG2, TRIANGULAR_SOLVE_ALG3 CUSOLVERRF_FACTORIZATION_ALG2 TRIANGULAR_SOLVE_ALG2, TRIANGULAR_SOLVE_ALG3 Parameter MemSpace In/out Meaning handle host in/out The handle to the cuSolverRF library"
  },
  {
    "id": 27420,
    "content": "cusolverRfResetValues()  cusolverStatus_t cusolverRfResetValues ( /* Input (in the device memory) */ int n , int nnzA , int * csrRowPtrA , int * csrColIndA , double * csrValA , int * P , int * Q , /* Output */ cusolverRfHandle_t handle ); This routine updates internal data structures with the values of the new coefficient matrix It is assumed that the arrays csrRowPtrA , csrColIndA , P and Q"
  },
  {
    "id": 27422,
    "content": "This assumption reflects the fact that the sparsity pattern of coefficient matrices as well as reordering to minimize fill-in and pivoting remain the same in the set of linear systems: \\(A_{i}x_{i} = f_{i}\\) This routine may be called multiple times, once for each of the linear systems: \\(A_{i}x_{i} = f_{i}\\) Parameter MemSpace In/out Meaning n host input The number of rows (and columns) of"
  },
  {
    "id": 27423,
    "content": "matrix A csrRowPtrA device input The array of offsets corresponding to the start of each row in the arrays csrColIndA and csrValA This array has also an extra entry at the end that stores the number of non-zero elements in the matrix csrColIndA device input The array of column indices corresponding to the non-zero elements in the matrix csrValA device input The array of values corresponding to the"
  },
  {
    "id": 27429,
    "content": "cusolverRfSetMatrixFormat()  cusolverStatus_t cusolverRfSetMatrixFormat ( cusolverRfHandle_t handle , cusolverRfMatrixFormat_t format , cusolverRfUnitDiagonal_t diag ); This routine sets the matrix format used in the cusolverRfSetupDevice() , cusolverRfSetupHost() , cusolverRfResetValues() , cusolverRfExtractBundledFactorsHost() and cusolverRfExtractSplitFactorsHost() routines It may be called"
  },
  {
    "id": 27435,
    "content": "cusolverRfSetNumericProperties()  cusolverStatus_t cusolverRfSetNumericProperties ( cusolverRfHandle_t handle , double zero , double boost ); This routine sets the numeric values used for checking for ‘’zero’’ pivot and for boosting it in the cusolverRfRefactor() and cusolverRfSolve() routines It may be called multiple times prior to cusolverRfRefactor() and cusolverRfSolve() routines boost host"
  },
  {
    "id": 27440,
    "content": "cusolverRfSetResetValuesFastMode()  cusolverStatus_t cusolverRfSetResetValuesFastMode ( cusolverRfHandle_t handle , cusolverRfResetValuesFastMode_t fastMode ); This routine sets the mode used in the cusolverRfResetValues routine"
  },
  {
    "id": 27441,
    "content": "The fast mode requires extra memory and is recommended only if very fast calls to cusolverRfResetValues() are needed"
  },
  {
    "id": 27446,
    "content": "cusolverRfSetAlgs()  cusolverStatus_t cusolverRfSetAlgs ( cusolverRfHandle_t handle , cusolverRfFactorization_t fact_alg , cusolverRfTriangularSolve_t alg ); This routine sets the algorithm used for the refactorization in cusolverRfRefactor() and the triangular solve in cusolverRfSolve() alg host input The enumerated algorithm type"
  },
  {
    "id": 27450,
    "content": "cusolverRfSolve()  cusolverStatus_t cusolverRfSolve ( /* Input (in the device memory) */ cusolverRfHandle_t handle , int * P , int * Q , int nrhs , double * Temp , int ldt , /* Input/Output (in the device memory) */ double * XF , /* Input */ int ldxf ); This routine performs the forward and backward solve with the lower \\(L\\in R^{nxn}\\) and upper \\(U\\in R^{nxn}\\) triangular factors resulting"
  },
  {
    "id": 27451,
    "content": "from the LU re-factorization: \\(A = L*U\\) which is assumed to have been computed by a prior call to the cusolverRfRefactor() routine"
  },
  {
    "id": 27452,
    "content": "The routine can solve linear systems with multiple right-hand-sides (RHS): \\(AX = {(LU)}X = L{(UX)} = LY = F~{where}~UX = Y\\) even though currently only a single RHS is supported"
  },
  {
    "id": 27453,
    "content": "This routine may be called multiple times, once for each of the linear systems: \\(A_{i}x_{i} = f_{i}\\) Parameter MemSpace In/out Meaning handle host output The handle to the cuSolverRF library"
  },
  {
    "id": 27454,
    "content": "XF device in/out The dense matrix that contains the right-hand-sides F and solutions X (of size ldxf*nrhs ) ldxf host input The leading dimension of dense matrix XF ( ldxf >= n )"
  },
  {
    "id": 27458,
    "content": "cusolverRfBatchSetupHost()  cusolverStatus_t cusolverRfBatchSetupHost ( /* Input (in the host memory) */ int batchSize , int n , int nnzA , int * h_csrRowPtrA , int * h_csrColIndA , double * h_csrValA_array [], int nnzL , int * h_csrRowPtrL , int * h_csrColIndL , double * h_csrValL , int nnzU , int * h_csrRowPtrU , int * h_csrColIndU , double * h_csrValU , int * h_P , int * h_Q , /* Output */"
  },
  {
    "id": 27459,
    "content": "cusolverRfHandle_t handle ); This routine assembles the internal data structures of the cuSolverRF library for batched operation"
  },
  {
    "id": 27460,
    "content": "It is called after the call to the cusolverRfCreate() routine, and before any other batched routines The batched operation assumes that the user has the following linear systems: \\(A_{j}x_{j} = b_{j}{,\\ j\\ =\\ 1,2, ,\\ batchSize}\\) where each matrix in the set: \\(\\{ A_{j}\\}\\) has the same sparsity pattern, and quite similar such that factorization can be done by the same permutation P and Q This"
  },
  {
    "id": 27461,
    "content": "routine accepts as input (on the host) the original matrix A (sparsity pattern and batched values), the lower (L) and upper (U) triangular factors, as well as the left (P) and the right (Q) permutations resulting from the full LU factorization of the first (i=1) linear system: \\(A_{i}x_{i} = f_{i}\\) The permutations P and Q represent the final composition of all the left and right reorderings"
  },
  {
    "id": 27462,
    "content": "applied to the original matrix A , respectively However, these permutations are often associated with partial pivoting and reordering to minimize fill-in, respectively"
  },
  {
    "id": 27464,
    "content": "The algorithm is memory-bound, once bandwidth limit is reached, there is no room to improve performance by large batchSize In practice, batchSize of 32 - 128 is often enough to obtain good performance, but in some cases larger batchSize might be beneficial The following routine needs to be called only once for a single linear system: \\(A_{i}x_{i} = f_{i}\\) Parameter MemSpace In/out Meaning"
  },
  {
    "id": 27466,
    "content": "h_csrRowPtrA host input The array of offsets corresponding to the start of each row in the arrays h_csrColIndA and h_csrValA h_csrColIndA host input The array of column indices corresponding to the non-zero elements in the matrix h_csrValA_array host input Array of pointers of size batchSize , each pointer points to the array of values corresponding to the non-zero elements in the matrix"
  },
  {
    "id": 27467,
    "content": "h_csrRowPtrL host input The array of offsets corresponding to the start of each row in the arrays h_csrColIndL and h_csrValL This array has also an extra entry at the end that stores the number of non-zero elements in the matrix L h_csrColIndL host input The array of column indices corresponding to the non-zero elements in the matrix L h_csrValL host input The array of values corresponding to the"
  },
  {
    "id": 27468,
    "content": "non-zero elements in the matrix L h_csrRowPtrU host input The array of offsets corresponding to the start of each row in the arrays h_csrColIndU and h_csrValU This array has also an extra entry at the end that stores the number of non-zero elements in the matrix U h_csrColIndU host input The array of column indices corresponding to the non-zero elements in the matrix U h_csrValU host input The"
  },
  {
    "id": 27474,
    "content": "cusolverRfBatchAnalyze()  cusolverStatus_t cusolverRfBatchAnalyze ( cusolverRfHandle_t handle ); This routine performs the appropriate analysis of parallelism available in the batched LU re-factorization"
  },
  {
    "id": 27475,
    "content": "It is assumed that a prior call to the cusolverRfBatchSetup[Host]() was done in order to create internal data structures needed for the analysis"
  },
  {
    "id": 27476,
    "content": "The following routine needs to be called only once for a single linear system: \\(A_{j}x_{j} = b_{j}{,\\ j\\ =\\ 1,2,"
  },
  {
    "id": 27477,
    "content": ",\\ batchSize}\\) Parameter Memory In/out Meaning handle host in/out The handle to the cuSolverRF library"
  },
  {
    "id": 27481,
    "content": "cusolverRfBatchResetValues()  cusolverStatus_t cusolverRfBatchResetValues ( /* Input (in the device memory) */ int batchSize , int n , int nnzA , int * csrRowPtrA , int * csrColIndA , double * csrValA_array [], int * P , int * Q , /* Output */ cusolverRfHandle_t handle ); This routine updates internal data structures with the values of the new coefficient matrix"
  },
  {
    "id": 27482,
    "content": "It is assumed that the arrays csrRowPtrA , csrColIndA , P and Q have not changed since the last call to the cusolverRfbatch_setup_host routine"
  },
  {
    "id": 27483,
    "content": "This assumption reflects the fact that the sparsity pattern of coefficient matrices as well as reordering to minimize fill-in and pivoting remain the same in the set of linear systems: \\(A_{j}x_{j} = b_{j}{,\\ j\\ =\\ 1,2,"
  },
  {
    "id": 27484,
    "content": ",\\ batchSize}\\) The input parameter csrValA_array is an array of pointers on device memory Parameter MemSpace In/out Meaning batchSize host input The number of matrices in batched mode csrValA_array device input Array of pointers of size batchSize , each pointer points to the array of values corresponding to the non-zero elements in the matrix"
  },
  {
    "id": 27488,
    "content": "cusolverRfBatchRefactor()  cusolverStatus_t cusolverRfBatchRefactor ( cusolverRfHandle_t handle ); This routine performs the LU re-factorization: \\(M_{j} = P*A_{j}*Q^{T} = L_{j}*U_{j}\\) exploring the available parallelism on the GPU"
  },
  {
    "id": 27489,
    "content": "It is assumed that a prior call to the cusolverRfBatchAnalyze() was done in order to find the available parallelism"
  },
  {
    "id": 27495,
    "content": "cusolverRfBatchSolve()  cusolverStatus_t cusolverRfBatchSolve ( /* Input (in the device memory) */ cusolverRfHandle_t handle , int * P , int * Q , int nrhs , double * Temp , int ldt , /* Input/Output (in the device memory) */ double * XF_array [], /* Input */ int ldxf ); To solve \\(A_{j}*x_{j} = b_{j}\\) , first we reform the equation by \\(M_{j}*Q*x_{j} = P*b_{j}\\) where \\(M_{j} = P*A_{j}*Q^{T}\\)"
  },
  {
    "id": 27496,
    "content": "Further cusolverRfBatch_Solve() takes over the remaining steps, including: \\(z_{j} = P*b_{j}\\) \\(M_{j}*y_{j} = z_{j}\\) \\(x_{j} = Q^{T}*y_{j}\\) The input parameter XF_array is an array of pointers on device memory"
  },
  {
    "id": 27497,
    "content": "If some matrix \\(A_{j}\\) failed the refactorization and \\(U_{j}\\) has some zero diagonal, backward solve would compute NAN The user has to call cusolverRfBatch_Zero_Pivot to check if refactorization is successful or not"
  },
  {
    "id": 27499,
    "content": "XF_array device in/out Array of pointers of size batchSize , each pointer points to the dense matrix that contains the right-hand-sides F and solutions X (of size ldxf*nrhs )"
  },
  {
    "id": 27503,
    "content": "cusolverRfBatchZeroPivot()  cusolverStatus_t cusolverRfBatchZeroPivot ( /* Input */ cusolverRfHandle_t handle /* Output (in the host memory) */ int * position ); Although \\(A_{j}\\) is close to each other, it does not mean \\(M_{j} = P*A_{j}*Q^{T} = L_{j}*U_{j}\\) exists for every j"
  },
  {
    "id": 27504,
    "content": "The user can query which matrix failed LU refactorization by checking corresponding value in position array If position(j) is k >= 0 , matrix \\(A_{j}\\) is not LU factorizable and its matrix \\(U_{j}{(j,j)}\\) is zero The return value of cusolverRfBatch_Zero_Pivot is CUSOLVER_STATUS_ZERO_PIVOT if there exists one \\(A_{j}\\) which failed LU refactorization The user can redo LU factorization to get new"
  },
  {
    "id": 27505,
    "content": "permutation P and Q if error code CUSOLVER_STATUS_ZERO_PIVOT is returned The value of position(j) reports singularity of matrix Aj , -1 if no structural/numerical zero, k >= 0 if Aj(k,k) is either structural zero or numerical zero"
  },
  {
    "id": 27509,
    "content": "It is not a reference for the cuSolverMG API data types and functions; that is provided in subsequent chapters"
  },
  {
    "id": 27517,
    "content": "Determinism  Currently all cuSolverMG API routines from a given toolkit version generate the same bit-wise results when the following conditions are respected : all GPUs participating to the computation have the same compute-capabilities and the same number of SMs The order of GPUs are not important because all have the same compute-capabilities"
  },
  {
    "id": 27522,
    "content": "There are seven columns of tiles, labeled as 0,1,2,3,4,5,6, distributed into three GPUs in a cyclic way, i"
  },
  {
    "id": 27524,
    "content": "For example, GPU 0 has column tile 0, 3, 6 (yellow tiles) and GPU 1 takes column tiles next to GPU 0 (blue tiles) Not all GPUs have the same number of tiles; in this example, GPU 0 has three tiles, others have only two tiles PACKED format aggregates three column tiles in a contiguous memory block while UNPACKED format distributes these three column tiles into different memory blocks The only"
  },
  {
    "id": 27525,
    "content": "difference between them is that PACKED format can have a big GEMM call instead of three GEMM calls in UNPACKED format So theoretically speaking, PACKED format can deliver better performance than UNPACKED format"
  },
  {
    "id": 27526,
    "content": "In order to achieve maximal performance, the user just needs to choose the proper tile size T_A to partition the matrix, not too small, for example 256 or above is enough"
  },
  {
    "id": 27527,
    "content": "There is another parameter, called LLD_A , to control the leading dimension of the local matrix in each GPU"
  },
  {
    "id": 27528,
    "content": "Example of cuSolverMG tiling for 3 GPUs  The processing grid in cuSolverMG is a list of GPU IDs, similar to the process ID in ScaLAPACK The former describes three logical devices that are selected to run cuSolverMG routines, and all have the same physical ID, 0 The current design only accepts 32 logical devices, that is, the length of deviceId is less or equal to 32 If the user chooses"
  },
  {
    "id": 27529,
    "content": "deviceId=1,1,1 , all columns tile are located in GPU 1, this will limit the size of the problem because of memory capacity of one GPU"
  },
  {
    "id": 27530,
    "content": "Besides, multiGPU routine adds extra overhead on data communication through the off-chip bus, which has a big performance impact if NVLINK is not supported or used"
  },
  {
    "id": 27531,
    "content": "It would be faster to run on a single GPU instead of running multiGPU version with devices of the same GPU ID"
  },
  {
    "id": 27535,
    "content": "Global Matrix Versus Local Matrix  Operating a submatrix of the matrix A is simple in dense linear algebra, just shift the pointer to the starting point of the submatrix relative to A However it is not simple to operate on a submatrix of a distributed matrix because different starting point of the submatrix changes the distribution of the layout of that submatrix Given a distributed matrix A ,"
  },
  {
    "id": 27536,
    "content": "the user can compute eigenvalues of the submatrix sub(A) by either calling syevd(A, IA, JA) or gathering sub(A) to another distributed matrix B and calling syevd(B, IB=1, JB=1)"
  },
  {
    "id": 27537,
    "content": "Usage of _bufferSize  There is no cudaMalloc inside cuSolverMG library, so the user must allocate the device workspace explicitly"
  },
  {
    "id": 27538,
    "content": "The routine xyz_bufferSize is to query the size of workspace of the routine xyz , for example xyz = syevd To make the API simple, xyz_bufferSize follows almost the same signature of xyz even it only depends on some parameters, for example, the device pointer is not used to decide the size of workspace In such cases, the user can pass a null pointer to xyz_bufferSize without breaking the"
  },
  {
    "id": 27544,
    "content": "For example, if the user has multiple streams to set up the matrix, stream synchronization or device synchronization is necessary to guarantee the distributed matrix is ready"
  },
  {
    "id": 27548,
    "content": "Context Switch  The user does not need to restore the device by cudaSetDevice() after each cuSolverMG call"
  },
  {
    "id": 27553,
    "content": "NVLINK  The peer-to-peer communication via NVLINK can dramatically reduce the overhead of data exchange among GPUs cuSolverMG does not enable NVLINK implicitly, instead, it gives this option back to the user, not to interfere with other libraries"
  },
  {
    "id": 27561,
    "content": "cuSolverMG Types  The float , double , cuComplex , and cuDoubleComplex data types are supported In addition, cuSolverMG uses some familiar types from cuBLAS"
  },
  {
    "id": 27565,
    "content": "cusolverMgHandle_t  This is a pointer type to an opaque cuSolverMG context, which the user must initialize by calling cusolverMgCreate() prior to calling any other library function"
  },
  {
    "id": 27566,
    "content": "An un-initialized handle object will lead to unexpected behavior, including crashes of cuSolverMG The handle created and returned by cusolverMgCreate() must be passed to every cuSolverMG function"
  },
  {
    "id": 27576,
    "content": "cusolverMgCreate()  cusolverStatus_t cusolverMgCreate ( cusolverMgHandle_t * handle ) This function initializes the cuSolverMG library and creates a handle on the cuSolverMG context"
  },
  {
    "id": 27581,
    "content": "cusolverMgDestroy()  cusolverStatus_t cusolverMgDestroy ( cusolverMgHandle_t handle ) This function releases CPU-side resources used by the cuSolverMG library"
  },
  {
    "id": 27586,
    "content": "cusolverMgDeviceSelect()  cusolverStatus_t cusolverMgDeviceSelect ( cusolverMgHandle_t handle , int nbDevices , int deviceId [] ) This function registers a subset of devices (GPUs) to cuSolverMG handle If the user sets deviceId=0,0,0 , then cuSolverMG treats them as three independent GPUs, one stream each, so concurrent kernel launches still hold"
  },
  {
    "id": 27592,
    "content": "cusolverMgCreateDeviceGrid()  cusolverStatus_t cusolverMgCreateDeviceGrid ( cusolverMgGrid_t * grid , int32_t numRowDevices , int32_t numColDevices , const int32_t deviceId [], cusolverMgGridMapping_t mapping ) This function sets up a grid of devices WARNING: cusolverMgCreateDeviceGrid() must be consistent with cusolverMgDeviceSelect() , i"
  },
  {
    "id": 27598,
    "content": "cusolverMgDestroyGrid()  cusolverStatus_t cusolverMgDestroyGrid ( cusolverMgGrid_t grid ) This function releases resources of a grid"
  },
  {
    "id": 27603,
    "content": "cusolverMgCreateMatDescr()  cusolverStatus_t cusolverMgCreateMatrixDesc ( cusolverMgMatrixDesc_t * desc , int64_t numRows , int64_t numCols , int64_t rowBlockSize , int64_t colBlockSize , cudaDataType_t dataType , const cusolverMgGrid_t grid ) This function sets up the matrix descriptor desc CUSOLVER_STATUS_INVALID_VALUE numRows , numCols , or rowBlockSize or colBlockSize is less than 0 numRows"
  },
  {
    "id": 27608,
    "content": "cusolverMgDestroyMatrixDesc()  cusolverStatus_t cusolverMgDestroyMatrixDesc ( cusolverMgMatrixDesc_t desc ) This function releases the matrix descriptor desc Parameter Memory In/out Meaning desc host input/output The matrix descriptor"
  },
  {
    "id": 27615,
    "content": "cusolverMgPotrf()  The following helper function can calculate the sizes needed for pre-allocated buffer for cusolverMgPotrf : cusolverStatus_t cusolverMgPotrf_bufferSize ( cusolverMgHandle_t handle , cublasFillMode_t uplo , int N , void * array_d_A [], int IA , int JA , cudaLibMgMatrixDesc_t descrA , cudaDataType computeType , int64_t * lwork ) The following routine: cusolverStatus_t"
  },
  {
    "id": 27616,
    "content": "cusolverMgPotrf ( cusolverMgHandle_t handle , cublasFillMode_t uplo , int N , void * array_d_A [], int IA , int JA , cudaLibMgMatrixDesc_t descrA , cudaDataType computeType , void * array_d_work [], int64_t lwork , int * info ) computes the Cholesky factorization of a Hermitian positive-definite matrix using the generic API interface"
  },
  {
    "id": 27617,
    "content": "If input parameter uplo is CUBLAS_FILL_MODE_LOWER , only lower triangular part of A is processed, and replaced by lower triangular Cholesky factor L : \\(A = L*L^{H}\\) If input parameter uplo is CUBLAS_FILL_MODE_UPPER , only upper triangular part of A is processed, and replaced by upper triangular Cholesky factor U : \\(A = U^{H}*U\\) The user has to provide device working space in array_d_work The"
  },
  {
    "id": 27618,
    "content": "size of array_d_work[j] is lwork which is the number of elements per device, returned by cusolverMgPotrf_bufferSize()"
  },
  {
    "id": 27620,
    "content": "The generic API has two different types, dataTypeA is data type of the matrix A , and computeType is compute type of the operation and data type of the workspace ( array_d_work ) descrA contains dataTypeA , so there is no explicit parameter of dataTypeA valid combination of data type and compute type DataTypeA ComputeType Meaning CUDA_R_32F CUDA_R_32F SPOTRF CUDA_R_64F CUDA_R_64F DPOTRF"
  },
  {
    "id": 27621,
    "content": "CUDA_C_32F CUDA_C_32F CPOTRF CUDA_C_64F CUDA_C_64F ZPOTRF API of potrf Parameter Memory In/out Meaning handle host input Handle to the cuSolverMg library context"
  },
  {
    "id": 27622,
    "content": "JA host input The column index in the global array A indicating the first column of sub(A) CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( M,N array containing sub(A) of dimension M * N JB host input The column index in the global array B indicating the first column of sub(B) CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( M,N array containing sub(A) of dimension N *"
  },
  {
    "id": 27623,
    "content": "N On exit, sub(A) contains the upper or lower triangular part of the inverse of A depending on the value of uplo argument CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( N array containing sub(A) of dimension N * N If uplo = CUBLAS_FILL_MODE_UPPER , the leading N-by-N upper triangular part of sub(A) contains the upper triangular part of the matrix sub(A) If uplo ="
  },
  {
    "id": 27624,
    "content": "CUBLAS_FILL_MODE_LOWER , the leading N-by-N lower triangular part of sub(A) contains the lower triangular part of the matrix sub(A) On exit, if jobz = CUSOLVER_EIG_MODE_VECTOR , and info = 0, sub(A) contains the orthonormal eigenvectors of the matrix sub(A) The eigenvalue values of sub(A) , in ascending order ie, sorted so that W(i) array of size lwork"
  },
  {
    "id": 27625,
    "content": "If info = i (> 0) , info indicates i off-diagonal elements of an intermediate tridiagonal form did not converge to zero"
  },
  {
    "id": 27626,
    "content": "CUSOLVER_STATUS_INVALID_VALUE Invalid parameters were passed ( N<0 , or lda<max(1,N) , or jobz is not CUSOLVER_EIG_MODE_NOVECTOR or CUSOLVER_EIG_MODE_VECTOR , or uplo is not CUBLAS_FILL_MODE_LOWER , or IA and JA are not 1, or N is bigger than dimension of global A , or the combination of dataType and computeType is not valid"
  },
  {
    "id": 27628,
    "content": "Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: CPU LAPACK routines from netlib, CLAPACK-3"
  },
  {
    "id": 27635,
    "content": "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer"
  },
  {
    "id": 27637,
    "content": "Neither the name of the copyright holders nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A"
  },
  {
    "id": 27638,
    "content": "PARTICULAR PURPOSE ARE DISCLAIMED IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,"
  },
  {
    "id": 27639,
    "content": "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE"
  },
  {
    "id": 27645,
    "content": "edu/gkhome/metis/metis/overview ) The following is license of METIS (Apache 2 0 license) Copyright 1995-2013, Regents of the University of Minnesota Licensed under the Apache License, Version 2 0 (the “License”); you may not use this file except in compliance with the License You may obtain a copy of the License at http: www apache org/licenses/LICENSE-2 0 Unless required by applicable law or"
  },
  {
    "id": 27646,
    "content": "agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied See the License for the specific language governing permissions and limitations under the License"
  },
  {
    "id": 27650,
    "content": "Copyright (c) 2003-2009, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from U"
  },
  {
    "id": 27652,
    "content": "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the copyright notice, this list of conditions and the following disclaimer Redistributions in binary form must reproduce the copyright notice, this list of conditions and the following disclaimer in the"
  },
  {
    "id": 27656,
    "content": "of Energy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission"
  },
  {
    "id": 27657,
    "content": "You are under no obligation whatsoever to provide any bug fixes, patches, or upgrades to the features, functionality or performance of the source code (“Enhancements”) to anyone; however, if you choose to make your Enhancements available either publicly, or directly to Lawrence Berkeley National Laboratory, without imposing a separate written license agreement for such Enhancements, then you"
  },
  {
    "id": 27658,
    "content": "hereby grant the following license: a non-exclusive, royalty-free perpetual license to install, use, modify, prepare derivative works, incorporate into other computer software, distribute, and sublicense such enhancements or derivative works thereof, in binary and source code form"
  },
  {
    "id": 27661,
    "content": "McKee, reducing the bandwidth of sparse symmetric matrices, ACM ‘69 Proceedings of the 1969 24th national conference, Pages 157-172"
  },
  {
    "id": 27662,
    "content": "Liu, An Implementation of a Pseudoperipheral Node Finder, ACM Transactions on Mathematical Software (TOMS) Volume 5 Issue 3, Sept"
  },
  {
    "id": 27666,
    "content": "[5] Alan George and Esmond Ng, An Implementation of Gaussian Elimination with Partial Pivoting for Sparse Systems, SIAM J [6] Alan George and Esmond Ng, Symbolic Factorization for Sparse Gaussian Elimination with Partial Pivoting, SIAM J"
  },
  {
    "id": 27668,
    "content": "Liu, A Fast Implementation of the Minimum Degree Algorithm Using Quotient Graphs, ACM Transactions on Mathematical Software, Vol 6, No"
  },
  {
    "id": 27669,
    "content": "Liu, Computer Solution of Large Sparse Positive Definite Systems, Englewood Cliffs, New Jersey: Prentice-Hall, 1981"
  },
  {
    "id": 27670,
    "content": "Duff, ALGORITHM 575 Permutations for a Zero-Free Diagonal, ACM Transactions on Mathematical Software, Vol 7, No 3, September 1981, Page 387-390 [12] Iain S Duff and Jacko Koster, On algorithms for permuting large entries to the diagonal of a sparse matrix, SIAM Journal on Matrix Analysis and Applications, 2001, Vol"
  },
  {
    "id": 27674,
    "content": "[14] YUJI NAKATSUKASA, ZHAOJUN BAI, AND FRANC¸OIS GYGI, OPTIMIZING HALLEY’S ITERATION FOR COMPUTING THE MATRIX POLAR DECOMPOSITION, SIAM J"
  },
  {
    "id": 27675,
    "content": "“Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions"
  },
  {
    "id": 27681,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 27682,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 27684,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 27685,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 27686,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 27687,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 27688,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 27689,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 27690,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 27691,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 27692,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 27693,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 27694,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 27701,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 27703,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 27704,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 27709,
    "content": "It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx , and linked with other modules by using the nvJitLink library or using cuLinkAddData of the CUDA Driver API"
  },
  {
    "id": 27710,
    "content": "This facility can often provide optimizations and performance not possible in a purely offline static compilation"
  },
  {
    "id": 27711,
    "content": "In the absence of NVRTC (or any runtime compilation support in CUDA), users needed to spawn a separate process to execute nvcc at runtime if they wished to implement runtime compilation in their applications or libraries, and, unfortunately, this approach has the following drawbacks: The compilation overhead tends to be higher than necessary End users are required to install nvcc and related"
  },
  {
    "id": 27713,
    "content": "NVRTC addresses these issues by providing a library interface that eliminates overhead associated with spawning separate processes, disk I/O,and so on, while keeping application deployment simple"
  },
  {
    "id": 27717,
    "content": "System Requirements  NVRTC requires the following system configuration: Operating System: Linux x86_64, Linux ppc64le, Linux aarch64 or Windows x86_64"
  },
  {
    "id": 27721,
    "content": "Installation  NVRTC is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vrtc"
  },
  {
    "id": 27722,
    "content": "h bin vrtc64_Major Release VersionMinor Release Version_0 dll bin vrtc-builtins64_Major Release VersionMinor Release Version dll lib\\x64 vrtc lib lib\\x64 vrtc_static lib lib\\x64 vrtc-builtins_static lib doc\\pdf\\NVRTC_User_Guide pdf On Linux: include/nvrtc"
  },
  {
    "id": 27733,
    "content": "Error Handling  NVRTC defines the following enumeration type and function for API call error handling"
  },
  {
    "id": 27734,
    "content": "Functions const char * nvrtcGetErrorString (nvrtcResult result) nvrtcGetErrorString is a helper function that returns a string describing the given nvrtcResult code, e"
  },
  {
    "id": 27741,
    "content": "Values: enumerator NVRTC_SUCCESS  enumerator NVRTC_ERROR_OUT_OF_MEMORY  enumerator NVRTC_ERROR_PROGRAM_CREATION_FAILURE  enumerator NVRTC_ERROR_INVALID_INPUT  enumerator NVRTC_ERROR_INVALID_PROGRAM  enumerator NVRTC_ERROR_INVALID_OPTION  enumerator NVRTC_ERROR_COMPILATION  enumerator NVRTC_ERROR_BUILTIN_OPERATION_FAILURE  enumerator NVRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION "
  },
  {
    "id": 27742,
    "content": "enumerator NVRTC_ERROR_NO_LOWERED_NAMES_BEFORE_COMPILATION  enumerator NVRTC_ERROR_NAME_EXPRESSION_NOT_VALID  enumerator NVRTC_ERROR_INTERNAL_ERROR  enumerator NVRTC_ERROR_TIME_FILE_WRITE_FAILED  3"
  },
  {
    "id": 27745,
    "content": "Functions  const char * nvrtcGetErrorString ( nvrtcResult result )  nvrtcGetErrorString is a helper function that returns a string describing the given nvrtcResult code, e"
  },
  {
    "id": 27752,
    "content": "Functions nvrtcResult nvrtcGetNumSupportedArchs (int *numArchs) nvrtcGetNumSupportedArchs sets the output parameter numArchs with the number of architectures supported by NVRTC nvrtcResult nvrtcGetSupportedArchs (int *supportedArchs) nvrtcGetSupportedArchs populates the array passed via the output parameter supportedArchs with the architectures supported by NVRTC nvrtcResult nvrtcVersion (int"
  },
  {
    "id": 27753,
    "content": "*major, int *minor) nvrtcVersion sets the output parameters major and minor with the CUDA Runtime Compilation version number"
  },
  {
    "id": 27757,
    "content": "Functions  nvrtcResult nvrtcGetNumSupportedArchs ( int * numArchs )  nvrtcGetNumSupportedArchs sets the output parameter numArchs with the number of architectures supported by NVRTC"
  },
  {
    "id": 27758,
    "content": "This can then be used to pass an array to nvrtcGetSupportedArchs to get the supported architectures Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT nvrtcResult nvrtcGetSupportedArchs ( int * supportedArchs )  nvrtcGetSupportedArchs populates the array passed via the output parameter supportedArchs with the architectures supported by NVRTC see nvrtcGetNumSupportedArchs Parameters supportedArchs –"
  },
  {
    "id": 27759,
    "content": "[out] sorted array of supported architectures Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT nvrtcResult nvrtcVersion ( int * major , int * minor )  nvrtcVersion sets the output parameters major and minor with the CUDA Runtime Compilation version number"
  },
  {
    "id": 27760,
    "content": "Functions nvrtcResult nvrtcAddNameExpression (nvrtcProgram prog, const char *const name_expression) nvrtcAddNameExpression notes the given name expression denoting the address of a global function or device /__constant__ variable nvrtcResult nvrtcCompileProgram (nvrtcProgram prog, int numOptions, const char *const *options) nvrtcCompileProgram compiles the given program nvrtcResult"
  },
  {
    "id": 27761,
    "content": "nvrtcCreateProgram (nvrtcProgram *prog, const char *src, const char *name, int numHeaders, const char *const *headers, const char *const *includeNames) nvrtcCreateProgram creates an instance of nvrtcProgram with the given input parameters, and sets the output parameter prog with it nvrtcResult nvrtcDestroyProgram (nvrtcProgram *prog) nvrtcDestroyProgram destroys the given program nvrtcResult"
  },
  {
    "id": 27762,
    "content": "nvrtcGetCUBIN (nvrtcProgram prog, char *cubin) nvrtcGetCUBIN stores the cubin generated by the previous compilation of prog in the memory pointed by cubin nvrtcResult nvrtcGetCUBINSize (nvrtcProgram prog, size_t *cubinSizeRet) nvrtcGetCUBINSize sets the value of cubinSizeRet with the size of the cubin generated by the previous compilation of prog nvrtcResult nvrtcGetLTOIR (nvrtcProgram prog, char"
  },
  {
    "id": 27763,
    "content": "*LTOIR) nvrtcGetLTOIR stores the LTO IR generated by the previous compilation of prog in the memory pointed by LTOIR nvrtcResult nvrtcGetLTOIRSize (nvrtcProgram prog, size_t *LTOIRSizeRet) nvrtcGetLTOIRSize sets the value of LTOIRSizeRet with the size of the LTO IR generated by the previous compilation of prog nvrtcResult nvrtcGetLoweredName (nvrtcProgram prog, const char *const name_expression,"
  },
  {
    "id": 27764,
    "content": "const char **lowered_name) nvrtcGetLoweredName extracts the lowered (mangled) name for a global function or device /__constant__ variable, and updates *lowered_name to point to it nvrtcResult nvrtcGetNVVM (nvrtcProgram prog, char *nvvm) DEPRECATION NOTICE: This function will be removed in a future release nvrtcResult nvrtcGetNVVMSize (nvrtcProgram prog, size_t *nvvmSizeRet) DEPRECATION NOTICE:"
  },
  {
    "id": 27765,
    "content": "This function will be removed in a future release nvrtcResult nvrtcGetOptiXIR (nvrtcProgram prog, char *optixir) nvrtcGetOptiXIR stores the OptiX IR generated by the previous compilation of prog in the memory pointed by optixir nvrtcResult nvrtcGetOptiXIRSize (nvrtcProgram prog, size_t *optixirSizeRet) nvrtcGetOptiXIRSize sets the value of optixirSizeRet with the size of the OptiX IR generated by"
  },
  {
    "id": 27766,
    "content": "the previous compilation of prog nvrtcResult nvrtcGetPTX (nvrtcProgram prog, char *ptx) nvrtcGetPTX stores the PTX generated by the previous compilation of prog in the memory pointed by ptx nvrtcResult nvrtcGetPTXSize (nvrtcProgram prog, size_t *ptxSizeRet) nvrtcGetPTXSize sets the value of ptxSizeRet with the size of the PTX generated by the previous compilation of prog (including the trailing"
  },
  {
    "id": 27767,
    "content": "NULL ) nvrtcResult nvrtcGetProgramLog (nvrtcProgram prog, char *log) nvrtcGetProgramLog stores the log generated by the previous compilation of prog in the memory pointed by log nvrtcResult nvrtcGetProgramLogSize (nvrtcProgram prog, size_t *logSizeRet) nvrtcGetProgramLogSize sets logSizeRet with the size of the log generated by the previous compilation of prog (including the trailing NULL )"
  },
  {
    "id": 27772,
    "content": "Functions  nvrtcResult nvrtcAddNameExpression ( nvrtcProgram prog , const char * const name_expression )  nvrtcAddNameExpression notes the given name expression denoting the address of a global function or device /__constant__ variable"
  },
  {
    "id": 27773,
    "content": "The identical name expression string must be provided on a subsequent call to nvrtcGetLoweredName to extract the lowered name"
  },
  {
    "id": 27774,
    "content": "name_expression – [in] constant expression denoting the address of a global function or device /__constant__ variable"
  },
  {
    "id": 27775,
    "content": "Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_PROGRAM NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_NO_NAME_EXPRESSIONS_AFTER_COMPILATION nvrtcResult nvrtcCompileProgram ( nvrtcProgram prog , int numOptions , const char * const * options )  nvrtcCompileProgram compiles the given program Returns NVRTC_SUCCESS NVRTC_ERROR_OUT_OF_MEMORY NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM"
  },
  {
    "id": 27776,
    "content": "NVRTC_ERROR_INVALID_OPTION NVRTC_ERROR_COMPILATION NVRTC_ERROR_BUILTIN_OPERATION_FAILURE NVRTC_ERROR_TIME_FILE_WRITE_FAILED nvrtcResult nvrtcCreateProgram ( nvrtcProgram * prog , const char * src , const char * name , int numHeaders , const char * const * headers , const char * const * includeNames )  nvrtcCreateProgram creates an instance of nvrtcProgram with the given input parameters, and sets"
  },
  {
    "id": 27779,
    "content": "Returns NVRTC_SUCCESS NVRTC_ERROR_OUT_OF_MEMORY NVRTC_ERROR_PROGRAM_CREATION_FAILURE NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcDestroyProgram ( nvrtcProgram * prog )  nvrtcDestroyProgram destroys the given program Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetCUBIN ( nvrtcProgram prog , char * cubin )  nvrtcGetCUBIN stores the cubin generated"
  },
  {
    "id": 27781,
    "content": "No cubin is available if the value specified to -arch is a virtual architecture instead of an actual architecture"
  },
  {
    "id": 27782,
    "content": "Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetCUBINSize ( nvrtcProgram prog , size_t * cubinSizeRet )  nvrtcGetCUBINSize sets the value of cubinSizeRet with the size of the cubin generated by the previous compilation of prog The value of cubinSizeRet is set to 0 if the value specified to -arch is a virtual architecture instead of an actual"
  },
  {
    "id": 27783,
    "content": "architecture Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetLTOIR ( nvrtcProgram prog , char * LTOIR )  nvrtcGetLTOIR stores the LTO IR generated by the previous compilation of prog in the memory pointed by LTOIR Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetLTOIRSize ( nvrtcProgram prog , size_t *"
  },
  {
    "id": 27784,
    "content": "LTOIRSizeRet )  nvrtcGetLTOIRSize sets the value of LTOIRSizeRet with the size of the LTO IR generated by the previous compilation of prog Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetLoweredName ( nvrtcProgram prog , const char * const name_expression , const char * * lowered_name )  nvrtcGetLoweredName extracts the lowered (mangled) name for a"
  },
  {
    "id": 27786,
    "content": "The memory containing the name is released when the NVRTC program is destroyed by nvrtcDestroyProgram"
  },
  {
    "id": 27787,
    "content": "The identical name expression must have been previously provided to nvrtcAddNameExpression lowered_name – [out] initialized by the function to point to a C string containing the lowered (mangled) name corresponding to the provided name expression"
  },
  {
    "id": 27788,
    "content": "Returns NVRTC_SUCCESS NVRTC_ERROR_NO_LOWERED_NAMES_BEFORE_COMPILATION NVRTC_ERROR_INVALID_PROGRAM NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_NAME_EXPRESSION_NOT_VALID nvrtcResult nvrtcGetNVVM ( nvrtcProgram prog , char * nvvm )  DEPRECATION NOTICE: This function will be removed in a future release nvrtcResult nvrtcGetNVVMSize ( nvrtcProgram prog , size_t * nvvmSizeRet )  DEPRECATION NOTICE: This"
  },
  {
    "id": 27789,
    "content": "function will be removed in a future release nvrtcResult nvrtcGetOptiXIR ( nvrtcProgram prog , char * optixir )  nvrtcGetOptiXIR stores the OptiX IR generated by the previous compilation of prog in the memory pointed by optixir No OptiX IR is available if the program was compiled with options incompatible with OptiX IR generation Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT"
  },
  {
    "id": 27790,
    "content": "NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetOptiXIRSize ( nvrtcProgram prog , size_t * optixirSizeRet )  nvrtcGetOptiXIRSize sets the value of optixirSizeRet with the size of the OptiX IR generated by the previous compilation of prog The value of nvrtcGetOptiXIRSize is set to 0 if the program was compiled with options incompatible with OptiX IR generation Returns NVRTC_SUCCESS"
  },
  {
    "id": 27791,
    "content": "NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetPTX ( nvrtcProgram prog , char * ptx )  nvrtcGetPTX stores the PTX generated by the previous compilation of prog in the memory pointed by ptx Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetPTXSize ( nvrtcProgram prog , size_t * ptxSizeRet )  nvrtcGetPTXSize sets the value of"
  },
  {
    "id": 27792,
    "content": "ptxSizeRet with the size of the PTX generated by the previous compilation of prog (including the trailing NULL ) Returns NVRTC_SUCCESS NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetProgramLog ( nvrtcProgram prog , char * log )  nvrtcGetProgramLog stores the log generated by the previous compilation of prog in the memory pointed by log Returns NVRTC_SUCCESS"
  },
  {
    "id": 27793,
    "content": "NVRTC_ERROR_INVALID_INPUT NVRTC_ERROR_INVALID_PROGRAM nvrtcResult nvrtcGetProgramLogSize ( nvrtcProgram prog , size_t * logSizeRet )  nvrtcGetProgramLogSize sets logSizeRet with the size of the log generated by the previous compilation of prog (including the trailing NULL ) Note that compilation log may be generated with warnings and informative messages, even when the compilation of prog"
  },
  {
    "id": 27794,
    "content": "succeeds Typedefs  typedef struct _nvrtcProgram * nvrtcProgram  nvrtcProgram is the unit of compilation, and an opaque handle for a program To compile a CUDA program string, an instance of nvrtcProgram must be created first with nvrtcCreateProgram , then compiled with nvrtcCompileProgram"
  },
  {
    "id": 27798,
    "content": "Option names with two preceding dashs ( -- ) are long option names and option names with one preceding dash ( - ) are short option names When a compile option takes an argument, an assignment operator ( = ) is used to separate the compile option argument from the compile option name, e"
  },
  {
    "id": 27801,
    "content": "Alternatively, the compile option name and the argument can be specified in separate strings without an assignment operator,"
  },
  {
    "id": 27804,
    "content": "Single-character short option names, such as -D , -U , and -I , do not require an assignment operator, and the compile option name and the argument can be present in the same string with or without spaces between them"
  },
  {
    "id": 27805,
    "content": "The valid compiler options are: Compilation targets --gpu-architecture= ( -arch ) Specify the name of the class of GPU architectures for which the input must be compiled"
  },
  {
    "id": 27806,
    "content": "Valid s: compute_50 compute_52 compute_53 compute_60 compute_61 compute_62 compute_70 compute_72 compute_75 compute_80 compute_87 compute_89 compute_90 compute_90a sm_50 sm_52 sm_53 sm_60 sm_61 sm_62 sm_70 sm_72 sm_75 sm_80 sm_87 sm_89 sm_90 sm_90a Default: compute_52 Separate compilation / whole-program compilation --device-c ( -dc ) Generate relocatable code that can be linked with other"
  },
  {
    "id": 27807,
    "content": "relocatable device code --relocatable-device-code={true|false} ( -rdc ) Enable (disable) the generation of relocatable device code Default: false --extensible-whole-program ( -ewp ) Do extensible whole program compilation of device code When specified along with ‘-G’, enables limited debug information generation for optimized device code (currently, only line number information)"
  },
  {
    "id": 27808,
    "content": "--ptxas-options ( -Xptxas ) --ptxas-options= Specify options directly to ptxas, the PTX optimizing assembler"
  },
  {
    "id": 27809,
    "content": "--maxrregcount= ( -maxrregcount ) Specify the maximum amount of registers that GPU functions can use"
  },
  {
    "id": 27810,
    "content": "Until a function-specific limit, a higher value will generally increase the performance of individual GPU threads that execute this function However, because thread registers are allocated from a global register pool on each GPU, a higher value of this option will also reduce the maximum thread block size, thereby reducing the amount of thread parallelism"
  },
  {
    "id": 27811,
    "content": "Value less than the minimum registers required by ABI will be bumped up by the compiler to ABI minimum limit"
  },
  {
    "id": 27812,
    "content": "--ftz={true|false} ( -ftz ) When performing single-precision floating-point operations, flush denormal values to zero or preserve denormal values Default: false --prec-sqrt={true|false} ( -prec-sqrt ) For single-precision floating-point square root, use IEEE round-to-nearest mode or use a faster approximation Default: true --prec-div={true|false} ( -prec-div ) For single-precision floating-point"
  },
  {
    "id": 27813,
    "content": "division and reciprocals, use IEEE round-to-nearest mode or use a faster approximation Default: true --fmad={true|false} ( -fmad ) Enables (disables) the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations (FMAD, FFMA, or DFMA)"
  },
  {
    "id": 27814,
    "content": "--extra-device-vectorization ( -extra-device-vectorization ) Enables more aggressive device code vectorization in the NVVM optimizer"
  },
  {
    "id": 27815,
    "content": "--modify-stack-limit={true|false} ( -modify-stack-limit ) On Linux, during compilation, use setrlimit() to increase stack size to maximum allowed"
  },
  {
    "id": 27816,
    "content": "Default: true --dlink-time-opt ( -dlto ) Generate intermediate code for later link-time optimization"
  },
  {
    "id": 27817,
    "content": "Note: when this option is used the nvrtcGetLTOIR API should be used, as PTX or Cubin will not be generated Note: when this option is used the nvrtcGetOptiX API should be used, as PTX or Cubin will not be generated"
  },
  {
    "id": 27818,
    "content": "--jump-table-density= [0-101] ( -jtd ) Specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx"
  },
  {
    "id": 27820,
    "content": "= The contents of are tokenized and preprocessed as if they appeared during translation phase three in a #define directive"
  },
  {
    "id": 27821,
    "content": "--include-path= ( -I ) Add the directory to the list of directories to be searched for headers --no-source-include ( -no-source-include ) The preprocessor by default adds the directory of each input sources to the include path"
  },
  {
    "id": 27822,
    "content": "Language Dialect --std={c++03|c++11|c++14|c++17|c++20} ( -std={c++11|c++14|c++17|c++20} ) Set language dialect to C++03, C++11, C++14, C++17 or C++20 Default: c++17 --builtin-move-forward={true|false} ( -builtin-move-forward ) Provide builtin definitions of std::move and std::forward , when C++11 or later language dialect is selected Default: true --builtin-initializer-list={true|false} ("
  },
  {
    "id": 27823,
    "content": "-builtin-initializer-list ) Provide builtin definitions of std::initializer_list class and member functions when C++11 or later language dialect is selected"
  },
  {
    "id": 27824,
    "content": "--restrict ( -restrict ) Programmer assertion that all kernel pointer parameters are restrict pointers"
  },
  {
    "id": 27825,
    "content": "--device-as-default-execution-space ( -default-device ) Treat entities with no execution space annotation as __device__ entities"
  },
  {
    "id": 27826,
    "content": "--optimization-info= ( -opt-info ) Provide optimization reports for the specified kind of optimization"
  },
  {
    "id": 27827,
    "content": "(Default) --no-display-error-number ( -no-err-no ) Disables the display of a diagnostic number for warning messages"
  },
  {
    "id": 27828,
    "content": "--brief-diagnostics={true|false} ( -brief-diag ) This option disables or enables showing source line and column info in a diagnostic"
  },
  {
    "id": 27829,
    "content": "Default: false --time= ( -time ) Generate a comma separated value table with the time taken by each compilation phase, and append it at the end of the file given as the option argument If the file does not exist, the column headings are generated in the first row of the table Split compilation attempts to reduce compile time by enabling the compiler to run certain optimization passes concurrently"
  },
  {
    "id": 27830,
    "content": "This option accepts a numerical value that specifies the maximum number of threads the compiler can use One can also allow the compiler to use the maximum threads available on the system by setting —split-compile=0"
  },
  {
    "id": 27831,
    "content": "--fdevice-syntax-only ( -fdevice-syntax-only ) Ends device compilation after front-end syntax checking"
  },
  {
    "id": 27836,
    "content": "CUDA Runtime Functions that are provided by the cudadevrt device code library, typically named with prefix “cuda”, e"
  },
  {
    "id": 27839,
    "content": "Types and macros associated with CUDA Runtime and Driver APIs, provided by cuda/tools/cudart/driver_types"
  },
  {
    "id": 27846,
    "content": "Functions nvrtcResult nvrtcGetTypeName (const std::type_info &tinfo, std::string *result) nvrtcGetTypeName stores the source level name of a type in the given std::string location nvrtcResult nvrtcGetTypeName (std::string *result) nvrtcGetTypeName stores the source level name of the template type argument T in the given std::string location"
  },
  {
    "id": 27850,
    "content": "Functions  inline nvrtcResult nvrtcGetTypeName ( const std :: type_info & tinfo , std :: string * result )  nvrtcGetTypeName stores the source level name of a type in the given std::string location"
  },
  {
    "id": 27852,
    "content": "It uses abi::__cxa_demangle or UnDecorateSymbolName function calls to extract the type name, when using gcc/clang or cl"
  },
  {
    "id": 27854,
    "content": "If the name extraction fails, it will return NVRTC_INTERNAL_ERROR, otherwise *result is initialized with the extracted name"
  },
  {
    "id": 27855,
    "content": "Windows-specific notes: nvrtcGetTypeName() is not multi-thread safe because it calls UnDecorateSymbolName(), which is not multi-thread safe"
  },
  {
    "id": 27856,
    "content": "Returns NVRTC_SUCCESS NVRTC_ERROR_INTERNAL_ERROR template nvrtcResult nvrtcGetTypeName ( std :: string * result )  nvrtcGetTypeName stores the source level name of the template type argument T in the given std::string location"
  },
  {
    "id": 27861,
    "content": "Execution Space  NVRTC uses __host__ as the default execution space, and it generates an error if it encounters any host code in the input That is, if the input contains entities with explicit __host__ annotations or no execution space annotation, NVRTC will emit an error NVRTC provides a compile option, --device-as-default-execution-space (refer to Supported Compile Options ), that enables an"
  },
  {
    "id": 27862,
    "content": "alternative compilation mode, in which entities with no execution space annotations are treated as __device__ entities"
  },
  {
    "id": 27866,
    "content": "Users can, however, use the nvJitLink library or cuLinkAddData in the CUDA Driver API to link the generated relocatable PTX code with other relocatable code To generate relocatable PTX code, the compile option --relocatable-device-code=true or --device-c is required"
  },
  {
    "id": 27869,
    "content": "Dynamic Parallelism  NVRTC supports dynamic parallelism under the following conditions: Compilation target must be compute 35 or higher"
  },
  {
    "id": 27870,
    "content": "Either separate compilation ( --relocatable-device-code=true or --device-c ) or extensible whole program compilation ( --extensible-whole-program ) must be enabled Generated PTX must be linked against the CUDA device runtime (cudadevrt) library (refer to Separate Compilation )"
  },
  {
    "id": 27875,
    "content": "Integer sizes in bits for LLP64 and LP64  short int long long long pointers and size_t LLP64 16 32 32 64 64 LP64 16 32 64 64 64 NVRTC implements LP64 on Linux and LLP64 on Windows"
  },
  {
    "id": 27879,
    "content": "Include Syntax  When nvrtcCompileProgram() is called, the current working directory is added to the header search path used for locating files included with the quoted syntax (for example, #include \"foo"
  },
  {
    "id": 27883,
    "content": "Predefined Macros  __CUDACC_RTC__ : useful for distinguishing between runtime and offline nvcc compilation in user code"
  },
  {
    "id": 27884,
    "content": "__CUDACC_VER_MAJOR__ : defined with the major version number as returned by nvrtcVersion __CUDACC_VER_MINOR__ : defined with the minor version number as returned by nvrtcVersion __NVCC_DIAG_PRAGMA_SUPPORT__ : defined with same semantics as with offline nvcc compilation"
  },
  {
    "id": 27885,
    "content": "__CUDACC_RTC_INT128__ : defined when -device-int128 flag is specified during compilation, and indicates that __int128 type is supported"
  },
  {
    "id": 27894,
    "content": "Predefined Types  clock_t size_t ptrdiff_t va_list : Note that the definition of this type may be different than the one selected by nvcc when compiling CUDA code Predefined types such as dim3 , char4 , etc , that are available in the CUDA Runtime headers when compiling offline with nvcc are also available, unless otherwise noted"
  },
  {
    "id": 27895,
    "content": "std::initializer_list : implicitly provided in C++11 and later dialects, unless -builtin-initializer-list=false is specified std::move, std::forward : implicitly provided in C++11 and later dialects, unless -builtin-move-forward=false is specified"
  },
  {
    "id": 27898,
    "content": "Builtin Functions  Builtin functions provided by the CUDA Runtime headers when compiling offline with nvcc are available, unless otherwise noted"
  },
  {
    "id": 27901,
    "content": "Basic Usage  This section of the document uses a simple example, Single-Precision α⋅X Plus Y (SAXPY), shown in Figure 1 to explain what is involved in runtime compilation with NVRTC CUDA source string for SAXPY const char * saxpy = \"   \\ extern \\\" C \\\" __global__   \\ void saxpy(float a, float *x, float *y, float *out, size_t n)   \\ {   \\ size_t tid = blockIdx"
  },
  {
    "id": 27909,
    "content": "Alternatively, the compile option -I can be used if the header is guaranteed to exist in the file system at runtime"
  },
  {
    "id": 27910,
    "content": "Once the instance of nvrtcProgram for compilation is created, it can be compiled by nvrtcCompileProgram as shown in Figure 3"
  },
  {
    "id": 27911,
    "content": "Two compile options are used in this example, --gpu-architecture=compute_80 and --fmad=false , to generate code for the compute_80 architecture and to disable the contraction of floating-point multiplies and adds/subtracts into floating-point multiply-add operations Other combinations of compile options can be used as needed and Supported Compile Options lists valid compile options Compilation of"
  },
  {
    "id": 27912,
    "content": "SAXPY for compute_80 with FMAD enabled const char * opts [] = { \"--gpu-architecture=compute_80\" , \"--fmad=false\" }; nvrtcCompileProgram ( prog , prog 2 , numOptions opts ); options After the compilation completes, users can obtain the program compilation log and the generated PTX as Figure 4 shows NVRTC does not generate valid PTX when the compilation fails, and it may generate program compilation"
  },
  {
    "id": 27913,
    "content": "log even when the compilation succeeds if needed An nvrtcProgram can be compiled by nvrtcCompileProgram multiple times with different compile options, and users can only retrieve the PTX and the log generated by the last compilation Obtaining generated PTX and program compilation log Obtain compilation log from the program size_t logSize ; nvrtcGetProgramLogSize ( prog , & logSize ); char * log ="
  },
  {
    "id": 27914,
    "content": "new char [ logSize ]; nvrtcGetProgramLog ( prog , log ); Obtain PTX from the program size_t ptxSize ; nvrtcGetPTXSize ( prog , & ptxSize ); char * ptx = new char [ ptxSize ]; nvrtcGetPTX ( prog , ptx ); When the instance of nvrtcProgram is no longer needed, it can be destroyed by nvrtcDestroyProgram as shown in Figure 5 Destruction of nvrtcProgram nvrtcDestroyProgram ( & prog ); The generated PTX"
  },
  {
    "id": 27916,
    "content": "Accessing Lowered Names  NVRTC will mangle __global__ function names and names of __device__ and __constant__ variables as specified by the IA64 ABI"
  },
  {
    "id": 27917,
    "content": "If the generated PTX is being loaded using the CUDA Driver API, the kernel function or __device__ / __constant__ variable must be looked up by name, but this is hard to do when the name has been mangled To address this problem, NVRTC provides API functions that map source level __global__ function or __device__ / __constant__ variable names to the mangled names present in the generated PTX"
  },
  {
    "id": 27918,
    "content": "The two API functions nvrtcAddNameExpression and nvrtcGetLoweredName work together to provide this functionality First, a ‘name expression’ string denoting the address for the __global__ function or __device__ / __constant__ variable is provided to nvrtcAddNameExpression During compilation, NVRTC will parse the name expression string as a C++ constant expression at the end of the user program The"
  },
  {
    "id": 27919,
    "content": "constant expression must provide the address of the __global__ function or __device__ / __constant__ variable Finally, the function nvrtcGetLoweredName is called with the original name expression and it returns a pointer to the lowered name NVRTC guarantees that any __global__ function or __device__/__constant__ variable referenced in a call to nvrtcAddNameExpression will be present in the"
  },
  {
    "id": 27924,
    "content": "Some relevant snippets: The GPU source code (‘gpu_program’) contains definitions of various __global__ functions/function templates and __device__ / __constant__ variables: const char * gpu_program = \" \\ __device__ int V1; set from host code \\ static __global__ void f1(int *result) { *result = V1 + 10; } \\ namespace N1 { \\ namespace N2 { \\ __constant__ int V2; set from host code \\ __global__ void"
  },
  {
    "id": 27925,
    "content": "f2(int *result) { *result = V2 + 20; } \\ } \\ } \\ template \\ __global__ void f3(int *result) { *result = sizeof(T); } \\ The host source code invokes nvrtcAddNameExpression with various name expressions referring to the address of __global__ functions and __device__ / __constant__ variables: kernel_name_vec for ( size_t i = 0 ; i \\ __global__ void f3(int *result) { *result = sizeof(T); } \\ \" ; The"
  },
  {
    "id": 27926,
    "content": "host code function getKernelNameForType creates the name expression for a __global__ function template instantiation based on the host template type T The name of the type T is extracted using nvrtcGetTypeName : template std :: string getKernelNameForType ( void ) { Look up the source level name string for the type \"T\" using nvrtcGetTypeName() and use it to create the kernel name std :: string"
  },
  {
    "id": 27927,
    "content": "type_name ; NVRTC_SAFE_CALL ( nvrtcGetTypeName ( & type_name )); return std :: string ( \"f3\" ; } The name expressions are presented to NVRTC using the nvrtcAddNameExpression function: name_vec"
  },
  {
    "id": 27929,
    "content": "3, the DLL name was of the form “nvrtc64_XY_0 dll”, where X = MAJOR, Y = MINOR The NVRTC shared library in this CUDA toolkit will have the same soname (Linux) or DLL name (Windows) as an NVRTC shared library in a previous minor version of the same CUDA toolkit Similarly, the NVRTC shared library in CUDA 11 3 and later 11 x releases will have the same soname (Linux) or DLL name (Windows) as the"
  },
  {
    "id": 27932,
    "content": "As a consequence of the versioning scheme described above, an NVRTC client that links against a particular NVRTC shared library will continue to work with a future NVRTC shared library with a matching soname (Linux) or DLL name (Windows) This allows the NVRTC client to take advantage of bug fixes and enhancements available in the more recent NVRTC shared library 1 However, the more recent NVRTC"
  },
  {
    "id": 27933,
    "content": "shared library may generate PTX with a version that is not accepted by the CUDA Driver API functions of an older CUDA driver, as explained in the Best Practices Guide Some approaches to resolving this issue: Install a more recent CUDA driver that is compatible with the CUDA toolkit containing the NVRTC library being used Alternately, an NVRTC client can either link against the static NVRTC library"
  },
  {
    "id": 27934,
    "content": "or redistribute a specific version of the NVRTC shared library and use dlopen (Linux) or LoadLibrary (Windows) functions to use that library at run time Either approach allows the NVRTC client to maintain control over the version of NVRTC being used during deployment, to ensure predictable functionality and performance"
  },
  {
    "id": 27937,
    "content": "NVRTC-builtins Library  The NVRTC-builtins library contains helper code that is part of the NVRTC package Each NVRTC library is only compatible with the NVRTC-builtins library from the same CUDA toolkit"
  },
  {
    "id": 27941,
    "content": "Thread Safety  Multiple threads can invoke NVRTC API functions concurrently, as long as there is no race condition In this context, a race condition is defined to occur if multiple threads concurrently invoke NVRTC API functions with the same nvrtcProgram argument, where at least one thread is invoking either nvrtcCompileProgram or nvrtcAddNameExpression 2"
  },
  {
    "id": 27943,
    "content": "3, NVRTC allows concurrent invocations of nvrtcCompileProgram to potentially concurrently also invoke the embedded NVVM optimizer/codegen phase"
  },
  {
    "id": 27949,
    "content": "Stack Size  On Linux, NVRTC will increase the stack size to the maximum allowed using the setrlimit() function during compilation This reduces the chance that the compiler will run out of stack when processing complex input sources Because setrlimit() changes the stack size for the entire process, it will also affect other application threads that may be executing concurrently The command line"
  },
  {
    "id": 27953,
    "content": "NVRTC Static Library  The NVRTC static library references functions defined in the NVRTC-builtins static library and the PTX compiler static library"
  },
  {
    "id": 27955,
    "content": "cpp)  #include #include #include #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x) \\ do { \\ nvrtcResult result = x; \\ if (result"
  },
  {
    "id": 27956,
    "content": "= NVRTC_SUCCESS) { \\ std::cerr ( i ); hY [ i ] = static_cast ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize )); Execute SAXPY void *"
  },
  {
    "id": 27957,
    "content": "args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 , grid dim NUM_THREADS , 1 , 1 , block dim 0 , NULL , shared mem and stream args , 0 )); arguments CUDA_SAFE_CALL ( cuCtxSynchronize ()); Retrieve and print output CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i #include #include #include #include"
  },
  {
    "id": 27959,
    "content": "= NVRTC_SUCCESS) { \\ std::cerr  \\ __global__ void f3(int *result) { *result = sizeof(T); }  \\  \"; int main () {   Create an instance of nvrtcProgram nvrtcProgram prog ; NVRTC_SAFE_CALL ( nvrtcCreateProgram ( & prog ,   prog gpu_program ,   buffer \"prog"
  },
  {
    "id": 27960,
    "content": "cu\" ,   name 0 ,   numHeaders NULL ,   headers NULL ));   includeNames   add all name expressions for kernels std :: vector kernel_name_vec ; std :: vector variable_name_vec ; std :: vector variable_initial_value ; std :: vector expected_result ;   note the name expressions are parsed as constant expressions kernel_name_vec"
  },
  {
    "id": 27961,
    "content": "for ( size_t i = 0 ; i #include #include #include #include #define NVRTC_SAFE_CALL(x) \\ do { \\ nvrtcResult result = x; \\ if (result"
  },
  {
    "id": 27962,
    "content": "= NVRTC_SUCCESS) { \\ std::cerr \\ __global__ void f3(int *result) { *result = sizeof(T); } \\ \" ; note: this structure is also defined in GPU code string namespace N1 { struct S1_t { int i ; double d ; }; }; template std :: string getKernelNameForType ( void ) { Look up the source level name string for the type \"T\" using nvrtcGetTypeName() and use it to create the kernel name std :: string"
  },
  {
    "id": 27963,
    "content": "type_name ; NVRTC_SAFE_CALL ( nvrtcGetTypeName ( & type_name )); return std :: string ( \"f3\" ; } int main () { Create an instance of nvrtcProgram nvrtcProgram prog ; NVRTC_SAFE_CALL ( nvrtcCreateProgram ( & prog , prog gpu_program , buffer \"gpu_program cu\" , name 0 , numHeaders NULL , headers NULL )); includeNames add all name expressions for kernels std :: vector name_vec ; std :: vector"
  },
  {
    "id": 27965,
    "content": "CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << \" * \" << hX [ i ] << \" + \" << hY [ i ] << \" = \" << hOut [ i ] << ' ' ; } Release resources CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ("
  },
  {
    "id": 27966,
    "content": "context )); free ( cubin ); delete [] hX ; delete [] hY ; delete [] hOut ; delete [] LTOIR ; return 0 ; } 14"
  },
  {
    "id": 27968,
    "content": "Device LTO Build Instructions  Assuming the environment variable CUDA_PATH points to the CUDA Toolkit installation directory, build this example as: Compile offline"
  },
  {
    "id": 27970,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 27971,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 27973,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 27974,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 27975,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 27976,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 27977,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 27978,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 27979,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 27980,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 27981,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 27982,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 27983,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 27992,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 27994,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 27995,
    "content": "1 Changes to compiler optimizer heuristics in the newer NVRTC shared library may also potentially cause performance perturbations for generated code"
  },
  {
    "id": 27996,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2014-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 28001,
    "content": "Introduction  The Fatbin Creator APIs are a set of APIs which can be used at runtime to combine multiple CUDA objects into one CUDA fat binary (fatbin) The functionality in this library is similar to the fatbinary offline tool in the CUDA toolkit, with the following advantages: Support for runtime fatbin creation"
  },
  {
    "id": 28010,
    "content": "Installation  The Fatbin Creator library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vFatbin h lib\\x64 vFatbin dll lib\\x64 vFatbin_static lib doc\\pdf vFatbin_User_Guide pdf On Linux: include/nvFatbin"
  },
  {
    "id": 28013,
    "content": "Error codes  Enumerations nvFatbinResult The enumerated type nvFatbinResult defines API call result codes Functions const char * nvFatbinGetErrorString (nvFatbinResult result) nvFatbinGetErrorString returns an error description string for each error code"
  },
  {
    "id": 28017,
    "content": "Enumerations  enum nvFatbinResult  The enumerated type nvFatbinResult defines API call result codes"
  },
  {
    "id": 28018,
    "content": "Values: enumerator NVFATBIN_SUCCESS  enumerator NVFATBIN_ERROR_INTERNAL  enumerator NVFATBIN_ERROR_ELF_ARCH_MISMATCH  enumerator NVFATBIN_ERROR_ELF_SIZE_MISMATCH  enumerator NVFATBIN_ERROR_MISSING_PTX_VERSION  enumerator NVFATBIN_ERROR_NULL_POINTER  enumerator NVFATBIN_ERROR_COMPRESSION_FAILED  enumerator NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED  enumerator"
  },
  {
    "id": 28019,
    "content": "NVFATBIN_ERROR_UNRECOGNIZED_OPTION  enumerator NVFATBIN_ERROR_INVALID_ARCH  enumerator NVFATBIN_ERROR_INVALID_NVVM  enumerator NVFATBIN_ERROR_EMPTY_INPUT  enumerator NVFATBIN_ERROR_MISSING_PTX_ARCH  enumerator NVFATBIN_ERROR_PTX_ARCH_MISMATCH  enumerator NVFATBIN_ERROR_MISSING_FATBIN  enumerator NVFATBIN_ERROR_INVALID_INDEX  enumerator NVFATBIN_ERROR_IDENTIFIER_REUSE  3"
  },
  {
    "id": 28022,
    "content": "Functions  const char * nvFatbinGetErrorString ( nvFatbinResult result )  nvFatbinGetErrorString returns an error description string for each error code Parameters result – [in] error code Returns nullptr, if result is NVFATBIN_SUCCESS a string, if result is not NVFATBIN_SUCCESS 3"
  },
  {
    "id": 28024,
    "content": "Fatbinary Creation  Functions nvFatbinResult nvFatbinAddCubin (nvFatbinHandle handle, const void *code, size_t size, const char *arch, const char *identifier) nvFatbinAddCubin adds a CUDA binary to the fatbinary nvFatbinResult nvFatbinAddIndex (nvFatbinHandle handle, const void *code, size_t size, const char *identifier) nvFatbinAddIndex adds an index file to the fatbinary nvFatbinResult"
  },
  {
    "id": 28025,
    "content": "nvFatbinAddLTOIR (nvFatbinHandle handle, const void *code, size_t size, const char *arch, const char *identifier, const char *optionsCmdLine) nvFatbinAddLTOIR adds LTOIR to the fatbinary nvFatbinResult nvFatbinAddPTX (nvFatbinHandle handle, const char *code, size_t size, const char *arch, const char *identifier, const char *optionsCmdLine) nvFatbinAddPTX adds PTX to the fatbinary nvFatbinResult"
  },
  {
    "id": 28026,
    "content": "nvFatbinAddReloc (nvFatbinHandle handle, const void *code, size_t size) nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary nvFatbinResult nvFatbinCreate (nvFatbinHandle *handle_indirect, const char **options, size_t optionsCount) nvFatbinCreate creates a new handle nvFatbinResult nvFatbinDestroy (nvFatbinHandle *handle_indirect) nvFatbinDestroy destroys the handle"
  },
  {
    "id": 28027,
    "content": "nvFatbinResult nvFatbinGet (nvFatbinHandle handle, void *buffer) nvFatbinGet returns the completed fatbinary nvFatbinResult nvFatbinSize (nvFatbinHandle handle, size_t *size) nvFatbinSize returns the fatbinary's size nvFatbinResult nvFatbinVersion (unsigned int *major, unsigned int *minor) nvFatbinVersion returns the current version of nvFatbin Typedefs nvFatbinHandle nvFatbinHandle is the unit of"
  },
  {
    "id": 28032,
    "content": "Functions  nvFatbinResult nvFatbinAddCubin ( nvFatbinHandle handle , const void * code , size_t size , const char * arch , const char * identifier )  nvFatbinAddCubin adds a CUDA binary to the fatbinary"
  },
  {
    "id": 28034,
    "content": "Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_ELF_ARCH_MISMATCH NVFATBIN_ERROR_ELF_SIZE_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddIndex ( nvFatbinHandle handle , const void * code , size_t size , const char * identifier ) "
  },
  {
    "id": 28037,
    "content": "Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_INVALID_INDEX NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddLTOIR ( nvFatbinHandle handle , const void * code , size_t size , const char * arch , const char * identifier , const char * optionsCmdLine ) "
  },
  {
    "id": 28040,
    "content": "Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddPTX ( nvFatbinHandle handle , const char * code , size_t size , const char * arch , const char * identifier , const char *"
  },
  {
    "id": 28042,
    "content": "If the final character is not ‘\\0’, one will be added automatically, but in doing so, the code will be copied if it hasn’t already been copied"
  },
  {
    "id": 28044,
    "content": "Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_PTX_ARCH_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED, NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_MISSING_PTX_VERSION NVFATBIN_ERROR_MISSING_PTX_ARCH NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinAddReloc ( nvFatbinHandle handle ,"
  },
  {
    "id": 28045,
    "content": "const void * code , size_t size )  nvFatbinAddReloc adds relocatable PTX entries from a host object to the fatbinary Note that each relocatable ptx source must have a unique identifier (the identifiers are taken from the object’s entries) Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INVALID_ARCH NVFATBIN_ERROR_PTX_ARCH_MISMATCH NVFATBIN_ERROR_COMPRESSION_FAILED,"
  },
  {
    "id": 28046,
    "content": "NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_COMPRESSED_SIZE_EXCEEDED NVFATBIN_ERROR_EMPTY_INPUT NVFATBIN_ERROR_MISSING_PTX_VERSION NVFATBIN_ERROR_MISSING_PTX_ARCH NVFATBIN_ERROR_IDENTIFIER_REUSE NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinCreate ( nvFatbinHandle * handle_indirect , const char * * options , size_t optionsCount )  nvFatbinCreate creates a new handle Parameters"
  },
  {
    "id": 28047,
    "content": "handle_indirect – [out] Address of nvFatbin handle options – [in] An array of strings, each containing a single option Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_UNRECOGNIZED_OPTION NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinDestroy ( nvFatbinHandle * handle_indirect )  nvFatbinDestroy destroys the handle"
  },
  {
    "id": 28049,
    "content": "Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinGet ( nvFatbinHandle handle , void * buffer )  nvFatbinGet returns the completed fatbinary Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinSize ( nvFatbinHandle handle , size_t * size )  nvFatbinSize returns the fatbinary’s size size – [out] The"
  },
  {
    "id": 28050,
    "content": "fatbinary’s size Returns NVFATBIN_SUCCESS NVFATBIN_ERROR_NULL_POINTER NVFATBIN_ERROR_INTERNAL nvFatbinResult nvFatbinVersion ( unsigned int * major , unsigned int * minor )  nvFatbinVersion returns the current version of nvFatbin Parameters major – [out] The major version Typedefs  typedef struct _nvFatbinHandle * nvFatbinHandle  nvFatbinHandle is the unit of fatbin creation, and an opaque"
  },
  {
    "id": 28051,
    "content": "handle for a program To create a fatbin, an instance of nvFatbinHandle must be created first with nvFatbinCreate()"
  },
  {
    "id": 28054,
    "content": "Supported Options  nvFatbin supports the options below Options that take a value have an assignment operator ( = ) followed by the option value, with no spaces, e"
  },
  {
    "id": 28060,
    "content": "Basic Usage  This section of the document uses a simple example to explain how to use the Fatbin Creator APIs to link a program This example assumes we want to create a fatbin with a CUBIN for sm_52, PTX for sm_61, and LTOIR for sm_70 We can create an instance of the fatbin creator and obtain an api handle to it as shown in Figure 1 Fatbin Creator creation and initialization of a program"
  },
  {
    "id": 28061,
    "content": "nvFatbinHandle handle ; nvFatbinCreate ( & handle , nullptr , 0 ); Assume that we already have three inputs stored in std::vector ‘s (CUBIN, PTX, and LTOIR), which could be from code created with nvrtc and stored into vectors"
  },
  {
    "id": 28062,
    "content": "(They do not have to be in vectors, this merely illustrates that both the data itself and its size are needed"
  },
  {
    "id": 28063,
    "content": ") We can add the inputs as shown in Figure 2 And to allocate memory, we need to query the size of the fatbin which is done as shown in Figure 3 Query size of the created fatbin nvFatbinSize ( linker , & fatbinSize ); The fatbin can now be queried as shown in Figure 4 Query the created fatbin void * fatbin = malloc ( fatbinSize ); nvFatbinGet ( handle , fatbin ); When the fatbin creator is not"
  },
  {
    "id": 28064,
    "content": "needed anymore, it can be destroyed as shown in Figure 5 For example, you can create a fatbin from a cubin created with 11"
  },
  {
    "id": 28068,
    "content": "Example: Runtime fatbin creation  This section demonstrates runtime fatbin creation These two cubins are then passed to nvFatbin* API functions, which put the cubins into a fatbin"
  },
  {
    "id": 28069,
    "content": "Note that this example requires a compatible GPU with drivers and NVRTC to work, even though the library doesn’t require either"
  },
  {
    "id": 28073,
    "content": "cpp)  #include #include #include #include #include #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x) \\ do { \\ nvrtcResult result = x; \\ if (result"
  },
  {
    "id": 28074,
    "content": "= NVRTC_SUCCESS) { \\ std::cerr ( i ); hY [ i ] = static_cast ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize )); Execute SAXPY void *"
  },
  {
    "id": 28075,
    "content": "args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 , grid dim NUM_THREADS , 1 , 1 , block dim 0 , NULL , shared mem and stream args , 0 )); arguments CUDA_SAFE_CALL ( cuCtxSynchronize ()); Retrieve and print output CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << \" * \""
  },
  {
    "id": 28076,
    "content": "<< hX [ i ] << \" + \" << hY [ i ] << \" = \" << hOut [ i ] << ' ' ; } Release resources CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ( cuCtxDestroy ( context )); delete [] hX ; delete [] hY ; delete [] hOut ; Release resources free ( fatbin ); delete [] (( char * ) known );"
  },
  {
    "id": 28079,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 28080,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 28082,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 28083,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 28084,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 28085,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 28086,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 28087,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 28088,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 28089,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 28090,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 28091,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 28092,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 28101,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 28103,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 28104,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2023-2024, NVIDIA Corporation & Affiliates"
  },
  {
    "id": 28109,
    "content": "Introduction  The JIT Link APIs are a set of APIs which can be used at runtime to link together GPU devide code"
  },
  {
    "id": 28110,
    "content": "The APIs accept inputs in multiple formats, either host objects, host libraries, fatbins, device cubins, PTX, or LTO-IR"
  },
  {
    "id": 28111,
    "content": "The output is a linked cubin that can be loaded by cuModuleLoadData and cuModuleLoadDataEx of the CUDA Driver API"
  },
  {
    "id": 28112,
    "content": "Link Time Optimization can also be performed when given LTO-IR or higher level formats that include LTO-IR"
  },
  {
    "id": 28113,
    "content": "The functionality in this library is similar to the cuLink* APIs in the CUDA Driver, with the following advantages: Support for Link Time Optimization Allow users to use runtime linking with the latest Toolkit version that is supported as part of CUDA Toolkit release This support may not be available in the CUDA Driver APIs if the application is running with an older driver installed in the system"
  },
  {
    "id": 28118,
    "content": "System Requirements  The JIT Link library requires the following system configuration: POSIX threads support for non-Windows platform"
  },
  {
    "id": 28122,
    "content": "Installation  The JIT Link library is part of the CUDA Toolkit release and the components are organized as follows in the CUDA toolkit installation directory: On Windows: include vJitLink h lib\\x64 vJitLink dll lib\\x64 vJitLink_static lib doc\\pdf vJitLink_User_Guide pdf On Linux: include/nvJitLink"
  },
  {
    "id": 28125,
    "content": "Error codes  Enumerations nvJitLinkResult The enumerated type nvJitLinkResult defines API call result codes"
  },
  {
    "id": 28129,
    "content": "Enumerations  enum nvJitLinkResult  The enumerated type nvJitLinkResult defines API call result codes"
  },
  {
    "id": 28130,
    "content": "Values: enumerator NVJITLINK_SUCCESS  enumerator NVJITLINK_ERROR_UNRECOGNIZED_OPTION  enumerator NVJITLINK_ERROR_MISSING_ARCH  enumerator NVJITLINK_ERROR_INVALID_INPUT  enumerator NVJITLINK_ERROR_PTX_COMPILE  enumerator NVJITLINK_ERROR_NVVM_COMPILE  enumerator NVJITLINK_ERROR_INTERNAL  enumerator NVJITLINK_ERROR_THREADPOOL  enumerator NVJITLINK_ERROR_UNRECOGNIZED_INPUT  3"
  },
  {
    "id": 28132,
    "content": "Linking  Enumerations nvJitLinkInputType The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd* APIs Functions nvJitLinkResult nvJitLinkAddData (nvJitLinkHandle handle, nvJitLinkInputType inputType, const void *data, size_t size, const char *name) nvJitLinkAddData adds data image to the link nvJitLinkResult nvJitLinkAddFile (nvJitLinkHandle handle,"
  },
  {
    "id": 28133,
    "content": "nvJitLinkInputType inputType, const char *fileName) nvJitLinkAddFile reads data from file and links it in nvJitLinkResult nvJitLinkComplete (nvJitLinkHandle handle) nvJitLinkComplete does the actual link nvJitLinkResult nvJitLinkCreate (nvJitLinkHandle *handle, uint32_t numOptions, const char **options) nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets"
  },
  {
    "id": 28134,
    "content": "the output parameter handle nvJitLinkResult nvJitLinkDestroy (nvJitLinkHandle *handle) nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL nvJitLinkResult nvJitLinkGetErrorLog (nvJitLinkHandle handle, char *log) nvJitLinkGetErrorLog puts any error messages in the log nvJitLinkResult nvJitLinkGetErrorLogSize (nvJitLinkHandle handle, size_t *size)"
  },
  {
    "id": 28135,
    "content": "nvJitLinkGetErrorLogSize gets the size of the error log nvJitLinkResult nvJitLinkGetInfoLog (nvJitLinkHandle handle, char *log) nvJitLinkGetInfoLog puts any info messages in the log nvJitLinkResult nvJitLinkGetInfoLogSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetInfoLogSize gets the size of the info log nvJitLinkResult nvJitLinkGetLinkedCubin (nvJitLinkHandle handle, void *cubin)"
  },
  {
    "id": 28136,
    "content": "nvJitLinkGetLinkedCubin gets the linked cubin nvJitLinkResult nvJitLinkGetLinkedCubinSize (nvJitLinkHandle handle, size_t *size) nvJitLinkGetLinkedCubinSize gets the size of the linked cubin nvJitLinkResult nvJitLinkGetLinkedPtx (nvJitLinkHandle handle, char *ptx) nvJitLinkGetLinkedPtx gets the linked ptx nvJitLinkResult nvJitLinkGetLinkedPtxSize (nvJitLinkHandle handle, size_t *size)"
  },
  {
    "id": 28138,
    "content": "nvJitLinkResult nvJitLinkVersion (unsigned int *major, unsigned int *minor) nvJitLinkVersion returns the current version of nvJitLink"
  },
  {
    "id": 28143,
    "content": "Enumerations  enum nvJitLinkInputType  The enumerated type nvJitLinkInputType defines the kind of inputs that can be passed to nvJitLinkAdd* APIs"
  },
  {
    "id": 28144,
    "content": "Values: enumerator NVJITLINK_INPUT_NONE  enumerator NVJITLINK_INPUT_CUBIN  enumerator NVJITLINK_INPUT_PTX  enumerator NVJITLINK_INPUT_LTOIR  enumerator NVJITLINK_INPUT_FATBIN  enumerator NVJITLINK_INPUT_OBJECT  enumerator NVJITLINK_INPUT_LIBRARY  enumerator NVJITLINK_INPUT_ANY  3"
  },
  {
    "id": 28147,
    "content": "Functions  static inline nvJitLinkResult nvJitLinkAddData ( nvJitLinkHandle handle , nvJitLinkInputType inputType , const void * data , size_t size , const char * name )  nvJitLinkAddData adds data image to the link Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkAddFile ( nvJitLinkHandle handle , nvJitLinkInputType"
  },
  {
    "id": 28148,
    "content": "inputType , const char * fileName )  nvJitLinkAddFile reads data from file and links it in Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkComplete ( nvJitLinkHandle handle )  nvJitLinkComplete does the actual link Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult"
  },
  {
    "id": 28149,
    "content": "nvJitLinkCreate ( nvJitLinkHandle * handle , uint32_t numOptions , const char * * options )  nvJitLinkCreate creates an instance of nvJitLinkHandle with the given input options, and sets the output parameter handle Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_UNRECOGNIZED_OPTION NVJITLINK_ERROR_MISSING_ARCH NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult"
  },
  {
    "id": 28150,
    "content": "nvJitLinkDestroy ( nvJitLinkHandle * handle )  nvJitLinkDestroy frees the memory associated with the given handle and sets it to NULL Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLog ( nvJitLinkHandle handle , char * log )  nvJitLinkGetErrorLog puts any error messages in the log Returns NVJITLINK_SUCCESS"
  },
  {
    "id": 28151,
    "content": "NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetErrorLogSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetErrorLogSize gets the size of the error log Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLog ( nvJitLinkHandle handle , char * log ) "
  },
  {
    "id": 28152,
    "content": "nvJitLinkGetInfoLog puts any info messages in the log Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetInfoLogSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetInfoLogSize gets the size of the info log Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult"
  },
  {
    "id": 28153,
    "content": "nvJitLinkGetLinkedCubin ( nvJitLinkHandle handle , void * cubin )  nvJitLinkGetLinkedCubin gets the linked cubin Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedCubinSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetLinkedCubinSize gets the size of the linked cubin Returns NVJITLINK_SUCCESS"
  },
  {
    "id": 28154,
    "content": "NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedPtx ( nvJitLinkHandle handle , char * ptx )  nvJitLinkGetLinkedPtx gets the linked ptx Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL static inline nvJitLinkResult nvJitLinkGetLinkedPtxSize ( nvJitLinkHandle handle , size_t * size )  nvJitLinkGetLinkedPtxSize"
  },
  {
    "id": 28155,
    "content": "gets the size of the linked ptx Returns NVJITLINK_SUCCESS NVJITLINK_ERROR_INVALID_INPUT NVJITLINK_ERROR_INTERNAL nvJitLinkResult nvJitLinkVersion ( unsigned int * major , unsigned int * minor )  nvJitLinkVersion returns the current version of nvJitLink Typedefs  typedef struct nvJitLink * nvJitLinkHandle  nvJitLinkHandle is the unit of linking, and an opaque handle for a program To link inputs,"
  },
  {
    "id": 28160,
    "content": "Options that take a value have an assignment operator ( = ) followed by the option value, with no spaces, e"
  },
  {
    "id": 28163,
    "content": "-variables-used= Pass list of variables that are used; any not in the list can be removed -optimize-unused-variables Normally device code optimization is limited by not knowing what the host code references With this option it can assume that if a variable is not referenced in device code then it can be removed"
  },
  {
    "id": 28164,
    "content": "-jump-table-density= When doing LTO, specify the case density percentage in switch statements, and use it as a minimal threshold to determine whether jump table(brx"
  },
  {
    "id": 28168,
    "content": "Basic Usage  This section of the document uses a simple example to explain how to use the JIT Link APIs to link a program This example assumes we want to link for sm_80, but whatever arch is installed on the system should be used"
  },
  {
    "id": 28169,
    "content": "Linker creation and initialization of a program nvJitLink_t linker ; const char * link_options [] = { \"-arch=sm_80\" }; nvJitLinkCreate ( & linker , 1 , link_options ); Assume that we already have two relocatable input files (a"
  },
  {
    "id": 28172,
    "content": "Inputs to linker nvJitLinkAddFile ( linker , NVJITLINK_INPUT_OBJECT , \"a o\" ); nvJitLinkAddFile ( linker , NVJITLINK_INPUT_OBJECT , \"b"
  },
  {
    "id": 28173,
    "content": "o\" ); Now the actual link can be done as shown in Figure 3 Linking of the PTX program nvJitLinkComplete ( linker ); The linked GPU assembly code can now be obtained And to allocate memory, we need to query the size of the image of the linked GPU assembly code which is done as shown in Figure 4 Query size of the linked assembly image nvJitLinkGetLinkedCubinSize ( linker , & cubinSize ); The image"
  },
  {
    "id": 28174,
    "content": "of the linked GPU assembly code can now be queried as shown in Figure 5 Query the linked assembly image elf = ( char * ) malloc ( cubinSize ); nvJitLinkGetLinkedCubin ( linker , ( void * ) elf ); When the linker is not needed anymore, it can be destroyed as shown in Figure 6"
  },
  {
    "id": 28175,
    "content": "Compatibility  The nvJitLink library is compatible across minor versions in a release, but may not be compatible across major versions"
  },
  {
    "id": 28176,
    "content": "The library version itself must be >= the maximum version of the inputs, and the shared library version must be >= the version that was linked with"
  },
  {
    "id": 28177,
    "content": "For example, you can link an object created with 12 0 and one with 12 1 if your nvJitLink library is version 12"
  },
  {
    "id": 28179,
    "content": "If it was linked with 12 1, then you can replace and use the nvJitLink shared library with any version 12"
  },
  {
    "id": 28181,
    "content": "On the flip side, you cannot use 12 0 to link 12 1 objects, nor use 12 0 nvJitLink library to run 12"
  },
  {
    "id": 28188,
    "content": "Example: Device LTO (link time optimization)  This section demonstrates device link time optimization (LTO)"
  },
  {
    "id": 28189,
    "content": "The first unit is generated offline using nvcc , by specifying the architecture as ‘ -arch lto_XX ’ (see offline"
  },
  {
    "id": 28193,
    "content": "These two units are then passed to libnvJitLink* API functions, which link together the LTO IR, run the optimizer on the linked IR, and generate a cubin (see online"
  },
  {
    "id": 28202,
    "content": "cpp)  #include #include #include #include #include #define NUM_THREADS 128 #define NUM_BLOCKS 32 #define NVRTC_SAFE_CALL(x) \\ do { \\ nvrtcResult result = x; \\ if (result"
  },
  {
    "id": 28203,
    "content": "= NVRTC_SUCCESS) { \\ std::cerr 0) { \\ char *log = (char*)malloc(lsize); \\ result = nvJitLinkGetErrorLog(h, log); \\ if (result == NVJITLINK_SUCCESS) { \\ std::cerr ( i ); hY [ i ] = static_cast ( i * 2 ); } CUdeviceptr dX , dY , dOut ; CUDA_SAFE_CALL ( cuMemAlloc ( & dX , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dY , bufferSize )); CUDA_SAFE_CALL ( cuMemAlloc ( & dOut , bufferSize ));"
  },
  {
    "id": 28204,
    "content": "CUDA_SAFE_CALL ( cuMemcpyHtoD ( dX , hX , bufferSize )); CUDA_SAFE_CALL ( cuMemcpyHtoD ( dY , hY , bufferSize )); Execute SAXPY void * args [] = { & a , & dX , & dY , & dOut , & n }; CUDA_SAFE_CALL ( cuLaunchKernel ( kernel , NUM_BLOCKS , 1 , 1 , grid dim NUM_THREADS , 1 , 1 , block dim 0 , NULL , shared mem and stream args , 0 )); arguments CUDA_SAFE_CALL ( cuCtxSynchronize ()); Retrieve and"
  },
  {
    "id": 28205,
    "content": "print output CUDA_SAFE_CALL ( cuMemcpyDtoH ( hOut , dOut , bufferSize )); for ( size_t i = 0 ; i < n ; ++ i ) { std :: cout << a << \" * \" << hX [ i ] << \" + \" << hY [ i ] << \" = \" << hOut [ i ] << ' ' ; } Release resources CUDA_SAFE_CALL ( cuMemFree ( dX )); CUDA_SAFE_CALL ( cuMemFree ( dY )); CUDA_SAFE_CALL ( cuMemFree ( dOut )); CUDA_SAFE_CALL ( cuModuleUnload ( module )); CUDA_SAFE_CALL ("
  },
  {
    "id": 28206,
    "content": "cuCtxDestroy ( context )); free ( cubin ); delete [] hX ; delete [] hY ; delete [] hOut ; delete [] LTOIR ; return 0 ; } 6"
  },
  {
    "id": 28208,
    "content": "Build Instructions  Assuming the environment variable CUDA_PATH points to CUDA Toolkit installation directory, build this example as: Compile offline"
  },
  {
    "id": 28210,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 28211,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 28213,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 28214,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 28215,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 28216,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 28217,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 28218,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 28219,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 28220,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 28221,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 28222,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 28223,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 28232,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 28234,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 28235,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2022-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 28237,
    "content": "5 | PDF | Archive cuSPARSE The API reference guide for cuSPARSE, the CUDA sparse matrix library Introduction  The cuSPARSE library contains a set of GPU-accelerated basic linear algebra subroutines used for handling sparse matrices that perform significantly faster than CPU-only alternatives"
  },
  {
    "id": 28238,
    "content": "Depending on the specific operation, the library targets matrices with sparsity ratios in the range between 70%-99"
  },
  {
    "id": 28240,
    "content": "It is implemented on top of the NVIDIA® CUDA™ runtime (which is part of the CUDA Toolkit) and is designed to be called from C and C++"
  },
  {
    "id": 28241,
    "content": "see also cuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication cuSPARSE Release Notes : cuda-toolkit-release-notes cuSPARSE GitHub Samples : CUDALibrarySamples Nvidia Developer Forum : GPU-Accelerated Libraries Provide Feedback : Math-Libs-Feedback @ nvidia"
  },
  {
    "id": 28242,
    "content": "Library Organization and Features  The cuSPARSE library is organized in two set of APIs: The Legacy APIs , inspired by the Sparse BLAS standard, provide a limited set of functionalities and will not be improved in future releases , even if standard maintenance is still ensured"
  },
  {
    "id": 28244,
    "content": "They allow computing the most common sparse linear algebra operations, such as sparse matrix-vector (SpMV) and sparse matrix-matrix multiplication (SpMM), in a flexible way"
  },
  {
    "id": 28245,
    "content": "The new APIs have the following capabilities and features: Set matrix data layouts , number of batches , and storage formats (for example, CSR, COO, and so on)"
  },
  {
    "id": 28246,
    "content": "Provide constant descriptors for vector and matrix inputs to support const-safe interface and guarantee that the APIs do not modify their inputs"
  },
  {
    "id": 28249,
    "content": "Static Library Support  Starting with CUDA 6 5, the cuSPARSE library is also delivered in a static form as libcusparse_static"
  },
  {
    "id": 28251,
    "content": "For example, to compile a small application using cuSPARSE against the dynamic library , the following command can be used: nvcc my_cusparse_app cu - lcusparse - o my_cusparse_app Whereas to compile against the static library , the following command has to be used: nvcc my_cusparse_app cu - lcusparse_static - o my_cusparse_app It is also possible to use the native Host C++ compiler"
  },
  {
    "id": 28252,
    "content": "Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line"
  },
  {
    "id": 28253,
    "content": "c - lcusparse_static - lcudart_static - lpthread - ldl - I / include - L / lib64 - o my_cusparse_app Note that in the latter case, the library cuda is not needed"
  },
  {
    "id": 28254,
    "content": "In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available"
  },
  {
    "id": 28258,
    "content": "0, cuSPARSE will depend on nvJitLink library for JIT (Just-In-Time) LTO (Link-Time-Optimization) capabilities; refer to the cusparseSpMMOp APIs for more information"
  },
  {
    "id": 28259,
    "content": "If the user links to the dynamic library , the environment variables for loading the libraries at run-time (such as LD_LIBRARY_PATH on Linux and PATH on Windows) must include the path where libnvjitlink"
  },
  {
    "id": 28261,
    "content": "If linking to the static library , the user needs to link with -lnvjitlink and set the environment variables for loading the libraries at compile-time LIBRARY_PATH/PATH accordingly"
  },
  {
    "id": 28263,
    "content": "Using the cuSPARSE API  This chapter describes how to use the cuSPARSE library API It is not a reference for the cuSPARSE API data types and functions; that is provided in subsequent chapters"
  },
  {
    "id": 28266,
    "content": "APIs Usage Notes  The cuSPARSE library allows developers to access the computational resources of the NVIDIA graphics processing unit (GPU) The cuSPARSE APIs assume that input and output data (vectors and matrices) reside in GPU (device) memory"
  },
  {
    "id": 28267,
    "content": "\\(\\alpha\\) and \\(\\beta\\) ) can be passed by reference on the host or the device, instead of only being allowed to be passed by value on the host"
  },
  {
    "id": 28268,
    "content": "This allows library functions to execute asynchronously using streams even when they are generated by a previous kernel resulting in maximum parallelism"
  },
  {
    "id": 28269,
    "content": "The handle to the cuSPARSE library context is initialized using the function and is explicitly passed to every subsequent library function call This allows the user to have more control over the library setup when using multiple host threads and multiple GPUs"
  },
  {
    "id": 28270,
    "content": "It is the responsibility of the developer to allocate memory and to copy data between GPU memory and CPU memory using standard CUDA runtime API routines, such as cudaMalloc() , cudaFree() , cudaMemcpy() , and cudaMemcpyAsync()"
  },
  {
    "id": 28271,
    "content": "The cuSPARSE library functions are executed asynchronously with respect to the host and may return control to the application on the host before the result is ready Developers can use the cudaDeviceSynchronize() function to ensure that the execution of a particular cuSPARSE library routine has completed A developer can also use the cudaMemcpy() routine to copy data from the device to the host and"
  },
  {
    "id": 28272,
    "content": "vice versa, using the cudaMemcpyDeviceToHost and cudaMemcpyHostToDevice parameters, respectively In this case there is no need to add a call to cudaDeviceSynchronize() because the call to cudaMemcpy() with the above parameters is blocking and completes only when the results are ready on the host"
  },
  {
    "id": 28275,
    "content": "Deprecated APIs  The cuSPARSE library documentation explicitly indicates the set of APIs/enumerators/data structures that are deprecated The library policy for deprecated APIs is the following: An API is marked [[DEPRECATED]] on a release X"
  },
  {
    "id": 28279,
    "content": "2) The documentation indices a replacement if available Otherwise, the functionality will not be maintained in the future The API will be removed in the release X+1"
  },
  {
    "id": 28283,
    "content": "0) Correctness bugs are still addressed even for deprecated APIs, while performance issues are not always ensured"
  },
  {
    "id": 28284,
    "content": "In addition to the documentation, deprecated APIs generate a compile-time warning for most platforms when used"
  },
  {
    "id": 28285,
    "content": "Deprecation warnings can be disabled by defining the macro DISABLE_CUSPARSE_DEPRECATED before including cusparse"
  },
  {
    "id": 28289,
    "content": "Thread Safety  The library is thread safe and its functions can be called from multiple host threads, even with the same handle When multiple threads share the same handle, extreme care needs to be taken when the handle configuration is changed because that change will affect potentially subsequent cuSPARSE calls in all threads So it is not recommended that multiple thread share the same"
  },
  {
    "id": 28293,
    "content": "Result Reproducibility  The design of cuSPARSE prioritizes performance over bit-wise reproducibility Operations using transpose or conjugate-transpose cusparseOperation_t have no reproducibility guarantees"
  },
  {
    "id": 28294,
    "content": "For the remaining operations, performing the same API call twice with the exact same arguments, on the same machine, with the same executable will produce bit-wise identical results"
  },
  {
    "id": 28295,
    "content": "This bit-wise reproducibility can be disrupted by changes to: hardware, CUDA drivers, cuSPARSE version, memory alignment of the data, or algorithm selection"
  },
  {
    "id": 28298,
    "content": "NaN and Inf Propagation  Floating-point numbers have special values for NaN (not-a-number) and Inf (infinity) NaN and Inf appear in the output only if the algorithms happen to generate or propagate them Because the algorithms are subject to change based on toolkit version and runtime considerations, so too are the propagation behaviours of NaN and Inf NaN propagation is different in cuSPARSE"
  },
  {
    "id": 28299,
    "content": "than in typical dense numerical linear algebra, such as cuBLAS The dot product between vectors [0, 1, 0] and [1, 1, NaN] is NaN when using typical dense numerical algorithms, but will be 1 0 with typical sparse numerical algorithms"
  },
  {
    "id": 28302,
    "content": "Parallelism with Streams  If the application performs several small independent computations, or if it makes data transfers in parallel with the computation, CUDA streams can be used to overlap these tasks To achieve the overlap of computation between the tasks, the developer should create CUDA streams using the function cudaStreamCreate() and set the stream to be used by each individual"
  },
  {
    "id": 28303,
    "content": "cuSPARSE library routine by calling cusparseSetStream() just before calling the actual cuSPARSE routine Then, computations performed in separate streams would be overlapped automatically on the GPU, when possible This approach is especially useful when the computation performed by a single task is relatively small and is not enough to fill the GPU with work, or when there is a data transfer that"
  },
  {
    "id": 28304,
    "content": "can be performed in parallel with the computation When streams are used, we recommend using the new cuSPARSE API with scalar parameters and results passed by reference in the device memory to achieve maximum computational overlap Although a developer can create many streams, in practice it is not possible to have more than 16 concurrent kernels executing at the same time"
  },
  {
    "id": 28307,
    "content": "Compatibility and Versioning  The cuSPARSE APIs are intended to be backward compatible at the source level with future releases (unless stated otherwise in the release notes of a specific future release) In other words, if a program uses cuSPARSE, it should continue to compile and work correctly with newer versions of cuSPARSE without source code changes Using different versions of the cusparse"
  },
  {
    "id": 28313,
    "content": "BUILD These version fields are incremented based on the following rules: MAJOR : API breaking changes or new CUDA major version (breaking changes at lower level, e"
  },
  {
    "id": 28315,
    "content": "drivers, compilers, libraries) MINOR : new APIs and functionalities PATCH : Bug fixes or performance improvements (or * new CUDA release) BUILD : Internal build number * Different CUDA toolkit releases ensure distinct library versions even if there are no changes at library level"
  },
  {
    "id": 28318,
    "content": "Optimization Notes  Most of the cuSPARSE routines can be optimized by exploiting CUDA Graphs capture and Hardware Memory Compression features More in details, a single cuSPARSE call or a sequence of calls can be captured by a CUDA Graph and executed in a second moment"
  },
  {
    "id": 28320,
    "content": "A full example of CUDA graphs capture applied to a cuSPARSE routine can be found in cuSPARSE Library Samples - CUDA Graph"
  },
  {
    "id": 28321,
    "content": "Secondly, the data types and functionalities involved in cuSPARSE are suitable for Hardware Memory Compression available in Ampere GPU devices (compute capability 8"
  },
  {
    "id": 28323,
    "content": "The feature allows memory compression for data with enough zero bytes without no loss of information A full example of Hardware Memory Compression applied to a cuSPARSE routine can be found in cuSPARSE Library Samples - Memory Compression"
  },
  {
    "id": 28325,
    "content": "cuSPARSE Storage Formats  The cuSPARSE library supports dense and sparse vector, and dense and sparse matrix formats"
  },
  {
    "id": 28328,
    "content": "Index Base  The library supports zero- and one-based indexing to ensure the compatibility with C/C++ and Fortran languages respectively The index base is selected through the cusparseIndexBase_t type"
  },
  {
    "id": 28335,
    "content": "Dense Vector Format  Dense vectors are represented with a single data array that is stored linearly in memory, such as the following \\(7 \\times 1\\) dense vector The indices array represent the positions of the corresponding nonzero values in the equivalent array in dense format For example, the dense vector in section 3"
  },
  {
    "id": 28338,
    "content": "Sparse vector representation  Note The cuSPARSE routines assume that the indices are provided in increasing order and that each index appears only once"
  },
  {
    "id": 28346,
    "content": "Dense Matrix Format  A dense matrix can be stored in both row-major and column-major memory layout (ordering) and it is represented by the following parameters The leading dimension , which must be Greater than or equal to the number of columns in the row-major layout Greater than or equal to the number of rows in the column-major layout The pointers to the values array of length \\(rows \\times"
  },
  {
    "id": 28347,
    "content": "leading\\; dimension\\) in the row-major layout \\(columns \\times leading\\; dimension\\) in the column-major layout The following figure represents a \\(5 \\times 2\\) dense matrix with both memory layouts Dense matrix representations  The indices within the matrix represents the contiguous locations in memory The leading dimension is useful to represent a sub-matrix within the original one Sub-matrix"
  },
  {
    "id": 28352,
    "content": "The pointers to the row indices array of length nnz that contains the row indices of the corresponding elements in the values array The pointers to the column indices array of length nnz that contains the column indices of the corresponding elements in the values array The pointers to the values array of length nnz that holds all nonzero values of the matrix in row-major ordering Note If the"
  },
  {
    "id": 28353,
    "content": "column indices within a given row are not unique, the correctness of the computation is not always ensured Given an entry in the COO format (zero-base), the corresponding position in the dense matrix is computed as: row-major rows_indices [ i ] * leading_dimension + column_indices [ i ] column-major column_indices [ i ] * leading_dimension + rows_indices [ i ] 3"
  },
  {
    "id": 28356,
    "content": "Compressed Sparse Row (CSR)  The CSR format is similar to COO, where the row indices are compressed and replaced by an array of offsets The pointers to the row offsets array of length number of rows + 1 that represents the starting position of each row in the columns and values arrays Given an entry in the CSR format (zero-base), the corresponding position in the dense matrix is computed as:"
  },
  {
    "id": 28357,
    "content": "row-major row * leading_dimension + column_indices [ row_offsets [ row ] + k ] column-major column_indices [ row_offsets [ row ] + k ] * leading_dimension + row 3"
  },
  {
    "id": 28360,
    "content": "Compressed Sparse Column (CSC)  The CSC format is similar to COO, where the column indices are compressed and replaced by an array of offsets The pointers to the column offsets array of length number of column + 1 that represents the starting position of each column in the columns and values arrays The pointers to the row indices array of length nnz that contains row indices of the corresponding"
  },
  {
    "id": 28361,
    "content": "elements in the values array The pointers to the values array of length nnz that holds all nonzero values of the matrix in column-major ordering Note The CSR format has exactly the same memory layout as its transpose in CSC format (and vice versa) Note If the row indices within a given column are not unique, the correctness of the computation is not always ensured Given an entry in the CSC format"
  },
  {
    "id": 28362,
    "content": "(zero-base), the corresponding position in the dense matrix is computed as: row-major column * leading_dimension + row_indices [ column_offsets [ column ] + k ] column-major row_indices [ column_offsets [ column ] + k ] * leading_dimension + column 3"
  },
  {
    "id": 28365,
    "content": "Sliced Ellpack (SELL)  The Sliced Ellpack format is standardized and well-known as the state of the art"
  },
  {
    "id": 28366,
    "content": "This format allows to significantly improve the performance of all problems that involve low variability in the number of nonzero elements per row"
  },
  {
    "id": 28367,
    "content": "A matrix in the Sliced Ellpack format is divided into slices of an exact number of rows ( \\(sliceSize\\) ), defined by the user"
  },
  {
    "id": 28370,
    "content": ", the maximum non-zeros per row) is found for each slice, and every row in the slice is padded to the maximum row length"
  },
  {
    "id": 28371,
    "content": "A \\(m \\times n\\) sparse matrix \\(A\\) is equivalent to a sliced sparse matrix \\(A_{s}\\) with \\(nslices = \\left \\lceil{\\frac{m}{sliceSize}} ight  ceil\\) slice rows and \\(n\\) columns"
  },
  {
    "id": 28374,
    "content": "The pointer to the slice offsets of length \\(nslices + 1\\) that holds offsets of the slides corresponding to the columns and values arrays The pointer to the column indices array of length sellValuesSize that contains column indices of the corresponding elements in the values array The pointer to the values array of length sellValuesSize that holds all non-zero values and padding in column-major"
  },
  {
    "id": 28380,
    "content": "Block Sparse Row (BSR)  The BSR format is similar to CSR, where the column indices represent two-dimensional blocks instead of a single matrix entry A matrix in the Block Sparse Row format is organized into blocks of size \\(blockSize\\) , defined by the user A \\(m \\times n\\) sparse matrix \\(A\\) is equivalent to a block sparse matrix \\(A_{B}\\) : \\(mb \\times nb\\) with \\(mb = \\frac{m}{blockSize}\\)"
  },
  {
    "id": 28381,
    "content": "block rows and \\(nb = \\frac{n}{blockSize}\\) block columns If \\(m\\) or \\(n\\) is not multiple of \\(blockSize\\) , the user needs to pad the matrix with zeros However, the internal storage format of blocks can be column-major ( cusparseDirection_t=CUSPARSE_DIRECTION_COLUMN ) or row-major ( cusparseDirection_t=CUSPARSE_DIRECTION_ROW ), independently of the base index The pointers to the row block"
  },
  {
    "id": 28382,
    "content": "offsets array of length number of row blocks + 1 that represents the starting position of each row block in the columns and values arrays The pointers to the column block indices array of length nnzb that contains the location of the corresponding elements in the values array The pointers to the values array of length nnzb that holds all nonzero values of the matrix The following example shows a"
  },
  {
    "id": 28387,
    "content": "Blocked Ellpack (BLOCKED-ELL)  The Blocked Ellpack format is similar to the standard Ellpack, where the column indices represent two-dimensional blocks instead of a single matrix entry A matrix in the Blocked Ellpack format is organized into blocks of size \\(blockSize\\) , defined by the user The number of columns per row \\(nEllCols\\) is also defined by the user ( \\(nEllCols \\le n\\) ) A \\(m"
  },
  {
    "id": 28388,
    "content": "\\times n\\) sparse matrix \\(A\\) is equivalent to a Blocked-ELL matrix \\(A_{B}\\) : \\(mb \\times nb\\) with \\(mb = \\left \\lceil{\\frac{m}{blockSize}} ight ceil\\) block rows , and \\(nb = \\left \\lceil{\\frac{nEllCols}{blockSize}} ight ceil\\) block columns If \\(m\\) or \\(n\\) is not multiple of \\(blockSize\\) , then the remaining elements are zero The pointers to the column block indices array of length \\(mb"
  },
  {
    "id": 28389,
    "content": "\\times nb\\) that contains the location of the corresponding elements in the values array The pointers to the values array of length \\(m \\times nEllCols\\) that holds all nonzero values of the matrix in row-major ordering The following example shows a \\(9 \\times 9\\) matrix represented in Blocked-ELL format"
  },
  {
    "id": 28393,
    "content": "Extended BSR Format (BSRX) [DEPRECATED]  BSRX is the same as the BSR format, but the array bsrRowPtrA is separated into two parts The first nonzero block of each row is still specified by the array bsrRowPtrA , which is the same as in BSR, but the position next to the last nonzero block of each row is specified by the array bsrEndPtrA bsrValA (pointer) Points to the data array of length \\(nnzb"
  },
  {
    "id": 28394,
    "content": "\\ast blockDim^{2}\\) that holds all the elements of the nonzero blocks of A bsrRowPtrA (pointer) Points to the integer array of length mb that holds indices into the arrays bsrColIndA and bsrValA ; bsrRowPtrA(i) is the position of the first nonzero block of the i th block row in bsrColIndA and bsrValA bsrEndPtrA (pointer) Points to the integer array of length mb that holds indices into the arrays"
  },
  {
    "id": 28395,
    "content": "bsrColIndA and bsrValA ; bsrRowPtrA(i) is the position next to the last nonzero block of the i th block row in bsrColIndA and bsrValA bsrColIndA (pointer) Points to the integer array of length nnzb that contains the column indices of the corresponding blocks in array bsrValA"
  },
  {
    "id": 28396,
    "content": "\\(A_{b} = \\begin{bmatrix} A_{00} & A_{01} & A_{02} \\\\ A_{10} & A_{11} & A_{12} \\\\ \\end{bmatrix}\\) Assume it has this BSR format \\(\\begin{matrix} \\text{bsrValA of BSR} & = & \\begin{bmatrix} A_{00} & A_{01} & A_{10} & A_{11} & A_{12} \\\\ \\end{bmatrix} \\\\ \\text{bsrRowPtrA of BSR} & = & \\begin{bmatrix} {0\\phantom{"
  },
  {
    "id": 28397,
    "content": "0}} & {2\\phantom{ 0}} & 5 \\\\ \\end{bmatrix} \\\\ \\text{bsrColIndA of BSR} & = & \\begin{bmatrix} {0\\phantom{ 0}} & {1\\phantom{ 0}} & {0\\phantom{ 0}} & {1\\phantom{ 0}} & 2 \\\\ \\end{bmatrix} \\\\ \\end{matrix}\\) The bsrRowPtrA of the BSRX format is simply the first two elements of the bsrRowPtrA BSR format \\(\\begin{matrix} \\text{bsrRowPtrA of BSRX} & = & \\begin{bmatrix} {0\\phantom{ 0}} & 2 \\\\ \\end{bmatrix}"
  },
  {
    "id": 28398,
    "content": "\\\\ \\text{bsrEndPtrA of BSRX} & = & \\begin{bmatrix} {2\\phantom{ 0}} & 5 \\\\ \\end{bmatrix} \\\\ \\end{matrix}\\) The advantage of the BSRX format is that the developer can specify a submatrix in the original BSR format by modifying bsrRowPtrA and bsrEndPtrA while keeping bsrColIndA and bsrValA unchanged For example, to create another block matrix \\(\\widetilde{A} = \\begin{bmatrix} O & O & O \\\\ O & A_{11}"
  },
  {
    "id": 28399,
    "content": "& O \\\\ \\end{bmatrix}\\) that is slightly different from \\(A\\) , the developer can keep bsrColIndA and bsrValA , but reconstruct \\(\\widetilde{A}\\) by properly setting of bsrRowPtrA and bsrEndPtrA \\(\\begin{matrix} {\\text{bsrValA of }\\widetilde{A}} & = & \\begin{bmatrix} A_{00} & A_{01} & A_{10} & A_{11} & A_{12} \\\\ \\end{bmatrix} \\\\ {\\text{bsrColIndA of }\\widetilde{A}} & = & \\begin{bmatrix} {0\\phantom{"
  },
  {
    "id": 28400,
    "content": "0}} & {1\\phantom{ 0}} & {0\\phantom{ 0}} & {1\\phantom{ 0}} & 2 \\\\ \\end{bmatrix} \\\\ {\\text{bsrRowPtrA of }\\widetilde{A}} & = & \\begin{bmatrix} {0\\phantom{ 0}} & 3 \\\\ \\end{bmatrix} \\\\ {\\text{bsrEndPtrA of }\\widetilde{A}} & = & \\begin{bmatrix} {0\\phantom{ 0}} & 4 \\\\ \\end{bmatrix} \\\\ \\end{matrix}\\) 4"
  },
  {
    "id": 28401,
    "content": "cudaDataType_t  The section describes the types shared by multiple CUDA Libraries and defined in the header file library_types"
  },
  {
    "id": 28403,
    "content": "If a specific GPU model does not provide native support for a given data type, the routine returns CUSPARSE_STATUS_ARCH_MISMATCH error"
  },
  {
    "id": 28404,
    "content": "Unsupported data types and Compute Capability (CC): __half on GPUs with CC - while level is one of the following levels: 0 - Off - logging is disabled (default) 1 - Error - only errors will be logged 2 - Trace - API calls that launch CUDA kernels will log their parameters and important information 3 - Hints - hints that can potentially improve the application’s performance 4 - Info - provides"
  },
  {
    "id": 28405,
    "content": "general information about the library execution, may contain details about heuristic status 5 - API Trace - API calls will log their parameter and important information CUSPARSE_LOG_MASK= - while mask is a combination of the following masks: 0 - Off 1 - Error 2 - Trace 4 - Hints 8 - Info 16 - API Trace CUSPARSE_LOG_FILE= - while file name is a path to a logging file"
  },
  {
    "id": 28407,
    "content": "3, it is also possible to dump sparse matrices (CSR, CSC, COO, SELL, BSR) in binary files during the creation by setting the environment variable CUSPARSE_STORE_INPUT_MATRIX"
  },
  {
    "id": 28409,
    "content": "See: cusparseLoggerSetCallback() cusparseLoggerSetFile() cusparseLoggerOpenFile() cusparseLoggerSetLevel() cusparseLoggerSetMask() cusparseLoggerForceDisable() Note The logging mechanism is not available for the legacy APIs"
  },
  {
    "id": 28413,
    "content": "cusparseLoggerSetCallback()  cusparseStatus_t cusparseLoggerSetCallback ( cusparseLoggerCallback_t callback ) Experimental : The function sets the logging callback function In/out Meaning callback IN Pointer to a callback function where cusparseLoggerCallback_t has the following signature: void ( * cusparseLoggerCallback_t )( int logLevel , const char * functionName , const char * message )"
  },
  {
    "id": 28414,
    "content": "Param In/out Meaning logLevel IN Selected log level functionName IN The name of the API that logged this message message IN The log message See cusparseStatus_t for the description of the return status 4"
  },
  {
    "id": 28417,
    "content": "cusparseLoggerSetFile()  cusparseStatus_t cusparseLoggerSetFile ( FILE * file ) Experimental : The function sets the logging output file Note: once registered using this function call, the provided file handle must not be closed unless the function is called again to switch to a different file handle File should have write permission See cusparseStatus_t for the description of the return status 4"
  },
  {
    "id": 28420,
    "content": "cusparseLoggerOpenFile()  cusparseStatus_t cusparseLoggerOpenFile ( const char * logFile ) Experimental : The function opens a logging output file in the given path In/out Meaning logFile IN Path of the logging output file See cusparseStatus_t for the description of the return status 4"
  },
  {
    "id": 28423,
    "content": "cusparseLoggerSetLevel()  cusparseStatus_t cusparseLoggerSetLevel ( int level ) Experimental : The function sets the value of the logging level In/out Meaning level IN Value of the logging level See cusparseStatus_t for the description of the return status 4"
  },
  {
    "id": 28426,
    "content": "cusparseLoggerSetMask()  cusparseStatus_t cusparseLoggerSetMask ( int mask ) Experimental : The function sets the value of the logging mask In/out Meaning mask IN Value of the logging mask See cusparseStatus_t for the description of the return status 5"
  },
  {
    "id": 28427,
    "content": "Naming Conventions  The cuSPARSE legacy functions are available for data types float , double , cuComplex , and cuDoubleComplex The sparse Level 2, and Level 3 functions follow this naming convention: cusparse [][] where can be S , D , C , Z , or X , corresponding to the data types float , double , cuComplex , cuDoubleComplex , and the generic type, respectively The can be dense , coo , csr , or"
  },
  {
    "id": 28428,
    "content": "csc , corresponding to the dense, coordinate, compressed sparse row, and compressed sparse column formats, respectively"
  },
  {
    "id": 28434,
    "content": "cusparseAction_t  This type indicates whether the operation is performed only on indices or on data and indices CUSPARSE_ACTION_NUMERIC the operation is performed on data and indices"
  },
  {
    "id": 28439,
    "content": "typedef struct { cusparseMatrixType_t MatrixType ; cusparseFillMode_t FillMode ; cusparseDiagType_t DiagType ; cusparseIndexBase_t IndexBase ; } cusparseMatDescr_t ; 5"
  },
  {
    "id": 28442,
    "content": "Notice that for symmetric, Hermitian and triangular matrices only their lower or upper part is assumed to be stored The whole idea of matrix type and fill mode is to keep minimum storage for symmetric/Hermitian matrix, and also to take advantage of symmetric property on SpMV (Sparse Matrix Vector multiplication) To compute y=A*x when A is symmetric and only lower triangular part is stored, two"
  },
  {
    "id": 28444,
    "content": "Given the fact that the transpose operation y=L^T*x is 10x slower than non-transpose version y=L*x , the symmetric property does not show up any performance gain"
  },
  {
    "id": 28445,
    "content": "It is better for the user to extend the symmetric matrix to a general matrix and apply y=A*x with matrix type CUSPARSE_MATRIX_TYPE_GENERAL"
  },
  {
    "id": 28446,
    "content": "In general, SpMV, preconditioners (incomplete Cholesky or incomplete LU) and triangular solver are combined together in iterative solvers, for example PCG and GMRES If the user always uses general matrix (instead of symmetric matrix), there is no need to support other than general matrix in preconditioners Therefore the new routines, [bsr|csr]sv2 (triangular solver), [bsr|csr]ilu02 (incomplete"
  },
  {
    "id": 28447,
    "content": "LU) and [bsr|csr]ic02 (incomplete Cholesky), only support matrix type CUSPARSE_MATRIX_TYPE_GENERAL CUSPARSE_MATRIX_TYPE_TRIANGULAR the matrix is triangular"
  },
  {
    "id": 28451,
    "content": "cusparseColorInfo_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csrcolor()"
  },
  {
    "id": 28455,
    "content": "cusparseSolvePolicy_t [DEPRECATED]  This type indicates whether level information is generated and used in csrsv2, csric02, csrilu02, bsrsv2, bsric02 and bsrilu02 CUSPARSE_SOLVE_POLICY_USE_LEVEL generate and use level information"
  },
  {
    "id": 28459,
    "content": "bsric02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsric02_bufferSize() , bsric02_analysis() , and bsric02()"
  },
  {
    "id": 28463,
    "content": "bsrilu02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrilu02_bufferSize() , bsrilu02_analysis() , and bsrilu02()"
  },
  {
    "id": 28467,
    "content": "bsrsm2Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrsm2_bufferSize() , bsrsm2_analysis() , and bsrsm2_solve()"
  },
  {
    "id": 28471,
    "content": "bsrsv2Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in bsrsv2_bufferSize() , bsrsv2_analysis() , and bsrsv2_solve()"
  },
  {
    "id": 28475,
    "content": "csric02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csric02_bufferSize() , csric02_analysis() , and csric02()"
  },
  {
    "id": 28479,
    "content": "csrilu02Info_t [DEPRECATED]  This is a pointer type to an opaque structure holding the information used in csrilu02_bufferSize() , csrilu02_analysis() , and csrilu02()"
  },
  {
    "id": 28486,
    "content": "cusparseCreateColorInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateColorInfo ( cusparseColorInfo_t * info ) This function creates and initializes the cusparseColorInfo_t structure to default values Input info the pointer to the cusparseColorInfo_t structure See cusparseStatus_t for the description of the return status 5"
  },
  {
    "id": 28489,
    "content": "cusparseCreateMatDescr()  cusparseStatus_t cusparseCreateMatDescr ( cusparseMatDescr_t * descrA ) This function initializes the matrix descriptor"
  },
  {
    "id": 28490,
    "content": "It sets the fields MatrixType and IndexBase to the default values CUSPARSE_MATRIX_TYPE_GENERAL and CUSPARSE_INDEX_BASE_ZERO , respectively, while leaving other fields uninitialized"
  },
  {
    "id": 28491,
    "content": "cusparseDestroyColorInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyColorInfo ( cusparseColorInfo_t info ) This function destroys and releases any memory required by the structure Input info the pointer to the structure of csrcolor() See cusparseStatus_t for the description of the return status 5"
  },
  {
    "id": 28494,
    "content": "cusparseDestroyMatDescr()  cusparseStatus_t cusparseDestroyMatDescr ( cusparseMatDescr_t descrA ) This function releases the memory allocated for the matrix descriptor cusparseGetMatDiagType()  cusparseDiagType_t cusparseGetMatDiagType ( const cusparseMatDescr_t descrA ) This function returns the DiagType field of the matrix descriptor descrA Returned One of the enumerated diagType types"
  },
  {
    "id": 28498,
    "content": "cusparseGetMatFillMode()  cusparseFillMode_t cusparseGetMatFillMode ( const cusparseMatDescr_t descrA ) This function returns the FillMode field of the matrix descriptor descrA Returned One of the enumerated fillMode types"
  },
  {
    "id": 28502,
    "content": "cusparseGetMatIndexBase()  cusparseIndexBase_t cusparseGetMatIndexBase ( const cusparseMatDescr_t descrA ) This function returns the IndexBase field of the matrix descriptor descrA Returned One of the enumerated indexBase types"
  },
  {
    "id": 28506,
    "content": "cusparseGetMatType()  cusparseMatrixType_t cusparseGetMatType ( const cusparseMatDescr_t descrA ) This function returns the MatrixType field of the matrix descriptor descrA"
  },
  {
    "id": 28511,
    "content": "cusparseSetMatDiagType()  cusparseStatus_t cusparseSetMatDiagType ( cusparseMatDescr_t descrA , cusparseDiagType_t diagType ) This function sets the DiagType field of the matrix descriptor descrA cusparseSetMatFillMode()  cusparseStatus_t cusparseSetMatFillMode ( cusparseMatDescr_t descrA , cusparseFillMode_t fillMode ) This function sets the FillMode field of the matrix descriptor descrA"
  },
  {
    "id": 28512,
    "content": "cusparseSetMatIndexBase()  cusparseStatus_t cusparseSetMatIndexBase ( cusparseMatDescr_t descrA , cusparseIndexBase_t base ) This function sets the IndexBase field of the matrix descriptor descrA cusparseSetMatType()  cusparseStatus_t cusparseSetMatType ( cusparseMatDescr_t descrA , cusparseMatrixType_t type ) This function sets the MatrixType field of the matrix descriptor descrA"
  },
  {
    "id": 28513,
    "content": "cusparseCreateCsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateCsric02Info ( csric02Info_t * info ); This function creates and initializes the solve and analysis structure of incomplete Cholesky to default values cusparseDestroyCsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t"
  },
  {
    "id": 28514,
    "content": "cusparseDestroyCsric02Info ( csric02Info_t info ); This function destroys and releases any memory required by the structure cusparseCreateCsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateCsrilu02Info ( csrilu02Info_t * info ); This function creates and initializes the solve and analysis structure of incomplete LU to default values"
  },
  {
    "id": 28515,
    "content": "cusparseDestroyCsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyCsrilu02Info ( csrilu02Info_t info ); This function destroys and releases any memory required by the structure cusparseCreateBsrsv2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrsv2Info ( bsrsv2Info_t *"
  },
  {
    "id": 28516,
    "content": "info ); This function creates and initializes the solve and analysis structure of bsrsv2 to default values cusparseDestroyBsrsv2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrsv2Info ( bsrsv2Info_t info ); This function destroys and releases any memory required by the structure cusparseCreateBsrsm2Info() [DEPRECATED]  > The routine"
  },
  {
    "id": 28517,
    "content": "will be removed in the next major release cusparseStatus_t cusparseCreateBsrsm2Info ( bsrsm2Info_t * info ); This function creates and initializes the solve and analysis structure of bsrsm2 to default values cusparseDestroyBsrsm2Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrsm2Info ( bsrsm2Info_t info ); This function destroys and"
  },
  {
    "id": 28518,
    "content": "releases any memory required by the structure cusparseCreateBsric02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsric02Info ( bsric02Info_t * info ); This function creates and initializes the solve and analysis structure of block incomplete Cholesky to default values cusparseDestroyBsric02Info() [DEPRECATED]  > The routine will be"
  },
  {
    "id": 28519,
    "content": "removed in the next major release cusparseStatus_t cusparseDestroyBsric02Info ( bsric02Info_t info ); This function destroys and releases any memory required by the structure cusparseCreateBsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateBsrilu02Info ( bsrilu02Info_t * info ); This function creates and initializes the solve and"
  },
  {
    "id": 28520,
    "content": "analysis structure of block incomplete LU to default values cusparseDestroyBsrilu02Info() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyBsrilu02Info ( bsrilu02Info_t info ); This function destroys and releases any memory required by the structure cusparseCreatePruneInfo() [DEPRECATED]  > The routine will be removed in the next major release"
  },
  {
    "id": 28521,
    "content": "cusparseStatus_t cusparseCreatePruneInfo ( pruneInfo_t * info ); This function creates and initializes structure of prune to default values cusparseDestroyPruneInfo() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseDestroyPruneInfo ( pruneInfo_t info ); This function destroys and releases any memory required by the structure"
  },
  {
    "id": 28522,
    "content": "The size of vector x should be \\((nb \\ast blockDim)\\) at least, and the size of vector y should be \\((mb \\ast blockDim)\\) at least; otherwise, the kernel may return CUSPARSE_STATUS_EXECUTION_FAILED because of an out-of-bounds array"
  },
  {
    "id": 28523,
    "content": "For example, suppose the user has a CSR format and wants to try bsrmv() , the following code demonstrates how to use csr2bsr() conversion and bsrmv() multiplication in single precision Suppose that A is m x n sparse matrix represented by CSR format,   hx is a host vector of size n, and hy is also a host vector of size m"
  },
  {
    "id": 28526,
    "content": "bsrVal array of nnz \\(( =\\) csrRowPtrA(mb) \\(-\\) csrRowPtrA(0) \\()\\) nonzero blocks of matrix \\(A\\) bsrRowPtr integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one bsrColInd integer array of nnz \\(( =\\) csrRowPtrA(mb) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix \\(A\\) If row \\(i\\) is not specified in"
  },
  {
    "id": 28528,
    "content": "For example, consider the \\(2 \\times 3\\) block matrix \\(A\\) : \\(\\begin{matrix} {A = \\begin{bmatrix} A_{11} & A_{12} & O \\\\ A_{21} & A_{22} & A_{23} \\\\ \\end{bmatrix}} \\\\ \\end{matrix}\\) and its one-based BSR format (three vector form) is \\(\\begin{matrix} \\text{bsrVal} & = & \\begin{bmatrix} A_{11} & A_{12} & A_{21} & A_{22} & A_{23} \\\\ \\end{bmatrix} \\\\ \\text{bsrRowPtr} & = & \\begin{bmatrix}"
  },
  {
    "id": 28530,
    "content": "0}} & {3\\phantom{ 0}} & 6 \\\\ \\end{bmatrix} \\\\ \\text{bsrColInd} & = & \\begin{bmatrix} {1\\phantom{ 0}} & {2\\phantom{ 0}} & {1\\phantom{ 0}} & {2\\phantom{ 0}} & 3 \\\\ \\end{bmatrix} \\\\ \\end{matrix}\\) Suppose we want to do the following bsrmv operation on a matrix \\(\\overset{¯}{A}\\) which is slightly different from \\(A\\) \\(\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\end{bmatrix}:=alpha \\ast (\\widetilde{A} ="
  },
  {
    "id": 28531,
    "content": "\\begin{bmatrix} O & O & O \\\\ O & A_{22} & O \\\\ \\end{bmatrix}) \\ast \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ \\end{bmatrix} + \\begin{bmatrix} y_{1} \\\\ {beta \\ast y_{2}} \\\\ \\end{bmatrix}\\) We don’t need to create another BSR format for the new matrix \\(\\overset{¯}{A}\\) , all that we should do is to keep bsrVal and bsrColInd unchanged, but modify bsrRowPtr and add an additional array bsrEndPtr which"
  },
  {
    "id": 28532,
    "content": "points to the last nonzero elements per row of \\(\\overset{¯}{A}\\) plus 1 For example, the following bsrRowPtr and bsrEndPtr can represent matrix \\(\\overset{¯}{A}\\) : \\(\\begin{matrix} \\text{bsrRowPtr} & = & \\begin{bmatrix} {1\\phantom{ 0}} & 4 \\\\ \\end{bmatrix} \\\\ \\text{bsrEndPtr} & = & \\begin{bmatrix} {1\\phantom{ 0}} & 5 \\\\ \\end{bmatrix} \\\\ \\end{matrix}\\) Further we can use a mask operator"
  },
  {
    "id": 28533,
    "content": "(specified by array bsrMaskPtr ) to update particular block row indices of \\(y\\) only because \\(y_{1}\\) is never changed The mask operator is equivalent to the following operation: \\(\\begin{bmatrix} \\\\ y_{2} \\\\ \\end{bmatrix}:=alpha \\ast \\begin{bmatrix}"
  },
  {
    "id": 28535,
    "content": "\\\\ O & A_{22} & O \\\\ \\end{bmatrix} \\ast \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ \\end{bmatrix} + beta \\ast \\begin{bmatrix} \\\\ y_{2} \\\\ \\end{bmatrix}\\) If a block row is not present in the bsrMaskPtr , then no calculation is performed on that row, and the corresponding value in y is unmodified In this case, first row block is not present in bsrMaskPtr , so bsrRowPtr[0] and bsrEndPtr[0] are not"
  },
  {
    "id": 28540,
    "content": "0}} & 5 \\\\ \\end{bmatrix} \\\\ \\end{matrix}\\) bsrxmv() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture A couple of comments on bsrxmv() : Only blockDim > 1 is supported Only CUSPARSE_OPERATION_NON_TRANSPOSE and CUSPARSE_MATRIX_TYPE_GENERAL are supported"
  },
  {
    "id": 28541,
    "content": "Parameters bsrMaskPtr , bsrRowPtr , bsrEndPtr and bsrColInd are consistent with base index, either one-based or zero-based"
  },
  {
    "id": 28542,
    "content": "bsrMaskPtr integer array of sizeOfMask elements that contains the indices corresponding to updated block rows bsrEndPtr integer array of mb elements that contains the end of the every block row plus one"
  },
  {
    "id": 28543,
    "content": "A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand-side and the solution vectors; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) = \\begin{cases} A & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\ A^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\ A^{H} &"
  },
  {
    "id": 28544,
    "content": "\\text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\ \\end{cases}\\) Although there are six combinations in terms of parameter trans and the upper (lower) triangular part of A , bsrsv2_bufferSize() returns the maximum size buffer among these combinations The buffer size depends on the dimensions mb , blockDim , and the number of nonzero blocks of the matrix nnzb If the user changes the"
  },
  {
    "id": 28545,
    "content": "matrix, it is necessary to call bsrsv2_bufferSize() again to have the correct buffer size; otherwise a segmentation fault may occur"
  },
  {
    "id": 28546,
    "content": "The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context"
  },
  {
    "id": 28548,
    "content": "The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , while the supported diagonal types are CUSPARSE_DIAG_TYPE_UNIT and CUSPARSE_DIAG_TYPE_NON_UNIT"
  },
  {
    "id": 28549,
    "content": "bsrValA array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) nonzero blocks of matrix A bsrRowPtrA integer array of mb \\(+ 1\\) elements that contains the start of every block row and the end of the last block row plus one bsrColIndA integer array of nnzb \\(( =\\) bsrRowPtrA(mb) \\(-\\) bsrRowPtrA(0) \\()\\) column indices of the nonzero blocks of matrix A A is an (mb*blockDim)x(mb*blockDim)"
  },
  {
    "id": 28550,
    "content": "sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand side and the solution vectors; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) = \\begin{cases} A & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\ A^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\ A^{H} & \\text{if trans =="
  },
  {
    "id": 28551,
    "content": "CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\ \\end{cases}\\) The block of BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW"
  },
  {
    "id": 28552,
    "content": "The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , and the fill mode and diagonal type are ignored It is expected that this function will be executed only once for a given matrix and a particular operation type"
  },
  {
    "id": 28553,
    "content": "Function bsrsv2_analysis() reports a structural zero and computes level information, which stored in the opaque structure info"
  },
  {
    "id": 28554,
    "content": "To disable level information, the user needs to specify the policy of the triangular solver as CUSPARSE_SOLVE_POLICY_NO_LEVEL Function bsrsv2_analysis() always reports the first structural zero, even when parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL No structural zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if block A(j,j) is missing for some j It is the user’s choice"
  },
  {
    "id": 28555,
    "content": "whether to call bsrsv2_solve() if bsrsv2_analysis() reports a structural zero In this case, the user can still call bsrsv2_solve() , which will return a numerical zero at the same position as a structural zero"
  },
  {
    "id": 28556,
    "content": "This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context"
  },
  {
    "id": 28557,
    "content": "policy the supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL"
  },
  {
    "id": 28558,
    "content": "A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); x and y are the right-hand-side and the solution vectors; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) = \\begin{cases} A & \\text{if trans == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\ A^{T} & \\text{if trans == CUSPARSE_OPERATION_TRANSPOSE} \\\\ A^{H} &"
  },
  {
    "id": 28559,
    "content": "\\text{if trans == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE} \\\\ \\end{cases}\\) The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW"
  },
  {
    "id": 28561,
    "content": "Although bsrsv2_solve() can be done without level information, the user still needs to be aware of consistency If bsrsv2_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrsv2_solve() can be run with or without levels On the other hand, if bsrsv2_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrsv2_solve() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise,"
  },
  {
    "id": 28564,
    "content": "In this case, CUSPARSE_SOLVE_POLICY_NO_LEVEL performs better than CUSPARSE_SOLVE_POLICY_USE_LEVEL If the user has an iterative solver, the best approach is to do bsrsv2_analysis() with CUSPARSE_SOLVE_POLICY_USE_LEVEL once Then do bsrsv2_solve() with CUSPARSE_SOLVE_POLICY_NO_LEVEL in the first run, and with CUSPARSE_SOLVE_POLICY_USE_LEVEL in the second run, and pick the fastest one to perform the"
  },
  {
    "id": 28566,
    "content": "The numerical zero of bsrsv02_solve() means there exists some block A(j,j) that is not invertible No numerical zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if A(j,j) is not invertible for some j"
  },
  {
    "id": 28567,
    "content": "The function supports the following properties if pBuffer = NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture For example, suppose L is a lower triangular matrix with unit diagonal, then the following code solves L*y=x by level information"
  },
  {
    "id": 28568,
    "content": "Suppose that L is m x m sparse matrix represented by BSR format, The number of block rows/columns is mb, and the number of nonzero blocks is nnzb Assumption: - dimension of matrix L is m(=mb*blockDim), - matrix L has nnz(=nnzb*blockDim*blockDim) nonzero elements, - handle is already created by cusparseCreate(), - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of L on device memory, - d_x is right"
  },
  {
    "id": 28570,
    "content": "cusparseMatDescr_t descr = 0 ; bsrsv2Info_t info = 0 ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1 cudaMalloc (( void ** ) & pBuffer , pBufferSize ); step 4: perform analysis cusparseDbsrsv2_analysis ( handle , dir , trans , mb , nnzb , descr , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info , policy , pBuffer ); L has unit"
  },
  {
    "id": 28571,
    "content": "diagonal, so no structural zero is reported status = cusparseXbsrsv2_zeroPivot ( handle , info , & structural_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"L(%d,%d) is missing \" , structural_zero , structural_zero ); } step 5: solve L*y = x cusparseDbsrsv2_solve ( handle , dir , trans , mb , nnzb , & alpha , descr , d_bsrVal , d_bsrRowPtr , d_bsrColInd , blockDim , info , d_x ,"
  },
  {
    "id": 28572,
    "content": "d_y , policy , pBuffer ); L has unit diagonal, so no numerical zero is reported status = cusparseXbsrsv2_zeroPivot ( handle , info , & numerical_zero ); if ( CUSPARSE_STATUS_ZERO_PIVOT == status ){ printf ( \"L(%d,%d) is zero \" , numerical_zero , numerical_zero ); } step 6: free resources cudaFree ( pBuffer ); cusparseDestroyBsrsv2Info ( info ); cusparseDestroyMatDescr ( descr ); cusparseDestroy ("
  },
  {
    "id": 28574,
    "content": "info structure with information collected during the analysis phase (that should have been passed to the solve phase unchanged)"
  },
  {
    "id": 28579,
    "content": "cusparseXbsrsv2_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrsv2_zeroPivot ( cusparseHandle_t handle , bsrsv2Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) is either structural zero or numerical zero (singular block) The routine requires no extra storage The"
  },
  {
    "id": 28580,
    "content": "routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context info info contains a structural zero or numerical zero if the user already called bsrsv2_analysis() or bsrsv2_solve() Output position if no structural or numerical"
  },
  {
    "id": 28582,
    "content": "Notice that by interchanging the rows and columns of the result you are implicitly transposing the matrix Call the gemvi() function with the cusparseOperation_t parameter set to CUSPARSE_OPERATION_NON_TRANSPOSE and with the interchanged rows and columns of the matrix stored in CSC format This (implicitly) multiplies the vector by the transpose of the matrix in the original CSR format"
  },
  {
    "id": 28583,
    "content": "The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture The function cusparsegemvi_bufferSize() returns the size of buffer used in cusparsegemvi()"
  },
  {
    "id": 28584,
    "content": "x sparse vector of nnz elements of size n if \\(\\text{op}(A)=A\\) , and size m if \\(\\text{op}(A)=A^{T}\\) y dense vector of m elements if \\(\\text{op}(A)=A\\) , and n elements if \\(\\text{op}(A)=A^{T}\\) Output y updated dense vector"
  },
  {
    "id": 28587,
    "content": "cuSPARSE Level 3 Function Reference  This chapter describes sparse linear algebra functions that perform operations between sparse and (usually tall) dense matrices"
  },
  {
    "id": 28588,
    "content": "If op(B)=B , it must be at least \\(\\max\\text{(1,\\ k)}\\) If op(B) = B , it must be at least max(1, n) It must be at least \\(\\max\\text{(1,\\ m)}\\) if op(A)=A and at least \\(\\max\\text{(1,\\ k)}\\) otherwise A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); B and X are the right-hand-side and the solution"
  },
  {
    "id": 28589,
    "content": "matrices; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) == \\text{CUSPARSE_OPERATION_NON_TRANSPOSE}\\) Although there are six combinations in terms of parameter trans and the upper (and lower) triangular part of A , bsrsm2_bufferSize() returns the maximum size of the buffer among these combinations The buffer size depends on dimension mb,blockDim and the number of nonzeros of the matrix, nnzb If the"
  },
  {
    "id": 28590,
    "content": "user changes the matrix, it is necessary to call bsrsm2_bufferSize() again to get the correct buffer size, otherwise a segmentation fault may occur pBufferSizeInBytes number of bytes of the buffer used in bsrsm2_analysis() and bsrsm2_solve() A is an (mb*blockDim)x(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA ); B and X"
  },
  {
    "id": 28591,
    "content": "are the right-hand-side and the solution matrices; \\(\\alpha\\) is a scalar; and \\(\\text{op}(A) == \\text{CUSPARSE_OPERATION_NON_TRANSPOSE}\\) and \\(\\text{op}(X) = \\begin{cases} X & \\text{if transX == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\ X^{T} & \\text{if transX == CUSPARSE_OPERATION_TRANSPOSE} \\\\ X^{H} & \\text{if transX == CUSPARSE_OPERATION_CONJUGATE_TRANSPOSE (not supported)} \\\\ \\end{cases}\\) and"
  },
  {
    "id": 28592,
    "content": "op(B) and op(X) are equal The block of BSR format is of size blockDim*blockDim , stored in column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN Function bsrsm2_analysis() reports a structural zero and computes the level information stored in opaque structure info Function bsrsm2_analysis() always reports the first"
  },
  {
    "id": 28593,
    "content": "structural zero, even if the parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL Besides, no structural zero is reported if CUSPARSE_DIAG_TYPE_UNIT is specified, even if block A(j,j) is missing for some j The user must call cusparseXbsrsm2_query_zero_pivot() to know where the structural zero is If bsrsm2_analysis() reports a structural zero, the solve will return a numerical zero in the same"
  },
  {
    "id": 28594,
    "content": "position as the structural zero but this result X is meaningless policy The supported policies are CUSPARSE_SOLVE_POLICY_NO_LEVEL and CUSPARSE_SOLVE_POLICY_USE_LEVEL The block of BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA , which is either CUSPARSE_DIRECTION_ROW or CUSPARSE_DIRECTION_COLUMN Although bsrsm2_solve() can be done"
  },
  {
    "id": 28595,
    "content": "without level information, the user still needs to be aware of consistency If bsrsm2_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrsm2_solve() can be run with or without levels On the other hand, if bsrsm2_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrsm2_solve() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is"
  },
  {
    "id": 28596,
    "content": "returned Function bsrsm02_solve() has the same behavior as bsrsv02_solve() , reporting the first numerical zero, including a structural zero The user must call cusparseXbsrsm2_query_zero_pivot() to know where the numerical zero is The computational pattern of transpose(X) with matrix X in column-major order is equivalent to X with matrix X in row-major order In-place is supported and requires that"
  },
  {
    "id": 28598,
    "content": "The function supports the following properties if pBuffer = NULL : The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context"
  },
  {
    "id": 28604,
    "content": "cusparseXbsrsm2_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrsm2_zeroPivot ( cusparseHandle_t handle , bsrsm2Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) is either a structural zero or a numerical zero (singular block) info info contains a structural zero or a"
  },
  {
    "id": 28605,
    "content": "numerical zero if the user already called bsrsm2_analysis() or bsrsm2_solve() Output position if no structural or numerical zero, position is -1; otherwise, if A(j,j) is missing or U(j,j) is zero, position=j"
  },
  {
    "id": 28608,
    "content": "cuSPARSE Extra Function Reference  This chapter describes the extra routines used to manipulate sparse matrices"
  },
  {
    "id": 28609,
    "content": "Since A and B have different sparsity patterns, cuSPARSE adopts a two-step approach to complete sparse matrix C"
  },
  {
    "id": 28610,
    "content": "In the first step, the user allocates csrRowPtrC of m+1 elements and uses function cusparseXcsrgeam2Nnz() to determine csrRowPtrC and the total number of nonzero elements In the second step, the user gathers nnzC (number of nonzero elements of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC(m)-csrRowPtrC(0)) and allocates csrValC, csrColIndC of nnzC elements respectively,"
  },
  {
    "id": 28612,
    "content": "In order to do any one of the three, the user should use the routine csr2csc() to convert \\(A\\) | \\(B\\) to \\(A^{T}\\) | \\(B^{T}\\) If either A or B is symmetric or Hermitian, then the user must extend the matrix to a full one and reconfigure the MatrixType field of the descriptor to CUSPARSE_MATRIX_TYPE_GENERAL If the sparsity pattern of matrix C is known, the user can skip the call to function"
  },
  {
    "id": 28613,
    "content": "cusparseXcsrgeam2Nnz() For example, suppose that the user has an iterative algorithm which would update A and B iteratively but keep the sparsity patterns The user can call function cusparseXcsrgeam2Nnz() once to set up the sparsity pattern of C , then call function cusparse[S|D|C|Z]geam() only for each iteration If the user wants \\(C = 0 \\times A + 1 \\times B^{T}\\) , then csr2csc() is better than"
  },
  {
    "id": 28614,
    "content": "csrgeam2() csrgeam2() is the same as csrgeam() except csrgeam2() needs explicit buffer where csrgeam() allocates the buffer internally"
  },
  {
    "id": 28615,
    "content": "csrValA array of nnzA \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A csrRowPtrA integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one csrColIndA integer array of nnzA \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A csrValB array of nnzB \\(( =\\) csrRowPtrB(m)"
  },
  {
    "id": 28616,
    "content": "\\(-\\) csrRowPtrB(0) \\()\\) nonzero elements of matrix B csrRowPtrB integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one csrColIndB integer array of nnzB \\(( =\\) csrRowPtrB(m) \\(-\\) csrRowPtrB(0) \\()\\) column indices of the nonzero elements of matrix B Output csrValC array of nnzC \\(( =\\) csrRowPtrC(m) \\(-\\) csrRowPtrC(0) \\()\\) nonzero"
  },
  {
    "id": 28617,
    "content": "elements of matrix C csrRowPtrC integer array of m \\(+ 1\\) elements that contains the start of every row and the end of the last row plus one csrColIndC integer array of nnzC \\(( =\\) csrRowPtrC(m) \\(-\\) csrRowPtrC(0) \\()\\) column indices of the nonzero elements of matrix C"
  },
  {
    "id": 28618,
    "content": "cuSPARSE Preconditioners Reference  This chapter describes the routines that implement different preconditioners"
  },
  {
    "id": 28622,
    "content": "Incomplete Cholesky Factorization: level 0 [DEPRECATED]  Different algorithms for ic0 are discussed in this section"
  },
  {
    "id": 28623,
    "content": "If the user changes the matrix, it is necessary to call csric02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur"
  },
  {
    "id": 28624,
    "content": "csrValA array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A csrColIndA integer array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) column indices of the nonzero elements of matrix A"
  },
  {
    "id": 28625,
    "content": "Function csric02_analysis() reports a structural zero and computes level information stored in the opaque structure info"
  },
  {
    "id": 28627,
    "content": "To disable level information, the user must specify the policy of csric02_analysis() and csric02() as CUSPARSE_SOLVE_POLICY_NO_LEVEL Function csric02_analysis() always reports the first structural zero, even if the policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL It is the user’s choice whether to call csric02() if csric02_analysis() reports a structural zero In this case, the user can still call"
  },
  {
    "id": 28628,
    "content": "csric02() , which will return a numerical zero at the same position as the structural zero Although csric02() can be done without level information, the user still needs to be aware of consistency If csric02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , csric02() can be run with or without levels On the other hand, if csric02_analysis() is called with"
  },
  {
    "id": 28629,
    "content": "CUSPARSE_SOLVE_POLICY_NO_LEVEL , csric02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned Function csric02() only takes the lower triangular part of matrix A to perform factorization The matrix type must be CUSPARSE_MATRIX_TYPE_GENERAL , the fill mode and diagonal type are ignored, and the strictly upper triangular part is ignored and never"
  },
  {
    "id": 28630,
    "content": "touched In other words, from the point of view of csric02() A is Hermitian and only the lower triangular part is provided"
  },
  {
    "id": 28631,
    "content": "Note In practice, a positive definite matrix may not have incomplete cholesky factorization To the best of our knowledge, only matrix M can guarantee the existence of incomplete cholesky factorization If csric02() failed cholesky factorization and reported a numerical zero, it is possible that incomplete cholesky factorization does not exist For example, suppose A is a real m × m matrix, the"
  },
  {
    "id": 28632,
    "content": "following code solves the precondition system M*y = x where M is the product of Cholesky factorization L and its transpose"
  },
  {
    "id": 28633,
    "content": "\\(M = LL^{H}\\)   Suppose that A is m x m sparse matrix represented by CSR format,   Assumption:   - handle is already created by cusparseCreate(),   - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory,   - d_x is right hand side vector on device memory,   - d_y is solution vector on device memory"
  },
  {
    "id": 28634,
    "content": "cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; csric02Info_t info_M = 0 ; csrsv2Info_t info_L = 0 ; csrsv2Info_t info_Lt = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_Lt ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1"
  },
  {
    "id": 28635,
    "content": "cudaMalloc (( void ** ) & pBuffer , pBufferSize );   step 4: perform analysis of incomplete Cholesky on M   perform analysis of triangular solve on L   perform analysis of triangular solve on L'   The lower triangular part of M has the same sparsity pattern as L, so   we can do analysis of csric02 and csrsv2 simultaneously"
  },
  {
    "id": 28636,
    "content": "csrValA_valM array of nnz \\(( =\\) csrRowPtrA(m) \\(-\\) csrRowPtrA(0) \\()\\) nonzero elements of matrix A Output csrValA_valM matrix containing the incomplete-Cholesky lower triangular factor"
  },
  {
    "id": 28641,
    "content": "cusparseXcsric02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXcsric02_zeroPivot ( cusparseHandle_t handle , csric02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero; otherwise, position=-1 info info contains structural"
  },
  {
    "id": 28643,
    "content": "The buffer size depends on the dimensions of mb , blockDim , and the number of nonzero blocks of the matrix nnzb If the user changes the matrix, it is necessary to call bsric02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major as determined by parameter dirA ,"
  },
  {
    "id": 28645,
    "content": "Function bsric02_analysis() reports structural zero and computes level information stored in the opaque structure info"
  },
  {
    "id": 28646,
    "content": "To disable level information, the user needs to specify the parameter policy of bsric02[_analysis| ] as CUSPARSE_SOLVE_POLICY_NO_LEVEL Function bsric02_analysis always reports the first structural zero, even when parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL It is the user’s choice whether to call bsric02() if bsric02_analysis() reports a structural zero In this case, the user can still call"
  },
  {
    "id": 28647,
    "content": "bsric02() , which returns a numerical zero in the same position as the structural zero Although bsric02() can be done without level information, the user must be aware of consistency If bsric02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsric02() can be run with or without levels On the other hand, if bsric02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL ,"
  },
  {
    "id": 28648,
    "content": "bsric02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned The numerical zero of bsric02() means there exists some block Lj,j) that is not invertible The bsric02() function only takes the lower triangular part of matrix A to perform factorization In other words, from the point of view of bsric02() , A is Hermitian and only the lower triangular"
  },
  {
    "id": 28650,
    "content": "The following code solves precondition system M*y = x , where M is the product of Cholesky factorization L and its transpose"
  },
  {
    "id": 28651,
    "content": "\\(M = LL^{H}\\)   Suppose that A is m x m sparse matrix represented by BSR format,   The number of block rows/columns is mb, and   the number of nonzero blocks is nnzb"
  },
  {
    "id": 28652,
    "content": "Assumption:   - handle is already created by cusparseCreate(),   - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of A on device memory,   - d_x is right hand side vector on device memory,   - d_y is solution vector on device memory"
  },
  {
    "id": 28653,
    "content": "cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; bsric02Info_t info_M = 0 ; bsrsv2Info_t info_L = 0 ; bsrsv2Info_t info_Lt = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_Lt ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1"
  },
  {
    "id": 28654,
    "content": "cudaMalloc (( void ** ) & pBuffer , pBufferSize ); step 4: perform analysis of incomplete Cholesky on M perform analysis of triangular solve on L perform analysis of triangular solve on L' The lower triangular part of M has the same sparsity pattern as L, so we can do analysis of bsric02 and bsrsv2 simultaneously Output bsrValA matrix containing the incomplete-Cholesky lower triangular factor"
  },
  {
    "id": 28659,
    "content": "cusparseXbsric02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsric02_zeroPivot ( cusparseHandle_t handle , bsric02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero (the block is not positive definite) info info contains a"
  },
  {
    "id": 28660,
    "content": "structural zero or a numerical zero if the user already called bsric02_analysis() or bsric02() Output position If no structural or numerical zero, position is -1, otherwise if A(j,j) is missing or L(j,j) is not positive definite, position=j"
  },
  {
    "id": 28664,
    "content": "Incomplete LU Factorization: level 0 [DEPRECATED]  Different algorithms for ilu0 are discussed in this section"
  },
  {
    "id": 28669,
    "content": "cusparsecsrilu02_numericBoost() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseScsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , float * boost_val ) cusparseStatus_t cusparseDcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , double *"
  },
  {
    "id": 28670,
    "content": "boost_val ) cusparseStatus_t cusparseCcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , cuComplex * boost_val ) cusparseStatus_t cusparseZcsrilu02_numericBoost ( cusparseHandle_t handle , csrilu02Info_t info , int enable_boost , double * tol , cuDoubleComplex * boost_val ) The user can use a boost value to replace a numerical value in"
  },
  {
    "id": 28671,
    "content": "incomplete LU factorization The tol is used to determine a numerical zero, and the boost_val is used to replace a numerical zero To enable a boost value, the user has to set parameter enable_boost to 1 before calling csrilu02() To disable a boost value, the user can call csrilu02_numericBoost() again with parameter enable_boost=0"
  },
  {
    "id": 28672,
    "content": "The buffer size depends on the dimension m and nnz , the number of nonzeros of the matrix If the user changes the matrix, it is necessary to call csrilu02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur"
  },
  {
    "id": 28673,
    "content": "Function csrilu02_analysis() reports a structural zero and computes level information stored in the opaque structure info The level information can extract more parallelism during incomplete LU factorization; however csrilu02() can be done without level information To disable level information, the user must specify the policy of csrilu02() as CUSPARSE_SOLVE_POLICY_NO_LEVEL It is the user’s"
  },
  {
    "id": 28674,
    "content": "choice whether to call csrilu02() if csrilu02_analysis() reports a structural zero In this case, the user can still call csrilu02() , which will return a numerical zero at the same position as the structural zero Although csrilu02() can be done without level information, the user still needs to be aware of consistency If csrilu02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL ,"
  },
  {
    "id": 28675,
    "content": "csrilu02() can be run with or without levels On the other hand, if csrilu02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , csrilu02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise, CUSPARSE_STATUS_INVALID_VALUE is returned"
  },
  {
    "id": 28676,
    "content": "For example, suppose A is a real m × m matrix, the following code solves precondition system M*y = x where M is the product of LU factors L and U"
  },
  {
    "id": 28677,
    "content": "Suppose that A is m x m sparse matrix represented by CSR format,   Assumption:   - handle is already created by cusparseCreate(),   - (d_csrRowPtr, d_csrColInd, d_csrVal) is CSR of A on device memory,   - d_x is right hand side vector on device memory,   - d_y is solution vector on device memory"
  },
  {
    "id": 28678,
    "content": "cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; cusparseMatDescr_t descr_U = 0 ; csrilu02Info_t info_M = 0 ; csrsv2Info_t info_L = 0 ; csrsv2Info_t info_U = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_U ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1"
  },
  {
    "id": 28679,
    "content": "cudaMalloc (( void ** ) & pBuffer , pBufferSize ); step 4: perform analysis of incomplete Cholesky on M perform analysis of triangular solve on L perform analysis of triangular solve on U The lower(upper) triangular part of M has the same sparsity pattern as L(U), we can do analysis of csrilu0 and csrsv2 simultaneously Output csrValA_valM matrix containing the incomplete-LU lower and upper"
  },
  {
    "id": 28685,
    "content": "cusparseXcsrilu02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXcsrilu02_zeroPivot ( cusparseHandle_t handle , csrilu02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero; otherwise, position=-1"
  },
  {
    "id": 28686,
    "content": "The routine requires no extra storage The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle Handle to the cuSPARSE library context"
  },
  {
    "id": 28687,
    "content": "info info contains structural zero or numerical zero if the user already called csrilu02_analysis() or csrilu02() Parameter tol is used to determine a numerical zero, and boost_val is used to replace a numerical zero The behavior is as follows: if tol >= fabs(A(j,j)) , then reset each diagonal element of block A(j,j) by boost_val"
  },
  {
    "id": 28688,
    "content": "To enable a boost value, the user sets parameter enable_boost to 1 before calling bsrilu02() To disable the boost value, the user can call bsrilu02_numericBoost() with parameter enable_boost=0"
  },
  {
    "id": 28689,
    "content": "\\(A \\approx LU\\) A is an (mb*blockDim)*(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA"
  },
  {
    "id": 28690,
    "content": "If the user changes the matrix, it is necessary to call bsrilu02_bufferSize() again to have the correct buffer size; otherwise, a segmentation fault may occur"
  },
  {
    "id": 28692,
    "content": "\\(A \\approx LU\\) A is an (mb*blockDim)×(mb*blockDim) sparse matrix that is defined in BSR storage format by the three arrays bsrValA , bsrRowPtrA , and bsrColIndA"
  },
  {
    "id": 28693,
    "content": "Function bsrilu02_analysis() reports a structural zero and computes level information stored in the opaque structure info"
  },
  {
    "id": 28694,
    "content": "To disable level information, the user needs to specify the parameter policy of bsrilu02[_analysis| ] as CUSPARSE_SOLVE_POLICY_NO_LEVEL Function bsrilu02_analysis() always reports the first structural zero, even with parameter policy is CUSPARSE_SOLVE_POLICY_NO_LEVEL It is the user’s choice whether to call bsrilu02() if bsrilu02_analysis() reports a structural zero In this case, the user can"
  },
  {
    "id": 28695,
    "content": "still call bsrilu02() , which will return a numerical zero at the same position as the structural zero"
  },
  {
    "id": 28696,
    "content": "The block in BSR format is of size blockDim*blockDim , stored as column-major or row-major determined by parameter dirA , which is either CUSPARSE_DIRECTION_COLUMN or CUSPARSE_DIRECTION_ROW"
  },
  {
    "id": 28697,
    "content": "Although bsrilu02() can be used without level information, the user must be aware of consistency If bsrilu02_analysis() is called with policy CUSPARSE_SOLVE_POLICY_USE_LEVEL , bsrilu02() can be run with or without levels On the other hand, if bsrilu02_analysis() is called with CUSPARSE_SOLVE_POLICY_NO_LEVEL , bsrilu02() can only accept CUSPARSE_SOLVE_POLICY_NO_LEVEL ; otherwise,"
  },
  {
    "id": 28698,
    "content": "CUSPARSE_STATUS_INVALID_VALUE is returned The numerical zero of bsrilu02() means there exists some block U(j,j) that is not invertible"
  },
  {
    "id": 28699,
    "content": "The following code solves precondition system M*y = x , where M is the product of LU factors L and U"
  },
  {
    "id": 28700,
    "content": "Suppose that A is m x m sparse matrix represented by BSR format,   The number of block rows/columns is mb, and   the number of nonzero blocks is nnzb Assumption:   - handle is already created by cusparseCreate(),   - (d_bsrRowPtr, d_bsrColInd, d_bsrVal) is BSR of A on device memory,   - d_x is right hand side vector on device memory"
  },
  {
    "id": 28701,
    "content": "cusparseMatDescr_t descr_M = 0 ; cusparseMatDescr_t descr_L = 0 ; cusparseMatDescr_t descr_U = 0 ; bsrilu02Info_t info_M = 0 ; bsrsv2Info_t info_L = 0 ; bsrsv2Info_t info_U = 0 ; int pBufferSize_M ; int pBufferSize_L ; int pBufferSize_U ; int pBufferSize ; void * pBuffer = 0 ; int structural_zero ; int numerical_zero ; const double alpha = 1"
  },
  {
    "id": 28702,
    "content": "cudaMalloc (( void ** ) & pBuffer , pBufferSize );   step 4: perform analysis of incomplete LU factorization on M   perform analysis of triangular solve on L   perform analysis of triangular solve on U   The lower(upper) triangular part of M has the same sparsity pattern as L(U),   we can do analysis of bsrilu0 and bsrsv2 simultaneously"
  },
  {
    "id": 28704,
    "content": "Output bsrValA matrix containing the incomplete-LU lower and upper triangular factors See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28709,
    "content": "cusparseXbsrilu02_zeroPivot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseXbsrilu02_zeroPivot ( cusparseHandle_t handle , bsrilu02Info_t info , int * position ) If the returned error code is CUSPARSE_STATUS_ZERO_PIVOT , position=j means A(j,j) has either a structural zero or a numerical zero (the block is not invertible) info info contains"
  },
  {
    "id": 28710,
    "content": "structural zero or numerical zero if the user already called bsrilu02_analysis() or bsrilu02() Output position if no structural or numerical zero, position is -1; otherwise if A(j,j) is missing or U(j,j) is not invertible, position=j"
  },
  {
    "id": 28715,
    "content": "\\(A \\ast X = B\\) The coefficient matrix A of each of these tri-diagonal linear system is defined with three vectors corresponding to its lower ( dl ), main ( d ), and upper ( du ) matrix diagonals; the right-hand sides are stored in the dense matrix B Assuming A is of size m and base-1, dl , d and du are defined by the following formula: dl(i) := A(i, i-1) for i=1,2, ,m The first element of dl is"
  },
  {
    "id": 28717,
    "content": "d(i) = A(i,i) for i=1,2, ,m du(i) = A(i,i+1) for i=1,2, ,m The last element of du is out-of-bound ( du(m) := A(m,m+1) ), so du(m) = 0"
  },
  {
    "id": 28718,
    "content": "The routine does perform pivoting, which usually results in more accurate and more stable results than cusparsegtsv_nopivot() or cusparsegtsv2_nopivot() at the expense of some execution time The routine does not perform any pivoting and uses a combination of the Cyclic Reduction (CR) and the Parallel Cyclic Reduction (PCR) algorithms to find the solution"
  },
  {
    "id": 28723,
    "content": "Batched Tridiagonal Solve  Different algorithms for batched tridiagonal solve are discussed in this section"
  },
  {
    "id": 28724,
    "content": "The different matrices are assumed to be of the same size and are stored with a fixed batchStride in memory The lower diagonal \\(dl^{(i)}\\) that corresponds to the i th linear system starts at location dl+batchStride×i in memory The main diagonal \\(d^{(i)}\\) that corresponds to the i th linear system starts at location d+batchStride×i in memory The upper diagonal \\(du^{(i)}\\) that corresponds to"
  },
  {
    "id": 28725,
    "content": "the i th linear system starts at location du+batchStride×i in memory The right-hand-side \\(x^{(i)}\\) that corresponds to the i th linear system starts at location x+batchStride×i in memory batchStride stride (number of elements) that separates the vectors of every system (must be at least m ) batchStride stride (number of elements) that separates the vectors of every system (must be at least n )"
  },
  {
    "id": 28729,
    "content": "From stability perspective, cuThomas is not numerically stable because it does not have pivoting From performance perspective, LU with partial pivoting and QR is about 10% to 20% slower than cuThomas algo algo = 0: cuThomas (unstable algorithm); algo = 1: LU with pivoting (stable algorithm); algo = 2: QR (stable algorithm) m the size of the linear system"
  },
  {
    "id": 28735,
    "content": "Batched Pentadiagonal Solve  Different algorithms for batched pentadiagonal solve are discussed in this section"
  },
  {
    "id": 28736,
    "content": "Assuming A is of size m and base-1, ds , dl , d , du and dw are defined by the following formula: ds(i) := A(i, i-2) for i=1,2, ,m The first two elements of ds is out-of-bound ( ds(1) := A(1,-1) , ds(2) := A(2,0) ), so ds(1) = 0 and ds(2) = 0 dl(i) := A(i, i-1) for i=1,2, ,m The first element of dl is out-of-bound ( dl(1) := A(1,0) ), so dl(1) = 0 dw(i) = A(i,i+2) for i=1,2, ,m The last two"
  },
  {
    "id": 28737,
    "content": "elements of dw is out-of-bound ( dw(m-1) := A(m-1,m+1) , dw(m) := A(m,m+2) ), so dw(m-1) = 0 and dw(m) = 0"
  },
  {
    "id": 28738,
    "content": "The function supports the following properties if pBuffer = NULL The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context"
  },
  {
    "id": 28739,
    "content": "ds dense array containing the lower diagonal (distance 2 to the diagonal) of the penta-diagonal linear system dl dense array containing the lower diagonal (distance 1 to the diagonal) of the penta-diagonal linear system du dense array containing the upper diagonal (distance 1 to the diagonal) of the penta-diagonal linear system dw dense array containing the upper diagonal (distance 2 to the"
  },
  {
    "id": 28745,
    "content": "cuSPARSE Reorderings Reference  This chapter describes the reordering routines used to manipulate sparse matrices"
  },
  {
    "id": 28746,
    "content": "The coloring is an assignment of colors (integer numbers) to nodes, such that neighboring nodes have distinct colors An approximate coloring algorithm is used in this routine, and is stopped when a certain percentage of nodes has been colored The rest of the nodes are assigned distinct colors (an increasing sequence of integers numbers, starting from the last integer used previously) The last two"
  },
  {
    "id": 28747,
    "content": "auxiliary routines can be used to extract the resulting number of colors, their assignment and the associated reordering The reordering is such that nodes that have been assigned the same color are reordered to be next to each other"
  },
  {
    "id": 28748,
    "content": "The matrix A passed to this routine, must be stored as a general matrix and have a symmetric sparsity pattern"
  },
  {
    "id": 28749,
    "content": "csrRowPtrA integer array of m+1 elements that contains the start of every row and the end of the last row plus one"
  },
  {
    "id": 28754,
    "content": "Output ncolors The number of distinct colors used (at most the size of the matrix, but likely much smaller)"
  },
  {
    "id": 28755,
    "content": "coloring The resulting coloring permutation reordering The resulting reordering permutation (untouched if NULL) See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28758,
    "content": "cuSPARSE Format Conversion Reference  This chapter describes the conversion routines between different sparse and dense storage formats"
  },
  {
    "id": 28759,
    "content": "coosort , csrsort , cscsort , and csru2csr are sorting routines without malloc inside, the following table estimates the buffer size routine buffer size maximum problem size if buffer is limited by 2GB coosort > 16*n bytes 125M csrsort or cscsort > 20*n bytes 100M csru2csr 'd' > 28*n bytes ; 'z' > 36*n bytes 71M for ‘d’ and 55M for ‘z’ 5"
  },
  {
    "id": 28762,
    "content": "Let m(=mb*blockDim) be the number of rows of A and n(=nb*blockDim) be number of columns of A , then A and C are m*n sparse matrices The BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) nonzero blocks, whereas the sparse matrix A contains nnz(=nnzb*blockDim*blockDim) elements The requirements are as follows: csrRowPtrC of m+1 elements csrValC of nnz elements csrColIndC of nnz"
  },
  {
    "id": 28763,
    "content": "elements The general procedure is as follows: Given BSR format (bsrRowPtrA, bsrcolIndA, bsrValA) and blocks of BSR format are stored in column-major order bsrRowPtrA integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix A csrRowPtrC integer array of m+1 elements that contains the start of every row and the end of the last row"
  },
  {
    "id": 28764,
    "content": "plus one of matrix C This sparsity pattern of the result matrix can also be seen as the transpose of the original sparse matrix, but the memory layout of a block does not change"
  },
  {
    "id": 28765,
    "content": "The user must call gebsr2gebsc_bufferSize() to determine the size of the buffer required by gebsr2gebsc() , allocate the buffer, and pass the buffer pointer to gebsr2gebsc()"
  },
  {
    "id": 28766,
    "content": "The routine requires no extra storage if pBuffer = NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input handle handle to the cuSPARSE library context"
  },
  {
    "id": 28767,
    "content": "bsrRowPtr integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one"
  },
  {
    "id": 28768,
    "content": "If rowBlockDimA=1 and colBlockDimA=1 , cusparse[S|D|C|Z]gebsr2gebsr() is the same as cusparse[S|D|C|Z]csr2gebsr() If rowBlockDimC=1 and colBlockDimC=1 , cusparse[S|D|C|Z]gebsr2gebsr() is the same as cusparse[S|D|C|Z]gebsr2csr()"
  },
  {
    "id": 28769,
    "content": "A is an m*n sparse matrix where m(=mb*rowBlockDim) is the number of rows of A , and n(=nb*colBlockDim) is the number of columns of A"
  },
  {
    "id": 28770,
    "content": "The general BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) nonzero blocks The matrix C is also general BSR format with a different block size, rowBlockDimC*colBlockDimC If m is not a multiple of rowBlockDimC , or n is not a multiple of colBlockDimC , zeros are filled in"
  },
  {
    "id": 28771,
    "content": "First, the user allocates bsrRowPtrC of mc+1 elements and uses function cusparseXgebsr2gebsrNnz() to determine the number of nonzero block columns per block row of matrix C Second, the user gathers nnzc (number of non-zero block columns of matrix C ) from either (nnzc=*nnzTotalDevHostPtr) or (nnzc=bsrRowPtrC[mc]-bsrRowPtrC[0]) and allocates bsrValC of nnzc*rowBlockDimC*colBlockDimC elements and"
  },
  {
    "id": 28773,
    "content": "The user must call gebsr2gebsr_bufferSize() to know the size of the buffer required by gebsr2gebsr() , allocate the buffer, and pass the buffer pointer to gebsr2gebsr()"
  },
  {
    "id": 28774,
    "content": "The general procedure is as follows:   Given general BSR format (bsrRowPtrA, bsrColIndA, bsrValA) and   blocks of BSR format are stored in column-major order"
  },
  {
    "id": 28775,
    "content": "bsrRowPtrC integer array of mc+1 elements that contains the start of every block row and the end of the last block row plus one of matrix C"
  },
  {
    "id": 28776,
    "content": "Let m(=mb*rowBlockDim) be number of rows of A and n(=nb*colBlockDim) be number of columns of A , then A and C are m*n sparse matrices The general BSR format of A contains nnzb(=bsrRowPtrA[mb] - bsrRowPtrA[0]) non-zero blocks, whereas sparse matrix A contains nnz(=nnzb*rowBlockDim*colBlockDim) elements The requirements are as follows: csrRowPtrC of m+1 elements csrValC of nnz elements csrColIndC"
  },
  {
    "id": 28777,
    "content": "of nnz elements The general procedure is as follows: Given general BSR format (bsrRowPtrA, bsrColIndA, bsrValA) and blocks of BSR format are stored in column-major order cusparseDirection_t dir = CUSPARSE_DIRECTION_COLUMN ; int m = mb * rowBlockDim ; int n = nb * colBlockDim ; int nnzb = bsrRowPtrA [ mb ] - bsrRowPtrA [ 0 ]; number of blocks int nnz = nnzb * rowBlockDim * colBlockDim ; number of"
  },
  {
    "id": 28778,
    "content": "elements cudaMalloc (( void ** ) & csrRowPtrC , sizeof ( int ) * ( m + 1 )); cudaMalloc (( void ** ) & csrColIndC , sizeof ( int ) * nnz ); cudaMalloc (( void ** ) & csrValC , sizeof ( float ) * nnz ); cusparseSgebsr2csr ( handle , dir , mb , nb , descrA , bsrValA , bsrRowPtrA , bsrColIndA , rowBlockDim , colBlockDim , descrC , csrValC , csrRowPtrC , csrColIndC ); The routine requires no extra"
  },
  {
    "id": 28779,
    "content": "storage The routine supports asynchronous execution The routine supports CUDA graph capture Input handle handle to the cuSPARSE library context The matrix A is a m*n sparse matrix and matrix C is a (mb*rowBlockDim)*(nb*colBlockDim) sparse matrix, where mb(=(m+rowBlockDim-1)/rowBlockDim) is the number of block rows of C , and nb(=(n+colBlockDim-1)/colBlockDim) is the number of block columns of C If"
  },
  {
    "id": 28780,
    "content": "m is not multiple of rowBlockDim or n is not multiple of colBlockDim , zeros are filled in First, the user allocates bsrRowPtrC of mb+1 elements and uses function cusparseXcsr2gebsrNnz() to determine the number of nonzero block columns per block row Second, the user gathers nnzb (number of nonzero block columns of matrix C ) from either (nnzb=*nnzTotalDevHostPtr) or"
  },
  {
    "id": 28781,
    "content": "(nnzb=bsrRowPtrC[mb]-bsrRowPtrC[0]) and allocates bsrValC of nnzb*rowBlockDim*colBlockDim elements and bsrColIndC of nnzb integers"
  },
  {
    "id": 28782,
    "content": "The user must obtain the size of the buffer required by csr2gebsr() by calling csr2gebsr_bufferSize() , allocate the buffer, and pass the buffer pointer to csr2gebsr()"
  },
  {
    "id": 28783,
    "content": "The general procedure is as follows:   Given CSR format (csrRowPtrA, csrColIndA, csrValA) and   blocks of BSR format are stored in column-major order"
  },
  {
    "id": 28784,
    "content": "csrRowPtrA integer array of m+1 elements that contains the start of every row and the end of the last row plus one of matrix A bsrRowPtrC integer array of mb+1 elements that contains the start of every block row and the end of the last block row plus one of matrix C"
  },
  {
    "id": 28789,
    "content": "cusparsecoo2csr()  cusparseStatus_t cusparseXcoo2csr ( cusparseHandle_t handle , const int * cooRowInd , int nnz , int m , int * csrRowPtr , cusparseIndexBase_t idxBase ) This function converts the array containing the uncompressed row indices (corresponding to COO format) into an array of compressed row pointers (corresponding to CSR format) It can also be used to convert the array containing"
  },
  {
    "id": 28790,
    "content": "the uncompressed column indices (corresponding to COO format) into an array of column pointers (corresponding to CSC format) nnz number of non-zeros of the sparse matrix (that is also the length of array cooRowInd ) Output csrRowPtr integer array of m+1 elements that contains the start of every row and the end of the last row plus one"
  },
  {
    "id": 28794,
    "content": "cusparsecsr2coo()  cusparseStatus_t cusparseXcsr2coo ( cusparseHandle_t handle , const int * csrRowPtr , int nnz , int m , int * cooRowInd , cusparseIndexBase_t idxBase ) This function converts the array containing the compressed row pointers (corresponding to CSR format) into an array of uncompressed row indices (corresponding to COO format) It can also be used to convert the array containing"
  },
  {
    "id": 28795,
    "content": "the compressed column indices (corresponding to CSC format) into an array of uncompressed column indices (corresponding to COO format) csrRowPtr integer array of m+1 elements that contains the start of every row and the end of the last row plus one nnz number of nonzeros of the sparse matrix (that is also the length of array cooRowInd ) Notice that this routine can also be used to convert a matrix"
  },
  {
    "id": 28797,
    "content": "It is executed asynchronously with respect to the host, and it may return control to the application on the host before the result is ready"
  },
  {
    "id": 28798,
    "content": "The function cusparseCsr2cscEx2_bufferSize() returns the size of the workspace needed by cusparseCsr2cscEx2() User needs to allocate a buffer of this size and give that buffer to cusparseCsr2cscEx2() as an argument"
  },
  {
    "id": 28800,
    "content": "dirA direction that specifies whether to count nonzero elements by CUSPARSE_DIRECTION_ROW or by CUSPARSE_DIRECTION_COLUMN Output nnzPerRowColumn array of size m or n containing the number of nonzero elements per row or column, respectively"
  },
  {
    "id": 28804,
    "content": "cusparseCreateIdentityPermutation() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseCreateIdentityPermutation ( cusparseHandle_t handle , int n , int * p ); This function creates an identity map The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host"
  },
  {
    "id": 28805,
    "content": "description handle host handle to the cuSPARSE library context Output parameter device or host description p device integer array of dimensions n"
  },
  {
    "id": 28809,
    "content": "cusparseXcoosort()  cusparseStatus_t cusparseXcoosort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * cooRows , const int * cooCols , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcoosortByRow ( cusparseHandle_t handle , int m , int n , int nnz , int * cooRows , int * cooCols , int * P , void * pBuffer ) cusparseStatus_t cusparseXcoosortByColumn ("
  },
  {
    "id": 28810,
    "content": "cusparseHandle_t handle , int m , int n , int nnz , int * cooRows , int * cooCols , int * P , void * pBuffer ); This function sorts COO format A is an m×n sparse matrix that is defined in COO storage format by the three arrays cooVals , cooRows , and cooCols coosort uses stable sort on signed integer, so the value of cooRows or cooCols can be negative If the user wants to compute sorted cooVal , P"
  },
  {
    "id": 28811,
    "content": "must be set as 0:1:(nnz-1) before coosort() , and after coosort() , new sorted value array satisfies cooVal_sorted = cooVal(P)"
  },
  {
    "id": 28812,
    "content": "This usually happens if the user only reads a COO array first and needs to decide the dimension m or n later"
  },
  {
    "id": 28813,
    "content": "The routine requires no extra storage if pBuffer = NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available Input parameter device or host description handle host handle to the cuSPARSE library context pBuffer device buffer allocated by the user; the size is"
  },
  {
    "id": 28815,
    "content": "Output parameter device or host description cooRows device integer array of nnz sorted row indices of A"
  },
  {
    "id": 28816,
    "content": "See cusparseStatus_t for the description of the return status Please visit cuSPARSE Library Samples - cusparseXcoosortByRow for a code example"
  },
  {
    "id": 28820,
    "content": "cusparseXcsrsort()  cusparseStatus_t cusparseXcsrsort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * csrRowPtr , const int * csrColInd , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcsrsort ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const int * csrRowPtr , int * csrColInd , int * P , void * pBuffer ) This"
  },
  {
    "id": 28822,
    "content": "If the user wants to compute sorted csrVal , P must be set as 0:1:(nnz-1) before csrsort() , and after csrsort() , new sorted value array satisfies csrVal_sorted = csrVal(P)"
  },
  {
    "id": 28823,
    "content": "csrRowsPtr device integer array of m+1 elements that contains the start of every row and the end of the last row plus one"
  },
  {
    "id": 28825,
    "content": "Output parameter device or host description csrColInd device integer array of nnz sorted column indices of A"
  },
  {
    "id": 28829,
    "content": "cusparseXcscsort()  cusparseStatus_t cusparseXcscsort_bufferSizeExt ( cusparseHandle_t handle , int m , int n , int nnz , const int * cscColPtr , const int * cscRowInd , size_t * pBufferSizeInBytes ) cusparseStatus_t cusparseXcscsort ( cusparseHandle_t handle , int m , int n , int nnz , const cusparseMatDescr_t descrA , const int * cscColPtr , int * cscRowInd , int * P , void * pBuffer ) This"
  },
  {
    "id": 28831,
    "content": "If the user wants to compute sorted cscVal , P must be set as 0:1:(nnz-1) before cscsort() , and after cscsort() , new sorted value array satisfies cscVal_sorted = cscVal(P)"
  },
  {
    "id": 28832,
    "content": "cscColPtr device integer array of n+1 elements that contains the start of every column and the end of the last column plus one"
  },
  {
    "id": 28834,
    "content": "If the user has a matrix A of CSR format which is unsorted, and implements his own code (which can be CPU or GPU kernel) based on this special order (for example, diagonal first, then lower triangle, then upper triangle), and wants to convert it to CSR format when calling CUSPARSE library, and then convert it back when doing something else on his/her kernel"
  },
  {
    "id": 28835,
    "content": "For example, suppose the user wants to solve a linear system Ax=b by the following iterative scheme \\(x^{(k+1)} = x^{(k)} + L^{(-1)}*(b - Ax^{(k)})\\) The code heavily uses SpMV and triangular solve"
  },
  {
    "id": 28836,
    "content": "Assume that the user has an in-house design of SpMV (Sparse Matrix-Vector multiplication) based on special order of A do step 1: compute residual vector r = b - A x (k) by in-house SpMV step 2: B := sort(A), and L is lower triangular part of B (only sort A once and keep the permutation vector) step 3: solve z = L (-1) * ( b - A x (k) ) by cusparseXcsrsv step 4: add correction x (k+1) = x (k) + z"
  },
  {
    "id": 28837,
    "content": "step 5: A := unsort(B) (use permutation vector to get back the unsorted CSR) until convergence The requirements of step 2 and step 5 are In-place operation The conversion between unsorted CSR and sorted CSR may needs several times, but the function only generates the permutation vector P once In order to keep the permutation vector invisible, we need an opaque structure called csru2csrInfo Then"
  },
  {
    "id": 28838,
    "content": "two functions ( cusparseCreateCsru2csrInfo , cusparseDestroyCsru2csrInfo ) are used to initialize and to destroy the opaque structure cusparse[S|D|C|Z]csru2csr performs forward transformation from unsorted CSR to sorted CSR First call uses csrsort to generate the permutation vector P , and subsequent call uses P to do transformation cusparse[S|D|C|Z]csr2csru performs backward transformation from"
  },
  {
    "id": 28840,
    "content": "The routine cusparsecsru2csr() has the following properties: The routine requires no extra storage if pBuffer = NULL The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsecsr2csru() has the following properties if pBuffer = NULL : The routine"
  },
  {
    "id": 28841,
    "content": "requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture The following tables describe parameters of csr2csru_bufferSizeExt and csr2csru"
  },
  {
    "id": 28843,
    "content": "The supported matrix type is CUSPARSE_MATRIX_TYPE_GENERAL , Also, the supported index bases are CUSPARSE_INDEX_BASE_ZERO and CUSPARSE_INDEX_BASE_ONE"
  },
  {
    "id": 28845,
    "content": "Output parameter device or host description csrVal device array of nnz sorted nonzero elements of matrix A"
  },
  {
    "id": 28846,
    "content": "Given a dense matrix A and a non-negative value threshold , the function returns a sparse matrix C , defined by \\(\\begin{matrix} {{C(i,j)} = {A(i,j)}} & \\text{if\\ |A(i,j)|\\ >\\ threshold} \\\\ \\end{matrix}\\) The implementation adopts a two-step approach to do the conversion"
  },
  {
    "id": 28847,
    "content": "First, the user allocates csrRowPtrC of m+1 elements and uses function pruneDense2csrNnz() to determine the number of nonzeros columns per row Second, the user gathers nnzC (number of nonzeros of matrix C ) from either (nnzC=*nnzTotalDevHostPtr) or (nnzC=csrRowPtrC[m]-csrRowPtrC[0]) and allocates csrValC of nnzC elements and csrColIndC of nnzC integers"
  },
  {
    "id": 28848,
    "content": "The user must obtain the size of the buffer required by pruneDense2csr() by calling pruneDense2csr_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneDense2csr()"
  },
  {
    "id": 28849,
    "content": "The routine cusparsepruneDense2csrNnz() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparseDpruneDense2csr() has the following"
  },
  {
    "id": 28850,
    "content": "properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context"
  },
  {
    "id": 28852,
    "content": "Output parameter device or host description nnzTotalDevHostPtr device or host total number of nonzero of matrix C"
  },
  {
    "id": 28853,
    "content": "csrRowsPtrC device integer array of m+1 elements that contains the start of every row and the end of the last row plus one"
  },
  {
    "id": 28854,
    "content": "Given a sparse matrix A and a non-negative value threshold , the function returns a sparse matrix C , defined by \\(\\begin{matrix} {{C(i,j)} = {A(i,j)}} & \\text{if\\ |A(i,j)|\\ >\\ threshold} \\\\ \\end{matrix}\\) The implementation adopts a two-step approach to do the conversion"
  },
  {
    "id": 28855,
    "content": "First, the user allocates csrRowPtrC of m+1 elements and uses function pruneCsr2csrNnz() to determine the number of nonzeros columns per row"
  },
  {
    "id": 28856,
    "content": "The user must obtain the size of the buffer required by pruneCsr2csr() by calling pruneCsr2csr_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneCsr2csr()"
  },
  {
    "id": 28857,
    "content": "The routine cusparsepruneCsr2csrNnz() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsepruneCsr2csr() has the following properties:"
  },
  {
    "id": 28858,
    "content": "The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context"
  },
  {
    "id": 28859,
    "content": "csrRowsPtrA device integer array of m+1 elements that contains the start of every row and the end of the last row plus one"
  },
  {
    "id": 28860,
    "content": "Given a dense matrix A and a non-negative value percentage , the function computes sparse matrix C by the following three steps: Step 1: sort absolute value of A in ascending order \\(\\begin{matrix} {key\\ :=\\ sort(\\ |A|\\ )} \\\\ \\end{matrix}\\) Step 2: choose threshold by the parameter percentage \\(\\begin{matrix} {pos\\ =\\ ceil(m*n*(percentage/100))\\ -\\ 1} \\\\ {pos\\ =\\ min(pos,\\ m*n-1)} \\\\ {pos\\ =\\"
  },
  {
    "id": 28861,
    "content": "max(pos,\\ 0)} \\\\ {threshold\\ =\\ key\\lbrack pos brack} \\\\ \\end{matrix}\\) Step 3: call pruneDense2csr() by with the parameter threshold"
  },
  {
    "id": 28862,
    "content": "First, the user allocates csrRowPtrC of m+1 elements and uses function pruneDense2csrNnzByPercentage() to determine the number of nonzeros columns per row"
  },
  {
    "id": 28863,
    "content": "The user must obtain the size of the buffer required by pruneDense2csrByPercentage() by calling pruneDense2csrByPercentage_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneDense2csrByPercentage()"
  },
  {
    "id": 28864,
    "content": "This is different from pruneCsr2csrByPercentage() Examples of prune chapter provides a simple example of pruneDense2csrNnzByPercentage()"
  },
  {
    "id": 28865,
    "content": "The routine cusparsepruneDense2csrNnzByPercentage() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsepruneDense2csrByPercentage() has"
  },
  {
    "id": 28866,
    "content": "the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context"
  },
  {
    "id": 28867,
    "content": "Given a sparse matrix A and a non-negative value percentage , the function computes sparse matrix C by the following three steps: Step 1: sort absolute value of A in ascending order \\(\\begin{matrix} {key\\ :=\\ sort(\\ |csrValA|\\ )} \\\\ \\end{matrix}\\) Step 2: choose threshold by the parameter percentage \\(\\begin{matrix} {pos\\ =\\ ceil(nnzA*(percentage/100))\\ -\\ 1} \\\\ {pos\\ =\\ min(pos,\\ nnzA-1)} \\\\"
  },
  {
    "id": 28868,
    "content": "{pos\\ =\\ max(pos,\\ 0)} \\\\ {threshold\\ =\\ key\\lbrack pos brack} \\\\ \\end{matrix}\\) Step 3: call pruneCsr2csr() by with the parameter threshold"
  },
  {
    "id": 28869,
    "content": "First, the user allocates csrRowPtrC of m+1 elements and uses function pruneCsr2csrNnzByPercentage() to determine the number of nonzeros columns per row"
  },
  {
    "id": 28870,
    "content": "The user must obtain the size of the buffer required by pruneCsr2csrByPercentage() by calling pruneCsr2csrByPercentage_bufferSizeExt() , allocate the buffer, and pass the buffer pointer to pruneCsr2csrByPercentage()"
  },
  {
    "id": 28871,
    "content": "The routine cusparsepruneCsr2csrNnzByPercentage() has the following properties: This function requires temporary extra storage that is allocated internally The routine supports asynchronous execution if the Stream Ordered Memory Allocator is available The routine supports CUDA graph capture if the Stream Ordered Memory Allocator is available The routine cusparsepruneCsr2csrByPercentage() has the"
  },
  {
    "id": 28872,
    "content": "following properties: The routine requires no extra storage The routine supports asynchronous execution The routine supports CUDA graph capture Input parameter device or host description handle host handle to the cuSPARSE library context"
  },
  {
    "id": 28873,
    "content": "Given a sparse matrix A and a non-negative value threshold, the function returns nnzPerRow(the number of nonzeros columns per row) and nnzC(the total number of nonzeros) of a sparse matrix C, defined by \\(\\begin{matrix} {{C(i,j)} = {A(i,j)}} & \\text{if\\ |A(i,j)|\\ >\\ threshold} \\\\ \\end{matrix}\\) A key assumption for the cuComplex and cuDoubleComplex case is that this tolerance is given as the real"
  },
  {
    "id": 28877,
    "content": "Output nnzPerRow this array contains the number of elements whose absolute values are greater than tol per row nnzC host/device pointer of the total number of elements whose absolute values are greater than tol"
  },
  {
    "id": 28879,
    "content": "cuSPARSE Generic APIs  The cuSPARSE Generic APIs allow computing the most common sparse linear algebra operations, such as sparse matrix-vector (SpMV) and sparse matrix-matrix multiplication (SpMM), in a flexible way"
  },
  {
    "id": 28880,
    "content": "The new APIs have the following capabilities and features: Set matrix data layouts, number of batches, and storage formats (for example, CSR, COO, and so on)"
  },
  {
    "id": 28885,
    "content": "Value Meaning CUSPARSE_FORMAT_COO The matrix is stored in Coordinate (COO) format organized in Structure of Arrays (SoA) layout CUSPARSE_FORMAT_CSR The matrix is stored in Compressed Sparse Row (CSR) format CUSPARSE_FORMAT_CSC The matrix is stored in Compressed Sparse Column (CSC) format CUSPARSE_FORMAT_BLOCKED_ELL The matrix is stored in Blocked-Ellpack (Blocked-ELL) format"
  },
  {
    "id": 28886,
    "content": "CUSPARSE_FORMAT_SLICED_ELL The matrix is stored in Sliced-Ellpack (Sliced-ELL) format CUSPARSE_FORMAT_BSR The matrix is stored in Block Sparse Row (BSR) format 6"
  },
  {
    "id": 28889,
    "content": "Value Meaning CUSPARSE_ORDER_ROW The matrix is stored in row-major CUSPARSE_ORDER_COL The matrix is stored in column-major 6"
  },
  {
    "id": 28893,
    "content": "Value Meaning CUSPARSE_INDEX_32I 32-bit signed integer [1, 2^31 - 1] CUSPARSE_INDEX_64I 64-bit signed integer [1, 2^63 - 1] 6"
  },
  {
    "id": 28895,
    "content": "Dense Vector APIs  The cuSPARSE helper functions for dense vector descriptor are described in this section See the Dense Vector Format section for the detailed description of the storage format"
  },
  {
    "id": 28899,
    "content": "cusparseCreateDnVec()  cusparseStatus_t cusparseCreateDnVec ( cusparseDnVecDescr_t * dnVecDescr , int64_t size , void * values , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstDnVec ( cusparseConstDnVecDescr_t * dnVecDescr , int64_t size , const void * values , cudaDataType valueType ) This function initializes the dense vector descriptor dnVecDescr Memory In/out Meaning dnVecDescr"
  },
  {
    "id": 28900,
    "content": "HOST OUT Dense vector descriptor size HOST IN Size of the dense vector values DEVICE IN Values of the dense vector Array with size elements valueType HOST IN Enumerator specifying the datatype of values cusparseCreateDnVec() has the following constraints: values must be aligned to the size of the datatype specified by valueType"
  },
  {
    "id": 28905,
    "content": "cusparseDestroyDnVec()  cusparseStatus_t cusparseDestroyDnVec ( cusparseConstDnVecDescr_t dnVecDescr )   non-const descriptor supported This function releases the host memory allocated for the dense vector descriptor dnVecDescr Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28909,
    "content": "cusparseDnVecGet()  cusparseStatus_t cusparseDnVecGet ( cusparseDnVecDescr_t dnVecDescr , int64_t * size , void ** values , cudaDataType * valueType ) cusparseStatus_t cusparseConstDnVecGet ( cusparseConstDnVecDescr_t dnVecDescr , int64_t * size , const void ** values , cudaDataType * valueType ) This function returns the fields of the dense vector descriptor dnVecDescr Memory In/out Meaning"
  },
  {
    "id": 28910,
    "content": "dnVecDescr HOST IN Dense vector descriptor size HOST OUT Size of the dense vector values DEVICE OUT Values of the dense vector Array with nnz elements valueType HOST OUT Enumerator specifying the datatype of values See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28914,
    "content": "cusparseDnVecGetValues()  cusparseStatus_t cusparseDnVecGetValues ( cusparseDnVecDescr_t dnVecDescr , void ** values ) cusparseStatus_t cusparseConstDnVecGetValues ( cusparseConstDnVecDescr_t dnVecDescr , const void ** values ) This function returns the values field of the dense vector descriptor dnVecDescr Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor values DEVICE OUT Values"
  },
  {
    "id": 28919,
    "content": "cusparseDnVecSetValues()  cusparseStatus_t cusparseDnVecSetValues ( cusparseDnVecDescr_t dnVecDescr , void * values ) This function set the values field of the dense vector descriptor dnVecDescr Memory In/out Meaning dnVecDescr HOST IN Dense vector descriptor values DEVICE IN Values of the dense vector Array with size elements cusparseDnVecSetValues() has the following constraints: values must"
  },
  {
    "id": 28923,
    "content": "Sparse Vector APIs  The cuSPARSE helper functions for sparse vector descriptor are described in this section See the Sparse Vector Format section for the detailed description of the storage format"
  },
  {
    "id": 28927,
    "content": "cusparseCreateSpVec()  cusparseStatus_t cusparseCreateSpVec ( cusparseSpVecDescr_t * spVecDescr , int64_t size , int64_t nnz , void * indices , void * values , cusparseIndexType_t idxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstSpVec ( cusparseConstSpVecDescr_t * spVecDescr , int64_t size , int64_t nnz , const void * indices , const void *"
  },
  {
    "id": 28928,
    "content": "values , cusparseIndexType_t idxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spVecDescr Memory In/out Meaning spVecDescr HOST OUT Sparse vector descriptor size HOST IN Size of the sparse vector nnz HOST IN Number of non-zero entries of the sparse vector indices DEVICE IN Indices of the sparse vector Array with nnz elements"
  },
  {
    "id": 28929,
    "content": "idxType HOST IN Enumerator specifying the data type of indices idxBase HOST IN Enumerator specifying the the index base of indices valueType HOST IN Enumerator specifying the datatype of values cusparseCreateSpVec() has the following constraints: indices and values must be aligned to the size of the datatypes specified by idxType and valueType , respectively"
  },
  {
    "id": 28933,
    "content": "cusparseDestroySpVec()  cusparseStatus_t cusparseDestroySpVec ( cusparseConstSpVecDescr_t spVecDescr )   non-const descriptor supported This function releases the host memory allocated for the sparse vector descriptor spVecDescr Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28937,
    "content": "cusparseSpVecGet()  cusparseStatus_t cusparseSpVecGet ( cusparseSpVecDescr_t spVecDescr , int64_t * size , int64_t * nnz , void ** indices , void ** values , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstSpVecGet ( cusparseConstSpVecDescr_t spVecDescr , int64_t * size , int64_t * nnz , const void ** indices , const void"
  },
  {
    "id": 28938,
    "content": "** values , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse vector descriptor spVecDescr Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor size HOST OUT Size of the sparse vector nnz HOST OUT Number of non-zero entries of the sparse vector indices DEVICE OUT Indices of the sparse vector Array"
  },
  {
    "id": 28939,
    "content": "with nnz elements idxType HOST OUT Enumerator specifying the data type of indices idxBase HOST OUT Enumerator specifying the the index base of indices valueType HOST OUT Enumerator specifying the datatype of values See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28943,
    "content": "cusparseSpVecGetIndexBase()  cusparseStatus_t cusparseSpVecGetIndexBase ( cusparseConstSpVecDescr_t spVecDescr , non-const descriptor supported cusparseIndexBase_t * idxBase ) This function returns the idxBase field of the sparse vector descriptor spVecDescr Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor idxBase HOST OUT Enumerator specifying the the index base of indices See"
  },
  {
    "id": 28948,
    "content": "cusparseSpVecGetValues()  cusparseStatus_t cusparseSpVecGetValues ( cusparseSpVecDescr_t spVecDescr , void ** values ) cusparseStatus_t cusparseConstSpVecGetValues ( cusparseConstSpVecDescr_t spVecDescr , const void ** values ) This function returns the values field of the sparse vector descriptor spVecDescr Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor values DEVICE OUT"
  },
  {
    "id": 28954,
    "content": "cusparseSpVecSetValues()  cusparseStatus_t cusparseSpVecSetValues ( cusparseSpVecDescr_t spVecDescr , void * values ) This function set the values field of the sparse vector descriptor spVecDescr Memory In/out Meaning spVecDescr HOST IN Sparse vector descriptor values DEVICE IN Values of the sparse vector Array with nnz elements cusparseDnVecSetValues() has the following constraints: values must"
  },
  {
    "id": 28958,
    "content": "Dense Matrix APIs  The cuSPARSE helper functions for dense matrix descriptor are described in this section See the Dense Matrix Format section for the detailed description of the storage format"
  },
  {
    "id": 28962,
    "content": "cusparseCreateDnMat()  cusparseStatus_t cusparseCreateDnMat ( cusparseDnMatDescr_t * dnMatDescr , int64_t rows , int64_t cols , int64_t ld , void * values , cudaDataType valueType , cusparseOrder_t order ) cusparseStatus_t cusparseCreateConstDnMat ( cusparseConstDnMatDescr_t * dnMatDescr , int64_t rows , int64_t cols , int64_t ld , const void * values , cudaDataType valueType , cusparseOrder_t"
  },
  {
    "id": 28963,
    "content": "order ) The function initializes the dense matrix descriptor dnMatDescr Memory In/out Meaning dnMatDescr HOST OUT Dense matrix descriptor rows HOST IN Number of rows of the dense matrix cols HOST IN Number of columns of the dense matrix ld HOST IN Leading dimension of the dense matrix values DEVICE IN Values of the dense matrix Array with size elements valueType HOST IN Enumerator specifying the"
  },
  {
    "id": 28964,
    "content": "datatype of values order HOST IN Enumerator specifying the memory layout of the dense matrix cusparseCreateDnMat() has the following constraints: values must be aligned to the size of the datatype specified by valueType"
  },
  {
    "id": 28968,
    "content": "cusparseDestroyDnMat()  cusparseStatus_t cusparseDestroyDnMat ( cusparseConstDnMatDescr_t dnMatDescr )   non-const descriptor supported This function releases the host memory allocated for the dense matrix descriptor dnMatDescr Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28972,
    "content": "cusparseDnMatGet()  cusparseStatus_t cusparseDnMatGet ( cusparseDnMatDescr_t dnMatDescr , int64_t * rows , int64_t * cols , int64_t * ld , void ** values , cudaDataType * type , cusparseOrder_t * order ) cusparseStatus_t cusparseConstDnMatGet ( cusparseConstDnMatDescr_t dnMatDescr , int64_t * rows , int64_t * cols , int64_t * ld , const void ** values , cudaDataType * type , cusparseOrder_t *"
  },
  {
    "id": 28973,
    "content": "order ) This function returns the fields of the dense matrix descriptor dnMatDescr Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor rows HOST OUT Number of rows of the dense matrix cols HOST OUT Number of columns of the dense matrix ld HOST OUT Leading dimension of the dense matrix values DEVICE OUT Values of the dense matrix Array with ld * cols elements valueType HOST OUT"
  },
  {
    "id": 28974,
    "content": "Enumerator specifying the datatype of values order HOST OUT Enumerator specifying the memory layout of the dense matrix See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28978,
    "content": "cusparseDnMatGetValues()  cusparseStatus_t cusparseDnMatGetValues ( cusparseDnMatDescr_t dnMatDescr , void ** values ) cusparseStatus_t cusparseConstDnMatGetValues ( cusparseConstDnMatDescr_t dnMatDescr , const void ** values ) This function returns the values field of the dense matrix descriptor dnMatDescr Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor values DEVICE OUT Values"
  },
  {
    "id": 28984,
    "content": "cusparseDnMatSetValues()  cusparseStatus_t cusparseDnMatSetValues ( cusparseDnMatDescr_t dnMatDescr , void * values ) This function sets the values field of the dense matrix descriptor dnMatDescr Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor values DEVICE IN Values of the dense matrix Array with ld * cols elements cusparseDnMatSetValues() has the following constraints: values"
  },
  {
    "id": 28989,
    "content": "cusparseDnMatGetStridedBatch()  cusparseStatus_t cusparseDnMatGetStridedBatch ( cusparseConstDnMatDescr_t dnMatDescr , non-const descriptor supported int * batchCount , int64_t * batchStride ) The function returns the number of batches and the batch stride of the dense matrix descriptor dnMatDescr Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor batchCount HOST OUT Number of"
  },
  {
    "id": 28990,
    "content": "batches of the dense matrix batchStride HOST OUT Address offset between a matrix and the next one in the batch See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28994,
    "content": "cusparseDnMatSetStridedBatch()  cusparseStatus_t cusparseDnMatSetStridedBatch ( cusparseDnMatDescr_t dnMatDescr , int batchCount , int64_t batchStride ) The function sets the number of batches and the batch stride of the dense matrix descriptor dnMatDescr Memory In/out Meaning dnMatDescr HOST IN Dense matrix descriptor batchCount HOST IN Number of batches of the dense matrix batchStride HOST IN"
  },
  {
    "id": 28995,
    "content": "Address offset between a matrix and the next one in the batch batchStride ≥ ld * cols if the matrix uses column-major layout, batchStride ≥ ld * rows otherwise See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 28998,
    "content": "Sparse Matrix APIs  The cuSPARSE helper functions for sparse matrix descriptor are described in this section"
  },
  {
    "id": 28999,
    "content": "See the COO , CSR , CSC , SELL , BSR , Blocked-Ell sections for the detailed description of the storage formats"
  },
  {
    "id": 29007,
    "content": "cusparseCreateCoo()  cusparseStatus_t cusparseCreateCoo ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * cooRowInd , void * cooColInd , void * cooValues , cusparseIndexType_t cooIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCoo ( cusparseConstSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols ,"
  },
  {
    "id": 29008,
    "content": "int64_t nnz , const void * cooRowInd , const void * cooColInd , const void * cooValues , cusparseIndexType_t cooIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the COO format (Structure of Arrays layout) Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse"
  },
  {
    "id": 29009,
    "content": "matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix cooRowInd DEVICE IN Row indices of the sparse matrix Array with nnz elements cooIdxType HOST IN Data type of cooRowInd and cooColInd idxBase HOST IN Index base of cooRowInd and cooColInd valueType HOST IN Datatype of cooValues cusparseCreateCoo() has the following constraints:"
  },
  {
    "id": 29010,
    "content": "cooRowInd , cooColInd , and cooValues must be aligned to the size of the datatypes specified by cooIdxType , cooIdxType , and valueType"
  },
  {
    "id": 29016,
    "content": "cusparseCooGet()  cusparseStatus_t cusparseCooGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * nnz , void ** cooRowInd , void ** cooColInd , void ** cooValues , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstCooGet ( cusparseConstSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols ,"
  },
  {
    "id": 29017,
    "content": "int64_t * nnz , const void ** cooRowInd , const void ** cooColInd , const void ** cooValues , cusparseIndexType_t * idxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in COO format (Array of Structures layout) Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of"
  },
  {
    "id": 29018,
    "content": "rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix cooRowInd DEVICE OUT Row indices of the sparse matrix Array nnz elements cooIdxType HOST OUT Data type of cooRowInd and cooColInd idxBase HOST OUT Index base of cooRowInd and cooColInd valueType HOST OUT Datatype of cooValues See cusparseStatus_t for the"
  },
  {
    "id": 29024,
    "content": "cusparseCooSetPointers()  cusparseStatus_t cusparseCooSetPointers ( cusparseSpMatDescr_t spMatDescr , void * cooRows , void * cooColumns , void * cooValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor cooRows DEVICE IN Row indices of the sparse matrix Array with nnz elements cusparseCooSetPointers()"
  },
  {
    "id": 29025,
    "content": "has the following constraints: cooRows , cooColumns , and cooValues must be aligned to the size of their corresponding datatypes specified in spMatDescr"
  },
  {
    "id": 29030,
    "content": "cusparseCooSetStridedBatch()  cusparseStatus_t cusparseCooSetStridedBatch ( cusparseSpMatDescr_t spMatDescr , int batchCount , int64_t batchStride ) This function sets the batchCount and the batchStride fields of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix batchStride HOST IN"
  },
  {
    "id": 29031,
    "content": "address offset between consecutive batches See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 29039,
    "content": "cusparseCreateCsr()  cusparseStatus_t cusparseCreateCsr ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * csrRowOffsets , void * csrColInd , void * csrValues , cusparseIndexType_t csrRowOffsetsType , cusparseIndexType_t csrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCsr ( cusparseConstSpMatDescr_t"
  },
  {
    "id": 29040,
    "content": "* spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * csrRowOffsets , const void * csrColInd , const void * csrValues , cusparseIndexType_t csrRowOffsetsType , cusparseIndexType_t csrColIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the CSR format Memory In/out Meaning spMatDescr HOST OUT"
  },
  {
    "id": 29041,
    "content": "Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix csrRowOffsets DEVICE IN Row offsets of the sparse matrix Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the"
  },
  {
    "id": 29042,
    "content": "sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix csrRowOffsets DEVICE OUT Row offsets of the sparse matrix Array with nnz elements csrRowOffsetsType HOST OUT Data type of csrRowOffsets csrColIndType HOST OUT Data type of csrColInd idxBase HOST OUT Index base of csrRowOffsets and csrColInd valueType HOST OUT Datatype of csrValues See cusparseStatus_t for the description of"
  },
  {
    "id": 29048,
    "content": "cusparseCsrSetPointers()  cusparseStatus_t cusparseCsrSetPointers ( cusparseSpMatDescr_t spMatDescr , void * csrRowOffsets , void * csrColInd , void * csrValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor csrRowOffsets DEVICE IN Row offsets of the sparse matrix Array with nnz elements"
  },
  {
    "id": 29049,
    "content": "cusparseCsrSetPointers() has the following constraints: csrRowOffsets , csrColInd , and csrValues must be aligned to the size of their corresponding datatypes specified in spMatDescr"
  },
  {
    "id": 29054,
    "content": "cusparseCsrSetStridedBatch()  cusparseStatus_t cusparseCsrSetStridedBatch ( cusparseSpMatDescr_t spMatDescr , int batchCount , int64_t offsetsBatchStride , int64_t columnsValuesBatchStride ) This function sets the batchCount and the batchStride fields of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches"
  },
  {
    "id": 29055,
    "content": "of the sparse matrix offsetsBatchStride HOST IN Address offset between consecutive batches for the row offset array columnsValuesBatchStride HOST IN Address offset between consecutive batches for the column and value arrays See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 29063,
    "content": "cusparseCreateCsc()  cusparseStatus_t cusparseCreateCsc ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t nnz , void * cscColOffsets , void * cscRowInd , void * cscValues , cusparseIndexType_t cscColOffsetsType , cusparseIndexType_t cscRowIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstCsc ( cusparseConstSpMatDescr_t"
  },
  {
    "id": 29064,
    "content": "* spMatDescr , int64_t rows , int64_t cols , int64_t nnz , const void * cscColOffsets , const void * cscRowInd , const void * cscValues , cusparseIndexType_t cscColOffsetsType , cusparseIndexType_t cscRowIndType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr in the CSC format Memory In/out Meaning spMatDescr HOST OUT"
  },
  {
    "id": 29065,
    "content": "Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of non-zero entries of the sparse matrix cscColOffsets DEVICE IN Column offsets of the sparse matrix Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the"
  },
  {
    "id": 29066,
    "content": "sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix cscColOffsets DEVICE OUT Col offsets of the sparse matrix Array with nnz elements cscColOffsetsType HOST OUT Data type of cscColOffsets cscRowIndType HOST OUT Data type of cscRowInd idxBase HOST OUT Index base of cscColOffsets and cscRowInd valueType HOST OUT Datatype of cscValues See cusparseStatus_t for the description of"
  },
  {
    "id": 29072,
    "content": "cusparseCscSetPointers()  cusparseStatus_t cusparseCscSetPointers ( cusparseSpMatDescr_t spMatDescr , void * cscColOffsets , void * cscRowInd , void * cscValues ) This function sets the pointers of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor cscColOffsets DEVICE IN Col offsets of the sparse matrix Array with nnz elements"
  },
  {
    "id": 29073,
    "content": "cusparseCscSetPointers() has the following constraints: cscColOffsets , cscRowInd , and cscValues must be aligned to the size of their corresponding datatypes specified in spMatDescr"
  },
  {
    "id": 29081,
    "content": "cusparseCreateBlockedEll()  cusparseStatus_t cusparseCreateBlockedEll ( cusparseSpMatDescr_t * spMatDescr , int64_t rows , int64_t cols , int64_t ellBlockSize , int64_t ellCols , void * ellColInd , void * ellValue , cusparseIndexType_t ellIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) cusparseStatus_t cusparseCreateConstBlockedEll ( cusparseConstSpMatDescr_t * spMatDescr ,"
  },
  {
    "id": 29082,
    "content": "int64_t rows , int64_t cols , int64_t ellBlockSize , int64_t ellCols , const void * ellColInd , const void * ellValue , cusparseIndexType_t ellIdxType , cusparseIndexBase_t idxBase , cudaDataType valueType ) This function initializes the sparse matrix descriptor spMatDescr for the Blocked-Ellpack (ELL) format Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of"
  },
  {
    "id": 29083,
    "content": "rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix ellBlockSize HOST IN Size of the ELL-Block ellCols HOST IN Actual number of columns of the Blocked-Ellpack format ( ellValue columns) ellColInd DEVICE IN Blocked-ELL Column indices Array with [ellCols / ellBlockSize][rows / ellBlockSize] elements ellValue DEVICE IN Values of the sparse matrix Array with rows * ellCols"
  },
  {
    "id": 29084,
    "content": "elements ellIdxType HOST IN Data type of ellColInd idxBase HOST IN Index base of ellColInd valueType HOST IN Data type of ellValue Blocked-ELL Column indices ( ellColInd ) are in the range [0, cols / ellBlockSize -1]"
  },
  {
    "id": 29090,
    "content": "cusparseBlockedEllGet()  cusparseStatus_t cusparseBlockedEllGet ( cusparseSpMatDescr_t spMatDescr , int64_t * rows , int64_t * cols , int64_t * ellBlockSize , int64_t * ellCols , void ** ellColInd , void ** ellValue , cusparseIndexType_t * ellIdxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) cusparseStatus_t cusparseConstBlockedEllGet ( cusparseConstSpMatDescr_t spMatDescr ,"
  },
  {
    "id": 29091,
    "content": "int64_t * rows , int64_t * cols , int64_t * ellBlockSize , int64_t * ellCols , const void ** ellColInd , const void ** ellValue , cusparseIndexType_t * ellIdxType , cusparseIndexBase_t * idxBase , cudaDataType * valueType ) This function returns the fields of the sparse matrix descriptor spMatDescr stored in Blocked-Ellpack (ELL) format Memory In/out Meaning spMatDescr HOST IN Sparse matrix"
  },
  {
    "id": 29092,
    "content": "descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of the sparse matrix ellBlockSize HOST OUT Size of the ELL-Block ellCols HOST OUT Actual number of columns of the Blocked-Ellpack format ellColInd DEVICE OUT Column indices for the ELL-Block Array with [cols / ellBlockSize][rows / ellBlockSize] elements ellValue DEVICE OUT Values of the sparse matrix Array"
  },
  {
    "id": 29093,
    "content": "with rows * ellCols elements ellIdxType HOST OUT Data type of ellColInd idxBase HOST OUT Index base of ellColInd valueType HOST OUT Datatype of ellValue See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 29101,
    "content": "Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor rows HOST IN Number of rows of the sparse matrix cols HOST IN Number of columns of the sparse matrix nnz HOST IN Number of nonzero elements in the sparse matrix sellValuesSize HOST IN Total number of elements in sellValues array (nonzero and padding) sliceSize HOST IN The number of rows per slice sellSliceOffsets DEVICE IN Slice"
  },
  {
    "id": 29102,
    "content": "offsets of the sparse matrix Array of size \\(\\left \\lceil{\\frac{rows}{sliceSize}} ight ceil + 1\\) sellColInd DEVICE IN Column indexes of the sparse matrix Array of size sellValuesSize elements sellSliceOffsetsType HOST IN Data type of sellSliceOffsets sellColIndType HOST IN Data type of sellColInd idxBase HOST IN Index base of sellColInd valueType HOST IN Data type of sellValues Note Sliced"
  },
  {
    "id": 29103,
    "content": "Ellpack Column array sellColInd contains -1 values for indicating padded entries cusparseCreateSlicedEll() has the following constraints: sellSliceOffsets , sellColInd , and sellValues must be aligned to the size of the datatypes specified by sellSliceOffsetsType , sellColIndType , and valueType , respectively"
  },
  {
    "id": 29111,
    "content": "Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor brows HOST IN Number of block rows of the sparse matrix bcols HOST IN Number of block columns of the sparse matrix bnnz HOST IN Number of blocks of the sparse matrix rowBlockSize HOST IN Number of rows of each block colBlockSize HOST IN Number of columns of each block bsrRowOffsets DEVICE IN Block row offsets of the sparse matrix"
  },
  {
    "id": 29112,
    "content": "Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST IN Number of batches of the sparse matrix offsetsBatchStride HOST IN Address offset between consecutive batches for the row offset array columnsBatchStride HOST IN Address offset between consecutive batches for the column array valuesBatchStride HOST IN Address offset between consecutive batches for the values array"
  },
  {
    "id": 29121,
    "content": "cusparseDestroySpMat()  cusparseStatus_t cusparseDestroySpMat ( cusparseConstSpMatDescr_t spMatDescr )   non-const descriptor supported This function releases the host memory allocated for the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 29126,
    "content": "cusparseSpMatGetSize()  cusparseStatus_t cusparseSpMatGetSize ( cusparseConstSpMatDescr_t spMatDescr , non-const descriptor supported int64_t * rows , int64_t * cols , int64_t * nnz ) This function returns the sizes of the sparse matrix spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor rows HOST OUT Number of rows of the sparse matrix cols HOST OUT Number of columns of"
  },
  {
    "id": 29127,
    "content": "the sparse matrix nnz HOST OUT Number of non-zero entries of the sparse matrix See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 29132,
    "content": "cusparseSpMatGetFormat()  cusparseStatus_t cusparseSpMatGetFormat ( cusparseConstSpMatDescr_t spMatDescr , non-const descriptor supported cusparseFormat_t * format ) This function returns the format field of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor format HOST OUT Storage format of the sparse matrix See cusparseStatus_t for the"
  },
  {
    "id": 29138,
    "content": "cusparseSpMatGetIndexBase()  cusparseStatus_t cusparseSpMatGetIndexBase ( cusparseConstSpMatDescr_t spMatDescr , non-const descriptor supported cusparseIndexBase_t * idxBase ) This function returns the idxBase field of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor idxBase HOST OUT Index base of the sparse matrix See cusparseStatus_t for"
  },
  {
    "id": 29144,
    "content": "cusparseSpMatGetValues()  cusparseStatus_t cusparseSpMatGetValues ( cusparseSpMatDescr_t spMatDescr , void ** values ) cusparseStatus_t cusparseConstSpMatGetValues ( cusparseConstSpMatDescr_t spMatDescr , const void ** values ) This function returns the values field of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor values DEVICE OUT"
  },
  {
    "id": 29150,
    "content": "cusparseSpMatSetValues()  cusparseStatus_t cusparseSpMatSetValues ( cusparseSpMatDescr_t spMatDescr , void * values ) This function sets the values field of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor values DEVICE IN Values of the sparse matrix Array with nnz elements cusparseSpMatSetValues() has the following constraints: values"
  },
  {
    "id": 29156,
    "content": "cusparseSpMatGetStridedBatch()  cusparseStatus_t cusparseSpMatGetStridedBatch ( cusparseConstSpMatDescr_t spMatDescr , non-const descriptor supported int * batchCount ) This function returns the batchCount field of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor batchCount HOST OUT Number of batches of the sparse matrix See"
  },
  {
    "id": 29162,
    "content": "cusparseSpMatGetAttribute()  cusparseStatus_t cusparseSpMatGetAttribute ( cusparseConstSpMatDescr_t spMatDescr , non-const descriptor supported cusparseSpMatAttribute_t attribute , void * data , size_t dataSize ) The function gets the attributes of the sparse matrix descriptor spMatDescr Memory In/out Meaning spMatDescr HOST IN Sparse matrix descriptor attribute HOST IN Attribute enumerator data"
  },
  {
    "id": 29163,
    "content": "HOST OUT Attribute value dataSize HOST IN Size of the attribute in bytes for safety Attribute Meaning Possible Values CUSPARSE_SPMAT_FILL_MODE Indicates if the lower or upper part of a matrix is stored in sparse storage CUSPARSE_FILL_MODE_LOWER CUSPARSE_FILL_MODE_UPPER CUSPARSE_SPMAT_DIAG_TYPE Indicates if the matrix diagonal entries are unity CUSPARSE_DIAG_TYPE_NON_UNIT CUSPARSE_DIAG_TYPE_UNIT"
  },
  {
    "id": 29169,
    "content": "cusparseSpMatSetAttribute()  cusparseStatus_t cusparseSpMatSetAttribute ( cusparseSpMatDescr_t spMatDescr , cusparseSpMatAttribute_t attribute , const void * data , size_t dataSize ) The function sets the attributes of the sparse matrix descriptor spMatDescr Param Memory In/out Meaning spMatDescr HOST OUT Sparse matrix descriptor attribute HOST IN Attribute enumerator data HOST IN Attribute"
  },
  {
    "id": 29170,
    "content": "value dataSize HOST IN Size of the attribute in bytes for safety Attribute Meaning Possible Values CUSPARSE_SPMAT_FILL_MODE Indicates if the lower or upper part of a matrix is stored in sparse storage CUSPARSE_FILL_MODE_LOWER CUSPARSE_FILL_MODE_UPPER CUSPARSE_SPMAT_DIAG_TYPE Indicates if the matrix diagonal entries are unity CUSPARSE_DIAG_TYPE_NON_UNIT CUSPARSE_DIAG_TYPE_UNIT See cusparseStatus_t"
  },
  {
    "id": 29177,
    "content": "cusparseAxpby()  cusparseStatus_t cusparseAxpby ( cusparseHandle_t handle , const void * alpha , cusparseConstSpVecDescr_t vecX ,   non-const descriptor supported const void * beta , cusparseDnVecDescr_t vecY ) The function computes the sum of a sparse vector vecX and a dense vector vecY"
  },
  {
    "id": 29178,
    "content": "\\(\\mathbf{Y} = \\alpha\\mathbf{X} + \\beta\\mathbf{Y}\\) In other words, for i = 0 to n -1 Y [ i ] = beta * Y [ i ] for i = 0 to nnz -1 Y [ X_indices [ i ]] += alpha * X_values [ i ] Param"
  },
  {
    "id": 29183,
    "content": "cusparseGather()  cusparseStatus_t cusparseGather ( cusparseHandle_t handle , cusparseConstDnVecDescr_t vecY , non-const descriptor supported cusparseSpVecDescr_t vecX ) The function gathers the elements of the dense vector vecY into the sparse vector vecX In other words, for i = 0 to nnz -1 X_values [ i ] = Y [ X_indices [ i ]] Param Please visit cuSPARSE Library Samples - cusparseGather for a"
  },
  {
    "id": 29188,
    "content": "cusparseScatter()  cusparseStatus_t cusparseScatter ( cusparseHandle_t handle , cusparseConstSpVecDescr_t vecX , non-const descriptor supported cusparseDnVecDescr_t vecY ) The function scatters the elements of the sparse vector vecX into the dense vector vecY In other words, for i = 0 to nnz -1 Y [ X_indices [ i ]] = X_values [ i ] Param Please visit cuSPARSE Library Samples - cusparseScatter"
  },
  {
    "id": 29193,
    "content": "cusparseRot() [DEPRECATED]  > The routine will be removed in the next major release cusparseStatus_t cusparseRot ( cusparseHandle_t handle , const void * c_coeff , const void * s_coeff , cusparseSpVecDescr_t vecX , cusparseDnVecDescr_t vecY ) The function computes the Givens rotation matrix \\(G = \\begin{bmatrix} c & s \\\\ {- s} & c \\\\ \\end{bmatrix}\\) to a sparse vecX and a dense vector vecY In"
  },
  {
    "id": 29194,
    "content": "other words, for i = 0 to nnz -1 Y [ X_indices [ i ]] = c * Y [ X_indices [ i ]] - s * X_values [ i ] X_values [ i ] = c * X_values [ i ] + s * Y [ X_indices [ i ]] Param"
  },
  {
    "id": 29195,
    "content": "cusparseSpMV() supports the following algorithms: Algorithm Notes CUSPARSE_SPMV_ALG_DEFAULT Default algorithm for any sparse matrix format"
  },
  {
    "id": 29197,
    "content": "Performance notes: CUSPARSE_SPMV_COO_ALG1 and CUSPARSE_SPMV_CSR_ALG1 provide higher performance than CUSPARSE_SPMV_COO_ALG2 and CUSPARSE_SPMV_CSR_ALG2"
  },
  {
    "id": 29198,
    "content": "In general, opA == CUSPARSE_OPERATION_NON_TRANSPOSE is 3x faster than opA = CUSPARSE_OPERATION_NON_TRANSPOSE"
  },
  {
    "id": 29199,
    "content": "It is beneficial when we need to run cusparseSpMV() multiple times with a same matrix ( cusparseSpMV_preprocess() is executed only once)"
  },
  {
    "id": 29200,
    "content": "cusparseSpMV() has the following properties: The routine requires extra storage for CSR/CSC format (all algorithms) and for COO format with CUSPARSE_SPMV_COO_ALG2 algorithm Provides deterministic (bit-wise) results for each run only for CUSPARSE_SPMV_COO_ALG2 and CUSPARSE_SPMV_CSR_ALG2 algorithms, and opA == CUSPARSE_OPERATION_NON_TRANSPOSE cusparseSpMV() supports the following optimizations :"
  },
  {
    "id": 29201,
    "content": "CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the return status"
  },
  {
    "id": 29202,
    "content": "The function cusparseSpSV_analysis() performs the analysis phase, while cusparseSpSV_solve() executes the solve phase for a sparse triangular linear system"
  },
  {
    "id": 29203,
    "content": "The routine supports arbitrary sparsity for the input matrix, but only the upper or lower triangular part is taken into account in the computation"
  },
  {
    "id": 29204,
    "content": "NOTE: all parameters must be consistent across cusparseSpSV API calls and the matrix descriptions and externalBuffer must not be modified between cusparseSpSV_analysis() and cusparseSpSV_solve()"
  },
  {
    "id": 29205,
    "content": "The function cusparseSpSV_updateMatrix() can be used to update the values on the sparse matrix stored inside the opaque data structure spsvDescr Param"
  },
  {
    "id": 29206,
    "content": "Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A vecX HOST IN Dense vector X vecY HOST IN/OUT Dense vector Y computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize"
  },
  {
    "id": 29207,
    "content": "HOST OUT Number of bytes of workspace needed by cusparseSpSV_analysis() and cusparseSpSV_solve() externalBuffer DEVICE IN/OUT Pointer to a workspace buffer of at least bufferSize bytes"
  },
  {
    "id": 29208,
    "content": "This functions supports the following update strategies ( updatePart ): Strategy Notes CUSPARSE_SPSV_UPDATE_GENERAL Updates the sparse matrix values with values of newValues array CUSPARSE_SPSV_UPDATE_DIAGONAL Updates the diagonal part of the matrix with diagonal values stored in newValues array That is, newValues has the new diagonal values only See cusparseStatus_t for the description of the"
  },
  {
    "id": 29210,
    "content": "\\(\\mathbf{C} = \\alpha op\\left( \\mathbf{A} ight) \\cdot op\\left( \\mathbf{B} ight) + \\beta\\mathbf{C}\\) where op(A) is a sparse matrix of size \\(m \\times k\\) op(B) is a dense matrix of size \\(k \\times n\\) C is a dense matrix of size \\(m \\times n\\) \\(\\alpha\\) and \\(\\beta\\) are scalars The routine can be also used to perform the multiplication of a dense matrix and a sparse matrix by switching the"
  },
  {
    "id": 29212,
    "content": "The function cusparseSpMM_bufferSize() returns the size of the workspace needed by cusparseSpMM() The function cusparseSpMM_preprocess() can be called before cusparseSpMM to speedup the actual computation It is useful when cusparseSpMM is called multiple times with the same sparsity pattern ( matA )"
  },
  {
    "id": 29214,
    "content": "cusparseSpMM() has the following properties: The routine requires no extra storage for CUSPARSE_SPMM_COO_ALG1 , CUSPARSE_SPMM_COO_ALG3 , CUSPARSE_SPMM_COO_ALG4 , CUSPARSE_SPMM_BSR_ALG1 The routine supports asynchronous execution Provides deterministic (bit-wise) results for each run only for CUSPARSE_SPMM_COO_ALG2 , CUSPARSE_SPMM_CSR_ALG3 , and CUSPARSE_SPMM_BSR_ALG1 algorithms compute-sanitizer"
  },
  {
    "id": 29215,
    "content": "could report false race conditions for this routine This is for optimization purposes and does not affect the correctness of the computation The routine allows the indices of matA to be unsorted cusparseSpMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression Please visit cuSPARSE Library Samples - cusparseSpMM CSR and cusparseSpMM COO for a code example For"
  },
  {
    "id": 29217,
    "content": "The function has the following limitations: The pointer mode must be equal to CUSPARSE_POINTER_MODE_HOST Only opA == CUSPARSE_OPERATION_NON_TRANSPOSE is supported"
  },
  {
    "id": 29218,
    "content": "Experimental : The function performs the multiplication of a sparse matrix matA and a dense matrix matB with custom operators \\({C^{\\prime}}_{ij} = \\text{epilogue}\\left( {\\sum_{k}^{\\oplus}{op\\left( A_{ik} ight) \\otimes op\\left( B_{kj} ight),C_{ij}}} ight)\\) where op(A) is a sparse matrix of size \\(m \\times k\\) op(B) is a dense matrix of size \\(k \\times n\\) C is a dense matrix of size \\(m \\times"
  },
  {
    "id": 29219,
    "content": "n\\) \\(\\oplus\\) , \\(\\otimes\\) , and \\(\\text{epilogue}\\) are custom add , mul , and epilogue operators respectively Also, for matrix A and B \\(\\text{op}(A) = \\begin{cases} A & \\text{if op(A) == CUSPARSE_OPERATION_NON_TRANSPOSE} \\\\ A^{T} & \\text{if op(A) == CUSPARSE_OPERATION_TRANSPOSE} \\\\ \\end{cases}\\) \\(\\text{op}(B) = \\begin{cases} B & {\\text{if op(}B\\text{) == CUSPARSE_OPERATION_NON_TRANSPOSE}} \\\\"
  },
  {
    "id": 29220,
    "content": "B^{T} & {\\text{if op(}B\\text{) == CUSPARSE_OPERATION_TRANSPOSE}} \\\\ \\end{cases}\\) Only opA == CUSPARSE_OPERATION_NON_TRANSPOSE is currently supported The function cusparseSpMMOp_createPlan() returns the size of the workspace and the compiled kernel needed by cusparseSpMMOp() Param The function cusparseSpSM_analysis() performs the analysis phase, while cusparseSpSM_solve() executes the solve phase"
  },
  {
    "id": 29221,
    "content": "for a sparse triangular linear system cusparseSpSM_bufferSize() requires a buffer size for the analysis phase which is proportional to number of non-zero entries of the sparse matrix The externalBuffer is stored into spsmDescr and used by cusparseSpSM_solve() For this reason, the device memory buffer must be deallocated only after cusparseSpSM_solve() NOTE: all parameters must be consistent across"
  },
  {
    "id": 29222,
    "content": "cusparseSpSM API calls and the matrix descriptions and externalBuffer must not be modified between cusparseSpSM_analysis() and cusparseSpSM_solve() Param Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication of type computeType matA HOST IN Sparse matrix A"
  },
  {
    "id": 29223,
    "content": "matB HOST IN Dense matrix B matC HOST IN/OUT Dense matrix C computeType HOST IN Datatype in which the computation is executed alg HOST IN Algorithm for the computation bufferSize HOST OUT Number of bytes of workspace needed by cusparseSpSM_analysis() and cusparseSpSM_solve() externalBuffer DEVICE IN/OUT Pointer to a workspace buffer of at least bufferSize bytes The same device pointer must be"
  },
  {
    "id": 29224,
    "content": "provided to the values parameter of the dense matrices matB and matC All other dense matrix descriptor parameters (e"
  },
  {
    "id": 29226,
    "content": ", order ) can be set independently cusparseSpSM_bufferSize() and cusparseSpSM_analysis() routines accept descriptors of NULL values for matB and matC These two routines do not accept NULL descriptors The routine allows the indices of matA to be unsorted cusparseSpSM() supports the following optimizations : CUDA graph capture Hardware Memory Compression cusparseSpSM_updateMatrix() updates the"
  },
  {
    "id": 29227,
    "content": "sparse matrix after calling the analysis phase This functions supports the following update strategies ( updatePart ): Strategy Notes CUSPARSE_SPSM_UPDATE_GENERAL Updates the sparse matrix values with values of newValues array CUSPARSE_SPSM_UPDATE_DIAGONAL Updates the diagonal part of the matrix with diagonal values stored in newValues array"
  },
  {
    "id": 29228,
    "content": "The function cusparseSDDMM_preprocess() can be called before cusparseSDDMM to speedup the actual computation It is useful when cusparseSDDMM is called multiple times with the same sparsity pattern ( matC ) cusparseSDDMM() for CUSPASRE_FORMAT_BSR supports block sizes of 2, 4, 8, 16, 32, 64 and 128 cusparseSDDMM() supports the following algorithms: Algorithm Notes CUSPARSE_SDDMM_ALG_DEFAULT Default"
  },
  {
    "id": 29229,
    "content": "algorithm cusparseSDDMM() has the following properties: The routine requires no extra storage Provides deterministic (bit-wise) results for each run The routine supports asynchronous execution The routine allows the indices of matC to be unsorted cusparseSDDMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the"
  },
  {
    "id": 29231,
    "content": "\\(\\mathbf{C^{\\prime}} = \\alpha op\\left( \\mathbf{A}  ight) \\cdot op\\left( \\mathbf{B}  ight) + \\beta\\mathbf{C}\\) where \\(\\alpha,\\) \\(\\beta\\) are scalars, and \\(\\mathbf{C},\\) \\(\\mathbf{C^{\\prime}}\\) have the same sparsity pattern"
  },
  {
    "id": 29232,
    "content": "The functions cusparseSpGEMM_workEstimation() , cusparseSpGEMM_estimateMemory() , and cusparseSpGEMM_compute() are used for both determining the buffer size and performing the actual computation"
  },
  {
    "id": 29233,
    "content": "Memory In/out Meaning handle HOST IN Handle to the cuSPARSE library context opA HOST IN Operation op(A) opB HOST IN Operation op(B) alpha HOST or DEVICE IN \\(\\alpha\\) scalar used for multiplication matA HOST IN Sparse matrix A matB HOST IN Sparse matrix B beta HOST or DEVICE IN \\(\\beta\\) scalar used for multiplication matC HOST IN/OUT Sparse matrix C computeType HOST IN Enumerator specifying the"
  },
  {
    "id": 29234,
    "content": "datatype in which the computation is executed alg HOST IN Enumerator specifying the algorithm for the computation spgemmDescr HOST IN/OUT Opaque descriptor for storing internal data used across the three steps num_prods HOST OUT Pointer to a 64-bit integer that stores the number of intermediate products calculated by cusparseSpGEMM_workEstimation chunk_fraction HOST IN The fraction of total"
  },
  {
    "id": 29237,
    "content": "CUSPARSE_SPGEMM_ALG2 Algorithm 2 Invokes cusparseSpGEMM_estimateMemory to get the amount of the memory required for the computation"
  },
  {
    "id": 29239,
    "content": "Invokes cusparseSpGEMM_estimateMemory to get the amount of the memory required for the computation The user can control the amount of required memory by changing the chunk size via chunk_fraction The chunk size is a fraction of total intermediate products: chunk_fraction * (*num_prods)"
  },
  {
    "id": 29240,
    "content": "cusparseSpGEMM() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine allows the indices of matA and matB to be unsorted The routine guarantees the indices of matC to be sorted cusparseSpGEMM() supports the following optimizations : CUDA graph capture Hardware Memory Compression See cusparseStatus_t for the description of the"
  },
  {
    "id": 29242,
    "content": "\\(\\mathbf{C^{\\prime}} = \\alpha op\\left( \\mathbf{A}  ight) \\cdot op\\left( \\mathbf{B}  ight) + \\beta\\mathbf{C}\\) where \\(\\alpha\\) and \\(\\beta\\) are scalars"
  },
  {
    "id": 29243,
    "content": "The functions cusparseSpGEMMreuse_workEstimation() , cusparseSpGEMMreuse_nnz() , and cusparseSpGEMMreuse_copy() are used for determining the buffer size and performing the actual computation"
  },
  {
    "id": 29244,
    "content": "MEMORY REQUIREMENT: cusparseSpGEMMreuse requires to keep in memory all intermediate products to reuse the structure of the output matrix On the other hand, the number of intermediate products is orders of magnitude higher than the number of non-zero entries in general"
  },
  {
    "id": 29245,
    "content": "In order to minimize the memory requirements, the routine uses multiple buffers that can be deallocated after they are no more needed"
  },
  {
    "id": 29246,
    "content": "If the number of intermediate product exceeds 2^31-1 , the routine will returns CUSPARSE_STATUS_INSUFFICIENT_RESOURCES status"
  },
  {
    "id": 29247,
    "content": "Currently, the function has the following limitations: Only 32-bit indices CUSPARSE_INDEX_32I is supported Only CSR format CUSPARSE_FORMAT_CSR is supported Only opA , opB equal to CUSPARSE_OPERATION_NON_TRANSPOSE are supported The data types combinations currently supported for cusparseSpGEMMreuse are listed below"
  },
  {
    "id": 29248,
    "content": "Uniform-precision computation: A / B / C / computeType CUDA_R_32F CUDA_R_64F CUDA_C_16F [DEPRECATED] CUDA_C_16BF [DEPRECATED] CUDA_C_32F CUDA_C_64F Mixed-precision computation: [DEPRECATED] A / B C computeType CUDA_R_16F CUDA_R_16F CUDA_R_32F CUDA_R_16BF CUDA_R_16BF CUDA_R_32F cusparseSpGEMMreuse routine runs for the following algorithm: Algorithm Notes CUSPARSE_SPGEMM_DEFAULT"
  },
  {
    "id": 29250,
    "content": "Provides deterministic (bit-wise) structure for the output matrix for each run, while value computation is not deterministic CUSPARSE_SPGEMM_CSR_ALG_DETERMINITIC Provides deterministic (bit-wise) structure for the output matrix and value computation for each run"
  },
  {
    "id": 29251,
    "content": "cusparseSpGEMMreuse() has the following properties: The routine requires no extra storage The routine supports asynchronous execution The routine allows the indices of matA and matB to be unsorted The routine guarantees the indices of matC to be sorted cusparseSpGEMMreuse() supports the following optimizations : CUDA graph capture Hardware Memory Compression Refer to cusparseStatus_t for the"
  },
  {
    "id": 29252,
    "content": "description of the return status Please visit cuSPARSE Library Samples - cusparseSpGEMMreuse for a code example"
  },
  {
    "id": 29256,
    "content": "cusparseSparseToDense()  cusparseStatus_t cusparseSparseToDense_bufferSize ( cusparseHandle_t handle , cusparseConstSpMatDescr_t matA , non-const descriptor supported cusparseDnMatDescr_t matB , cusparseSparseToDenseAlg_t alg , size_t * bufferSize ) cusparseStatus_t cusparseSparseToDense ( cusparseHandle_t handle , cusparseConstSpMatDescr_t matA , non-const descriptor supported"
  },
  {
    "id": 29257,
    "content": "cusparseDnMatDescr_t matB , cusparseSparseToDenseAlg_t alg , void * buffer ) The function converts the sparse matrix matA in CSR, CSC, or COO format into its dense representation matB The function cusparseSparseToDense_bufferSize() returns the size of the workspace needed by cusparseSparseToDense() The function cusparseDenseToSparse_bufferSize() returns the size of the workspace needed by"
  },
  {
    "id": 29258,
    "content": "cusparseDenseToSparse_analysis() The function cusparseDenseToSparse_analysis() updates the number of non-zero elements in the sparse matrix descriptor matB"
  },
  {
    "id": 29259,
    "content": "The user is responsible to allocate the memory required by the sparse matrix: Row/Column indices and value arrays for CSC and CSR respectively Row, column, value arrays for COO Column ( ellColInd ), value ( ellValue ) arrays for Blocked-ELL Finally, we call cusparseDenseToSparse_convert() for filling the arrays allocated in the previous step"
  },
  {
    "id": 29260,
    "content": "Please visit cuSPARSE Library Samples - cusparseDenseToSparse (CSR) and cuSPARSE Library Samples - cusparseDenseToSparse (Blocked-ELL) for code examples"
  },
  {
    "id": 29262,
    "content": "cuSPARSE Fortran Bindings  The cuSPARSE library is implemented using the C-based CUDA toolchain, and it thus provides a C-style API that makes interfacing to applications written in C or C++ trivial There are also many applications implemented in Fortran that would benefit from using cuSPARSE, and therefore a cuSPARSE Fortran interface has been developed Unfortunately, Fortran-to-C calling"
  },
  {
    "id": 29263,
    "content": "conventions are not standardized and differ by platform and toolchain In particular, differences may exist in the following areas: Symbol names (capitalization, name decoration) Argument passing (by value or reference) Passing of pointer arguments (size of the pointer) To provide maximum flexibility in addressing those differences, the cuSPARSE Fortran interface is provided in the form of wrapper"
  },
  {
    "id": 29266,
    "content": "This file also contains a few additional wrapper functions (for cudaMalloc() , cudaMemset , and so on) that can be used to allocate memory on the GPU The cuSPARSE Fortran wrapper code is provided as an example only and needs to be compiled into an application for it to call the cuSPARSE API functions"
  },
  {
    "id": 29267,
    "content": "Providing this source code allows users to make any changes necessary for a particular platform and toolchain"
  },
  {
    "id": 29268,
    "content": "The cuSPARSE Fortran wrapper code has been used to demonstrate interoperability with the compilers g95 0 91 (on 32-bit and 64-bit Linux) and g95 0 92 (on 32-bit and 64-bit Mac OS X) In order to use other compilers, users have to make any changes to the wrapper code that may be required"
  },
  {
    "id": 29269,
    "content": "The direct wrappers, intended for production code, substitute device pointers for vector and matrix arguments in all cuSPARSE functions"
  },
  {
    "id": 29270,
    "content": "To use these interfaces, existing applications need to be modified slightly to allocate and deallocate data structures in GPU memory space (using CUDA_MALLOC() and CUDA_FREE() ) and to copy data between GPU and CPU memory spaces (using the CUDA_MEMCPY() routines)"
  },
  {
    "id": 29272,
    "content": "c map device pointers to the OS-dependent type size_t , which is 32 bits wide on 32-bit platforms and 64 bits wide on a 64-bit platforms"
  },
  {
    "id": 29273,
    "content": "One approach to dealing with index arithmetic on device pointers in Fortran code is to use C-style macros and to use the C preprocessor to expand them"
  },
  {
    "id": 29275,
    "content": "The function GET_SHIFTED_ADDRESS() , provided with the cuSPARSE Fortran wrappers, can also be used, as shown in example B"
  },
  {
    "id": 29276,
    "content": "This example should be compiled with ARCH_64 defined as 1 on a 64-bit OS system and as undefined on a 32-bit OS system"
  },
  {
    "id": 29277,
    "content": "$ (cudaStat6 /= 0)) then write(*,*) \"Device malloc failed\" write(*,*) \"cudaStat1=\",cudaStat1 write(*,*) \"cudaStat2=\",cudaStat2 write(*,*) \"cudaStat3=\",cudaStat3 write(*,*) \"cudaStat4=\",cudaStat4 write(*,*) \"cudaStat5=\",cudaStat5 write(*,*) \"cudaStat6=\",cudaStat6 stop 2 endif cudaStat1 = cuda_memcpy_fort2c_int(cooRowIndex,cooRowIndexHostPtr, $ nnz*4,1) cudaStat2 ="
  },
  {
    "id": 29278,
    "content": "cuda_memcpy_fort2c_int(cooColIndex,cooColIndexHostPtr, $ nnz*4,1) cudaStat3 = cuda_memcpy_fort2c_real(cooVal, cooValHostPtr, $ nnz*8,1) cudaStat4 = cuda_memcpy_fort2c_real(y, yHostPtr, $ 2*n*8,1) cudaStat5 = cuda_memcpy_fort2c_int(xInd, xIndHostPtr, $ nnz_vector*4,1) cudaStat6 = cuda_memcpy_fort2c_real(xVal, xValHostPtr, $ nnz_vector*8,1) if ((cudaStat1 /= 0)"
  },
  {
    "id": 29280,
    "content": "epsilon)) then write(*,*) \"fortran example test FAILED\" else write(*,*) \"fortran example test PASSED\" endif c deallocate GPU memory and exit call cuda_free(cooRowIndex) call cuda_free(cooColIndex) call cuda_free(cooVal) call cuda_free(xInd) call cuda_free(xVal) call cuda_free(y) call cuda_free(z) call cuda_free(csrRowPtr) call cusparse_destroy_mat_descr(descrA) call cusparse_destroy(handle) stop"
  },
  {
    "id": 29282,
    "content": "Acknowledgements  NVIDIA would like to thank the following individuals and institutions for their contributions: The cusparsegtsv implementation is derived from a version developed by Li-Wen Chang from the University of Illinois"
  },
  {
    "id": 29283,
    "content": "The cusparsegtsvInterleavedBatch adopts cuThomasBatch developed by Pedro Valero-Lara and Ivan Martínez-Pérez from Barcelona Supercomputing Center and BSC/UPC NVIDIA GPU Center of Excellence"
  },
  {
    "id": 29288,
    "content": "Garland, “Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors” , Supercomputing, 2009"
  },
  {
    "id": 29290,
    "content": "0 User’s Guide”, Technical Report CNA-150, Center for Numerical Analysis, University of Texas, 1979 Naumov, “Incomplete-LU and Cholesky Preconditioned Iterative Methods Using cuSPARSE and cuBLAS” , Technical Report and White Paper, 2011"
  },
  {
    "id": 29295,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 29296,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 29298,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 29299,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 29300,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 29301,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 29302,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 29303,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 29304,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 29305,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 29306,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 29307,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 29308,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 29315,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 29317,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 29318,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 29322,
    "content": "pageBottom();}\nNVIDIA CUDA Toolkit Documentation Search In: Entire Site Just This Document clear search search CUDA Toolkit v12"
  },
  {
    "id": 29326,
    "content": "1 ( older ) - Last updated July 1, 2024 - Send Feedback cuRAND The API reference guide for cuRAND, the CUDA random number generation library"
  },
  {
    "id": 29327,
    "content": "GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU This direct path increases system bandwidth and decreases the latency and utilization load on the CPU This document provides information about the cuFile APIs that are used in applications and frameworks to leverage GDS technology and describes the"
  },
  {
    "id": 29328,
    "content": "intent, context, and operation of those APIs which are part of the GDS technology Note: The APIs and descriptions are subject to change without notice"
  },
  {
    "id": 29330,
    "content": "Usage This section describes the operation of the cuFile APIs Because the functionality is part of the CUDA Driver C API, the APIs use the cuFile prefix and camel case motif of the CUDA Driver"
  },
  {
    "id": 29336,
    "content": "peer to peer transfer using GPUDirect™ is supported to and from device memory on supported file system and hardware configurations"
  },
  {
    "id": 29337,
    "content": "The APIs will refer to this memory address as buffer pointer unless the API specifically applies to a particular type of memory"
  },
  {
    "id": 29341,
    "content": "If they are not called proactively, their actions will occur reactively: If cuFile{DriverOpen, HandleRegister, BufRegister} is called on a driver, file, or buffer, respectively that has been opened or registered by a previous cuFile * API call, this will result in an error Calling cuFile{BufDeregister, HandleDeregister, DriverClose} on a buffer, file, or driver, respectively that has never been"
  },
  {
    "id": 29343,
    "content": "For these errors, the output parameters of the APIs are left in an undefined state, and there are no other side effects"
  },
  {
    "id": 29344,
    "content": "If it is not used, driver initialization happens implicitly at the first use of the cuFile{HandleRegister, Read, Write, BufRegister} APIs"
  },
  {
    "id": 29345,
    "content": "(Mandatory) cuFileHandleRegister turns an OS-specific file descriptor into a CUfileHandle_t and performs checking on the GDS supportability based on the mount point and the way that the file was opened"
  },
  {
    "id": 29346,
    "content": "If this API is not called, an internal registered memory is used if required on the first time the buffer is used, for example, in cuFile{Read, Write} cuFile{BufDeregister, HandleDeregister} explicitly frees a buffer and file resources, respectively If this API is not called, the buffer and resources are implicitly freed when the driver is closed using cuFileDriverClose If this API is not called,"
  },
  {
    "id": 29347,
    "content": "the driver resources are implicitly freed when dlclose() is performed on the library handle or when the process is terminated"
  },
  {
    "id": 29350,
    "content": "Driver, File, and Buffer Management This section describes the overall workflow to manage the driver, the file, and buffer management: Call cuFileDriverOpen() to initialize the state of the critical performance path"
  },
  {
    "id": 29351,
    "content": "Allocate GPU memory with cudaMalloc, cudaMallocManaged , cuMem* APIs or host memory using cudaMallocHost , malloc or mmap"
  },
  {
    "id": 29352,
    "content": "To register the buffer, call cuFileBufRegister to initialize the buffer state of the critical performance path"
  },
  {
    "id": 29354,
    "content": "This step evaluates the suitability of the file state and the file mount for GDS and initializes the file state of the critical performance path"
  },
  {
    "id": 29355,
    "content": "Call IO APIs such as cuFileRead / cuFileWrite on an existing cuFile handle and existing buffer If the cuFileBufRegister has not been previously called on the buffer pointer, cuFileRead/cuFileWrite will use internal registered buffers when required"
  },
  {
    "id": 29356,
    "content": "Note: Not using the cuFileDeregister and cuFileDriverClose APIs (steps 5, 6, and 7) might unnecessarily consume resources, as shown by tools such as valgrind"
  },
  {
    "id": 29360,
    "content": "cuFile Compatibility Mode Use Cases cuFile APIs can be used in different scenarios: Developers building GPUDirect Storage applications with cuFile APIs, but don’t have the supported hardware configurations Developers building applications running on GPU cards that have CUDA compute capability > 6, but don’t have BAR space exposed Behavior The cuFile library provides a mechanism for cuFile reads"
  },
  {
    "id": 29361,
    "content": "and writes to use compatibility mode using POSIX pread , pwrite , and aio_submit APIS respectively to host memory and copying to GPU memory when applicable The behavior of compatibility mode with cuFile APIs is determined by the following configuration parameters Configuration Option (default) cuFile IO Behavior “allow_compat_mode\": true If true , falls back to using compatibility mode when the"
  },
  {
    "id": 29362,
    "content": "library detects that the buffer file descriptor opened cannot use GPUDirect Storage “force_compat_mode\": false If true , this option can be used to force all IO to use compatibility mode"
  },
  {
    "id": 29365,
    "content": "“gds_rdma_write_support\": true If false , forces compatibility mode to be used for writes even when the underlying file system is capable of performing GPUDirect Storage writes Note: If the option is “false”, this option will override and disable any filesystem-specific option to enable RDMA writes “posix_unaligned_writes” : false If true , forces compatibility mode to be used for writes where"
  },
  {
    "id": 29366,
    "content": "the file offset and/or IO size is not aligned to Page Boundary (4KB) “lustre:posix_gds_min_kb” : 0 For a lustre filesystem, if greater than 0 , compatibility mode is used for IO sizes between [1 - posix_gds_min_kb ] specified in kB “weka:rdma_write_support” : false If this option is false , all writes to WekaFS will use compatibility mode Note: If the option is set to “false” , cuFile library will"
  },
  {
    "id": 29367,
    "content": "use the posix path even if the allow_compat_mode option is true or false “gpfs:gds_write_support” : false If this option is false, all writes to IBM Spectrum Scale will use compatibility mode Note: If the option is set to “false” , cuFile library will use the posix path even if the allow_compat_mode option is true or false “rdma_dynamic_routing\": false, “rdma_dynamic_routing_order\": [ \" “SYS_MEM”"
  },
  {
    "id": 29368,
    "content": "] If rdma_dynamic_routing is set to true and rdma_dynamic_routing_order is set to [“SYS_MEM”] , then all IO for DFS will use compatibility mode In addition to the above configuration options, compatibility mode will be used as a fallback option for following use cases For wekaFS or IBM Spectrum Scale mounts: If there are no rdma_dev_addr_list specified, or failure to register MR with ib device For"
  },
  {
    "id": 29369,
    "content": "WekaFS and IBM Spectrum Scale: If the kernel returns -ENOTSUP for GPUDirect Storage read/write cuFile Stream and cuFile Batch APIs on IBM Spectrum Scale or WekaFS All Async and batch operations will internally use compatibility mode IO Limitations Compatible mode does not work in cases where the GPUs have CUDA compute capability less than 6"
  },
  {
    "id": 29372,
    "content": "cuFile API Specification This section provides information about the cuFile APIs that are used from the CPU to enable applications and frameworks"
  },
  {
    "id": 29374,
    "content": "5 | PDF | Archive cuFFT API Reference The API reference guide for cuFFT, the CUDA Fast Fourier Transform library Introduction  This document describes cuFFT, the NVIDIA® CUDA® Fast Fourier Transform (FFT) product"
  },
  {
    "id": 29375,
    "content": "The cuFFTW library is provided as a porting tool to enable users of FFTW to start using NVIDIA GPUs with a minimum amount of effort"
  },
  {
    "id": 29376,
    "content": "The FFT is a divide-and-conquer algorithm for efficiently computing discrete Fourier transforms of complex or real-valued data sets"
  },
  {
    "id": 29377,
    "content": "It is one of the most important and widely used numerical algorithms in computational physics and general signal processing"
  },
  {
    "id": 29378,
    "content": "The cuFFT library provides a simple interface for computing FFTs on an NVIDIA GPU, which allows users to quickly leverage the floating-point power and parallelism of the GPU in a highly optimized and tested FFT library The cuFFT product supports a wide range of FFT inputs and options efficiently on NVIDIA GPUs This version of the cuFFT library supports the following features: Algorithms highly"
  },
  {
    "id": 29379,
    "content": "optimized for input sizes that can be written in the form \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\)"
  },
  {
    "id": 29383,
    "content": "An \\(O\\left( n\\log n  ight)\\) algorithm for every input data size Half-precision (16-bit floating point), single-precision (32-bit floating point) and double-precision (64-bit floating point)"
  },
  {
    "id": 29384,
    "content": "Real valued input or output require less computations and data than complex values and often have faster time to solution Types supported are: C2C - Complex input to complex output R2C - Real input to complex output C2R - Symmetric complex input to real output 1D, 2D and 3D transforms Execution of multiple 1D, 2D and 3D transforms simultaneously"
  },
  {
    "id": 29385,
    "content": "In-place and out-of-place transforms Arbitrary intra- and inter-dimension element strides (strided layout) FFTW compatible data layout Execution of transforms across multiple GPUs Streamed execution, enabling asynchronous computation and data movement The cuFFTW library provides the FFTW3 API to facilitate porting of existing FFTW applications"
  },
  {
    "id": 29390,
    "content": "The Discrete Fourier transform (DFT) maps a complex-valued vector \\(x_{k}\\) ( time domain ) into its frequency domain representation given by: \\(X_{k} = \\sum\\limits_{n = 0}^{N - 1}x_{n}e^{-2\\pi i\\frac{kn}{N}}\\) where \\(X_{k}\\) is a complex-valued vector of the same size"
  },
  {
    "id": 29392,
    "content": "The cuFFT API is modeled after FFTW , which is one of the most popular and efficient CPU-based FFT libraries"
  },
  {
    "id": 29393,
    "content": "cuFFT provides a simple configuration mechanism called a plan that uses internal building blocks to optimize the transform for the given configuration and the particular GPU hardware selected Then, when the execution function is called, the actual transform takes place following the plan of execution The advantage of this approach is that once the user creates a plan, the library retains whatever"
  },
  {
    "id": 29394,
    "content": "state is needed to execute the plan multiple times without recalculation of the configuration This model works well for cuFFT because different kinds of FFTs require different thread configurations and GPU resources, and the plan interface provides a simple way of reusing configurations"
  },
  {
    "id": 29395,
    "content": "Computing a number BATCH of one-dimensional DFTs of size NX using cuFFT will typically look like this: #define NX 256 #define BATCH 10 #define RANK 1 cudaMalloc (( void ** ) & data , sizeof ( cufftComplex ) * NX * BATCH ); cufftPlanMany ( & plan , RANK , NX , & iembed , istride , idist , & oembed , ostride , odist , CUFFT_C2C , BATCH );"
  },
  {
    "id": 29396,
    "content": "They consist of compiled programs ready for users to incorporate into applications with the compiler and linker"
  },
  {
    "id": 29397,
    "content": "By selecting Download CUDA Production Release users are all able to install the package containing the CUDA Toolkit, SDK code samples and development drivers The Linux release for simplecuFFT assumes that the root install directory is /usr/local/cuda and that the locations of the products are contained there as follows"
  },
  {
    "id": 29399,
    "content": "so inc/cufft h cuFFT library with Xt functionality {lib, lib64}/libcufft so inc/cufftXt h cuFFTW library {lib, lib64}/libcufftw so inc/cufftw"
  },
  {
    "id": 29404,
    "content": "cu file and the library included in the link line A single compile and link line might appear as /usr/local/cuda/bin/nvcc [options] filename cu … -I/usr/local/cuda/inc -L/usr/local/cuda/lib -lcufft Of course there will typically be many compile lines and the compiler g++ may be used for linking so long as the library path is set correctly"
  },
  {
    "id": 29405,
    "content": "Users of the FFTW interface (see FFTW Interface to cuFFT ) should include cufftw h and link with both cuFFT and cuFFTW libraries This means any memory allocated by cudaMalloc , cudaMallocHost and cudaMallocManaged or registered with cudaHostRegister can be used as input, output or plan work area with cuFFT and cuFFTW functions For the best performance input data, output data and plan work area"
  },
  {
    "id": 29406,
    "content": "should reside in device memory cuFFTW library also supports input data and output data that is not GPU visible"
  },
  {
    "id": 29409,
    "content": "Fourier Transform Setup  The first step in using the cuFFT Library is to create a plan using one of the following: cufftPlan1D() / cufftPlan2D() / cufftPlan3D() - Create a simple plan for a 1D/2D/3D transform respectively"
  },
  {
    "id": 29410,
    "content": "cufftXtMakePlanMany() - Creates a plan supporting batched input and strided data layouts for any supported precision Among the plan creation functions, cufftPlanMany() allows use of more complicated data layouts and batched executions"
  },
  {
    "id": 29415,
    "content": "*n[rank-1] cufftComplex or cufftDoubleComplex elements (where batch denotes the number of transforms that will be executed in parallel, rank is the number of dimensions of the input data (see Multidimensional Transforms ) and n[] is the array of transform dimensions) for single and double-precision transforms respectively"
  },
  {
    "id": 29421,
    "content": "The next step in using the library is to call an execution function such as cufftExecC2C() (see Parameter cufftType ) which will perform the transform with the specifications defined at planning"
  },
  {
    "id": 29422,
    "content": "One can create a cuFFT plan and perform multiple transforms on different data sets by providing different input and output pointers"
  },
  {
    "id": 29423,
    "content": "Once the plan is no longer needed, the cufftDestroy() function should be called to release the resources allocated for the plan"
  },
  {
    "id": 29427,
    "content": "Free Memory Requirement  The first program call to any cuFFT function causes the initialization of the cuFFT kernels"
  },
  {
    "id": 29432,
    "content": "Plan Initialization Time  During plan initialization, cuFFT conducts a series of steps, including heuristics to determine which kernels to be used as well as kernel module loads"
  },
  {
    "id": 29434,
    "content": "0, cuFFT delivers a larger portion of kernels using the CUDA Parallel Thread eXecution assembly form (PTX code), instead of the binary form (cubin object) The PTX code of cuFFT kernels are loaded and compiled further to the binary code by the CUDA device driver at runtime when a cuFFT plan is initialized JIT compilation slightly increases cuFFT plan initialization time, depending on the transform"
  },
  {
    "id": 29435,
    "content": "size and the speed of the host CPU (see Module load driver API ) But the JIT overhead occurs only when a binary code is generated for the first time during plan initialization using one of the plan creation functions The device driver automatically caches a copy of the generated binary code to avoid repeating the compilation in subsequent invocations"
  },
  {
    "id": 29436,
    "content": "If necessary, CUDA_CACHE_PATH or CUDA_CACHE_MAXSIZE can be customized to set the cache folder and max size (see detail in CUDA Environmental Variables ), but the default settings are fine in general"
  },
  {
    "id": 29439,
    "content": "Fourier Transform Types  Apart from the general complex-to-complex (C2C) transform, cuFFT implements efficiently two other types: real-to-complex (R2C) and complex-to-real (C2R)"
  },
  {
    "id": 29440,
    "content": "It can be easily shown that in this case the output satisfies Hermitian symmetry ( \\(X_{k} = X_{N - k}^{\\ast}\\) , where the star denotes complex conjugation)"
  },
  {
    "id": 29441,
    "content": "The converse is also true: for complex-Hermitian input the inverse transform will be purely real-valued"
  },
  {
    "id": 29443,
    "content": "Transform execution functions for single and double-precision are defined separately as: cufftExecC2C() / cufftExecZ2Z() - complex-to-complex transforms for single/double precision cufftExecR2C() / cufftExecD2Z() - real-to-complex forward transform for single/double precision cufftExecC2R() / cufftExecZ2D() - complex-to-real inverse transform for single/double precision"
  },
  {
    "id": 29445,
    "content": "For one-dimensional signals, this requires the 0th element (and the \\(\\frac{N}{2}\\) th input if N is even) to be real-valued, i"
  },
  {
    "id": 29447,
    "content": "For d-dimension signals, this means \\(x_{(n_{1},n_{2},\\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\\ldots,N_{d} - n_{d})}^{\\ast}\\)"
  },
  {
    "id": 29448,
    "content": "Functions cufftXtExec() and cufftXtExecDescriptor() can perform transforms on any of the supported types"
  },
  {
    "id": 29452,
    "content": "Half-precision cuFFT Transforms  Half-precision transforms have the following limitations: Minimum GPU architecture is SM_53 Sizes are restricted to powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany"
  },
  {
    "id": 29459,
    "content": "Bfloat16-precision cuFFT Transforms  cuFFT supports bfloat16 precision using the nv_bfloat16 data type Please note that cuFFT utilizes a combination of single- and bfloat16-precision arithmetic operations when computing the FFT in bfloat16 precision Bfloat16-precision transforms have similar limitations to half-precision transforms: Minimum GPU architecture is SM_80 Sizes are restricted to"
  },
  {
    "id": 29460,
    "content": "powers of two only Strides on the real part of real-to-complex and complex-to-real transforms are not supported More than one GPU is not supported Transforms spanning more than 4 billion elements are not supported Please refer to cufftXtMakePlanMany function for plan creation details"
  },
  {
    "id": 29465,
    "content": "Data Layout  In the cuFFT Library, data layout depends strictly on the configuration and the transform type In the case of general complex-to-complex transform both the input and output data shall be a cufftComplex / cufftDoubleComplex array in single- and double-precision modes respectively"
  },
  {
    "id": 29466,
    "content": "In C2R mode an input array \\((x_{1},x_{2},\\ldots,x_{\\lfloor\\frac{N}{2} floor + 1})\\) of only non-redundant complex elements is required The output array \\((X_{1},X_{2},\\ldots,X_{N})\\) consists of cufftReal / cufftDouble elements in this mode Finally, R2C demands an input array \\((X_{1},X_{2},\\ldots,X_{N})\\) of real values and returns an array \\((x_{1},x_{2},\\ldots,x_{\\lfloor\\frac{N}{2} floor +"
  },
  {
    "id": 29468,
    "content": "In real-to-complex and complex-to-real transforms the size of input data and the size of output data differ Therefore input data for real-to-complex and output data for complex-to-real must be padded Expected sizes of input/output data for 1-d transforms are summarized in the table below: FFT type input data size output data size C2C \\(x\\) cufftComplex \\(x\\) cufftComplex C2R \\(\\left\\lfloor"
  },
  {
    "id": 29469,
    "content": "\\frac{x}{2} ight floor + 1\\) cufftComplex \\(x\\) cufftReal R2C* \\(x\\) cufftReal \\(\\left\\lfloor \\frac{x}{2} ight floor + 1\\) cufftComplex The real-to-complex transform is implicitly a forward transform For an in-place real-to-complex transform where FFTW compatible output is desired, the input size must be padded to \\(\\left( {\\lfloor\\frac{N}{2} floor + 1} ight)\\) complex elements For out-of-place"
  },
  {
    "id": 29470,
    "content": "transforms, input and output sizes match the logical transform size \\(N\\) and the non-redundant size \\(\\lfloor\\frac{N}{2} floor + 1\\) , respectively For in-place complex-to-real FFTs where FFTW compatible output is selected (default padding mode), the input size is assumed to be \\(\\lfloor\\frac{N}{2} floor + 1\\) cufftComplex elements Note that in-place complex-to-real FFTs may overwrite arbitrary"
  },
  {
    "id": 29472,
    "content": "Similar to the one-dimensional case, the frequency domain representation of real-valued input data satisfies Hermitian symmetry, defined as: \\(x_{(n_{1},n_{2},\\ldots,n_{d})} = x_{(N_{1} - n_{1},N_{2} - n_{2},\\ldots,N_{d} - n_{d})}^{\\ast}\\) C2R and R2C algorithms take advantage of this fact by operating only on half of the elements of signal array, namely on: \\(x_{\\mathbf{n}}\\) for \\(\\mathbf{n}"
  },
  {
    "id": 29473,
    "content": "\\in \\{ 1,\\ldots,N_{1}\\} \\times \\ldots \\times \\{ 1,\\ldots,N_{d - 1}\\} \\times \\{ 1,\\ldots,\\lfloor\\frac{N_{d}}{2} floor + 1\\}\\)"
  },
  {
    "id": 29474,
    "content": "The general rules of data alignment described in Data Layout apply to higher-dimensional transforms Advanced Data Layout  The advanced data layout feature allows transforming only a subset of an input array, or outputting to only a portion of a larger data structure"
  },
  {
    "id": 29475,
    "content": "It can be set by calling function: cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ); Passing inembed or onembed set to NULL is a special case and is equivalent to passing n for each"
  },
  {
    "id": 29476,
    "content": "This is same as the basic data layout and other advanced parameters such as istride are ignored If the advanced parameters are to be used, then all of the advanced interface parameters must be specified correctly Advanced parameters are defined in units of the relevant data type ( cufftReal , cufftDoubleReal , cufftComplex , or cufftDoubleComplex ) Advanced layout can be perceived as an"
  },
  {
    "id": 29477,
    "content": "additional layer of abstraction above the access to input/output data arrays An element of coordinates [z][y][x] in signal number b in the batch will be associated with the following addresses in the memory: 1D input[ b * idist + x * istride ] output[ b * odist + x * ostride ] 2D input[ b * idist` + (x * inembed[1] + y) * istride ] output[ b * odist + (x * onembed[1] + y) * ostride ] 3D input[ b *"
  },
  {
    "id": 29478,
    "content": "idist + ((x * inembed[1] + y) * inembed[2] + z) * istride ] output[ b * odist + ((x * onembed[1] + y) * onembed[2] + z) * ostride ] The istride and ostride parameters denote the distance between two successive input and output elements in the least significant (that is, the innermost) dimension respectively In a single 1D transform, if every input element is to be used in the transform, istride"
  },
  {
    "id": 29479,
    "content": "should be set to \\(1\\) ; if every other input element is to be used in the transform, then istride should be set to \\(2\\) Similarly, in a single 1D transform, if it is desired to output final elements one after another compactly, ostride should be set to \\(1\\) ; if spacing is desired between the least significant dimension output data, ostride should be set to the distance between the elements The"
  },
  {
    "id": 29480,
    "content": "inembed and onembed parameters define the number of elements in each dimension in the input array and the output array respectively The inembed[rank-1] contains the number of elements in the least significant (innermost) dimension of the input data excluding the istride elements; the number of total elements in the least significant dimension of the input array is then istride*inembed[rank-1] The"
  },
  {
    "id": 29481,
    "content": "inembed[0] or onembed[0] corresponds to the most significant (that is, the outermost) dimension and is effectively ignored since the idist or odist parameter provides this information instead Note that the size of each dimension of the transform should be less than or equal to the inembed and onembed values for the corresponding dimension, that is n[i] ≤ inembed[i] , n[i] ≤ onembed[i] , where \\(i"
  },
  {
    "id": 29482,
    "content": "\\in \\{ 0,\\ldots,rank - 1\\}\\) The idist and odist parameters indicate the distance between the first element of two consecutive batches in the input and output data"
  },
  {
    "id": 29485,
    "content": "Streamed cuFFT Transforms  Every cuFFT plan may be associated with a CUDA stream Once so associated, all launches of the internal stages of that plan take place through the specified stream Streaming of cuFFT execution allows for potential overlap between transforms and memory copies"
  },
  {
    "id": 29487,
    "content": ") If no stream is associated with a plan, launches take place in stream(0) , the default CUDA stream"
  },
  {
    "id": 29488,
    "content": "cuFFT uses private streams internally to sort operations, including event syncrhonization cuFFT does not guarantee ordering of internal operations, and the order is only preserved with respect to the streams set by the user"
  },
  {
    "id": 29490,
    "content": "In previous versions of cuFFT, cufftSetStream() returns an error in the multiple GPU case Likewise, calling certain multi-GPU functions such as cufftXtSetCallback() after setting a stream with cufftSetStream() will result in an error (see API functions for more details)"
  },
  {
    "id": 29491,
    "content": "Please note that in order to overlap plans using single plan handle user needs to manage work area buffers Work area can be set by cufftSetWorkArea function"
  },
  {
    "id": 29494,
    "content": "Multiple GPU cuFFT Transforms  cuFFT supports using up to sixteen GPUs connected to a CPU to perform Fourier Transforms whose calculations are distributed across the GPUs"
  },
  {
    "id": 29495,
    "content": "An API has been defined to allow users to write new code or modify existing code to use this functionality"
  },
  {
    "id": 29496,
    "content": "Some existing functions such as the creation of a plan using cufftCreate() also apply in the multiple GPU case"
  },
  {
    "id": 29497,
    "content": "The memory on the GPUs is managed by helper functions cufftXtMalloc()/cufftXtFree() and cufftXtMemcpy() using the cudaLibXtDesc descriptor Performance is a function of the bandwidth between the GPUs, the computational ability of the individual GPUs, and the type and number of FFT to be performed"
  },
  {
    "id": 29504,
    "content": "Note that multiple GPU execution is not guaranteed to solve a given size problem in a shorter time than single GPU execution"
  },
  {
    "id": 29505,
    "content": "The general steps in defining and executing a transform with this API are: cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used Optional: cufftEstimate{1d,2d,3d,Many}() - estimate the sizes of the work areas required These are the same functions used in the single GPU case although the definition of the argument workSize reflects the"
  },
  {
    "id": 29506,
    "content": "number of GPUs used Optional: cufftGetSize{1d,2d,3d,Many}() - refined estimate of the sizes of the work areas required This is the same function used in the single GPU case although the definition of the argument workSize reflects the number of GPUs used"
  },
  {
    "id": 29507,
    "content": "cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - execute the plan cufftXtMemcpy() - copy data from the GPUs cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2"
  },
  {
    "id": 29510,
    "content": "Plan Specification and Work Areas  In the single GPU case a plan is created by a call to cufftCreate() followed by a call to cufftMakePlan*() For multiple GPUs, the GPUs to use for execution are identified by a call to cufftXtSetGPUs() and this must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*() Also the strides and batches apply to the entire plan across all"
  },
  {
    "id": 29511,
    "content": "GPUs associated with the plan Once a plan is locked by a call to cufftMakePlan*() , different descriptors may be specified in calls to cufftXtExecDescriptor*() to execute the plan on different data sets, but the new descriptors must use the same GPUs in the same order As in the single GPU case, cufftEstimateSize{Many,1d,2d,3d}() and cufftGetSize{Many,1d,2d,3d}() give estimates of the work area"
  },
  {
    "id": 29512,
    "content": "sizes required for a multiple GPU plan and in this case workSize points to a size_t array, one entry per GPU Similarly the actual work size returned by cufftGetSize() is a size_t array, one entry per GPU in the multiple GPU case"
  },
  {
    "id": 29516,
    "content": "Helper Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution"
  },
  {
    "id": 29518,
    "content": "To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMalloc() and cufftXtFree() functions The function cufftXtMalloc() returns a descriptor which specifies the location of these memories To provide similar functionality in the multiple GPU case, cuFFT includes cufftXtMemcpy() which allows users to copy between host and multiple GPU memories or even between the GPU"
  },
  {
    "id": 29520,
    "content": "All single GPU cuFFT FFTs return output the data in natural order, that is the ordering of the result is the same as if a DFT had been performed on the data Some Fast Fourier Transforms produce intermediate results where the data is left in a permutation of the natural output When cufftXtMemcpy() is used to copy data from GPU memory back to host memory, the results are in natural order regardless"
  },
  {
    "id": 29521,
    "content": "of whether the data on the GPUs is in natural order or permuted Using CUFFT_COPY_DEVICE_TO_DEVICE allows users to copy data from the permuted data format produced after a single transform to the natural order on GPUs"
  },
  {
    "id": 29525,
    "content": "Multiple GPU 2D and 3D Transforms on Permuted Input  For single 2D or 3D transforms on multiple GPUs, when cufftXtMemcpy() distributes the data to the GPUs, the array is divided on the X axis for two GPUs half of the X dimenson points, for all Y (and Z) values, are copied to each of the GPUs When the transform is computed, the data are permuted such that they are divided on the Y axis When cuFFT"
  },
  {
    "id": 29527,
    "content": "This is done because many algorithms compute a forward FFT, then perform some point-wise operation on the result, and then compute the inverse FFT"
  },
  {
    "id": 29528,
    "content": "To avoid this, cufftXtMemcpy and cufftXtExecDescriptor() keep track of the data ordering so that the correct operation is used"
  },
  {
    "id": 29530,
    "content": "cufftCreate() - create an empty plan, as in the single GPU case cufftXtSetGPUs() - define which GPUs are to be used cufftMakePlan{1d,2d,3d,Many}() - create the plan"
  },
  {
    "id": 29531,
    "content": "cufftXtMalloc() - allocate descriptor and data on the GPUs cufftXtMemcpy() - copy data to the GPUs cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the forward FFT userFunction() - modify the data in the frequency domain cufftXtExecDescriptorC2C()/cufftXtExecDescriptorZ2Z() - compute the inverse FFT Note that it was not necessary to copy/permute the data between execute calls"
  },
  {
    "id": 29532,
    "content": "cufftXtMemcpy() - copy data to the host cufftXtFree() - free any memory allocated with cufftXtMalloc() cufftDestroy() - free cuFFT plan resources 2"
  },
  {
    "id": 29535,
    "content": "Supported Functionality  Starting with cuFFT version 7 0, a subset of single GPU functionality is supported for multiple GPU execution"
  },
  {
    "id": 29536,
    "content": "Requirements and limitations: All GPUs must have the same CUDA architecture level and support Unified Virtual Address Space"
  },
  {
    "id": 29537,
    "content": "For an application that uses the CUDA Driver API, running cuFFT on multiple GPUs is only compatible with applications using the primary context on each GPU Running cuFFT on more than 8 GPUs (16 GPUs is max) is supported on machines with NVLink only"
  },
  {
    "id": 29538,
    "content": "While transforms with batch count greater than one do not impose additional constraints, those with a single batch have some restrictions Single-batch FFTs support only in-place mode, and have additional constraints depending on the FFT type For batch size m on n GPUs : The first m % n GPUs execute \\(\\left\\lfloor \\frac{m}{n} ight floor+\\ 1\\) transforms Batch size output differences: Single GPU"
  },
  {
    "id": 29539,
    "content": "cuFFT results are always returned in natural order When multiple GPUs are used to perform more than one transform, the results are also returned in natural order When multiple GPUs are used to perform a single transform the results are returned in a permutation of the normal results to reduce communication time This behavior is summarized in the following table: Number of GPUs Number of transforms"
  },
  {
    "id": 29540,
    "content": "Output Order on GPUs One One or multiple transforms Natural order Multiple One Permuted results Multiple Multiple Natural order To produce natural order results in GPU memory for multi-GPU runs in the 1D single transform case, requires calling cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE 2D and 3D multi-GPU transforms support execution of a transform given permuted order results as input It is"
  },
  {
    "id": 29541,
    "content": "also possible to use cufftXtMemcpy() with CUFFT_COPY_DEVICE_TO_DEVICE to return 2D or 3D data to natural order See the cuFFT Code Examples section for single GPU and multiple GPU examples"
  },
  {
    "id": 29544,
    "content": "cuFFT Callback Routines  Callback routines are user-supplied kernel routines that cuFFT will call when loading or storing data"
  },
  {
    "id": 29546,
    "content": "4, support for callback functionality using separately compiled device code is deprecated on all GPU architectures Callback functionality will continue to be supported for all GPU architectures"
  },
  {
    "id": 29550,
    "content": "Overview of the cuFFT Callback Routine Feature  cuFFT provides a set of APIs that allow the cuFFT user to provide CUDA functions that re-direct or manipulate the data as it is loaded prior to processing the FFT, or stored once the FFT has been done For the load callback, cuFFT passes the callback routine the address of the input data and the offset to the value to be loaded from device memory,"
  },
  {
    "id": 29551,
    "content": "and the callback routine returns the value it wishes cuFFT to use instead For the store callback, cuFFT passes the callback routine the value it has computed, along with the address of the output data and the offset to the value to be written to device memory, and the callback routine modifies the value and stores the modified result In order to provide a callback to cuFFT, a plan is created and"
  },
  {
    "id": 29552,
    "content": "configured normally using the extensible plan APIs After the call to cufftCreate and cufftMakePlan , the user may associate a load callback routine, or a store callback routine, or both, with the plan, by calling cufftXtSetCallback The caller also has the option to specify a device pointer to an opaque structure they wish to associate with the plan The caller may use this structure to remember"
  },
  {
    "id": 29553,
    "content": "plan dimensions and strides, or have a pointer to auxiliary data, etc With some restrictions, the callback routine is allowed to request shared memory for its own use If the requested amount of shared memory is available, cufft will pass a pointer to it when it calls the callback routine CUFFT allows for 8 types of callback routine, one for each possible combination of: load or store, real or"
  },
  {
    "id": 29554,
    "content": "complex, single precision or double It is the caller’s responsibility to provide a routine that matches the function prototype for the type of routine specified If there is already a callback of the specified type associated with the plan, the set callback function will replace it with the new one The general steps in defining and executing a transform with callbacks are: cufftCreate() - create an"
  },
  {
    "id": 29555,
    "content": "empty plan, as in the single GPU case cufftMakePlan{1d,2d,3d,Many}() - create the plan cufftXtSetCallback() - called for load and/or store callback for this plan cufftExecC2C() etc - execute the plan cufftDestroy() - free cuFFT plan resources Callback functions are not supported on transforms with a dimension size that does not factor into primes smaller than 127 Callback functions on plans whose"
  },
  {
    "id": 29556,
    "content": "dimensions’ prime factors are limited to 2, 3, 5, and 7 can safely call __syncthreads() Note The callback API is available in the statically linked cuFFT library only, and only on 64 bit LINUX operating systems"
  },
  {
    "id": 29560,
    "content": "Specifying Load and Store Callback Routines  In order to associate a callback routine with a plan, it is necessary to obtain a device pointer to the callback routine As an example, if the user wants to specify a load callback for an R2C transform, they would write the device code for the callback function, and define a global device variable that contains a pointer to the function: __device__"
  },
  {
    "id": 29561,
    "content": "cufftReal myOwnCallback ( void * dataIn , size_t offset , void * callerInfo , void * sharedPtr ) { cufftReal ret ; use offset, dataIn, and optionally callerInfo to compute the return value return ret ; } __device__ cufftCallbackLoadR myOwnCallbackPtr = myOwnCallback ; From the host side, the user then has to get the address of the callback routine, which is stored in myOwnCallbackPtr This is done"
  },
  {
    "id": 29562,
    "content": "with cudaMemcpyFromSymbol , as follows: cufftCallbackLoadR hostCopyOfCallbackPtr ; cudaMemcpyFromSymbol ( & hostCopyOfCallbackPtr , myOwnCallbackPtr , sizeof ( hostCopyOfCallbackPtr )); hostCopyOfCallbackPtr then contains the device address of the callback routine, that should be passed to cufftXtSetCallback Note that, for multi-GPU transforms, hostCopyOfCallbackPtr will need to be an array of"
  },
  {
    "id": 29564,
    "content": "Please note that __managed__ variables are not suitable to pass to cufftSetCallback due to restrictions on variable usage (See the NVIDIA CUDA Programming Guide for more information about __managed__ variables)"
  },
  {
    "id": 29568,
    "content": "Callback Routine Function Details  Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to load data prior to the transform"
  },
  {
    "id": 29569,
    "content": "typedef cufftComplex ( * cufftCallbackLoadC )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleComplex ( * cufftCallbackLoadZ )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftReal ( * cufftCallbackLoadR )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); typedef cufftDoubleReal ( *"
  },
  {
    "id": 29570,
    "content": "cufftCallbackLoadD )( void * dataIn , size_t offset , void * callerInfo , void * sharedPointer ); Parameters for all of the load callbacks are defined as below: offset : offset of the input element from the start of output data dataIn : device pointer to the start of the input array that was passed in the cufftExecute call callerInfo : device pointer to the optional caller specified data passed in"
  },
  {
    "id": 29571,
    "content": "the cufftXtSetCallback call sharedPointer : pointer to shared memory, valid only if the user has called cufftXtSetCallbackSharedSize()"
  },
  {
    "id": 29572,
    "content": "Below are the function prototypes, and typedefs for pointers to the user supplied callback routines that cuFFT calls to store data after completion of the transform This is because a store callback function is responsible not only for transforming the data as desired, but also for writing the data to the desired location This allows the store callback to rearrange the data, for example to shift"
  },
  {
    "id": 29574,
    "content": "typedef void ( * cufftCallbackStoreC )( void * dataOut , size_t offset , cufftComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreZ )( void * dataOut , size_t offset , cufftDoubleComplex element , void * callerInfo , void * sharedPointer ); typedef void ( * cufftCallbackStoreR )( void * dataOut , size_t offset , cufftReal element , void * callerInfo ,"
  },
  {
    "id": 29575,
    "content": "void * sharedPointer ); typedef void ( * cufftCallbackStoreD )( void * dataOut , size_t offset , cufftDoubleReal element , void * callerInfo , void * sharedPointer ); Parameters for all of the store callbacks are defined as below: offset : offset of the output element from the start of output data dataOut : device pointer to the start of the output array that was passed in the cufftExecute call"
  },
  {
    "id": 29576,
    "content": "element : the real or complex result computed by CUFFT for the element specified by the offset argument"
  },
  {
    "id": 29580,
    "content": "Coding Considerations for the cuFFT Callback Routine Feature  cuFFT supports callbacks on all types of transforms, dimension, batch, stride between elements or number of GPUs cuFFT supports a wide range of parameters, and based on those for a given plan, it attempts to optimize performance The number of kernels launched, and for each of those, the number of blocks launched and the number of"
  },
  {
    "id": 29582,
    "content": "For some configurations, cuFFT will load or store (and process) multiple inputs or outputs per thread For some configurations, threads may load or store inputs or outputs in any order, and cuFFT does not guarantee that the inputs or outputs handled by a given thread will be contiguous cuFFT will call the load callback routine, for each point in the input, once and only once Similarly it will call"
  },
  {
    "id": 29583,
    "content": "the store callback routine, for each point in the output, once and only once the input and output data are in the same memory location) the store callback for a given element cannot overwrite other elements It can either overwrite the given element, or write in a completely distinct output buffer When more than one kernel are used to implement a transform, the thread and block structure of the"
  },
  {
    "id": 29584,
    "content": "first kernel (the one that does the load) is often different from the thread and block structure of the last kernel (the one that does the store)"
  },
  {
    "id": 29585,
    "content": "One common use of callbacks is to reduce the amount of data read or written to memory, either by selective filtering or via type conversions"
  },
  {
    "id": 29586,
    "content": "When more than one kernel are used to implement a transform, cuFFT alternates using the workspace and the output buffer to write intermediate results This means that the output buffer must always be large enough to accommodate the entire transform For multi-GPU transforms, the index passed to the callback routine is the element index from the start of data on that GPU , not from the start of the"
  },
  {
    "id": 29587,
    "content": "entire input or output data array For transforms whose dimensions can be factored into powers of 2, 3, 5, or 7, cuFFT guarantees that it will call the load and store callback routines from points in the kernel that is safe to call __syncthreads function from within callback routine Caller is responsible for guaranteeing that the callback routine is at a point where the callback code has converged,"
  },
  {
    "id": 29588,
    "content": "to avoid deadlock For plans whose dimensions are factored into higher primes, results of a callback routine calling __syncthreads are not defined"
  },
  {
    "id": 29593,
    "content": "No Ordering Guarantees Within a Kernel  Note that there are no guarantees on the relative order of execution of blocks within a grid For instance, reordering data (such as an FFT-shift) could rely on the order of execution of the blocks"
  },
  {
    "id": 29597,
    "content": "Thread Safety  cuFFT APIs are thread safe as long as different host threads execute FFTs using different plans and the output data are disjoint"
  },
  {
    "id": 29601,
    "content": "The stream associated with a cuFFT plan must meet the requirements stated in Creating a Graph Using Stream Capture"
  },
  {
    "id": 29604,
    "content": "0 onward), CUDA Graphs are no longer supported for callback routines that load data in out-of-place mode transforms"
  },
  {
    "id": 29605,
    "content": "An upcoming release will update the cuFFT callback implementation, removing this limitation cuFFT deprecated callback functionality based on separate compiled device code in cuFFT 11"
  },
  {
    "id": 29615,
    "content": "For example, on linux, to compile a small application using cuFFT against the dynamic library, the following command can be used: nvcc mCufftApp c -lcufft -o myCufftApp For cufftw on Linux, to compile a small application against the dynamic library, the following command can be used: nvcc mCufftwApp c -lcufftw -lcufft -o myCufftwApp Whereas to compile against the static cuFFT library, extra steps"
  },
  {
    "id": 29617,
    "content": "To determine if a specific SM is included in the cuFFT library, one may use cuobjdump utility For example, if you wish to know if SM_50 is included, the command to run is cuobjdump -arch sm_50 libcufft_static"
  },
  {
    "id": 29620,
    "content": "Depending on the Host Operating system, some additional libraries like pthread or dl might be needed on the linking line"
  },
  {
    "id": 29621,
    "content": "In the case of a system which does not have the CUDA driver installed, this allows the application to gracefully manage this issue and potentially run if a CPU-only path is available"
  },
  {
    "id": 29622,
    "content": "The callback routines are CUDA device code, and must be separately compiled with NVCC and linked with the cuFFT library If you specify an SM when compiling your callback functions, you must specify one of the SM’s cuFFT includes"
  },
  {
    "id": 29626,
    "content": "Static library without callback support  Starting with cuFFT version 9 2, a new variant of the cuFTT static library, libcufft_static_nocallback"
  },
  {
    "id": 29628,
    "content": "This new version does not contain callback functionality and can be linked using the host compiler only"
  },
  {
    "id": 29631,
    "content": "Accuracy and Performance  A DFT can be implemented as a matrix vector multiplication that requires \\(O(N^{2})\\) operations"
  },
  {
    "id": 29632,
    "content": "However, the cuFFT Library employs the Cooley-Tukey algorithm to reduce the number of required operations to optimize the performance of particular transform sizes"
  },
  {
    "id": 29634,
    "content": "Hence the performance of any transform size that can be factored as \\(2^{a} \\times 3^{b} \\times 5^{c} \\times 7^{d}\\) (where a , b , c , and d are non-negative integers) is optimized in the cuFFT library"
  },
  {
    "id": 29635,
    "content": "The nvJitLink library is loaded dynamically, and should be present in the system’s dynamic linking path (e"
  },
  {
    "id": 29642,
    "content": "cuFFT API Reference  This chapter specifies the behavior of the cuFFT library functions by describing their input/output parameters, data types, and error codes The cuFFT library is initialized upon the first invocation of an API function, and cuFFT shuts down automatically when all user-created FFT plans are destroyed"
  },
  {
    "id": 29645,
    "content": "Return value cufftResult  All cuFFT Library return values except for CUFFT_SUCCESS indicate that the current API call failed and the user should reconfigure to correct the problem } cufftResult ; Users are encouraged to check return values from cuFFT functions for errors as shown in cuFFT Code Examples"
  },
  {
    "id": 29651,
    "content": "cufftPlan1d()  cufftResult cufftPlan1d ( cufftHandle * plan , int nx , cufftType type , int batch ) ;  Creates a 1D FFT plan configuration for a specified signal size and data type type[In] – The transform data type (e"
  },
  {
    "id": 29658,
    "content": "cufftPlan2d()  cufftResult cufftPlan2d ( cufftHandle * plan , int nx , int ny , cufftType type ) ;  Creates a 2D FFT plan configuration according to specified signal sizes and data type"
  },
  {
    "id": 29659,
    "content": "nx[In] – The transform size in the x dimension This is slowest changing dimension of a transform (strided in memory) type[In] – The transform data type (e"
  },
  {
    "id": 29666,
    "content": "cufftPlan3d()  cufftResult cufftPlan3d ( cufftHandle * plan , int nx , int ny , int nz , cufftType type ) ;  Creates a 3D FFT plan configuration according to specified signal sizes and data type"
  },
  {
    "id": 29675,
    "content": "cufftPlanMany()  cufftResult cufftPlanMany ( cufftHandle * plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch ) ;  Creates a FFT plan configuration of dimension rank , with sizes specified in the array n The cufftPlanMany() API supports more complicated input and output data layouts via the advanced data"
  },
  {
    "id": 29676,
    "content": "layout parameters: inembed , istride , idist , onembed , ostride , and odist If inembed and onembed are set to NULL , all other stride information is ignored, and default strides are used Please note that behavior of cufftPlanMany function when inembed and onembed is NULL is different than corresponding function in FFTW library fftw_plan_many_dft n[In] – Array of size rank , describing the size of"
  },
  {
    "id": 29677,
    "content": "each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory istride[In] – Indicates the distance between two successive input elements in the least significant (i"
  },
  {
    "id": 29680,
    "content": "idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory ostride[In] – Indicates the distance between two successive output elements in the output array in the least significant (i"
  },
  {
    "id": 29683,
    "content": "odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data"
  },
  {
    "id": 29687,
    "content": "cuFFT Extensible Plans  This API separates handle creation from plan generation This makes it possible to change plan settings, which may alter the outcome of the plan generation phase, before the plan is actually generated"
  },
  {
    "id": 29691,
    "content": "cufftCreate()  cufftResult cufftCreate ( cufftHandle * plan )  Creates only an opaque handle, and allocates small data structures on the host"
  },
  {
    "id": 29696,
    "content": "cufftDestroy()  cufftResult cufftDestroy ( cufftHandle plan )  Frees all GPU resources associated with a cuFFT plan and destroys the internal plan data structure This function should be called once a plan is no longer needed, to avoid wasting GPU memory Return values CUFFT_SUCCESS – cuFFT successfully destroyed the FFT plan"
  },
  {
    "id": 29700,
    "content": "cufftMakePlan1d()  cufftResult cufftMakePlan1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a 1D FFT plan configuration for a specified signal size and data type"
  },
  {
    "id": 29701,
    "content": "If cufftXtSetGPUs() was called prior to this call with multiple GPUs, then workSize will contain multiple sizes"
  },
  {
    "id": 29709,
    "content": "cufftMakePlan2d()  cufftResult cufftMakePlan2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 2D FFT plan configuration according to specified signal sizes and data type type[In] – The transform data type (e"
  },
  {
    "id": 29716,
    "content": "cufftMakePlan3d()  cufftResult cufftMakePlan3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  Following a call to cufftCreate() makes a 3D FFT plan configuration according to specified signal sizes and data type type[In] – The transform data type (e"
  },
  {
    "id": 29723,
    "content": "cufftMakePlanMany()  cufftResult cufftMakePlanMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank , with sizes specified in the array n rank[In] – Dimensionality of the transform (1,"
  },
  {
    "id": 29724,
    "content": "2, or 3) n[In] – Array of size rank , describing the size of each dimension, n[0] being the size of the outermost and n[rank-1] innermost (contiguous) dimension of a transform For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127 inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory, inembed[0]"
  },
  {
    "id": 29725,
    "content": "being the storage dimension of the outermost dimension istride[In] – Indicates the distance between two successive input elements in the least significant (i"
  },
  {
    "id": 29727,
    "content": ", innermost) dimension idist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the input data onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension ostride[In] – Indicates the distance between two successive output elements in the output"
  },
  {
    "id": 29730,
    "content": ", innermost) dimension odist[In] – Indicates the distance between the first element of two consecutive signals in a batch of the output data type[In] – The transform data type (e"
  },
  {
    "id": 29732,
    "content": ", CUFFT_R2C for single precision real to complex) For 2 GPUs this must be a complex to complex transform"
  },
  {
    "id": 29736,
    "content": "cufftMakePlanMany64()  cufftResult cufftMakePlanMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  Following a call to cufftCreate() makes a FFT plan configuration of dimension rank ,"
  },
  {
    "id": 29738,
    "content": "This API is identical to cufftMakePlanMany except that the arguments specifying sizes and strides are 64 bit integers cuFFT planning selects 32 bit kernels whenever possible to avoid any overhead due to 64 bit arithmetic"
  },
  {
    "id": 29739,
    "content": "For transforms whose size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127 For real to complex and complex to real transforms whose size exceeds 4G elements, the fastest changing dimension must be even"
  },
  {
    "id": 29740,
    "content": "The cufftPlanMany64() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist"
  },
  {
    "id": 29741,
    "content": "For multiple GPUs and rank equal to 2 or 3, the sizes must be factorable into primes less than or equal to 127"
  },
  {
    "id": 29742,
    "content": "inembed[In] – Pointer of size rank that indicates the storage dimensions of the input data in memory"
  },
  {
    "id": 29746,
    "content": "cufftXtMakePlanMany()  cufftResult cufftXtMakePlanMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  Following a call to"
  },
  {
    "id": 29747,
    "content": "cufftCreate() makes an FFT plan configuration of dimension rank , with sizes specified in the array n"
  },
  {
    "id": 29748,
    "content": "Type specifiers inputtype , outputtype and executiontype dictate type and precision of transform to be performed Parameters inputtype and outputtype need to match transform type complex-to-complex, real-to-complex or complex-to-real Example: for a half-precision real-to-complex transform, parameters inputtype , outputtype and executiontype would have values of CUDA_R_16F , CUDA_C_16F and"
  },
  {
    "id": 29749,
    "content": "CUDA_C_16F respectively Similarly, a bfloat16 complex-to-real transform would use CUDA_C_16BF for inputtype and executiontype , and CUDA_R_16BF for outputtype"
  },
  {
    "id": 29750,
    "content": "The cufftXtMakePlanMany() API supports more complicated input and output data layouts via the advanced data layout parameters: inembed , istride , idist , onembed , ostride , and odist onembed[In] – Pointer of size rank that indicates the storage dimensions of the output data in memory, onembed[0] being the storage dimension of the outermost dimension"
  },
  {
    "id": 29754,
    "content": "cuFFT Plan Properties  Users can further customize cuFFT plans using plan properties These properties can be set, queried and reset on a per-plan basis as needed, using the routines listed in this section The current supported properties are listed below: Property Underlying Type Description Behavior NVFFT_PLAN_PROPERTY_INT64_PATIENT_JIT long long int Runtime LTO kernels are enabled when set to"
  },
  {
    "id": 29755,
    "content": "not-zero value See Link-Time Optimized Kernels Runtime LTO kernles are disabled when set to zero (default) Can be set / reset before planning Cannot be set / reset after planning 3"
  },
  {
    "id": 29758,
    "content": "cufftSetPlanPropertyInt64()  cufftResult cufftSetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , const long long int propertyValueInt64 ) ;  Associates a cuFFT plan with a property identified by the key property The value for the property is given by value propertyValueInt64 , which is a signed long long integer"
  },
  {
    "id": 29761,
    "content": "some properties cannot be set after calling a planning routine for the plan, see cuFFT Plan Properties ) cufftGetPlanPropertyInt64()  cufftResult cufftGetPlanPropertyInt64 ( cufftHandle plan , cufftProperty property , long long int * propertyValueInt64 ) ;  Retrieves the property value identified by the key property associated with the cuFFT plan plan The value for the property, which is a"
  },
  {
    "id": 29762,
    "content": "signed long long integer, is set in the address space pointed by propertyValueInt64 cufftResetPlanProperty()  cufftResult cufftResetPlanProperty ( cufftHandle plan , cufftProperty property ) ;  Resets the value of the property identified by the key property , associated with the cuFFT plan plan , to its default value CUFFT_NOT_SUPPORTED – The property is not supported for plan , or cannot be"
  },
  {
    "id": 29763,
    "content": "reset at present time (see Behavior column on cuFFT Plan Properties ) cuFFT Estimated Size of Work Area  During plan execution, cuFFT requires a work area for temporary storage of intermediate results The cufftEstimate*() calls return an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings"
  },
  {
    "id": 29764,
    "content": "Large prime numbers, however, use different algorithms and may need up to the eight times that of a similarly sized power of 2"
  },
  {
    "id": 29765,
    "content": "These routines return estimated workSize values which may still be smaller than the actual values needed especially for values of n that are not multiples of powers of 2, 3, 5 and 7 More refined values are given by the cufftGetSize*() routines, but these values may still be conservative"
  },
  {
    "id": 29769,
    "content": "cufftEstimate1d()  cufftResult cufftEstimate1d ( int nx , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results This call returns an estimate for the size of the work area required, given the specified parameters, and assuming default plan settings"
  },
  {
    "id": 29774,
    "content": "cufftEstimate2d()  cufftResult cufftEstimate2d ( int nx , int ny , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results"
  },
  {
    "id": 29779,
    "content": "cufftEstimate3d()  cufftResult cufftEstimate3d ( int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results"
  },
  {
    "id": 29784,
    "content": "cufftEstimateMany()  cufftResult cufftEstimateMany ( int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  During plan execution, cuFFT requires a work area for temporary storage of intermediate results The cufftEstimateMany() API supports more complicated input and output data layouts via the"
  },
  {
    "id": 29786,
    "content": "*workSize[Out] – Pointer to the size of the work space Return values CUFFT_SUCCESS – cuFFT successfully returned the size of the work space"
  },
  {
    "id": 29789,
    "content": "cuFFT Refined Estimated Size of Work Area  The cufftGetSize*() routines give a more accurate estimate of the work area size required for a plan than the cufftEstimate*() routines as they take into account any plan settings that may have been made As discussed in the section cuFFT Estimated Size of Work Area , the workSize value(s) returned may be conservative especially for values of n that are"
  },
  {
    "id": 29794,
    "content": "cufftGetSize1d()  cufftResult cufftGetSize1d ( cufftHandle plan , int nx , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate1d() , given the specified parameters, and taking into account any plan settings that may have been made"
  },
  {
    "id": 29799,
    "content": "cufftGetSize2d()  cufftResult cufftGetSize2d ( cufftHandle plan , int nx , int ny , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate2d() , given the specified parameters, and taking into account any plan settings that may have been made"
  },
  {
    "id": 29804,
    "content": "cufftGetSize3d()  cufftResult cufftGetSize3d ( cufftHandle plan , int nx , int ny , int nz , cufftType type , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimate3d() , given the specified parameters, and taking into account any plan settings that may have been made"
  },
  {
    "id": 29808,
    "content": "cufftGetSizeMany()  cufftResult cufftGetSizeMany ( cufftHandle plan , int rank , int * n , int * inembed , int istride , int idist , int * onembed , int ostride , int odist , cufftType type , int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters, and taking into account"
  },
  {
    "id": 29813,
    "content": "cufftGetSizeMany64()  cufftResult cufftGetSizeMany64 ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , long long int * onembed , long long int ostride , long long int odist , cufftType type , long long int batch , size_t * workSize ) ;  This call gives a more accurate estimate of the work area size required for a plan"
  },
  {
    "id": 29814,
    "content": "than cufftEstimateSizeMany() , given the specified parameters, and taking into account any plan settings that may have been made"
  },
  {
    "id": 29815,
    "content": "For transforms whose total size exceeds 4G elements, the dimensions specified in the array n must be factorable into primes that are less than or equal to 127 For real to complex and complex to real transforms whose total size exceeds 4G elements, the fastest changing dimension must be even"
  },
  {
    "id": 29819,
    "content": "cufftXtGetSizeMany()  cufftResult cufftXtGetSizeMany ( cufftHandle plan , int rank , long long int * n , long long int * inembed , long long int istride , long long int idist , cudaDataType inputtype , long long int * onembed , long long int ostride , long long int odist , cudaDataType outputtype , long long int batch , size_t * workSize , cudaDataType executiontype ) ;  This call gives a more"
  },
  {
    "id": 29820,
    "content": "accurate estimate of the work area size required for a plan than cufftEstimateSizeMany() , given the specified parameters that match signature of cufftXtMakePlanMany function, and taking into account any plan settings that may have been made"
  },
  {
    "id": 29821,
    "content": "For more information about valid combinations of inputtype , outputtype and executiontype parameters please refer to documentation of cufftXtMakePlanMany function executiontype[In] ( cudaDataType ) – Type of data to be used for computations"
  },
  {
    "id": 29824,
    "content": "cufftGetSize()  cufftResult cufftGetSize ( cufftHandle plan , size_t * workSize ) ;  Once plan generation has been done, either with the original API or the extensible API, this call returns the actual size of the work area required to support the plan Callers who choose to manage work area allocation within their application must use this call after plan generation, and after any cufftSet*()"
  },
  {
    "id": 29831,
    "content": "cufftSetAutoAllocation()  cufftResult cufftSetAutoAllocation ( cufftHandle plan , int autoAllocate ) ;  cufftSetAutoAllocation() indicates that the caller intends to allocate and manage work areas for plans that have been generated If cufftSetAutoAllocation() has been called with autoAllocate set to 0 (“false”) prior to one of the cufftMakePlan*() calls, cuFFT does not allocate the work area"
  },
  {
    "id": 29836,
    "content": "cufftSetWorkArea()  cufftResult cufftSetWorkArea ( cufftHandle plan , void * workArea ) ;  cufftSetWorkArea() overrides the work area pointer associated with a plan The cufftExecute*() calls assume that the work area pointer is valid and that it points to a contiguous region in device memory that does not overlap with any other work area For multiple GPUs, multiple work area pointers must be"
  },
  {
    "id": 29841,
    "content": "cufftXtSetWorkAreaPolicy()  cufftResult cufftXtSetWorkAreaPolicy ( cufftHandle plan , cufftXtWorkAreaPolicy policy , size_t * workSize ) ;  cufftXtSetWorkAreaPolicy() indicates that the caller intends to change work area size for a given plan handle cuFFT’s default behavior is to allocate the work area at plan generation time with a default size that depends on the plan type and other"
  },
  {
    "id": 29842,
    "content": "parameters If cufftXtSetWorkAreaPolicy() has been called with the policy parameter set to CUFFT_WORKAREA_MINIMAL , cuFFT will attempt to re-plan the handle to use zero bytes of work area memory If the cufftXtSetWorkAreaPolicy() call is successful the auto-allocated work area memory is released"
  },
  {
    "id": 29843,
    "content": "Currently the policies CUFFT_WORKAREA_PERFORMANCE , CUFFT_WORKAREA_USER and the workSize parameter are not supported and reserved for use in future cuFFT releases"
  },
  {
    "id": 29850,
    "content": "cufftExecC2C() and cufftExecZ2Z()  cufftResult cufftExecC2C ( cufftHandle plan , cufftComplex * idata , cufftComplex * odata , int direction ) ;  cufftResult cufftExecZ2Z ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleComplex * odata , int direction ) ;  cufftExecC2C() ( cufftExecZ2Z() ) executes a single-precision (double-precision) complex-to-complex transform plan in the"
  },
  {
    "id": 29851,
    "content": "transform direction as specified by direction parameter CUFFT_INVALID_VALUE – At least one of the parameters idata , odata , and direction is not valid"
  },
  {
    "id": 29856,
    "content": "cufftExecR2C() and cufftExecD2Z()  cufftResult cufftExecR2C ( cufftHandle plan , cufftReal * idata , cufftComplex * odata ) ;  cufftResult cufftExecD2Z ( cufftHandle plan , cufftDoubleReal * idata , cufftDoubleComplex * odata ) ;  cufftExecR2C() ( cufftExecD2Z() ) executes a single-precision (double-precision) real-to-complex, implicitly forward, cuFFT transform plan Pointers to idata and"
  },
  {
    "id": 29857,
    "content": "odata are both required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex data type in double-precision transforms Note the data layout differences between in-place and out-of-place transforms as described in Parameter cufftType CUFFT_INVALID_VALUE – At least one of the parameters idata and odata is not valid"
  },
  {
    "id": 29861,
    "content": "cufftExecC2R() and cufftExecZ2D()  cufftResult cufftExecC2R ( cufftHandle plan , cufftComplex * idata , cufftReal * odata ) ;  cufftResult cufftExecZ2D ( cufftHandle plan , cufftDoubleComplex * idata , cufftDoubleReal * odata ) ;  cufftExecC2R() ( cufftExecZ2D() ) executes a single-precision (double-precision) complex-to-real, implicitly inverse, cuFFT transform plan and pointers are both"
  },
  {
    "id": 29862,
    "content": "required to be aligned to cufftComplex data type in single-precision transforms and cufftDoubleComplex type in double-precision transforms"
  },
  {
    "id": 29867,
    "content": "cufftXtExec()  cufftResult cufftXtExec ( cufftHandle plan , void * input , void * output , int direction ) ;  Function cufftXtExec executes any cuFFT transform regardless of precision and type"
  },
  {
    "id": 29872,
    "content": "cufftXtExecDescriptor()  cufftResult cufftXtExecDescriptor ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  Function cufftXtExecDescriptor() executes any cuFFT transform regardless of precision and type cuFFT uses the GPU memory pointed to by cudaLibXtDesc *input descriptor as input data and cudaLibXtDesc *output as output data"
  },
  {
    "id": 29879,
    "content": "cufftXtSetGPUs()  cufftResult cufftXtSetGPUs ( cufftHandle plan , int nGPUs , int * whichGPUs ) ;  cufftXtSetGPUs() identifies which GPUs are to be used with the plan As in the single GPU case cufftCreate() creates a plan and cufftMakePlan*() does the plan generation"
  },
  {
    "id": 29882,
    "content": "0, this call will return an error if a non-default stream has been associated with the plan Note that the call to cufftXtSetGPUs() must occur after the call to cufftCreate() and prior to the call to cufftMakePlan*()"
  },
  {
    "id": 29883,
    "content": "Parameter whichGPUs of cufftXtSetGPUs() function determines ordering of the GPUs with respect to data decomposition (first data chunk is placed on GPU denoted by first element of whichGPUs )"
  },
  {
    "id": 29884,
    "content": "CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or a non-default stream has been associated with the plan in cuFFT prior to 10"
  },
  {
    "id": 29891,
    "content": "cufftXtSetWorkArea()  cufftResult cufftXtSetWorkArea ( cufftHandle plan , void * * workArea ) ;  cufftXtSetWorkArea() overrides the work areas associated with a plan"
  },
  {
    "id": 29892,
    "content": "The cufftXtExec*() calls assume that the work area is valid and that it points to a contiguous region in each device memory that does not overlap with any other work area"
  },
  {
    "id": 29901,
    "content": "cufftXtExecDescriptorC2C() and cufftXtExecDescriptorZ2Z()  cufftResult cufftXtExecDescriptorC2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftResult cufftXtExecDescriptorZ2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output , int direction ) ;  cufftXtExecDescriptorC2C() ( cufftXtExecDescriptorZ2Z() ) executes a single-precision"
  },
  {
    "id": 29902,
    "content": "(double-precision) complex-to-complex transform plan in the transform direction as specified by direction parameter Since only in-place multiple GPU functionality is supported, this function also stores the result in the cudaLibXtDesc *input arrays CUFFT_INVALID_VALUE – At least one of the parameters input and direction is not valid"
  },
  {
    "id": 29907,
    "content": "cufftXtExecDescriptorR2C() and cufftXtExecDescriptorD2Z()  cufftResult cufftXtExecDescriptorR2C ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorD2Z ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorR2C() ( cufftXtExecDescriptorD2Z() ) executes a single-precision (double-precision)"
  },
  {
    "id": 29909,
    "content": "input[Out] – Contains the complex Fourier coefficients Return values CUFFT_SUCCESS – cuFFT successfully executed the FFT plan"
  },
  {
    "id": 29914,
    "content": "cufftXtExecDescriptorC2R() and cufftXtExecDescriptorZ2D()  cufftResult cufftXtExecDescriptorC2R ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftResult cufftXtExecDescriptorZ2D ( cufftHandle plan , cudaLibXtDesc * input , cudaLibXtDesc * output ) ;  cufftXtExecDescriptorC2R() ( cufftXtExecDescriptorZ2D() ) executes a single-precision (double-precision)"
  },
  {
    "id": 29919,
    "content": "Memory Allocation and Data Movement Functions  Multiple GPU cuFFT execution functions assume a certain data layout in terms of what input data has been copied to which GPUs prior to execution, and what output data resides in which GPUs post execution"
  },
  {
    "id": 29925,
    "content": "cufftXtMalloc()  cufftResult cufftXtMalloc ( cufftHandle plan , cudaLibXtDesc * * descriptor , cufftXtSubFormat format ) ;  cufftXtMalloc() allocates a descriptor, and all memory for data in GPUs associated with the plan, and returns a pointer to the descriptor Note the descriptor contains an array of device pointers so that the application may preprocess or postprocess the data on the GPUs"
  },
  {
    "id": 29926,
    "content": "The enumerated parameter cufftXtSubFormat_t indicates if the buffer will be used for input or output"
  },
  {
    "id": 29935,
    "content": "Parameter cufftXtSubFormat  cufftXtSubFormat_t is an enumerated type that indicates if the buffer will be used for input or output and the ordering of the data typedef enum cufftXtSubFormat_t { CUFFT_XT_FORMAT_INPUT , by default input is in linear order across GPUs CUFFT_XT_FORMAT_OUTPUT , by default output is in scrambled order depending on transform CUFFT_XT_FORMAT_INPLACE , by default inplace"
  },
  {
    "id": 29936,
    "content": "is input order, which is linear across GPUs CUFFT_XT_FORMAT_INPLACE_SHUFFLED , shuffled output order after execution of the transform CUFFT_FORMAT_UNDEFINED } cufftXtSubFormat ; 3"
  },
  {
    "id": 29940,
    "content": "cufftXtFree()  cufftResult cufftXtFree ( cudaLibXtDesc * descriptor ) ;  cufftXtFree() frees the descriptor and all memory associated with it Return values CUFFT_SUCCESS – cuFFT successfully allows user to free descriptor and associated GPU memory"
  },
  {
    "id": 29945,
    "content": "cufftXtMemcpy()  cufftResult cufftXtMemcpy ( cufftHandle plan , void * dstPointer , void * srcPointer , cufftXtCopyType type ) ;  cufftXtMemcpy() copies data between buffers on the host and GPUs or between GPUs Calling cufftXtMemcpy function for multi-GPU batched FFT plans with CUFFT_COPY_DEVICE_TO_DEVICE transfer type is not supported"
  },
  {
    "id": 29951,
    "content": "Return values CUFFT_SUCCESS – cuFFT successfully allows user to copy memory between host and GPUs or between GPUs"
  },
  {
    "id": 29957,
    "content": "Parameter cufftXtCopyType  cufftXtCopyType_t is an enumerated type for multiple GPU functions that specifies the type of copy for cufftXtMemcpy()"
  },
  {
    "id": 29958,
    "content": "CUFFT_COPY_HOST_TO_DEVICE copies data from a contiguous host buffer to multiple device buffers, in the layout cuFFT requires for input data"
  },
  {
    "id": 29959,
    "content": "dstPointer must point to a cudaLibXtDesc structure, and srcPointer must point to a host memory buffer"
  },
  {
    "id": 29960,
    "content": "CUFFT_COPY_DEVICE_TO_HOST copies data from multiple device buffers, in the layout cuFFT produces for output data, to a contiguous host buffer"
  },
  {
    "id": 29961,
    "content": "dstPointer must point to a host memory buffer, and srcPointer must point to a cudaLibXtDesc structure"
  },
  {
    "id": 29962,
    "content": "CUFFT_COPY_DEVICE_TO_DEVICE copies data from multiple device buffers, in the layout cuFFT produces for output data, to multiple device buffers, in the layout cuFFT requires for input data"
  },
  {
    "id": 29963,
    "content": "dstPointer and srcPointer must point to different cudaLibXtDesc structures (and therefore memory locations)"
  },
  {
    "id": 29964,
    "content": "typedef enum cufftXtCopyType_t { CUFFT_COPY_HOST_TO_DEVICE , CUFFT_COPY_DEVICE_TO_HOST , CUFFT_COPY_DEVICE_TO_DEVICE } cufftXtCopyType ; 3"
  },
  {
    "id": 29967,
    "content": "cudaXtDesc  A descriptor type used in multiple GPU routines that contains information about the GPUs and their memory locations struct cudaXtDesc_t { int version ; descriptor version int nGPUs ; number of GPUs int GPUs [ MAX_CUDA_DESCRIPTOR_GPUS ]; array of device IDs void * data [ MAX_CUDA_DESCRIPTOR_GPUS ]; array of pointers to data, one per GPU size_t size [ MAX_CUDA_DESCRIPTOR_GPUS ]; array"
  },
  {
    "id": 29968,
    "content": "of data sizes, one per GPU void * cudaXtState ; opaque CUDA utility structure }; typedef struct cudaXtDesc_t cudaXtDesc ; 3"
  },
  {
    "id": 29972,
    "content": "cudaLibXtDesc  A descriptor type used in multiple GPU routines that contains information about the library used struct cudaLibXtDesc_t { int version ;  descriptor version cudaXtDesc * descriptor ;  multi-GPU memory descriptor libFormat library ;  which library recognizes the format int subFormat ;  library specific enumerator of sub formats void * libDescriptor ;  library specific descriptor e"
  },
  {
    "id": 29974,
    "content": "cufftXtSetCallback()  cufftResult cufftXtSetCallback ( cufftHandle plan , void * * callbackRoutine , cufftXtCallbackType type , void * * callerInfo )  cufftXtSetCallback() specifies a load or store callback to be used with the plan"
  },
  {
    "id": 29976,
    "content": "If there was already a callback of this type associated with the plan, this new callback routine replaces it If the new callback requires shared memory, you must call cufftXtSetCallbackSharedSize with the amount of shared memory it needs"
  },
  {
    "id": 29982,
    "content": "cufftXtClearCallback()  cufftResult cufftXtClearCallback ( cufftHandle plan , cufftXtCallbackType type )  cufftXtClearCallback() instructs cuFFT to stop invoking the specified callback type when executing the plan Return values CUFFT_SUCCESS – cuFFT successfully disassociated the callback function with the plan"
  },
  {
    "id": 29986,
    "content": "cufftXtSetCallbackSharedSize()  cufftResult cufftXtSetCallbackSharedSize ( cufftHandle plan , cufftXtCallbackType type , size_t sharedSize )  cufftXtSetCallbackSharedSize() instructs cuFFT to dynamically allocate shared memory at launch time, for use by the callback"
  },
  {
    "id": 29987,
    "content": "Return values CUFFT_SUCCESS – cuFFT will invoke the callback routine with a pointer to the requested amount of shared memory CUFFT_ALLOC_FAILED – cuFFT will not be able to allocate the requested amount of shared memory"
  },
  {
    "id": 29990,
    "content": "cufftSetStream()  cufftResult cufftSetStream ( cufftHandle plan , cudaStream_t stream ) ;  Associates a CUDA stream with a cuFFT plan All kernel launches made during plan execution are now done through the associated stream, enabling overlap with activity in other streams (e"
  },
  {
    "id": 29992,
    "content": "The association remains until the plan is destroyed or the stream is changed with another call to cufftSetStream() For previous versions of cuFFT, cufftSetStream() will return an error in multiple GPU plans"
  },
  {
    "id": 29997,
    "content": "However, repeated calls to cufftSetStream() with streams from different contexts incur a small time penalty Optimal performance is obtained when repeated calls to cufftSetStream use streams from the same CUDA context"
  },
  {
    "id": 29999,
    "content": "CUFFT_INVALID_PLAN – The plan parameter is not a valid handle, or plan is multi-gpu in cuFFT version prior to 10"
  },
  {
    "id": 30004,
    "content": "cufftGetVersion()  cufftResult cufftGetVersion ( int * version ) ;  Returns the version number of cuFFT Return values CUFFT_SUCCESS – cuFFT successfully returned the version number"
  },
  {
    "id": 30007,
    "content": "cufftGetProperty()  cufftResult cufftGetProperty ( libraryPropertyType type , int * value ) ;  Return in *value the number for the property described by type of the dynamically linked CUFFT library The cufftType data type is an enumeration of the types of transform data supported by cuFFT"
  },
  {
    "id": 30008,
    "content": "typedef enum cufftType_t { CUFFT_R2C = 0x2a ,   Real to complex (interleaved) CUFFT_C2R = 0x2c ,   Complex (interleaved) to real CUFFT_C2C = 0x29 ,   Complex to complex (interleaved) CUFFT_D2Z = 0x6a ,   Double to double-complex (interleaved) CUFFT_Z2D = 0x6c ,   Double-complex (interleaved) to double CUFFT_Z2Z = 0x69   Double-complex to double-complex (interleaved) } cufftType ; 3"
  },
  {
    "id": 30011,
    "content": "Parameters for Transform Direction  The cuFFT library defines forward and inverse Fast Fourier Transforms according to the sign of the complex exponential term"
  },
  {
    "id": 30012,
    "content": "#define cuFFTFORWARD -1 #define cuFFTINVERSE 1 cuFFT performs un-normalized FFTs; that is, performing a forward FFT on an input data set followed by an inverse FFT on the resulting set yields data that is equal to the input, scaled by the number of elements Scaling either transform by the reciprocal of the size of the data set is left for the user to perform as seen fit"
  },
  {
    "id": 30016,
    "content": "Type definitions for callbacks  The cuFFT library supports callback funtions for all combinations of single or double precision, real or complex data, load or store"
  },
  {
    "id": 30018,
    "content": "cufftComplex  A single-precision, floating-point complex data type that consists of interleaved real and imaginary components cufftDoubleComplex  A double-precision, floating-point complex data type that consists of interleaved real and imaginary components"
  },
  {
    "id": 30019,
    "content": "cudaDataType  The cudaDataType data type is an enumeration of the types supported by CUDA libraries"
  },
  {
    "id": 30020,
    "content": "typedef enum cudaDataType_t { CUDA_R_16F = 2 , 16 bit real CUDA_C_16F = 6 , 16 bit complex CUDA_R_32F = 0 , 32 bit real CUDA_C_32F = 4 , 32 bit complex CUDA_R_64F = 1 , 64 bit real CUDA_C_64F = 5 , 64 bit complex CUDA_R_8I = 3 , 8 bit real as a signed integer CUDA_C_8I = 7 , 8 bit complex as a pair of signed integers CUDA_R_8U = 8 , 8 bit real as an unsigned integer CUDA_C_8U = 9 8 bit complex as"
  },
  {
    "id": 30027,
    "content": "Z would yield MAJOR_VERSION=X , MINOR_VERSION=Y , PATCH_LEVEL=Z ) typedef enum libraryPropertyType_t { MAJOR_VERSION , MINOR_VERSION , PATCH_LEVEL } libraryPropertyType ; 4"
  },
  {
    "id": 30028,
    "content": "cuFFT Code Examples  For simple examples of complex and real 1D, 2D, and 3D transforms that use cuFFT to perform forward and inverse FFTs, refer to the cuFFT Library samples on GitHub"
  },
  {
    "id": 30030,
    "content": "Multiple GPU Data Organization  This chapter explains how data are distributed between the GPUs, before and after a multiple GPU transform For simplicity, it is assumed in this chapter that the caller has specified GPU 0 and GPU 1 to perform the transform"
  },
  {
    "id": 30033,
    "content": "Multiple GPU Data Organization for Batched Transforms  For batches of transforms, each individual transform is executed on a single GPU For a batch of size m performed on n GPUs, where m is not divisible by n , the first m % n GPUs will perform \\(\\left\\lfloor \\frac{m}{n} ight floor+\\ 1\\) transforms For example, in a batch of 15 transforms performed on 4 GPUs, the first three GPUs would perform 4"
  },
  {
    "id": 30034,
    "content": "transforms, and the last GPU would perform 3 transforms This approach removes the need for data exchange between the GPUs, and results in nearly perfect scaling for cases where the batch size is divisible by the number of GPUs"
  },
  {
    "id": 30037,
    "content": "Multiple GPU Data Organization for Single 2D and 3D Transforms  Single transforms performed on multiple GPUs require the data to be divided between the GPUs For example with 2 GPUs, for 2D and 3D transforms with even sized dimensions, each GPU does half of the transform in (rank - 1) dimensions Since 2D and 3D transforms support sizes other than powers of 2, it is possible that the data can not"
  },
  {
    "id": 30038,
    "content": "be evenly distributed among the GPUs In general for the case of n GPUs, a dimension of size m that is not a multiple of n would be distributed such that the first m % n GPUs would get one extra row for 2D transforms, one extra plane for 3D transforms Take for example, a 2D transform on 4 GPUs, using an array declared in C as data[x][y] , where x is 65 and y is 99 The surface is distributed prior"
  },
  {
    "id": 30039,
    "content": "to the transform such that GPU 0 receives a surface with dimensions [17][99] , and GPUs 1…3 receive surfaces with dimensions [16][99] After the transform, each GPU again has a portion of the surface, but divided in the y dimension GPU 3 has a surface with dimensions [65][24] For a 3D transform on 4 GPUs consider an array declared in C as data[x][y][z] , where x is 103, y is 122, and z is 64 The"
  },
  {
    "id": 30040,
    "content": "volume is distributed prior to the transform such that each GPUs 0…2 receive volumes with dimensions [26][122][64] , and GPU 3 receives a volume with dimensions [25][122][64] GPUs 0 and 1 have a volumes with dimensions [103][31][64] , and GPUs 2 and 3 have volumes with dimensions [103][30][64]"
  },
  {
    "id": 30043,
    "content": "Multiple-GPU Data Organization for Single 1D Transforms  By default for 1D transforms, the initial distribution of data to the GPUs is similar to the 2D and 3D cases It is possible to perform this redistribution in the copy from host memory, in cases where the application does not need to pre-process the data prior to the transform To do this, the application can create the data descriptor with"
  },
  {
    "id": 30044,
    "content": "cufftXtMalloc using the sub-format CUFFT_XT_FORMAT_1D_INPUT_SHUFFLED cuFFT performs multiple GPU 1D transforms by decomposing the transform size into factors Factor1 and Factor2 , and treating the data as a grid of size Factor1 x Factor2 The four steps done to calculate the 1D FFT are: Factor1 transforms of size Factor2 , data exchange between the GPUs, a pointwise twiddle multiplication, and"
  },
  {
    "id": 30046,
    "content": "To gain efficiency by overlapping computation with data exchange, cuFFT breaks the whole transform into independent segments or strings, which can be processed while others are in flight"
  },
  {
    "id": 30047,
    "content": "A side effect of this algorithm is that the output of the transform is not in linear order The output in GPU memory is in strings, each of which is composed of Factor2 substrings of equal size"
  },
  {
    "id": 30048,
    "content": "Each substring contains contiguous results starting Factor1 elements subsequent to start of the previous substring See the example below: transform size = 1024 number of strings = 8 Factor1 = 64 Factor2 = 16 substrings per string for output layout is Factor2 ( 16 ) string size = 1024 / 8 = 128 substring size = 128 / 16 = 8 stride between substrings = 1024 / 16 = Factor1 ( 64 ) On GPU 0 : string 0"
  },
  {
    "id": 30064,
    "content": "1023 The cufftXtQueryPlan API allows the caller to retrieve a structure containing the number of strings, the decomposition factors, and (in the case of power of 2 size) some useful mask and shift elements"
  },
  {
    "id": 30065,
    "content": "It also shows how to translate from an index in the host input array to the corresponding index on the device, and vice versa"
  },
  {
    "id": 30066,
    "content": "/* * These routines demonstrate the use of cufftXtQueryPlan to get the 1D * factorization and convert between permuted and linear indexes factor2 ); cufftDestroy ( plan ); return 0 ; } /* * Given an index into a permuted array, and the GPU index return the * corresponding linear index from the beginning of the input buffer"
  },
  {
    "id": 30067,
    "content": "whichString = ( linearIx >> factors -> substringShift ) & whichStringMask ; the first stringCount/2 strings are in the first GPU, the rest are in the second * GPUIx = whichString / ( factors -> stringCount / 2 ); next determine which substring within the string has our index the substring index is in the next higher order bits of the index whichSubstring = ( linearIx >> ( factors ->"
  },
  {
    "id": 30068,
    "content": "substringShift + whichStringShift )) & factors -> factor2Mask ; now we can re-assemble the index * permutedIx = indexInSubstring ; * permutedIx += whichSubstring substringShift ; if ( * GPUIx ) { * permutedIx += whichString stringShift ; } else { * permutedIx += ( whichString - ( factors -> stringCount / 2 ) ) stringShift ; } return CUFFT_SUCCESS ; } 6"
  },
  {
    "id": 30069,
    "content": "FFTW Conversion Guide  cuFFT differs from FFTW in that FFTW has many plans and a single execute function while cuFFT has fewer plans, but multiple execute functions The cuFFT execute functions determine the precision (single or double) and whether the input is complex or real valued"
  },
  {
    "id": 30070,
    "content": "FFTW function cuFFT function fftw_plan_dft_1d(), fftw_plan_dft_r2c_1d(), fftw_plan_dft_c2r_1d() cufftPlan1d() fftw_plan_dft_2d(), fftw_plan_dft_r2c_2d(), fftw_plan_dft_c2r_2d() cufftPlan2d() fftw_plan_dft_3d(), fftw_plan_dft_r2c_3d(), fftw_plan_dft_c2r_3d() cufftPlan3d() fftw_plan_dft(), fftw_plan_dft_r2c(), fftw_plan_dft_c2r() cufftPlanMany() fftw_plan_many_dft(), fftw_plan_many_dft_r2c(),"
  },
  {
    "id": 30071,
    "content": "fftw_plan_many_dft_c2r() cufftPlanMany() fftw_execute() cufftExecC2C(), cufftExecZ2Z(), cufftExecR2C(), cufftExecD2Z(), cufftExecC2R(), cufftExecZ2D() fftw_destroy_plan() cufftDestroy() 7"
  },
  {
    "id": 30072,
    "content": "This allows applications using FFTW to use NVIDIA GPUs with minimal modifications to program source code"
  },
  {
    "id": 30073,
    "content": "To use the interface first do the following two steps It is recommended that you replace the include file fftw3"
  },
  {
    "id": 30074,
    "content": "h with cufftw h Instead of linking with the double/single precision libraries such as fftw3/fftw3f libraries, link with both the cuFFT and cuFFTW libraries Ensure the search path includes the directory containing cuda_runtime_api"
  },
  {
    "id": 30075,
    "content": "h After an application is working using the FFTW3 interface, users may want to modify their code to move data to and from the GPU and use the routines documented in the FFTW Conversion Guide for the best performance Section in FFTW manual Supported Unsupported Complex numbers fftw_complex, fftwf_complex types Precision double fftw3 , single fftwf3 long double fftw3l , quad precision fftw3q are"
  },
  {
    "id": 30076,
    "content": "not supported since CUDA functions operate on double and single precision floating-point quantities Memory Allocation fftw_malloc(), fftw_free(), fftw_alloc_real(), fftw_alloc_complex(), fftwf_alloc_real(), fftwf_alloc_complex() Multi-threaded FFTW fftw3_threads, fftw3_omp are not supported Distributed-memory FFTW with MPI fftw3_mpi,fftw3f_mpi are not supported Note that for each of the double"
  },
  {
    "id": 30077,
    "content": "precision functions below there is a corresponding single precision version with the letters fftw replaced by fftwf"
  },
  {
    "id": 30083,
    "content": "8: CUDA Graphs capture is no longer supported for callback routines that load data in out-of-place mode transforms"
  },
  {
    "id": 30085,
    "content": "4: Support for callback functionality using separately compiled device code is deprecated on all GPU architectures Support for GPU architectures SM35, SM37 (Kepler), and SM50, SM52 (Maxwell) is deprecated"
  },
  {
    "id": 30086,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 30087,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 30089,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 30090,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 30091,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 30092,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 30093,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 30094,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 30095,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 30096,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 30097,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 30098,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 30099,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 30106,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 30108,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 30109,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2007-2024, NVIDIA Corporation & affiliates"
  },
  {
    "id": 30114,
    "content": "nvJPEG Decoder  The nvJPEG library provides high-performance, GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications The library offers single and batched JPEG decoding capabilities which efficiently utilize the available GPU resources for optimum performance; and the flexibility for users to manage the memory"
  },
  {
    "id": 30115,
    "content": "allocation needed for decoding The nvJPEG library enables the following functions: use the JPEG image data stream as input; retrieve the width and height of the image from the data stream, and use this retrieved information to manage the GPU memory allocation and the decoding A dedicated API is provided for retrieving the image information from the raw JPEG image data stream The nvJPEG library"
  },
  {
    "id": 30116,
    "content": "supports the following: JPEG options: Baseline and Progressive JPEG decoding/encoding 8 bits per pixel Huffman bitstream decoding Upto 4 channel JPEG bitstreams 8- and 16-bit quantization tables The following chroma subsampling for the 3 color channels Y, Cb, Cr (Y, U, V): 4:4:4 4:2:2 4:2:0 4:4:0 4:1:1 4:1:0 Features: Hybrid decoding using both the CPU (i"
  },
  {
    "id": 30123,
    "content": "nvJPEG Encoder  The encoding functions of the nvJPEG library perform GPU-accelerated compression of user’s image data to the JPEG bitstream User can provide input data in a number of formats and colorspaces, and control the encoding process with parameters Encoding functionality will allocate temporary buffers using user-provided memory allocator Before calling the encoding functions the user"
  },
  {
    "id": 30124,
    "content": "should perform a few prerequisite steps using the helper functions described in nvJPEG Encoder Helper API Reference"
  },
  {
    "id": 30128,
    "content": "When using decoder APIs across multiple threads, the following decoder types should be instantiated separately for each thread: nvjpegJpegStream_t , nvjpegJpegState_t , nvjpegBufferDevice_t , nvjpegBufferPinned_t When using encoder APIs across multiple threads, nvjpegEncoderState_t should be instantiated separately for each thread"
  },
  {
    "id": 30132,
    "content": "Multi-GPU support  The nvJPEG states and handles are bound to the device that was set as current during their creation"
  },
  {
    "id": 30136,
    "content": "Hardware Acceleration  Hardware accelerated JPEG decode is available on the following GPUs - A100, A30, H100 Platforms which support hardware accelerated JPEG decode: Windows Linux (x86_64, PowerPC, ARM64) 2"
  },
  {
    "id": 30137,
    "content": "Using JPEG Decoding  ​The nvJPEG library provides functions for both the decoding of a single image, and batched decoding of multiple images"
  },
  {
    "id": 30141,
    "content": "Single Image Decoding  For single-image decoding you provide the data size and a pointer to the file data, and the decoded image is placed in the output buffer"
  },
  {
    "id": 30142,
    "content": "Create nvJPEG library handle with one of the helper functions nvjpegCreateSimple() or nvjpegCreateEx() The following helper functions are available in the nvJPEG library: nvjpegStatus_t nvjpegGetProperty(libraryPropertyType type, int *value); [DEPRECATED] nvjpegStatus_t nvjpegCreate(nvjpegBackend_t backend, nvjpegHandle_t *handle , nvjpeg_dev_allocator allocator); nvjpegStatus_t"
  },
  {
    "id": 30143,
    "content": "nvjpegCreateSimple(nvjpegHandle_t *handle); nvjpegStatus_t nvjpegCreateEx(nvjpegBackend_t backend, nvjpegDevAllocator_t *dev_allocator, nvjpegPinnedAllocator_t *pinned_allocator, unsigned int flags, nvjpegHandle_t *handle); nvjpegStatus_t nvjpegDestroy(nvjpegHandle_t handle); nvjpegStatus_t nvjpegJpegStateCreate(nvjpegHandle_t handle, nvjpegJpegState_t *jpeg_handle); nvjpegStatus_t"
  },
  {
    "id": 30144,
    "content": "nvjpegJpegStateDestroy(nvjpegJpegState handle); Other helper functions such as nvjpegSet*() and nvjpegGet*() can be used to configure the library functionality on per-handle basis"
  },
  {
    "id": 30145,
    "content": "Retrieve the width and height information from the JPEG-encoded image by using the nvjpegGetImageInfo() function Below is the signature of nvjpegGetImageInfo() function: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); For each image to be decoded,"
  },
  {
    "id": 30146,
    "content": "pass the JPEG data pointer and data length to the above function One of the outputs of the above nvjpegGetImageInfo() function is nvjpegChromaSubsampling_t This parameter is an enum type, and its enumerator list is composed of the chroma subsampling property retrieved from the JPEG image See the signature of this function below: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle ,"
  },
  {
    "id": 30147,
    "content": "nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); In the above nvjpegDecode() function, the parameters nvjpegOutputFormat_t , nvjpegImage_t , and cudaStream_t can be used to set the output behavior of the nvjpegDecode() function You provide the cudaStream_t parameter to indicate the"
  },
  {
    "id": 30148,
    "content": "stream to which your asynchronous tasks are submitted The ``nvjpegOutputFormat_t`` parameter: The nvjpegOutputFormat_t parameter can be set to one of the output_format settings below: output_format Meaning NVJPEG_OUTPUT_UNCHANGED Return the decoded image planar format For example, if output_format is set to NVJPEG_OUTPUT_Y or NVJPEG_OUTPUT_RGBI , or NVJPEG_OUTPUT_BGRI then the output is written"
  },
  {
    "id": 30149,
    "content": "only to channel[0] of nvjpegImage_t , and the other channels are not touched Alternately, in the case of planar output, the data is written to the corresponding channels of the nvjpegImage_t destination structure Finally, in the case of grayscale JPEG and RGB output, the luminance is used to create the grayscale RGB The below table explains the combinations of the output formats and the number of"
  },
  {
    "id": 30151,
    "content": "No of Channels in bitstream 1 2 3 4 Output Format NVJPEG_OUTPUT_UNCHANGED Yes Yes Yes Yes NVJPEG_OUTPUT_YUV Only the first channel of the output is populated No Yes No NVJPEG_OUTPUT_Y Yes No Yes Yes (a) NVJPEG_OUTPUT_RGB Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGR Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_RGBI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_BGRI Yes (b) No Yes Yes (a) NVJPEG_OUTPUT_UNCHANGEDI_U16"
  },
  {
    "id": 30153,
    "content": "As mentioned above, an important benefit of the nvjpegGetImageInfo() function is the ability to utilize the image information retrieved from the the input JPEG image to allocate proper GPU memory for your decoding operation The nvjpegGetImageInfo() function returns the widths , heights and nComponents parameters nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char *"
  },
  {
    "id": 30154,
    "content": "data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); You can use the retrieved parameters, widths , heights and nComponents , to calculate the required size for the output buffers, either for a single decoded JPEG, or for every decoded JPEG in a batch"
  },
  {
    "id": 30155,
    "content": "The nvjpegImage_t structure that holds the output pointers is defined as follows: typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; NVJPEG_MAX_COMPONENT is the maximum number of color components the nvJPEG library supports in the current release For generic images, this is the maximum number of encoded channels that the"
  },
  {
    "id": 30157,
    "content": "Finally, when you call the nvjpegDecode() function with the parameters as described above, the nvjpegDecode() function fills the output buffers with the decoded data"
  },
  {
    "id": 30161,
    "content": "Decode using Decoupled Phases  The nvJPEG library allows further separation of the host and device phases of the decode process Below is the sequence of API calls to decode a single image Initialize all the items that are used in the decoding process: Create the library handle using one of the library handle initialization routines"
  },
  {
    "id": 30162,
    "content": "Choose decoder implementation nvjpegBackend_t , and create decoder using nvjpegDecoderCreate() Create the pinned and device buffers used by the decoder using the below APIs respectively nvjpegBufferPinnedCreate() nvjpegBufferDeviceCreate() Link the buffers to the JPEG state using the following APIs respectively: nvjpegStateAttachPinnedBuffer() nvjpegStateAttachDeviceBuffer() Create decode"
  },
  {
    "id": 30163,
    "content": "parameters using the below API This is used to set the output format, and enable ROI decode: nvjpegDecodeParamsCreate() Perform decoding: Parse the jpeg bit-stream using nvjpegJpegStreamParse() Encoded bitstream information, like channel dimensions, can be retrieved using the below API nvjpegJpegStreamGetComponentsNum() nvjpegJpegStreamGetComponentDimensions() Call the decode API in the below"
  },
  {
    "id": 30164,
    "content": "sequence to decode the image: nvjpegDecodeJpegHost() nvjpegDecodeJpegTransferToDevice() nvjpegDecodeJpegDevice() 2"
  },
  {
    "id": 30167,
    "content": "Batched Image Decoding  For the batched image decoding you provide pointers to multiple file data in the memory, and also provide the buffer sizes for each file data The nvJPEG library will decode these multiple images, and will place the decoded data in the output buffers that you specified in the parameters"
  },
  {
    "id": 30172,
    "content": "Single Phase  For batched image decoding in single phase, follow these steps: Call nvjpegDecodeBatchedInitialize() function to initialize the batched decoder If the size of the batch changes, or if the batch decoding fails, then call the nvjpegDecodeBatchedInitialize() function again"
  },
  {
    "id": 30178,
    "content": "nvJPEG Backend  typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , NVJPEG_BACKEND_GPU_HYBRID = 2 , NVJPEG_BACKEND_HARDWARE = 3 , NVJPEG_BACKEND_GPU_HYBRID_DEVICE = 4 , NVJPEG_BACKEND_HARDWARE_DEVICE = 5 , NVJPEG_BACKEND_LOSSLESS_JPEG = 6 } nvjpegBackend_t ; The nvjpegBackend_t enum is used to select either default back-end by default, or use GPU decoding for baseline JPEG"
  },
  {
    "id": 30179,
    "content": "images, or use CPU for Huffman decoding nvjpegDecodeBatched will use GPU decoding for baseline JPEG images with interleaved scan when batch size is greater than 50 Can be used only with batched decode APIs for baseline JPEG images without restart intervals"
  },
  {
    "id": 30180,
    "content": "NVJPEG_BACKEND_LOSSLESS_JPEG Supports lossless jpeg bitstreams as defined in the jpeg 92 standard Bitstreams with up to 2 channels and prediction mode 1 are supported"
  },
  {
    "id": 30184,
    "content": "nvJPEG Bitstream Handle  struct nvjpegJpegStream ; typedef struct nvjpegJpegStream * nvjpegJpegStream_t ; This handle stores the bit-stream parameters on the host This helps retrieve bitstream meta-data using APIs defined in nvJPEG Stream API"
  },
  {
    "id": 30188,
    "content": "nvJPEG Decode Device Buffer Handle  struct nvjpegBufferDevice ; typedef struct nvjpegBufferDevice * nvjpegBufferDevice_t ; This nvjpegBufferDevice_t is used by decoder states to store the intermediate information in device memory"
  },
  {
    "id": 30192,
    "content": "nvJPEG Decode Parameter Handle  struct nvjpegDecodeParams ; typedef struct nvjpegDecodeParams * nvjpegDecodeParams_t ; This decoder parameter handle stores the parameters like output format, and the ROI decode parameters that are set using APIs defined in nvJPEG Chroma Subsampling"
  },
  {
    "id": 30196,
    "content": "nvJPEG Decode Pinned Buffer Handle  struct nvjpegBufferPinned ; typedef struct nvjpegBufferPinned * nvjpegBufferPinned_t ; This nvjpegBufferPinned_t handle is used by decoder states to store the intermediate information on pinned memory"
  },
  {
    "id": 30200,
    "content": "nvJPEG Decoder Handle  struct nvjpegJpegDecoder ; typedef struct nvjpegJpegDecoder * nvjpegJpegDecoder_t ; This decoder handle stores the intermediate decoder data, which is shared across the decoding stages"
  },
  {
    "id": 30205,
    "content": "nvJPEG Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMalloc )( void ** , size_t , unsigned int flags ); typedef int ( * tPinnedFree )( void * ); typedef struct { tPinnedMalloc pinned_malloc ; tPinnedFree pinned_free ; } nvjpegPinnedAllocator_t ; When the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set as a pointer to the above"
  },
  {
    "id": 30206,
    "content": "nvjpegPinnedAllocator_t structure, then this structure will be used for allocating and releasing host pinned memory for copying data to/from device"
  },
  {
    "id": 30207,
    "content": "The function prototypes for the memory allocation and memory freeing functions are similar to the cudaHostAlloc() and cudaFreeHost() functions However, if the nvjpegPinnedAllocator_t *allocator parameter in the nvjpegCreateEx() function is set to NULL, then the default memory allocation functions cudaHostAlloc() and cudaFreeHost() will be used When using nvjpegCreate() or nvjpegCreateSimple()"
  },
  {
    "id": 30212,
    "content": "nvJPEG Extended Host Pinned Memory Allocator Interface  typedef int ( * tPinnedMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tPinnedFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tPinnedMallocV2 pinned_malloc ; tPinnedFreeV2 pinned_free ; void * pinned_ctx ; } nvjpegPinnedAllocatorV2_t ; Extended pinned"
  },
  {
    "id": 30213,
    "content": "allocators support stream ordered allocations along with user defined context information pinned_ctx When invoking the allocators, nvJPEG will pass pinned_ctx as input to the extended pinned allocators"
  },
  {
    "id": 30217,
    "content": "nvJPEG Image  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t structure (or structures, in the case of batched decode) is used to fill with the pointers and pitches of allocated buffers Member Description NVJPEG_MAX_COMPONENT Maximum number of color components the nvJPEG library supports"
  },
  {
    "id": 30221,
    "content": "nvJPEG Device Memory Allocator Interface  typedef int ( * tDevMalloc )( void ** , size_t ); typedef int ( * tDevFree )( void * ); typedef struct { tDevMalloc dev_malloc ; tDevFree dev_free ; } nvjpegDevAllocator_t ; Users can tell the library to use their own device memory allocator"
  },
  {
    "id": 30222,
    "content": "The function prototypes for the memory allocation and memory freeing functions are similar to the cudaMalloc() and cudaFree() functions"
  },
  {
    "id": 30223,
    "content": "A pointer to the nvjpegDevAllocator_t structure, with properly filled fields, should be provided to the nvjpegCreate() function"
  },
  {
    "id": 30224,
    "content": "NULL is accepted, in which case the default memory allocation functions cudaMalloc() and cudaFree() is used"
  },
  {
    "id": 30225,
    "content": "When the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set as a pointer to the above nvjpegDevAllocator_t structure, then this structure is used for allocating and releasing the device memory However, if the nvjpegDevAllocator_t *allocator parameter in the nvjpegCreate() or nvjpegCreateEx() function is set to NULL, then the default memory"
  },
  {
    "id": 30226,
    "content": "allocation functions cudaMalloc() and cudaFree() will be used When using nvjpegCreateSimple() function to create library handle the default device memory allocator will be used"
  },
  {
    "id": 30230,
    "content": "nvJPEG Extended Device Memory Allocator Interface  typedef int ( * tDevMallocV2 )( void * ctx , void ** ptr , size_t size , cudaStream_t stream ); typedef int ( * tDevFreeV2 )( void * ctx , void * ptr , size_t size , cudaStream_t stream ); typedef struct { tDevMallocV2 dev_malloc ; tDevFreeV2 dev_free ; void * dev_ctx ; } nvjpegDevAllocatorV2_t ; Extended device allocators support stream ordered"
  },
  {
    "id": 30231,
    "content": "allocations along with user defined context information dev_ctx When invoking the allocators, nvJPEG will pass dev_ctx as input to the extended device allocators"
  },
  {
    "id": 30235,
    "content": "nvJPEG Opaque JPEG Decoding State Handle  struct nvjpegJpegState ; typedef struct nvjpegJpegState * nvjpegJpegState_t ; The nvjpegJpegState structure stores the temporary JPEG information The same JPEG handle should be used across the decoding phases for the same image or batch Multiple threads are allowed to share the JPEG state handle only when processing same batch during first phase ("
  },
  {
    "id": 30240,
    "content": "nvJPEG Opaque Library Handle Struct  struct nvjpegHandle ; typedef struct nvjpegHandle * nvjpegHandle_t ; The library handle is used in any consecutive nvJPEG library calls, and should be initialized first The library handle is thread safe, and can be used by multiple threads simultaneously"
  },
  {
    "id": 30244,
    "content": "nvJPEG Output Pointer Struct  typedef struct { unsigned char * channel [ NVJPEG_MAX_COMPONENT ]; size_t pitch [ NVJPEG_MAX_COMPONENT ]; } nvjpegImage_t ; The nvjpegImage_t struct holds the pointers to the output buffers, and holds the corresponding strides of those buffers for the image decoding See Single Image Decoding on how to set up the nvjpegImage_t struct"
  },
  {
    "id": 30248,
    "content": "nvJPEG Jpeg Encoding  typedef enum { NVJPEG_ENCODING_UNKNOWN = 0x0 , NVJPEG_ENCODING_BASELINE_DCT = 0xc0 , NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN = 0xc1 , NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN = 0xc2 , NVJPEG_ENCODING_LOSSLESS_HUFFMAN = 0xc3 } nvjpegJpegEncoding_t ; The nvjpegJpegEncoding_t enum lists the JPEG encoding types that are supported by the nvJPEG library The enum values are"
  },
  {
    "id": 30249,
    "content": "based on the markers defined in the JPEG specification Member Description NVJPEG_ENCODING_UNKNOWN This value is returned for all the JPEG markers not supported by the nvJPEG library NVJPEG_ENCODING_BASELINE_DCT Corresponds to the JPEG marker 0xc0, refer to the JPEG spec for more details NVJPEG_ENCODING_EXTENDED_SEQUENTIAL_DCT_HUFFMAN Corresponds to the JPEG marker 0xc1, refer to the JPEG spec for"
  },
  {
    "id": 30250,
    "content": "more details NVJPEG_ENCODING_PROGRESSIVE_DCT_HUFFMAN Corresponds to the JPEG marker 0xc2, refer to the JPEG spec for more details NVJPEG_ENCODING_LOSSLESS_HUFFMAN Corresponds to the JPEG marker 0xc3, refer to the JPEG spec for more details"
  },
  {
    "id": 30254,
    "content": "nvJPEG Scale Factor  typedef enum { NVJPEG_SCALE_NONE = 0 , NVJPEG_SCALE_1_BY_2 = 1 , NVJPEG_SCALE_1_BY_4 = 2 , NVJPEG_SCALE_1_BY_8 = 3 } nvjpegScaleFactor_t ; The nvjpegScaleFactor_t enum lists all the scale factors supported by the library This feature is supported when nvjpeg handles are intstaniated using NVJPEG_BACKEND_HARDWARE Member Description NVJPEG_SCALE_NONE Decoded output is not"
  },
  {
    "id": 30255,
    "content": "scaled NVJPEG_SCALE_1_BY_2 Decoded output width and height are scaled by a factor of 1/2 NVJPEG_SCALE_1_BY_4 Decoded output width and height are scaled by a factor of 1/4 NVJPEG_SCALE_1_BY_8 Decoded output width and height are scaled by a factor of 1/8 2"
  },
  {
    "id": 30258,
    "content": "nvJPEG Flags  #define NVJPEG_FLAGS_DEFAULT 0 #define NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE 1 #define NVJPEG_FLAGS_ENABLE_MEMORY_POOLS 2 #define NVJPEG_FLAGS_BITSTREAM_STRICT 4 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE 8 #define NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY 16 #define NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION 32 nvJPEG flags provide additional controls when initializing the library"
  },
  {
    "id": 30260,
    "content": "NVJPEG_FLAGS_HW_DECODE_NO_PIPELINE To be used when the library is initialized with NVJPEG_BACKEND_HARDWARE"
  },
  {
    "id": 30263,
    "content": "NVJPEG_FLAGS_BITSTREAM_STRICT nvJPEG library will try to decode a bitstream even if it doesn’t strictly follow the JPEG specification"
  },
  {
    "id": 30264,
    "content": "NVJPEG_FLAGS_REDUCED_MEMORY_DECODE When using NVJPEG_BACKEND_HYBRID or NVJPEG_BACKEND_GPU_HYBRID backends, enabling this flag will reduce the memory usage of the decoding whenever possible NVJPEG_FLAGS_REDUCED_MEMORY_DECODE_ZERO_COPY Using this flag enables zero-copy memory when feasible on supported platforms NVJPEG_FLAGS_UPSAMPLING_WITH_INTERPOLATION Using this flag enables the decoder to use"
  },
  {
    "id": 30269,
    "content": "nvJPEG Exif Orientation  typedef enum { NVJPEG_ORIENTATION_UNKNOWN = 0 , NVJPEG_ORIENTATION_NORMAL = 1 , NVJPEG_ORIENTATION_FLIP_HORIZONTAL = 2 , NVJPEG_ORIENTATION_ROTATE_180 = 3 , NVJPEG_ORIENTATION_FLIP_VERTICAL = 4 , NVJPEG_ORIENTATION_TRANSPOSE = 5 , NVJPEG_ORIENTATION_ROTATE_90 = 6 , NVJPEG_ORIENTATION_TRANSVERSE = 7 , NVJPEG_ORIENTATION_ROTATE_270 = 8 } nvjpegExifOrientation_t ; The"
  },
  {
    "id": 30270,
    "content": "nvjpegExifOrientation_t enum represents the exif orientation in a jfif(jpeg) file Exif orientation information is typically used to denote the digital camera sensor orientation at the time of image capture Member Description NVJPEG_ORIENTATION_UNKNOWN Exif orientation information is not available in the bitstream"
  },
  {
    "id": 30271,
    "content": "NVJPEG_ORIENTATION_FLIP_HORIZONTAL Decoded output should be mirrored/flipped horizontally NVJPEG_ORIENTATION_TRANSPOSE Decoded output should be flipped/mirrored horizontally followed by a 90 degrees counter-clockwise rotation NVJPEG_ORIENTATION_ROTATE_90 Decoded output should be rotated 90 degrees counter-clockwise NVJPEG_ORIENTATION_TRANSVERSE Decoded output should be flipped/mirrored"
  },
  {
    "id": 30272,
    "content": "horizontally followed by a 270 degrees counter-clockwise rotation NVJPEG_ORIENTATION_ROTATE_270 Decoded output should be rotated 270 degrees counter-clockwise"
  },
  {
    "id": 30275,
    "content": "nvJPEG API Reference  This section describes the nvJPEG decoder API nvjpegGetProperty()  Gets the numeric value for the major or minor version, or the patch level, of the nvJPEG library Signature: nvjpegStatus_t nvjpegGetProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported"
  },
  {
    "id": 30276,
    "content": "libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL int *value Output Host The numeric value corresponding to the specific libraryPropertyType requested Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes"
  },
  {
    "id": 30281,
    "content": "nvjpegGetCudartProperty()  Gets the numeric value for the major version, minor version, or the patch level of the CUDA toolkit that was used to build nvJPEG library"
  },
  {
    "id": 30282,
    "content": "Signature: nvjpegStatus_t nvjpegGetCudartProperty ( libraryPropertyType type , int * value ); Parameters: Parameter Input / Output Memory Description libraryPropertyType type Input Host One of the supported libraryPropertyType values, that is, MAJOR_VERSION, MINOR_VERSION or PATCH_LEVEL"
  },
  {
    "id": 30287,
    "content": "nvjpegCreate() [DEPRECATED]  Allocates and initializes the library handle Use either nvjpegCreateSimple() or nvjpegCreateEx() functions to create the library handle Signature: nvjpegStatus_t nvjpegCreate ( nvjpegBackend_t backend , nvjpegDevAllocator_t * allocator , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend"
  },
  {
    "id": 30289,
    "content": "If NULL is provided, then the default CUDA runtime cudaMalloc() and cudaFree() functions will be used"
  },
  {
    "id": 30290,
    "content": "The nvjpegBackend_t parameter is an enum type, with the below enumerated list values: typedef enum { NVJPEG_BACKEND_DEFAULT = 0 , NVJPEG_BACKEND_HYBRID = 1 , } nvjpegBackend_t ; Returns: nvjpegStatus_t - An error code as specified in nvJPEG API Return Codes"
  },
  {
    "id": 30295,
    "content": "nvjpegCreateSimple()  Allocates and initializes the library handle, with default codec implementations selected by library and default memory allocators Signature: nvjpegStatus_t nvjpegCreateSimple ( nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t *handle Input/Output Host The library handle"
  },
  {
    "id": 30302,
    "content": "Signature: nvjpegStatus_t nvjpegCreateEx ( nvjpegBackend_t backend , nvjpegDevAllocator_t * dev_allocator , nvjpegPinnedAllocator_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API"
  },
  {
    "id": 30303,
    "content": "If NULL is provided, then the default CUDA runtime functions cudaMalloc() and cudaFree() will be used If NULL is provided, then the default CUDA runtime functions cudaHostAlloc() and cudaFreeHost() will be used"
  },
  {
    "id": 30310,
    "content": "Signature: nvjpegStatus_t nvjpegCreateExV2 ( nvjpegBackend_t backend , nvjpegDevAllocatorV2_t * dev_allocator , nvjpegPinnedAllocatorV2_t * pinned_allocator , unsigned int flags , nvjpegHandle_t * handle ); Parameters: Parameter Input / Output Memory Description nvjpegBackend_t backend Input Host Backend parameter for nvjpegDecodeBatched() API See nvjpegPinnedAllocatorV2_t structure description"
  },
  {
    "id": 30315,
    "content": "nvjpegDestroy()  Releases the library handle Signature: nvjpegStatus_t nvjpegDestroy ( nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input/Output Host The library handle to release"
  },
  {
    "id": 30320,
    "content": "nvjpegSetDeviceMemoryPadding()  Use the provided padding for all device memory allocations with specified library handle"
  },
  {
    "id": 30322,
    "content": "Signature: nvjpegStatus_t nvjpegSetDeviceMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input Host Device memory padding to use for all further device memory allocations nvjpegHandle_t handle Input/Output Host The library handle"
  },
  {
    "id": 30327,
    "content": "nvjpegGetDeviceMemoryPadding()  Retrieve the device memory padding that is currently used for the specified library handle Signature: nvjpegStatus_t nvjpegGetDeviceMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Device memory padding that is currently used for device memory allocations"
  },
  {
    "id": 30332,
    "content": "nvjpegSetPinnedMemoryPadding()  Use the provided padding for all pinned host memory allocations with specified library handle A large number will help to amortize the need for pinned host memory reallocations when needed Signature: nvjpegStatus_t nvjpegSetPinnedMemoryPadding ( size_t padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t padding Input"
  },
  {
    "id": 30338,
    "content": "nvjpegGetPinnedMemoryPadding()  Retrieve the pinned host memory padding that is currently used for specified library handle Signature: nvjpegStatus_t nvjpegGetPinnedMemoryPadding ( size_t * padding , nvjpegHandle_t handle ); Parameters: Parameter Input / Output Memory Description size_t *padding Output Host Pinned host memory padding that is currently used for pinned host memory allocations"
  },
  {
    "id": 30343,
    "content": "nvjpegGetHardwareDecoderInfo()  Retrieve hardware decoder details such as number of engines and number of cores available in each engine"
  },
  {
    "id": 30344,
    "content": "Signature: nvjpegStatus_t nvjpegGetHardwareDecoderInfo ( nvjpegHandle_t handle , unsigned int * num_engines , unsigned int * num_cores_per_engine ); Parameters: nvjpegHandle_t handle Input Host The library handle unsigned int* num_engines Input/Output Host Retrieves number of engines available for decode unsigned int* num_cores_per_engine Input/Output Host Retrieves number of cores per engine"
  },
  {
    "id": 30349,
    "content": "nvjpegJpegStateCreate()  Allocates and initializes the internal structure required for the JPEG processing Signature: nvjpegStatus_t nvjpegJpegStateCreate ( nvjpegHandle_t handle , nvjpegJpegState_t * jpeg_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle nvjpegJpegState_t *jpeg_handle Input/Output Host The image state handle"
  },
  {
    "id": 30354,
    "content": "nvjpegJpegStateDestroy()  Releases the image internal structure Signature: nvjpegStatus_t nvjpegJpegStateDestroy ( nvjpegJpegState handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState handle Input/Output Host The image state handle"
  },
  {
    "id": 30359,
    "content": "nvjpegDecoderCreate()  Creates a decoder handle Signature: nvjpegStatus_t nvjpegDecoderCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegBackend_t implementation , nvjpegJpegDecoder_t * decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t nvjpeg_handle Input Host Library handle nvjpegBackend_t backend Input Host Backend parameter for the decoder_handle"
  },
  {
    "id": 30366,
    "content": "nvjpegDecoderDestroy()  Destroys the decoder handle Signature: nvjpegStatus_t nvjpegDecoderDestroy ( nvjpegJpegDecoder_t decoder_handle ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle Input/Output Host Decoder handle"
  },
  {
    "id": 30371,
    "content": "nvjpegDecoderJpegSupported()  Determines whether the decoder_handle is able to handle the bit-stream stored in jpeg_stream Signature: nvjpegStatus_t nvjpegDecoderJpegSupported ( nvjpegJpegDecoder_t decoder_handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegJpegDecoder_t decoder_handle"
  },
  {
    "id": 30372,
    "content": "Input Host Decoder state handle nvjpegJpegStream_t jpeg_stream Input Host Bit stream meta-data nvjpegDecodeParams_t decode_params Input Host Decoder output configuration int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported Returns: nvjpegStatus_t — An error code as specified in nvJPEG"
  },
  {
    "id": 30378,
    "content": "nvjpegDecoderStateCreate()  Creates the decoder_state internal structure The decoder_state is associated with the nvjpegBackend_t implementation that was used to create the decoder_handle Signature: nvjpegStatus_t nvjpegDecoderStateCreate ( nvjpegHandle_t nvjpeg_handle , nvjpegJpegDecoder_t decoder_handle , nvjpegJpegState_t * decoder_state ); Parameters: Parameter Input / Output Memory"
  },
  {
    "id": 30379,
    "content": "Description nvjpegHandle_t nvjpeg_handle Input Host Library handle nvjpegJpegState_t* decoder_state Input/Output Host nvJPEG Image State Handle"
  },
  {
    "id": 30384,
    "content": "nvjpegJpegStreamCreate()  Creates jpeg_stream that is used to parse the JPEG bitstream and store bitstream parameters Signature: nvjpegStatus_t nvjpegJpegStreamCreate ( nvjpegHandle_t handle , nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle"
  },
  {
    "id": 30390,
    "content": "nvjpegJpegStreamDestroy()  Destroys the jpeg_stream structure Signature: nvjpegStatus_t nvjpegJpegStreamDestroy ( nvjpegJpegStream_t * jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t *jpeg_stream Input Host Bitstream handle Returns: nvjpegStatus_t — An error code as specified in nvJPEG API Return Codes"
  },
  {
    "id": 30395,
    "content": "nvjpegBufferPinnedCreate()  Creates a pinned buffer handle Signature: nvjpegStatus_t nvjpegBufferPinnedCreate ( nvjpegHandle_t handle , nvjpegPinnedAllocator_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegBufferPinned_t* buffer Input/Output Host nvJPEG pinned buffer object"
  },
  {
    "id": 30400,
    "content": "nvjpegBufferPinnedCreateV2()  Creates a pinned buffer handle using extended allocators Signature: nvjpegStatus_t nvjpegBufferPinnedCreateV2 ( nvjpegHandle_t handle , nvjpegPinnedAllocatorV2_t * pinned_allocator , nvjpegBufferPinned_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegPinnedAllocatorV2_t* pinned_allocator"
  },
  {
    "id": 30406,
    "content": "nvjpegBufferPinnedDestroy()  Destroys a pinned buffer handle Signature: nvjpegStatus_t nvjpegBufferPinnedDestroy ( nvjpegBufferPinned_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer object"
  },
  {
    "id": 30412,
    "content": "The pinned_buffer is used by the decoder to store the intermediate information that is used across the decoding stages"
  },
  {
    "id": 30413,
    "content": "Pinned buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory"
  },
  {
    "id": 30414,
    "content": "Signature: nvjpegStatus_t nvjpegStateAttachPinnedBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferPinned_t pinned_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state nvjpegBufferPinned_t pinned_buffer Input Host nvJPEG pinned buffer container"
  },
  {
    "id": 30419,
    "content": "nvjpegBufferPinnedRetrieve()  Retrieves the pinned memory pointer and size from the nvJPEG pinned buffer handle Signature: nvjpegStatus_t nvjpegBufferPinnedRetrieve ( nvjpegBufferPinned_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t buffer Input Host nvJPEG pinned buffer container void** ptr Input/Output Host Pointer to the"
  },
  {
    "id": 30425,
    "content": "nvjpegBufferPinnedResize()  Resize the pinned buffer to the specified size in bytes This API can be used to pre-allocate the pinned buffer to a large value and avoid allocator calls during decode Signature: nvjpegStatus_t nvjpegBufferPinnedResize ( nvjpegBufferPinned_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferPinned_t"
  },
  {
    "id": 30426,
    "content": "buffer Input Host nvJPEG pinned buffer container cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferPinned_t buffer is initialized using stream ordered allocators"
  },
  {
    "id": 30431,
    "content": "nvjpegBufferDeviceCreate()  Creates the device buffer handle Signature: nvjpegStatus_t nvjpegBufferDeviceCreate ( nvjpegHandle_t handle , nvjpegDevAllocator_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegBufferDevice_t* buffer Input/Output Host nvJPEG device buffer container"
  },
  {
    "id": 30436,
    "content": "nvjpegBufferDeviceCreateV2()  Creates the device buffer handle using extended allocators Signature: nvjpegStatus_t nvjpegBufferDeviceCreateV2 ( nvjpegHandle_t handle , nvjpegDevAllocatorV2_t * device_allocator , nvjpegBufferDevice_t * buffer ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegDevAllocatorV2_t* device_allocator Input"
  },
  {
    "id": 30442,
    "content": "nvjpegBufferDeviceDestroy()  Destroys the device buffer handle Signature: nvjpegStatus_t nvjpegBufferDeviceDestroy ( nvjpegBufferDevice_t buffer ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host/Device nvJPEG device buffer container"
  },
  {
    "id": 30449,
    "content": "The device_buffer is used by the decoder to store the intermediate information that is used across the decoding stages"
  },
  {
    "id": 30450,
    "content": "Device buffer can be attached to different decoder states, which helps to switch between implementations without allocating extra memory"
  },
  {
    "id": 30451,
    "content": "Signature: nvjpegStatus_t nvjpegStateAttachDeviceBuffer ( nvjpegJpegState_t decoder_state , nvjpegBufferDevice_t device_buffer ); Parameters: Parameter Input / Output Memory Description nvjpegJpegState_t decoder_state Input Host nvJPEG decoder state nvjpegBufferDevice_t device buffer Input Host/Device nvJPEG device buffer container"
  },
  {
    "id": 30456,
    "content": "nvjpegBufferDeviceRetrieve()  Retrieve the device memory pointer and size from the nvJPEG device buffer handle Signature: nvjpegStatus_t nvjpegBufferDeviceRetrieve ( nvjpegBufferDevice_t buffer , size_t * size , void ** ptr ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t buffer Input Host nvJPEG device buffer container void** ptr Input/Output Host Pointer to the"
  },
  {
    "id": 30462,
    "content": "nvjpegBufferDeviceResize()  Resize the device buffer to the specified size in bytes This API can be used to pre-allocate the device buffer to a large value and avoid allocator calls during decode Signature: nvjpegStatus_t nvjpegBufferDeviceResize ( nvjpegBufferDevice_t buffer , size_t size , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegBufferDevice_t"
  },
  {
    "id": 30463,
    "content": "buffer Input Host nvJPEG device buffer container cudaStream_t stream Input Host CUDA stream to use when nvjpegBufferDevice_t buffer is initialized using stream ordered allocators"
  },
  {
    "id": 30470,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeParamsCreate ( nvjpegHandle_t handle , nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host Library handle nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters"
  },
  {
    "id": 30475,
    "content": "nvjpegDecodeParamsDestroy()  Destroys the decode_params handle Signature: nvjpegStatus_t nvjpegDecodeParamsDestroy ( nvjpegDecodeParams_t * decode_params ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t *decode_params Input/Output Host Decode output parameters"
  },
  {
    "id": 30479,
    "content": "Retrieve Encoded Image Information API  The helper functions for retrieving the encoded image information"
  },
  {
    "id": 30485,
    "content": "Signature: nvjpegStatus_t nvjpegGetImageInfo ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int * nComponents , nvjpegChromaSubsampling_t * subsampling , int * widths , int * heights ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle int *widths Output Host Pointer to the first element of array of size"
  },
  {
    "id": 30486,
    "content": "NVJPEG_MAX_COMPONENT, where the width of each channel (up to NVJPEG_MAX_COMPONENT) will be saved int *heights Output Host Pointer to the first element of array of size NVJPEG_MAX_COMPONENT, where the height of each channel (up to NVJPEG_MAX_COMPONENT) will be saved"
  },
  {
    "id": 30497,
    "content": "nvjpegJpegStreamParse()  Parses the bitstream and stores the metadata in the jpeg_stream struct Signature: nvjpegStatus_t nvjpegJpegStreamParse ( nvjpegHandle_t handle , const unsigned char * data , size_t length , int save_metadata , int save_stream , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle"
  },
  {
    "id": 30499,
    "content": ") will be saved in the internal JpegStream structure for future usage int save_stream Input Host If not 0, then the whole jpeg stream will be copied to the internal JpegStream structure, and the pointer to the JPEG file data will not be needed after this call If 0, then JpegStream will just save the pointers (to JPEG file data), and these pointers will be used later during the image decoding"
  },
  {
    "id": 30500,
    "content": "nvjpegJpegStream_t jpeg_stream Input/Output Host/Device The nvJPEG bitstream handle that stores the parsed bitstream information"
  },
  {
    "id": 30506,
    "content": "nvjpegJpegStreamParseHeader()  Parses only the header of the bit-stream and stores the header information in the jpeg_stream struct Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle"
  },
  {
    "id": 30513,
    "content": "Parses the JPEG tables bitstream and stores the jpeg tables in jpeg_stream Signature: nvjpegStatus_t nvjpegJpegStreamParseHeader ( nvjpegHandle_t handle , const unsigned char * data , size_t length , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle nvjpegJpegStream_t jpeg_stream Input/Output Host The"
  },
  {
    "id": 30520,
    "content": "nvjpegJpegStreamGetFrameDimensions()  Extracts the JPEG frame dimensions from the bitstream Signature: nvjpegStatus_t nvjpegJpegStreamGetFrameDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int * width , unsigned int * height ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle unsigned int* height Output Host Frame width"
  },
  {
    "id": 30526,
    "content": "nvjpegJpegStreamGetComponentsNum()  Extracts the JPEG frame dimensions from the bitstream Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentsNum ( nvjpegJpegStream_t jpeg_stream , unsigned int * components_num ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle unsigned int* components_num Output Host Number of encoded"
  },
  {
    "id": 30533,
    "content": "nvjpegJpegStreamGetComponentDimensions()  Extracts the component dimensions from the bitstream Signature: nvjpegStatus_t nvjpegJpegStreamGetComponentDimensions ( nvjpegJpegStream_t jpeg_stream , unsigned int component , unsigned int * width , unsigned int * height ) Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle unsigned int*"
  },
  {
    "id": 30541,
    "content": "For 3-channel images it tries to assign one of the known chroma sub-sampling values based on the sampling information present in the bitstream, else it returns NVJPEG_CSS_UNKNOWN"
  },
  {
    "id": 30542,
    "content": "Signature: nvjpegStatus_t nvjpegJpegStreamGetChromaSubsampling ( nvjpegJpegStream_t jpeg_stream , nvjpegChromaSubsampling_t * chroma_subsampling ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle nvjpegChromaSubsampling_t* chroma_subsampling Output Host Chroma subsampling for the 1- or 3- channel encoding"
  },
  {
    "id": 30548,
    "content": "nvjpegJpegStreamGetJpegEncoding()  This function obtains the JPEG encoding type from the jpeg_stream Signature: nvjpegStatus_t nvjpegJpegStreamGetJpegEncoding ( nvjpegJpegStream_t jpeg_stream , nvjpegJpegEncoding_t * jpeg_encoding ); Parameters: Parameter Input / Output Memory Description jpeg_stream In Host Input bitstream handle jpeg_encoding Out Host Encoding type obtained—baseline or"
  },
  {
    "id": 30555,
    "content": "nvjpegJpegStreamGetExifOrientation()  Extracts the exif orientation from the bitstream Returns NVJPEG_ORIENTATION_UNKNOWN if the exif marker/orientation information is not present Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetExifOrientation ( nvjpegJpegStream_t jpeg_stream , nvjpegExifOrientation_t * orientation_flag ); Parameters: Parameter Input / Output Memory Description"
  },
  {
    "id": 30556,
    "content": "nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle nvjpegExifOrientation_t *orientation_flag Output Host Exif orientation in JPEG stream"
  },
  {
    "id": 30562,
    "content": "nvjpegJpegStreamGetSamplePrecision()  Extracts the sample precision(bit depth) from the bitstream Signature: nvjpegStatus_t NVJPEGAPI nvjpegJpegStreamGetSamplePrecision ( nvjpegJpegStream_t jpeg_stream , unsigned int * precision ); Parameters: Parameter Input / Output Memory Description nvjpegJpegStream_t jpeg_stream Input Host Bitstream handle unsigned int *precision Output Host Sample"
  },
  {
    "id": 30572,
    "content": "​nvjpegDecode()  Decodes a single image, and writes the decoded image in the desired format to the output buffers From CUDA 11 onwards, nvjpegDecode() picks the best available back-end for a given image, user no longer has control on this"
  },
  {
    "id": 30573,
    "content": "Signature: nvjpegStatus_t nvjpegDecode ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , size_t length , nvjpegOutputFormat_t output_format , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle nvjpegOutputFormat_t output_format Input Host Format in"
  },
  {
    "id": 30574,
    "content": "which the decoded output will be saved nvjpegImage_t *destination Input/Output Host/Device Pointer to the structure that describes the output destination This structure should be on the host (CPU), but the pointers in this structure should be pointing to the device (i"
  },
  {
    "id": 30576,
    "content": ", GPU) memory cudaStream_t stream Input Host The CUDA stream where all of the GPU work will be submitted"
  },
  {
    "id": 30582,
    "content": "The initialization parameters include the batch size, the maximum number of CPU threads, and the specific output format in which the decoded image will be saved"
  },
  {
    "id": 30583,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeBatchedInitialize ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int max_cpu_threads , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle int max_cpu_threads Input Host This parameter is no longer used by the library"
  },
  {
    "id": 30588,
    "content": "​nvjpegDecodeBatched()  Decodes the batch of images, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function"
  },
  {
    "id": 30589,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeBatched ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle const unsigned char *const *data Input Host Pointer to the first"
  },
  {
    "id": 30591,
    "content": "The size of the array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() batch initialization function Size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function"
  },
  {
    "id": 30592,
    "content": "nvjpegImage_t *destinations Input/Output Host/Device Pointer to the first element of array of output descriptors"
  },
  {
    "id": 30593,
    "content": "The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize(), the batch initialization function"
  },
  {
    "id": 30599,
    "content": "nvjpegDecodeBatchedEx()  This API helps to Decodes the batch of images with ROI, and writes them to the buffers described in the destination parameter in a format provided to nvjpegDecodeBatchedInitialize() function"
  },
  {
    "id": 30600,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeBatchedEx ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * const * data , const size_t * lengths , nvjpegImage_t * destinations , nvjpegDecodeParams_t * decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle"
  },
  {
    "id": 30601,
    "content": "The size of array is assumed to be batch_size provided to nvjpegDecodeBatchedInitialize() , the batch initialization function"
  },
  {
    "id": 30602,
    "content": "nvjpegDecodeParams_t *decode_params Input Host Setting ROI Decode parameters cudaStream_t stream Input Host The CUDA stream where all the GPU work will be submitted"
  },
  {
    "id": 30607,
    "content": "nvjpegDecodeBatchedSupported()  This API helps determine whether an image can be decoded by nvjpegDecodeBatched User can parse the bitstream header using nvjpegJpegStreamParseHeader and then call this API to determine whether the image can be decoded"
  },
  {
    "id": 30608,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeBatchedSupported ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle"
  },
  {
    "id": 30609,
    "content": "int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , non zero value indicates that the bitstream is not supported"
  },
  {
    "id": 30614,
    "content": "nvjpegDecodeBatchedSupportedEx()  This API helps determine whether an image can be decoded by nvjpegDecodeBatchedEx User can parse the bitstream header using nvjpegJpegStreamParseHeader and set the ROI in the decode params then call this API to determine whether the image can be decoded"
  },
  {
    "id": 30615,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeBatchedSupportedEx ( nvjpegHandle_t handle , nvjpegJpegStream_t jpeg_stream , nvjpegDecodeParams_t decode_params , int * is_supported ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host nvjpeg library handle"
  },
  {
    "id": 30616,
    "content": "int* is_supported Output Host Return value of 0 indicates bitstream can be decoded by the decoder_handle , a non zero value indicates that the bitstream is not supported"
  },
  {
    "id": 30621,
    "content": "nvjpegDecodeBatchedPreAllocate()  This is an experimental API that can be used with nvjpegDecodeBatched"
  },
  {
    "id": 30622,
    "content": "When decoding images with varying sizes and chroma subsampling, performance is limited by the repeated cuda calls made by the library to free/allocate device memory This API attempts to avoid this problem by allocating device memory prior to the actual decoding"
  },
  {
    "id": 30623,
    "content": "Users have the option to call this API with values that are unlikely to be exceeded when nvjpegDecodeBatched is called"
  },
  {
    "id": 30624,
    "content": "Note Note: This functionality is available only when the nvjpegHandle_t is instantiated using NVJPEG_BACKEND_HARDWARE"
  },
  {
    "id": 30625,
    "content": "If the image dimensions at the time of decode exceed what was provided, then the library will resize the device buffers"
  },
  {
    "id": 30626,
    "content": "If the images being decoded have different chroma subsamplings, then the chroma_subsampling field should be set to NVJPEG_CSS_444 to ensure that the device memory can be reused"
  },
  {
    "id": 30627,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeBatchedPreAllocate ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , int batch_size , int width , int height , nvjpegChromaSubsampling_t chroma_subsampling , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle nvjpegChromaSubsampling_t chroma_subsampling"
  },
  {
    "id": 30633,
    "content": "nvjpegDecodeBatchedParseJpegTables()  To be used along with batched decode APIs when decoding JPEG bitstreams from a TIFF file The external Huffman and quantization tables will be applied to all the JPEG bitstreams in the batch"
  },
  {
    "id": 30634,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeBatchedParseJpegTables ( nvjpegHandle_t handle , nvjpegJpegState_t jpeg_handle , const unsigned char * data , const size_t length ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle"
  },
  {
    "id": 30639,
    "content": "Decode API—Decoupled Decoding  This set of decoding API works with the bitstream handles, decode parameter handles, pinned and device buffers handles as input, thus decoupling JPEG bitstream parse, buffer management and setting up decoder parameters from the decode process itself"
  },
  {
    "id": 30640,
    "content": "Multiphase decoupled single image decoding consists of three phases: Host Mixed Device Each of the above decodings is carried on according to its individual semantics Phases on different images can be carried out with different decoding state handles simultaneously, while sharing of some helper objects is possible The following snippet explains how to use the API to prefetch the host stage of the"
  },
  {
    "id": 30641,
    "content": "processing: first do all of the host work on the host, and then submit the rest of decoding work to the device If a pinned buffer is attached to the decoder state, then the pinned buffer object will be used to allocate the pinned memory required for the host decoding phase There wouldn’t be allocation if the pinned buffer object already handles the required amount of pinned memory If a pinned"
  },
  {
    "id": 30642,
    "content": "buffer object is not attached, then the state will use heap host memory to allocate the memory required for the host processing Hence the device selection, device initialization, and device memory initialization can be done later in the decoding process"
  },
  {
    "id": 30643,
    "content": "The parsed stream handle that is available after calling the nvjpegJpegStreamParse() function should be provided to this function"
  },
  {
    "id": 30644,
    "content": "Signature: nnvjpegStatus_t nvjpegDecodeJpegHost ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegDecodeParams_t decode_params , nvjpegJpegStream_t jpeg_stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle nvjpegJpegStream_t jpeg_stream Input Host Handle to the parsed bitstream data"
  },
  {
    "id": 30649,
    "content": "nvjpegDecodeJpegTransferToDevice()  This phase contains both host and device operations This phase should be called only after the host phase with the same decoder handle, decoder state handle and parsed jpeg stream handle Device should be initialized and device buffer should be attached to decoder_state handle using nvjpegStateAttachDeviceBuffer() prior to calling this API For the host memory"
  },
  {
    "id": 30650,
    "content": "buffer, this phase will use whatever was used in the host phase: either the attached pinned buffer or the state’s host memory buffer Signature: nvjpegStatus_t nvjpegDecodeJpegTransferToDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description"
  },
  {
    "id": 30651,
    "content": "nvjpegHandle_t handle Input Host The library handle cudaStream_t stream Input Host The CUDA stream to which all the GPU tasks will be submitted"
  },
  {
    "id": 30656,
    "content": "nvjpegDecodeJpegDevice()  This phase consists of decode operations that take place mainly on the device (no significant host side computation is done)"
  },
  {
    "id": 30657,
    "content": "This phase should be called after nvjpegDecodeJpegTransferToDevice() for a given decoder_state handle and decoder handle"
  },
  {
    "id": 30658,
    "content": "In this function call, the host memory buffers are not used, so if the pinned buffer was attached to the state, then it can be reused somewhere else"
  },
  {
    "id": 30659,
    "content": "Note that at this point the Jpeg stream handle is not needed anymore, since parts that are needed for device decoding will be copied to the device memory in the previous phase"
  },
  {
    "id": 30660,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeJpegDevice ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegImage_t * destination , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle nvjpegImage_t *destination Input/Output Host/Device Pointer to a structure that describes the"
  },
  {
    "id": 30661,
    "content": "output destination This structure should be on host, but the pointers in this structure should be pointing to the device memory"
  },
  {
    "id": 30667,
    "content": "nvjpegDecodeJpeg()  This is a single phase API with the flexibility to select nvJPEG back-end when creating an nvjpegJpegDecoder_t object"
  },
  {
    "id": 30668,
    "content": "The user has the option to call this API instead of making three separate calls to nvjpegDecodeJpegHost() , nvjpegDecodeJpegTransferToDevice() , and nvjpegDecodeJpegDevice()"
  },
  {
    "id": 30669,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeJpeg ( nvjpegHandle_t handle , nvjpegJpegDecoder_t decoder , nvjpegJpegState_t decoder_state , nvjpegJpegStream_t jpeg_bitstream , nvjpegImage_t * destination , nvjpegDecodeParams_t decode_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description nvjpegHandle_t handle Input Host The library handle"
  },
  {
    "id": 30670,
    "content": "This structure should be on the host, but the pointers in this structure should be pointing to the device memory"
  },
  {
    "id": 30675,
    "content": "nvJPEG Decode Parameters  This category of APIs is used to set the decoding parameters These APIs should be used with the decode APIs defined in Decode API—Decoupled Decoding"
  },
  {
    "id": 30680,
    "content": "nvjpegDecodeParamsSetOutputFormat()  This function is used to set the decode output format The output parameter of nvjpegOutputFormat_t defaults to NVJPEG_OUTPUT_UNCHANGED if not set using this API Signature: nvjpegStatus_t nvjpegDecodeParamsSetOutputFormat ( nvjpegDecodeParams_t decode_params , nvjpegOutputFormat_t output_format ); Parameters: Parameter Input / Output Memory Description"
  },
  {
    "id": 30681,
    "content": "nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle nvjpegOutputFormat_t output_format Input Host See step 6 of Single Image Decoding"
  },
  {
    "id": 30686,
    "content": "nvjpegDecodeParamsSetROI()  This function enables the region of interest-only (ROI-only) decode To disable the ROI-only, i"
  },
  {
    "id": 30690,
    "content": "That is: offset_x cannot be lower than zero, or offset_x + roi_width cannot be larger than the JPEG image width If the output format is NVJPEG_OUTPUT_YUV or NVJPEG_OUTPUT_UNCHANGED, then the offset_x and offset_y values have to be multiples of the maximum subsampling factor, as defined in the JPEG standard Signature: nvjpegStatus_t nvjpegDecodeParamsSetROI ( nvjpegDecodeParams_t decode_params ,"
  },
  {
    "id": 30691,
    "content": "int offset_x , int offset_y , int roi_width , int roi_height ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host The decode output parameter handle int offset_x Input Host Image offset along the horizontal direction relative to the top left corner int offset_y Input Host Image offset along the vertical direction relative to the top left corner"
  },
  {
    "id": 30697,
    "content": "nvjpegDecodeParamsSetAllowCMYK()  If enabled, the nvJPEG library assumes that the JPEG with 4 encoded color components is in CMYK colorspace, and enables the conversion to RGB/YUV colorspace"
  },
  {
    "id": 30698,
    "content": "The conversion is based on the subtractive scheme—this behavior matches OpenCV’s handling of 4-component JPEGs"
  },
  {
    "id": 30699,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeParamsSetAllowCMYK ( nvjpegDecodeParams_t decode_params , int allow_cmyk ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle int allow_cmyk Input Host Enable CMYK to RGB conversion"
  },
  {
    "id": 30705,
    "content": "Note This feature is currently supported only when nvJPEG decoder handle is created using NVJPEG_BACKEND_HARDWARE"
  },
  {
    "id": 30706,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeParamsSetScaleFactor ( nvjpegDecodeParams_t decode_params , nvjpegScaleFactor_t scale_factor ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle"
  },
  {
    "id": 30707,
    "content": "When setting a scale factor value, the recommended allocation of the destination parameters is as follows: Use nvjpegGetImageInfo() , or nvjpegJpegStreamGetFrameDimensions() to extract the dimensions of each channel"
  },
  {
    "id": 30708,
    "content": "Let height[NVJPEG_MAX_COMPONENT] and width[NVJPEG_MAX_COMPONENT] be 2 arrays which store the height and width"
  },
  {
    "id": 30709,
    "content": "When ExifOrientation is enabled, the output buffers should be allocated based on the rotated dimensions"
  },
  {
    "id": 30710,
    "content": "If the orientation is set as NVJPEG_ORIENTATION_UNKNOWN , the library will default to NVJPEG_ORIENTATION_HORIZONTAL"
  },
  {
    "id": 30711,
    "content": "Signature: nvjpegStatus_t nvjpegDecodeParamsSetExifOrientation ( nvjpegDecodeParams_t decode_params , nvjpegExifOrientation_t orientation ); Parameters: Parameter Input / Output Memory Description nvjpegDecodeParams_t decode_params Input Host Decode output parameter handle nvjpegExifOrientation_t orientation Input Host Set the exif orientation for the decode output"
  },
  {
    "id": 30715,
    "content": "nvJPEG API Return Codes  The nvJPEG API adheres to the following return codes and their indicators: typedef enum { NVJPEG_STATUS_SUCCESS = 0 , NVJPEG_STATUS_NOT_INITIALIZED = 1 , NVJPEG_STATUS_INVALID_PARAMETER = 2 , NVJPEG_STATUS_BAD_JPEG = 3 , NVJPEG_STATUS_JPEG_NOT_SUPPORTED = 4 , NVJPEG_STATUS_ALLOCATOR_FAILURE = 5 , NVJPEG_STATUS_EXECUTION_FAILED = 6 , NVJPEG_STATUS_ARCH_MISMATCH = 7 ,"
  },
  {
    "id": 30716,
    "content": "NVJPEG_STATUS_INTERNAL_ERROR = 8 , NVJPEG_STATUS_IMPLEMENTATION_NOT_SUPPORTED = 9 } nvjpegStatus_t ; Description of the returned error codes: Returned Error (Returned Code) Description NVJPEG_STATUS_SUCCESS (0) The API call has finished successfully"
  },
  {
    "id": 30717,
    "content": "Note that many of the calls are asynchronous and some of the errors may be seen only after synchronization"
  },
  {
    "id": 30718,
    "content": "NVJPEG_STATUS_JPEG_NOT_SUPPORTED (4) Attempting to decode a JPEG stream that is not supported by the nvJPEG library"
  },
  {
    "id": 30719,
    "content": "NVJPEG_STATUS_ALLOCATOR_FAILURE (5) The user-provided allocator functions, for either memory allocation or for releasing the memory, returned a non-zero code"
  },
  {
    "id": 30720,
    "content": "NVJPEG_STATUS_ARCH_MISMATCH (7) The device capabilities are not enough for the set of input parameters provided (input parameters such as backend, encoded stream parameters, output format)"
  },
  {
    "id": 30721,
    "content": "nvJPEG Chroma Subsampling  One of the outputs of the nvjpegGetImageInfo() API is nvjpegChromaSubsampling_t This parameter is an enum type, and its enumerator list comprises of the chroma subsampling property retrieved from the encoded JPEG image The nvjpegGetImageInfo() function currently supports the following chroma subsampling types: typedef enum { NVJPEG_CSS_444 , NVJPEG_CSS_422 ,"
  },
  {
    "id": 30722,
    "content": "NVJPEG_CSS_420 , NVJPEG_CSS_440 , NVJPEG_CSS_411 , NVJPEG_CSS_410 , NVJPEG_CSS_GRAY , NVJPEG_CSS_410V , NVJPEG_CSS_UNKNOWN } nvjpegChromaSubsampling_t ; 2"
  },
  {
    "id": 30725,
    "content": "Examples of nvJPEG  nvJPEG Decode sample can be found here: https: github com/NVIDIA/CUDALibrarySamples/tree/master/nvJPEG/nvJPEG-Decoder 3 JPEG Encoding  This section describes the encoding functions of the nvJPEG Library"
  },
  {
    "id": 30728,
    "content": "Using the Encoder  The user should perform the below prerequisite steps before calling the nvJPEG encoding functions See also nvJPEG Encoder Helper API Reference"
  },
  {
    "id": 30732,
    "content": "Encoding the Parameters  The user should create an encoding parameters structure with nvjpegEncoderParamsCreate() function"
  },
  {
    "id": 30733,
    "content": "User can use an appropriate nvjpegEncoderParamsSet*() function to set a specific parameter The quality parameter can be set, using the nvjpegEncoderParamsSetQuality() function, to an integer value between 1 and 100, and this quality parameter will be used as a base for generating the JPEG quantization tables Note Occasionally, when encoding high entropy input data, such as random images, the"
  },
  {
    "id": 30736,
    "content": "We recommend restarting the encoding with slightly lower quality factor or using a real-world images if possible Note The encoding parameters structure can be reused to compress multiple images simultaneously, but no changes to the parameters should be made during the ongoing encoding, or the encoding result will be undefined"
  },
  {
    "id": 30740,
    "content": "Encoding the State  The user should create the encoding state structure using nvjpegEncoderStateCreate() function Note The encoding state structure can be reused to encode a series of images, but no encoding should be performed on multiple images with the same encoding state at the same time—otherwise the result of the encodings will be undefined"
  },
  {
    "id": 30744,
    "content": "Encoding the Image  The nvJPEG library provides a few interfaces for compressing the image in different formats and colorspaces"
  },
  {
    "id": 30751,
    "content": "If the chroma subsampling in the encoding parameters is the same as input chroma subsampling, then the user’s input data will be directly used in the JPEG compression Otherwise chroma will be resampled to match the chroma subsampling of the encoding parameters"
  },
  {
    "id": 30753,
    "content": "For example: Image dimensions: 123x321 Input chroma subsampling: NVJPEG_CSS_410 Chroma subsampling factor for this chroma subsampling: 4x2 Given the above, the encoder library expects the user to provide: Y plane with size: 123 x 321 Cb and Cr plane with size: 31 x 161 3"
  },
  {
    "id": 30759,
    "content": ", how data should be provided in the source argument, is determined by the input_format argument For example, if the user has interleaved the RGB image of size W x H , stored continuously, and the pointer to it is pImage , then source should be: source channel[0] = pImage source pitch[0] = W*3 When the same image is stored in planar format, with image planes pointers stored continuously in the"
  },
  {
    "id": 30760,
    "content": "array pImage[3] , then source should be: source channel[0] = pImage[0] source channel[1] = pImage[1] source channel[2] = pImage[2] The pitch values for each channel in the source parameter should be set accordingly to the data layout"
  },
  {
    "id": 30765,
    "content": "Retrieving the Compressed Stream  Often it is not feasible to accurately predict the final compressed data size of the final JPEG stream for any input data and parameters The nvJPEG library, while encoding, will calculate the size of the final stream, allocate temporary buffer in the encoder state and save the compressed data in the encoding state’s buffer In order to get final compressed JPEG"
  },
  {
    "id": 30766,
    "content": "stream, the user should provide the memory buffer large enough to store this compressed data There are two options for how to do this: Use the upper bound on compressed JPEG stream size for the given parameters and image dimensions: Use the nvjpegEncodeRetrieveBitstream() function to retrieve the maximum possible JPEG stream size at any given time Retrieve the compressed JPEG stream from the"
  },
  {
    "id": 30767,
    "content": "encoder state after successful encoding, using the nvjpegEncodeRetrieveBitstream() and the allocated buffer Wait for the encoding to complete, and retrieve the exact size of required buffer, as below: Encode the image using one of the encoding functions Use the nvjpegEncodeRetrieveBitstream() function to retrieve the size in bytes of the compressed JPEG stream Use the"
  },
  {
    "id": 30768,
    "content": "nvjpegEncodeRetrieveBitstream() function to populate your buffer with the compressed JPEG stream Note As the same encoding image state can be reused to compress a series of images, the nvjpegEncodeRetrieveBitstream() function will return the result for the last compressed image"
  },
  {
    "id": 30772,
    "content": "JPEG Encoding Example  See below the example code, and the block diagram shown in Figure 1 , for encoding with nvJPEG Encoder"
  },
  {
    "id": 30773,
    "content": "data (), & length , 0 );   write stream to file cudaStreamSynchronize ( stream ); std :: ofstream output_file ( \"test jpg\" , std :: ios :: out | std :: ios :: binary ); output_file"
  },
  {
    "id": 30778,
    "content": "nvjpegInputFormat_t  typedef enum { NVJPEG_INPUT_RGB = 3 , NVJPEG_INPUT_BGR = 4 , NVJPEG_INPUT_RGBI = 5 , NVJPEG_INPUT_BGRI = 6 } nvjpegInputFormat_t ; The nvjpegInputFormat_t enum is used to select the color model and pixel format of the input image Pixel format is interleaved BGR"
  },
  {
    "id": 30782,
    "content": "nvjpegEncoderState_t  The nvjpegEncoderState_t structure stores intermediate buffers and variables used for compression"
  },
  {
    "id": 30793,
    "content": "nvjpegEncoderStateCreate()  Creates encoder state that stores intermediate buffers used in compression Signature: nvjpegStatus_t nvjpegEncoderStateCreate ( nvjpegHandle_t handle , nvjpegEncoderState_t * encoder_state , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle encoder_state Output Host Pointer to the encoder state structure,"
  },
  {
    "id": 30794,
    "content": "where the new state will be placed stream Inputt Host CUDA stream where all the required device operations will be placed"
  },
  {
    "id": 30798,
    "content": "nvjpegEncoderStateDestroy()  Destroys the encoder state Signature: nvjpegStatus_t nvjpegEncoderStateDestroy ( nvjpegEncoderState_t encoder_state ); Parameters: Parameter Input / Output Memory Description encoder_state Input/Output Host Encoder state structure that will be released"
  },
  {
    "id": 30802,
    "content": "nvjpegEncoderParamsCreate()  Creates the structure that holds the compression parameters Signature: nvjpegStatus_t nvjpegEncoderParamsCreate ( nvjpegHandle_t handle , nvjpegEncoderParams_t * encoder_params , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle encoder_params Output Host Pointer to the location where the new parameters"
  },
  {
    "id": 30807,
    "content": "nvjpegEncoderParamsDestroy()  Destroys the encoder parameters structure Signature: nvjpegEncoderParamsDestroy ( nvjpegEncoderParams_t encoder_params ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder params structure that will be released"
  },
  {
    "id": 30811,
    "content": "nvjpegEncoderParamsSetEncoding()  Sets the parameter quality in the encoder parameters structure Signature: nvjpegStatus_t nvjpegEncoderParamsSetEncoding ( nvjpegEncoderParams_t encoder_params , nvjpegJpegEncoding_t etype , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle stream Input Host CUDA"
  },
  {
    "id": 30816,
    "content": "nvjpegEncoderParamsSetQuality()  Sets the parameter quality in the encoder parameters structure Signature: nvjpegStatus_t nvjpegEncoderParamsSetQuality ( nvjpegEncoderParams_t encoder_params , const int quality , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameterss structure handle quality Input Host Integer value of"
  },
  {
    "id": 30817,
    "content": "quality between 1 and 100, where 100 is the highest quality stream Input Host CUDA stream where all the required device operations will be placed"
  },
  {
    "id": 30821,
    "content": "nvjpegEncoderParamsSetOptimizedHuffman()  Sets whether or not to use optimized Huffman Using optimized Huffman produces smaller JPEG bitstream sizes with the same quality, but with slower performance Signature: nvjpegStatus_t nvjpegEncoderParamsSetOptimizedHuffman ( nvjpegEncoderParams_t encoder_params , const int optimized , cudaStream_t stream ); Parameters: Parameter Input / Output Memory"
  },
  {
    "id": 30822,
    "content": "Description encoder_params Input/Output Host Encoder parameters structure handle stream Input Host CUDA stream where all the required device operations will be placed"
  },
  {
    "id": 30826,
    "content": "nvjpegEncoderParamsSetSamplingFactors()  Sets which chroma subsampling will be used for JPEG compression"
  },
  {
    "id": 30827,
    "content": "Signature: nvjpegStatus_t nvjpegEncoderParamsSetSamplingFactors ( nvjpegEncoderParams_t encoder_params , const nvjpegChromaSubsampling_t chroma_subsampling , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_params Input/Output Host Encoder parameters structure handle If the input is in YUV color model and chroma_subsampling is different from the subsampling"
  },
  {
    "id": 30828,
    "content": "factors of source image, then the NVJPEG library will convert subsampling to the value of chroma_subsampling stream Input Host CUDA stream where all the required device operations will be placed"
  },
  {
    "id": 30835,
    "content": "nvjpegEncodeGetBufferSize()  Returns the maximum possible buffer size that is needed to store the compressed JPEG stream, for the given input parameters Signature: nvjpegStatus_t nvjpegEncodeGetBufferSize ( nvjpegHandle_t handle , const nvjpegEncoderParams_t encoder_params , int image_width , int image_height , size_t * max_stream_length ); Parameters: Parameter Input / Output Memory Description"
  },
  {
    "id": 30840,
    "content": "nvjpegEncodeYUV()  Compresses the image in YUV colorspace to JPEG stream using the provided parameters, and stores it in the state structure"
  },
  {
    "id": 30841,
    "content": "Signature: nvjpegStatus_t nvjpegEncodeYUV ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegChromaSubsampling_t chroma_subsampling , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle encoder_state"
  },
  {
    "id": 30842,
    "content": "Input/Output Host Internal structure that holds the temporary buffers required for the compression and also stores the final compressed JPEG stream source Input Host Pointer to the nvjpeg structure that holds the device pointers to the Y, U(Cb) and V(Cr) image planes and the respective strides chroma_subsampling Input Host Chroma subsampling of the input data"
  },
  {
    "id": 30846,
    "content": "nvjpegEncodeImage()  Compresses the image in the provided format to the JPEG stream using the provided parameters, and stores it in the state structure"
  },
  {
    "id": 30847,
    "content": "Signature: nvjpegStatus_t nvjpegEncodeImage ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , const nvjpegEncoderParams_t encoder_params , const nvjpegImage_t * source , nvjpegInputFormat_t input_format , int image_width , int image_height , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle input_format Input Host Value of"
  },
  {
    "id": 30852,
    "content": "nvjpegEncodeRetrieveBitstream()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions If data parameter is NULL then the encoder will return compressed stream size in the length parameter If data is not NULL then the provided length parameter should contain the data buffer size If the provided length is less than compressed stream size,"
  },
  {
    "id": 30853,
    "content": "then an error will be returned Otherwise the compressed stream will be stored in the data buffer and the actual compressed buffer size will be stored in the length parameter Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstream ( nvjpegHandle_t handle , nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory"
  },
  {
    "id": 30854,
    "content": "Description handle Input Host Library handle encoder_state Input/Output Host The encoder_state that was previously used in one of the encoder functions data Input/Output Host Pointer to the buffer in the host memory where the compressed stream will be stored On return the NVJPEG library will store the actual compressed stream size in this parameter"
  },
  {
    "id": 30858,
    "content": "nvjpegEncodeRetrieveBitstreamDevice()  Retrieves the compressed stream from the encoder state that was previously used in one of the encoder functions data parameter should be on device memory If data parameter is NULL then the encoder will return compressed stream size in the length parameter Signature: nvjpegStatus_t nvjpegEncodeRetrieveBitstreamDevice ( nvjpegHandle_t handle ,"
  },
  {
    "id": 30859,
    "content": "nvjpegEncoderState_t encoder_state , unsigned char * data , size_t * length , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description handle Input Host Library handle data Input/Output Device Pointer to the buffer in the device memory where the compressed stream will be stored"
  },
  {
    "id": 30868,
    "content": "nvjpegEncoderParamsCopyMetadata()  Copies the metadata (JFIF, APP, EXT, and COM markers) from the parsed stream"
  },
  {
    "id": 30869,
    "content": "Signature: nvjpegStatus_t nvjpegEncoderParamsCopyMetadata ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression stream In Host CUDA stream where all the"
  },
  {
    "id": 30874,
    "content": "nvjpegEncoderParamsCopyQuantizationTables()  Copies the quantization tables from the parsed stream Signature: nvjpegStatus_t nvjpegEncoderParamsCopyQuantizationTables ( nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encode_params Out Host Encoder parameters that will be used for compression"
  },
  {
    "id": 30878,
    "content": "nvjpegEncoderParamsCopyHuffmanTables() [Deprecated]  nvjpegEncoderParamsCopyHuffmanTables() is now deprecated"
  },
  {
    "id": 30879,
    "content": "Due to precision differences in the JPEG encode/decode process, the input huffman tables may no longer be valid for the image being encoded and may result in corrupt bitstream"
  },
  {
    "id": 30880,
    "content": "Signature: nvjpegStatus_t nvjpegEncoderParamsCopyHuffmanTables ( nvjpegEncoderState_t encoder_state , nvjpegEncoderParams_t encode_params , nvjpegJpegStream_t jpeg_stream , cudaStream_t stream ); Parameters: Parameter Input / Output Memory Description encoder_state In/Out Host Internal structure that stores the temporary buffers required for the compression"
  },
  {
    "id": 30884,
    "content": "Known Issues  Decoupled APIs, when initialized with NVJPEG_BACKEND_GPU_HYBRID , may not be able to correctly decode jpeg bitstreams which have out of bound run length codes"
  },
  {
    "id": 30888,
    "content": "Notice  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained"
  },
  {
    "id": 30889,
    "content": "herein NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any"
  },
  {
    "id": 30891,
    "content": "Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete"
  },
  {
    "id": 30892,
    "content": "NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”) NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA"
  },
  {
    "id": 30893,
    "content": "product referenced in this document NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage NVIDIA accepts no liability for inclusion"
  },
  {
    "id": 30894,
    "content": "and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is"
  },
  {
    "id": 30895,
    "content": "suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document NVIDIA"
  },
  {
    "id": 30896,
    "content": "accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document Information"
  },
  {
    "id": 30897,
    "content": "published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of"
  },
  {
    "id": 30898,
    "content": "NVIDIA Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND"
  },
  {
    "id": 30899,
    "content": "OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS ” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES,"
  },
  {
    "id": 30900,
    "content": "INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability"
  },
  {
    "id": 30901,
    "content": "towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product"
  },
  {
    "id": 30908,
    "content": "Trademarks  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U"
  },
  {
    "id": 30910,
    "content": "Other company and product names may be trademarks of the respective companies with which they are associated"
  },
  {
    "id": 30911,
    "content": "Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright © 2018-2024, NVIDIA Corporation & affiliates"
  }
]